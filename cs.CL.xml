<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;Prompt&#30340;&#20316;&#25991;Trait&#35780;&#20998;&#27169;&#22411;&#65292;&#36890;&#36807;&#20316;&#25991;&#25552;&#31034;&#20851;&#27880;&#21644;Traint&#30456;&#20284;&#24615;loss&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20316;&#25991;&#25552;&#31034;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16826</link><description>&lt;p&gt;
Prompt-&#21644;Trait&#20851;&#31995;&#24863;&#30693;&#30340;&#36328;Prompt&#20316;&#25991;Trait&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Prompt- and Trait Relation-aware Cross-prompt Essay Trait Scoring. (arXiv:2305.16826v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;Prompt&#30340;&#20316;&#25991;Trait&#35780;&#20998;&#27169;&#22411;&#65292;&#36890;&#36807;&#20316;&#25991;&#25552;&#31034;&#20851;&#27880;&#21644;Traint&#30456;&#20284;&#24615;loss&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20316;&#25991;&#25552;&#31034;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#65288;AES&#65289;&#30340;&#30446;&#30340;&#26159;&#23545;&#20889;&#20316;&#20027;&#39064;&#36827;&#34892;&#35780;&#20998;&#30340;&#25991;&#31456;&#65292;&#35813;&#20027;&#39064;&#23450;&#20041;&#20102;&#20889;&#20316;&#20027;&#39064;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;AES&#31995;&#32479;&#20551;&#23450;&#23545;&#20110;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#30456;&#21516;&#25552;&#31034;&#35780;&#20998;&#25991;&#31456;&#65292;&#24182;&#20998;&#37197;&#20165;&#25972;&#20307;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35774;&#32622;&#19982;&#23454;&#38469;&#25945;&#32946;&#24773;&#20917;&#20914;&#31361;&#65307;&#29305;&#23450;&#25552;&#31034;&#30340;&#39044;&#20998;&#32423;&#25991;&#31456;&#32570;&#20047;&#65292;&#24182;&#19988;&#38656;&#35201;&#35814;&#32454;&#30340;&#23376;&#37327;&#35268;&#30340;Trait&#20998;&#25968;&#12290;&#22240;&#27492;&#65292;&#39044;&#27979;&#30475;&#19981;&#35265;&#30340;Prompt&#25991;&#31456;&#30340;&#21508;&#31181;Trait&#20998;&#25968;&#65288;&#31216;&#20026;&#36328;Prompt&#20316;&#25991;Trait&#35780;&#20998;&#65289;&#26159;AES&#30340;&#19968;&#39033;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65306;Prompt-&#21644;Trait&#20851;&#31995;&#24863;&#30693;&#30340;&#36328;Prompt&#20316;&#25991;Trait&#35780;&#20998;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#20316;&#25991;&#25552;&#31034;&#20851;&#27880;&#21644;&#21033;&#29992;&#30001;&#20027;&#39064;&#24314;&#27169;&#26426;&#21046;&#25552;&#21462;&#30340;&#20027;&#39064;&#36830;&#36143;&#24615;&#29305;&#24449;&#23545;&#20316;&#25991;&#24863;&#30693;&#36827;&#34892;&#32534;&#30721;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#26631;&#35760;&#25968;&#25454;&#65307;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29978;&#33267;&#22312;&#36328;Prompt&#35774;&#32622;&#20013;&#20063;&#32771;&#34385;&#21040;&#20316;&#25991;&#30340;&#25552;&#31034;&#24682;&#23432;&#12290;&#20026;&#20102;&#20419;&#36827;&#22810;Trait&#35780;&#20998;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;Trait&#30456;&#20284;&#24615;lo
&lt;/p&gt;
&lt;p&gt;
Automated essay scoring (AES) aims to score essays written for a given prompt, which defines the writing topic. Most existing AES systems assume to grade essays of the same prompt as used in training and assign only a holistic score. However, such settings conflict with real-education situations; pre-graded essays for a particular prompt are lacking, and detailed trait scores of sub-rubrics are required. Thus, predicting various trait scores of unseen-prompt essays (called cross-prompt essay trait scoring) is a remaining challenge of AES. In this paper, we propose a robust model: prompt- and trait relation-aware cross-prompt essay trait scorer. We encode prompt-aware essay representation by essay-prompt attention and utilizing the topic-coherence feature extracted by the topic-modeling mechanism without access to labeled data; therefore, our model considers the prompt adherence of an essay, even in a cross-prompt setting. To facilitate multi-trait scoring, we design trait-similarity lo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#12289;&#22522;&#20110;&#21152;&#26435;&#24179;&#22343;&#30340;&#39046;&#22495;&#23545;&#40784;&#21069;&#32512;&#24179;&#22343;&#26041;&#27861;&#65288;DAPA&#65289;&#65292;&#29992;&#20110;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#28304;&#22495;&#25193;&#23637;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16820</link><description>&lt;p&gt;
&#38754;&#21521;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#30340;&#39046;&#22495;&#23545;&#40784;&#21069;&#32512;&#24179;&#22343;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Domain Aligned Prefix Averaging for Domain Generalization in Abstractive Summarization. (arXiv:2305.16820v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#12289;&#22522;&#20110;&#21152;&#26435;&#24179;&#22343;&#30340;&#39046;&#22495;&#23545;&#40784;&#21069;&#32512;&#24179;&#22343;&#26041;&#27861;&#65288;DAPA&#65289;&#65292;&#29992;&#20110;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#28304;&#22495;&#25193;&#23637;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20110;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#65292;&#22522;&#20110;&#21152;&#26435;&#24179;&#22343;&#30340;&#39046;&#22495;&#23545;&#40784;&#21069;&#32512;&#24179;&#22343;&#26041;&#27861;&#65288;DAPA&#65289;&#12290;&#36890;&#36807;&#32473;&#23450;&#22810;&#20010;&#28304;&#22495;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20026;&#27599;&#20010;&#22495;&#35757;&#32451;&#19968;&#20010;&#21069;&#32512;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#21069;&#32512;&#29983;&#25104;&#23569;&#37327;&#30446;&#26631;&#22495;&#25991;&#26723;&#30340;&#25688;&#35201;&#65292;&#35745;&#31639;&#25152;&#38656;&#30340;&#26435;&#37325;&#26469;&#24179;&#22343;&#28304;&#21069;&#32512;&#12290;&#22312;DAPA&#20013;&#65292;&#21069;&#32512;&#35843;&#25972;&#20801;&#35768;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#65292;&#21152;&#26435;&#24179;&#22343;&#20801;&#35768;&#26377;&#25928;&#22320;&#28155;&#21152;&#26032;&#30340;&#28304;&#22495;&#12290;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#25688;&#35201;&#39046;&#22495;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;DAPA&#34920;&#29616;&#20986;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#21069;&#32512;&#24179;&#22343;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization is hitherto an underexplored area applied in abstractive summarization. Moreover, most existing works on domain generalization have sophisticated training algorithms. In this paper, we propose a lightweight, weight averaging based, Domain Aligned Prefix Averaging approach to domain generalization for abstractive summarization. Given a number of source domains, our method first trains a prefix for each one of them. These source prefixes generate summaries for a small number of target domain documents. The similarity of the generated summaries to their corresponding documents is used for calculating weights required to average source prefixes. In DAPA, prefix tuning allows for lightweight finetuning, and weight averaging allows for the computationally efficient addition of new source domains. When evaluated on four diverse summarization domains, DAPA shows comparable or better performance against the baselines, demonstrating the effectiveness of its prefix averaging
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#65292;&#22312;&#20219;&#21153;&#33258;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#19982;&#31283;&#20581;&#30340;&#25512;&#29702;&#36807;&#31243;&#30456;&#32467;&#21512;&#19979;&#65292;&#32431;NLI&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#26356;&#22797;&#26434;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.16819</link><description>&lt;p&gt;
&#22312;&#36731;&#24494;&#25512;&#21160;&#19979;&#65292;NLI&#27169;&#22411;&#21487;&#20197;&#39640;&#25928;&#12289;&#31283;&#20581;&#22320;&#39044;&#27979;&#24544;&#23454;&#24230;
&lt;/p&gt;
&lt;p&gt;
With a Little Push, NLI Models can Robustly and Efficiently Predict Faithfulness. (arXiv:2305.16819v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#65292;&#22312;&#20219;&#21153;&#33258;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#19982;&#31283;&#20581;&#30340;&#25512;&#29702;&#36807;&#31243;&#30456;&#32467;&#21512;&#19979;&#65292;&#32431;NLI&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#26356;&#22797;&#26434;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#29983;&#25104;&#19981;&#24544;&#23454;&#30340;&#36755;&#20986;&#65292;&#36825;&#20123;&#36755;&#20986;&#19982;&#20854;&#36755;&#20837;&#19981;&#19968;&#33268;&#12290;&#36825;&#20123;&#19981;&#24544;&#23454;&#30340;&#29983;&#25104;&#20250;&#21361;&#21450;&#21040;&#24212;&#29992;&#31243;&#24207;&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#20449;&#20219;&#65292;&#20363;&#22914;&#27010;&#36848;&#25110;&#20154;&#26426;&#20114;&#21160;&#65292;&#36825;&#20419;&#20351;&#38656;&#35201;&#33258;&#21160;&#24544;&#23454;&#24615;&#24230;&#37327;&#26631;&#20934;&#30340;&#20135;&#29983;&#12290;&#20026;&#20102;&#23454;&#26045;&#36825;&#26679;&#30340;&#26631;&#20934;&#65292;NLI&#27169;&#22411;&#20284;&#20046;&#24456;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#35299;&#20915;&#19968;&#20010;&#19982;&#20247;&#22810;&#20808;&#21069;&#30740;&#31350;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#36328;&#25968;&#25454;&#38598;&#21487;&#38752;&#25191;&#34892;&#26041;&#38754;&#65292;NLI&#27169;&#22411;&#38656;&#35201;&#26114;&#36149;&#30340;&#38468;&#21152;&#26426;&#22120;&#65292;&#20363;&#22914;&#22312;&#36755;&#20837;&#21644;&#29983;&#25104;&#30340;&#21477;&#23376;&#30340;&#31515;&#21345;&#23572;&#31215;&#19978;&#36816;&#34892;&#25512;&#29702;&#65292;&#25110;&#36890;&#36807;&#25903;&#25345;&#38382;&#31572;&#27493;&#39588;&#26469;&#25903;&#25345;&#23427;&#20204;&#12290;&#22312;&#36825;&#20221;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#20219;&#21153;&#33258;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#19982;&#31283;&#20581;&#30340;&#25512;&#29702;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#32431;NLI&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#26356;&#22797;&#26434;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#25514;&#26045;&#65306;&#65288;1&#65289;&#22686;&#21152;NLI&#22521;&#35757;&#25968;&#25454;&#65292;&#20197;&#36866;&#24212;&#23545;&#35805;&#20013;&#24544;&#23454;&#24230;&#39044;&#27979;&#30340;&#29305;&#27530;&#24615;&#65307;&#65288;2&#65289;&#21033;&#29992;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional language models still generate unfaithful output that is not supported by their input. These unfaithful generations jeopardize trust in real-world applications such as summarization or human-machine interaction, motivating a need for automatic faithfulness metrics. To implement such metrics, NLI models seem attractive, since they solve a strongly related task that comes with a wealth of prior research and data. But recent research suggests that NLI models require costly additional machinery to perform reliably across datasets, e.g., by running inference on a cartesian product of input and generated sentences, or supporting them with a question-generation/answering step.  In this work we show that pure NLI models _can_ outperform more complex metrics when combining task-adaptive data augmentation with robust inference procedures. We propose: (1) Augmenting NLI training data to adapt NL inferences to the specificities of faithfulness prediction in dialogue; (2) Making use of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#21809;&#21644;&#21487;&#25511;&#30340;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27468;&#35789;&#32763;&#35793;&#24418;&#24335;&#21270;&#20026;&#21463;&#32422;&#26463;&#30340;&#32763;&#35793;&#38382;&#39064;&#65292;&#23558;&#32763;&#35793;&#23398;&#25991;&#29486;&#20013;&#30340;&#29702;&#35770;&#25351;&#23548;&#21644;&#23454;&#38469;&#25216;&#26415;&#36716;&#21270;&#20026;&#39537;&#21160;&#30340;NMT&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#33521;&#27721;&#27468;&#35789;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#22312;&#38271;&#24230;&#20934;&#30830;&#24615;&#12289;&#38901;&#24459;&#20934;&#30830;&#24615;&#21644;&#21333;&#35789;&#36793;&#30028;&#21484;&#22238;&#29575;&#19978;&#24471;&#21040;&#20248;&#31168;&#30340;&#25104;&#32489;&#65292;&#30456;&#27604;&#26420;&#32032;&#30340;&#24494;&#35843;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.16816</link><description>&lt;p&gt;
&#36328;&#36234;&#36793;&#30028;&#30340;&#27468;&#26354;&#65306;&#21487;&#21809;&#21644;&#21487;&#25511;&#30340;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Songs Across Borders: Singable and Controllable Neural Lyric Translation. (arXiv:2305.16816v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#21809;&#21644;&#21487;&#25511;&#30340;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27468;&#35789;&#32763;&#35793;&#24418;&#24335;&#21270;&#20026;&#21463;&#32422;&#26463;&#30340;&#32763;&#35793;&#38382;&#39064;&#65292;&#23558;&#32763;&#35793;&#23398;&#25991;&#29486;&#20013;&#30340;&#29702;&#35770;&#25351;&#23548;&#21644;&#23454;&#38469;&#25216;&#26415;&#36716;&#21270;&#20026;&#39537;&#21160;&#30340;NMT&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#33521;&#27721;&#27468;&#35789;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#22312;&#38271;&#24230;&#20934;&#30830;&#24615;&#12289;&#38901;&#24459;&#20934;&#30830;&#24615;&#21644;&#21333;&#35789;&#36793;&#30028;&#21484;&#22238;&#29575;&#19978;&#24471;&#21040;&#20248;&#31168;&#30340;&#25104;&#32489;&#65292;&#30456;&#27604;&#26420;&#32032;&#30340;&#24494;&#35843;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#36890;&#29992;&#39046;&#22495;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#26041;&#27861;&#24471;&#21040;&#20102;&#26174;&#33879;&#21457;&#23637;&#65292;&#20294;&#26159;&#20854;&#36755;&#20986;&#30340;&#33258;&#28982;&#24230;&#21644;&#38899;&#20048;&#32422;&#26463;&#30340;&#19981;&#36275;&#20351;&#23427;&#20204;&#26080;&#27861;&#20135;&#29983;&#21487;&#21809;&#30340;&#27468;&#35789;&#32763;&#35793;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#27468;&#35789;&#32763;&#35793;&#24418;&#24335;&#21270;&#20026;&#21463;&#32422;&#26463;&#30340;&#32763;&#35793;&#38382;&#39064;&#65292;&#23558;&#32763;&#35793;&#23398;&#25991;&#29486;&#20013;&#30340;&#29702;&#35770;&#25351;&#23548;&#21644;&#23454;&#38469;&#25216;&#26415;&#36716;&#21270;&#20026;&#39537;&#21160;&#30340;NMT&#26041;&#27861;&#65292;&#25506;&#32034;&#26356;&#22909;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#23454;&#20363;&#21270;&#20026;&#19968;&#31181;&#33521;&#27721;&#27468;&#35789;&#32763;&#35793;&#31995;&#32479;&#26469;&#22635;&#34917;&#21487;&#21809;&#27468;&#35789;&#32763;&#35793;&#21697;&#36136;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#38271;&#24230;&#20934;&#30830;&#24615;&#12289;&#38901;&#24459;&#20934;&#30830;&#24615;&#21644;&#21333;&#35789;&#36793;&#30028;&#21484;&#22238;&#29575;&#26041;&#38754;&#20998;&#21035;&#36798;&#21040;&#20102;99.85%&#12289;99.00%&#21644;95.52%&#12290;&#22312;&#20027;&#35266;&#35780;&#20272;&#20013;&#65292;&#19982;&#26420;&#32032;&#30340;&#24494;&#35843;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25972;&#20307;&#36136;&#37327;&#19978;&#34920;&#29616;&#20986;&#20102;75%&#30340;&#30456;&#23545;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of general-domain neural machine translation (NMT) methods has advanced significantly in recent years, but the lack of naturalness and musical constraints in the outputs makes them unable to produce singable lyric translations. This paper bridges the singability quality gap by formalizing lyric translation into a constrained translation problem, converting theoretical guidance and practical techniques from translatology literature to prompt-driven NMT approaches, exploring better adaptation methods, and instantiating them to an English-Chinese lyric translation system. Our model achieves 99.85%, 99.00%, and 95.52% on length accuracy, rhyme accuracy, and word boundary recall. In our subjective evaluation, our model shows a 75% relative enhancement on overall quality, compared against naive fine-tuning (Code available at https://github.com/Sonata165/ControllableLyricTranslation).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;GenQ&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#29031;&#39038;&#32773;&#21644;&#23401;&#23376;&#20043;&#38388;&#30340;&#23545;&#35805;&#20419;&#36827;&#23401;&#23376;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#25991;&#21270;&#32972;&#26223;&#21644;&#35821;&#22659;&#21464;&#21270;&#20197;&#25552;&#39640;&#31995;&#32479;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16809</link><description>&lt;p&gt;
GenQ&#65306;&#33258;&#21160;&#21270;&#38382;&#31572;&#29983;&#25104;&#22120;&#20197;&#24110;&#21161;&#29031;&#39038;&#32773;&#19982;&#23401;&#23376;&#20849;&#35835;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
GenQ: Automated Question Generation to Support Caregivers While Reading Stories with Children. (arXiv:2305.16809v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;GenQ&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#29031;&#39038;&#32773;&#21644;&#23401;&#23376;&#20043;&#38388;&#30340;&#23545;&#35805;&#20419;&#36827;&#23401;&#23376;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#25991;&#21270;&#32972;&#26223;&#21644;&#35821;&#22659;&#21464;&#21270;&#20197;&#25552;&#39640;&#31995;&#32479;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#29031;&#39038;&#32773;&#35810;&#38382;&#24320;&#25918;&#24335;&#38382;&#39064;&#20197;&#28608;&#21457;&#19982;&#23401;&#23376;&#30340;&#23545;&#35805;&#26102;&#65292;&#21487;&#20197;&#20419;&#36827;&#23401;&#23376;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;&#34429;&#28982;&#26377;&#21033;&#29992;&#25216;&#26415;&#24037;&#20855;&#26469;&#25903;&#25345;&#36825;&#20010;&#36807;&#31243;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#8221;&#30340;&#31354;&#38388;&#65292;&#20294;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#29616;&#26377;&#30340;&#29983;&#25104;&#31867;&#20154;&#35821;&#35328;&#38382;&#39064;&#30340;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#26377;&#30410;&#12290;&#27492;&#22806;&#65292;&#29992;&#20110;&#24320;&#21457;&#36825;&#20123;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#31995;&#32479;&#30340;&#22521;&#35757;&#25968;&#25454;&#36890;&#24120;&#27809;&#26377;&#32771;&#34385;&#21040;&#20154;&#21475;&#32479;&#35745;&#23398;&#65292;&#20294;&#20855;&#26377;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#30340;&#20154;&#21487;&#33021;&#20250;&#25552;&#20986;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;&#20316;&#20026;&#20026;&#25289;&#19969;&#35028;&#20799;&#31461;&#35774;&#35745;&#26234;&#33021;&#38405;&#35835;&#25903;&#25345;&#24212;&#29992;&#31243;&#24207;&#30340;&#24191;&#27867;&#39033;&#30446;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#20174;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#23398;&#30340;&#25289;&#19969;&#35028;&#25252;&#29702;&#20154;&#21592;&#21644;&#38750;&#25252;&#29702;&#20154;&#21592;&#20197;&#21450;&#20854;&#20182;&#20154;&#21475;&#32479;&#35745;&#23398;&#32972;&#26223;&#30340;&#25252;&#29702;&#20154;&#21592;&#21644;&#38750;&#25252;&#29702;&#20154;&#21592;&#20013;&#32676;&#38598;&#22823;&#37327;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#20013;&#20010;&#20307;&#12289;&#25991;&#21270;&#21644;&#29615;&#22659;&#22240;&#32032;&#20013;&#20171;&#30340;&#38382;&#39064;&#25552;&#38382;&#30340;&#21464;&#21270;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#33258;&#21160;&#20135;&#29983;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
When caregivers ask open--ended questions to motivate dialogue with children, it facilitates the child's reading comprehension skills.Although there is scope for use of technological tools, referred here as "intelligent tutoring systems", to scaffold this process, it is currently unclear whether existing intelligent systems that generate human--language like questions is beneficial. Additionally, training data used in the development of these automated question generation systems is typically sourced without attention to demographics, but people with different cultural backgrounds may ask different questions. As a part of a broader project to design an intelligent reading support app for Latinx children, we crowdsourced questions from Latinx caregivers and noncaregivers as well as caregivers and noncaregivers from other demographics. We examine variations in question--asking within this dataset mediated by individual, cultural, and contextual factors. We then design a system that autom
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#23383;&#31215;&#26497;&#24230;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#32763;&#35793;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.16806</link><description>&lt;p&gt;
GPT&#26159;&#21542;&#20250;&#20135;&#29983;&#26356;&#19981;&#20934;&#30830;&#30340;&#32763;&#35793;?
&lt;/p&gt;
&lt;p&gt;
Do GPTs Produce Less Literal Translations?. (arXiv:2305.16806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#23383;&#31215;&#26497;&#24230;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#32763;&#35793;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3&#65292;&#24050;&#32463;&#25104;&#20026;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25110;&#29702;&#35299;&#20219;&#21153;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20219;&#21153;&#20013;&#65292;&#24050;&#26377;&#22810;&#39033;&#30740;&#31350;&#25506;&#32034;&#21033;&#29992;few-shot&#25552;&#31034;&#26426;&#21046;&#20174;LLMs&#20013;&#24341;&#20986;&#26356;&#22909;&#30340;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#30456;&#23545;&#36739;&#23569;&#22320;&#20851;&#27880;&#36825;&#31181;&#32763;&#35793;&#19982;&#26631;&#20934;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#29983;&#25104;&#32763;&#35793;&#30340;&#36136;&#37327;&#24046;&#24322;&#12290;&#26412;&#30740;&#31350;&#20174;&#25991;&#23383;&#23545;&#40784;&#21644;&#21333;&#35843;&#24615;&#31561;&#26041;&#38754;&#65292;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#26412;&#25991;&#23383;&#31215;&#26497;&#24230;&#65292;&#21457;&#29616;GPT&#20174;&#33521;&#35821;&#65288;E-X&#65289;&#32763;&#35793;&#30340;&#25991;&#26412;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#20063;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#21516;&#26102;&#65292;&#24403;&#32763;&#35793;&#21477;&#23376;&#38271;&#24230;&#22686;&#21152;&#26102;&#65292;&#36825;&#31181;&#24046;&#21035;&#23601;&#23588;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks. On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs. However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models. In this work, we investigate these differences in terms of the literalness of translations produced by the two systems. Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E-X) from GPTs tend to be less literal, while exhibiting similar or better scores on MT quality metrics. We demonstrate that this finding is borne out in human evaluations as well. We then show that these differences are especially pronounced when translating senten
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#21644;&#25197;&#30697;&#30340;&#25163;&#35821;&#35270;&#39057;&#25688;&#35201;&#25216;&#26415;&#65292;&#33021;&#22815;&#36873;&#20986;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#20851;&#38190;&#24103;&#12290;</title><link>http://arxiv.org/abs/2305.16801</link><description>&lt;p&gt;
&#22522;&#20110;&#26354;&#29575;&#21644;&#25197;&#30697;&#30340;&#25163;&#35821;&#35270;&#39057;&#25688;&#35201;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Motion-Based Sign Language Video Summarization using Curvature and Torsion. (arXiv:2305.16801v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16801
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#21644;&#25197;&#30697;&#30340;&#25163;&#35821;&#35270;&#39057;&#25688;&#35201;&#25216;&#26415;&#65292;&#33021;&#22815;&#36873;&#20986;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#20851;&#38190;&#24103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25688;&#35201;&#25216;&#26415;&#22312;&#24456;&#22810;&#22522;&#20110;&#35270;&#39057;&#30340;&#24212;&#29992;&#20013;&#37117;&#38750;&#24120;&#26377;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25163;&#35821;&#35270;&#39057;&#25688;&#35201;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#20174;&#35270;&#39057;&#24103;&#20013;&#25552;&#21462;&#30340;&#19977;&#32500;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#26469;&#27169;&#22411;&#21270;&#27599;&#19968;&#24103;&#20013;&#30340;&#19977;&#32500;&#36816;&#21160;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#21644;&#25197;&#30697;&#30340;&#26032;&#22411;&#20449;&#24687;&#20989;&#25968;&#65292;&#20197;&#20415;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#20851;&#38190;&#24103;&#12290;
&lt;/p&gt;
&lt;p&gt;
An interesting problem in many video-based applications is the generation of short synopses by selecting the most informative frames, a procedure which is known as video summarization. For sign language videos the benefits of using the $t$-parameterized counterpart of the curvature of the 2-D signer's wrist trajectory to identify keyframes, have been recently reported in the literature. In this paper we extend these ideas by modeling the 3-D hand motion that is extracted from each frame of the video. To this end we propose a new informative function based on the $t$-parameterized curvature and torsion of the 3-D trajectory. The method to characterize video frames as keyframes depends on whether the motion occurs in 2-D or 3-D space. Specifically, in the case of 3-D motion we look for the maxima of the harmonic mean of the curvature and torsion of the target's trajectory; in the planar motion case we seek for the maxima of the trajectory's curvature. The proposed 3-D feature is experime
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#26816;&#27979;&#36777;&#35770;&#25991;&#26412;&#20013;&#38656;&#35201;&#25913;&#36827;&#30340;&#35770;&#28857;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#23398;&#20064;&#21327;&#20316;&#32534;&#36753;&#34892;&#20026;&#25429;&#25417;&#38544;&#21547;&#30340;&#20462;&#35746;&#27169;&#24335;&#26469;&#24320;&#21457;&#25351;&#23548;&#20316;&#32773;&#25913;&#36827;&#35770;&#28857;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.16799</link><description>&lt;p&gt;
&#26159;&#21542;&#20462;&#25913;&#65306;&#23398;&#20064;&#26816;&#27979;&#21487;&#25913;&#36827;&#35770;&#28857;&#20197;&#25903;&#25345;&#36777;&#35770;&#20889;&#20316;
&lt;/p&gt;
&lt;p&gt;
To Revise or Not to Revise: Learning to Detect Improvable Claims for Argumentative Writing Support. (arXiv:2305.16799v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16799
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#26816;&#27979;&#36777;&#35770;&#25991;&#26412;&#20013;&#38656;&#35201;&#25913;&#36827;&#30340;&#35770;&#28857;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#23398;&#20064;&#21327;&#20316;&#32534;&#36753;&#34892;&#20026;&#25429;&#25417;&#38544;&#21547;&#30340;&#20462;&#35746;&#27169;&#24335;&#26469;&#24320;&#21457;&#25351;&#23548;&#20316;&#32773;&#25913;&#36827;&#35770;&#28857;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#36777;&#35770;&#25991;&#26412;&#30340;&#25514;&#36766;&#22312;&#39640;&#31561;&#25945;&#32946;&#21644;&#32844;&#19994;&#21457;&#23637;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#25991;&#26412;&#20013;&#19981;&#21516;&#35770;&#28857;&#26159;&#21542;&#38656;&#35201;&#20462;&#25913;&#21450;&#22914;&#20309;&#20462;&#25913;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21021;&#23398;&#32773;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35782;&#21035;&#38656;&#35201;&#29305;&#23450;&#20462;&#25913;&#30340;&#36777;&#35770;&#35770;&#28857;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#36890;&#36807;&#20174;&#22312;&#32447;&#36777;&#35770;&#20013;&#23398;&#20064;&#21327;&#20316;&#32534;&#36753;&#34892;&#20026;&#65292;&#25105;&#20204;&#35797;&#22270;&#25429;&#25417;&#38544;&#21547;&#30340;&#20462;&#35746;&#27169;&#24335;&#65292;&#20197;&#24320;&#21457;&#26088;&#22312;&#25351;&#23548;&#20316;&#32773;&#22914;&#20309;&#36827;&#19968;&#27493;&#25913;&#36827;&#20854;&#35770;&#28857;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#27604;&#36739;&#20102;&#24120;&#35265;&#35789;&#23884;&#20837;&#27169;&#22411;&#25429;&#25417;&#30456;&#21516;&#25991;&#26412;&#19981;&#21516;&#29256;&#26412;&#20043;&#38388;&#24046;&#24322;&#30340;&#33021;&#21147;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#20889;&#20316;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#24212;&#23545;&#22522;&#20110;&#20462;&#35746;&#30340;&#35821;&#26009;&#24211;&#30340;&#22024;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20462;&#35746;&#36317;&#31163;&#30340;&#26032;&#30340;&#37319;&#26679;&#31574;&#30053;&#12290;&#19982;&#20808;&#21069;&#24037;&#20316;&#30340;&#26041;&#27861;&#30456;&#21453;&#65292;&#36825;&#31181;&#37319;&#26679;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#39069;&#22806;&#27880;&#37322;&#21644;&#35780;&#20272;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing the phrasing of argumentative text is crucial in higher education and professional development. However, assessing whether and how the different claims in a text should be revised is a hard task, especially for novice writers. In this work, we explore the main challenges to identifying argumentative claims in need of specific revisions. By learning from collaborative editing behaviors in online debates, we seek to capture implicit revision patterns in order to develop approaches aimed at guiding writers in how to further improve their arguments. We systematically compare the ability of common word embedding models to capture the differences between different versions of the same text, and we analyze their impact on various types of writing issues. To deal with the noisy nature of revision-based corpora, we propose a new sampling strategy based on revision distance. Opposed to approaches from prior work, such sampling can be done without employing additional annotations and j
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#24335;&#30340;&#29992;&#25143;&#28385;&#24847;&#24230;&#24314;&#27169;&#26694;&#26550;SG-USM&#65292;&#23427;&#29305;&#21035;&#27169;&#25311;&#20102;&#31995;&#32479;&#31243;&#24230;&#30340;&#28385;&#36275;&#29992;&#25143;&#20851;&#20110;&#20219;&#21153;&#23646;&#24615;&#30340;&#20559;&#22909;&#31243;&#24230;&#65292;&#20197;&#39044;&#27979;&#29992;&#25143;&#30340;&#28385;&#24847;&#24230;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.16798</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#20013;&#22522;&#20110;&#27169;&#24335;&#30340;&#29992;&#25143;&#28385;&#24847;&#24230;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Schema-Guided User Satisfaction Modeling for Task-Oriented Dialogues. (arXiv:2305.16798v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#24335;&#30340;&#29992;&#25143;&#28385;&#24847;&#24230;&#24314;&#27169;&#26694;&#26550;SG-USM&#65292;&#23427;&#29305;&#21035;&#27169;&#25311;&#20102;&#31995;&#32479;&#31243;&#24230;&#30340;&#28385;&#36275;&#29992;&#25143;&#20851;&#20110;&#20219;&#21153;&#23646;&#24615;&#30340;&#20559;&#22909;&#31243;&#24230;&#65292;&#20197;&#39044;&#27979;&#29992;&#25143;&#30340;&#28385;&#24847;&#24230;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#28385;&#24847;&#24230;&#24314;&#27169;&#65288;USM&#65289;&#26159;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#35780;&#20272;&#30340;&#19968;&#31181;&#27969;&#34892;&#36873;&#25321;&#65292;&#20854;&#20013;&#29992;&#25143;&#28385;&#24847;&#24230;&#36890;&#24120;&#21462;&#20915;&#20110;&#31995;&#32479;&#26159;&#21542;&#23454;&#29616;&#20102;&#29992;&#25143;&#30340;&#20219;&#21153;&#30446;&#26631;&#12290;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20351;&#29992;&#20219;&#21153;&#26550;&#26500;&#65288;task schema&#65289;&#26469;&#32534;&#30721;&#29992;&#25143;&#30340;&#20219;&#21153;&#30446;&#26631;&#12290;&#29616;&#26377;&#30340;USM&#30740;&#31350;&#24573;&#30053;&#20102;&#20351;&#29992;&#20219;&#21153;&#26550;&#26500;&#26174;&#24335;&#24314;&#27169;&#29992;&#25143;&#30340;&#20219;&#21153;&#30446;&#26631;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#24335;&#30340;&#29992;&#25143;&#28385;&#24847;&#24230;&#24314;&#27169;&#26694;&#26550;SG-USM&#12290;&#23427;&#29305;&#21035;&#27169;&#25311;&#20102;&#31995;&#32479;&#31243;&#24230;&#30340;&#28385;&#36275;&#29992;&#25143;&#20851;&#20110;&#20219;&#21153;&#23646;&#24615;&#30340;&#20559;&#22909;&#31243;&#24230;&#65292;&#20197;&#39044;&#27979;&#29992;&#25143;&#30340;&#28385;&#24847;&#24230;&#27700;&#24179;&#12290;SG-USM&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#32534;&#30721;&#23545;&#35805;&#19978;&#19979;&#25991;&#21644;&#20219;&#21153;&#23646;&#24615;&#65292;&#24182;&#37319;&#29992;&#23653;&#34892;&#34920;&#31034;&#23618;&#26469;&#23398;&#20064;&#23545;&#35805;&#20013;&#23436;&#25104;&#20102;&#22810;&#23569;&#20219;&#21153;&#23646;&#24615;&#65292;&#37325;&#35201;&#24615;&#39044;&#27979;&#22120;&#29992;&#20110;&#35745;&#31639;&#20219;&#21153;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#37325;&#35201;&#20219;&#21153;&#23646;&#24615;&#20197;&#39044;&#27979;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;&#25105;&#20204;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;SG-USM&#65292;&#24182;&#26174;&#31034;&#23427;&#20248;&#20110;&#29616;&#26377;&#30340;USM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
User Satisfaction Modeling (USM) is one of the popular choices for task-oriented dialogue systems evaluation, where user satisfaction typically depends on whether the user's task goals were fulfilled by the system. Task-oriented dialogue systems use task schema, which is a set of task attributes, to encode the user's task goals. Existing studies on USM neglect explicitly modeling the user's task goals fulfillment using the task schema. In this paper, we propose SG-USM, a novel schema-guided user satisfaction modeling framework. It explicitly models the degree to which the user's preferences regarding the task attributes are fulfilled by the system for predicting the user's satisfaction level. SG-USM employs a pre-trained language model for encoding dialogue context and task attributes. Further, it employs a fulfillment representation layer for learning how many task attributes have been fulfilled in the dialogue, an importance predictor component for calculating the importance of task 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#26657;&#20934;&#21518;&#30340;Transformer&#27169;&#22411;&#26469;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#21387;&#21147;&#21644;&#25233;&#37057;&#30151;&#29366;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.16797</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#31038;&#20132;&#23186;&#20307;&#21387;&#21147;&#21644;&#25233;&#37057;&#35782;&#21035;&#27169;&#22411;&#30340;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Calibration of Transformer-based Models for Identifying Stress and Depression in Social Media. (arXiv:2305.16797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#26657;&#20934;&#21518;&#30340;Transformer&#27169;&#22411;&#26469;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#21387;&#21147;&#21644;&#25233;&#37057;&#30151;&#29366;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#24555;&#33410;&#22863;&#30340;&#29983;&#27963;&#20013;&#65292;&#21387;&#21147;&#21644;&#25233;&#37057;&#30151;&#30340;&#21457;&#30149;&#29575;&#21576;&#29616;&#19978;&#21319;&#36235;&#21183;&#12290;&#31038;&#20132;&#23186;&#20307;&#20026;&#26089;&#26399;&#21457;&#29616;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20171;&#32461;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#24182;&#35757;&#32451;&#27973;&#23618;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#12290;&#20854;&#20182;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25110;Transformer&#27169;&#22411;&#12290;&#23613;&#31649;Transformer&#27169;&#22411;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#20016;&#23500;&#30340;&#23454;&#38469;&#30693;&#35782;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26088;&#22312;&#22686;&#24378;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#30740;&#31350;&#65292;&#20294;&#27809;&#26377;&#20808;&#21069;&#30340;&#24037;&#20316;&#21033;&#29992;&#36825;&#20123;&#20462;&#25913;&#26469;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#26816;&#27979;&#21387;&#21147;&#21644;&#25233;&#37057;&#30151;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20854;&#39044;&#27979;&#20013;&#30340;&#32622;&#20449;&#24230;&#21487;&#38752;&#24615;&#23545;&#20110;&#39640;&#39118;&#38505;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23578;&#26410;&#26377;&#20808;&#21069;&#30340;&#24037;&#20316;&#32771;&#34385;&#27169;&#22411;&#30340;&#26657;&#20934;&#12290;&#20026;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#36807;&#27169;&#22411;&#26657;&#20934;&#26469;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#21387;&#21147;&#21644;&#25233;&#37057;&#30151;&#29366;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's fast-paced world, the rates of stress and depression present a surge. Social media provide assistance for the early detection of mental health conditions. Existing methods mainly introduce feature extraction approaches and train shallow machine learning classifiers. Other researches use deep neural networks or transformers. Despite the fact that transformer-based models achieve noticeable improvements, they cannot often capture rich factual knowledge. Although there have been proposed a number of studies aiming to enhance the pretrained transformer-based models with extra information or additional modalities, no prior work has exploited these modifications for detecting stress and depression through social media. In addition, although the reliability of a machine learning model's confidence in its predictions is critical for high-risk applications, there is no prior work taken into consideration the model calibration. To resolve the above issues, we present the first study i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;'RSTformer'&#30340;&#25688;&#35201;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20840;&#38754;&#34701;&#21512;&#20102;&#35805;&#35821;&#20851;&#31995;&#31867;&#22411;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20197;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#20026;&#22522;&#30784;&#65292;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#65292;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16784</link><description>&lt;p&gt;
&#32467;&#21512;&#35805;&#35821;&#32467;&#26500;&#20998;&#24067;&#30340;&#38271;&#25991;&#26412;&#33258;&#21160;&#25688;&#35201;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Incorporating Distributions of Discourse Structure for Long Document Abstractive Summarization. (arXiv:2305.16784v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;'RSTformer'&#30340;&#25688;&#35201;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20840;&#38754;&#34701;&#21512;&#20102;&#35805;&#35821;&#20851;&#31995;&#31867;&#22411;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20197;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#20026;&#22522;&#30784;&#65292;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#65292;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25991;&#26412;&#25688;&#35201;&#65292;&#35805;&#35821;&#32467;&#26500;&#22312;&#36776;&#35782;&#25991;&#26412;&#26680;&#24515;&#20869;&#23481;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#21487;&#24796;&#30340;&#26159;&#65292;&#20043;&#21069;&#23558;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#65288;RST&#65289;&#24341;&#20837;&#22522;&#20110;transformer&#30340;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#30340;&#30740;&#31350;&#20165;&#32771;&#34385;&#20102;&#26680;&#24515;&#37096;&#20998;&#30340;&#27880;&#37322;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#21508;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#35805;&#35821;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;'RSTformer'&#30340;&#26032;&#22411;&#25688;&#35201;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20840;&#38754;&#34701;&#21512;&#20102;&#35805;&#35821;&#20851;&#31995;&#31867;&#22411;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;RST-attention&#26426;&#21046;&#26159;&#22522;&#20110;&#25991;&#26723;&#32423;&#20462;&#36766;&#32467;&#26500;&#30340;Longformer&#26694;&#26550;&#30340;&#25193;&#23637;&#12290;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#27169;&#22411;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#20984;&#26174;&#20986;&#20854;&#22312;&#22810;&#20010;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#19978;&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
For text summarization, the role of discourse structure is pivotal in discerning the core content of a text. Regrettably, prior studies on incorporating Rhetorical Structure Theory (RST) into transformer-based summarization models only consider the nuclearity annotation, thereby overlooking the variety of discourse relation types. This paper introduces the 'RSTformer', a novel summarization model that comprehensively incorporates both the types and uncertainty of rhetorical relations. Our RST-attention mechanism, rooted in document-level rhetorical structure, is an extension of the recently devised Longformer framework. Through rigorous evaluation, the model proposed herein exhibits significant superiority over state-of-the-art models, as evidenced by its notable performance on several automatic metrics and human evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411; (MLLMs)&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;&#30340;&#19981;&#21516;&#22240;&#32032;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#32508;&#36848;&#65292;&#23558;&#36825;&#20123;&#22240;&#32032;&#20998;&#25104;&#20116;&#31867;&#24182;&#25552;&#20379;&#20102;&#36807;&#21435;&#30740;&#31350;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#26412;&#25991;&#30340;&#24037;&#20316;&#26088;&#22312;&#20840;&#38754;&#32972;&#26223;&#21644;&#32479;&#19968;MLLMs&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#29616;&#26377;&#30740;&#31350;&#27969;&#12290;</title><link>http://arxiv.org/abs/2305.16768</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#22240;&#32032;&#30340;&#20849;&#21516;&#29702;&#35299;&#65306;&#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Towards a Common Understanding of Contributing Factors for Cross-Lingual Transfer in Multilingual Language Models: A Review. (arXiv:2305.16768v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411; (MLLMs)&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;&#30340;&#19981;&#21516;&#22240;&#32032;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#32508;&#36848;&#65292;&#23558;&#36825;&#20123;&#22240;&#32032;&#20998;&#25104;&#20116;&#31867;&#24182;&#25552;&#20379;&#20102;&#36807;&#21435;&#30740;&#31350;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#26412;&#25991;&#30340;&#24037;&#20316;&#26088;&#22312;&#20840;&#38754;&#32972;&#26223;&#21644;&#32479;&#19968;MLLMs&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#29616;&#26377;&#30740;&#31350;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#34920;&#29616;&#20986;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#36825;&#31181;&#33021;&#21147;&#30340;&#36861;&#27714;&#24182;&#26410;&#26126;&#30830;&#22320;&#34701;&#20837;&#22823;&#22810;&#25968;MLLM&#35774;&#35745;&#20013;&#65292;&#24456;&#38590;&#23545;&#20854;&#20986;&#29616;&#36827;&#34892;&#29420;&#29305;&#32780;&#30452;&#25509;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#32508;&#36848;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#30740;&#31350;MLLM&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;&#30340;&#19981;&#21516;&#22240;&#32032;&#30340;&#25991;&#29486;&#65292;&#24182;&#35814;&#32454;&#27010;&#36848;&#21644;&#35752;&#35770;&#20102;&#36825;&#20123;&#22240;&#32032;&#12290;&#20026;&#20102;&#22686;&#24378;&#36825;&#20010;&#32508;&#36848;&#30340;&#32467;&#26500;&#24182;&#20415;&#20110;&#19982;&#26410;&#26469;&#30340;&#30740;&#31350;&#25972;&#21512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20116;&#31867;&#36825;&#26679;&#30340;&#22240;&#32032;&#12290;&#38500;&#25552;&#20379;&#36807;&#21435;&#30740;&#31350;&#30340;&#32463;&#39564;&#35777;&#25454;&#27010;&#36848;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#22312;&#20855;&#26377;&#19968;&#33268;&#21457;&#29616;&#30340;&#30740;&#31350;&#20013;&#30340;&#20849;&#35782;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#30683;&#30462;&#30340;&#30740;&#31350;&#20013;&#30340;&#20914;&#31361;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#26088;&#22312;&#35299;&#37322;MLLM&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;&#30340;&#29616;&#26377;&#30740;&#31350;&#27969;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32972;&#26223;&#21644;&#32479;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, pre-trained Multilingual Language Models (MLLMs) have shown a strong ability to transfer knowledge across different languages. However, given that the aspiration for such an ability has not been explicitly incorporated in the design of the majority of MLLMs, it is challenging to obtain a unique and straightforward explanation for its emergence. In this review paper, we survey literature that investigates different factors contributing to the capacity of MLLMs to perform zero-shot cross-lingual transfer and subsequently outline and discuss these factors in detail. To enhance the structure of this review and to facilitate consolidation with future studies, we identify five categories of such factors. In addition to providing a summary of empirical evidence from past studies, we identify consensuses among studies with consistent findings and resolve conflicts among contradictory ones. Our work contextualizes and unifies existing research streams which aim at explaining th
&lt;/p&gt;</description></item><item><title>&#32972;&#21253;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#24378;&#22823;&#30340;&#24314;&#27169;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#12290;&#23427;&#23398;&#20064;&#27599;&#20010;&#21333;&#35789;&#30340;&#22810;&#20010;&#38750;&#19978;&#19979;&#25991;&#24863;&#30693;&#21521;&#37327;&#65292;&#24182;&#23558;&#21333;&#35789;&#34920;&#31034;&#20026;&#19978;&#19979;&#25991;&#20381;&#36182;&#30340;&#38750;&#36127;&#32447;&#24615;&#32452;&#21512;&#12290;&#24863;&#30693;&#21521;&#37327;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#21333;&#35789;&#19981;&#21516;&#30340;&#26041;&#38754;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#24178;&#39044;&#36825;&#20123;&#21487;&#35299;&#37322;&#30340;&#38057;&#23376;&#20197;&#21487;&#39044;&#27979;&#30340;&#26041;&#24335;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#22312;&#35789;&#27719;&#30456;&#20284;&#24615;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#29978;&#33267;&#20248;&#20110;&#19968;&#20010;6B&#21442;&#25968;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#21333;&#35789;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2305.16765</link><description>&lt;p&gt;
&#32972;&#21253;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Backpack Language Models. (arXiv:2305.16765v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16765
&lt;/p&gt;
&lt;p&gt;
&#32972;&#21253;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#24378;&#22823;&#30340;&#24314;&#27169;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#12290;&#23427;&#23398;&#20064;&#27599;&#20010;&#21333;&#35789;&#30340;&#22810;&#20010;&#38750;&#19978;&#19979;&#25991;&#24863;&#30693;&#21521;&#37327;&#65292;&#24182;&#23558;&#21333;&#35789;&#34920;&#31034;&#20026;&#19978;&#19979;&#25991;&#20381;&#36182;&#30340;&#38750;&#36127;&#32447;&#24615;&#32452;&#21512;&#12290;&#24863;&#30693;&#21521;&#37327;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#21333;&#35789;&#19981;&#21516;&#30340;&#26041;&#38754;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#24178;&#39044;&#36825;&#20123;&#21487;&#35299;&#37322;&#30340;&#38057;&#23376;&#20197;&#21487;&#39044;&#27979;&#30340;&#26041;&#24335;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#22312;&#35789;&#27719;&#30456;&#20284;&#24615;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#29978;&#33267;&#20248;&#20110;&#19968;&#20010;6B&#21442;&#25968;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#21333;&#35789;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32972;&#21253;&#65288;Backpacks&#65289;&#65306;&#19968;&#31181;&#23558;&#24378;&#22823;&#30340;&#24314;&#27169;&#24615;&#33021;&#19982;&#21487;&#35299;&#37322;&#24615;&#21644;&#25511;&#21046;&#25509;&#21475;&#32467;&#21512;&#30340;&#26032;&#22411;&#31070;&#32463;&#26550;&#26500;&#12290;&#32972;&#21253;&#23398;&#20064;&#35789;&#27719;&#34920;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#22810;&#20010;&#38750;&#19978;&#19979;&#25991;&#24863;&#30693;&#21521;&#37327;&#65292;&#24182;&#23558;&#24207;&#21015;&#20013;&#30340;&#21333;&#35789;&#34920;&#31034;&#20026;&#19978;&#19979;&#25991;&#20381;&#36182;&#30340;&#38750;&#36127;&#32447;&#24615;&#32452;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;&#21518;&#65292;&#24863;&#30693;&#21521;&#37327;&#19987;&#38376;&#21270;&#20102;&#65292;&#27599;&#20010;&#24863;&#30693;&#21521;&#37327;&#32534;&#30721;&#20102;&#21333;&#35789;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#26816;&#26597;&#24863;&#30693;&#21521;&#37327;&#22312;&#36755;&#20986;&#31354;&#38388;&#19978;&#30340;&#65288;&#38750;&#19978;&#19979;&#25991;&#65292;&#32447;&#24615;&#65289;&#25237;&#24433;&#26469;&#35299;&#37322;&#19968;&#20010;&#24863;&#30693;&#21521;&#37327;&#65292;&#24182;&#24178;&#39044;&#36825;&#20123;&#21487;&#35299;&#37322;&#30340;&#38057;&#23376;&#20197;&#21487;&#39044;&#27979;&#30340;&#26041;&#24335;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;OpenWebText&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#21253;&#21547;170M&#21442;&#25968;&#30340;&#32972;&#21253;&#35821;&#35328;&#27169;&#22411;&#65292;&#19982;124M&#21442;&#25968;&#30340;GPT-2&#23567;&#22411;Transformer&#30340;&#25439;&#22833;&#30456;&#21305;&#37197;&#12290;&#22312;&#35789;&#27719;&#30456;&#20284;&#24615;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#32972;&#21253;&#24863;&#30693;&#21521;&#37327;&#29978;&#33267;&#20248;&#20110;&#19968;&#20010;6B&#21442;&#25968;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#21333;&#35789;&#23884;&#20837;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#31616;&#21333;&#30340;&#31639;&#27861;&#26469;&#24178;&#39044;&#24863;&#30693;&#21521;&#37327;&#65292;&#20197;&#23454;&#29616;&#29305;&#23450;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Backpacks: a new neural architecture that marries strong modeling performance with an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative linear combination of sense vectors in this sequence. We find that, after training, sense vectors specialize, each encoding a different aspect of a word. We can interpret a sense vector by inspecting its (non-contextual, linear) projection onto the output space, and intervene on these interpretable hooks to change the model's behavior in predictable ways. We train a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124Mparameter) Transformer. On lexical similarity evaluations, we find that Backpack sense vectors outperform even a 6B-parameter Transformer LM's word embeddings. Finally, we present simple algorithms that intervene on sense vectors to perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20154;&#36947;&#20027;&#20041;&#26412;&#20307;&#20026;&#22522;&#30784;&#30340;&#26032;&#22411;&#35821;&#35328;&#27169;&#22411;HumBert&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#21644;&#20943;&#23569;&#20559;&#35265;&#65292;&#20197;&#23454;&#29616;&#23545;&#20154;&#36947;&#20027;&#20041;&#25968;&#25454;&#20998;&#26512;&#30340;&#26377;&#25928;&#21644;&#36947;&#24503;&#24847;&#35782;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.16756</link><description>&lt;p&gt;
&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#23454;&#29616;&#21253;&#23481;&#21644;&#20559;&#35265;&#24863;&#30693;&#30340;&#20154;&#36947;&#20027;&#20041;&#21709;&#24212;&#20837;&#21475;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification. (arXiv:2305.16756v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20154;&#36947;&#20027;&#20041;&#26412;&#20307;&#20026;&#22522;&#30784;&#30340;&#26032;&#22411;&#35821;&#35328;&#27169;&#22411;HumBert&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#21644;&#20943;&#23569;&#20559;&#35265;&#65292;&#20197;&#23454;&#29616;&#23545;&#20154;&#36947;&#20027;&#20041;&#25968;&#25454;&#20998;&#26512;&#30340;&#26377;&#25928;&#21644;&#36947;&#24503;&#24847;&#35782;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#36947;&#20027;&#20041;&#21361;&#26426;&#26399;&#38388;&#65292;&#20934;&#30830;&#21644;&#24555;&#36895;&#30340;&#24773;&#20917;&#20998;&#26512;&#23545;&#20110;&#39640;&#25928;&#22320;&#25552;&#20379;&#20154;&#36947;&#20027;&#20041;&#25588;&#21161;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#26159;&#20154;&#36947;&#20027;&#20041;&#21407;&#21017;&#21644;&#19981;&#30041;&#20219;&#20309;&#20154;&#33853;&#21518;&#21407;&#21017;&#30340;&#22522;&#30784;&#12290;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#21487;&#20197;&#26497;&#22823;&#22320;&#21463;&#30410;&#20110;&#36825;&#31181;&#25968;&#25454;&#20998;&#26512;&#65292;&#20363;&#22914;&#65292;&#25353;&#29031;&#20154;&#36947;&#20027;&#20041;&#26412;&#20307;&#23545;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#36890;&#36807;&#24494;&#35843;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26469;&#23454;&#29616;&#65292;&#28041;&#21450;&#19968;&#20123;&#23454;&#36341;&#21644;&#36947;&#24503;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#31232;&#30095;&#21644;&#22797;&#26434;&#23376;&#39046;&#22495;&#19978;&#30340;&#25928;&#26524;&#19981;&#20339;&#20197;&#21450;&#31038;&#20250;&#20559;&#35265;&#21644;&#19981;&#33391;&#20851;&#32852;&#30340;&#32534;&#30721;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#20154;&#36947;&#20027;&#20041;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#21644;&#36947;&#24503;&#24847;&#35782;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807; (1) &#24341;&#20837;&#19968;&#20010;&#36866;&#21512;&#20154;&#36947;&#20027;&#20041;&#20998;&#26512;&#26694;&#26550;&#30340;&#26032;&#26550;&#26500;&#65292;(2) &#21019;&#24314;&#21644;&#21457;&#24067;&#19968;&#20010;&#26032;&#30340;&#20154;&#36947;&#20027;&#20041;&#29305;&#23450; LLM&#65292;&#31216;&#20026; HumBert&#65292;&#24182;&#19988; (3) &#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#24335;&#26469;&#34913;&#37327;&#21644;&#20943;&#23569;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and rapid situation analysis during humanitarian crises is critical to delivering humanitarian aid efficiently and is fundamental to humanitarian imperatives and the Leave No One Behind (LNOB) principle. This data analysis can highly benefit from language processing systems, e.g., by classifying the text data according to a humanitarian ontology. However, approaching this by simply fine-tuning a generic large language model (LLM) involves considerable practical and ethical issues, particularly the lack of effectiveness on data-sparse and complex subdomains, and the encoding of societal biases and unwanted associations. In this work, we aim to provide an effective and ethically-aware system for humanitarian data analysis. We approach this by (1) introducing a novel architecture adjusted to the humanitarian analysis framework, (2) creating and releasing a novel humanitarian-specific LLM called HumBert, and (3) proposing a systematic way to measure and mitigate biases. Our experi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30495;&#23454;&#23454;&#20307;&#30340;&#26174;&#33879;&#36127;&#38754;&#38472;&#36848;&#30340;&#33021;&#21147;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21542;&#23450;&#38472;&#36848;&#20013;&#30340;&#20107;&#23454;&#27010;&#24565;&#19978;&#20173;&#26377;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.16755</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#26174;&#33879;&#30340;&#36127;&#38754;&#22768;&#26126;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can large language models generate salient negative statements?. (arXiv:2305.16755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30495;&#23454;&#23454;&#20307;&#30340;&#26174;&#33879;&#36127;&#38754;&#38472;&#36848;&#30340;&#33021;&#21147;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21542;&#23450;&#38472;&#36848;&#20013;&#30340;&#20107;&#23454;&#27010;&#24565;&#19978;&#20173;&#26377;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#20851;&#20110;&#29616;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#26174;&#33879;&#65288;&#26377;&#36259;&#30340;&#65289;&#36127;&#38754;&#38472;&#36848;&#30340;&#33021;&#21147;; &#36825;&#26159;&#36807;&#21435;&#20960;&#24180;&#20013;&#28044;&#29616;&#20986;&#30340;&#19968;&#20010;&#30740;&#31350;&#35838;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#38646;&#28857;&#21644;k&#27425;&#26080;&#32422;&#26463;&#25506;&#38024;&#26469;&#25506;&#27979;LLMs&#65292;&#24182;&#19982;&#20256;&#32479;&#30340;&#21542;&#23450;&#29983;&#25104;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#27169;&#24335;&#30340;&#25991;&#26412;&#25552;&#21462;&#21644;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#25512;&#29702;&#20197;&#21450;&#20247;&#21253;&#37329;&#26631;&#35821;&#21477;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#20027;&#39064;&#29983;&#25104;&#21015;&#34920;&#30340;&#27491;&#30830;&#24615;&#21644;&#26174;&#30528;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#26377;&#25351;&#23548;&#30340;&#25506;&#38024;&#30830;&#23454;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#36127;&#38754;&#38472;&#36848;&#30340;&#36136;&#37327;&#65292;&#19982;&#26080;&#25351;&#23548;&#30340;&#21464;&#20307;&#30456;&#27604;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20004;&#20010;&#25552;&#31034;&#65292;LLMs&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#36127;&#38754;&#20107;&#23454;&#30340;&#27010;&#24565;&#65292;&#24120;&#24120;&#29983;&#25104;&#35768;&#22810;&#21547;&#31946;&#19981;&#28165;&#30340;&#38472;&#36848;&#65292;&#25110;&#32773;&#24102;&#26377;&#36127;&#38754;&#20851;&#38190;&#35789;&#20294;&#20855;&#26377;&#31215;&#26497;&#24847;&#20041;&#30340;&#38472;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the ability of large language models (LLMs) to generate salient (interesting) negative statements about real-world entities; an emerging research topic of the last few years. We probe the LLMs using zero- and k-shot unconstrained probes, and compare with traditional methods for negation generation, i.e., pattern-based textual extractions and knowledge-graph-based inferences, as well as crowdsourced gold statements. We measure the correctness and salience of the generated lists about subjects from different domains. Our evaluation shows that guided probes do in fact improve the quality of generated negatives, compared to the zero-shot variant. Nevertheless, using both prompts, LLMs still struggle with the notion of factuality of negatives, frequently generating many ambiguous statements, or statements with negative keywords but a positive meaning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22269;&#38469;&#21327;&#35758;&#20013;&#33258;&#21160;&#21270;&#25552;&#21462;&#27491;&#24335;&#21046;&#24230;&#35774;&#35745;&#30340;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#20110;&#12298;&#26080;&#24418;&#25991;&#21270;&#36951;&#20135;&#20445;&#25252;&#20844;&#32422;&#12299;&#30340;&#27979;&#35797;&#20998;&#26512;&#20102;&#27491;&#24335;&#21046;&#24230;&#35774;&#35745;&#20013;&#21442;&#19982;&#32773;&#30340;&#21487;&#35265;&#24615;&#21644;&#37325;&#35201;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.16750</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#22269;&#38469;&#21327;&#35758;&#20013;&#21046;&#24230;&#35774;&#35745;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Automating the Analysis of Institutional Design in International Agreements. (arXiv:2305.16750v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22269;&#38469;&#21327;&#35758;&#20013;&#33258;&#21160;&#21270;&#25552;&#21462;&#27491;&#24335;&#21046;&#24230;&#35774;&#35745;&#30340;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#20110;&#12298;&#26080;&#24418;&#25991;&#21270;&#36951;&#20135;&#20445;&#25252;&#20844;&#32422;&#12299;&#30340;&#27979;&#35797;&#20998;&#26512;&#20102;&#27491;&#24335;&#21046;&#24230;&#35774;&#35745;&#20013;&#21442;&#19982;&#32773;&#30340;&#21487;&#35265;&#24615;&#21644;&#37325;&#35201;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#33258;&#21160;&#22320;&#20174;&#22269;&#38469;&#21327;&#35758;&#20013;&#25552;&#21462;&#27491;&#24335;&#21046;&#24230;&#35774;&#35745;&#12289;&#35268;&#33539;&#12289;&#35268;&#21017;&#21644;&#21442;&#19982;&#32773;&#31561;&#30693;&#35782;&#12290;&#37325;&#28857;&#26159;&#20998;&#26512;&#35268;&#33539;&#25991;&#21270;&#36951;&#20135;&#20851;&#31995;&#20851;&#38190;&#26041;&#38754;&#30340;&#27491;&#24335;&#21046;&#24230;&#35774;&#35745;&#20013;&#21442;&#19982;&#32773;&#30340;&#21487;&#35265;&#24615;&#21644;&#37325;&#35201;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25152;&#24320;&#21457;&#30340;&#24037;&#20855;&#37319;&#29992;&#20102;&#22810;&#31181;&#25216;&#26415;&#65292;&#22914;&#25910;&#38598;&#27861;&#24459;&#25991;&#20214;&#12289;&#29992;&#21046;&#24230;&#35821;&#27861;&#27880;&#37322;&#36825;&#20123;&#25991;&#20214;&#24182;&#20351;&#29992;&#22270;&#20998;&#26512;&#26041;&#27861;&#26469;&#25506;&#32034;&#27491;&#24335;&#21046;&#24230;&#35774;&#35745;&#12290;&#35813;&#31995;&#32479;&#23545;2003&#24180;&#12298;&#26080;&#24418;&#25991;&#21270;&#36951;&#20135;&#20445;&#25252;&#20844;&#32422;&#12299;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the automatic knowledge extraction of formal institutional design - norms, rules, and actors - from international agreements. The focus was to analyze the relationship between the visibility and centrality of actors in the formal institutional design in regulating critical aspects of cultural heritage relations. The developed tool utilizes techniques such as collecting legal documents, annotating them with Institutional Grammar, and using graph analysis to explore the formal institutional design. The system was tested against the 2003 UNESCO Convention for the Safeguarding of the Intangible Cultural Heritage.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#24335;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#65292;&#26080;&#38656;&#28155;&#21152;&#26032;&#21442;&#25968;&#65292;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#24182;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16742</link><description>&lt;p&gt;
&#26080;&#38656;&#24341;&#20837;&#26032;&#30340;&#24310;&#36831;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning without Introducing New Latency. (arXiv:2305.16742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#24335;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#65292;&#26080;&#38656;&#28155;&#21152;&#26032;&#21442;&#25968;&#65292;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#24182;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26368;&#36817;&#23637;&#31034;&#20986;&#26126;&#26174;&#30340;&#25104;&#23601;&#65292;&#26377;&#25928;&#22320;&#21305;&#37197;&#20102;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21033;&#29992;&#26126;&#26174;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#22240;&#27492;&#35299;&#20915;&#20102;&#23384;&#20648;&#21644;&#36890;&#20449;&#38480;&#21046;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#21508;&#31181;PEFT&#26041;&#27861;&#20173;&#21463;&#20854;&#22266;&#26377;&#29305;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#31232;&#30095;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#21482;&#28041;&#21450;&#20462;&#25913;&#29616;&#26377;&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#24494;&#35843;&#21442;&#25968;&#30340;&#36873;&#25321;&#26159;&#20219;&#21153;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#28155;&#21152;&#26032;&#21442;&#25968;&#30340;PEFT&#26041;&#27861;&#36890;&#24120;&#20250;&#24341;&#20837;&#39069;&#22806;&#30340;&#25512;&#26029;&#24310;&#36831;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20197;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#24335;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#30340;&#21487;&#34892;&#24615;&#65292;&#20854;&#20013;&#25152;&#26377;&#19979;&#28216;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#25513;&#30721;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#21442;&#25968;&#30340;&#24133;&#24230;&#20449;&#24687;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#23398;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) of pre-trained language models has recently demonstrated remarkable achievements, effectively matching the performance of full fine-tuning while utilizing significantly fewer trainable parameters, and consequently addressing the storage and communication constraints. Nonetheless, various PEFT methods are limited by their inherent characteristics. In the case of sparse fine-tuning, which involves modifying only a small subset of the existing parameters, the selection of fine-tuned parameters is task- and domain-specific, making it unsuitable for federated learning. On the other hand, PEFT methods with adding new parameters typically introduce additional inference latency. In this paper, we demonstrate the feasibility of generating a sparse mask in a task-agnostic manner, wherein all downstream tasks share a common mask. Our approach, which relies solely on the magnitude information of pre-trained parameters, surpasses existing methodologies by a si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#25509;&#22312;&#25991;&#26412;&#20013;&#25805;&#20316;&#30340;&#24182;&#21015;&#35299;&#26512;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#20998;&#21106;&#21644;&#37325;&#26500;&#33539;&#20363;&#26469;&#24674;&#22797;&#24182;&#21015;&#32467;&#26500;&#20013;&#32570;&#22833;&#30340;&#20803;&#32032;&#12290;&#22522;&#20110;&#23454;&#29992;&#30340;&#21160;&#35789;&#30465;&#30053;&#26694;&#26550;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20221;&#30001;&#30495;&#23454;&#35821;&#26009;&#24211;&#26500;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#24182;&#27604;&#36739;&#29616;&#26377;&#35299;&#26512;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.16740</link><description>&lt;p&gt;
&#22312;&#21160;&#35789;&#30465;&#30053;&#32467;&#26500;&#20013;&#30340;&#24182;&#21015;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Conjunct Resolution in the Face of Verbal Omissions. (arXiv:2305.16740v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#25509;&#22312;&#25991;&#26412;&#20013;&#25805;&#20316;&#30340;&#24182;&#21015;&#35299;&#26512;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#20998;&#21106;&#21644;&#37325;&#26500;&#33539;&#20363;&#26469;&#24674;&#22797;&#24182;&#21015;&#32467;&#26500;&#20013;&#32570;&#22833;&#30340;&#20803;&#32032;&#12290;&#22522;&#20110;&#23454;&#29992;&#30340;&#21160;&#35789;&#30465;&#30053;&#26694;&#26550;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20221;&#30001;&#30495;&#23454;&#35821;&#26009;&#24211;&#26500;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#24182;&#27604;&#36739;&#29616;&#26377;&#35299;&#26512;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#35789;&#30465;&#30053;&#26159;VP&#24182;&#21015;&#32467;&#26500;&#20013;&#22797;&#26434;&#30340;&#21477;&#27861;&#29616;&#35937;&#12290;&#24403;&#21160;&#35789;&#21450;&#20854;&#65288;&#19968;&#20123;&#65289;&#35770;&#20803;&#22312;&#21021;&#22987;&#23376;&#21477;&#20013;&#24471;&#21040;&#26126;&#30830;&#35828;&#26126;&#21518;&#65292;&#22312;&#38543;&#21518;&#30340;&#23376;&#21477;&#20013;&#34987;&#30465;&#30053;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#36825;&#31181;&#29616;&#35937;&#12290;&#24674;&#22797;&#36825;&#20123;&#30465;&#30053;&#20803;&#32032;&#23545;&#20110;&#20934;&#30830;&#35299;&#37322;&#21477;&#23376;&#26159;&#24517;&#35201;&#30340;&#65292;&#34429;&#28982;&#20154;&#31867;&#24456;&#23481;&#26131;&#32780;&#19988;&#30452;&#35266;&#22320;&#22635;&#20805;&#32570;&#22833;&#20449;&#24687;&#65292;&#20294;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#20165;&#38480;&#20110;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#12289;&#21512;&#25104;&#25968;&#25454;&#21019;&#24314;&#26041;&#27861;&#21644;&#20381;&#36182;&#22270;&#32423;&#21035;&#30340;&#35299;&#26512;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#25509;&#22312;&#25991;&#26412;&#20013;&#25805;&#20316;&#30340;&#24182;&#21015;&#35299;&#26512;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#20998;&#21106;&#21644;&#37325;&#26500;&#33539;&#20363;&#26469;&#24674;&#22797;&#24182;&#21015;&#32467;&#26500;&#20013;&#32570;&#22833;&#30340;&#20803;&#32032;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#21160;&#35789;&#30465;&#30053;&#26694;&#26550;&#65292;&#25551;&#36848;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#30465;&#30053;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#21487;&#25193;&#23637;&#30340;&#25910;&#38598;&#26041;&#27861;&#12290;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20221;&#30001;&#30495;&#23454;&#35821;&#26009;&#24211;&#26500;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#24182;&#27604;&#36739;&#29616;&#26377;&#35299;&#26512;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verbal omissions are complex syntactic phenomena in VP coordination structures. They occur when verbs and (some of) their arguments are omitted from subsequent clauses after being explicitly stated in an initial clause. Recovering these omitted elements is necessary for accurate interpretation of the sentence, and while humans easily and intuitively fill in the missing information, state-of-the-art models continue to struggle with this task. Previous work is limited to small-scale datasets, synthetic data creation methods, and to resolution methods in the dependency-graph level. In this work we propose a conjunct resolution task that operates directly on the text and makes use of a split-and-rephrase paradigm in order to recover the missing elements in the coordination structure. To this end, we first formulate a pragmatic framework of verbal omissions which describes the different types of omissions, and develop an automatic scalable collection method. Based on this method, we curate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AlignScore&#65292;&#19968;&#31181;&#26032;&#30340;&#32508;&#21512;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#21508;&#31181;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#20449;&#24687;&#23545;&#40784;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#30340;&#19981;&#21516;&#36755;&#20837;/&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2305.16739</link><description>&lt;p&gt;
AlignScore&#65306;&#20351;&#29992;&#32479;&#19968;&#30340;&#23545;&#40784;&#20989;&#25968;&#35780;&#20272;&#20107;&#23454;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
AlignScore: Evaluating Factual Consistency with a Unified Alignment Function. (arXiv:2305.16739v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AlignScore&#65292;&#19968;&#31181;&#26032;&#30340;&#32508;&#21512;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#21508;&#31181;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#20449;&#24687;&#23545;&#40784;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#30340;&#19981;&#21516;&#36755;&#20837;/&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25991;&#26412;&#29983;&#25104;&#24212;&#29992;&#38656;&#35201;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#36755;&#20837;&#20449;&#24687;&#22312;&#20107;&#23454;&#19978;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#33258;&#21160;&#35780;&#20272;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#21508;&#31181;&#24230;&#37327;&#26631;&#20934;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#20989;&#25968;&#65292;&#20363;&#22914;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#25110;&#38382;&#31572;&#65288;QA&#65289;&#65292;&#36825;&#20123;&#20989;&#25968;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#12290;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#22240;&#27492;&#38590;&#20197;&#35780;&#20272;&#19981;&#21516;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#24773;&#20917;&#65288;&#20363;&#22914;&#30683;&#30462;&#12289;&#24187;&#35273;&#65289;&#65292;&#36825;&#20123;&#24773;&#20917;&#21457;&#29983;&#22312;&#19981;&#21516;&#20219;&#21153;&#30340;&#19981;&#21516;&#36755;&#20837;/&#36755;&#20986;&#65288;&#20363;&#22914;&#21477;&#23376;&#12289;&#25991;&#26723;&#65289;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AlignScore&#65292;&#36825;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21508;&#31181;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#24773;&#20917;&#30340;&#26032;&#22411;&#32508;&#21512;&#24230;&#37327;&#26631;&#20934;&#12290;AlignScore&#26159;&#22522;&#20110;&#20004;&#20010;&#20219;&#24847;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#20449;&#24687;&#23545;&#40784;&#30340;&#19968;&#33324;&#20989;&#25968;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35757;&#32451;&#26694;&#26550;&#26469;&#38598;&#25104;&#22823;&#37327;&#25968;&#25454;&#28304;&#30340;&#23545;&#40784;&#20989;&#25968;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#26469;&#33258;7&#20010;&#30693;&#21517;&#20219;&#21153;&#65288;NLI&#12289;QA&#12289;&#25991;&#31456;&#25913;&#20889;&#12289;&#24037;&#20316;&#22330;&#26223;&#25991;&#26412;&#31561;&#65289;&#30340;4.7M&#20010;&#35757;&#32451;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many text generation applications require the generated text to be factually consistent with input information. Automatic evaluation of factual consistency is challenging. Previous work has developed various metrics that often depend on specific functions, such as natural language inference (NLI) or question answering (QA), trained on limited data. Those metrics thus can hardly assess diverse factual inconsistencies (e.g., contradictions, hallucinations) that occur in varying inputs/outputs (e.g., sentences, documents) from different tasks. In this paper, we propose AlignScore, a new holistic metric that applies to a variety of factual inconsistency scenarios as above. AlignScore is based on a general function of information alignment between two arbitrary text pieces. Crucially, we develop a unified training framework of the alignment function by integrating a large diversity of data sources, resulting in 4.7M training examples from 7 well-established tasks (NLI, QA, paraphrasing, fac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AMPERE&#30340;&#26041;&#27861;&#65292;&#38754;&#21521;AMR&#30340;&#21069;&#32512;&#29983;&#25104;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25104;&#21151;&#24341;&#20837;&#20102;AMR&#30340;&#20449;&#24687;&#65292;&#25552;&#21319;&#20102;&#29983;&#25104;&#24335;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16734</link><description>&lt;p&gt;
AMPERE: &#38754;&#21521;AMR&#30340;&#21069;&#32512;&#29983;&#25104;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AMPERE: AMR-Aware Prefix for Generation-Based Event Argument Extraction Model. (arXiv:2305.16734v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AMPERE&#30340;&#26041;&#27861;&#65292;&#38754;&#21521;AMR&#30340;&#21069;&#32512;&#29983;&#25104;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25104;&#21151;&#24341;&#20837;&#20102;AMR&#30340;&#20449;&#24687;&#65292;&#25552;&#21319;&#20102;&#29983;&#25104;&#24335;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#26088;&#22312;&#35782;&#21035;&#26576;&#20107;&#20214;&#30340;&#35770;&#20803;&#21450;&#20854;&#29305;&#23450;&#30340;&#35282;&#33394;&#12290;&#29983;&#25104;&#24335;&#30340;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#27169;&#22411;&#30456;&#27604;&#20110;&#20998;&#31867;&#24335;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#30340;&#29983;&#25104;&#24335;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#21644;&#25552;&#31034;&#35774;&#35745;&#65292;&#27809;&#26377;&#25972;&#21512;&#20854;&#20182;&#22312;&#20998;&#31867;&#24335;&#27169;&#22411;&#20013;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#30340;&#20449;&#24687;&#65288;&#22914;&#36755;&#20837;&#27573;&#33853;&#30340;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;AMR&#65289;&#12290;&#30001;&#20110;&#29983;&#25104;&#24335;&#27169;&#22411;&#20013;&#36890;&#24120;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#65292;&#32780;AMR&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#24418;&#24335;&#65292;&#22240;&#27492;&#23558;&#36825;&#26679;&#30340;&#20449;&#24687;&#25972;&#21512;&#21040;&#29983;&#25104;&#24335;&#27169;&#22411;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;AMR&#25972;&#21512;&#21040;&#29983;&#25104;&#24335;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#27169;&#22411;&#20013;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AMPERE&#65292;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#27599;&#23618;&#29983;&#25104;AMR&#24863;&#30693;&#21069;&#32512;&#12290;&#22240;&#27492;&#65292;&#35813;&#21069;&#32512;&#20026;&#29983;&#25104;&#24335;&#30340;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#27169;&#22411;&#24341;&#20837;&#20102;AMR&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event argument extraction (EAE) identifies event arguments and their specific roles for a given event. Recent advancement in generation-based EAE models has shown great performance and generalizability over classification-based models. However, existing generation-based EAE models mostly focus on problem re-formulation and prompt design, without incorporating additional information that has been shown to be effective for classification-based models, such as the abstract meaning representation (AMR) of the input passages. Incorporating such information into generation-based models is challenging due to the heterogeneous nature of the natural language form prevalently used in generation-based models and the structured form of AMRs. In this work, we study strategies to incorporate AMR into generation-based EAE models. We propose AMPERE, which generates AMR-aware prefixes for every layer of the generation model. Thus, the prefix introduces AMR information to the generation-based EAE model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#24182;&#20026;&#20854;&#20998;&#37197;&#24773;&#24863;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#30456;&#20851;&#30340;&#23454;&#39564;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16731</link><description>&lt;p&gt;
&#35782;&#21035;&#24773;&#24863;&#20307;&#39564;&#32773;&#20316;&#20026;&#24773;&#24863;&#20998;&#26512;&#30340;&#20808;&#20915;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Emotion Experiencer Recognition as a Prerequisite for Experiencer-Specific Emotion Analysis. (arXiv:2305.16731v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#24182;&#20026;&#20854;&#20998;&#37197;&#24773;&#24863;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#30456;&#20851;&#30340;&#23454;&#39564;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35282;&#33394;&#26631;&#27880;&#26088;&#22312;&#25552;&#21462;&#25991;&#26412;&#20013;&#25551;&#36848;&#35841;&#32463;&#21382;&#24773;&#24863;&#12289;&#20026;&#20160;&#20040;&#20197;&#21450;&#23545;&#35841;&#30340;&#20449;&#24687;&#12290;&#36825;&#36890;&#24120;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24314;&#27169;&#20219;&#21153;&#65292;&#22914;&#26524;&#35201;&#22238;&#31572;&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#35841;&#24863;&#21463;&#21040;&#20102;&#21738;&#31181;&#24773;&#24863;&#65292;&#36825;&#21487;&#33021;&#20250;&#36807;&#20110;&#22797;&#26434;&#12290;&#26412;&#25991;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#33258;&#21160;&#26816;&#27979;&#25991;&#26412;&#20013;&#30340;&#24773;&#24863;&#20307;&#39564;&#32773;&#24182;&#38543;&#21518;&#20026;&#20854;&#20998;&#37197;&#24773;&#24863;&#65292;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24182;&#21576;&#29616;&#20102;&#30456;&#20851;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion role labeling aims at extracting who is described in text to experience an emotion, why, and towards whom. This is often a challenging modelling task which might be overly sophisticated if the main question to answer is who feels which emotion. Recently, Troiano et al. (2022) proposed a data set that focuses on assigning emotion labels and appraisal labels to individual entities in text and Wegge et al. (2022) presented the first modelling experiments. Their experiencer-specific emotion prediction model has, however, only been evaluated on gold-annotated experiencers, due to the unavailability of an automatic experiencer detection approach. We fill this gap with the first experiments to automatically detect emotion experiencers in text and, subsequently, assign them emotions. We show that experiencer detection in text is a challenging task, with a precision of .82 and a recall of .56 (F1 =.66). Consequently, the performance of the experiencer-specific emotion detection pipeline
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GLOSS&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21512;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#23545;&#12290;&#35813;&#27169;&#22411;&#22312;&#22235;&#20010;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#19978;&#30340;&#23454;&#39564;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#21644;&#22312;&#21333;&#35821;&#25991;&#26412;&#19978;&#36816;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16724</link><description>&lt;p&gt;
&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#20013;&#30340;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Code-Switched Text Synthesis in Unseen Language Pairs. (arXiv:2305.16724v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GLOSS&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21512;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#23545;&#12290;&#35813;&#27169;&#22411;&#22312;&#22235;&#20010;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#19978;&#30340;&#23454;&#39564;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#21644;&#22312;&#21333;&#35821;&#25991;&#26412;&#19978;&#36816;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38024;&#23545;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#21512;&#25104;&#30340;&#30740;&#31350;&#22823;&#22810;&#38656;&#35201;&#22312;&#30446;&#26631;&#35821;&#35328;&#23545;&#20013;&#30340;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#38480;&#21046;&#20102;&#27169;&#22411;&#22312;&#32570;&#20047;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#37096;&#32626;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21512;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;GLOSS&#65292;&#36825;&#26159;&#19968;&#20010;&#24314;&#31435;&#22312;&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65288;PMMTM&#65289;&#20043;&#19978;&#65292;&#24182;&#24102;&#26377;&#39069;&#22806;&#30340;&#20195;&#30721;&#20999;&#25442;&#27169;&#22359;&#30340;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22359;&#65292;&#26080;&#35770;&#26159;&#36866;&#37197;&#22120;&#36824;&#26159;&#39069;&#22806;&#30340;&#21069;&#32512;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#20013;&#23398;&#20064;&#20195;&#30721;&#20999;&#25442;&#27169;&#24335;&#65292;&#32780;GLOSS&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;PMMTM&#34987;&#20923;&#32467;&#12290;&#25105;&#20204;&#21482;&#35843;&#25972;&#20195;&#30721;&#20999;&#25442;&#27169;&#22359;&#30340;&#35774;&#35745;&#65292;&#38450;&#27490;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#38024;&#23545;&#28151;&#21512;&#20195;&#30721;&#35757;&#32451;&#25968;&#25454;&#30340;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;GLOSS&#34920;&#29616;&#20986;&#20102;&#36328;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#23545;&#36827;&#34892;&#24402;&#32435;&#21644;&#21512;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#30446;&#26631;&#35821;&#35328;&#21333;&#35821;&#25991;&#26412;&#30340;&#33258;&#35757;&#32451;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;GLOSS&#20248;&#20110;&#20854;&#20182;&#20174;&#20855;&#26377;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#35821;&#35328;&#23545;&#20013;&#35843;&#25972;&#30340;&#27169;&#22411;&#21644;&#22312;&#21333;&#35821;&#25991;&#26412;&#19978;&#36816;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#31561;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing efforts on text synthesis for code-switching mostly require training on code-switched texts in the target language pairs, limiting the deployment of the models to cases lacking code-switched data. In this work, we study the problem of synthesizing code-switched texts for language pairs absent from the training data. We introduce GLOSS, a model built on top of a pre-trained multilingual machine translation model (PMMTM) with an additional code-switching module. This module, either an adapter or extra prefixes, learns code-switching patterns from code-switched data during training, while the primary component of GLOSS, i.e., the PMMTM, is frozen. The design of only adjusting the code-switching module prevents our model from overfitting to the constrained training data for code-switching. Hence, GLOSS exhibits the ability to generalize and synthesize code-switched texts across a broader spectrum of language pairs. Additionally, we develop a self-training algorithm on target langu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NER&#35821;&#26009;&#24211;&#65292;&#20174;&#26410;&#27880;&#37322;&#30340;&#21382;&#21490;&#25991;&#26412;&#20013;&#24341;&#23548;&#27880;&#37322;&#31649;&#36947;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;NER&#27169;&#22411;&#26469;&#35782;&#21035;&#21382;&#21490;&#25991;&#29486;&#20013;&#30340;&#20154;&#21517;&#21644;&#22320;&#21517;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.16718</link><description>&lt;p&gt;
&#21382;&#21490;&#27431;&#27954;&#30340;&#20154;&#19982;&#22320;&#26041;&#65306;&#24341;&#23548;&#27880;&#37322;&#31649;&#36947;&#21644;&#19968;&#20010;&#26032;&#30340;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#22312;&#26202;&#26399;&#20013;&#19990;&#32426;&#25991;&#26412;&#20013;
&lt;/p&gt;
&lt;p&gt;
People and Places of Historical Europe: Bootstrapping Annotation Pipeline and a New Corpus of Named Entities in Late Medieval Texts. (arXiv:2305.16718v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NER&#35821;&#26009;&#24211;&#65292;&#20174;&#26410;&#27880;&#37322;&#30340;&#21382;&#21490;&#25991;&#26412;&#20013;&#24341;&#23548;&#27880;&#37322;&#31649;&#36947;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;NER&#27169;&#22411;&#26469;&#35782;&#21035;&#21382;&#21490;&#25991;&#29486;&#20013;&#30340;&#20154;&#21517;&#21644;&#22320;&#21517;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#39044;&#20808;&#35757;&#32451;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#23545;&#29616;&#20195;&#35821;&#26009;&#24211;&#38750;&#24120;&#20934;&#30830;&#65292;&#20294;&#30001;&#20110;OCR&#38169;&#35823;&#21644;&#35821;&#35328;&#24046;&#24322;&#65292;&#23427;&#20204;&#22312;&#21382;&#21490;&#25991;&#26412;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;NER&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;36&#19975;&#20010;&#21477;&#23376;&#65292;&#20027;&#35201;&#26159;&#20351;&#29992;&#25463;&#20811;&#35821;&#12289;&#25289;&#19969;&#35821;&#21644;&#24503;&#35821;&#32534;&#20889;&#30340;&#26202;&#26399;&#20013;&#19990;&#32426;&#25991;&#20070;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#20174;&#24050;&#30693;&#30340;&#21382;&#21490;&#20154;&#29289;&#21644;&#22320;&#28857;&#21015;&#34920;&#20197;&#21450;&#26410;&#27880;&#37322;&#30340;&#21382;&#21490;&#25991;&#26412;&#20013;&#24320;&#22987;&#65292;&#24182;&#20351;&#29992;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#33258;&#21160;&#24341;&#23548;NER&#27880;&#37322;&#30340;&#35821;&#26009;&#24211;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;NER&#27169;&#22411;&#65292;&#23427;&#22312;&#25163;&#21160;&#27880;&#37322;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;72.81-93.98&#65285;&#30340;&#23454;&#20307;&#32423;&#31934;&#24230;&#21644;58.14-81.77&#65285;&#30340;&#21484;&#22238;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#24110;&#21161;&#24212;&#23545;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#12290;&#20026;&#20102;&#20351;&#20854;&#20182;&#20154;&#33021;&#22815;&#37325;&#29616;&#21644;&#24314;&#31435;&#25105;&#20204;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#65292;&#27169;&#22411;&#21644;&#23454;&#39564;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although pre-trained named entity recognition (NER) models are highly accurate on modern corpora, they underperform on historical texts due to differences in language OCR errors. In this work, we develop a new NER corpus of 3.6M sentences from late medieval charters written mainly in Czech, Latin, and German.  We show that we can start with a list of known historical figures and locations and an unannotated corpus of historical texts, and use information retrieval techniques to automatically bootstrap a NER-annotated corpus. Using our corpus, we train a NER model that achieves entity-level Precision of 72.81-93.98% with 58.14-81.77% Recall on a manually-annotated test dataset. Furthermore, we show that using a weighted loss function helps to combat class imbalance in token classification tasks. To make it easy for others to reproduce and build upon our work, we publicly release our corpus, models, and experimental code.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Parse-Instructed Prefix&#30340;&#35821;&#27861;&#25511;&#21046;&#37322;&#20041;&#29983;&#25104;&#30340;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;&#20102;10&#20493;&#65292;&#24182;&#22312;&#20004;&#20010;benchmark&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.16701</link><description>&lt;p&gt;
PIP&#65306;&#35821;&#27861;&#25511;&#21046;&#30340;&#37322;&#20041;&#29983;&#25104;&#30340;&#35299;&#26512;&#25351;&#23548;&#21069;&#32512;
&lt;/p&gt;
&lt;p&gt;
PIP: Parse-Instructed Prefix for Syntactically Controlled Paraphrase Generation. (arXiv:2305.16701v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16701
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Parse-Instructed Prefix&#30340;&#35821;&#27861;&#25511;&#21046;&#37322;&#20041;&#29983;&#25104;&#30340;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;&#20102;10&#20493;&#65292;&#24182;&#22312;&#20004;&#20010;benchmark&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#25511;&#21046;&#30340;&#37322;&#20041;&#29983;&#25104;&#38656;&#35201;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#29305;&#23450;&#30340;&#35821;&#27861;&#32467;&#26500;&#20026;&#21477;&#23376;&#29983;&#25104;&#37322;&#20041;&#12290;&#29616;&#26377;&#30340;fine-tuning&#26041;&#27861;&#38656;&#35201;&#26356;&#26032;&#27169;&#22411;&#30340;&#25152;&#26377;&#21442;&#25968;&#65292;&#25104;&#26412;&#39640;&#26114;&#12290;&#22312;&#21442;&#25968;&#26377;&#25928;&#23398;&#20064;&#30340;&#26368;&#26032;&#30740;&#31350;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Parse-Instructed Prefix (PIP),&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#22312;&#20302;&#25968;&#25454;&#35774;&#32622;&#20013;&#35843;&#25972;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#26174;&#30528;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#12290; &#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#25351;&#23548;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;&#21069;&#32512;&#25429;&#33719;&#35821;&#27861;&#30456;&#20851;&#30693;&#35782;&#65306;&#30452;&#25509;&#21021;&#22987;&#21270;&#65288;PIP-Direct&#65289;&#21644;&#38388;&#25509;&#20248;&#21270;&#65288;PIP-Indirect&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;fine-tuning&#26041;&#27861;&#30456;&#27604;&#65292;PIP&#26159;&#19968;&#31181;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20855;&#26377;10&#20493;&#26356;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#19982;&#29616;&#26377;&#30340;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;&#30456;&#27604;&#65292;PIP&#22312;&#25429;&#33719;&#35821;&#27861;&#25511;&#21046;&#20449;&#24687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#24471;&#30410;&#20110;&#23558;&#35821;&#27861;&#35299;&#26512;&#26641;&#20316;&#20026;&#25351;&#23548;&#21069;&#32512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PIP&#22312;&#20004;&#20010;&#35821;&#27861;&#25511;&#21046;&#30340;&#37322;&#20041;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;&#24456;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Syntactically controlled paraphrase generation requires language models to generate paraphrases for sentences according to specific syntactic structures. Existing fine-tuning methods for this task are costly as all the parameters of the model need to be updated during the training process. Inspired by recent studies on parameter-efficient learning, we propose Parse-Instructed Prefix (PIP), a novel adaptation of prefix-tuning to tune large pre-trained language models on syntactically controlled paraphrase generation task in a low-data setting with significantly less training cost. We introduce two methods to instruct a model's encoder prefix to capture syntax-related knowledge: direct initiation (PIP-Direct) and indirect optimization (PIP-Indirect). In contrast to traditional fine-tuning methods for this task, PIP is a compute-efficient alternative with 10 times less learnable parameters. Compared to existing prefix-tuning methods, PIP excels at capturing syntax control information, ach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#35805;-KB&#20210;&#35009;&#26694;&#26550;&#65288;DKAF&#65289;&#65292;&#21487;&#20197;&#39044;&#27979;&#27599;&#20010;&#35757;&#32451;&#23545;&#35805;&#30340;&#24403;&#20195;KB&#24555;&#29031;&#20174;&#32780;&#20943;&#23569;&#23545;&#35805;-KB&#19981;&#19968;&#33268;&#24615;&#12290;&#24182;&#19988;&#65292;&#22312;&#26032;&#24314;&#31435;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DKAF&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#24182;&#25552;&#39640;&#20102;TOD&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16697</link><description>&lt;p&gt;
DKAF&#65306;&#29992;&#20110;&#20855;&#26377;&#23545;&#35805;-KB&#19981;&#19968;&#33268;&#24615;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;KB&#20210;&#35009;
&lt;/p&gt;
&lt;p&gt;
DKAF: KB Arbitration for Learning Task-Oriented Dialog Systems with Dialog-KB Inconsistencies. (arXiv:2305.16697v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#35805;-KB&#20210;&#35009;&#26694;&#26550;&#65288;DKAF&#65289;&#65292;&#21487;&#20197;&#39044;&#27979;&#27599;&#20010;&#35757;&#32451;&#23545;&#35805;&#30340;&#24403;&#20195;KB&#24555;&#29031;&#20174;&#32780;&#20943;&#23569;&#23545;&#35805;-KB&#19981;&#19968;&#33268;&#24615;&#12290;&#24182;&#19988;&#65292;&#22312;&#26032;&#24314;&#31435;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DKAF&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#24182;&#25552;&#39640;&#20102;TOD&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#36890;&#24120;&#20250;&#23558;&#20854;&#21709;&#24212;&#22522;&#20110;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#12290;&#36825;&#20123;KB&#21487;&#33021;&#26159;&#21160;&#24577;&#30340;&#65292;&#21487;&#33021;&#32463;&#24120;&#26356;&#26032;&#12290;&#29616;&#26377;&#30340;&#23398;&#20064;TOD&#20195;&#29702;&#30340;&#26041;&#27861;&#20551;&#23450;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#21487;&#29992;&#20110;&#27599;&#20010;&#21333;&#29420;&#23545;&#35805;&#30340;KB&#24555;&#29031;&#26159;&#21487;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#21482;&#26377;&#26368;&#26032;&#30340;KB&#24555;&#29031;&#21487;&#29992;&#20110;&#35757;&#32451;&#65292;&#22240;&#27492;&#65292;&#35757;&#32451;&#23545;&#35805;&#21487;&#33021;&#21253;&#21547;&#19982;&#26368;&#26032;KB&#20914;&#31361;&#30340;&#20107;&#23454;&#12290;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#36825;&#20123;&#23545;&#35805;-KB&#19981;&#19968;&#33268;&#21487;&#33021;&#20250;&#20351;TOD&#20195;&#29702;&#23398;&#20064;&#31639;&#27861;&#28151;&#28102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20855;&#26377;&#23545;&#35805;-KB&#19981;&#19968;&#33268;&#24615;&#30340;&#23398;&#20064;TOD&#20195;&#29702;&#30340;&#26032;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;-KB&#20210;&#35009;&#26694;&#26550;&#65288;DKAF&#65289;&#65292;&#36890;&#36807;&#39044;&#27979;&#27599;&#20010;&#35757;&#32451;&#23545;&#35805;&#30340;&#24403;&#20195;KB&#24555;&#29031;&#26469;&#20943;&#23569;&#23545;&#35805;-KB&#19981;&#19968;&#33268;&#24615;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#39044;&#27979;&#30340;KB&#24555;&#29031;&#29992;&#20110;&#35757;&#32451;&#19979;&#28216;TOD&#20195;&#29702;&#12290;&#30001;&#20110;&#27809;&#26377;&#23545;&#35805;-KB&#19981;&#19968;&#33268;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#23384;&#22312;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#30340;TOD&#20195;&#29702;&#12290;&#26032;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;DKAF&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#24182;&#25552;&#39640;&#20102;TOD&#20195;&#29702;&#25269;&#24481;&#23545;&#35805;-KB&#19981;&#19968;&#33268;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialog (TOD) agents often ground their responses on external knowledge bases (KBs). These KBs can be dynamic and may be updated frequently. Existing approaches for learning TOD agents assume the KB snapshot contemporary to each individual dialog is available during training. However, in real-world scenarios, only the latest KB snapshot is available during training and as a result, the train dialogs may contain facts conflicting with the latest KB. These dialog-KB inconsistencies in the training data may potentially confuse the TOD agent learning algorithm.  In this work, we define the novel problem of learning a TOD agent with dialog-KB inconsistencies in the training data. We propose a Dialog-KB Arbitration Framework (DKAF) which reduces the dialog-KB inconsistencies by predicting the contemporary KB snapshot for each train dialog. These predicted KB snapshots are then used for training downstream TOD agents. As there are no existing datasets with dialog-KB inconsistenci
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#21512;&#25104;&#26631;&#35782;&#31526;&#30340;&#22810;&#35270;&#35282;&#26631;&#35782;&#31526;&#26469;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26816;&#32034;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16675</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#26631;&#35782;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multiview Identifiers Enhanced Generative Retrieval. (arXiv:2305.16675v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#21512;&#25104;&#26631;&#35782;&#31526;&#30340;&#22810;&#35270;&#35282;&#26631;&#35782;&#31526;&#26469;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26816;&#32034;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20854;&#31616;&#21333;&#22320;&#23558;&#26597;&#35810;&#19982;&#29616;&#26377;&#27573;&#33853;&#21305;&#37197;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#29983;&#25104;&#27573;&#33853;&#30340;&#26631;&#35782;&#31526;&#23383;&#31526;&#20018;&#20316;&#20026;&#26816;&#32034;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26631;&#35782;&#31526;&#24517;&#39035;&#36275;&#22815;&#29420;&#29305;&#20197;&#20195;&#34920;&#19968;&#20010;&#27573;&#33853;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#25968;&#23383;ID&#25110;&#25991;&#26412;&#29255;&#27573;&#65288;&#22914;&#26631;&#39064;&#25110;&#23376;&#23383;&#31526;&#20018;&#65289;&#20316;&#20026;&#26631;&#35782;&#31526;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26631;&#35782;&#31526;&#19981;&#33021;&#24456;&#22909;&#22320;&#35206;&#30422;&#19968;&#20010;&#27573;&#33853;&#30340;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#26631;&#35782;&#31526;&#65292;&#21363;&#22522;&#20110;&#27573;&#33853;&#20869;&#23481;&#29983;&#25104;&#30340;&#21512;&#25104;&#26631;&#35782;&#31526;&#65292;&#21487;&#20197;&#25972;&#21512;&#25991;&#26412;&#29255;&#27573;&#32570;&#20047;&#30340;&#24773;&#22659;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21516;&#26102;&#32771;&#34385;&#22810;&#35270;&#35282;&#26631;&#35782;&#31526;&#65292;&#21253;&#25324;&#21512;&#25104;&#26631;&#35782;&#31526;&#12289;&#26631;&#39064;&#21644;&#23376;&#23383;&#31526;&#20018;&#12290;&#36825;&#20123;&#26631;&#35782;&#31526;&#30340;&#35270;&#35282;&#30456;&#20114;&#34917;&#20805;&#65292;&#26377;&#21161;&#20110;&#20174;&#22810;&#20010;&#35282;&#24230;&#32508;&#21512;&#25490;&#21517;&#27573;&#33853;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#24335;&#26816;&#32034;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instead of simply matching a query to pre-existing passages, generative retrieval generates identifier strings of passages as the retrieval target. At a cost, the identifier must be distinctive enough to represent a passage. Current approaches use either a numeric ID or a text piece (such as a title or substrings) as the identifier. However, these identifiers cannot cover a passage's content well. As such, we are motivated to propose a new type of identifier, synthetic identifiers, that are generated based on the content of a passage and could integrate contextualized information that text pieces lack. Furthermore, we simultaneously consider multiview identifiers, including synthetic identifiers, titles, and substrings. These views of identifiers complement each other and facilitate the holistic ranking of passages from multiple perspectives. We conduct a series of experiments on three public datasets, and the results indicate that our proposed approach performs the best in generative 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25968;&#24179;&#34913;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#35299;&#20915;&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#39046;&#22495;&#20013;&#26497;&#24230;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#26631;&#31614;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#30340;&#26041;&#24335;&#65292;&#26377;&#25928;&#22320;&#23545;&#31232;&#30095;&#20998;&#25968;&#39044;&#27979;&#36827;&#34892;&#27491;&#38754;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2305.16664</link><description>&lt;p&gt;
&#22810;&#26041;&#38754;&#21457;&#38899;&#35780;&#20272;&#30340;&#20998;&#25968;&#24179;&#34913;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Score-balanced Loss for Multi-aspect Pronunciation Assessment. (arXiv:2305.16664v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25968;&#24179;&#34913;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#35299;&#20915;&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#39046;&#22495;&#20013;&#26497;&#24230;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#26631;&#31614;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#30340;&#26041;&#24335;&#65292;&#26377;&#25928;&#22320;&#23545;&#31232;&#30095;&#20998;&#25968;&#39044;&#27979;&#36827;&#34892;&#27491;&#38754;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25216;&#26415;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#24050;&#32463;&#21521;&#35780;&#20272;&#27969;&#30021;&#24230;&#21644;&#37325;&#38899;&#31561;&#22810;&#20010;&#26041;&#38754;&#35780;&#20215;&#21457;&#38899;&#30340;&#31995;&#32479;&#36807;&#28193;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#27599;&#20010;&#26041;&#38754;&#20869;&#30340;&#20998;&#25968;&#26631;&#31614;&#39640;&#24230;&#19981;&#24179;&#34913;&#65292;&#20294;&#29616;&#26377;&#30340;&#30740;&#31350;&#24456;&#23569;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#8212;&#8212;&#20998;&#25968;&#24179;&#34913;&#25439;&#22833;&#65292;&#20197;&#35299;&#20915;&#19981;&#22343;&#21248;&#25968;&#25454;&#36896;&#25104;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#23545;&#22810;&#25968;&#20998;&#25968;&#30340;&#20559;&#32622;&#12290;&#20316;&#20026;&#19968;&#31181;&#37325;&#26032;&#21152;&#26435;&#30340;&#26041;&#27861;&#65292;&#24403;&#39044;&#27979;&#20998;&#25968;&#23646;&#20110;&#23569;&#25968;&#31867;&#26102;&#65292;&#25105;&#20204;&#20998;&#37197;&#26356;&#39640;&#30340;&#25104;&#26412;&#65292;&#20174;&#32780;&#24341;&#23548;&#27169;&#22411;&#23545;&#31232;&#30095;&#20998;&#25968;&#39044;&#27979;&#33719;&#24471;&#27491;&#38754;&#21453;&#39304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#21152;&#26435;&#22240;&#23376;&#65292;&#21033;&#29992;&#20102;&#26377;&#25928;&#26679;&#26412;&#25968;&#30340;&#27010;&#24565;&#21644;&#20998;&#25968;&#30340;&#25490;&#21517;&#12290;&#25105;&#20204;&#22312; speechocean762 &#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#33509;&#24178;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#30340;&#19981;&#24179;&#34913;&#20998;&#25968;&#12290;&#25913;&#36827;&#32467;&#26524;&#23588;&#20854;&#22312;&#36825;&#31181;&#19981;&#24179;&#34913;&#30340;&#26041;&#38754;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With rapid technological growth, automatic pronunciation assessment has transitioned toward systems that evaluate pronunciation in various aspects, such as fluency and stress. However, despite the highly imbalanced score labels within each aspect, existing studies have rarely tackled the data imbalance problem. In this paper, we suggest a novel loss function, score-balanced loss, to address the problem caused by uneven data, such as bias toward the majority scores. As a re-weighting approach, we assign higher costs when the predicted score is of the minority class, thus, guiding the model to gain positive feedback for sparse score prediction. Specifically, we design two weighting factors by leveraging the concept of an effective number of samples and using the ranks of scores. We evaluate our method on the speechocean762 dataset, which has noticeably imbalanced scores for several aspects. Improved results particularly on such uneven aspects prove the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>GDA&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20851;&#31995;&#25991;&#26412;&#22686;&#24378;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#37319;&#29992;&#20004;&#20010;&#20114;&#34917;&#27169;&#22359;&#65292;&#20445;&#25345;&#35821;&#20041;&#21644;&#35821;&#27861;&#32467;&#26500;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#20351;&#29992;&#23454;&#20307;&#25552;&#31034;&#25193;&#23637;&#19978;&#19979;&#25991;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;GDA&#36229;&#36234;&#20102;&#29616;&#26377;&#22686;&#24378;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16663</link><description>&lt;p&gt;
GDA: &#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
GDA: Generative Data Augmentation Techniques for Relation Extraction Tasks. (arXiv:2305.16663v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16663
&lt;/p&gt;
&lt;p&gt;
GDA&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20851;&#31995;&#25991;&#26412;&#22686;&#24378;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#37319;&#29992;&#20004;&#20010;&#20114;&#34917;&#27169;&#22359;&#65292;&#20445;&#25345;&#35821;&#20041;&#21644;&#35821;&#27861;&#32467;&#26500;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#20351;&#29992;&#23454;&#20307;&#25552;&#31034;&#25193;&#23637;&#19978;&#19979;&#25991;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;GDA&#36229;&#36234;&#20102;&#29616;&#26377;&#22686;&#24378;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#36275;&#22815;&#30340;&#35757;&#32451;&#26631;&#27880;&#26102;&#65292;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#21487;&#20197;&#22312;&#21477;&#23376;&#20013;&#25552;&#21462;&#20986;&#20004;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#33719;&#24471;&#36825;&#31181;&#26631;&#27880;&#26159;&#36153;&#21147;&#30340;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#38480;&#21046;&#30340;&#26631;&#27880;&#33539;&#22260;&#20043;&#22806;&#29983;&#25104;&#20266;&#26631;&#27880;&#21477;&#23376;&#12290;&#24403;&#37319;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#22686;&#24378;&#26102;&#65292;&#36825;&#20123;&#25216;&#26415;&#26080;&#27861;&#20445;&#25345;&#21407;&#22987;&#21477;&#23376;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;seq2seq&#27169;&#22411;&#34920;&#36798;&#20851;&#31995;&#26102;&#26080;&#27861;&#20445;&#25345;&#21477;&#23376;&#30340;&#35821;&#27861;&#32467;&#26500;&#65292; resulting in less diverse augmentations &#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#20851;&#31995;&#25991;&#26412;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#31216;&#20026;GDA&#65292;&#23427;&#20351;&#29992;&#20004;&#20010;&#20114;&#34917;&#27169;&#22359;&#26469;&#20445;&#25345;&#35821;&#20041;&#19968;&#33268;&#24615;&#21644;&#35821;&#27861;&#32467;&#26500;&#12290;&#25105;&#20204;&#37319;&#29992;&#29983;&#25104;&#24335;&#20844;&#24335;&#65292;&#24182;&#35774;&#35745;&#19968;&#20010;&#22810;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#20197;&#23454;&#29616;&#21327;&#21516;&#25928;&#24212;&#12290;&#27492;&#22806;&#65292;GDA&#37319;&#29992;&#23454;&#20307;&#25552;&#31034;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#23558;&#23454;&#20307;&#30340;&#19978;&#19979;&#25991;&#25193;&#23637;&#21040;&#21477;&#23376;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GDA&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;NYT10&#21644;SemEval2010 Task 8&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) tasks show promising performance in extracting relations from two entities mentioned in sentences, given sufficient annotations available during training. Such annotations would be labor-intensive to obtain in practice. Existing work adopts data augmentation techniques to generate pseudo-annotated sentences beyond limited annotations. These techniques neither preserve the semantic consistency of the original sentences when rule-based augmentations are adopted, nor preserve the syntax structure of sentences when expressing relations using seq2seq models, resulting in less diverse augmentations. In this work, we propose a dedicated augmentation technique for relational texts, named GDA, which uses two complementary modules to preserve both semantic consistency and syntax structures. We adopt a generative formulation and design a multi-tasking solution to achieve synergies. Furthermore, GDA adopts entity hints as the prior knowledge of the generative model to augm
&lt;/p&gt;</description></item><item><title>LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;Adaplanner&#33258;&#36866;&#24212;&#25913;&#36827;&#33258;&#24049;&#30340;&#35745;&#21010;&#20197;&#24212;&#23545;&#29615;&#22659;&#21453;&#39304;&#65292;&#20026;&#27492;&#25552;&#20986;&#35745;&#21010;&#20869;&#22806;&#30340;&#25913;&#36827;&#31574;&#30053;&#20197;&#21450;&#20195;&#30721;&#39118;&#26684;&#30340;LLM&#25552;&#31034;&#32467;&#26500;&#21644;&#25216;&#33021;&#21457;&#29616;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.16653</link><description>&lt;p&gt;
AdaPlanner:&#33258;&#36866;&#24212;&#35268;&#21010;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#12290; &#65288;arXiv&#65306;2305.16653v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
AdaPlanner: Adaptive Planning from Feedback with Language Models. (arXiv:2305.16653v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16653
&lt;/p&gt;
&lt;p&gt;
LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;Adaplanner&#33258;&#36866;&#24212;&#25913;&#36827;&#33258;&#24049;&#30340;&#35745;&#21010;&#20197;&#24212;&#23545;&#29615;&#22659;&#21453;&#39304;&#65292;&#20026;&#27492;&#25552;&#20986;&#35745;&#21010;&#20869;&#22806;&#30340;&#25913;&#36827;&#31574;&#30053;&#20197;&#21450;&#20195;&#30721;&#39118;&#26684;&#30340;LLM&#25552;&#31034;&#32467;&#26500;&#21644;&#25216;&#33021;&#21457;&#29616;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#20013;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#36138;&#23146;&#22320;&#37319;&#21462;&#34892;&#21160;&#32780;&#27809;&#26377;&#35745;&#21010;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#19981;&#21487;&#36866;&#24212;&#29615;&#22659;&#21453;&#39304;&#30340;&#38745;&#24577;&#35745;&#21010;&#12290;&#22240;&#27492;&#65292;&#38543;&#30528;&#38382;&#39064;&#22797;&#26434;&#24615;&#21644;&#35745;&#21010;&#27700;&#24179;&#30340;&#22686;&#21152;&#65292;LLM&#20195;&#29702;&#30340;&#39034;&#24207;&#20915;&#31574;&#24615;&#33021;&#20250;&#36864;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#29615;&#26041;&#27861;AdaPlanner&#65292;&#23427;&#20801;&#35768;LLM&#20195;&#29702;&#26681;&#25454;&#29615;&#22659;&#21453;&#39304;&#33258;&#36866;&#24212;&#22320;&#25913;&#36827;&#20854;&#33258;&#21160;&#29983;&#25104;&#30340;&#35745;&#21010;&#12290;&#22312;AdaPlanner&#20013;&#65292;LLM&#20195;&#29702;&#36890;&#36807;&#35745;&#21010;&#20869;&#21644;&#35745;&#21010;&#22806;&#30340;&#25913;&#36827;&#31574;&#30053;&#33258;&#36866;&#24212;&#22320;&#25913;&#36827;&#20854;&#35745;&#21010;&#12290;&#20026;&#20102;&#20943;&#36731;&#24187;&#35273;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20195;&#30721;&#39118;&#26684;&#30340;LLM&#25552;&#31034;&#32467;&#26500;&#65292;&#20419;&#36827;&#20102;&#36328;&#21508;&#31181;&#20219;&#21153;&#65292;&#29615;&#22659;&#21644;&#20195;&#29702;&#33021;&#21147;&#30340;&#35745;&#21010;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#33021;&#21457;&#29616;&#26426;&#21046;&#65292;&#21033;&#29992;&#25104;&#21151;&#30340;&#35745;&#21010;&#20316;&#20026;&#23569;&#37327;&#31034;&#20363;&#65292;&#20351;&#35745;&#21010;&#26356;&#20855;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#35328;&#36866;&#37197;&#22120;&#65288;TADA&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#36866;&#37197;&#22120;&#23545;&#40784;&#38750;&#26631;&#20934;&#32654;&#24335;&#33521;&#35821;&#26041;&#35328;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#26631;&#20934;&#32654;&#24335;&#33521;&#35821;&#30340;&#20219;&#21153;&#29305;&#23450;&#36866;&#37197;&#22120;&#32452;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#35328;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#26041;&#35328;&#33521;&#35821;NLP&#30340;&#24191;&#27867;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16651</link><description>&lt;p&gt;
TADA:&#33521;&#35821;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#35328;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
TADA: Task-Agnostic Dialect Adapters for English. (arXiv:2305.16651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#35328;&#36866;&#37197;&#22120;&#65288;TADA&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#36866;&#37197;&#22120;&#23545;&#40784;&#38750;&#26631;&#20934;&#32654;&#24335;&#33521;&#35821;&#26041;&#35328;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#26631;&#20934;&#32654;&#24335;&#33521;&#35821;&#30340;&#20219;&#21153;&#29305;&#23450;&#36866;&#37197;&#22120;&#32452;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#35328;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#26041;&#35328;&#33521;&#35821;NLP&#30340;&#24191;&#27867;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#30340;&#20027;&#35201;&#20986;&#21457;&#28857;&#65292;&#20294;&#26159;&#23545;&#20110;&#26631;&#20934;&#32654;&#24335;&#33521;&#35821;&#20197;&#22806;&#30340;&#20854;&#20182;&#33521;&#35821;&#26041;&#35328;&#30340;&#20351;&#29992;&#32773;&#65292;&#20854;&#22833;&#36133;&#29575;&#36739;&#39640;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#25110;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#26041;&#35328;&#21644;&#20219;&#21153;&#23545;&#36827;&#34892;&#24178;&#39044;&#65292;&#36825;&#20351;&#24471;&#40065;&#26834;&#30340;&#26041;&#35328;&#33521;&#35821;NLP&#30340;&#24191;&#27867;&#37319;&#29992;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36866;&#37197;&#22120;&#23545;&#40784;&#38750;&#26631;&#20934;&#32654;&#24335;&#33521;&#35821;&#26041;&#35328;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#26631;&#20934;&#32654;&#24335;&#33521;&#35821;&#30340;&#20219;&#21153;&#29305;&#23450;&#36866;&#37197;&#22120;&#32452;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#35328;&#36866;&#24212;&#12290;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#35328;&#36866;&#37197;&#22120;&#65288;TADA&#65289;&#22312;&#27809;&#26377;&#29305;&#23450;&#20219;&#21153;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#20102;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;4&#20010;&#26041;&#35328;&#21464;&#20307;&#30340;&#26041;&#35328;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models, the dominant starting point for Natural Language Processing (NLP) applications, fail at a higher rate for speakers of English dialects other than Standard American English (SAE). Prior work addresses this using task-specific data or synthetic data augmentation, both of which require intervention for each dialect and task pair. This poses a scalability issue that prevents the broad adoption of robust dialectal English NLP. We introduce a simple yet effective method for task-agnostic dialect adaptation by aligning non-SAE dialects using adapters and composing them with task-specific adapters from SAE. Task-Agnostic Dialect Adapters (TADA) improve dialectal robustness on 4 dialectal variants of the GLUE benchmark without task-specific supervision.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#30005;&#24433;&#21644;&#30005;&#35270;&#21095;&#20013;&#35821;&#29992;&#27169;&#24335;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#20102;&#20960;&#31181;&#26803;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#30001;&#22899;&#28436;&#21592;&#25198;&#28436;&#30340;&#35282;&#33394;&#34429;&#28982;&#23569;&#35265;&#65292;&#20294;&#23427;&#20204;&#26356;&#23481;&#26131;&#21551;&#21160;&#26032;&#30340;&#23545;&#35805;&#32447;&#32034;&#12290;</title><link>http://arxiv.org/abs/2305.16648</link><description>&lt;p&gt;
&#25103;&#21095;&#23545;&#35805;&#26803;&#29702;
&lt;/p&gt;
&lt;p&gt;
Dramatic Conversation Disentanglement. (arXiv:2305.16648v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#30005;&#24433;&#21644;&#30005;&#35270;&#21095;&#20013;&#35821;&#29992;&#27169;&#24335;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#20102;&#20960;&#31181;&#26803;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#30001;&#22899;&#28436;&#21592;&#25198;&#28436;&#30340;&#35282;&#33394;&#34429;&#28982;&#23569;&#35265;&#65292;&#20294;&#23427;&#20204;&#26356;&#23481;&#26131;&#21551;&#21160;&#26032;&#30340;&#23545;&#35805;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#30005;&#24433;&#21644;&#30005;&#35270;&#21095;&#20013;&#30340;&#23545;&#35805;&#26803;&#29702;&#12290;&#19982;&#20197;&#24448;&#30340;&#30740;&#31350;&#37325;&#28857;&#25918;&#22312; IRC &#32842;&#22825;&#23460;&#23545;&#35805;&#19978;&#30340;&#23545;&#35805;&#26803;&#29702;&#19981;&#21516;&#65292;&#30005;&#24433;&#21644;&#30005;&#35270;&#21095;&#25552;&#20379;&#20102;&#19968;&#20010;&#30740;&#31350;&#38754;&#23545;&#38754;&#22810;&#26041;&#20114;&#21160;&#20013;&#22797;&#26434;&#30340;&#35821;&#29992;&#27169;&#24335;&#30340;&#31354;&#38388;&#12290;&#26412;&#25991;&#20511;&#37492;&#31038;&#20250;&#35821;&#35328;&#23398;&#12289;&#31038;&#20250;&#23398;&#21644;&#30005;&#24433;&#30740;&#31350;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#23558;&#35805;&#39064;&#21644;&#35752;&#35770;&#27425;&#24207;&#30340;&#27010;&#24565;&#33853;&#23454;&#21040;&#25103;&#21095;&#25991;&#26412;&#30340;&#23545;&#35805;&#32447;&#32034;&#19978;&#65292;&#24182;&#20351;&#29992;&#35813;&#23450;&#20041;&#23545;&#21253;&#25324; 831 &#37096;&#30005;&#24433;&#30340;&#20849;&#35745; 10,033 &#27573;&#23545;&#35805;&#36827;&#34892;&#20102;&#25968;&#25454;&#38598;&#27880;&#37322;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#35813;&#25103;&#21095;&#25968;&#25454;&#38598;&#19978;&#20960;&#31181;&#26803;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#24212;&#29992;&#20110;&#26803;&#29702;&#20102; 808 &#37096;&#30005;&#24433;&#12290;&#19982;&#39044;&#26399;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#36807;&#21435; 40 &#24180;&#30340;&#24179;&#22343;&#23545;&#35805;&#32447;&#32034;&#38271;&#24230;&#24182;&#27809;&#26377;&#26174;&#33879;&#20943;&#23569;&#65292;&#19988;&#30001;&#22899;&#28436;&#21592;&#25198;&#28436;&#30340;&#35282;&#33394;&#34429;&#28982;&#23569;&#35265;&#65292;&#20294;&#23427;&#20204;&#26356;&#23481;&#26131;&#21551;&#21160;&#26032;&#30340;&#23545;&#35805;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new dataset for studying conversation disentanglement in movies and TV series. While previous work has focused on conversation disentanglement in IRC chatroom dialogues, movies and TV shows provide a space for studying complex pragmatic patterns of floor and topic change in face-to-face multi-party interactions. In this work, we draw on theoretical research in sociolinguistics, sociology, and film studies to operationalize a conversational thread (including the notion of a floor change) in dramatic texts, and use that definition to annotate a dataset of 10,033 dialogue turns (comprising 2,209 threads) from 831 movies. We compare the performance of several disentanglement models on this dramatic dataset, and apply the best-performing model to disentangle 808 movies. We see that, contrary to expectation, average thread lengths do not decrease significantly over the past 40 years, and characters portrayed by actors who are women, while underrepresented, initiate more new conv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#21644;&#39044;&#27979;&#26694;&#26550;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#32477;&#23545;&#25512;&#29702;&#33021;&#21147;&#26469;&#25552;&#39640;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16646</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23569;&#26679;&#26412;&#30340;&#32477;&#23545;&#25512;&#29702;&#26469;&#25552;&#39640;&#20107;&#20214;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning. (arXiv:2305.16646v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#21644;&#39044;&#27979;&#26694;&#26550;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#32477;&#23545;&#25512;&#29702;&#33021;&#21147;&#26469;&#25552;&#39640;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#25512;&#29702;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20107;&#20214;&#65292;&#24110;&#21161;&#25552;&#39640;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24314;&#27169;&#21644;&#39044;&#27979;&#26694;&#26550;&#65292;&#20854;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#32477;&#23545;&#25512;&#29702;&#20197;&#36741;&#21161;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#65306;&#20107;&#20214;&#27169;&#22411;&#22312;&#32473;&#23450;&#36807;&#21435;&#30340;&#24773;&#20917;&#19979;&#25552;&#20986;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;; &#22312;&#20960;&#20010;&#19987;&#23478;&#27880;&#37322;&#31034;&#33539;&#30340;&#25351;&#23548;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#20102;&#20026;&#27599;&#20010;&#25552;&#35758;&#25552;&#20379;&#21487;&#33021;&#30340;&#21407;&#22240;; &#19968;&#20010;&#25628;&#32034;&#27169;&#22359;&#25214;&#21040;&#19982;&#21407;&#22240;&#21305;&#37197;&#30340;&#20808;&#21069;&#20107;&#20214;; &#19968;&#20010;&#35780;&#20998;&#20989;&#25968;&#23398;&#20250;&#26816;&#26597;&#26816;&#32034;&#21040;&#30340;&#20107;&#20214;&#26159;&#21542;&#23454;&#38469;&#19978;&#21487;&#20197;&#23548;&#33268;&#25552;&#35758;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65288;&#20122;&#39532;&#36874;&#35780;&#35770;&#21644;GDELT&#65289;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550; - &#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147; - &#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have shown astonishing performance on a wide range of reasoning tasks. In this paper, we investigate whether they could reason about real-world events and help improve the prediction accuracy of event sequence models. We design a modeling and prediction framework where a large language model performs abductive reasoning to assist an event sequence model: the event model proposes predictions on future events given the past; instructed by a few expert-annotated demonstrations, the language model learns to suggest possible causes for each proposal; a search module finds out the previous events that match the causes; a scoring function learns to examine whether the retrieved events could actually cause the proposal. Through extensive experiments on two challenging real-world datasets (Amazon Review and GDELT), we demonstrate that our framework -- thanks to the reasoning ability of language models -- could significantly outperform the state-of-the-art event sequence mo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26426;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20107;&#20214;&#21465;&#36848;&#32467;&#26500;&#33258;&#21160;&#20998;&#26512;&#31461;&#35805;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#20026;&#20998;&#26512;&#25552;&#20379;&#21160;&#35789;&#20107;&#20214;&#27880;&#37322;&#26041;&#26696;&#65292;&#24182;&#20197;&#24615;&#21035;&#20026;&#20363;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.16641</link><description>&lt;p&gt;
&#31461;&#35805;&#26159;&#21542;&#20844;&#24179;&#65311;&#20998;&#26512;&#20799;&#31461;&#31461;&#35805;&#30340;&#26102;&#38388;&#21465;&#20107;&#20107;&#20214;&#38142;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Fairy Tales Fair? Analyzing Gender Bias in Temporal Narrative Event Chains of Children's Fairy Tales. (arXiv:2305.16641v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26426;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20107;&#20214;&#21465;&#36848;&#32467;&#26500;&#33258;&#21160;&#20998;&#26512;&#31461;&#35805;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#20026;&#20998;&#26512;&#25552;&#20379;&#21160;&#35789;&#20107;&#20214;&#27880;&#37322;&#26041;&#26696;&#65292;&#24182;&#20197;&#24615;&#21035;&#20026;&#20363;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#22312;&#25105;&#20204;&#30340;&#25991;&#21270;&#20013;&#36890;&#36807;&#25925;&#20107;&#23384;&#22312;&#65292;&#36825;&#22312;&#20154;&#25991;&#21644;&#31038;&#20250;&#31185;&#23398;&#25991;&#29486;&#20013;&#24471;&#21040;&#30830;&#35748;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21152;&#20837;&#20102;&#36825;&#31181;&#36328;&#23398;&#31185;&#30340;&#21162;&#21147;&#65292;&#24182;&#22312;&#20998;&#26512;&#25925;&#20107;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#26102;&#32771;&#34385;&#20102;&#20107;&#20214;&#21465;&#36848;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35745;&#31639;&#26426;&#20998;&#26512;&#26041;&#27861;&#65292;&#33258;&#21160;&#25552;&#21462;&#25925;&#20107;&#26102;&#38388;&#21465;&#20107;&#19979;&#27599;&#20010;&#35282;&#33394;&#30340;&#22522;&#20110;&#21160;&#35789;&#30340;&#20107;&#20214;&#38142;&#21644;&#35282;&#33394;&#23646;&#24615;&#65292;&#22914;&#24615;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social biases and stereotypes are embedded in our culture in part through their presence in our stories, as evidenced by the rich history of humanities and social science literature analyzing such biases in children stories. Because these analyses are often conducted manually and at a small scale, such investigations can benefit from the use of more recent natural language processing methods that examine social bias in models and data corpora. Our work joins this interdisciplinary effort and makes a unique contribution by taking into account the event narrative structures when analyzing the social bias of stories. We propose a computational pipeline that automatically extracts a story's temporal narrative verb-based event chain for each of its characters as well as character attributes such as gender. We also present a verb-based event annotation scheme that can facilitate bias analysis by including categories such as those that align with traditional stereotypes. Through a case study 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#22522;&#26412;&#24847;&#20041;&#37492;&#21035;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#38544;&#21947;&#26816;&#27979;&#20013;&#65292;&#32531;&#35299;&#20102;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16638</link><description>&lt;p&gt;
&#23545;&#25239;&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#31471;&#21040;&#31471;&#38544;&#21947;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adversarial Multi-task Learning for End-to-end Metaphor Detection. (arXiv:2305.16638v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#22522;&#26412;&#24847;&#20041;&#37492;&#21035;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#38544;&#21947;&#26816;&#27979;&#20013;&#65292;&#32531;&#35299;&#20102;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21947;&#26816;&#27979;&#65288;MD&#65289;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#20174;&#19968;&#20010;&#31216;&#20026;&#38544;&#21947;&#35782;&#21035;&#31243;&#24207;&#30340;&#35821;&#35328;&#35268;&#21017;&#24320;&#22987;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#22522;&#26412;&#24847;&#20041;&#37492;&#21035;&#65288;BSD&#65289;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;MD&#20013;&#12290;BSD&#24314;&#31435;&#22312;&#35789;&#20041;&#28040;&#27495;&#65288;WSD&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#21518;&#32773;&#25317;&#26377;&#20016;&#23500;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#23558;MD&#21644;BSD&#22312;&#30456;&#21516;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#20174;&#32780;&#21487;&#20197;&#23398;&#20064;&#21040;&#20219;&#21153;&#19981;&#21464;&#34920;&#31034;&#12290;&#20026;&#20102;&#25429;&#25417;&#31934;&#32454;&#30340;&#23545;&#40784;&#27169;&#24335;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;MD&#21644;BSD&#30340;&#22810;&#27169;&#24577;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23436;&#20840;&#31471;&#21040;&#31471;&#65292;&#21487;&#20197;&#32531;&#35299;MD&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#25253;&#21578;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#26159;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metaphor detection (MD) suffers from limited training data. In this paper, we started with a linguistic rule called Metaphor Identification Procedure and then proposed a novel multi-task learning framework to transfer knowledge in basic sense discrimination (BSD) to MD. BSD is constructed from word sense disambiguation (WSD), which has copious amounts of data. We leverage adversarial training to align the data distributions of MD and BSD in the same feature space, so task-invariant representations can be learned. To capture fine-grained alignment patterns, we utilize the multi-mode structures of MD and BSD. Our method is totally end-to-end and can mitigate the data scarcity problem in MD. Competitive results are reported on four public datasets. Our code and datasets are available.
&lt;/p&gt;</description></item><item><title>DataFinder&#33021;&#22815;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25512;&#33616;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#31185;&#23398;&#23478;&#22312;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#23547;&#25214;&#21512;&#36866;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.16636</link><description>&lt;p&gt;
DataFinder: &#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#25512;&#33616;&#31185;&#23398;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions. (arXiv:2305.16636v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16636
&lt;/p&gt;
&lt;p&gt;
DataFinder&#33021;&#22815;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25512;&#33616;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#31185;&#23398;&#23478;&#22312;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#23547;&#25214;&#21512;&#36866;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#21644;&#39564;&#35777;&#30740;&#31350;&#24819;&#27861;&#12290;&#37492;&#20110;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#30340;&#22686;&#38271;&#65292;&#25214;&#21040;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#20219;&#20309;&#30740;&#31350;&#38382;&#39064;&#23545;&#33021;&#22815;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#30340;&#35201;&#27714;&#37117;&#26377;&#26126;&#30830;&#21644;&#38544;&#21547;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#27169;&#24577;&#21644;&#39046;&#22495;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#19968;&#20010;&#30740;&#31350;&#24819;&#27861;&#30340;&#31616;&#30701;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#25512;&#33616;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#25214;&#21040;&#31526;&#21512;&#20182;&#20204;&#38656;&#27714;&#30340;&#30456;&#20851;&#25968;&#25454;&#38598;&#12290;&#25968;&#25454;&#38598;&#25512;&#33616;&#23384;&#22312;&#29420;&#29305;&#30340;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#65292;&#25968;&#25454;&#38598;&#24456;&#38590;&#30452;&#25509;&#32034;&#24341;&#36827;&#34892;&#25628;&#32034;&#65292;&#20063;&#27809;&#26377;&#29616;&#25104;&#30340;&#35821;&#26009;&#24211;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;DataFinder&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#33258;&#21160;&#26500;&#24314;&#30340;&#36739;&#22823;&#35757;&#32451;&#38598;&#65288;17500&#20010;&#26597;&#35810;&#65289;&#21644;&#19968;&#20010;&#36739;&#23567;&#30340;&#19987;&#23478;&#27880;&#37322;&#30340;&#35780;&#20272;&#38598;&#65288;392&#20010;&#26597;&#35810;&#65289;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21508;&#31181;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning relies on datasets to develop and validate research ideas. Given the growth of publicly available data, finding the right dataset to use is increasingly difficult. Any research question imposes explicit and implicit constraints on how well a given dataset will enable researchers to answer this question, such as dataset size, modality, and domain. We introduce a new task of recommending relevant datasets given a short natural language description of a research idea, to help people find relevant datasets for their needs. Dataset recommendation poses unique challenges as an information retrieval problem; datasets are hard to directly index for search and there are no corpora readily available for this task. To operationalize this task, we build the DataFinder Dataset which consists of a larger automatically-constructed training set (17.5K queries) and a smaller expert-annotated evaluation set (392 queries). Using this data, we compare various information retrieval 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#27604;&#36739;&#20102;&#38646;&#22522;&#30784;LLM&#21644;RoBERTa&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21457;&#29616;&#21363;&#20351;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#65292;ChatGPT&#30340;&#34920;&#29616;&#20063;&#24456;&#22909;&#65292;&#20294;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#26356;&#22909;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#21487;&#33021;&#32791;&#26102;</title><link>http://arxiv.org/abs/2305.16633</link><description>&lt;p&gt;
&#38646;&#22522;&#30784;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of LLMs for Financial Tasks. (arXiv:2305.16633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16633
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#27604;&#36739;&#20102;&#38646;&#22522;&#30784;LLM&#21644;RoBERTa&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21457;&#29616;&#21363;&#20351;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#65292;ChatGPT&#30340;&#34920;&#29616;&#20063;&#24456;&#22909;&#65292;&#20294;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#26356;&#22909;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#21487;&#33021;&#32791;&#26102;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#35768;&#22810;&#38646;&#22522;&#30784;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#24778;&#20154;&#34920;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38646;&#22522;&#30784;LLMs&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23558;ChatGPT&#19982;&#19968;&#20123;&#24320;&#28304;&#29983;&#25104;&#22411;LLM&#20197;&#21450;&#22312;&#27880;&#37322;&#25968;&#25454;&#19978;&#36827;&#34892;RoBERTa&#24494;&#35843;&#30340;&#24615;&#33021;&#22312;&#38646;&#22522;&#30784;&#27169;&#24335;&#19979;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#20851;&#20110;&#25968;&#25454;&#27880;&#37322;&#12289;&#24615;&#33021;&#24046;&#36317;&#20197;&#21450;&#22312;&#37329;&#34701;&#39046;&#22495;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#30340;&#19977;&#20010;&#30456;&#20851;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#65292;ChatGPT&#30340;&#34920;&#29616;&#20063;&#24456;&#22909;&#65292;&#20294;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#24378;&#35843;&#20102;&#20351;&#29992;&#29983;&#25104;&#22411;&#27169;&#22411;&#36827;&#34892;&#27880;&#37322;&#21487;&#33021;&#26159;&#32791;&#26102;&#30340;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24211;&#22312;CC BY-NC 4.0&#35768;&#21487;&#19979;&#20844;&#24320;&#22312;GitHub&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently large language models (LLMs) like ChatGPT have shown impressive performance on many natural language processing tasks with zero-shot. In this paper, we investigate the effectiveness of zero-shot LLMs in the financial domain. We compare the performance of ChatGPT along with some open-source generative LLMs in zero-shot mode with RoBERTa fine-tuned on annotated data. We address three inter-related research questions on data annotation, performance gaps, and the feasibility of employing generative models in the finance domain. Our findings demonstrate that ChatGPT performs well even without labeled data but fine-tuned models generally outperform it. Our research also highlights how annotating with generative models can be time-intensive. Our codebase is publicly available on GitHub under CC BY-NC 4.0 license.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;QG&#26041;&#27861;&#38656;&#35201;&#26356;&#22810;&#30340;&#21442;&#32771;&#25991;&#29486;&#26469;&#25552;&#39640;&#20854;&#26377;&#25928;&#24615;&#65292;&#21333;&#20010;&#21442;&#32771;&#19981;&#36275;&#20197;&#20840;&#38754;&#35780;&#20272;&#20854;&#28508;&#21147;&#12290;&#20351;&#29992;&#22810;&#20010;&#21442;&#32771;&#25991;&#29486;&#30340;&#35780;&#20272;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2305.16626</link><description>&lt;p&gt;
&#35780;&#20272;&#38382;&#31572;&#29983;&#25104;&#38656;&#35201;&#26356;&#22810;&#30340;&#21442;&#32771;&#25991;&#29486;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Question Generation Needs More References. (arXiv:2305.16626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16626
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;QG&#26041;&#27861;&#38656;&#35201;&#26356;&#22810;&#30340;&#21442;&#32771;&#25991;&#29486;&#26469;&#25552;&#39640;&#20854;&#26377;&#25928;&#24615;&#65292;&#21333;&#20010;&#21442;&#32771;&#19981;&#36275;&#20197;&#20840;&#38754;&#35780;&#20272;&#20854;&#28508;&#21147;&#12290;&#20351;&#29992;&#22810;&#20010;&#21442;&#32771;&#25991;&#29486;&#30340;&#35780;&#20272;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#29983;&#25104;(QG)&#26159;&#19968;&#39033;&#20219;&#21153;&#65292;&#22522;&#20110;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;&#21644;&#30446;&#26631;&#31572;&#26696;&#29983;&#25104;&#19968;&#20010;&#26377;&#25928;&#21644;&#27969;&#30021;&#30340;&#38382;&#39064;&#12290;&#26681;&#25454;&#19981;&#21516;&#30340;&#30446;&#30340;&#65292;&#21363;&#20351;&#32473;&#23450;&#30456;&#21516;&#30340;&#19978;&#19979;&#25991;&#65292;&#25945;&#24072;&#20063;&#21487;&#20197;&#25552;&#20986;&#20851;&#20110;&#19981;&#21516;&#27010;&#24565;&#30340;&#38382;&#39064;&#65292;&#29978;&#33267;&#30456;&#21516;&#30340;&#27010;&#24565;&#20063;&#21487;&#20197;&#29992;&#19981;&#21516;&#30340;&#26041;&#24335;&#20070;&#20889;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;QG&#30340;&#35780;&#20272;&#36890;&#24120;&#20381;&#36182;&#20110;&#21333;&#20010;&#22522;&#20110;&#21442;&#32771;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#20363;&#22914;n-gram&#24230;&#37327;&#25110;&#23398;&#20064;&#24230;&#37327;&#65292;&#36825;&#19981;&#36275;&#20197;&#20805;&#20998;&#35780;&#20272;QG&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#37325;&#26032;&#34920;&#36848;&#21442;&#32771;&#38382;&#39064;&#65292;&#20197;&#36827;&#34892;&#26356;&#24378;&#20581;&#30340;QG&#35780;&#20272;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-3&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#35821;&#20041;&#21644;&#21477;&#27861;&#22810;&#26679;&#30340;&#38382;&#39064;&#65292;&#28982;&#21518;&#37319;&#29992;&#27969;&#34892;&#30340;&#35780;&#20272;&#25351;&#26631;&#30340;&#31616;&#21333;&#32858;&#21512;&#20316;&#20026;&#26368;&#32456;&#24471;&#20998;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22810;&#20010;&#65288;&#20266;&#65289;&#21442;&#32771;&#25991;&#29486;&#23545;&#20110;QG&#35780;&#20272;&#26356;&#26377;&#25928;&#65292;&#21516;&#26102;&#19982;&#20154;&#31867;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;&#26356;&#39640;&#65292;&#32780;&#21333;&#20010;&#21442;&#32771;&#30340;&#35780;&#20272;&#21017;&#30456;&#23545;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question generation (QG) is the task of generating a valid and fluent question based on a given context and the target answer. According to various purposes, even given the same context, instructors can ask questions about different concepts, and even the same concept can be written in different ways. However, the evaluation for QG usually depends on single reference-based similarity metrics, such as n-gram-based metric or learned metric, which is not sufficient to fully evaluate the potential of QG methods. To this end, we propose to paraphrase the reference question for a more robust QG evaluation. Using large language models such as GPT-3, we created semantically and syntactically diverse questions, then adopt the simple aggregation of the popular evaluation metrics as the final scores. Through our experiments, we found that using multiple (pseudo) references is more effective for QG evaluation while showing a higher correlation with human evaluations than evaluation with a single r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#24314;&#21407;&#22987;&#25968;&#25454;&#23384;&#20648;&#24211;&#26469;&#25552;&#39640;kNN-MT&#30340;&#21521;&#19979;&#25968;&#25454;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#19978;&#28216;&#21644;&#19979;&#28216;&#39046;&#22495;&#20043;&#38388;&#30340;&#26174;&#30528;&#24046;&#36317;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.16599</link><description>&lt;p&gt;
&#36830;&#25509;&#19978;&#19979;&#25991;&#34920;&#31034;&#20013;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#29992;&#20110;k&#26368;&#36817;&#37051;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Bridging the Domain Gaps in Context Representations for k-Nearest Neighbor Neural Machine Translation. (arXiv:2305.16599v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#24314;&#21407;&#22987;&#25968;&#25454;&#23384;&#20648;&#24211;&#26469;&#25552;&#39640;kNN-MT&#30340;&#21521;&#19979;&#25968;&#25454;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#19978;&#28216;&#21644;&#19979;&#28216;&#39046;&#22495;&#20043;&#38388;&#30340;&#26174;&#30528;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
k&#26368;&#36817;&#37051;&#26426;&#22120;&#32763;&#35793;(kNN-MT)&#22240;&#20854;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36866;&#24212;&#26032;&#30340;&#32763;&#35793;&#39046;&#22495;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#36890;&#36807;&#20351;&#29992;&#19978;&#28216;NMT&#27169;&#22411;&#36941;&#21382;&#19979;&#28216;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#23427;&#37197;&#22791;&#20102;&#19968;&#20010;&#21253;&#21547;&#21521;&#37327;&#21270;&#38190;&#20540;&#23545;&#30340;&#25968;&#25454;&#23384;&#20648;&#24211;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26816;&#32034;&#36825;&#20123;&#20449;&#24687;&#26469;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#19978;&#19979;&#28216;&#39046;&#22495;&#20043;&#38388;&#32463;&#24120;&#23384;&#22312;&#26174;&#30528;&#24046;&#36317;&#65292;&#36825;&#20250;&#25439;&#23475;&#26816;&#32034;&#20934;&#30830;&#24615;&#21644;&#26368;&#32456;&#32763;&#35793;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#21407;&#22987;&#25968;&#25454;&#23384;&#20648;&#24211;&#26469;&#25552;&#39640;kNN-MT&#30340;&#25968;&#25454;&#26816;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20462;&#35746;&#22120;&#26469;&#20462;&#35746;&#20851;&#38190;&#34920;&#31034;&#65292;&#20351;&#20854;&#26356;&#36866;&#21512;&#19979;&#28216;&#39046;&#22495;&#12290;&#20462;&#35746;&#22120;&#20351;&#29992;&#25910;&#38598;&#30340;&#35821;&#20041;&#30456;&#20851;&#38190;-&#26597;&#35810;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#25552;&#20986;&#30340;&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#65306;&#19968;&#20010;&#26159;&#38190;-&#26597;&#35810;&#35821;&#20041;&#36317;&#31163;&#65292;&#30830;&#20445;&#27599;&#20010;&#20462;&#35746;&#30340;&#38190;&#34920;&#31034;&#22312;&#35821;&#20041;&#19978;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
$k$-Nearest neighbor machine translation ($k$NN-MT) has attracted increasing attention due to its ability to non-parametrically adapt to new translation domains. By using an upstream NMT model to traverse the downstream training corpus, it is equipped with a datastore containing vectorized key-value pairs, which are retrieved during inference to benefit translation. However, there often exists a significant gap between upstream and downstream domains, which hurts the retrieval accuracy and the final translation quality. To deal with this issue, we propose a novel approach to boost the datastore retrieval of $k$NN-MT by reconstructing the original datastore. Concretely, we design a reviser to revise the key representations, making them better fit for the downstream domain. The reviser is trained using the collected semantically-related key-queries pairs, and optimized by two proposed losses: one is the key-queries semantic distance ensuring each revised key representation is semanticall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21457;&#29616;&#31038;&#20250;&#25991;&#21270;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#23545;&#35805;&#30340;&#25972;&#20010;&#36807;&#31243;&#20013;&#25429;&#33719;&#38544;&#21547;&#29305;&#24449;&#24182;&#25552;&#39640;&#35268;&#33539;&#35782;&#21035;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16598</link><description>&lt;p&gt;
NormMark: &#19968;&#20010;&#20351;&#29992;&#24369;&#30417;&#30563;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21457;&#29616;&#31038;&#20250;&#25991;&#21270;&#35268;&#33539;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NormMark: A Weakly Supervised Markov Model for Socio-cultural Norm Discovery. (arXiv:2305.16598v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21457;&#29616;&#31038;&#20250;&#25991;&#21270;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#23545;&#35805;&#30340;&#25972;&#20010;&#36807;&#31243;&#20013;&#25429;&#33719;&#38544;&#21547;&#29305;&#24449;&#24182;&#25552;&#39640;&#35268;&#33539;&#35782;&#21035;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#25991;&#21270;&#35268;&#33539;&#26159;&#19968;&#31181;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#34892;&#20026;&#25351;&#23548;&#12290;&#23558;&#31038;&#20250;&#25991;&#21270;&#35268;&#33539;&#32435;&#20837;&#23545;&#35805;&#27169;&#22411;&#20013;&#21487;&#20197;&#29983;&#25104;&#36866;&#21512;&#31038;&#20250;&#25991;&#21270;&#32972;&#26223;&#30340;&#35805;&#35821;&#12290;&#29616;&#26377;&#30340;&#35268;&#33539;&#35782;&#21035;&#26041;&#27861;&#24448;&#24448;&#21482;&#20851;&#27880;&#23545;&#35805;&#30340;&#34920;&#23618;&#29305;&#24449;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#23545;&#35805;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NormMark&#65292;&#19968;&#31181;&#27010;&#29575;&#29983;&#25104;&#30340;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#20197;&#36143;&#31359;&#25972;&#20010;&#23545;&#35805;&#30340;&#38544;&#21547;&#29305;&#24449;&#20026;&#29305;&#28857;&#12290;&#36825;&#20123;&#29305;&#24449;&#26159;&#30001;&#31163;&#25955;&#21644;&#36830;&#32493;&#30340;&#28508;&#22312;&#21464;&#37327;&#22312;&#23545;&#35805;&#21382;&#21490;&#30340;&#26465;&#20214;&#19979;&#25429;&#33719;&#30340;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#35268;&#33539;&#35782;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#21487;&#20351;&#29992;&#21464;&#20998;&#25216;&#26415;&#23545;&#24369;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#20855;&#26377;&#26377;&#38480;&#35268;&#33539;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;F1&#24471;&#20998;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;GPT-3&#12290;
&lt;/p&gt;
&lt;p&gt;
Norms, which are culturally accepted guidelines for behaviours, can be integrated into conversational models to generate utterances that are appropriate for the socio-cultural context. Existing methods for norm recognition tend to focus only on surface-level features of dialogues and do not take into account the interactions within a conversation. To address this issue, we propose NormMark, a probabilistic generative Markov model to carry the latent features throughout a dialogue. These features are captured by discrete and continuous latent variables conditioned on the conversation history, and improve the model's ability in norm recognition. The model is trainable on weakly annotated data using the variational technique. On a dataset with limited norm annotations, we show that our approach achieves higher F1 score, outperforming current state-of-the-art methods, including GPT3.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#23398;&#20064;PET&#32467;&#26500;&#24182;&#22312;GLUE&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25506;&#35752;&#20102;PET&#26550;&#26500;&#35774;&#35745;&#36873;&#25321;&#23545;&#23454;&#38469;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.16597</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models. (arXiv:2305.16597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#23398;&#20064;PET&#32467;&#26500;&#24182;&#22312;GLUE&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25506;&#35752;&#20102;PET&#26550;&#26500;&#35774;&#35745;&#36873;&#25321;&#23545;&#23454;&#38469;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PET&#65289;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#37096;&#20998;&#27169;&#22411;&#21442;&#25968;&#30340;&#23567;&#22411;&#21387;&#32553;&#26356;&#26032;&#25110;&#28155;&#21152;&#21644;&#24494;&#35843;&#23569;&#37327;&#26032;&#30340;&#27169;&#22411;&#21442;&#25968;&#21040;&#39044;&#35757;&#32451;&#32593;&#32476;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#25163;&#24037;&#35774;&#35745;&#30340;PET&#26550;&#26500;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#36807;&#33258;&#21160;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#65292;&#23427;&#20204;&#26377;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#23398;&#20064;PET&#32467;&#26500;&#30340;&#26377;&#25928;NAS&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;GLUE&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;PET&#26550;&#26500;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient tuning (PET) methods fit pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters, or appending and fine-tuning a small number of new model parameters to the pre-trained network. Hand-designed PET architectures from the literature perform well in practice, but have the potential to be improved via automated neural architecture search (NAS). We propose an efficient NAS method for learning PET architectures via structured and unstructured pruning. We present experiments on GLUE demonstrating the effectiveness of our algorithm and discuss how PET architectural design choices affect performance in practice.
&lt;/p&gt;</description></item><item><title>ParaAMR&#26159;&#19968;&#20010;&#22522;&#20110;AMR&#21453;&#21521;&#32763;&#35793;&#21019;&#24314;&#30340;&#22823;&#35268;&#27169;&#35821;&#27861;&#22810;&#26679;&#21270;&#37322;&#20041;&#25968;&#25454;&#38598;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#37322;&#20041;&#25968;&#25454;&#38598;&#22312;&#35821;&#27861;&#19978;&#26356;&#20855;&#22810;&#26679;&#24615;&#65292;&#21487;&#29992;&#20110;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#21521;&#37327;&#12289;&#35821;&#27861;&#25511;&#21046;&#30340;&#37322;&#20041;&#29983;&#25104;&#21644;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2305.16585</link><description>&lt;p&gt;
ParaAMR: &#22522;&#20110;AMR&#21453;&#21521;&#32763;&#35793;&#30340;&#22823;&#35268;&#27169;&#35821;&#27861;&#22810;&#26679;&#21270;&#37322;&#20041;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation. (arXiv:2305.16585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16585
&lt;/p&gt;
&lt;p&gt;
ParaAMR&#26159;&#19968;&#20010;&#22522;&#20110;AMR&#21453;&#21521;&#32763;&#35793;&#21019;&#24314;&#30340;&#22823;&#35268;&#27169;&#35821;&#27861;&#22810;&#26679;&#21270;&#37322;&#20041;&#25968;&#25454;&#38598;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#37322;&#20041;&#25968;&#25454;&#38598;&#22312;&#35821;&#27861;&#19978;&#26356;&#20855;&#22810;&#26679;&#24615;&#65292;&#21487;&#29992;&#20110;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#21521;&#37327;&#12289;&#35821;&#27861;&#25511;&#21046;&#30340;&#37322;&#20041;&#29983;&#25104;&#21644;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37322;&#20041;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#20219;&#21153;&#12290;&#30417;&#30563;&#24335;&#37322;&#20041;&#29983;&#25104;&#27169;&#22411;&#38656;&#35201;&#20154;&#24037;&#27880;&#37322;&#65292;&#36153;&#29992;&#39640;&#26114;&#19988;&#38590;&#20197;&#25193;&#23637;&#12290;&#19982;&#20043;&#30456;&#27604;&#65292;&#33258;&#21160;&#27880;&#37322;&#30340;&#37322;&#20041;&#23545;&#36890;&#24120;&#32570;&#20047;&#35821;&#27861;&#22810;&#26679;&#24615;&#8212;&#8212;&#29983;&#25104;&#30340;&#37322;&#20041;&#21477;&#23376;&#22312;&#21477;&#27861;&#19978;&#19982;&#28304;&#21477;&#23376;&#38750;&#24120;&#30456;&#20284;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;ParaAMR&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#21453;&#21521;&#32763;&#35793;&#21019;&#24314;&#30340;&#22823;&#35268;&#27169;&#35821;&#27861;&#22810;&#26679;&#21270;&#37322;&#20041;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#20998;&#26512;&#12289;&#23450;&#24615;&#20363;&#23376;&#21644;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;ParaAMR&#30340;&#37322;&#20041;&#21644;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#37322;&#20041;&#25968;&#25454;&#38598;&#30456;&#27604;&#22312;&#35821;&#27861;&#19978;&#26356;&#20855;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#33391;&#22909;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;ParaAMR&#22312;&#19977;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;: &#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#21521;&#37327;&#12289;&#35821;&#27861;&#25511;&#21046;&#30340;&#37322;&#20041;&#29983;&#25104;&#21644;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Paraphrase generation is a long-standing task in natural language processing (NLP). Supervised paraphrase generation models, which rely on human-annotated paraphrase pairs, are cost-inefficient and hard to scale up. On the other hand, automatically annotated paraphrase pairs (e.g., by machine back-translation), usually suffer from the lack of syntactic diversity -- the generated paraphrase sentences are very similar to the source sentences in terms of syntax. In this work, we present ParaAMR, a large-scale syntactically diverse paraphrase dataset created by abstract meaning representation back-translation. Our quantitative analysis, qualitative examples, and human evaluation demonstrate that the paraphrases of ParaAMR are syntactically more diverse compared to existing large-scale paraphrase datasets while preserving good semantic similarity. In addition, we show that ParaAMR can be used to improve on three NLP tasks: learning sentence embeddings, syntactically controlled paraphrase ge
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#24314;&#27169;&#25104;&#22270;&#24418;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24605;&#32500;&#22270;&#8221;&#65288;GoT&#65289;&#25512;&#29702;&#36741;&#21161;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#23436;&#25104;&#26356;&#21152;&#30495;&#23454;&#30340;&#12289;&#22797;&#26434;&#30340;&#24605;&#32500;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.16582</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22270;&#24605;&#32500;&#25512;&#29702;&#65306;&#36229;&#36234;&#8220;&#24605;&#32500;&#38142;&#8221;&#30340;&#26377;&#21147;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models. (arXiv:2305.16582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16582
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#24314;&#27169;&#25104;&#22270;&#24418;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24605;&#32500;&#22270;&#8221;&#65288;GoT&#65289;&#25512;&#29702;&#36741;&#21161;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#23436;&#25104;&#26356;&#21152;&#30495;&#23454;&#30340;&#12289;&#22797;&#26434;&#30340;&#24605;&#32500;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;NLP&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#8220;&#24605;&#32500;&#38142;&#8221;&#65288;CoT&#65289;&#33021;&#22815;&#36890;&#36807;&#29983;&#25104;&#20013;&#38388;&#27493;&#39588;&#26469;&#24110;&#21161;LLMs&#23436;&#25104;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#24605;&#32500;&#36807;&#31243;&#24120;&#24120;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#32780;&#19981;&#21482;&#26159;&#31616;&#21333;&#30340;&#39034;&#24207;&#24605;&#32500;&#38142;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24605;&#32500;&#22270;&#8221;&#65288;GoT&#65289;&#25512;&#29702;&#65292;&#23427;&#19981;&#20165;&#23558;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#24314;&#27169;&#25104;&#38142;&#24335;&#32467;&#26500;&#65292;&#32780;&#19988;&#36824;&#24314;&#27169;&#25104;&#22270;&#24418;&#32467;&#26500;&#12290;&#36890;&#36807;&#23558;&#24605;&#32500;&#21333;&#20803;&#34920;&#31034;&#20026;&#33410;&#28857;&#65292;&#23427;&#20204;&#20043;&#38388;&#30340;&#36830;&#25509;&#20316;&#20026;&#36793;&#32536;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25429;&#25417;&#20102;&#20154;&#31867;&#24605;&#32500;&#30340;&#38750;&#39034;&#24207;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#24605;&#32500;&#36807;&#31243;&#30340;&#26356;&#21152;&#30495;&#23454;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread use of large language models (LLMs) in NLP tasks, researchers have discovered the potential of Chain-of-thought (CoT) to assist LLMs in accomplishing complex reasoning tasks by generating intermediate steps. However, human thought processes are often non-linear, rather than simply sequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT) reasoning, which models human thought processes not only as a chain but also as a graph. By representing thought units as nodes and connections between them as edges, our approach captures the non-sequential nature of human thinking and allows for a more realistic modeling of thought processes. Similar to Multimodal-CoT, we modeled GoT reasoning as a two-stage framework, generating rationales first and then producing the final answer. Specifically, we employ an additional graph-of-thoughts encoder for GoT representation learning and fuse the GoT representation with the original input representation through a gated 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#24577;&#21464;&#21270;&#20013;&#22122;&#22768;&#30340;&#19981;&#21516;&#31867;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#23427;&#20204;&#23545;&#38750;&#30417;&#30563;&#24418;&#24577;&#23398;&#33539;&#24335;&#23436;&#25104;&#21644;&#35821;&#24577;&#21464;&#21270;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#21457;&#29616;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#27604;&#22797;&#21046;&#20559;&#24046;&#30340;&#27169;&#22411;&#26356;&#20026;&#31283;&#20581;&#65292;CMLM&#39044;&#35757;&#32451;&#21487;&#25552;&#39640;transformer&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16581</link><description>&lt;p&gt;
&#35821;&#24577;&#21464;&#21270;&#20013;&#22122;&#22768;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An Investigation of Noise in Morphological Inflection. (arXiv:2305.16581v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16581
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#24577;&#21464;&#21270;&#20013;&#22122;&#22768;&#30340;&#19981;&#21516;&#31867;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#23427;&#20204;&#23545;&#38750;&#30417;&#30563;&#24418;&#24577;&#23398;&#33539;&#24335;&#23436;&#25104;&#21644;&#35821;&#24577;&#21464;&#21270;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#21457;&#29616;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#27604;&#22797;&#21046;&#20559;&#24046;&#30340;&#27169;&#22411;&#26356;&#20026;&#31283;&#20581;&#65292;CMLM&#39044;&#35757;&#32451;&#21487;&#25552;&#39640;transformer&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#35821;&#24577;&#32454;&#24494;&#21464;&#21270;&#31995;&#32479;&#30340;&#20851;&#27880;&#36234;&#26469;&#36234;&#22810;&#65292;&#32780;&#36825;&#20123;&#35821;&#35328;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#35757;&#32451;&#20013;&#30340;&#22122;&#22768;&#26159;&#19968;&#20010;&#20005;&#37325;&#20294;&#36804;&#20170;&#20026;&#27490;&#24456;&#23569;&#34987;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#35843;&#26597;&#30495;&#27491;&#26080;&#30417;&#30563;&#30340;&#24418;&#24577;&#33539;&#24335;&#23436;&#25104;&#31649;&#36947;&#20013;&#36935;&#21040;&#30340;&#22122;&#22768;&#31867;&#22411;&#24182;&#25506;&#32034;&#20854;&#23545;&#35821;&#24577;&#21464;&#21270;&#31995;&#32479;&#30340;&#24433;&#21709;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38169;&#35823;&#20998;&#31867;&#27861;&#21644;&#27880;&#37322;&#31649;&#36947;&#30340;&#27963;&#21160;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#22122;&#38899;&#23545;&#22810;&#31181;&#26368;&#20808;&#36827;&#30340;&#35821;&#24577;&#21464;&#21270;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23383;&#31526;&#32423;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;CMLM&#65289;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#25506;&#32034;&#20102;&#23427;&#23545;&#27169;&#22411;&#25239;&#22122;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21508;&#31181;&#20307;&#31995;&#32467;&#26500;&#21463;&#21040;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#22768;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#20294;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27604;&#24102;&#26377;&#22797;&#21046;&#20559;&#24046;&#30340;&#27169;&#22411;&#26356;&#20026;&#31283;&#20581;&#12290;CMLM&#39044;&#35757;&#32451;&#26377;&#21161;&#20110;transformers&#65292;&#20294;&#23545;LSTMs&#30340;&#24433;&#21709;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a growing focus on morphological inflection systems for languages where high-quality data is scarce, training data noise is a serious but so far largely ignored concern. We aim at closing this gap by investigating the types of noise encountered within a pipeline for truly unsupervised morphological paradigm completion and its impact on morphological inflection systems: First, we propose an error taxonomy and annotation pipeline for inflection training data. Then, we compare the effect of different types of noise on multiple state-of-the-art inflection models. Finally, we propose a novel character-level masked language modeling (CMLM) pretraining objective and explore its impact on the models' resistance to noise. Our experiments show that various architectures are impacted differently by separate types of noise, but encoder-decoders tend to be more robust to noise than models trained with a copy bias. CMLM pretraining helps transformers, but has lower impact on LSTMs.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;93&#21517;NLP&#21021;&#23398;&#32773;&#30340;&#35843;&#26597;&#65292;&#21457;&#29616;&#30740;&#31350;&#20316;&#32773;&#25552;&#20379;&#23436;&#25972;&#25991;&#26723;&#12289;&#26356;&#22909;&#30340;&#20195;&#30721;&#23454;&#36341;&#21644;&#26356;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#25991;&#20214;&#26159;&#21021;&#23398;&#32773;&#25104;&#21151;&#22797;&#29616;&#26368;&#36817;NLP&#35770;&#25991;&#32467;&#26524;&#30340;&#20851;&#38190;&#65292;&#24314;&#35758;NLP&#30740;&#31350;&#20154;&#21592;&#27880;&#37325;&#36825;&#20123;&#26041;&#38754;&#65292;&#26356;&#22909;&#22320;&#25903;&#25345;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2305.16579</link><description>&lt;p&gt;
&#20154;&#20154;&#21487;&#22797;&#29616;&#30340;NLP&#30740;&#31350;&#65306;&#21021;&#23398;&#32773;&#30340;&#38656;&#27714;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
NLP Reproducibility For All: Understanding Experiences of Beginners. (arXiv:2305.16579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16579
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;93&#21517;NLP&#21021;&#23398;&#32773;&#30340;&#35843;&#26597;&#65292;&#21457;&#29616;&#30740;&#31350;&#20316;&#32773;&#25552;&#20379;&#23436;&#25972;&#25991;&#26723;&#12289;&#26356;&#22909;&#30340;&#20195;&#30721;&#23454;&#36341;&#21644;&#26356;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#25991;&#20214;&#26159;&#21021;&#23398;&#32773;&#25104;&#21151;&#22797;&#29616;&#26368;&#36817;NLP&#35770;&#25991;&#32467;&#26524;&#30340;&#20851;&#38190;&#65292;&#24314;&#35758;NLP&#30740;&#31350;&#20154;&#21592;&#27880;&#37325;&#36825;&#20123;&#26041;&#38754;&#65292;&#26356;&#22909;&#22320;&#25903;&#25345;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#36817;&#24180;&#26469;&#24322;&#24120;&#28779;&#29190;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24613;&#20110;&#36827;&#20837;&#35813;&#39046;&#22495;&#65292;&#20294;&#30446;&#21069;&#30340;&#30740;&#31350;&#22797;&#29616;&#21162;&#21147;&#26159;&#21542;&#36275;&#20197;&#35753;&#36825;&#20123;&#21021;&#23398;&#32773;&#24212;&#29992;&#26368;&#26032;&#30340;&#36827;&#23637;&#36824;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#20102;&#35299;&#21021;&#23398;&#32773;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20171;&#32461;&#24615;&#30340;NLP&#35838;&#31243;&#20013;&#24320;&#23637;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#35753;&#23398;&#29983;&#22797;&#29616;&#26368;&#36817;NLP&#35770;&#25991;&#30340;&#32467;&#26524;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#30340;&#32534;&#31243;&#25216;&#33021;&#21644;&#23545;&#30740;&#31350;&#35770;&#25991;&#30340;&#29702;&#35299;&#23545;&#23436;&#25104;&#32451;&#20064;&#30340;&#20184;&#20986;&#20165;&#26377;&#38480;&#30340;&#24433;&#21709;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#30740;&#31350;&#20316;&#32773;&#30340;&#21487;&#35775;&#38382;&#24615;&#21162;&#21147;&#26159;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#21253;&#25324;&#23436;&#25972;&#30340;&#25991;&#26723;&#12289;&#26356;&#22909;&#30340;&#32534;&#30721;&#23454;&#36341;&#21644;&#26356;&#23481;&#26131;&#33719;&#21462;&#30340;&#25968;&#25454;&#25991;&#20214;&#12290;&#21069;&#36827;&#26102;&#65292;&#25105;&#20204;&#24314;&#35758;NLP&#30740;&#31350;&#20154;&#21592;&#23494;&#20999;&#20851;&#27880;&#36825;&#20123;&#24320;&#28304;&#24037;&#20316;&#30340;&#31616;&#21333;&#26041;&#38754;&#65292;&#24182;&#20351;&#29992;&#21021;&#23398;&#32773;&#30340;&#21453;&#39304;&#35265;&#35299;&#25552;&#20379;&#21487;&#25805;&#20316;&#30340;&#24819;&#27861;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20182;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
As natural language processing (NLP) has recently seen an unprecedented level of excitement, and more people are eager to enter the field, it is unclear whether current research reproducibility efforts are sufficient for this group of beginners to apply the latest developments. To understand their needs, we conducted a study with 93 students in an introductory NLP course, where students reproduced the results of recent NLP papers. Surprisingly, we find that their programming skill and comprehension of research papers have a limited impact on their effort spent completing the exercise. Instead, we find accessibility efforts by research authors to be the key to success, including complete documentation, better coding practice, and easier access to data files. Going forward, we recommend that NLP researchers pay close attention to these simple aspects of open-sourcing their work, and use insights from beginners' feedback to provide actionable ideas on how to better support them.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#21475;&#23646;&#24615;&#21644;&#26631;&#35760;&#38271;&#24230;&#23545;&#31038;&#20132;&#24120;&#35782;&#25512;&#29702;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#21517;&#23383;&#30340;&#20154;&#21475;&#23646;&#24615;&#21644;&#21517;&#23383;&#26631;&#35760;&#21270;&#38271;&#24230;&#37117;&#26159;&#24433;&#21709;&#31038;&#20132;&#24120;&#35782;&#25512;&#29702;&#27169;&#22411;&#34892;&#20026;&#30340;&#31995;&#32479;&#24615;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.16577</link><description>&lt;p&gt;
&#26165;&#27463;&#23572;&#21644;&#21335;&#24076;&#65306;&#20154;&#21475;&#23646;&#24615;&#21644;&#26631;&#35760;&#38271;&#24230;&#23545;&#22995;&#27663;&#20559;&#35265;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Nichelle and Nancy: The Influence of Demographic Attributes and Tokenization Length on First Name Biases. (arXiv:2305.16577v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#21475;&#23646;&#24615;&#21644;&#26631;&#35760;&#38271;&#24230;&#23545;&#31038;&#20132;&#24120;&#35782;&#25512;&#29702;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#21517;&#23383;&#30340;&#20154;&#21475;&#23646;&#24615;&#21644;&#21517;&#23383;&#26631;&#35760;&#21270;&#38271;&#24230;&#37117;&#26159;&#24433;&#21709;&#31038;&#20132;&#24120;&#35782;&#25512;&#29702;&#27169;&#22411;&#34892;&#20026;&#30340;&#31995;&#32479;&#24615;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22995;&#21517;&#26367;&#25442;&#23454;&#39564;&#34920;&#26126;&#65292;&#31038;&#20132;&#24120;&#35782;&#25512;&#29702;&#27169;&#22411;&#30340;&#20542;&#21521;&#24615;&#20250;&#27839;&#30528;&#31181;&#26063;&#12289;&#27665;&#26063;&#21644;&#24615;&#21035;&#30340;&#32500;&#24230;&#65292;&#31995;&#32479;&#24615;&#22320;&#34920;&#29616;&#20986;&#31038;&#20250;&#20559;&#35265;&#65288;An&#31561;&#20154;&#65292;2023&#24180;&#65289;&#12290;&#28982;&#32780;&#65292;&#21517;&#23383;&#30340;&#20154;&#21475;&#23646;&#24615;&#19982;&#35821;&#26009;&#24211;&#39057;&#29575;&#21644;&#26631;&#35760;&#21270;&#38271;&#24230;&#24378;&#30456;&#20851;&#65292;&#36825;&#21487;&#33021;&#20250;&#29420;&#31435;&#20110;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#25110;&#38500;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#20043;&#22806;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#21517;&#23383;&#26367;&#25442;&#23454;&#39564;&#65292;&#20197;&#34913;&#37327;&#36825;&#20123;&#22240;&#32032;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#23545;&#20854;&#20182;&#22240;&#32032;&#36827;&#34892;&#25511;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20010;&#21517;&#23383;&#30340;&#20154;&#21475;&#23646;&#24615;&#65288;&#31181;&#26063;&#12289;&#27665;&#26063;&#21644;&#24615;&#21035;&#65289;&#21644;&#21517;&#23383;&#26631;&#35760;&#21270;&#38271;&#24230;&#37117;&#26159;&#24433;&#21709;&#31038;&#20132;&#24120;&#35782;&#25512;&#29702;&#27169;&#22411;&#34892;&#20026;&#30340;&#31995;&#32479;&#24615;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through the use of first name substitution experiments, prior research has demonstrated the tendency of social commonsense reasoning models to systematically exhibit social biases along the dimensions of race, ethnicity, and gender (An et al., 2023). Demographic attributes of first names, however, are strongly correlated with corpus frequency and tokenization length, which may influence model behavior independent of or in addition to demographic factors. In this paper, we conduct a new series of first name substitution experiments that measures the influence of these factors while controlling for the others. We find that demographic attributes of a name (race, ethnicity, and gender) and name tokenization length are both factors that systematically affect the behavior of social commonsense reasoning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21453;&#20107;&#23454;&#26465;&#20214;&#27979;&#35797;&#20102;&#20116;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#22312;&#21453;&#20107;&#23454;&#24773;&#26223;&#20013;&#35206;&#30422;&#30495;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#65292;&#20294;&#36825;&#31181;&#25928;&#24212;&#36890;&#24120;&#30001;&#31616;&#21333;&#30340;&#35789;&#27719;&#32447;&#32034;&#39537;&#21160;&#12290;</title><link>http://arxiv.org/abs/2305.16572</link><description>&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#29702;&#65306;&#27979;&#35797;&#35821;&#35328;&#27169;&#22411;&#23545;&#20551;&#35774;&#24773;&#26223;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Counterfactual reasoning: Testing language models' understanding of hypothetical scenarios. (arXiv:2305.16572v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21453;&#20107;&#23454;&#26465;&#20214;&#27979;&#35797;&#20102;&#20116;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#22312;&#21453;&#20107;&#23454;&#24773;&#26223;&#20013;&#35206;&#30422;&#30495;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#65292;&#20294;&#36825;&#31181;&#25928;&#24212;&#36890;&#24120;&#30001;&#31616;&#21333;&#30340;&#35789;&#27719;&#32447;&#32034;&#39537;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#21306;&#20998;&#32479;&#35745;&#30456;&#20851;&#24615;&#21644;&#26356;&#31995;&#32479;&#30340;&#22522;&#20110;&#23545;&#30495;&#23454;&#19990;&#30028;&#29702;&#35299;&#30340;&#36923;&#36753;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#21453;&#20107;&#23454;&#26465;&#20214;&#23558;&#36825;&#20123;&#22240;&#32032;&#20998;&#24320;&#65292;&#24378;&#21046;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#20551;&#35774;&#25552;&#35758;&#39044;&#27979;&#24322;&#24120;&#21518;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#26469;&#33258;&#24515;&#29702;&#35821;&#35328;&#23398;&#23454;&#39564;&#20197;&#21450;&#26356;&#22823;&#35268;&#27169;&#30340;&#21463;&#25511;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#65292;&#20197;&#25506;&#27979;&#20116;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#39044;&#27979;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#22312;&#21453;&#20107;&#23454;&#24773;&#26223;&#20013;&#35206;&#30422;&#30495;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#26356;&#24378;&#30340;&#22522;&#32447;&#19990;&#30028;&#30693;&#35782;&#26696;&#20363;&#20013;&#65292;&#36825;&#31181;&#25928;&#24212;&#26356;&#20026;&#24378;&#22823;--&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#27169;&#22411;&#65292;&#36825;&#31181;&#25928;&#24212;&#20284;&#20046;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#31616;&#21333;&#30340;&#35789;&#27719;&#32447;&#32034;&#39537;&#21160;&#30340;&#12290;&#24403;&#25105;&#20204;&#32531;&#35299;&#19990;&#30028;&#30693;&#35782;&#21644;&#35789;&#27719;&#32447;&#32034;&#30340;&#24433;&#21709;&#20197;&#27979;&#35797;&#35821;&#35328;&#30340;&#35821;&#35328;&#23398;&#33258;&#30001;&#31243;&#24230;&#26102;
&lt;/p&gt;
&lt;p&gt;
Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on the understanding of real world. We tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from five pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge -- however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31867;&#22686;&#37327;&#20449;&#24687;&#25552;&#21462;&#20013;&#20998;&#31867;&#22120;&#28418;&#31227;&#22914;&#20309;&#23548;&#33268;&#36951;&#24536;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#35299;&#20915;&#26041;&#26696;&#26469;&#32531;&#35299;&#20998;&#31867;&#22120;&#28418;&#31227;&#12290;</title><link>http://arxiv.org/abs/2305.16559</link><description>&lt;p&gt;
&#22242;&#38431;&#21512;&#20316;&#24182;&#19981;&#24635;&#26159;&#22909;&#30340;&#65306;&#31867;&#22686;&#37327;&#20449;&#24687;&#25552;&#21462;&#20013;&#20998;&#31867;&#22120;&#28418;&#31227;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Teamwork Is Not Always Good: An Empirical Study of Classifier Drift in Class-incremental Information Extraction. (arXiv:2305.16559v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31867;&#22686;&#37327;&#20449;&#24687;&#25552;&#21462;&#20013;&#20998;&#31867;&#22120;&#28418;&#31227;&#22914;&#20309;&#23548;&#33268;&#36951;&#24536;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#35299;&#20915;&#26041;&#26696;&#26469;&#32531;&#35299;&#20998;&#31867;&#22120;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#23398;&#20064;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#19981;&#26029;&#20174;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#26032;&#31867;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#20043;&#21069;&#23398;&#20064;&#36807;&#30340;&#31867;&#12290;&#28982;&#32780;&#65292;&#24403;&#23398;&#20064;&#22686;&#37327;&#31867;&#26102;&#65292;&#20998;&#31867;&#22120;&#24517;&#39035;&#19981;&#26029;&#26356;&#26032;&#20197;&#32435;&#20837;&#26032;&#31867;&#65292;&#24182;&#19988;&#20915;&#31574;&#36793;&#30028;&#30340;&#28418;&#31227;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#26681;&#26412;&#24615;&#25361;&#25112;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#23384;&#20648;&#26087;&#31867;&#21035;&#26679;&#26412;&#20197;&#36827;&#34892;&#37325;&#28436;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#26356;&#35814;&#32454;&#22320;&#30740;&#31350;&#20102;&#20998;&#31867;&#22120;&#28418;&#31227;&#22914;&#20309;&#23548;&#33268;&#36951;&#24536;&#65292;&#24182;&#25454;&#27492;&#35774;&#35745;&#20102;&#22235;&#31181;&#31616;&#21333;&#20294;&#65288;&#36229;&#32423;&#65289;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#32531;&#35299;&#20998;&#31867;&#22120;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental learning (CIL) aims to develop a learning system that can continually learn new classes from a data stream without forgetting previously learned classes. When learning classes incrementally, the classifier must be constantly updated to incorporate new classes, and the drift in decision boundary may lead to severe forgetting. This fundamental challenge, however, has not yet been studied extensively, especially in the setting where no samples from old classes are stored for rehearsal. In this paper, we take a closer look at how the drift in the classifier leads to forgetting, and accordingly, design four simple yet (super-) effective solutions to alleviate the classifier drift: an Individual Classifiers with Frozen Feature Extractor (ICE) framework where we individually train a classifier for each learning session, and its three variants ICE-PL, ICE-O, and ICE-PL&amp;O which further take the logits of previously learned classes from old sessions or a constant logit of an Ot
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#31532;&#19968;&#20010;&#27880;&#37322;&#26377;&#32454;&#31890;&#24230;&#20107;&#23454;&#38169;&#35823;&#30340;&#23545;&#35805;&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#32454;&#31890;&#24230;&#20107;&#23454;&#38169;&#35823;&#26816;&#27979;&#20316;&#20026;&#19968;&#20010;&#21477;&#23376;&#32423;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#21644; SOTA &#27169;&#22411;&#30456;&#36817;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16548</link><description>&lt;p&gt;
&#23545;&#35805;&#25688;&#35201;&#20013;&#32454;&#31890;&#24230;&#20107;&#23454;&#38169;&#35823;&#30340;&#27880;&#37322;&#21644;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Annotating and Detecting Fine-grained Factual Errors for Dialogue Summarization. (arXiv:2305.16548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#31532;&#19968;&#20010;&#27880;&#37322;&#26377;&#32454;&#31890;&#24230;&#20107;&#23454;&#38169;&#35823;&#30340;&#23545;&#35805;&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#32454;&#31890;&#24230;&#20107;&#23454;&#38169;&#35823;&#26816;&#27979;&#20316;&#20026;&#19968;&#20010;&#21477;&#23376;&#32423;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#21644; SOTA &#27169;&#22411;&#30456;&#36817;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26684;&#24335;&#33391;&#22909;&#30340;&#25991;&#26723;&#65288;&#22914;&#26032;&#38395;&#25991;&#31456;&#65289;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#23545;&#35805;&#25688;&#35201;&#19968;&#30452;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#24102;&#26377;&#32454;&#31890;&#24230;&#20107;&#23454;&#38169;&#35823;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598; DIASUMFACT&#12290;&#25105;&#20204;&#23558;&#32454;&#31890;&#24230;&#20107;&#23454;&#38169;&#35823;&#26816;&#27979;&#23450;&#20041;&#20026;&#19968;&#20010;&#21477;&#23376;&#32423;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#20004;&#20010;&#27169;&#22411;&#37117;&#20135;&#29983;&#20102;&#27425;&#20248;&#32467;&#26524;&#65292;&#20845;&#20010;&#38169;&#35823;&#31867;&#21035;&#30340;&#23439;&#24179;&#22343; F1 &#20998;&#25968;&#32422;&#20026; 0.25&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#20505;&#36873;&#25490;&#21517;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411; ENDERANKER&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#19982; SOTA &#27169;&#22411;&#19981;&#30456;&#19978;&#19979;&#65292;&#21516;&#26102;&#38656;&#35201;&#36739;&#23569;&#30340;&#36164;&#28304;&#12290;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#35777;&#23454;&#20102;&#20174;&#23545;&#35805;&#25688;&#35201;&#20013;&#26816;&#27979;&#20107;&#23454;&#38169;&#35823;&#30340;&#25361;&#25112;&#65292;&#36825;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65292;&#32780;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#23454;&#39564;&#32467;&#26524;&#20026;&#27492;&#25552;&#20379;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
A series of datasets and models have been proposed for summaries generated for well-formatted documents such as news articles. Dialogue summaries, however, have been under explored. In this paper, we present the first dataset with fine-grained factual error annotations named DIASUMFACT. We define fine-grained factual error detection as a sentence-level multi-label classification problem, and we evaluate two state-of-the-art (SOTA) models on our dataset. Both models yield sub-optimal results, with a macro-averaged F1 score of around 0.25 over 6 error classes. We further propose an unsupervised model ENDERANKER via candidate ranking using pretrained encoder-decoder models. Our model performs on par with the SOTA models while requiring fewer resources. These observations confirm the challenges in detecting factual errors from dialogue summaries, which call for further studies, for which our dataset and results offer a solid foundation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#25913;&#36827;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#39640;&#20854;&#22312;&#38646;&#26679;&#26412;&#24773;&#22659;&#19979;&#30340;&#25991;&#26412;&#20998;&#31867;&#34920;&#29616;&#12290;&#36890;&#36807;&#24341;&#20837;&#38544;&#24335;&#21644;&#26174;&#24335;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#27880;&#20837;&#26041;&#38754;&#32423;&#21035;&#30340;&#29702;&#35299;&#65292;&#20197;&#24314;&#31435;&#20219;&#21153;&#23618;&#27425;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.16521</link><description>&lt;p&gt;
&#38754;&#21521;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#30340;&#26631;&#31614;&#26080;&#20851;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Label Agnostic Pre-training for Zero-shot Text Classification. (arXiv:2305.16521v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#25913;&#36827;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#39640;&#20854;&#22312;&#38646;&#26679;&#26412;&#24773;&#22659;&#19979;&#30340;&#25991;&#26412;&#20998;&#31867;&#34920;&#29616;&#12290;&#36890;&#36807;&#24341;&#20837;&#38544;&#24335;&#21644;&#26174;&#24335;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#27880;&#20837;&#26041;&#38754;&#32423;&#21035;&#30340;&#29702;&#35299;&#65292;&#20197;&#24314;&#31435;&#20219;&#21153;&#23618;&#27425;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#23384;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#23450;&#20041;&#26631;&#31614;&#65292;&#29992;&#20110;&#23558;&#32473;&#23450;&#30340;&#25991;&#26412;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#23384;&#22312;&#30528;&#29992;&#20110;&#25551;&#36848;&#32473;&#23450;&#25991;&#26412;&#30340;&#26080;&#38480;&#26631;&#31614;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#26681;&#25454;&#25991;&#26412;&#30340;&#26041;&#38754;&#65288;&#24773;&#24863;&#12289;&#20027;&#39064;&#31561;&#65289;&#21644;&#39046;&#22495;&#65288;&#37329;&#34701;&#12289;&#27861;&#24459;&#31561;&#65289;&#65292;&#26631;&#31614;&#30340;&#35299;&#37322;&#21487;&#33021;&#22823;&#19981;&#30456;&#21516;&#12290;&#36825;&#20351;&#24471;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#26412;&#22330;&#26223;&#19979;&#65292;&#21464;&#24471;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23545;&#19981;&#21516;&#26041;&#38754;&#21644;&#39046;&#22495;&#20013;&#24050;&#30693;&#21644;&#26410;&#30693;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21363;&#38544;&#24335;&#21644;&#26174;&#24335;&#39044;&#35757;&#32451;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#26102;&#27880;&#20837;&#20102;&#26041;&#38754;&#32423;&#21035;&#30340;&#29702;&#35299;&#65292;&#30446;&#30340;&#26159;&#35753;&#27169;&#22411;&#26500;&#24314;&#20219;&#21153;&#23618;&#27425;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional approaches to text classification typically assume the existence of a fixed set of predefined labels to which a given text can be classified. However, in real-world applications, there exists an infinite label space for describing a given text. In addition, depending on the aspect (sentiment, topic, etc.) and domain of the text (finance, legal, etc.), the interpretation of the label can vary greatly. This makes the task of text classification, particularly in the zero-shot scenario, extremely challenging. In this paper, we investigate the task of zero-shot text classification with the aim of improving the ability of pre-trained language models (PLMs) to generalize to both seen and unseen data across varying aspects and domains. To solve this we introduce two new simple yet effective pre-training strategies, Implicit and Explicit pre-training. These methods inject aspect-level understanding into the model at train time with the goal of conditioning the model to build task-l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#22791;&#26576;&#20123;&#39640;&#32423;&#35821;&#35328;&#23545;&#35805;&#34892;&#20026;&#65288;&#22914;&#37325;&#22797;&#29992;&#25143;&#25152;&#35828;&#30340;&#35805;&#65289;&#30340;&#20219;&#21153;&#22411;&#31995;&#32479;&#26356;&#21463;&#27426;&#36814;&#12289;&#26356;&#20540;&#24471;&#20449;&#20219;&#65292;&#28982;&#32780;&#37027;&#20123;&#19968;&#21619;&#27169;&#20223;&#29992;&#25143;&#36755;&#20837;&#30340;&#31995;&#32479;&#21364;&#23384;&#22312;&#35832;&#22810;&#19981;&#24544;&#23454;&#21709;&#24212;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.16519</link><description>&lt;p&gt;
&#30456;&#20449;&#38543;&#26426;&#40550;&#40521;&#30340;&#21361;&#38505;&#65306;&#33258;&#28982;&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#24544;&#35802;&#24230;&#19982;&#20449;&#20219;
&lt;/p&gt;
&lt;p&gt;
The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering. (arXiv:2305.16519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#22791;&#26576;&#20123;&#39640;&#32423;&#35821;&#35328;&#23545;&#35805;&#34892;&#20026;&#65288;&#22914;&#37325;&#22797;&#29992;&#25143;&#25152;&#35828;&#30340;&#35805;&#65289;&#30340;&#20219;&#21153;&#22411;&#31995;&#32479;&#26356;&#21463;&#27426;&#36814;&#12289;&#26356;&#20540;&#24471;&#20449;&#20219;&#65292;&#28982;&#32780;&#37027;&#20123;&#19968;&#21619;&#27169;&#20223;&#29992;&#25143;&#36755;&#20837;&#30340;&#31995;&#32479;&#21364;&#23384;&#22312;&#35832;&#22810;&#19981;&#24544;&#23454;&#21709;&#24212;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#33021;&#22815;&#29983;&#20135;&#20986;&#27969;&#30021;&#32780;&#20196;&#20154;&#20449;&#26381;&#30340;&#36755;&#20986;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#20250;&#20986;&#29616;&#38169;&#35823;&#65292;&#21363;&#20174;&#30693;&#35782;&#24211;&#20013;&#25552;&#21462;&#30340;&#31572;&#26696;&#19982;&#23458;&#35266;&#20107;&#23454;&#19981;&#31526;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#37027;&#20123;&#23637;&#29616;&#20986;&#20808;&#36827;&#35821;&#35328;&#23545;&#35805;&#34892;&#20026;&#65288;&#22914;&#37325;&#22797;&#29992;&#25143;&#25152;&#35828;&#30340;&#35805;&#65289;&#30340;&#20219;&#21153;&#22411;&#31995;&#32479;&#65292;&#23454;&#38469;&#19978;&#26356;&#21463;&#27426;&#36814;&#12289;&#26356;&#20540;&#24471;&#20449;&#20219;&#65292;&#32780;&#20854;&#20182;&#29616;&#35937;&#65288;&#22914;&#20195;&#35789;&#21644;&#30465;&#30053;&#65289;&#21017;&#19981;&#34987;&#38738;&#30544;&#12290;&#25105;&#20204;&#20197;&#24320;&#25918;&#22495;&#38382;&#31572;&#31995;&#32479;&#20026;&#27979;&#35797;&#22522;&#30784;&#65292;&#27604;&#36739;&#20102;&#25968;&#20010;&#24320;&#25918;&#24335;&#21644;&#23553;&#38381;&#24335;&#22270;&#20070;&#27169;&#22411;&#30340;&#20219;&#21153;&#29983;&#25104;&#23545;&#35805;&#12290;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#34920;&#26126;&#65292;&#31995;&#32479;&#22312;&#27169;&#20223;&#29992;&#25143;&#36755;&#20837;&#30340;&#21516;&#26102;&#25552;&#20379;&#19981;&#24544;&#23454;&#30340;&#21709;&#24212;&#65292;&#36825;&#31181;&#34920;&#29616;&#30475;&#20284;&#21487;&#20449;&#65292;&#23454;&#21017;&#21361;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are known to produce output which sounds fluent and convincing, but is also often wrong, e.g. "unfaithful" with respect to a rationale as retrieved from a knowledge base. In this paper, we show that task-based systems which exhibit certain advanced linguistic dialog behaviors, such as lexical alignment (repeating what the user said), are in fact preferred and trusted more, whereas other phenomena, such as pronouns and ellipsis are dis-preferred. We use open-domain question answering systems as our test-bed for task based dialog generation and compare several open- and closed-book models. Our results highlight the danger of systems that appear to be trustworthy by parroting user input while providing an unfaithful response.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#35757;&#32451;&#20351;&#29992;&#31034;&#20363;&#12289;&#19978;&#19979;&#25991;&#28436;&#31034;&#21644;&#29983;&#25104;&#26679;&#24335;&#35268;&#21017;&#26469;&#21152;&#24378;&#24320;&#28304;LLMs&#20197;&#36798;&#21040;&#19982;&#23553;&#38381;&#22411;API&#30340;&#24037;&#20855;&#25805;&#20316;&#24615;&#33021;&#21516;&#31561;&#29978;&#33267;&#26356;&#20248;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;ToolBench&#27979;&#35797;&#24471;&#20986;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#21516;&#26102;&#26412;&#25991;&#36824;&#35777;&#26126;&#20102;&#25913;&#36827;&#30340;&#24320;&#28304;LLMs&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16504</link><description>&lt;p&gt;
&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#24037;&#20855;&#25805;&#20316;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Tool Manipulation Capability of Open-source Large Language Models. (arXiv:2305.16504v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#35757;&#32451;&#20351;&#29992;&#31034;&#20363;&#12289;&#19978;&#19979;&#25991;&#28436;&#31034;&#21644;&#29983;&#25104;&#26679;&#24335;&#35268;&#21017;&#26469;&#21152;&#24378;&#24320;&#28304;LLMs&#20197;&#36798;&#21040;&#19982;&#23553;&#38381;&#22411;API&#30340;&#24037;&#20855;&#25805;&#20316;&#24615;&#33021;&#21516;&#31561;&#29978;&#33267;&#26356;&#20248;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;ToolBench&#27979;&#35797;&#24471;&#20986;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#21516;&#26102;&#26412;&#25991;&#36824;&#35777;&#26126;&#20102;&#25913;&#36827;&#30340;&#24320;&#28304;LLMs&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;( LLMs)&#36827;&#34892;&#36719;&#20214;&#24037;&#20855;&#25805;&#20316;&#30340;&#30740;&#31350;&#22823;&#22810;&#20381;&#36182;&#20110;&#23553;&#38381;&#27169;&#22411;API&#12290;&#30001;&#20110;&#21521;&#23553;&#38381;LLMAPI&#26381;&#21153;&#20844;&#24320;&#20449;&#24687;&#23384;&#22312;&#23433;&#20840;&#21644;&#40065;&#26834;&#24615;&#39118;&#38505;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24037;&#19994;&#37319;&#29992;&#21463;&#21040;&#20102;&#23454;&#36136;&#24615;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#37027;&#23601;&#26159;&#25105;&#20204;&#33021;&#21542;&#22312;&#23454;&#36341;&#20013;&#21152;&#24378;&#24320;&#28304;LLMs&#30340;&#21151;&#33021;&#65292;&#20351;&#20854;&#22312;&#24037;&#20855;&#25805;&#20316;&#26041;&#38754;&#19982;&#39046;&#20808;&#30340;&#23553;&#38381;LLM APIs&#31454;&#20105;&#12290;&#36890;&#36807;&#20998;&#26512;&#24120;&#35265;&#30340;&#24037;&#20855;&#25805;&#20316;&#22833;&#36133;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#24320;&#28304;LLMs&#21487;&#33021;&#38656;&#35201;&#35757;&#32451;&#20351;&#29992;&#31034;&#20363;&#12289;&#19978;&#19979;&#25991;&#28436;&#31034;&#21644;&#29983;&#25104;&#26679;&#24335;&#35268;&#21017;&#26469;&#35299;&#20915;&#22833;&#36133;&#12290;&#36825;&#20123;&#35265;&#35299;&#28608;&#21457;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;LLM&#25991;&#29486;&#20013;&#30340;&#32463;&#20856;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#21487;&#20197;&#23558;&#23427;&#20204;&#20316;&#20026;&#31243;&#24207;&#25968;&#25454;&#29983;&#25104;&#30340;&#27169;&#22411;&#23545;&#40784;&#12289;&#31995;&#32479;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#28436;&#31034;&#26816;&#32034;&#22120;&#26469;&#36866;&#24212;&#24320;&#28304;LLMs&#20197;&#23454;&#29616;&#24037;&#20855;&#25805;&#20316;&#30340;&#22686;&#24378;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#25216;&#26415;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;ToolBench&#65292;&#19968;&#20010;&#24037;&#20855;&#25805;&#20316;&#33021;&#21147;&#27979;&#35797;&#22871;&#20214;&#65292;&#21253;&#25324;&#29616;&#26377;API&#21644;&#25105;&#20204;&#25913;&#36827;&#30340;&#24320;&#28304;LLMs&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#32534;&#31243;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#25913;&#36827;&#30340;&#24320;&#28304;LLMs&#33021;&#22815;&#36798;&#21040;&#25110;&#36229;&#36234;&#29616;&#26377;API&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#24050;&#32534;&#20889;&#30340;&#31243;&#24207;&#36827;&#34892;&#36731;&#24494;&#20462;&#25913;&#31561;&#23454;&#38469;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21453;&#21521;&#24037;&#31243;&#27979;&#35797;&#21644;&#40657;&#30418;&#27979;&#35797;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on software tool manipulation with large language models (LLMs) mostly rely on closed model APIs. The industrial adoption of these models is substantially constrained due to the security and robustness risks in exposing information to closed LLM API services. In this paper, we ask can we enhance open-source LLMs to be competitive to leading closed LLM APIs in tool manipulation, with practical amount of human supervision. By analyzing common tool manipulation failures, we first demonstrate that open-source LLMs may require training with usage examples, in-context demonstration and generation style regulation to resolve failures. These insights motivate us to revisit classical methods in LLM literature, and demonstrate that we can adapt them as model alignment with programmatic data generation, system prompts and in-context demonstration retrievers to enhance open-source LLMs for tool manipulation. To evaluate these techniques, we create the ToolBench, a tool manipulation 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#20363;&#30340;&#27861;&#24459;&#24341;&#25991;&#39044;&#27979;&#26041;&#27861;&#65292;&#32467;&#21512;&#20808;&#20363;&#21644;&#31435;&#27861;&#35268;&#23450;&#24605;&#32500;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#21407;&#22411;&#26550;&#26500;&#22686;&#21152;&#21487;&#35299;&#37322;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#29616;&#24378;&#22823;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#32771;&#34385;&#20102;&#39640;&#39118;&#38505;&#20219;&#21153;&#22312;&#23454;&#38469;&#31038;&#20250;&#24433;&#21709;&#20013;&#30340;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.16490</link><description>&lt;p&gt;
&#22522;&#20110;&#26679;&#20363;&#30340;&#21487;&#35299;&#37322;&#24615;&#27861;&#24459;&#24341;&#25991;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Prototype-Based Interpretability for Legal Citation Prediction. (arXiv:2305.16490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#20363;&#30340;&#27861;&#24459;&#24341;&#25991;&#39044;&#27979;&#26041;&#27861;&#65292;&#32467;&#21512;&#20808;&#20363;&#21644;&#31435;&#27861;&#35268;&#23450;&#24605;&#32500;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#21407;&#22411;&#26550;&#26500;&#22686;&#21152;&#21487;&#35299;&#37322;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#29616;&#24378;&#22823;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#32771;&#34385;&#20102;&#39640;&#39118;&#38505;&#20219;&#21153;&#22312;&#23454;&#38469;&#31038;&#20250;&#24433;&#21709;&#20013;&#30340;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#35299;&#20915;&#20855;&#26377;&#24191;&#27867;&#31038;&#20250;&#24433;&#21709;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#39046;&#22495;&#65292;&#22914;&#27861;&#24459;&#65292;&#19987;&#23478;&#32463;&#24120;&#38656;&#35201;&#35299;&#37322;&#24615;&#20197;&#20415;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#20351;&#29992;&#33258;&#21160;&#21270;&#31995;&#32479;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#35299;&#20915;&#24212;&#29992;&#20110;&#37325;&#35201;&#38382;&#39064;&#30340;&#27861;&#24459;&#24341;&#25991;&#39044;&#27979;(LCP)&#30340;&#36825;&#20123;&#35201;&#27714;&#12290;&#25105;&#20204;&#35774;&#35745;&#20219;&#21153;&#65292;&#24182;&#31867;&#27604;&#24459;&#24072;&#30340;&#24605;&#32500;&#36807;&#31243;&#65292;&#21363;&#21442;&#32771;&#20808;&#20363;&#21644;&#31435;&#27861;&#35268;&#23450;&#12290;&#22312;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#27861;&#24459;&#19987;&#23478;&#30340;&#21453;&#39304;&#65292;&#25913;&#36827;&#20102;&#30446;&#26631;&#24341;&#25991;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#21407;&#22411;&#26550;&#26500;&#26469;&#22686;&#21152;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#36981;&#24490;&#24459;&#24072;&#20351;&#29992;&#30340;&#20915;&#31574;&#21442;&#25968;&#65292;&#23454;&#29616;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#24314;&#31435;&#22312;&#24182;&#21033;&#29992;&#20102;&#20855;&#26377;&#27861;&#24459;&#29366;&#20917;&#30340;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#20102;&#39640;&#39118;&#38505;&#20219;&#21153;&#22312;&#23454;&#38469;&#31038;&#20250;&#24433;&#21709;&#20013;&#30340;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has made significant progress in the past decade, and demonstrates potential to solve problems with extensive social impact. In high-stakes decision making areas such as law, experts often require interpretability for automatic systems to be utilized in practical settings. In this work, we attempt to address these requirements applied to the important problem of legal citation prediction (LCP). We design the task with parallels to the thought-process of lawyers, i.e., with reference to both precedents and legislative provisions. After initial experimental results, we refine the target citation predictions with the feedback of legal experts. Additionally, we introduce a prototype architecture to add interpretability, achieving strong performance while adhering to decision parameters used by lawyers. Our study builds on and leverages the state-of-the-art language processing models for law, while addressing vital considerations for high-stakes tasks with practical societal i
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#35780;&#20272;&#19981;&#21516;&#32676;&#20307;&#23545;&#24433;&#21709;&#21147;&#20449;&#24687;&#30340;&#21453;&#24212;&#30340;&#20219;&#21153;&#21644;&#25152;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#65292;&#31361;&#20986;&#20102;&#26032;&#20219;&#21153;&#22312;&#24314;&#27169;&#20013;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#65292;&#39044;&#27979;&#20102;&#27599;&#20010;&#21453;&#24212;&#30340;&#24773;&#24863;&#26497;&#24615;&#21644;&#24378;&#24230;&#65292;&#24182;&#35753;&#35780;&#20272;&#21644;&#24212;&#29992;&#26356;&#21152;&#21487;&#38752;&#12290;</title><link>http://arxiv.org/abs/2305.16470</link><description>&lt;p&gt;
&#35780;&#20272;&#19981;&#21516;&#32676;&#20307;&#23545;&#24433;&#21709;&#21147;&#20449;&#24687;&#30340;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Measuring the Effect of Influential Messages on Varying Personas. (arXiv:2305.16470v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16470
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#35780;&#20272;&#19981;&#21516;&#32676;&#20307;&#23545;&#24433;&#21709;&#21147;&#20449;&#24687;&#30340;&#21453;&#24212;&#30340;&#20219;&#21153;&#21644;&#25152;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#65292;&#31361;&#20986;&#20102;&#26032;&#20219;&#21153;&#22312;&#24314;&#27169;&#20013;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#65292;&#39044;&#27979;&#20102;&#27599;&#20010;&#21453;&#24212;&#30340;&#24773;&#24863;&#26497;&#24615;&#21644;&#24378;&#24230;&#65292;&#24182;&#35753;&#35780;&#20272;&#21644;&#24212;&#29992;&#26356;&#21152;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#29992;&#25143;&#23545;&#26032;&#38395;&#20107;&#20214;&#30340;&#21453;&#24212;&#33021;&#22815;&#23454;&#29616;&#26234;&#33021;&#20195;&#29702;&#25110;&#20869;&#23481;&#29983;&#25104;&#32773;&#20272;&#35745;&#19981;&#21516;&#31038;&#21306;&#30340;&#24433;&#21709;&#24182;&#20462;&#35746;&#26410;&#21457;&#24067;&#30340;&#20449;&#24687;&#65292;&#38450;&#27490;&#31038;&#20250;&#20914;&#31361;&#21644;&#36947;&#24503;&#20260;&#23475;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65306;&#29992;&#20110;&#26032;&#38395;&#23186;&#20307;&#30340;&#20154;&#35774;&#21453;&#24212;&#39044;&#27979;&#65292;&#20197;&#39044;&#27979;&#20154;&#35774;&#65288;&#25551;&#36848;&#20010;&#20154;&#25110;&#32676;&#20307;&#65289;&#23545;&#26032;&#38395;&#20449;&#24687;&#30340;&#21453;&#24212;&#12290;&#19982;&#20197;&#24448;&#20165;&#39044;&#27979;&#26032;&#38395;&#30340;&#36890;&#29992;&#35780;&#35770;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#20219;&#21153;&#19981;&#20165;&#22312;&#24314;&#27169;&#20013;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#65292;&#36824;&#39044;&#27979;&#20102;&#27599;&#20010;&#21453;&#24212;&#30340;&#24773;&#24863;&#26497;&#24615;&#21644;&#24378;&#24230;&#12290;&#36825;&#20351;&#24471;&#23545;&#20154;&#35774;&#30340;&#24515;&#29702;&#29366;&#24577;&#36827;&#34892;&#26356;&#20934;&#30830;&#21644;&#20840;&#38754;&#30340;&#25512;&#26029;&#25104;&#20026;&#21487;&#33021;&#12290;&#21516;&#26102;&#65292;&#29983;&#25104;&#30340;&#24773;&#24863;&#32500;&#24230;&#20351;&#24471;&#35780;&#20272;&#21644;&#24212;&#29992;&#26356;&#21152;&#21487;&#38752;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;Twitter&#30340;3,847&#20010;&#26032;&#38395;&#26631;&#39064;&#30340;13,357&#20010;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting how a user responds to news events enables important applications such as allowing intelligent agents or content producers to estimate the effect on different communities and revise unreleased messages to prevent unexpected bad outcomes such as social conflict and moral injury. We present a new task, Response Forecasting on Personas for News Media, to estimate the response a persona (characterizing an individual or a group) might have upon seeing a news message. Compared to the previous efforts which only predict generic comments to news, the proposed task not only introduces personalization in the modeling but also predicts the sentiment polarity and intensity of each response. This enables more accurate and comprehensive inference on the mental state of the persona. Meanwhile, the generated sentiment dimensions make the evaluation and application more reliable. We create the first benchmark dataset, which consists of 13,357 responses to 3,847 news headlines from Twitter. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ATINTER&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#37325;&#20889;&#23545;&#25239;&#24615;&#36755;&#20837;&#20197;&#20351;&#20854;&#23545;&#20110;&#19979;&#28216;&#25991;&#26412;&#20998;&#31867;&#22120;&#26469;&#35828;&#19981;&#20855;&#26377;&#23545;&#25239;&#24615;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#25915;&#20987;&#26426;&#21046;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#27604;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#19988;&#19981;&#20250;&#29306;&#29298;&#20219;&#21153;&#20934;&#30830;&#24615;</title><link>http://arxiv.org/abs/2305.16444</link><description>&lt;p&gt;
&#19981;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#21482;&#38656;&#37325;&#20889;&#65306;&#36890;&#36807;&#37325;&#20889;&#25991;&#26412;&#26469;&#23545;&#25239;&#23545;&#25239;&#24615;&#25200;&#21160;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Don't Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text. (arXiv:2305.16444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ATINTER&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#37325;&#20889;&#23545;&#25239;&#24615;&#36755;&#20837;&#20197;&#20351;&#20854;&#23545;&#20110;&#19979;&#28216;&#25991;&#26412;&#20998;&#31867;&#22120;&#26469;&#35828;&#19981;&#20855;&#26377;&#23545;&#25239;&#24615;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#25915;&#20987;&#26426;&#21046;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#27604;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#19988;&#19981;&#20250;&#29306;&#29298;&#20219;&#21153;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#36716;&#21270;&#36755;&#20837;&#20197;&#20445;&#25252;&#25991;&#26412;&#20998;&#31867;&#22120;&#20813;&#21463;&#23545;&#25239;&#24615;&#25915;&#20987;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;ATINTER&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25130;&#33719;&#24182;&#23398;&#20064;&#37325;&#20889;&#23545;&#25239;&#24615;&#36755;&#20837;&#20197;&#20351;&#20854;&#23545;&#20110;&#19979;&#28216;&#25991;&#26412;&#20998;&#31867;&#22120;&#26469;&#35828;&#19981;&#20855;&#26377;&#23545;&#25239;&#24615;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#21644;&#20116;&#31181;&#25915;&#20987;&#26426;&#21046;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ATINTER&#27604;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#19981;&#20250;&#29306;&#29298;&#20219;&#21153;&#20934;&#30830;&#24615;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;SST-2&#25968;&#25454;&#38598;&#36827;&#34892;&#24773;&#24863;&#20998;&#31867;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#27604;&#25105;&#20204;&#26368;&#22909;&#30340;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#26356;&#22810;&#30340;4%&#30340;&#23545;&#25239;&#20934;&#30830;&#24615;&#65292;&#32780;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#19979;&#38477;&#26356;&#23567;(0.5%&#27604;2.5%)&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ATINTER&#22312;&#19981;&#38656;&#35201;&#26126;&#30830;&#20026;&#36825;&#20123;&#35774;&#32622;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#27178;&#21521;&#25512;&#24191;&#21040;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#21644;&#20998;&#31867;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;ATINTER&#34987;&#35757;&#32451;&#20197;&#21024;&#38500;SST-2&#25968;&#25454;&#38598;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#26102;&#65292;&#23427;&#29978;&#33267;&#21487;&#20197;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can language models transform inputs to protect text classifiers against adversarial attacks? In this work, we present ATINTER, a model that intercepts and learns to rewrite adversarial inputs to make them non-adversarial for a downstream text classifier. Our experiments on four datasets and five attack mechanisms reveal that ATINTER is effective at providing better adversarial robustness than existing defense approaches, without compromising task accuracy. For example, on sentiment classification using the SST-2 dataset, our method improves the adversarial accuracy over the best existing defense approach by more than 4% with a smaller decrease in task accuracy (0.5% vs 2.5%). Moreover, we show that ATINTER generalizes across multiple downstream tasks and classifiers without having to explicitly retrain it for those settings. Specifically, we find that when ATINTER is trained to remove adversarial perturbations for the sentiment classification task on the SST-2 dataset, it even transfe
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#25968;&#23398;&#20844;&#24335;&#26102;&#38754;&#20020;&#30340;&#35789;&#27719;&#37327;&#36739;&#23567;&#12289;&#31526;&#21495;&#24207;&#21015;&#36739;&#38271;&#21644;&#38656;&#35201;&#39640;&#24230;&#31934;&#24230;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#24207;&#21015;&#21040;&#24207;&#21015;&#32593;&#32476;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.16433</link><description>&lt;p&gt;
&#25968;&#23398;&#20844;&#24335;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation for Mathematical Formulae. (arXiv:2305.16433v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16433
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#25968;&#23398;&#20844;&#24335;&#26102;&#38754;&#20020;&#30340;&#35789;&#27719;&#37327;&#36739;&#23567;&#12289;&#31526;&#21495;&#24207;&#21015;&#36739;&#38271;&#21644;&#38656;&#35201;&#39640;&#24230;&#31934;&#24230;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#24207;&#21015;&#21040;&#24207;&#21015;&#32593;&#32476;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#27169;&#31946;&#30340;&#34920;&#36798;&#35821;&#35328;&#21644;&#26126;&#30830;&#30340;&#20869;&#23481;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#25968;&#23398;&#20844;&#24335;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#38382;&#39064;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30456;&#27604;&#65292;&#25968;&#23398;&#20844;&#24335;&#20855;&#26377;&#26356;&#23567;&#30340;&#35789;&#27719;&#37327;&#21644;&#26356;&#38271;&#30340;&#31526;&#21495;&#24207;&#21015;&#65292;&#32780;&#32763;&#35793;&#38656;&#35201;&#26497;&#39640;&#30340;&#31934;&#24230;&#26469;&#28385;&#36275;&#25968;&#23398;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23436;&#25104;&#20102;&#20174;LaTeX&#21040;Mathematica&#30340;&#32763;&#35793;&#20197;&#21450;&#20174;LaTeX&#21040;&#35821;&#20041;LaTeX&#30340;&#32763;&#35793;&#12290;&#23613;&#31649;&#24490;&#29615;&#65292;&#36882;&#24402;&#21644;&#36716;&#25442;&#32593;&#32476;&#38590;&#20197;&#20445;&#30041;&#25152;&#26377;&#21253;&#21547;&#30340;&#20449;&#24687;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#21367;&#31215;&#24207;&#21015;&#21040;&#24207;&#21015;&#32593;&#32476;&#20998;&#21035;&#21487;&#20197;&#36798;&#21040;95.1&#65285;&#21644;90.7&#65285;&#30340;&#31934;&#30830;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of neural machine translation of mathematical formulae between ambiguous presentation languages and unambiguous content languages. Compared to neural machine translation on natural language, mathematical formulae have a much smaller vocabulary and much longer sequences of symbols, while their translation requires extreme precision to satisfy mathematical information needs. In this work, we perform the tasks of translating from LaTeX to Mathematica as well as from LaTeX to semantic LaTeX. While recurrent, recursive, and transformer networks struggle with preserving all contained information, we find that convolutional sequence-to-sequence networks achieve 95.1% and 90.7% exact matches, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#26631;&#37327;&#21103;&#35789;&#65292;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#8220;&#32477;&#23545;&#8221;&#19982;&#8220;&#30456;&#23545;&#8221;&#35789;&#30340;&#34920;&#29616;&#65292;&#22312;&#28041;&#21450;&#36923;&#36753;&#25512;&#29702;&#30340;NLP&#24212;&#29992;&#20013;&#38754;&#20020;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.16426</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#8220;&#32477;&#23545;&#8221;&#19982;&#8220;&#30456;&#23545;&#8221;&#21103;&#35789;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained language models. (arXiv:2305.16426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#26631;&#37327;&#21103;&#35789;&#65292;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#8220;&#32477;&#23545;&#8221;&#19982;&#8220;&#30456;&#23545;&#8221;&#35789;&#30340;&#34920;&#29616;&#65292;&#22312;&#28041;&#21450;&#36923;&#36753;&#25512;&#29702;&#30340;NLP&#24212;&#29992;&#20013;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#20041;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#20551;&#35774;&#20986;&#29616;&#22312;&#30456;&#20284;&#35821;&#22659;&#20013;&#30340;&#35789;&#20855;&#26377;&#30456;&#20284;&#30340;&#21547;&#20041;&#12290;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#31867;&#20284;&#20110;&#20027;&#39064;&#20851;&#32852;&#20294;&#22312;&#36923;&#36753;&#21147;&#24230;&#19978;&#19981;&#21516;&#30340;&#35789;&#24448;&#24448;&#34987;&#35270;&#20026;&#35821;&#20041;&#19978;&#30456;&#20284;&#65292;&#36825;&#23545;&#28041;&#21450;&#36923;&#36753;&#25512;&#29702;&#30340;NLP&#24212;&#29992;&#36896;&#25104;&#20102;&#26222;&#36941;&#25361;&#25112;&#12290;&#20851;&#20110;&#29616;&#20195;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#12289;RoBERTa&#21644;GPT-3&#65289;&#22312;&#36923;&#36753;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#30340;&#25253;&#21578;&#23384;&#22312;&#28151;&#26434;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#26631;&#37327;&#21103;&#35789;&#65292;&#36825;&#26159;&#19968;&#31867;&#20855;&#26377;&#24378;&#28872;&#36923;&#36753;&#21147;&#24230;&#30340;&#35789;&#27719;&#65292;&#25512;&#36827;&#20102;&#36825;&#19968;&#35752;&#35770;&#12290;&#36890;&#36807;&#20351;&#29992;&#19977;&#39033;&#19981;&#21516;&#30340;&#20219;&#21153;&#65288;&#21253;&#25324;&#33258;&#28982;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;&#26500;&#36896;&#30340;&#31034;&#20363;&#65289;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;BERT&#12289;RoBERTa&#12289;GPT-2&#21644;GPT-3&#22312;&#36825;&#20123;&#24120;&#35265;&#35789;&#35821;&#26041;&#38754;&#26159;&#21542;&#23637;&#29616;&#20986;&#19968;&#33324;&#30340;&#12289;&#31867;&#20154;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#38382;&#65306;1&#65289;&#36825;&#20123;&#27169;&#22411;&#33021;&#21542;&#21306;&#20998;&#36825;&#19977;&#20010;&#35821;&#20041;&#31867;&#22411;&#20013;&#30340;&#24046;&#24322;&#65311;
&lt;/p&gt;
&lt;p&gt;
Vector space models of word meaning all share the assumption that words occurring in similar contexts have similar meanings. In such models, words that are similar in their topical associations but differ in their logical force tend to emerge as semantically close, creating well-known challenges for NLP applications that involve logical reasoning. Modern pretrained language models, such as BERT, RoBERTa and GPT-3 hold the promise of performing better on logical tasks than classic static word embeddings. However, reports are mixed about their success. In the current paper, we advance this discussion through a systematic study of scalar adverbs, an under-explored class of words with strong logical force. Using three different tasks, involving both naturalistic social media data and constructed examples, we investigate the extent to which BERT, RoBERTa, GPT-2 and GPT-3 exhibit general, human-like, knowledge of these common words. We ask: 1) Do the models distinguish amongst the three sema
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29305;&#24322;&#24615;&#21644;&#24773;&#24863;&#20004;&#20010;&#35821;&#29992;&#29305;&#24449;&#22312;&#19981;&#21516;&#32676;&#20307;&#32972;&#26223;&#19979;&#26159;&#21542;&#20250;&#20986;&#29616;&#31995;&#32479;&#24615;&#21464;&#21270;&#65292;&#24182;&#23558;&#20854;&#19982;&#33258;&#28982;&#35821;&#35328;&#36755;&#20986;&#20013;&#30340;&#26032;&#32676;&#20307;&#20559;&#35265;&#26694;&#26550;&#30456;&#32852;&#31995;&#12290;&#21021;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#25512;&#25991;&#30340;&#29305;&#24322;&#24615;&#21644;&#24773;&#24863;&#31243;&#24230;&#19982;&#30417;&#30563;&#30340;&#32676;&#20307;&#20851;&#31995;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#19968;&#23450;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#22240;&#26524;&#25512;&#26029;&#25581;&#31034;&#20102;&#31070;&#32463;&#27169;&#22411;&#22312;&#20998;&#31867;&#26102;&#21487;&#38752;&#22320;&#20351;&#29992;&#24773;&#24863;&#65292;&#20294;&#20854;&#23545;&#29305;&#24322;&#24615;&#30340;&#20351;&#29992;&#26159;&#19981;&#30830;&#23450;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.16409</link><description>&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#30740;&#31350;&#29305;&#24322;&#24615;&#21644;&#24773;&#24863;&#23545;&#32676;&#20307;&#20559;&#35265;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Probing for the influence of affect and specificity on Intergroup Bias. (arXiv:2305.16409v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29305;&#24322;&#24615;&#21644;&#24773;&#24863;&#20004;&#20010;&#35821;&#29992;&#29305;&#24449;&#22312;&#19981;&#21516;&#32676;&#20307;&#32972;&#26223;&#19979;&#26159;&#21542;&#20250;&#20986;&#29616;&#31995;&#32479;&#24615;&#21464;&#21270;&#65292;&#24182;&#23558;&#20854;&#19982;&#33258;&#28982;&#35821;&#35328;&#36755;&#20986;&#20013;&#30340;&#26032;&#32676;&#20307;&#20559;&#35265;&#26694;&#26550;&#30456;&#32852;&#31995;&#12290;&#21021;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#25512;&#25991;&#30340;&#29305;&#24322;&#24615;&#21644;&#24773;&#24863;&#31243;&#24230;&#19982;&#30417;&#30563;&#30340;&#32676;&#20307;&#20851;&#31995;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#19968;&#23450;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#22240;&#26524;&#25512;&#26029;&#25581;&#31034;&#20102;&#31070;&#32463;&#27169;&#22411;&#22312;&#20998;&#31867;&#26102;&#21487;&#38752;&#22320;&#20351;&#29992;&#24773;&#24863;&#65292;&#20294;&#20854;&#23545;&#29305;&#24322;&#24615;&#30340;&#20351;&#29992;&#26159;&#19981;&#30830;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#30740;&#31350;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#36890;&#24120;&#38598;&#20013;&#22312;&#36127;&#38754;&#25110;&#36140;&#25439;&#35821;&#35328;&#30340;&#20351;&#29992;&#19978;&#65292;&#20294;Govindarajan&#31561;&#20154;&#65288;2023&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20559;&#35265;&#26694;&#26550;&#65292;&#21363;&#20197;&#32676;&#20307;&#31038;&#20250;&#32972;&#26223;&#20026;&#22522;&#30784;&#65292;&#30740;&#31350;&#20854;&#23545;&#35821;&#35328;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#20010;&#35821;&#29992;&#29305;&#24449;&#65288;&#29305;&#24322;&#24615;&#21644;&#24773;&#24863;&#65289;&#22312;&#19981;&#21516;&#30340;&#32676;&#20307;&#19978;&#19979;&#25991;&#20013;&#26159;&#21542;&#20250;&#31995;&#32479;&#24615;&#22320;&#21464;&#21270;&#65292;&#20174;&#32780;&#23558;&#36825;&#20010;&#26032;&#30340;&#20559;&#35265;&#26694;&#26550;&#19982;&#35821;&#35328;&#36755;&#20986;&#36830;&#25509;&#36215;&#26469;&#12290;&#21021;&#27493;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#25512;&#25991;&#30340;&#29305;&#24322;&#24615;&#21644;&#24773;&#24863;&#31243;&#24230;&#19982;&#30417;&#30563;&#30340;&#32676;&#20307;&#20851;&#31995;&#65288;IGR&#65289;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#36866;&#24230;&#30340;&#30456;&#20851;&#24615;&#12290;&#22240;&#26524;&#25512;&#26029;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#65292;&#34429;&#28982;&#34987;&#31934;&#35843;&#20026;IGR&#26631;&#31614;&#39044;&#27979;&#30340;&#31070;&#32463;&#27169;&#22411;&#21487;&#38752;&#22320;&#20351;&#29992;&#24773;&#24863;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#27169;&#22411;&#20351;&#29992;&#29305;&#24322;&#24615;&#26159;&#19981;&#30830;&#23450;&#30340;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#20197;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/venkatasg/intergroup-probing
&lt;/p&gt;
&lt;p&gt;
While existing work on studying bias in NLP focues on negative or pejorative language use, Govindarajan et al. (2023) offer a revised framing of bias in terms of intergroup social context, and its effects on language behavior. In this paper, we investigate if two pragmatic features (specificity and affect) systematically vary in different intergroup contexts -- thus connecting this new framing of bias to language output. Preliminary analysis finds modest correlations between specificity and affect of tweets with supervised intergroup relationship (IGR) labels. Counterfactual probing further reveals that while neural models finetuned for predicting IGR labels reliably use affect in classification, the model's usage of specificity is inconclusive. Code and data can be found at: https://github.com/venkatasg/intergroup-probing
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#21452;&#35821;&#31038;&#21306;&#20013;&#38750;&#24120;&#35268;&#27597;&#35821;&#20070;&#20889;&#30340;&#33050;&#26412;&#26631;&#20934;&#21270;&#38382;&#39064;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;transformer-based&#27169;&#22411;&#65292;&#24182;&#34920;&#26126;&#36825;&#31181;&#26631;&#20934;&#21270;&#20063;&#33021;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16407</link><description>&lt;p&gt;
&#21452;&#35821;&#31038;&#21306;&#38750;&#24120;&#35268;&#35821;&#35328;&#20070;&#20889;&#30340;&#33050;&#26412;&#26631;&#20934;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Script Normalization for Unconventional Writing of Under-Resourced Languages in Bilingual Communities. (arXiv:2305.16407v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#21452;&#35821;&#31038;&#21306;&#20013;&#38750;&#24120;&#35268;&#27597;&#35821;&#20070;&#20889;&#30340;&#33050;&#26412;&#26631;&#20934;&#21270;&#38382;&#39064;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;transformer-based&#27169;&#22411;&#65292;&#24182;&#34920;&#26126;&#36825;&#31181;&#26631;&#20934;&#21270;&#20063;&#33021;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#24191;&#27867;&#26222;&#21450;&#32473;&#35821;&#35328;&#20195;&#34920;&#24615;&#19981;&#20805;&#20998;&#30340;&#31038;&#21306;&#25552;&#20379;&#20102;&#20197;&#27597;&#35821;&#21019;&#20316;&#20869;&#23481;&#30340;&#21331;&#36234;&#26426;&#20250;&#65292;&#28982;&#32780;&#65292;&#22914;&#26524;&#21452;&#35821;&#31038;&#21306;&#30340;&#35821;&#35328;&#20351;&#29992;&#32773;&#20381;&#36182;&#20854;&#20182;&#20070;&#20889;&#35813;&#27597;&#35821;&#30340;&#25991;&#23383;&#25110;&#27491;&#23383;&#27861;&#26469;&#20070;&#20889;&#65292;&#37027;&#20040;&#23601;&#20250;&#28041;&#21450;&#21040;&#19968;&#20123;&#33050;&#26412;&#26631;&#20934;&#21270;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#23545;&#20027;&#35201;&#20351;&#29992;&#27874;&#26031; - &#38463;&#25289;&#20271;&#25991;&#23383;&#20070;&#20889;&#30340;&#20960;&#31181;&#35821;&#35328;&#36827;&#34892;&#33050;&#26412;&#26631;&#20934;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#20102;&#23569;&#37327;&#30495;&#23454;&#25968;&#25454;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#33050;&#26412;&#26631;&#20934;&#21270;&#20063;&#26377;&#21161;&#20110;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#35328;&#35782;&#21035;&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The wide accessibility of social media has provided linguistically under-represented communities with an extraordinary opportunity to create content in their native languages. This, however, comes with certain challenges in script normalization, particularly where the speakers of a language in a bilingual community rely on another script or orthography to write their native language. This paper addresses the problem of script normalization for several such languages that are mainly written in a Perso-Arabic script. Using synthetic data with various levels of noise and a transformer-based model, we demonstrate that the problem can be effectively remediated. We conduct a small-scale evaluation of real data as well. Our experiments indicate that script normalization is also beneficial to improve the performance of downstream tasks such as machine translation and language identification.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24773;&#22659;&#24863;&#30693;&#27880;&#24847;&#21147;&#23618;&#21450;&#26368;&#20248;&#20256;&#36755;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#35821;&#38899;&#35782;&#21035;&#30196;&#21574;&#30151;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#25429;&#25417;&#20102;&#27169;&#24577;&#20869;&#37096;&#21644;&#27169;&#24577;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#23454;&#29616;&#20102;&#27169;&#22411;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2305.16406</link><description>&lt;p&gt;
&#22522;&#20110;&#24773;&#22659;&#24863;&#30693;&#27880;&#24847;&#21147;&#23618;&#21450;&#26368;&#20248;&#20256;&#36755;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#35782;&#21035;&#33258;&#21457;&#35821;&#38899;&#20013;&#30340;&#30196;&#21574;&#30151;
&lt;/p&gt;
&lt;p&gt;
Context-Aware Attention Layers coupled with Optimal Transport Domain Adaptation methods for recognizing dementia from spontaneous speech. (arXiv:2305.16406v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24773;&#22659;&#24863;&#30693;&#27880;&#24847;&#21147;&#23618;&#21450;&#26368;&#20248;&#20256;&#36755;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#35821;&#38899;&#35782;&#21035;&#30196;&#21574;&#30151;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#25429;&#25417;&#20102;&#27169;&#24577;&#20869;&#37096;&#21644;&#27169;&#24577;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#23454;&#29616;&#20102;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#19968;&#31181;&#22797;&#26434;&#30340;&#31070;&#32463;&#35748;&#30693;&#30149;&#21464;&#65292;&#20063;&#26159;&#23548;&#33268;&#30196;&#21574;&#30151;&#26368;&#24120;&#35265;&#30340;&#21407;&#22240;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#38024;&#23545;&#36890;&#36807;&#33258;&#21457;&#35821;&#38899;&#35786;&#26029;&#30196;&#21574;&#30151;&#30340;&#30740;&#31350;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#12290;&#29616;&#26377;&#30340;&#20808;&#36827;&#26041;&#27861;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#20998;&#21035;&#35757;&#32451;&#35821;&#35328;&#21644;&#22768;&#23398;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#22810;&#25968;&#25237;&#31080;&#26041;&#27861;&#65292;&#20197;&#21450;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#22312;&#36755;&#20837;&#23618;&#36827;&#34892;&#32423;&#32852;&#25110;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#32423;&#32852;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19968;&#20123;&#26041;&#27861;&#37319;&#29992;&#20102;&#33258;&#25105;&#27880;&#24847;&#21147;&#23618;&#65292;&#35745;&#31639;&#34920;&#31034;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20294;&#27809;&#26377;&#32771;&#34385;&#21040;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#37117;&#27809;&#26377;&#32771;&#34385;&#21040;&#27169;&#22411;&#30340;&#26657;&#20934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;AD&#24739;&#32773;&#65292;&#25429;&#25417;&#20102;&#27169;&#24577;&#20869;&#37096;&#21644;&#27169;&#24577;&#38388;&#30340;&#20132;&#20114;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#38899;&#39057;&#25991;&#20214;&#36716;&#25442;&#20026;log-Mel&#20809;&#35889;&#22270;&#65292;&#23427;&#20204;&#30340;delta&#21644;delta-delta&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease (AD) constitutes a complex neurocognitive disease and is the main cause of dementia. Although many studies have been proposed targeting at diagnosing dementia through spontaneous speech, there are still limitations. Existing state-of-the-art approaches, which propose multimodal methods, train separately language and acoustic models, employ majority-vote approaches, and concatenate the representations of the different modalities either at the input level, i.e., early fusion, or during training. Also, some of them employ self-attention layers, which calculate the dependencies between representations without considering the contextual information. In addition, no prior work has taken into consideration the model calibration. To address these limitations, we propose some new methods for detecting AD patients, which capture the intraand cross-modal interactions. First, we convert the audio files into log-Mel spectrograms, their delta, and delta-delta and create in this
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25193;&#25955;-&#35821;&#35328;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#36716;&#25442;&#21644;&#35780;&#20272;&#65292;&#20171;&#32461;&#20102;&#29983;&#25104;-&#37492;&#21035;&#35780;&#20272;&#22522;&#20934;(GDBench)&#22522;&#20110;7&#20010;&#35270;&#35273;&#35821;&#35328;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#22312;&#32452;&#21512;&#24615;&#20219;&#21153;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;CLIP&#65292;&#36890;&#36807;&#24494;&#35843;&#21487;&#25552;&#39640;&#20854;&#32452;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16397</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#26159;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#22120;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Diffusion Models Vision-And-Language Reasoners?. (arXiv:2305.16397v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25193;&#25955;-&#35821;&#35328;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#36716;&#25442;&#21644;&#35780;&#20272;&#65292;&#20171;&#32461;&#20102;&#29983;&#25104;-&#37492;&#21035;&#35780;&#20272;&#22522;&#20934;(GDBench)&#22522;&#20110;7&#20010;&#35270;&#35273;&#35821;&#35328;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#22312;&#32452;&#21512;&#24615;&#20219;&#21153;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;CLIP&#65292;&#36890;&#36807;&#24494;&#35843;&#21487;&#25552;&#39640;&#20854;&#32452;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24050;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#23450;&#24615;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#19982;&#37492;&#21035;&#24335;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#19981;&#21516;&#65292;&#23558;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#32622;&#20110;&#33258;&#21160;&#32454;&#31890;&#24230;&#23450;&#37327;&#35780;&#20272;&#39640;&#32423;&#29616;&#35937;&#65288;&#22914;&#32452;&#21512;&#24615;&#65289;&#30340;&#20219;&#21153;&#20013;&#26159;&#19968;&#39033;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#23637;&#20102;&#20004;&#39033;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;DiffusionITM&#30340;&#26032;&#26041;&#27861;&#23558;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65288;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#26159;&#31283;&#23450;&#25193;&#25955;&#65289;&#36716;&#25442;&#20026;&#20219;&#20309;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;(ITM)&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;7&#20010;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#12289;&#20559;&#24046;&#35780;&#20272;&#21644;&#35814;&#32454;&#20998;&#26512;&#30340;&#29983;&#25104;-&#37492;&#21035;&#35780;&#20272;&#22522;&#20934;(GDBench)&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Stable Diffusion + DiffusionITM&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#32452;&#21512;&#24615;&#20219;&#21153;&#65288;&#22914;CLEVR&#21644;Winoground&#31561;&#65289;&#19978;&#20248;&#20110;CLIP&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MS-COCO&#19978;&#24494;&#35843;&#20445;&#25345;&#22270;&#20687;&#29305;&#24449;&#30340;&#36716;&#31227;&#35774;&#32622;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#32452;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-conditioned image generation models have recently shown immense qualitative success using denoising diffusion processes. However, unlike discriminative vision-and-language models, it is a non-trivial task to subject these diffusion-based generative models to automatic fine-grained quantitative evaluation of high-level phenomena such as compositionality. Towards this goal, we perform two innovations. First, we transform diffusion-based models (in our case, Stable Diffusion) for any image-text matching (ITM) task using a novel method called DiffusionITM. Second, we introduce the Generative-Discriminative Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language tasks, bias evaluation and detailed analysis. We find that Stable Diffusion + DiffusionITM is competitive on many tasks and outperforms CLIP on compositional tasks like like CLEVR and Winoground. We further boost its compositional performance with a transfer setup by fine-tuning on MS-COCO while retaining ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.16380</link><description>&lt;p&gt;
&#25195;&#25551;&#19982;&#25293;&#29031;&#65306;&#29702;&#35299;1&#23618;Transformer&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26631;&#35760;&#32452;&#25104;
&lt;/p&gt;
&lt;p&gt;
Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#22312;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20854;&#22914;&#20309;&#24037;&#20316;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#39044;&#27979;&#24615;&#25439;&#22833;&#65292;&#34920;&#31034;&#22914;&#20309;&#20174;&#26799;&#24230;&#35757;&#32451;&#21160;&#24577;&#20013;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#38024;&#23545;&#20855;&#26377;&#19968;&#20010;&#33258;&#25105;&#20851;&#27880;&#23618;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;1&#23618;Transformer&#65292;&#25105;&#20204;&#20197;&#25968;&#23398;&#20005;&#35880;&#30340;&#26041;&#24335;&#20998;&#26512;&#20854;&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#25171;&#24320;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#32452;&#21512;&#36755;&#20837;&#26631;&#35760;&#30340;&#21160;&#24577;&#36807;&#31243;&#30340;&#40657;&#30418;&#23376;&#65292;&#24182;&#25581;&#31034;&#20102;&#24213;&#23618;&#24402;&#32435;&#20559;&#24046;&#30340;&#26412;&#36136;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#12289;&#38271;&#36755;&#20837;&#24207;&#21015;&#21644;&#35299;&#30721;&#22120;&#23618;&#23398;&#20064;&#36895;&#24230;&#24555;&#20110;&#33258;&#25105;&#20851;&#27880;&#23618;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65306;&#20174;&#22343;&#21248;&#27880;&#24847;&#21147;&#24320;&#22987;&#65292;&#23427;&#36880;&#28176;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#65292;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#30452;&#21040;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#34987;&#25195;&#25551;&#24182;&#24635;&#32467;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#26174;&#31034;&#20102;&#26631;&#35760;&#39057;&#29575;&#21644;&#19978;&#19979;&#25991;&#22914;&#20309;&#24433;&#21709;&#27880;&#24847;&#26435;&#37325;&#65292;&#20197;&#21450;&#33258;&#25105;&#20851;&#27880;&#23618;&#21021;&#22987;&#21270;&#22914;&#20309;&#24433;&#21709;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#23545;&#35805;&#20195;&#29702;&#34892;&#20026;&#25551;&#36848;&#20026;&#35282;&#33394;&#25198;&#28436;&#65292;&#20197;&#36991;&#20813;&#36171;&#20104;&#20854;&#20154;&#31867;&#29305;&#24449;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#30740;&#31350;&#20195;&#29702;&#34892;&#20026;&#20013;&#30340;&#27450;&#39575;&#21644;&#33258;&#25105;&#24847;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.16367</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#35282;&#33394;&#25198;&#28436;
&lt;/p&gt;
&lt;p&gt;
Role-Play with Large Language Models. (arXiv:2305.16367v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#23545;&#35805;&#20195;&#29702;&#34892;&#20026;&#25551;&#36848;&#20026;&#35282;&#33394;&#25198;&#28436;&#65292;&#20197;&#36991;&#20813;&#36171;&#20104;&#20854;&#20154;&#31867;&#29305;&#24449;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#30740;&#31350;&#20195;&#29702;&#34892;&#20026;&#20013;&#30340;&#27450;&#39575;&#21644;&#33258;&#25105;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#35805;&#20195;&#29702;&#31243;&#24207;&#22312;&#34920;&#29616;&#19978;&#36234;&#26469;&#36234;&#25509;&#36817;&#20154;&#31867;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#26377;&#25928;&#30340;&#26041;&#24335;&#39640;&#23618;&#27425;&#25551;&#36848;&#20854;&#34892;&#20026;&#65292;&#32780;&#19981;&#20250;&#38519;&#20837;&#36171;&#20104;&#20854;&#20154;&#31867;&#29305;&#24449;&#30340;&#38519;&#38449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35282;&#33394;&#25198;&#28436;&#30340;&#27010;&#24565;&#65292;&#23558;&#23545;&#35805;&#20195;&#29702;&#31243;&#24207;&#30340;&#34892;&#20026;&#35270;&#20026;&#35282;&#33394;&#25198;&#28436;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20511;&#37492;&#29087;&#24713;&#30340;&#27665;&#38388;&#24515;&#29702;&#23398;&#26415;&#35821;&#65292;&#32780;&#19981;&#26159;&#36171;&#20104;&#23427;&#20204;&#23454;&#38469;&#19978;&#24182;&#19981;&#20855;&#22791;&#30340;&#20154;&#31867;&#29305;&#24449;&#12290;&#26412;&#25991;&#20197;(&#34920;&#38754;&#19978;&#30340;)&#27450;&#39575;&#21644;(&#34920;&#38754;&#19978;&#30340;)&#33258;&#25105;&#24847;&#35782;&#20026;&#20363;&#65292;&#25506;&#35752;&#20102;&#23545;&#35805;&#20195;&#29702;&#31243;&#24207;&#34892;&#20026;&#30340;&#20004;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
As dialogue agents become increasingly human-like in their performance, it is imperative that we develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. In this paper, we foreground the concept of role-play. Casting dialogue agent behaviour in terms of role-play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models they in fact lack. Two important cases of dialogue agent behaviour are addressed this way, namely (apparent) deception and (apparent) self-awareness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#28436;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#23398;&#20064;&#26041;&#27861;&#19982;&#25193;&#25955;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#28436;&#31034;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#24182;&#25552;&#39640;LLMs&#22312;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.16366</link><description>&lt;p&gt;
&#20943;&#23569;&#35868;&#22242;&#65306;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#28436;&#31034;&#23398;&#20064;&#22312;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving. (arXiv:2305.16366v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#28436;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#23398;&#20064;&#26041;&#27861;&#19982;&#25193;&#25955;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#28436;&#31034;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#24182;&#25552;&#39640;LLMs&#22312;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23436;&#20840;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#28436;&#31034;&#26684;&#24335;&#21644;&#32452;&#32455;&#26041;&#38754;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#22686;&#24378;LLMs&#30340;&#25928;&#33021;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#28436;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20803;&#32032;&#65306;&#31532;&#19968;&#65292;&#20174;&#24378;&#21270;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#23376;&#30446;&#26631;&#23398;&#20064;&#20013;&#27762;&#21462;&#32463;&#39564;&#65292;&#20026;&#27599;&#20010;&#28436;&#31034;&#31034;&#20363;&#26500;&#24314;&#19981;&#21516;&#30340;&#23376;&#30446;&#26631;&#65292;&#24182;&#26681;&#25454;&#30456;&#20851;&#30340;&#23376;&#30446;&#26631;&#23398;&#20064;&#29702;&#35770;&#26469;&#20248;&#21270;&#36825;&#20123;&#23376;&#30446;&#26631;&#12290;&#31532;&#20108;&#65292;&#21033;&#29992;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#26469;&#39044;&#27979;&#26368;&#20339;&#32452;&#32455;&#26041;&#24335;&#65292;&#21516;&#26102;&#35299;&#20915;&#28436;&#31034;&#32452;&#32455;&#39046;&#22495;&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#22797;&#26434;&#38382;&#39064;&#65306;&#23376;&#38598;&#36873;&#25321;&#21644;&#39034;&#24207;&#30830;&#23450;&#12290;&#36890;&#36807;&#23558;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#23398;&#20064;&#26041;&#27861;&#19982;&#25193;&#25955;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20316;&#32773;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#28436;&#31034;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#24182;&#25552;&#39640;LLMs&#22312;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models~(LLMs) present an intriguing avenue of exploration in the domain of formal theorem proving. Nonetheless, the full utilization of these models, particularly in terms of demonstration formatting and organization, remains an underexplored area. In an endeavor to enhance the efficacy of LLMs, we introduce a subgoal-based demonstration learning framework, consisting of two primary elements: Firstly, drawing upon the insights of subgoal learning from the domains of reinforcement learning and robotics, we propose the construction of distinct subgoals for each demonstration example and refine these subgoals in accordance with the pertinent theories of subgoal learning. Secondly, we build upon recent advances in diffusion models to predict the optimal organization, simultaneously addressing two intricate issues that persist within the domain of demonstration organization: subset selection and order determination. Through the integration of subgoal-based learning methodolog
&lt;/p&gt;</description></item><item><title>EDM3&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#20107;&#20214;&#26816;&#27979;&#21450;&#20854;&#23376;&#20219;&#21153;&#65292;&#20943;&#23569;&#20102;&#35823;&#24046;&#20256;&#25773;&#12290;&#19982;&#20808;&#21069;&#22522;&#20110;&#25968;&#25454;&#38598;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;EDM3&#21033;&#29992;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#20107;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.16357</link><description>&lt;p&gt;
EDM3&#65306;&#20107;&#20214;&#26816;&#27979;&#20316;&#20026;&#22810;&#20219;&#21153;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
EDM3: Event Detection as Multi-task Text Generation. (arXiv:2305.16357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16357
&lt;/p&gt;
&lt;p&gt;
EDM3&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#20107;&#20214;&#26816;&#27979;&#21450;&#20854;&#23376;&#20219;&#21153;&#65292;&#20943;&#23569;&#20102;&#35823;&#24046;&#20256;&#25773;&#12290;&#19982;&#20808;&#21069;&#22522;&#20110;&#25968;&#25454;&#38598;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;EDM3&#21033;&#29992;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#20107;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#26816;&#27979;&#25351;&#30340;&#26159;&#22312;&#25991;&#26412;&#20013;&#35782;&#21035;&#20107;&#20214;&#20986;&#29616;&#24182;&#21253;&#25324;&#20004;&#20010;&#23376;&#20219;&#21153;&#65307;&#20107;&#20214;&#35782;&#21035;&#21644;&#20107;&#20214;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EDM3&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#26500;&#24314;&#20102;&#19977;&#20010;&#29983;&#25104;&#24335;&#20219;&#21153;&#65306;&#35782;&#21035;&#12289;&#20998;&#31867;&#21644;&#32852;&#21512;&#26816;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;EDM3&#33021;&#22815;&#24110;&#21161;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#30693;&#35782;&#65292;&#20351;&#20854;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#20107;&#20214;&#26816;&#27979;&#21450;&#20854;&#23376;&#20219;&#21153;&#65292;&#20943;&#23569;&#20102;&#27969;&#27700;&#32447;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#35823;&#24046;&#20256;&#25773;&#12290;&#19982;&#20808;&#21069;&#22522;&#20110;&#25968;&#25454;&#38598;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;EDM3&#21033;&#29992;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#29616;&#26377;&#30693;&#35782;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#20219;&#20309;&#20998;&#31867;&#27169;&#24335;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20107;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#23545;EDM3&#36827;&#34892;&#20102;&#35780;&#20272;&#65306;RAMS&#12289;WikiEvents&#12289;MAVEN&#21644;MLEE&#65292;&#34920;&#26126;EDM3&#30340;&#24179;&#22343;&#21333;&#20219;&#21153;&#24615;&#33021;&#20248;&#20110;8.4&#65285;&#65292;&#24179;&#22343;&#26080;&#38656;&#25351;&#31034;&#24615;&#25552;&#31034;&#30340;&#22810;&#20219;&#21153;&#24615;&#33021;&#20248;&#20110;2.4&#65285;&#12290;&#25105;&#20204;&#22312;RAMS&#19978;&#33719;&#24471;&#20102;SOTA&#30340;&#32467;&#26524;&#65288;71.3&#65285;&#19982;65.1&#65285;&#30340;F-1&#65289;&#65292;&#22312;&#20854;&#20182;&#25968;&#25454;&#38598;&#19978;&#20063;&#34920;&#29616;&#20986;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;.
&lt;/p&gt;
&lt;p&gt;
Event detection refers to identifying event occurrences in a text and comprises of two subtasks; event identification and classification. We present EDM3, a novel approach for Event Detection that formulates three generative tasks: identification, classification, and combined detection. We show that EDM3 helps to learn transferable knowledge that can be leveraged to perform Event Detection and its subtasks concurrently, mitigating the error propagation inherent in pipelined approaches. Unlike previous dataset- or domain-specific approaches, EDM3 utilizes the existing knowledge of language models, allowing it to be trained over any classification schema. We evaluate EDM3 on multiple event detection datasets: RAMS, WikiEvents, MAVEN, and MLEE, showing that EDM3 outperforms 1) single-task performance by 8.4% on average and 2) multi-task performance without instructional prompts by 2.4% on average. We obtain SOTA results on RAMS (71.3% vs. 65.1% F-1) and competitive performance on other da
&lt;/p&gt;</description></item><item><title>PandaGPT&#26159;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#25509;&#21463;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#24182;&#29992;&#20110;&#29983;&#25104;&#22797;&#26434;&#20219;&#21153;&#36755;&#20986;&#30340;&#21487;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16355</link><description>&lt;p&gt;
PandaGPT: &#19968;&#31181;&#33021;&#22815;&#25191;&#34892;&#22270;&#20687;&#12289;&#38899;&#39057;&#25351;&#20196;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PandaGPT: One Model To Instruction-Follow Them All. (arXiv:2305.16355v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16355
&lt;/p&gt;
&lt;p&gt;
PandaGPT&#26159;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#25509;&#21463;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#24182;&#29992;&#20110;&#29983;&#25104;&#22797;&#26434;&#20219;&#21153;&#36755;&#20986;&#30340;&#21487;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PandaGPT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#40784;&#22270;&#20687;-&#25991;&#26412;&#23545;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;ImageBind&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;Vicuna&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#20854;&#20855;&#26377;&#35270;&#35273;&#21644;&#21548;&#35273;&#25351;&#20196;&#36319;&#36394;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38500;&#20102;&#21487;&#20197;&#29983;&#25104;&#35814;&#32454;&#30340;&#22270;&#20687;&#25551;&#36848;&#12289;&#20889;&#25925;&#20107;&#12289;&#36824;&#33021;&#22815;&#33258;&#28982;&#22320;&#32452;&#21512;&#22810;&#20010;&#25968;&#25454;&#30340;&#35821;&#20041;&#65292;&#20197;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present PandaGPT, an approach to emPower large lANguage moDels with visual and Auditory instruction-following capabilities. Our pilot experiments show that PandaGPT can perform complex tasks such as detailed image description generation, writing stories inspired by videos, and answering questions about audios. More interestingly, PandaGPT can take multimodal inputs simultaneously and compose their semantics naturally. For example, PandaGPT can connect how objects look in an image/video and how they sound in an audio. To do so, PandaGPT combines the multimodal encoders from ImageBind and the large language models from Vicuna. Notably, only aligned image-text pairs are required for the training of PandaGPT. Thanks to the strong capability of ImageBind in embedding data from different modalities into the same space, PandaGPT displays emergent, i.e. zero-shot, cross-modal behaviors for data other than image and text (e.g., video, audio, depth, thermal, and IMU). We hope that PandaGPT se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;M2S-ADD&#30340;&#26032;&#22411;ADD&#27169;&#22411;&#65292;&#36890;&#36807;&#21333;&#22768;&#36947;&#36716;&#31435;&#20307;&#22768;&#25216;&#26415;&#21457;&#29616;&#20266;&#36896;&#38899;&#39057;&#20013;&#30340;&#30495;&#23454;&#24615;&#32447;&#32034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16353</link><description>&lt;p&gt;
&#33258;&#25105;&#32972;&#21467;&#65306;&#21033;&#29992;&#21333;&#22768;&#36947;&#36716;&#31435;&#20307;&#22768;&#25216;&#26415;&#36827;&#34892;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Betray Oneself: A Novel Audio DeepFake Detection Model via Mono-to-Stereo Conversion. (arXiv:2305.16353v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;M2S-ADD&#30340;&#26032;&#22411;ADD&#27169;&#22411;&#65292;&#36890;&#36807;&#21333;&#22768;&#36947;&#36716;&#31435;&#20307;&#22768;&#25216;&#26415;&#21457;&#29616;&#20266;&#36896;&#38899;&#39057;&#20013;&#30340;&#30495;&#23454;&#24615;&#32447;&#32034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;(Audio Deepfake Detection, ADD)&#26088;&#22312;&#26816;&#27979;&#30001;&#25991;&#26412;&#36716;&#35821;&#38899;(TTS)&#12289;&#35821;&#38899;&#36716;&#25442;(VC)&#12289;&#37325;&#25918;&#31561;&#29983;&#25104;&#30340;&#34394;&#20551;&#38899;&#39057;&#65292;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20256;&#32479;&#30340;&#30740;&#31350;&#26041;&#27861;&#23558;&#21333;&#22768;&#36947;&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#37325;&#28857;&#22312;&#20110;&#31283;&#20581;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#26377;&#25928;&#30340;&#20998;&#31867;&#22120;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#38899;&#39057;&#20449;&#21495;&#20013;&#30340;&#21452;&#36890;&#36947;&#31435;&#20307;&#22768;&#20449;&#24687;&#20063;&#21253;&#21547;&#20102;&#28145;&#24230;&#20266;&#36896;&#30340;&#37325;&#35201;&#32447;&#32034;&#65292;&#36825;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ADD&#27169;&#22411;&#65292;&#31216;&#20026;M2S-ADD&#65292;&#26088;&#22312;&#22312;&#21333;&#22768;&#36947;&#36716;&#31435;&#20307;&#22768;&#36807;&#31243;&#20013;&#21457;&#29616;&#38899;&#39057;&#30495;&#23454;&#24615;&#32447;&#32034;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#31435;&#20307;&#22768;&#21512;&#25104;&#22120;&#23558;&#21333;&#22768;&#36947;&#25237;&#24433;&#21040;&#31435;&#20307;&#22768;&#20449;&#21495;&#19978;&#65292;&#28982;&#21518;&#37319;&#29992;&#21452;&#20998;&#25903;&#31070;&#32463;&#26550;&#26500;&#20998;&#21035;&#22788;&#29702;&#24038;&#21491;&#22768;&#36947;&#20449;&#21495;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#25581;&#31034;&#20102;&#20266;&#36896;&#38899;&#39057;&#20013;&#30340;&#20266;&#24433;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;ADD&#30340;&#24615;&#33021;&#12290;&#22312;ASVspoof2019&#25968;&#25454;&#24211;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;M2S-ADD&#22312;&#25152;&#26377;&#36755;&#20837;&#21333;&#22768;&#36947;&#30340;&#22522;&#32447;&#27169;&#22411;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio Deepfake Detection (ADD) aims to detect the fake audio generated by text-to-speech (TTS), voice conversion (VC) and replay, etc., which is an emerging topic. Traditionally we take the mono signal as input and focus on robust feature extraction and effective classifier design. However, the dual-channel stereo information in the audio signal also includes important cues for deepfake, which has not been studied in the prior work. In this paper, we propose a novel ADD model, termed as M2S-ADD, that attempts to discover audio authenticity cues during the mono-to-stereo conversion process. We first projects the mono to a stereo signal using a pretrained stereo synthesizer, then employs a dual-branch neural architecture to process the left and right channel signals, respectively. In this way, we effectively reveal the artifacts in the fake audio, thus improve the ADD performance. The experiments on the ASVspoof2019 database show that M2S-ADD outperforms all baselines that input mono. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#31216;&#20026;Lexinvariant&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#20219;&#20309;&#22266;&#23450;&#26631;&#35760;&#23884;&#20837;&#65292;&#23436;&#20840;&#20381;&#36182;&#19978;&#19979;&#25991;&#20013;&#26631;&#35760;&#30340;&#20849;&#29616;&#21644;&#37325;&#22797;&#12290;&#20316;&#32773;&#35777;&#26126;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;lexinvariant LM&#65292;&#20197;&#22810;&#39033;&#24335;&#26041;&#24335;&#19982;&#19978;&#19979;&#25991;&#38271;&#24230;&#25104;&#27604;&#20363;&#22320;&#25910;&#25947;&#21040;&#30495;&#23454;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#24120;&#37327;&#22240;&#23376;&#22312;&#35789;&#27719;&#34920;&#22823;&#23567;&#19979;&#20026;&#27425;&#32447;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16349</link><description>&lt;p&gt;
Lexinvariant&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lexinvariant Language Models. (arXiv:2305.16349v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#31216;&#20026;Lexinvariant&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#20219;&#20309;&#22266;&#23450;&#26631;&#35760;&#23884;&#20837;&#65292;&#23436;&#20840;&#20381;&#36182;&#19978;&#19979;&#25991;&#20013;&#26631;&#35760;&#30340;&#20849;&#29616;&#21644;&#37325;&#22797;&#12290;&#20316;&#32773;&#35777;&#26126;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;lexinvariant LM&#65292;&#20197;&#22810;&#39033;&#24335;&#26041;&#24335;&#19982;&#19978;&#19979;&#25991;&#38271;&#24230;&#25104;&#27604;&#20363;&#22320;&#25910;&#25947;&#21040;&#30495;&#23454;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#24120;&#37327;&#22240;&#23376;&#22312;&#35789;&#27719;&#34920;&#22823;&#23567;&#19979;&#20026;&#27425;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20196;&#29260;&#23884;&#20837;&#26159;&#20174;&#31163;&#25955;&#35789;&#27719;&#31526;&#21495;&#21040;&#36830;&#32493;&#21521;&#37327;&#30340;&#26144;&#23556;&#65292;&#26159;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#26680;&#24515;&#12290;&#20294;&#26159;&#65292;&#35789;&#27719;&#31526;&#21495;&#30340;&#21547;&#20041;&#20063;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#22312;&#38271;&#19978;&#19979;&#25991;&#20013;&#30340;&#32467;&#26500;&#35282;&#33394;&#26469;&#30830;&#23450;&#29978;&#33267;&#37325;&#26032;&#23450;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38382;&#65306;&#26159;&#21542;&#21487;&#33021;&#23384;&#22312;&#19968;&#31181;&#27809;&#26377;&#20219;&#20309;&#22266;&#23450;&#26631;&#35760;&#23884;&#20837;&#30340;&#24615;&#33021;&#33391;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#65311;&#36825;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#23436;&#20840;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#20013;&#26631;&#35760;&#30340;&#20849;&#29616;&#21644;&#37325;&#22797;&#65292;&#32780;&#19981;&#26159;&#20219;&#20309;&#26631;&#35760;&#30340;\textit{a priori}&#26631;&#35782;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;\textit{lexinvariant}&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#27719;&#31526;&#21495;&#19981;&#21464;&#65292;&#22240;&#27492;&#22312;&#23454;&#36341;&#20013;&#19981;&#38656;&#35201;&#22266;&#23450;&#30340;&#20196;&#29260;&#23884;&#20837;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;lexinvariant LM&#65292;&#20197;&#22810;&#39033;&#24335;&#26041;&#24335;&#19982;&#19978;&#19979;&#25991;&#38271;&#24230;&#25104;&#27604;&#20363;&#22320;&#25910;&#25947;&#21040;&#30495;&#23454;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#24120;&#37327;&#22240;&#23376;&#22312;&#35789;&#27719;&#34920;&#22823;&#23567;&#19979;&#20026;&#27425;&#32447;&#24615;&#12290;&#20854;&#27425;&#65292;&#35201;&#26500;&#24314;&#19968;&#20010;lexinvariant LM&#65292;&#25105;&#20204;&#21482;&#38656;&#20351;&#29992;&#38543;&#26426;&#39640;&#26031;&#20989;&#25968;&#23545;&#26631;&#35760;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Token embeddings, a mapping from discrete lexical symbols to continuous vectors, are at the heart of any language model (LM). However, lexical symbol meanings can also be determined and even redefined by their structural role in a long context. In this paper, we ask: is it possible for a language model to be performant without \emph{any} fixed token embeddings? Such a language model would have to rely entirely on the co-occurence and repetition of tokens in the context rather than the \textit{a priori} identity of any token. To answer this, we study \textit{lexinvariant}language models that are invariant to lexical symbols and therefore do not need fixed token embeddings in practice. First, we prove that we can construct a lexinvariant LM to converge to the true language model at a uniform rate that is polynomial in terms of the context length, with a constant factor that is sublinear in the vocabulary size. Second, to build a lexinvariant LM, we simply encode tokens using random Gauss
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#36130;&#21153;&#20449;&#24687;&#25552;&#21462;&#30340;&#26694;&#26550;&#65288;AFIE&#65289;&#65292;&#29992;&#20110;&#25552;&#21462;&#28151;&#21512;&#38271;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#19994;&#32489;&#25351;&#26631;&#65288;KPI&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;LLMs&#22686;&#24378;&#20102;&#36130;&#21153;&#25253;&#21578;&#20449;&#24687;&#30340;&#29702;&#35299;&#21644;&#25552;&#21462;&#33021;&#21147;&#65292;&#24182;&#32463;&#36807;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20854;&#22312;GPT-3.5&#21644;GPT-4&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;&#26420;&#32032;&#26041;&#27861;&#65292;&#24179;&#22343;&#31934;&#24230;&#25552;&#39640;&#20102;53.94&#65285;&#21644;33.77&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.16344</link><description>&lt;p&gt;
&#21033;&#29992;LLMs&#20174;&#28151;&#21512;&#38271;&#25991;&#26723;&#20013;&#26816;&#32034;KPI&#30340;&#20840;&#38754;&#26694;&#26550;&#19982;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Leveraging LLMs for KPIs Retrieval from Hybrid Long-Document: A Comprehensive Framework and Dataset. (arXiv:2305.16344v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#36130;&#21153;&#20449;&#24687;&#25552;&#21462;&#30340;&#26694;&#26550;&#65288;AFIE&#65289;&#65292;&#29992;&#20110;&#25552;&#21462;&#28151;&#21512;&#38271;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#19994;&#32489;&#25351;&#26631;&#65288;KPI&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;LLMs&#22686;&#24378;&#20102;&#36130;&#21153;&#25253;&#21578;&#20449;&#24687;&#30340;&#29702;&#35299;&#21644;&#25552;&#21462;&#33021;&#21147;&#65292;&#24182;&#32463;&#36807;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20854;&#22312;GPT-3.5&#21644;GPT-4&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;&#26420;&#32032;&#26041;&#27861;&#65292;&#24179;&#22343;&#31934;&#24230;&#25552;&#39640;&#20102;53.94&#65285;&#21644;33.77&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#34920;&#26684;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#23545;&#21253;&#21547;&#25991;&#26412;&#21644;&#34920;&#26684;&#25968;&#25454;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#20173;&#26410;&#34987;&#20805;&#20998;&#21457;&#25496;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#65292;&#20174;&#28151;&#26434;&#30340;&#38271;&#22411;&#36130;&#21153;&#25253;&#21578;&#20013;&#29702;&#35299;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#36130;&#21153;&#20449;&#24687;&#25552;&#21462;&#65288;AFIE&#65289;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;LLMs&#29702;&#35299;&#21644;&#25552;&#21462;&#36130;&#21153;&#25253;&#21578;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;AFIE&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#37329;&#34701;&#25253;&#21578;&#25968;&#20540;&#25552;&#21462;&#65288;FINE&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;GPT-3.5&#21644;GPT-4&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#39564;&#35777;&#65292;&#30456;&#23545;&#20110;&#26420;&#32032;&#26041;&#27861;&#65292;&#24179;&#22343;&#31934;&#24230;&#25552;&#39640;&#20102;53.94&#65285;&#21644;33.77&#65285;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;AFIE&#26694;&#26550;&#20026;&#20174;&#22797;&#26434;&#30340;&#28151;&#21512;&#25991;&#26723;&#20013;&#33258;&#21160;&#25552;&#21462;&#25968;&#20540;&#25552;&#20379;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate exceptional performance in textual understanding and tabular reasoning tasks. However, their ability to comprehend and analyze hybrid text, containing textual and tabular data, remains underexplored. In this research, we specialize in harnessing the potential of LLMs to comprehend critical information from financial reports, which are hybrid long-documents. We propose an Automated Financial Information Extraction (AFIE) framework that enhances LLMs' ability to comprehend and extract information from financial reports. To evaluate AFIE, we develop a Financial Reports Numerical Extraction (FINE) dataset and conduct an extensive experimental analysis. Our framework is effectively validated on GPT-3.5 and GPT-4, yielding average accuracy increases of 53.94% and 33.77%, respectively, compared to a naive method. These results suggest that the AFIE framework offers accuracy for automated numerical extraction from complex, hybrid documents.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Spark&#29983;&#24577;&#31995;&#32479;&#30340;&#20998;&#24067;&#24335;&#26550;&#26500;&#65292;&#21487;&#33258;&#21160;&#25552;&#21462;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#22312;&#26415;&#35821;&#25552;&#21462;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16343</link><description>&lt;p&gt;
&#22522;&#20110;Spark&#29983;&#24577;&#31995;&#32479;&#30340;&#20998;&#24067;&#24335;&#33258;&#21160;&#39046;&#22495;&#29305;&#23450;&#22810;&#35789;&#26415;&#35821;&#35782;&#21035;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Distributed Automatic Domain-Specific Multi-Word Term Recognition Architecture using Spark Ecosystem. (arXiv:2305.16343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Spark&#29983;&#24577;&#31995;&#32479;&#30340;&#20998;&#24067;&#24335;&#26550;&#26500;&#65292;&#21487;&#33258;&#21160;&#25552;&#21462;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#22312;&#26415;&#35821;&#25552;&#21462;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26415;&#35821;&#35782;&#21035;&#29992;&#20110;&#25552;&#21462;&#23646;&#20110;&#32473;&#23450;&#39046;&#22495;&#30340;&#29305;&#23450;&#26415;&#35821;&#12290;&#20026;&#20102;&#20934;&#30830;&#65292;&#36825;&#20123;&#22522;&#20110;&#35821;&#26009;&#24211;&#21644;&#35821;&#35328;&#20381;&#36182;&#30340;&#26041;&#27861;&#38656;&#35201;&#22788;&#29702;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#20197;&#25552;&#21462;&#20505;&#36873;&#26415;&#35821;&#65292;&#28982;&#21518;&#26681;&#25454;&#32473;&#23450;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#35780;&#20998;&#12290;&#20026;&#20102;&#25913;&#36827;&#25991;&#26412;&#39044;&#22788;&#29702;&#21644;&#20505;&#36873;&#26415;&#35821;&#30340;&#25552;&#21462;&#21644;&#35780;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Spark&#29983;&#24577;&#31995;&#32479;&#33258;&#21160;&#25552;&#21462;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#20998;&#24067;&#24335;&#26550;&#26500;&#12290;&#20027;&#35201;&#36129;&#29486;&#22914;&#19979;&#65306;&#65288;1&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#33258;&#21160;&#39046;&#22495;&#29305;&#23450;&#22810;&#35789;&#26415;&#35821;&#35782;&#21035;&#26550;&#26500;&#65292;&#26500;&#24314;&#22312;Spark&#29983;&#24577;&#31995;&#32479;&#20043;&#19978;&#65307;&#65288;2&#65289;&#20174;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23545;&#25105;&#20204;&#30340;&#26550;&#26500;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65307;&#65288;3&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#26131;&#20110;&#38598;&#25104;&#30340;Python&#23454;&#29616;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#20351;&#29992;&#22823;&#25968;&#25454;&#22788;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#19978;&#36827;&#34892;&#23454;&#39564;&#26469;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#26550;&#26500;&#30340;&#21487;&#34892;&#24615;&#65292;&#22312;&#26415;&#35821;&#25552;&#21462;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Term Recognition is used to extract domain-specific terms that belong to a given domain. In order to be accurate, these corpus and language-dependent methods require large volumes of textual data that need to be processed to extract candidate terms that are afterward scored according to a given metric. To improve text preprocessing and candidate terms extraction and scoring, we propose a distributed Spark-based architecture to automatically extract domain-specific terms. The main contributions are as follows: (1) propose a novel distributed automatic domain-specific multi-word term recognition architecture built on top of the Spark ecosystem; (2) perform an in-depth analysis of our architecture in terms of accuracy and scalability; (3) design an easy-to-integrate Python implementation that enables the use of Big Data processing in fields such as Computational Linguistics and Natural Language Processing. We prove empirically the feasibility of our architecture by performing ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InterFormer&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#65292;&#20197;&#23398;&#20064;&#26356;&#22909;&#30340;ASR&#34920;&#31034;&#12290;&#36890;&#36807;&#32452;&#21512;&#21367;&#31215;&#22359;&#21644;&#21464;&#24418;&#22120;&#22359;&#65292;&#20197;&#21450;&#24341;&#20837;BFIM&#21644;SFM&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#20132;&#20114;&#21644;&#34701;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#20844;&#20849;ASR&#25968;&#25454;&#38598;&#19978;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16342</link><description>&lt;p&gt;
InterFormer: &#28151;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#20132;&#20114;&#24335;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition. (arXiv:2305.16342v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InterFormer&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#65292;&#20197;&#23398;&#20064;&#26356;&#22909;&#30340;ASR&#34920;&#31034;&#12290;&#36890;&#36807;&#32452;&#21512;&#21367;&#31215;&#22359;&#21644;&#21464;&#24418;&#22120;&#22359;&#65292;&#20197;&#21450;&#24341;&#20837;BFIM&#21644;SFM&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#20132;&#20114;&#21644;&#34701;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#20844;&#20849;ASR&#25968;&#25454;&#38598;&#19978;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#32780;&#35328;&#65292;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#37117;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#26041;&#27861;&#24050;&#32463;&#35777;&#23454;&#65292;&#31616;&#21333;&#22320;&#21512;&#24182;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;ASR&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#24573;&#30053;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#20018;&#34892;&#26550;&#26500;&#26080;&#27861;&#21453;&#26144;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;InterFormer&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#65292;&#20197;&#23398;&#20064;&#26356;&#22909;&#30340;ASR&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#21367;&#31215;&#22359;&#19982;&#21464;&#24418;&#22120;&#22359;&#20197;&#24182;&#34892;&#35774;&#35745;&#30456;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#21521;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#65288;BFIM&#65289;&#21644;&#36873;&#25321;&#24615;&#34701;&#21512;&#27169;&#22359;&#65288;SFM&#65289;&#26469;&#23454;&#29616;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#20132;&#20114;&#21644;&#34701;&#21512;&#12290;&#22312;&#20844;&#20849;ASR&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;InterFormer&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20854;&#20182;Transformer&#21644;Conformer&#27169;&#22411;&#20855;&#26377;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The local and global features are both essential for automatic speech recognition (ASR). Many recent methods have verified that simply combining local and global features can further promote ASR performance. However, these methods pay less attention to the interaction of local and global features, and their series architectures are rigid to reflect local and global relationships. To address these issues, this paper proposes InterFormer for interactive local and global features fusion to learn a better representation for ASR. Specifically, we combine the convolution block with the transformer block in a parallel design. Besides, we propose a bidirectional feature interaction module (BFIM) and a selective fusion module (SFM) to implement the interaction and fusion of local and global features, respectively. Extensive experiments on public ASR datasets demonstrate the effectiveness of our proposed InterFormer and its superior performance over the other Transformer and Conformer models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;RAF&#23618;&#22788;&#29702;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16340</link><description>&lt;p&gt;
&#20998;&#27573;&#24490;&#29615;Transformer:&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. (arXiv:2305.16340v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;RAF&#23618;&#22788;&#29702;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#35270;&#35273;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#21576;&#20108;&#27425;&#22686;&#38271;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#25104;&#20026;&#19981;&#21487;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23558;&#25972;&#20010;&#24207;&#21015;&#21010;&#20998;&#25104;&#33509;&#24178;&#27573;&#12290;&#28982;&#21518;&#20351;&#29992;&#20855;&#26377;&#24490;&#29615;&#32467;&#26500;&#30340;&#31070;&#32463;&#20803;&#26469;&#32858;&#21512;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20855;&#26377;&#36739;&#20302;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#30340;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#27169;&#22411;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#20351;&#29992;&#23616;&#37096;Attention&#26426;&#21046;&#23545;&#21333;&#20010;&#27573;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#65292;&#23427;&#23558;&#20998;&#27573;Attention&#21644;&#24490;&#29615;Attention&#30456;&#32467;&#21512;&#12290;&#23427;&#20351;&#29992;&#24490;&#29615;accumulate and fire&#65288;RAF&#65289;&#23618;&#22312;&#30456;&#37051;&#27573;&#20043;&#38388;&#22788;&#29702;&#20449;&#24687;&#12290;&#36890;&#36807;&#26356;&#26032;key&#30340;&#20135;&#21697;&#26469;&#34917;&#20607;&#20943;&#23569;Attention&#31383;&#21475;&#38271;&#24230;&#20135;&#29983;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have shown dominant performance across a range of domains including language and vision. However, their computational cost grows quadratically with the sequence length, making their usage prohibitive for resource-constrained applications. To counter this, our approach is to divide the whole sequence into segments. The information across segments can then be aggregated using neurons with recurrence leveraging their inherent memory. Such an approach leads to models with sequential processing capability at a lower computation/memory cost. To investigate this idea, first, we examine the effects of using local attention mechanism on the individual segments. Then we propose a segmented recurrent transformer (SRformer) that combines segmented attention with recurrent attention. It uses recurrent accumulate and fire (RAF) layers to process information between consecutive segments. The loss caused by reducing the attention window length is compensated by updating the product of key
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#65292;GPT-3&#34920;&#29616;&#36739;&#24046;&#65292;&#29305;&#21035;&#26159;&#24403;&#38382;&#39064;&#19981;&#26159;&#29992;&#33521;&#35821;&#25552;&#20986;&#26102;&#12290;&#36825;&#19982;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#38382;&#39064;&#30340;&#35821;&#35328;&#24046;&#24322;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.16339</link><description>&lt;p&gt;
&#24403;&#38382;&#39064;&#19981;&#26159;&#29992;&#33521;&#35821;&#25552;&#20986;&#26102;&#65292;&#19981;&#35201;&#23436;&#20840;&#20449;&#20219;GPT
&lt;/p&gt;
&lt;p&gt;
Don't Trust GPT When Your Question Is Not In English. (arXiv:2305.16339v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16339
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#65292;GPT-3&#34920;&#29616;&#36739;&#24046;&#65292;&#29305;&#21035;&#26159;&#24403;&#38382;&#39064;&#19981;&#26159;&#29992;&#33521;&#35821;&#25552;&#20986;&#26102;&#12290;&#36825;&#19982;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#38382;&#39064;&#30340;&#35821;&#35328;&#24046;&#24322;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;LLMs&#20027;&#35201;&#20351;&#29992;&#33521;&#35821;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#22810;&#39033;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#35768;&#22810;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#30456;&#23545;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#22914;&#20309;&#33719;&#24471;&#23427;&#20204;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#20197;&#21450;&#34920;&#29616;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#30340;&#24046;&#24322;&#20173;&#28982;&#23384;&#22312;&#22522;&#26412;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#23545;LLMs&#30340;&#30740;&#31350;&#38750;&#24120;&#20851;&#38190;&#65292;&#22240;&#20026;&#29992;&#25143;&#21644;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#26469;&#33258;&#22810;&#31181;&#35821;&#35328;&#32972;&#26223;&#65292;&#21487;&#33021;&#24433;&#21709;&#20182;&#20204;&#23545;LLMs&#32467;&#26524;&#30340;&#21033;&#29992;&#21644;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#20197;&#23450;&#24615;&#35780;&#20272;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;LLMs&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;LLMs&#22312;&#36328;&#35821;&#35328;&#27867;&#21270;&#29616;&#35937;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21363;&#19981;&#20805;&#36275;&#30340;&#22810;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#23548;&#33268;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#35821;&#35328;&#36827;&#34892;&#20102;GPT-3&#30340;&#23454;&#39564;&#65292;&#36825;&#20123;&#35821;&#35328;&#28085;&#30422;&#20102;&#20174;&#21360;&#27431;&#35821;&#31995;&#21040;&#38750;&#21360;&#27431;&#35821;&#31995;&#30340;&#21508;&#31181;&#35821;&#35328;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21644;&#39564;&#35777;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#22312;&#35813;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20294;&#22914;&#26524;&#36755;&#20837;&#38382;&#39064;&#19981;&#26159;&#33521;&#35821;&#65292;GPT-3&#22312;&#20854;&#20182;&#35821;&#35328;&#19979;&#30340;&#34920;&#29616;&#26174;&#33879;&#36739;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#19982;&#35757;&#32451;&#35821;&#35328;&#21644;&#36755;&#20837;&#38382;&#39064;&#30340;&#35821;&#35328;&#24046;&#24322;&#26377;&#20851;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36827;&#34892;&#38750;&#33521;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26102;&#65292;&#38656;&#35201;&#35880;&#24910;&#20351;&#29992;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional natural language understanding abilities and have excelled in a variety of natural language processing (NLP)tasks in recent years. Despite the fact that most LLMs are trained predominantly in English, multiple studies have demonstrated their comparative performance in many other languages. However, fundamental questions persist regarding how LLMs acquire their multi-lingual abilities and how performance varies across different languages. These inquiries are crucial for the study of LLMs since users and researchers often come from diverse language backgrounds, potentially influencing their utilization and interpretation of LLMs' results. In this work, we propose a systematic way of qualifying the performance disparities of LLMs under multilingual settings. We investigate the phenomenon of across-language generalizations in LLMs, wherein insufficient multi-lingual training data leads to advanced multi-lingual capabilities. To acc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#30340;&#20915;&#31574;Transformer&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#20195;&#29702;&#22312;&#22788;&#29702;&#26032;&#20219;&#21153;&#19978;&#24615;&#33021;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16338</link><description>&lt;p&gt;
&#28145;&#24605;&#29087;&#34385;&#65306;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#30340;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Think Before You Act: Decision Transformers with Internal Working Memory. (arXiv:2305.16338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#30340;&#20915;&#31574;Transformer&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#20195;&#29702;&#22312;&#22788;&#29702;&#26032;&#20219;&#21153;&#19978;&#24615;&#33021;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#24050;&#32463;&#23637;&#31034;&#20102;&#36328;&#36234;&#22810;&#20010;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#20302;&#25928;&#24615;&#28304;&#20110;&#36951;&#24536;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#36890;&#36807;&#21442;&#25968;&#35760;&#24518;&#20854;&#34892;&#20026;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#22240;&#27492;&#65292;&#26032;&#20219;&#21153;&#30340;&#35757;&#32451;&#21487;&#33021;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#20808;&#21069;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#19982;LLM&#30340;&#38544;&#24335;&#35760;&#24518;&#26426;&#21046;&#19981;&#21516;&#65292;&#20154;&#33041;&#21033;&#29992;&#20998;&#24067;&#24335;&#23384;&#20648;&#22120;&#23384;&#20648;&#35760;&#24518;&#65292;&#20197;&#26377;&#25928;&#22320;&#31649;&#29702;&#21644;&#32452;&#32455;&#22810;&#31181;&#25216;&#33021;&#65292;&#20943;&#36731;&#20102;&#36951;&#24536;&#29616;&#35937;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#26469;&#23384;&#20648;&#12289;&#34701;&#21512;&#21644;&#26816;&#32034;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;Atari&#28216;&#25103;&#21644;&#20803;&#19990;&#30028;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35760;&#24518;&#24494;&#35843;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language model (LLM)-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and compute. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Thus inspired, we propose an internal working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in both Atari games and meta-world object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;BERT&#22312;&#29616;&#23454;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#19979;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21457;&#29616;&#29305;&#24449;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#26469;&#33258;&#27880;&#37322;&#32773;&#20998;&#27495;&#30340;&#21512;&#25104;&#26631;&#31614;&#22122;&#22768;&#20250;&#23548;&#33268;BERT&#30340;&#20998;&#31867;&#24615;&#33021;&#19979;&#38477;&#12290;&#25552;&#20986;&#19981;&#21516;&#31867;&#22411;&#30340;&#38598;&#25104;&#21644;&#22122;&#22768;&#28165;&#29702;&#26041;&#27861;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16337</link><description>&lt;p&gt;
&#22788;&#29702;BERT&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#29616;&#23454;&#26631;&#31614;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Handling Realistic Label Noise in BERT Text Classification. (arXiv:2305.16337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;BERT&#22312;&#29616;&#23454;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#19979;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21457;&#29616;&#29305;&#24449;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#26469;&#33258;&#27880;&#37322;&#32773;&#20998;&#27495;&#30340;&#21512;&#25104;&#26631;&#31614;&#22122;&#22768;&#20250;&#23548;&#33268;BERT&#30340;&#20998;&#31867;&#24615;&#33021;&#19979;&#38477;&#12290;&#25552;&#20986;&#19981;&#21516;&#31867;&#22411;&#30340;&#38598;&#25104;&#21644;&#22122;&#22768;&#28165;&#29702;&#26041;&#27861;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#26159;&#30001;&#20110;&#24265;&#20215;&#30340;&#25968;&#25454;&#26631;&#27880;&#26041;&#27861;&#65288;&#22914;&#32593;&#32476;&#29228;&#21462;&#25110;&#20247;&#21253;&#65289;&#23548;&#33268;&#30340;&#35757;&#32451;&#26631;&#31614;&#20013;&#30340;&#38169;&#35823;&#65292;&#36825;&#21487;&#33021;&#23545;&#30417;&#30563;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#26377;&#23475;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#25269;&#28040;&#26377;&#30417;&#30563;&#20998;&#31867;&#20013;&#38543;&#26426;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;BERT&#24050;&#32463;&#23545;&#39640;&#27604;&#29575;&#30340;&#38543;&#26426;&#27880;&#20837;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#30340;&#26631;&#31614;&#22122;&#22768;&#24182;&#19981;&#26159;&#38543;&#26426;&#30340;&#65292;&#32780;&#26159;&#32463;&#24120;&#19982;&#36755;&#20837;&#29305;&#24449;&#25110;&#20854;&#20182;&#27880;&#37322;&#32773;&#29305;&#23450;&#22240;&#32032;&#30456;&#20851;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;BERT&#22312;&#38754;&#23545;&#20004;&#31181;&#31867;&#22411;&#30340;&#29616;&#23454;&#26631;&#31614;&#22122;&#22768;&#65306;&#29305;&#24449;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#26469;&#33258;&#27880;&#37322;&#32773;&#20998;&#27495;&#30340;&#21512;&#25104;&#26631;&#31614;&#22122;&#22768;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;&#31867;&#22411;&#22122;&#22768;&#30340;&#23384;&#22312;&#26174;&#33879;&#38477;&#20302;&#20102;BERT&#20998;&#31867;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#38598;&#25104;&#21644;&#22122;&#22768;&#28165;&#29702;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#23427;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labels noise refers to errors in training labels caused by cheap data annotation methods, such as web scraping or crowd-sourcing, which can be detrimental to the performance of supervised classifiers. Several methods have been proposed to counteract the effect of random label noise in supervised classification, and some studies have shown that BERT is already robust against high rates of randomly injected label noise. However, real label noise is not random; rather, it is often correlated with input features or other annotator-specific factors. In this paper, we evaluate BERT in the presence of two types of realistic label noise: feature-dependent label noise, and synthetic label noise from annotator disagreements. We show that the presence of these types of noise significantly degrades BERT classification performance. To improve robustness, we evaluate different types of ensembles and noise-cleaning methods and compare their effectiveness against label noise across different datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20581;&#22766;&#30701;&#25991;&#26412;&#32858;&#31867;&#65288;RSTC&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26368;&#20248;&#36755;&#36816;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#65292;&#20197;&#21450;&#22522;&#20110;&#31867;&#21644;&#23454;&#20363;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#20581;&#22766;&#34920;&#31034;&#23398;&#20064;&#65292;&#24110;&#21161;&#25552;&#39640;&#23545;&#19981;&#24179;&#34913;&#21644;&#22122;&#38899;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16335</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#26368;&#20248;&#36755;&#36816;&#30340;&#20581;&#22766;&#30701;&#25991;&#26412;&#32858;&#31867;&#20013;&#21487;&#38752;&#20266;&#26631;&#31614;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Representation Learning with Reliable Pseudo-labels Generation via Self-Adaptive Optimal Transport for Short Text Clustering. (arXiv:2305.16335v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20581;&#22766;&#30701;&#25991;&#26412;&#32858;&#31867;&#65288;RSTC&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26368;&#20248;&#36755;&#36816;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#65292;&#20197;&#21450;&#22522;&#20110;&#31867;&#21644;&#23454;&#20363;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#20581;&#22766;&#34920;&#31034;&#23398;&#20064;&#65292;&#24110;&#21161;&#25552;&#39640;&#23545;&#19981;&#24179;&#34913;&#21644;&#22122;&#38899;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#25991;&#26412;&#32858;&#31867;&#22240;&#36755;&#20837;&#30340;&#19981;&#24179;&#34913;&#21644;&#22122;&#38899;&#25968;&#25454;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#23481;&#26131;&#22312;&#37325;&#24230;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#36864;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19988;&#26131;&#21463;&#21040;&#22122;&#22768;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20581;&#22766;&#30701;&#25991;&#26412;&#32858;&#31867;&#65288;RSTC&#65289;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#23545;&#19981;&#24179;&#34913;&#21644;&#22122;&#38899;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;RSTC&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65292;&#21363;&#20266;&#26631;&#35760;&#29983;&#25104;&#27169;&#22359;&#21644;&#20581;&#22766;&#34920;&#31034;&#23398;&#20064;&#27169;&#22359;&#12290;&#21069;&#32773;&#29983;&#25104;&#20266;&#26631;&#35760;&#65292;&#20026;&#21518;&#32773;&#25552;&#20379;&#30417;&#30563;&#65292;&#26377;&#21161;&#20110;&#26356;&#20581;&#22766;&#30340;&#34920;&#31034;&#21644;&#27491;&#30830;&#20998;&#31163;&#30340;&#32858;&#31867;&#12290;&#20026;&#20102;&#25552;&#20379;&#23545;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#20266;&#26631;&#31614;&#29983;&#25104;&#27169;&#22359;&#20013;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#26368;&#20248;&#36755;&#36816;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#25968;&#25454;&#20013;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#20581;&#22766;&#34920;&#31034;&#23398;&#20064;&#27169;&#22359;&#20013;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#22522;&#20110;&#31867;&#21644;&#23454;&#20363;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short text clustering is challenging since it takes imbalanced and noisy data as inputs. Existing approaches cannot solve this problem well, since (1) they are prone to obtain degenerate solutions especially on heavy imbalanced datasets, and (2) they are vulnerable to noises. To tackle the above issues, we propose a Robust Short Text Clustering (RSTC) model to improve robustness against imbalanced and noisy data. RSTC includes two modules, i.e., pseudo-label generation module and robust representation learning module. The former generates pseudo-labels to provide supervision for the later, which contributes to more robust representations and correctly separated clusters. To provide robustness against the imbalance in data, we propose self-adaptive optimal transport in the pseudo-label generation module. To improve robustness against the noise in data, we further introduce both class-wise and instance-wise contrastive learning in the robust representation learning module. Our empirical 
&lt;/p&gt;</description></item><item><title>OlaGPT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26234;&#33021;&#26694;&#26550;&#65292;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#22312;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26102;&#25152;&#37319;&#29992;&#30340;&#21508;&#31181;&#35748;&#30693;&#33021;&#21147;&#21644;&#19982;&#24037;&#20855;&#12289;&#30693;&#35782;&#21644;&#22806;&#37096;&#29615;&#22659;&#20449;&#24687;&#30340;&#20132;&#20114;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#31867;&#20154;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16334</link><description>&lt;p&gt;
OlaGPT&#65306;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#31867;&#20154;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities. (arXiv:2305.16334v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16334
&lt;/p&gt;
&lt;p&gt;
OlaGPT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26234;&#33021;&#26694;&#26550;&#65292;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#22312;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26102;&#25152;&#37319;&#29992;&#30340;&#21508;&#31181;&#35748;&#30693;&#33021;&#21147;&#21644;&#19982;&#24037;&#20855;&#12289;&#30693;&#35782;&#21644;&#22806;&#37096;&#29615;&#22659;&#20449;&#24687;&#30340;&#20132;&#20114;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#31867;&#20154;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#30740;&#31350;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29305;&#23450;&#25552;&#31034;&#30340;&#25351;&#23548;&#19979;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#38142;&#26469;&#25191;&#34892;&#25512;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#19978;&#19982;&#20154;&#31867;&#30340;&#33021;&#21147;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#19987;&#27880;&#20110;&#24605;&#32500;&#38142;&#65288;COT&#65289;&#21644;&#24037;&#20855;&#20351;&#29992;&#65292;&#32780;&#24573;&#35270;&#20102;&#37319;&#29992;&#21644;&#24212;&#29992;&#20154;&#31867;&#35748;&#30693;&#26694;&#26550;&#30340;&#37325;&#35201;&#24615;&#12290;&#20154;&#20204;&#36890;&#24120;&#22312;&#38754;&#23545;&#22797;&#26434;&#30340;&#25512;&#29702;&#25361;&#25112;&#26102;&#65292;&#20250;&#36816;&#29992;&#21508;&#31181;&#35748;&#30693;&#33021;&#21147;&#65292;&#24182;&#38656;&#35201;&#19982;&#24037;&#20855;&#12289;&#30693;&#35782;&#21644;&#22806;&#37096;&#29615;&#22659;&#20449;&#24687;&#30340;&#25152;&#26377;&#26041;&#38754;&#36827;&#34892;&#20132;&#20114;&#65292;&#25165;&#33021;&#23436;&#25104;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26234;&#33021;&#26694;&#26550;&#65292;&#31216;&#20026;OlaGPT&#12290;OlaGPT&#20180;&#32454;&#30740;&#31350;&#20102;&#35748;&#30693;&#26550;&#26500;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#35813;&#26694;&#26550;&#28041;&#21450;&#36817;&#20284;&#19981;&#21516;&#30340;&#35748;&#30693;&#27169;&#22359;&#65292;&#21253;&#25324;&#20851;&#27880;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In most current research, large language models (LLMs) are able to perform reasoning tasks by generating chains of thought through the guidance of specific prompts. However, there still exists a significant discrepancy between their capability in solving complex reasoning problems and that of humans. At present, most approaches focus on chains of thought (COT) and tool use, without considering the adoption and application of human cognitive frameworks. It is well-known that when confronting complex reasoning challenges, humans typically employ various cognitive abilities, and necessitate interaction with all aspects of tools, knowledge, and the external environment information to accomplish intricate tasks. This paper introduces a novel intelligent framework, referred to as OlaGPT. OlaGPT carefully studied a cognitive architecture framework, and propose to simulate certain aspects of human cognition. The framework involves approximating different cognitive modules, including attention,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#25991;&#26412;&#22686;&#24191;&#23545;ASR&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#36716;&#25442;&#20026;&#21512;&#25104;&#35821;&#38899;&#65292;&#23454;&#39564;&#21457;&#29616;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#22686;&#24191;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;ASR&#20934;&#30830;&#24230;&#65292;&#21487;&#20197;&#20316;&#20026;&#25913;&#36827;ASR&#31995;&#32479;&#30340;&#19968;&#31181;&#21487;&#34892;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.16333</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#38899;&#21512;&#25104;&#30340;&#25991;&#26412;&#29983;&#25104;&#29992;&#20110;ASR&#25968;&#25454;&#22686;&#24191;
&lt;/p&gt;
&lt;p&gt;
Text Generation with Speech Synthesis for ASR Data Augmentation. (arXiv:2305.16333v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#25991;&#26412;&#22686;&#24191;&#23545;ASR&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#36716;&#25442;&#20026;&#21512;&#25104;&#35821;&#38899;&#65292;&#23454;&#39564;&#21457;&#29616;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#22686;&#24191;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;ASR&#20934;&#30830;&#24230;&#65292;&#21487;&#20197;&#20316;&#20026;&#25913;&#36827;ASR&#31995;&#32479;&#30340;&#19968;&#31181;&#21487;&#34892;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#23569;&#23545;&#26114;&#36149;&#20154;&#24037;&#27880;&#37322;&#30340;&#20381;&#36182;&#65292;&#25968;&#25454;&#22686;&#24191;&#19968;&#30452;&#26159;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#39046;&#22495;&#30340;&#19968;&#20010;&#28909;&#38376;&#30740;&#31350;&#26041;&#21521;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20391;&#37325;&#20110;&#29992;&#20110;ASR&#25968;&#25454;&#22686;&#24191;&#30340;&#21512;&#25104;&#35821;&#38899;&#29983;&#25104;&#65292;&#32780;&#20854;&#19982;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#30340;&#32467;&#21512;&#21364;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#25506;&#32034;&#25991;&#26412;&#22686;&#24191;&#23545;ASR&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#25991;&#26412;&#22686;&#24191;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#27604;&#36739;&#12290;&#29983;&#25104;&#30340;&#21512;&#25104;&#25991;&#26412;&#28982;&#21518;&#36890;&#36807;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#31995;&#32479;&#36716;&#25442;&#20026;&#21512;&#25104;&#35821;&#38899;&#65292;&#24182;&#28155;&#21152;&#21040;ASR&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#21457;&#29616;&#65292;&#31070;&#32463;&#27169;&#22411;&#23454;&#29616;&#20102;9&#65285;-15&#65285;&#30340;&#30456;&#23545;WER&#25913;&#36827;&#65292;&#24182;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#29616;&#20195;&#31070;&#32463;&#26041;&#27861;&#65292;&#25991;&#26412;&#22686;&#24191;&#26159;&#25552;&#39640;ASR&#31995;&#32479;&#20934;&#30830;&#24615;&#30340;&#19968;&#31181;&#21487;&#34892;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiming at reducing the reliance on expensive human annotations, data synthesis for Automatic Speech Recognition (ASR) has remained an active area of research. While prior work mainly focuses on synthetic speech generation for ASR data augmentation, its combination with text generation methods is considerably less explored. In this work, we explore text augmentation for ASR using large-scale pre-trained neural networks, and systematically compare those to traditional text augmentation methods. The generated synthetic texts are then converted to synthetic speech using a text-to-speech (TTS) system and added to the ASR training data. In experiments conducted on three datasets, we find that neural models achieve 9%-15% relative WER improvement and outperform traditional methods. We conclude that text augmentation, particularly through modern neural approaches, is a viable tool for improving the accuracy of ASR systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#32452;&#21512;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#32452;&#21512;&#35270;&#35273;&#38382;&#31572;&#22522;&#20934;&#65292;&#21477;&#27861;&#31070;&#32463;&#27169;&#22359;&#33976;&#39311;&#31561;&#26041;&#27861;&#20197;&#25552;&#39640;&#32452;&#21512;&#33021;&#21147;&#65292;&#24182;&#25506;&#32034;&#20102;&#23545;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#30340;&#22240;&#26524;&#36861;&#36394;&#20197;&#23450;&#20301;&#37325;&#35201;&#31070;&#32463;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.16328</link><description>&lt;p&gt;
&#35270;&#35273;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Semantic Composition in Visually Grounded Language Models. (arXiv:2305.16328v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#32452;&#21512;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#32452;&#21512;&#35270;&#35273;&#38382;&#31572;&#22522;&#20934;&#65292;&#21477;&#27861;&#31070;&#32463;&#27169;&#22359;&#33976;&#39311;&#31561;&#26041;&#27861;&#20197;&#25552;&#39640;&#32452;&#21512;&#33021;&#21147;&#65292;&#24182;&#25506;&#32034;&#20102;&#23545;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#30340;&#22240;&#26524;&#36861;&#36394;&#20197;&#23450;&#20301;&#37325;&#35201;&#31070;&#32463;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#30340;&#24847;&#20041;&#21644;&#20854;&#29702;&#24819;&#34920;&#36798;&#26041;&#24335;&#26159;&#20160;&#20040;&#65311;&#20154;&#31867;&#35821;&#35328;&#34920;&#29616;&#21147;&#30340;&#24456;&#22823;&#19968;&#37096;&#20998;&#26469;&#33258;&#35821;&#20041;&#32452;&#21512;&#65292;&#21363;&#20154;&#31867;&#24515;&#26234;&#20197;&#23618;&#27425;&#21270;&#21644;&#20851;&#31995;&#24615;&#26041;&#24335;&#34920;&#31034;&#24847;&#20041;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#37096;&#20998;&#21477;&#23376;&#30340;&#24847;&#20041;&#23384;&#22312;&#20110;&#25991;&#26412;&#20043;&#22806;&#65292;&#38656;&#35201;&#22522;&#20110;&#24863;&#23448;&#12289;&#36816;&#21160;&#21644;&#20307;&#39564;&#27169;&#24577;&#36827;&#34892;&#20805;&#20998;&#30340;&#23398;&#20064;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26174;&#31034;&#20986;&#30456;&#24403;&#30340;&#32452;&#21512;&#33021;&#21147;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#31034;&#32452;&#21512;&#32467;&#26500;&#26102;&#20005;&#37325;&#22833;&#36133;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#27169;&#22411;&#26159;&#21542;&#21450;&#22914;&#20309;&#32452;&#21512;&#35270;&#35273;&#19978;&#19979;&#25991;&#35821;&#20041;&#20197;&#21450;&#22914;&#20309;&#25552;&#39640;&#20854;&#32452;&#21512;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; 1) WinogroundVQA&#65292;&#19968;&#20010;&#26032;&#30340;&#32452;&#21512;&#35270;&#35273;&#38382;&#31572;&#22522;&#20934;&#65292;2) &#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#20013;&#32452;&#21512;&#33021;&#21147;&#30340;&#21477;&#27861;&#31070;&#32463;&#27169;&#22359;&#33976;&#39311;&#65292;3) &#23545;&#20110;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#30340;&#22240;&#26524;&#36861;&#36394;&#65292;&#20197;&#23450;&#20301;&#37325;&#35201;&#30340;&#31070;&#32463;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is sentence meaning and its ideal representation? Much of the expressive power of human language derives from semantic composition, the mind's ability to represent meaning hierarchically &amp; relationally over constituents. At the same time, much sentential meaning is outside the text and requires grounding in sensory, motor, and experiential modalities to be adequately learned. Although large language models display considerable compositional ability, recent work shows that visually-grounded language models drastically fail to represent compositional structure. In this thesis, we explore whether &amp; how models compose visually grounded semantics, and how we might improve their ability to do so.  Specifically, we introduce 1) WinogroundVQA, a new compositional visual question answering benchmark, 2) Syntactic Neural Module Distillation, a measure of compositional ability in sentence embedding models, 3) Causal Tracing for Image Captioning Models to locate neural representations vital f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.16326</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#22522;&#20934;&#12289;&#22522;&#32447;&#21644;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#25163;&#21160;&#31579;&#36873;&#21644;&#25552;&#21462;&#30693;&#35782;&#21464;&#24471;&#22256;&#38590;&#12290;&#33258;&#21160;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;BioNLP&#65289;&#25216;&#26415;&#26377;&#21161;&#20110;&#20943;&#36731;&#36825;&#31181;&#36127;&#25285;&#12290;&#36817;&#24180;&#26469;&#65292;&#22914;GPT-3&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;BioNLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#26041;&#27861;&#24320;&#21457;&#21644;&#19979;&#28216;&#29992;&#25143;&#30340;&#24433;&#21709;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#65288;1&#65289;&#22312;&#22235;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#22312;&#20843;&#20010;BioNLP&#25968;&#25454;&#38598;&#20013;&#24314;&#31435;&#20102;GPT-3&#21644;GPT-4&#22312;&#38646;-shot&#21644;&#19968;-shot&#35774;&#32622;&#19979;&#30340;&#22522;&#20934;&#34920;&#29616;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20851;&#31995;&#25552;&#21462;&#65292;&#22810;&#26631;&#31614;&#25991;&#26723;&#20998;&#31867;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#25512;&#29702;&#65307;&#65288;2&#65289;&#23457;&#26597;&#20102;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#38169;&#35823;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#32570;&#22833;&#65292;&#19981;&#19968;&#33268;&#21644;&#19981;&#38656;&#35201;&#30340;&#20154;&#24037;&#20869;&#23481;&#65307;&#65288;3&#65289;&#25552;&#20986;&#20102;&#20351;&#29992;LLMs&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#23545;&#35805;&#31995;&#32479;&#30340;&#21457;&#23637;&#21382;&#21490;&#12289;&#22522;&#26412;&#36816;&#20316;&#12289;&#27969;&#34892;&#21644;&#26032;&#20852;&#25968;&#25454;&#38598;&#12289;&#20851;&#38190;&#36129;&#29486;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#25361;&#25112;&#65292;&#23637;&#26395;&#20102;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.16324</link><description>&lt;p&gt;
&#19982;&#26426;&#22120;&#23545;&#35805;&#65306;&#26032;&#20852;&#23545;&#35805;&#31995;&#32479;&#30340;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Talking with Machines: A Comprehensive Survey of Emergent Dialogue Systems. (arXiv:2305.16324v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#23545;&#35805;&#31995;&#32479;&#30340;&#21457;&#23637;&#21382;&#21490;&#12289;&#22522;&#26412;&#36816;&#20316;&#12289;&#27969;&#34892;&#21644;&#26032;&#20852;&#25968;&#25454;&#38598;&#12289;&#20851;&#38190;&#36129;&#29486;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#25361;&#25112;&#65292;&#23637;&#26395;&#20102;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;20&#19990;&#32426;&#30340;&#26368;&#26089;&#23454;&#39564;&#21040;&#29616;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#21464;&#21387;&#22120;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#23545;&#35805;&#31995;&#32479;&#30740;&#31350;&#19968;&#30452;&#22312;&#19981;&#26029;&#21457;&#23637;&#65292;&#22312;&#20247;&#22810;&#39046;&#22495;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#36825;&#20123;&#31995;&#32479;&#32508;&#36848;&#65292;&#36861;&#28335;&#23427;&#20204;&#30340;&#21382;&#21490;&#21457;&#23637;&#24182;&#26816;&#26597;&#23427;&#20204;&#30340;&#22522;&#26412;&#36816;&#20316;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#29992;&#20110;&#35757;&#32451;&#30340;&#27969;&#34892;&#21644;&#26032;&#20852;&#25968;&#25454;&#38598;&#65292;&#24182;&#35843;&#26597;&#20102;&#23545;&#35805;&#31995;&#32479;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#36129;&#29486;&#65292;&#21253;&#25324;&#20256;&#32479;&#31995;&#32479;&#21644;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#24120;&#35268;&#21644;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#28982;&#21518;&#31616;&#35201;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26410;&#26469;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
From the earliest experiments in the 20th century to the utilization of large language models and transformers, dialogue systems research has continued to evolve, playing crucial roles in numerous fields. This paper offers a comprehensive review of these systems, tracing their historical development and examining their fundamental operations. We analyze popular and emerging datasets for training and survey key contributions in dialogue systems research, including traditional systems and advanced machine learning methods. Finally, we consider conventional and transformer-based evaluation metrics, followed by a short discussion of prevailing challenges and future prospects in the field.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;UNITE&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#35780;&#20272;&#65292;&#21253;&#21547;&#26469;&#33258;12&#20010;&#20197;&#19978;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12289;&#36229;&#36807;3.9K&#31181;&#27169;&#24335;&#30340;SQL&#26597;&#35810;&#21644;29K&#20010;&#25968;&#25454;&#24211;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;Codex&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#35774;&#35745;&#30340;&#32534;&#30721;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#21487;&#26426;&#35835;&#30340;&#25968;&#25454;&#24211;&#30340;&#36136;&#37327;&#23545;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.16265</link><description>&lt;p&gt;
UNITE: &#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#35780;&#20272;&#30340;&#32479;&#19968;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
UNITE: A Unified Benchmark for Text-to-SQL Evaluation. (arXiv:2305.16265v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16265
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;UNITE&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#35780;&#20272;&#65292;&#21253;&#21547;&#26469;&#33258;12&#20010;&#20197;&#19978;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12289;&#36229;&#36807;3.9K&#31181;&#27169;&#24335;&#30340;SQL&#26597;&#35810;&#21644;29K&#20010;&#25968;&#25454;&#24211;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;Codex&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#35774;&#35745;&#30340;&#32534;&#30721;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#21487;&#26426;&#35835;&#30340;&#25968;&#25454;&#24211;&#30340;&#36136;&#37327;&#23545;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#23454;&#29992;&#30340;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#24212;&#35813;&#21487;&#20197;&#24456;&#22909;&#22320;&#27010;&#25324;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12289;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#24211;&#27169;&#24335;&#21644;&#26032;&#39062;&#30340;SQL&#26597;&#35810;&#32467;&#26500;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;UNITE&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#35780;&#20272;&#12290;&#35813;&#22522;&#20934;&#30001;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#21253;&#21547;&#26469;&#33258;12&#20010;&#20197;&#19978;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12289;&#36229;&#36807;3.9K&#31181;&#27169;&#24335;&#30340;SQL&#26597;&#35810;&#21644;29K&#20010;&#25968;&#25454;&#24211;&#12290;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;Spider&#22522;&#20934;&#30456;&#27604;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#32422;120K&#20010;&#39069;&#22806;&#30340;&#31034;&#20363;&#21644;&#19977;&#20493;&#30340;SQL&#27169;&#24335;&#65292;&#20363;&#22914;&#27604;&#36739;&#21644;&#24067;&#23572;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#26032;&#22522;&#20934;&#19978;&#23545;&#20845;&#31181;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#22120;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#65306;1&#65289;Codex&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65307;2&#65289;&#29305;&#21035;&#35774;&#35745;&#30340;&#32534;&#30721;&#26041;&#27861;&#65288;&#20363;&#22914;&#32422;&#26463;&#26463;&#25628;&#32034;&#65289;&#21487;&#20197;&#25552;&#39640;&#22312;&#39046;&#22495;&#20869;&#22806;&#30340;&#24615;&#33021;&#65307;3&#65289;&#21487;&#26426;&#35835;&#30340;&#25968;&#25454;&#24211;&#30340;&#36136;&#37327;&#23545;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
A practical text-to-SQL system should generalize well on a wide variety of natural language questions, unseen database schemas, and novel SQL query structures. To comprehensively evaluate text-to-SQL systems, we introduce a \textbf{UNI}fied benchmark for \textbf{T}ext-to-SQL \textbf{E}valuation (UNITE). It is composed of publicly available text-to-SQL datasets, containing natural language questions from more than 12 domains, SQL queries from more than 3.9K patterns, and 29K databases. Compared to the widely used Spider benchmark \cite{yu-etal-2018-spider}, we introduce $\sim$120K additional examples and a threefold increase in SQL patterns, such as comparative and boolean questions. We conduct a systematic study of six state-of-the-art (SOTA) text-to-SQL parsers on our new benchmark and show that: 1) Codex performs surprisingly well on out-of-domain datasets; 2) specially designed decoding methods (e.g. constrained beam search) can improve performance for both in-domain and out-of-doma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#24615;&#24863;&#30693;&#30340;&#30456;&#24178;&#24615;&#25439;&#22833;&#65292;&#21487;&#20197;&#24110;&#21161;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#20445;&#25345;&#39640;&#22810;&#26679;&#24615;&#21516;&#26102;&#65292;&#26356;&#22909;&#22320;&#23398;&#20064;&#35821;&#26009;&#24211;&#32423;&#21035;&#30340;&#36830;&#36143;&#24615;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.16199</link><description>&lt;p&gt;
&#38754;&#21521;&#22810;&#26679;&#24615;&#30340;&#30456;&#24178;&#25439;&#22833;&#29992;&#20110;&#25913;&#36827;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diversity-Aware Coherence Loss for Improving Neural Topic Models. (arXiv:2305.16199v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#24615;&#24863;&#30693;&#30340;&#30456;&#24178;&#24615;&#25439;&#22833;&#65292;&#21487;&#20197;&#24110;&#21161;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#20445;&#25345;&#39640;&#22810;&#26679;&#24615;&#21516;&#26102;&#65292;&#26356;&#22909;&#22320;&#23398;&#20064;&#35821;&#26009;&#24211;&#32423;&#21035;&#30340;&#36830;&#36143;&#24615;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#30340;&#26631;&#20934;&#26041;&#27861;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26694;&#26550;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20272;&#35745;&#21518;&#39564;&#21644;&#20808;&#39564;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#20197;&#21450;&#37325;&#24314;&#25439;&#22833;&#12290;&#30001;&#20110;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#26159;&#36890;&#36807;&#37325;&#26032;&#21019;&#24314;&#21508;&#20010;&#36755;&#20837;&#25991;&#26723;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#23427;&#20204;&#19981;&#20250;&#26126;&#30830;&#22320;&#25429;&#33719;&#35821;&#26009;&#24211;&#32423;&#21035;&#30340;&#20027;&#39064;&#35789;&#20043;&#38388;&#30340;&#36830;&#36143;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26679;&#24615;&#24863;&#30693;&#30340;&#30456;&#24178;&#24615;&#25439;&#22833;&#65292;&#40723;&#21169;&#27169;&#22411;&#23398;&#20064;&#35821;&#26009;&#24211;&#32423;&#21035;&#30340;&#36830;&#36143;&#24615;&#20998;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#20027;&#39064;&#20043;&#38388;&#30340;&#39640;&#22810;&#26679;&#24615;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#39044;&#35757;&#32451;&#25110;&#39069;&#22806;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard approach for neural topic modeling uses a variational autoencoder (VAE) framework that jointly minimizes the KL divergence between the estimated posterior and prior, in addition to the reconstruction loss. Since neural topic models are trained by recreating individual input documents, they do not explicitly capture the coherence between topic words on the corpus level. In this work, we propose a novel diversity-aware coherence loss that encourages the model to learn corpus-level coherence scores while maintaining a high diversity between topics. Experimental results on multiple datasets show that our method significantly improves the performance of neural topic models without requiring any pretraining or additional parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20197;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#21644;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#31561;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15722</link><description>&lt;p&gt;
&#38754;&#21521;&#20195;&#30721;&#28151;&#21512;&#30340;&#21360;&#22320;&#35821;-&#33521;&#35821;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative Study of Pre-Trained BERT Models for Code-Mixed Hindi-English Data. (arXiv:2305.15722v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20197;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#21644;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#31561;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#20195;&#30721;&#28151;&#21512;&#8221;&#26159;&#25351;&#22312;&#21516;&#19968;&#27573;&#25991;&#26412;&#20013;&#20351;&#29992;&#22810;&#31181;&#35821;&#35328;&#30340;&#29616;&#35937;&#12290;&#36825;&#31181;&#29616;&#35937;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24191;&#27867;&#23384;&#22312;&#65292;&#24182;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37319;&#32435;&#12290;&#26816;&#27979;&#35821;&#35328;&#20013;&#30340;&#22806;&#26469;&#20803;&#32032;&#24182;&#27491;&#30830;&#22788;&#29702;&#23427;&#20204;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#35768;&#22810;&#20154;&#20351;&#29992;&#20195;&#30721;&#28151;&#21512;&#35821;&#35328;&#65292;&#20854;&#20013;&#20219;&#19968;&#35821;&#35328;&#37117;&#26080;&#27861;&#29702;&#35299;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20302;&#36164;&#28304;&#30340;&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#35821;&#35328;&#65292;&#24182;&#25552;&#39640;&#19981;&#21516;&#20195;&#30721;&#28151;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#21644;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#39044;&#35757;&#32451;&#30340;&#19981;&#21516;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#21253;&#25324;&#20102;&#20195;&#30721;&#28151;&#21512;&#27169;&#22411;&#65288;&#22914;HingBERT&#12289;HingRoBERTa&#12289;HingRoBERTa-Mixed&#12289;mBERT&#65289;&#21644;&#38750;&#20195;&#30721;&#28151;&#21512;&#27169;&#22411;&#65288;&#22914;AlBERT&#12289;BERT&#12289;RoBERTa&#65289;&#65292;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term "Code Mixed" refers to the use of more than one language in the same text. This phenomenon is predominantly observed on social media platforms, with an increasing amount of adaptation as time goes on. It is critical to detect foreign elements in a language and process them correctly, as a considerable number of individuals are using code-mixed languages that could not be comprehended by understanding one of those languages. In this work, we focus on low-resource Hindi-English code-mixed language and enhancing the performance of different code-mixed natural language processing tasks such as sentiment analysis, emotion recognition, and hate speech identification. We perform a comparative analysis of different Transformer-based language Models pre-trained using unsupervised approaches. We have included the code-mixed models like HingBERT, HingRoBERTa, HingRoBERTa-Mixed, mBERT, and non-code-mixed models like AlBERT, BERT, and RoBERTa for comparative analysis of code-mixed Hindi-En
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#20250;&#35805;&#25628;&#32034;&#30340;ConvGQR&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#37325;&#26032;&#26500;&#36896;&#26597;&#35810;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2305.15645</link><description>&lt;p&gt;
ConvGQR&#65306;&#38754;&#21521;&#20250;&#35805;&#25628;&#32034;&#30340;&#29983;&#25104;&#24335;&#26597;&#35810;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
ConvGQR: Generative Query Reformulation for Conversational Search. (arXiv:2305.15645v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#20250;&#35805;&#25628;&#32034;&#30340;ConvGQR&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#37325;&#26032;&#26500;&#36896;&#26597;&#35810;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20250;&#35805;&#25628;&#32034;&#20013;&#65292;&#29992;&#25143;&#24403;&#21069;&#25628;&#32034;&#24847;&#22270;&#20381;&#36182;&#20110;&#20808;&#21069;&#30340;&#23545;&#35805;&#21382;&#21490;&#12290;&#20174;&#25972;&#20010;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#30830;&#23450;&#19968;&#20010;&#33391;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#36991;&#20813;&#26597;&#35810;&#32534;&#30721;&#22120;&#30340;&#26114;&#36149;&#37325;&#26032;&#35757;&#32451;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#26041;&#27861;&#23581;&#35797;&#23398;&#20064;&#19968;&#20010;&#37325;&#20889;&#27169;&#22411;&#65292;&#36890;&#36807;&#27169;&#20223;&#25163;&#21160;&#26597;&#35810;&#37325;&#20889;&#26469;&#21435;&#38500;&#24403;&#21069;&#26597;&#35810;&#30340;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#37325;&#20889;&#30340;&#26597;&#35810;&#24182;&#19981;&#24635;&#26159;&#26368;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;&#35757;&#32451;&#37325;&#20889;&#27169;&#22411;&#20250;&#38480;&#21046;&#27169;&#22411;&#20135;&#29983;&#33391;&#22909;&#25628;&#32034;&#26597;&#35810;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;ConvGQR&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#19968;&#20010;&#29992;&#20110;&#26597;&#35810;&#37325;&#20889;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#28508;&#22312;&#31572;&#26696;&#65292;&#20197;&#37325;&#26032;&#26500;&#36896;&#20250;&#35805;&#26597;&#35810;&#12290;&#36890;&#36807;&#32467;&#21512;&#20004;&#32773;&#65292;ConvGQR&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23558;&#26597;&#35810;&#37325;&#26500;&#19982;&#26816;&#32034;&#24615;&#33021;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#36873;&#25321;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#27169;&#22411;&#65292;&#29992;&#20110;&#39564;&#35777;ConvGQR&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conversational search, the user's real search intent for the current turn is dependent on the previous conversation history. It is challenging to determine a good search query from the whole conversation context. To avoid the expensive re-training of the query encoder, most existing methods try to learn a rewriting model to de-contextualize the current query by mimicking the manual query rewriting. However, manually rewritten queries are not always the best search queries. Training a rewriting model on them would limit the model's ability to produce good search queries. Another useful hint is the potential answer to the question. In this paper, we propose ConvGQR, a new framework to reformulate conversational queries based on generative pre-trained language models (PLMs), one for query rewriting and another for generating potential answers. By combining both, ConvGQR can produce better search queries. In addition, to relate query reformulation to retrieval performance, we propose a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34164;&#21547;-&#30683;&#30462;&#39044;&#27979;&#26041;&#27861;&#65292;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#23618;&#25968;&#25454;&#38598;&#20013;&#30340;&#38646;&#26679;&#20363;&#20998;&#31867;&#38382;&#39064;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20005;&#26684;&#38646;&#26679;&#20363;&#20998;&#23618;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.15282</link><description>&lt;p&gt;
&#19968;&#31181;&#20005;&#26684;&#38646;&#26679;&#20363;&#20998;&#23618;&#20998;&#31867;&#30340;&#31616;&#21333;&#26377;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification. (arXiv:2305.15282v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34164;&#21547;-&#30683;&#30462;&#39044;&#27979;&#26041;&#27861;&#65292;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#23618;&#25968;&#25454;&#38598;&#20013;&#30340;&#38646;&#26679;&#20363;&#20998;&#31867;&#38382;&#39064;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20005;&#26684;&#38646;&#26679;&#20363;&#20998;&#23618;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22522;&#20934;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#20363;&#25110;&#23569;&#26679;&#20363;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#36890;&#24120;&#26410;&#33021;&#20805;&#20998;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#20998;&#23618;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#20256;&#32479;&#30340;&#20998;&#23618;&#25968;&#25454;&#38598;&#19978;&#30340;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#26356;&#20855;&#25351;&#31034;&#24615;&#30340;&#38271;&#23614;&#39044;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#26356;&#23481;&#26131;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;LLMs&#30340;&#22522;&#30784;&#19978;&#20351;&#29992;&#34164;&#21547;-&#30683;&#30462;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#20005;&#26684;&#38646;&#26679;&#20363;&#24773;&#20917;&#19979;&#33719;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#65292;&#36825;&#26159;&#19968;&#31181;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLMs) have achieved strong performance on benchmark tasks, especially in zero or few-shot settings. However, these benchmarks often do not adequately address the challenges posed in the real-world, such as that of hierarchical classification. In order to address this challenge, we propose refactoring conventional tasks on hierarchical datasets into a more indicative long-tail prediction task. We observe LLMs are more prone to failure in these cases. To address these limitations, we propose the use of entailment-contradiction prediction in conjunction with LLMs, which allows for strong performance in a strict zero-shot setting. Importantly, our method does not require any parameter updates, a resource-intensive process and achieves strong performance across multiple datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23545;&#33889;&#33796;&#29273;&#35821;&#20013;&#30340;Whisper ASR&#36827;&#34892;&#20102;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#20026;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#22312;&#20027;&#39064;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.14580</link><description>&lt;p&gt;
&#35780;&#20272;OpenAI&#25552;&#20379;&#30340;Whisper ASR&#22312;Museum of the Person&#30340;&#29983;&#27963;&#21490;&#20013;&#36827;&#34892;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#21644;&#20027;&#39064;&#24314;&#27169;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Evaluating OpenAI's Whisper ASR for Punctuation Prediction and Topic Modeling of life histories of the Museum of the Person. (arXiv:2305.14580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23545;&#33889;&#33796;&#29273;&#35821;&#20013;&#30340;Whisper ASR&#36827;&#34892;&#20102;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#20026;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#22312;&#20027;&#39064;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#22312;&#20154;&#26426;&#20132;&#20114;&#24212;&#29992;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#21313;&#24180;&#20013;&#25552;&#20986;&#30340;&#33889;&#33796;&#29273;&#35821;ASR&#27169;&#22411;&#22312;&#27491;&#30830;&#35782;&#21035;&#33258;&#21160;&#36716;&#24405;&#20013;&#30340;&#26631;&#28857;&#31526;&#21495;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#36825;&#20351;&#24471;&#36825;&#20123;&#36716;&#24405;&#19981;&#33021;&#34987;&#20854;&#20182;&#31995;&#32479;&#12289;&#27169;&#22411;&#21644;&#29978;&#33267;&#26159;&#20154;&#31867;&#20351;&#29992;&#12290;&#26368;&#36817;&#65292;OpenAI&#25552;&#20986;&#20102;Whisper ASR&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#26377;&#26395;&#22788;&#29702;&#36825;&#20123;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#27425;&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#20013;Whisper&#30340;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#24615;&#33021;&#36827;&#34892;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#39564;&#35780;&#20272;&#26469;&#32771;&#34385;&#20851;&#20110;&#20572;&#39039;&#28857;&#65288;&#36887;&#21495;&#65289;&#21644;&#23436;&#25972;&#24605;&#24819;&#65288;&#24863;&#21497;&#12289;&#30097;&#38382;&#21644;&#21477;&#21495;&#65289;&#30340;&#29702;&#35770;&#26041;&#38754;&#65292;&#20197;&#21450;&#19982;&#22522;&#20110;&#36716;&#24405;&#30340;&#20027;&#39064;&#24314;&#27169;&#30456;&#20851;&#30340;&#23454;&#38469;&#26041;&#38754;&#65292;&#20351;&#29992;&#26631;&#28857;&#31526;&#21495;&#26469;&#25552;&#39640;&#24615;&#33021;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) systems play a key role in applications involving human-machine interactions. Despite their importance, ASR models for the Portuguese language proposed in the last decade have limitations in relation to the correct identification of punctuation marks in automatic transcriptions, which hinder the use of transcriptions by other systems, models, and even by humans. However, recently Whisper ASR was proposed by OpenAI, a general-purpose speech recognition model that has generated great expectations in dealing with such limitations. This chapter presents the first study on the performance of Whisper for punctuation prediction in the Portuguese language. We present an experimental evaluation considering both theoretical aspects involving pausing points (comma) and complete ideas (exclamation, question, and fullstop), as well as practical aspects involving transcript-based topic modeling - an application dependent on punctuation marks for promising performan
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SPEECH&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#33021;&#37327;&#24314;&#27169;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13617</link><description>&lt;p&gt;
SPEECH: &#22522;&#20110;&#33021;&#37327;&#30340;&#20107;&#20214;&#20013;&#24515;&#36229;&#29699;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres. (arXiv:2305.13617v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13617
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SPEECH&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#33021;&#37327;&#24314;&#27169;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#20013;&#24515;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;&#28041;&#21450;&#39044;&#27979;&#20107;&#20214;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#12290;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24773;&#20917;&#19979;&#65292;&#20107;&#20214;&#32467;&#26500;&#37117;&#20855;&#26377;&#22797;&#26434;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22240;&#27492;&#26377;&#25928;&#22320;&#34920;&#31034;&#36825;&#20123;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#20107;&#20214;&#20013;&#24515;&#36229;&#29699;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979; (SPEECH)&#12290; SPEECH &#20351;&#29992;&#22522;&#20110;&#33021;&#37327;&#30340;&#24314;&#27169;&#26469;&#27169;&#25311;&#20107;&#20214;&#32467;&#26500;&#32452;&#20214;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#22312;&#20004;&#20010;&#32479;&#19968;&#26631;&#27880;&#30340;&#20107;&#20214;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#21344;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event-centric structured prediction involves predicting structured outputs of events. In most NLP cases, event structures are complex with manifold dependency, and it is challenging to effectively represent these complicated structured events. To address these issues, we propose Structured Prediction with Energy-based Event-Centric Hyperspheres (SPEECH). SPEECH models complex dependency among event structured components with energy-based modeling, and represents event classes with simple but effective hyperspheres. Experiments on two unified-annotated event datasets indicate that SPEECH is predominant in event detection and event-relation extraction tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DR&#30340;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#19981;&#23545;&#31216;&#30340;&#23398;&#20064;&#29575;&#26469;&#35299;&#20915;&#30001;&#21512;&#20316;&#21338;&#24328;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25913;&#21892;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13599</link><description>&lt;p&gt;
&#19981;&#23545;&#31216;&#23398;&#20064;&#29575;&#30340;&#20998;&#31163;&#24335;&#29702;&#24615;&#21270;: &#19968;&#31181;&#28789;&#27963;&#30340;Lipschitz&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Decoupled Rationalization with Asymmetric Learning Rates: A Flexible Lipshitz Restraint. (arXiv:2305.13599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DR&#30340;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#19981;&#23545;&#31216;&#30340;&#23398;&#20064;&#29575;&#26469;&#35299;&#20915;&#30001;&#21512;&#20316;&#21338;&#24328;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25913;&#21892;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#33258;&#35828;&#26126;&#29702;&#24615;&#21270;&#27169;&#22411;&#36890;&#36807;&#21512;&#20316;&#21338;&#24328;&#26500;&#24314;&#65292;&#20854;&#20013;&#29983;&#25104;&#22120;&#20174;&#36755;&#20837;&#25991;&#26412;&#20013;&#36873;&#25321;&#26368;&#26131;&#29702;&#35299;&#30340;&#37096;&#20998;&#20316;&#20026;&#21407;&#29702;&#65292;&#25509;&#30528;&#39044;&#27979;&#22120;&#22522;&#20110;&#25152;&#36873;&#25321;&#30340;&#21407;&#29702;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21512;&#20316;&#21338;&#24328;&#21487;&#33021;&#20250;&#24341;&#21457;&#36864;&#21270;&#38382;&#39064;&#65292;&#39044;&#27979;&#22120;&#36807;&#24230;&#25311;&#21512;&#20110;&#30001;&#23578;&#26410;&#35757;&#32451;&#22909;&#30340;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#20449;&#24687;&#19981;&#36275;&#30340;&#37096;&#20998;&#65292;&#21453;&#36807;&#26469;&#23548;&#33268;&#29983;&#25104;&#22120;&#25910;&#25947;&#20110;&#36235;&#21521;&#20110;&#36873;&#25321;&#26080;&#24847;&#20041;&#30340;&#37096;&#20998;&#30340;&#27425;&#20248;&#27169;&#22411;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#23558;&#36864;&#21270;&#38382;&#39064;&#19982;&#39044;&#27979;&#22120;&#30340;Lipschitz&#36830;&#32493;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DR&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#28982;&#12289;&#28789;&#27963;&#22320;&#32422;&#26463;&#39044;&#27979;&#22120;&#30340;Lipschitz&#24120;&#25968;&#65292;&#24182;&#35299;&#20915;&#20102;&#36864;&#21270;&#38382;&#39064;&#12290;DR&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#23558;&#29983;&#25104;&#22120;&#21644;&#39044;&#27979;&#22120;&#20998;&#31163;&#65292;&#20026;&#23427;&#20204;&#20998;&#37197;&#19981;&#23545;&#31216;&#30340;&#23398;&#20064;&#29575;&#12290;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;DR&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
A self-explaining rationalization model is generally constructed by a cooperative game where a generator selects the most human-intelligible pieces from the input text as rationales, followed by a predictor that makes predictions based on the selected rationales. However, such a cooperative game may incur the degeneration problem where the predictor overfits to the uninformative pieces generated by a not yet well-trained generator and in turn, leads the generator to converge to a sub-optimal model that tends to select senseless pieces. In this paper, we theoretically bridge degeneration with the predictor's Lipschitz continuity. Then, we empirically propose a simple but effective method named DR, which can naturally and flexibly restrain the Lipschitz constant of the predictor, to address the problem of degeneration. The main idea of DR is to decouple the generator and predictor to allocate them with asymmetric learning rates. A series of experiments conducted on two widely used benchm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#25386;&#23041;&#20004;&#31181;&#20070;&#20889;&#24418;&#24335;&#35821;&#26009;&#24211;&#20013;&#30340;&#23454;&#20307;&#21644;&#20849;&#25351;&#26631;&#27880;&#25968;&#25454;&#21512;&#24182;&#21040;&#20102;&#36890;&#29992;&#20381;&#23384;&#35821;&#26009;&#24211;&#65288;UD treebanks&#65289;&#20013;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21152;&#20837;&#23454;&#20307;&#21644;&#20849;&#25351;&#20449;&#24687;&#30340;&#25386;&#23041;UD treebank&#65292;&#23545;&#26410;&#26469;&#35821;&#26009;&#24211;&#23545;&#40784;&#21644;&#20849;&#25351;&#27880;&#37322;&#24037;&#20316;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2305.13527</link><description>&lt;p&gt;
&#23558;&#25386;&#23041;UD Treebank&#19982;&#23454;&#20307;&#21644;&#20849;&#25351;&#20449;&#24687;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning the Norwegian UD Treebank with Entity and Coreference Information. (arXiv:2305.13527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#25386;&#23041;&#20004;&#31181;&#20070;&#20889;&#24418;&#24335;&#35821;&#26009;&#24211;&#20013;&#30340;&#23454;&#20307;&#21644;&#20849;&#25351;&#26631;&#27880;&#25968;&#25454;&#21512;&#24182;&#21040;&#20102;&#36890;&#29992;&#20381;&#23384;&#35821;&#26009;&#24211;&#65288;UD treebanks&#65289;&#20013;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21152;&#20837;&#23454;&#20307;&#21644;&#20849;&#25351;&#20449;&#24687;&#30340;&#25386;&#23041;UD treebank&#65292;&#23545;&#26410;&#26469;&#35821;&#26009;&#24211;&#23545;&#40784;&#21644;&#20849;&#25351;&#27880;&#37322;&#24037;&#20316;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#25386;&#23041;&#20004;&#31181;&#20070;&#20889;&#24418;&#24335;&#35821;&#26009;&#24211;&#9472;&#9472;Bokm{&#229;}l&#21644;Nynorsk&#30340;&#36890;&#29992;&#20381;&#23384;&#35821;&#26009;&#24211;&#65288;UD treebanks&#65289;&#20013;&#30340;&#23454;&#20307;&#21644;&#20849;&#25351;&#26631;&#27880;&#25968;&#25454;&#30340;&#21512;&#24182;&#38598;&#21512;&#12290;&#25152;&#21512;&#24182;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;Norwegian Named Entities&#65288;NorNE&#65289;&#21644;Norwegian Anaphora Resolution Corpus&#65288;NARC&#65289;&#20004;&#37096;&#20998;&#12290;&#34429;&#28982;NorNE&#19982;&#26087;&#29256;&#26412;&#30340;treebank&#23545;&#40784;&#65292;&#20294;NARC&#21017;&#26410;&#33021;&#23545;&#40784;&#65292;&#38656;&#35201;&#20174;&#21407;&#22987;&#27880;&#37322;&#21040;UD&#32467;&#26500;&#21644;CoNLL-U&#26684;&#24335;&#36827;&#34892;&#24191;&#27867;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#22312;&#27492;&#28436;&#31034;&#20102;&#36716;&#25442;&#21644;&#23545;&#40784;&#36807;&#31243;&#65292;&#24182;&#20998;&#26512;&#20102;&#21457;&#29616;&#30340;&#25968;&#25454;&#38382;&#39064;&#21644;&#38169;&#35823;&#65292;&#20854;&#20013;&#21253;&#25324;&#21407;&#22987;treebank&#20013;&#30340;&#25968;&#25454;&#20998;&#21106;&#37325;&#21472;&#38382;&#39064;&#12290;&#36825;&#20123;&#31243;&#24207;&#21644;&#24320;&#21457;&#30340;&#31995;&#32479;&#21487;&#33021;&#26377;&#21161;&#20110;&#26410;&#26469;&#30340;&#35821;&#26009;&#24211;&#23545;&#40784;&#21644;&#20849;&#25351;&#27880;&#37322;&#24037;&#20316;&#12290;&#21512;&#24182;&#30340;&#35821;&#26009;&#24211;&#21253;&#25324;&#31532;&#19968;&#20010;&#21152;&#20837;&#21629;&#21517;&#23454;&#20307;&#21644;&#20849;&#25351;&#20449;&#24687;&#30340;&#25386;&#23041;UD treebank&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a merged collection of entity and coreference annotated data grounded in the Universal Dependencies (UD) treebanks for the two written forms of Norwegian: Bokm{\aa}l and Nynorsk. The aligned and converted corpora are the \textit{Norwegian Named Entities} (NorNE) and \textit{Norwegian Anaphora Resolution Corpus} (NARC). While NorNE is aligned with an older version of the treebank, NARC is misaligned and requires extensive transformation from the original annotations to the UD structure and CoNLL-U format. We here demonstrate the conversion and alignment processes, along with an analysis of discovered issues and errors in the data -- some of which include data split overlaps in the original treebank. These procedures and the developed system may prove helpful for future corpus alignment and coreference annotation endeavors. The merged corpora comprise the first Norwegian UD treebank enriched with named entities and coreference information.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23545;&#22522;&#22240;&#38598;&#36827;&#34892;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;SPINDOCTOR&#65292;&#21487;&#20197;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13338</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#22240;&#38598;&#27010;&#25324;
&lt;/p&gt;
&lt;p&gt;
Gene Set Summarization using Large Language Models. (arXiv:2305.13338v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23545;&#22522;&#22240;&#38598;&#36827;&#34892;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;SPINDOCTOR&#65292;&#21487;&#20197;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#29983;&#29289;&#23398;&#23478;&#32463;&#24120;&#35299;&#37322;&#20174;&#39640;&#36890;&#37327;&#23454;&#39564;&#21644;&#35745;&#31639;&#20998;&#26512;&#20013;&#33719;&#24471;&#30340;&#22522;&#22240;&#21015;&#34920;&#12290;&#36825;&#36890;&#24120;&#26159;&#36890;&#36807;&#32479;&#35745;&#23500;&#38598;&#20998;&#26512;&#26469;&#23436;&#25104;&#30340;&#65292;&#35813;&#20998;&#26512;&#27979;&#37327;&#19982;&#22522;&#22240;&#25110;&#20854;&#23646;&#24615;&#30456;&#20851;&#30340;&#29983;&#29289;&#21151;&#33021;&#26415;&#35821;&#30340;&#36807;&#24230;&#25110;&#27424;&#34920;&#31034;&#31243;&#24230;&#65292;&#22522;&#20110;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#65288;&#20363;&#22914;Gene Ontology&#65288;GO&#65289;&#65289;&#20013;&#30340;&#32534;&#35793;&#26029;&#35328;&#12290;&#35299;&#37322;&#22522;&#22240;&#21015;&#34920;&#20063;&#21487;&#20197;&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#25991;&#26412;&#27010;&#25324;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#65292;&#21487;&#33021;&#30452;&#25509;&#21033;&#29992;&#31185;&#23398;&#25991;&#26412;&#24182;&#36991;&#20813;&#20381;&#36182;KB&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;SPINDOCTOR&#65288;&#31283;&#23450;&#30340;&#25552;&#31034;&#25554;&#20540;&#30340;&#21463;&#25511;&#26415;&#35821;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;&#27169;&#26495;&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;GPT&#27169;&#22411;&#25191;&#34892;&#22522;&#22240;&#38598;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#26631;&#20934;&#23500;&#38598;&#20998;&#26512;&#30340;&#34917;&#20805;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#22240;&#21151;&#33021;&#20449;&#24687;&#26469;&#28304;&#65306;&#65288;1&#65289;&#20174;&#37492;&#23450;&#30340;&#26412;&#20307;KB&#27880;&#37322;&#20013;&#33719;&#24471;&#30340;&#32467;&#26500;&#21270;&#25991;&#26412;&#65292;&#65288;2&#65289;&#20174;&#25991;&#26412;&#25366;&#25496;&#20013;&#25512;&#26029;&#30340;&#26412;&#20307;&#26415;&#35821;&#65292;&#20197;&#21450;&#65288;3&#65289;&#30452;&#25509;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#33719;&#24471;&#30340;&#26415;&#35821;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;1813&#20010;&#22522;&#22240;&#38598;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;SPINDOCTOR&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;GPT&#27169;&#22411;&#26174;&#33879;&#25913;&#21892;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#29983;&#25104;&#20154;&#31867;&#21487;&#35835;&#30340;&#22522;&#22240;&#21151;&#33021;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular biologists frequently interpret gene lists derived from high-throughput experiments and computational analysis. This is typically done as a statistical enrichment analysis that measures the over- or under-representation of biological function terms associated with genes or their properties, based on curated assertions from a knowledge base (KB) such as the Gene Ontology (GO). Interpreting gene lists can also be framed as a textual summarization task, enabling the use of Large Language Models (LLMs), potentially utilizing scientific texts directly and avoiding reliance on a KB.  We developed SPINDOCTOR (Structured Prompt Interpolation of Natural Language Descriptions of Controlled Terms for Ontology Reporting), a method that uses GPT models to perform gene set function summarization as a complement to standard enrichment analysis. This method can use different sources of gene functional information: (1) structured text derived from curated ontological KB annotations, (2) ontol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#33021;&#37327;&#20989;&#25968;&#26550;&#26500;&#21644;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#33021;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#35745;&#31639;&#21477;&#23376;&#24471;&#20998;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.12676</link><description>&lt;p&gt;
&#25506;&#32034;&#19981;&#21516;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#19979;&#22522;&#20110;&#33021;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Energy-based Language Models with Different Architectures and Training Methods for Speech Recognition. (arXiv:2305.12676v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#33021;&#37327;&#20989;&#25968;&#26550;&#26500;&#21644;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#33021;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#35745;&#31639;&#21477;&#23376;&#24471;&#20998;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;ELM&#65289;&#36890;&#36807;&#21442;&#25968;&#21270;&#33258;&#28982;&#35821;&#21477;&#30340;&#38750;&#24402;&#19968;&#21270;&#20998;&#24067;&#19982;&#27969;&#34892;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65288;ALM&#65289;&#26377;&#26681;&#26412;&#24615;&#21306;&#21035;&#12290;&#20316;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#24212;&#29992;&#65292;ELM&#24050;&#25104;&#21151;&#22320;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#20013;&#35745;&#31639;&#21477;&#23376;&#24471;&#20998;&#65292;&#20294;&#23427;&#20204;&#37117;&#20351;&#29992;&#19981;&#22826;&#29616;&#20195;&#30340;CNN&#25110;LSTM&#32593;&#32476;&#12290;&#38543;&#30528;Transformer&#32593;&#32476;&#21644;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#21644;GPT2&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;ELMs&#30340;&#33021;&#21147;&#24050;&#32463;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#33021;&#37327;&#20989;&#25968;&#26550;&#26500;&#21644;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#30740;&#31350;&#22312;&#20197;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#39592;&#24178;&#30340;&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;ELMs&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-based language models (ELMs) parameterize an unnormalized distribution for natural sentences and are radically different from popular autoregressive language models (ALMs). As an important application, ELMs have been successfully used as a means for calculating sentence scores in speech recognition, but they all use less-modern CNN or LSTM networks. The recent progress in Transformer networks and large pretrained models such as BERT and GPT2 opens new possibility to further advancing ELMs. In this paper, we explore different architectures of energy functions and different training methods to investigate the capabilities of ELMs in rescoring for speech recognition, all using large pretrained models as backbones.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22836;&#29366;&#24577;&#31354;&#38388;&#65288;MH-SSM&#65289;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#24182;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#30340;&#26032;&#30340;&#24615;&#33021;&#65292;&#26159;&#21464;&#21387;&#22120;&#21464;&#25442;&#22120;&#30340;&#20248;&#31168;&#26367;&#20195;&#26041;&#26696;&#12290;&#21516;&#26102;, MH-SSM&#23618;&#30340;&#24341;&#20837;&#20063;&#25552;&#39640;&#20102;&#21464;&#21387;&#22120;&#22359;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#29616;&#26377;&#26368;&#26032;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.12498</link><description>&lt;p&gt;
&#22810;&#22836;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Head State Space Model for Speech Recognition. (arXiv:2305.12498v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22836;&#29366;&#24577;&#31354;&#38388;&#65288;MH-SSM&#65289;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#24182;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#30340;&#26032;&#30340;&#24615;&#33021;&#65292;&#26159;&#21464;&#21387;&#22120;&#21464;&#25442;&#22120;&#30340;&#20248;&#31168;&#26367;&#20195;&#26041;&#26696;&#12290;&#21516;&#26102;, MH-SSM&#23618;&#30340;&#24341;&#20837;&#20063;&#25552;&#39640;&#20102;&#21464;&#21387;&#22120;&#22359;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#29616;&#26377;&#26368;&#26032;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#19968;&#20123;&#23567;&#35268;&#27169;&#30340;&#24207;&#21015;&#21644;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#24050;&#32463;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#35768;&#22810;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#36229;&#36234;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22836;&#29366;&#24577;&#31354;&#38388;&#65288;MH-SSM&#65289;&#26550;&#26500;&#65292;&#23427;&#37197;&#22791;&#20102;&#29305;&#27530;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#20854;&#20013;&#24182;&#34892;&#22836;&#34987;&#25945;&#25480;&#22914;&#20309;&#22312;&#24207;&#21015;&#25968;&#25454;&#19978;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;&#20316;&#20026;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#20013;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#30452;&#25509;&#26367;&#20195;&#26041;&#26696;&#65292;&#36825;&#20010;&#26032;&#27169;&#22411;&#22312;LibriSpeech&#35821;&#38899;&#35782;&#21035;&#35821;&#26009;&#24211;&#19978;&#26174;&#33879;&#20248;&#20110;&#21464;&#21387;&#22120;&#21464;&#25442;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#21464;&#21387;&#22120;&#22359;&#20013;&#22686;&#21152;&#20102;MH-SSM&#23618;&#65292;&#31216;&#20026;Stateformer&#65292;&#19981;&#20351;&#29992;&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;LibriSpeech&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#65292;&#24320;&#21457;&#38598;&#21644;&#27979;&#35797;&#38598;&#30340;&#35789;&#38169;&#35823;&#29575;&#20998;&#21035;&#20026;1.76&#65285; / 4.37&#65285;&#21644;1.91&#65285; / 4.36&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
State space models (SSMs) have recently shown promising results on small-scale sequence and language modelling tasks, rivalling and outperforming many attention-based approaches. In this paper, we propose a multi-head state space (MH-SSM) architecture equipped with special gating mechanisms, where parallel heads are taught to learn local and global temporal dynamics on sequence data. As a drop-in replacement for multi-head attention in transformer encoders, this new model significantly outperforms the transformer transducer on the LibriSpeech speech recognition corpus. Furthermore, we augment the transformer block with MH-SSMs layers, referred to as the Stateformer, achieving state-of-the-art performance on the LibriSpeech task, with word error rates of 1.76\%/4.37\% on the development and 1.91\%/4.36\% on the test sets without using an external language model.
&lt;/p&gt;</description></item><item><title>Glot500&#26159;&#19968;&#20010;&#27700;&#24179;&#25193;&#23637;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;511&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#30456;&#27604;&#20110;XLM-R&#22522;&#32447;&#65292;Glot500&#23637;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#39640;&#36164;&#28304;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#36136;&#37327;&#30340;&#20915;&#23450;&#22240;&#32032;&#21253;&#25324;&#35821;&#26009;&#24211;&#22823;&#23567;&#12289;&#33050;&#26412;&#12289;&#30456;&#20851;&#35821;&#35328;&#30340;&#8220;&#24110;&#21161;&#8221;&#21644;&#27169;&#22411;&#30340;&#24635;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.12182</link><description>&lt;p&gt;
Glot500&#65306;&#25193;&#23637;500&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#26009;&#24211;&#21644;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages. (arXiv:2305.12182v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12182
&lt;/p&gt;
&lt;p&gt;
Glot500&#26159;&#19968;&#20010;&#27700;&#24179;&#25193;&#23637;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;511&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#30456;&#27604;&#20110;XLM-R&#22522;&#32447;&#65292;Glot500&#23637;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#39640;&#36164;&#28304;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#36136;&#37327;&#30340;&#20915;&#23450;&#22240;&#32032;&#21253;&#25324;&#35821;&#26009;&#24211;&#22823;&#23567;&#12289;&#33050;&#26412;&#12289;&#30456;&#20851;&#35821;&#35328;&#30340;&#8220;&#24110;&#21161;&#8221;&#21644;&#27169;&#22411;&#30340;&#24635;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#19968;&#30452;&#19987;&#27880;&#20110;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#32422;100&#31181;&#35821;&#35328;&#20013;&#26356;&#21152;&#20986;&#33394;&#12290;&#25105;&#20204;&#36890;&#36807;&#19981;&#26029;&#30340;&#39044;&#35757;&#32451;&#65292;&#27700;&#24179;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;Glot500-m&#65292;&#36825;&#26159;&#19968;&#20010;&#35206;&#30422;&#20102;511&#31181;&#35821;&#35328;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#20960;&#20046;&#25152;&#26377;&#35821;&#35328;&#37117;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#37325;&#35201;&#37096;&#20998;&#26159;&#25910;&#38598;&#21644;&#28165;&#29702;Glot500-c&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#25324;&#36825;511&#31181;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#65292;&#21487;&#20197;&#35753;&#25105;&#20204;&#23545;Glot500-m&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#35821;&#35328;&#19978;&#35780;&#20272;&#20102;Glot500-m&#22312;&#20116;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#19982;XLM-R&#22522;&#32447;&#30456;&#27604;&#65292;Glot500-m&#22312;&#39640;&#36164;&#28304;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#34920;&#29616;&#37117;&#26377;&#20102;&#24456;&#22823;&#30340;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#27809;&#26377;&#21333;&#19968;&#22240;&#32032;&#21487;&#20197;&#35299;&#37322;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#30456;&#21453;&#65292;&#22810;&#20010;&#22240;&#32032;&#20915;&#23450;&#20102;&#36136;&#37327;&#65292;&#21253;&#25324;&#35821;&#26009;&#24211;&#22823;&#23567;&#12289;&#33050;&#26412;&#12289;&#30456;&#20851;&#35821;&#35328;&#30340;&#8220;&#24110;&#21161;&#8221;&#20197;&#21450;&#27169;&#22411;&#30340;&#24635;&#23481;&#37327;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#65306;&#25105;&#20204;&#19981;&#24212;&#35813;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23616;&#38480;&#20110;&#19990;&#30028;&#35821;&#35328;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#32780;&#26159;&#24212;&#35813;&#35753;&#23427;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
The NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., making them better for about 100 languages. We instead scale LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM that covers 511 languages, almost all of them low-resource. An important part of this effort is to collect and clean Glot500-c, a corpus that covers these 511 languages and allows us to train Glot500-m. We evaluate Glot500-m on five diverse tasks across these languages. We observe large improvements for both high-resource and lowresource languages compared to an XLM-R baseline. Our analysis shows that no single factor explains the quality of multilingual LLM representations. Rather, a combination of factors determines quality including corpus size, script, "help" from related languages and the total capacity of the model. Our work addresses an important goal of NLP research: we should not limit NLP to a small fraction of the world's languages and instead 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312; PLMs &#20013;&#27880;&#20837;&#19978;&#19979;&#25991; NER &#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#23569;&#37327;&#31034;&#24847;&#23454;&#20363;&#21363;&#21487;&#21160;&#24577;&#35782;&#21035;&#26032;&#31867;&#22411;&#30340;&#23454;&#20307;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11038</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Learning In-context Learning for Named Entity Recognition. (arXiv:2305.11038v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312; PLMs &#20013;&#27880;&#20837;&#19978;&#19979;&#25991; NER &#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#23569;&#37327;&#31034;&#24847;&#23454;&#20363;&#21363;&#21487;&#21160;&#24577;&#35782;&#21035;&#26032;&#31867;&#22411;&#30340;&#23454;&#20307;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21463;&#21040;&#23454;&#20307;&#31867;&#22411;&#30340;&#22810;&#26679;&#24615;&#12289;&#26032;&#23454;&#20307;&#31867;&#22411;&#30340;&#20986;&#29616;&#21644;&#39640;&#36136;&#37327;&#26631;&#27880;&#30340;&#32570;&#20047;&#31561;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#19978;&#19979;&#25991;NER&#33021;&#21147;&#26377;&#25928;&#22320;&#27880;&#20837;&#21040;PLMs&#20013;&#65292;&#24182;&#19988;&#21482;&#20351;&#29992;&#23569;&#37327;&#31034;&#24847;&#23454;&#20363;&#23601;&#33021;&#21160;&#24577;&#35782;&#21035;&#26032;&#31867;&#22411;&#30340;&#23454;&#20307;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;PLMs&#24314;&#27169;&#20026;&#19968;&#20010;&#20803;&#20989;&#25968; $\mathcal{ \lambda_ {\text{instruction, demonstrations, text}}. M}$&#65292;&#24182;&#36890;&#36807;&#23558;&#26032;&#30340;&#25351;&#31034;&#21644;&#31034;&#20363;&#24212;&#29992;&#20110;PLMs&#26469;&#38544;&#21547;&#22320;&#26500;&#24314;&#26032;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#65292;&#21363; $\mathcal{ (\lambda . M) }$(instruction, demonstrations) $\to$ $\mathcal{F}$&#65292;&#20854;&#20013; $\mathcal{F}$ &#23558;&#25104;&#20026;&#19968;&#20010;&#26032;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#65292;&#21363; $\mathcal{F}$: text $\to$ entities&#12290;&#20026;&#20102;&#23558;&#19978;&#36848;&#19978;&#19979;&#25991;NER&#33021;&#21147;&#27880;&#20837;PLMs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#20989;&#25968;&#39044;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#27604;&#36739;&#65288;&#25351;&#31034;&#12289;&#31034;&#20363;&#65289;-identity&#21644;&#65288;&#25513;&#30422;&#21518;&#30340;&#25351;&#31034;&#12289;&#31034;&#20363;&#65289;-identity&#26469;&#39044;&#35757;&#32451;PLMs&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#20351;&#29992;&#23569;&#37327;&#31034;&#24847;&#23454;&#20363;&#26377;&#25928;&#22320;&#35782;&#21035;&#26032;&#31867;&#22411;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named entity recognition in real-world applications suffers from the diversity of entity types, the emergence of new entity types, and the lack of high-quality annotations. To address the above problems, this paper proposes an in-context learning-based NER approach, which can effectively inject in-context NER ability into PLMs and recognize entities of novel types on-the-fly using only a few demonstrative instances. Specifically, we model PLMs as a meta-function $\mathcal{ \lambda_ {\text{instruction, demonstrations, text}}. M}$, and a new entity extractor can be implicitly constructed by applying new instruction and demonstrations to PLMs, i.e., $\mathcal{ (\lambda . M) }$(instruction, demonstrations) $\to$ $\mathcal{F}$ where $\mathcal{F}$ will be a new entity extractor, i.e., $\mathcal{F}$: text $\to$ entities. To inject the above in-context NER ability into PLMs, we propose a meta-function pre-training algorithm, which pre-trains PLMs by comparing the (instruction, demonstration)-i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#21435;&#22122;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#20934;&#30830;&#22320;&#22312;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#20013;&#36873;&#25321;&#21487;&#20449;&#30340;&#20266;&#26631;&#31614;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#22312;DocRED&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11029</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#21435;&#22122;&#22312;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction. (arXiv:2305.11029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#21435;&#22122;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#20934;&#30830;&#22320;&#22312;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#20013;&#36873;&#25321;&#21487;&#20449;&#30340;&#20266;&#26631;&#31614;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#22312;DocRED&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#26088;&#22312;&#25512;&#26029;&#25991;&#26723;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#35821;&#20041;&#20851;&#31995;&#12290;&#36828;&#31243;&#30417;&#30563;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#33258;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#21487;&#20197;&#25552;&#39640;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19981;&#21487;&#38752;&#30340;&#20266;&#26631;&#31614;&#20250;&#24102;&#26469;&#26032;&#30340;&#22122;&#22768;&#65292;&#20363;&#22914;&#28155;&#21152;&#34394;&#20551;&#30340;&#20266;&#26631;&#31614;&#21644;&#22833;&#21435;&#27491;&#30830;&#30340;&#30417;&#30563;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#36873;&#25321;&#26377;&#25928;&#30340;&#20266;&#26631;&#31614;&#26469;&#21435;&#22122;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#20173;&#28982;&#26159;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25216;&#26415;&#26469;&#30830;&#23450;&#20266;&#26631;&#31614;&#26159;&#21542;&#21487;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#26631;&#31614;&#21435;&#22122;&#30340;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;UGDRE&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#20363;&#32423;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#27979;&#37327;&#20102;&#20855;&#26377;&#37325;&#21472;&#20851;&#31995;&#30340;&#20266;&#26631;&#31614;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#32771;&#34385;&#23454;&#20363;&#32423;&#21644;&#20851;&#31995;&#32423;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26631;&#31614;&#21435;&#22122;&#32452;&#20214;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36873;&#25321;&#21487;&#38752;&#30340;&#20266;&#26631;&#31614;&#36827;&#34892;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;DocRED&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level relation extraction (DocRE) aims to infer complex semantic relations among entities in a document. Distant supervision (DS) is able to generate massive auto-labeled data, which can improve DocRE performance. Recent works leverage pseudo labels generated by the pre-denoising model to reduce noise in DS data. However, unreliable pseudo labels bring new noise, e.g., adding false pseudo labels and losing correct DS labels. Therefore, how to select effective pseudo labels to denoise DS data is still a challenge in document-level distant relation extraction. To tackle this issue, we introduce uncertainty estimation technology to determine whether pseudo labels can be trusted. In this work, we propose a Document-level distant Relation Extraction framework with Uncertainty Guided label denoising, UGDRE. Specifically, we propose a novel instance-level uncertainty estimation method, which measures the reliability of the pseudo labels with overlapping relations. By further consider
&lt;/p&gt;</description></item><item><title>MolXPT&#26159;&#19968;&#20010;&#25991;&#26412;&#21253;&#35013;&#30340;&#32479;&#19968;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;SMILES&#20316;&#20026;&#36755;&#20837;&#65292;&#21487;&#20197;&#25552;&#39640;&#20998;&#23376;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#19988;&#20351;&#24471;&#22522;&#20110;&#38646;shot&#30340;&#20998;&#23376;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10688</link><description>&lt;p&gt;
MolXPT&#65306;&#20351;&#29992;&#25991;&#26412;&#21253;&#35013;&#20998;&#23376;&#36827;&#34892;&#29983;&#25104;&#24615;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
MolXPT: Wrapping Molecules with Text for Generative Pre-training. (arXiv:2305.10688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10688
&lt;/p&gt;
&lt;p&gt;
MolXPT&#26159;&#19968;&#20010;&#25991;&#26412;&#21253;&#35013;&#30340;&#32479;&#19968;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;SMILES&#20316;&#20026;&#36755;&#20837;&#65292;&#21487;&#20197;&#25552;&#39640;&#20998;&#23376;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#19988;&#20351;&#24471;&#22522;&#20110;&#38646;shot&#30340;&#20998;&#23376;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#24182;&#19988;&#30456;&#20851;&#25216;&#26415;&#24050;&#32463;&#34987;&#24212;&#29992;&#21040;&#20102;&#20998;&#23376;&#24314;&#27169;&#20013;&#12290;&#32771;&#34385;&#21040;&#25991;&#26412;&#26159;&#31185;&#23398;&#21457;&#29616;&#26368;&#37325;&#35201;&#30340;&#35760;&#24405;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; MolXPT&#65292;&#19968;&#20010;&#22312; SMILES &#19978;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013; SMILES &#34987;&#25991;&#26412;&#21253;&#35013;&#12290;&#31616;&#21333;&#26469;&#35828;&#65292;&#25105;&#20204;&#26816;&#27979;&#27599;&#20010;&#24207;&#21015;&#20013;&#30340;&#20998;&#23376;&#21517;&#31216;&#65292;&#24182;&#23558;&#23427;&#20204;&#26367;&#25442;&#20026;&#30456;&#24212;&#30340; SMILES&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;SMILES &#21487;&#20197;&#21033;&#29992;&#21608;&#22260;&#25991;&#26412;&#30340;&#20449;&#24687;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#20197;&#19978;&#21253;&#35013;&#30340;&#24207;&#21015;&#65292;&#26159;&#30001;&#26469;&#33258; PubMed &#30340;&#25991;&#26412;&#24207;&#21015;&#21644;&#26469;&#33258; PubChem &#30340; SMILES &#24207;&#21015;&#32452;&#25104;&#30340;&#65292;&#23427;&#20204;&#37117;&#34987;&#36755;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MolXPT &#22312; MoleculeNet &#19978;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#24378;&#22522;&#20934;&#27169;&#22411;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#22312;&#25991;&#26412;-&#20998;&#23376;&#32763;&#35793;&#20013;&#34920;&#29616;&#19982;&#26368;&#20339;&#27169;&#22411;&#30456;&#24403;&#65292;&#32780;&#20351;&#29992;&#30340;&#21442;&#25968;&#19981;&#21040;&#20854;&#19968;&#21322;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#38646;-shot&#29983;&#25104;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative pre-trained Transformer (GPT) has demonstrates its great success in natural language processing and related techniques have been adapted into molecular modeling. Considering that text is the most important record for scientific discovery, in this paper, we propose MolXPT, a unified language model of text and molecules pre-trained on SMILES (a sequence representation of molecules) wrapped by text. Briefly, we detect the molecule names in each sequence and replace them to the corresponding SMILES. In this way, the SMILES could leverage the information from surrounding text, and vice versa. The above wrapped sequences, text sequences from PubMed and SMILES sequences from PubChem are all fed into a language model for pre-training. Experimental results demonstrate that MolXPT outperforms strong baselines of molecular property prediction on MoleculeNet, performs comparably to the best model in text-molecule translation while using less than half of its parameters, and enables zero
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;NLP&#39046;&#22495;&#20869;&#23578;&#26410;&#30740;&#31350;&#30340;&#38382;&#39064;&#65306;&#24773;&#26223;&#21644;&#32454;&#33268;&#22320;&#29702;&#35299;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;PersoNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10156</link><description>&lt;p&gt;
&#38405;&#35835;&#36807;&#31243;&#20013;&#23545;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Personality Understanding of Fictional Characters during Book Reading. (arXiv:2305.10156v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;NLP&#39046;&#22495;&#20869;&#23578;&#26410;&#30740;&#31350;&#30340;&#38382;&#39064;&#65306;&#24773;&#26223;&#21644;&#32454;&#33268;&#22320;&#29702;&#35299;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;PersoNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#26159;&#38405;&#35835;&#25925;&#20107;&#30340;&#20851;&#38190;&#12290;&#38543;&#30528;&#35835;&#32773;&#19982;&#25925;&#20107;&#30340;&#20114;&#21160;&#65292;&#20182;&#20204;&#23545;&#19968;&#20010;&#20154;&#29289;&#30340;&#29702;&#35299;&#20250;&#26681;&#25454;&#26032;&#30340;&#20107;&#20214;&#21644;&#20449;&#24687;&#32780;&#28436;&#21464;&#65307;&#24182;&#19988;&#21487;&#20197;&#24863;&#30693;&#21040;&#22810;&#20010;&#31934;&#32454;&#30340;&#20010;&#24615;&#26041;&#38754;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#65306;&#24773;&#22659;&#21644;&#31934;&#32454;&#30340;&#20010;&#24615;&#29702;&#35299;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;NLP&#39046;&#22495;&#20013;&#27809;&#26377;&#24471;&#21040;&#30740;&#31350;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#27169;&#20223;&#38405;&#35835;&#36807;&#31243;&#30340;&#36866;&#24403;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;PersoNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;&#27880;&#37322;&#31574;&#30053;&#28041;&#21450;&#29992;&#22312;&#32447;&#38405;&#35835;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#31508;&#35760;&#20316;&#20026;&#21407;&#22987;&#20070;&#31821;&#30340;&#20195;&#29702;&#36827;&#34892;&#27880;&#37322;&#12290;&#23454;&#39564;&#21644;&#20154;&#20307;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26500;&#24314;&#26082;&#26377;&#25928;&#21448;&#20934;&#30830;&#65307;&#25105;&#20204;&#30340;&#20219;&#21153;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#38271;&#26399;&#30340;&#19978;&#19979;&#25991;&#20197;&#23454;&#29616;&#23545;&#26426;&#22120;&#21644;&#20154;&#31867;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/Gorov/personet_acl23&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comprehending characters' personalities is a crucial aspect of story reading. As readers engage with a story, their understanding of a character evolves based on new events and information; and multiple fine-grained aspects of personalities can be perceived. This leads to a natural problem of situated and fine-grained personality understanding. The problem has not been studied in the NLP field, primarily due to the lack of appropriate datasets mimicking the process of book reading. We present the first labeled dataset PersoNet for this problem. Our novel annotation strategy involves annotating user notes from online reading apps as a proxy for the original books. Experiments and human studies indicate that our dataset construction is both efficient and accurate; and our task heavily relies on long-term context to achieve accurate predictions for both machines and humans. The dataset is available at https://github.com/Gorov/personet_acl23.
&lt;/p&gt;</description></item><item><title>sustain.AI&#26159;&#19968;&#20010;&#26234;&#33021;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#23457;&#35745;&#24072;&#12289;&#37329;&#34701;&#25237;&#36164;&#32773;&#20197;&#21450;&#24191;&#22823;&#20844;&#20247;&#39640;&#25928;&#22320;&#20998;&#26512;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#24182;&#36890;&#36807;&#19982;GRI&#26631;&#20934;&#21305;&#37197;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#25512;&#33616;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.08711</link><description>&lt;p&gt;
sustain.AI: &#19968;&#31181;&#20998;&#26512;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
sustain.AI: a Recommender System to analyze Sustainability Reports. (arXiv:2305.08711v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08711
&lt;/p&gt;
&lt;p&gt;
sustain.AI&#26159;&#19968;&#20010;&#26234;&#33021;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#23457;&#35745;&#24072;&#12289;&#37329;&#34701;&#25237;&#36164;&#32773;&#20197;&#21450;&#24191;&#22823;&#20844;&#20247;&#39640;&#25928;&#22320;&#20998;&#26512;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#24182;&#36890;&#36807;&#19982;GRI&#26631;&#20934;&#21305;&#37197;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#25512;&#33616;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;sustain.AI&#65292;&#36825;&#26159;&#19968;&#20010;&#26234;&#33021;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#24110;&#21161;&#23457;&#35745;&#24072;&#12289;&#37329;&#34701;&#25237;&#36164;&#32773;&#20197;&#21450;&#24191;&#22823;&#20844;&#20247;&#39640;&#25928;&#22320;&#20998;&#26512;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#12290;&#35813;&#24037;&#20855;&#21033;&#29992;&#20102;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#26550;&#26500;&#65292;&#23558;&#22522;&#20110;BERT&#30340;&#32534;&#30721;&#27169;&#22359;&#19982;&#22810;&#26631;&#31614;&#20998;&#31867;&#22836;&#30456;&#32467;&#21512;&#65292;&#23558;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#20013;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#19982;&#20840;&#29699;&#25253;&#21578;&#20513;&#35758;&#65288;GRI&#65289;&#26631;&#20934;&#20013;&#30340;&#30456;&#24212;&#27861;&#24459;&#27861;&#35268;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26032;&#39062;&#30340;&#24503;&#22269;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#22987;&#32456;&#23454;&#29616;&#20102;&#19982;&#22810;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#26356;&#39640;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;sustain.AI&#24050;&#32463;&#20844;&#24320;&#22312;https://sustain.ki.nrw/&#19978;&#25552;&#20379;&#32473;&#25152;&#26377;&#20154;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present $\text{sustain.AI}$, an intelligent, context-aware recommender system that assists auditors and financial investors as well as the general public to efficiently analyze companies' sustainability reports. The tool leverages an end-to-end trainable architecture that couples a BERT-based encoding module with a multi-label classification head to match relevant text passages from sustainability reports to their respective law regulations from the Global Reporting Initiative (GRI) standards. We evaluate our model on two novel German sustainability reporting data sets and consistently achieve a significantly higher recommendation performance compared to multiple strong baselines. Furthermore, $\text{sustain.AI}$ is publicly available for everyone at https://sustain.ki.nrw/.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#20559;&#22909;&#26816;&#26597;&#34920;&#65292;&#20197;&#36229;&#36234;&#30456;&#20851;&#20998;&#26512;&#35780;&#20272;NLG&#33258;&#21160;&#25351;&#26631;&#65292;&#24182;&#20998;&#26512;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25351;&#26631;&#21450;&#20854;&#22312;&#19977;&#20010;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.08566</link><description>&lt;p&gt;
&#36229;&#36234;&#30456;&#20851;&#20998;&#26512;&#30340;NLG&#35780;&#20272;&#25351;&#26631;&#65306;&#19968;&#31181;&#32463;&#39564;&#24230;&#37327;&#20559;&#22909;&#26816;&#26597;&#34920;
&lt;/p&gt;
&lt;p&gt;
NLG Evaluation Metrics Beyond Correlation Analysis: An Empirical Metric Preference Checklist. (arXiv:2305.08566v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#20559;&#22909;&#26816;&#26597;&#34920;&#65292;&#20197;&#36229;&#36234;&#30456;&#20851;&#20998;&#26512;&#35780;&#20272;NLG&#33258;&#21160;&#25351;&#26631;&#65292;&#24182;&#20998;&#26512;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25351;&#26631;&#21450;&#20854;&#22312;&#19977;&#20010;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;NLG&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#22522;&#20110;&#26159;&#21542;&#23558;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#29992;&#20316;&#19978;&#19979;&#25991;&#25110;&#30446;&#26631;&#26469;&#35745;&#31639;&#25351;&#26631;&#65292;&#20998;&#20026;&#65288;i&#65289;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#65288;ii&#65289;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24230;&#37327;&#20559;&#22909;&#26816;&#26597;&#34920;&#20316;&#20026;&#35780;&#20272;&#33258;&#21160;&#25351;&#26631;&#22312;&#19977;&#20010;NLG&#20219;&#21153;&#20013;&#30340;&#37492;&#21035;&#21147;&#30340;&#26694;&#26550;&#65306;&#25991;&#26412;&#25688;&#35201;&#65292;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#21644;&#21463;&#25511;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we analyze NLG automatic metrics based on whether human evaluation aspect is used as context or objective to compute the metrics: (i) Task-agnostic and (ii) Human-aligned. Task-agnostic metrics, such as Perplexity, BLEU, BERTScore, are cost-effective and highly adaptable to diverse NLG tasks, yet they have a weak correlation with human. Human-aligned metrics (CTC, CtrlEval, UniEval) improves correlation level by incorporating desirable human-like qualities as training objective. However, their effectiveness at discerning system-level performance and quality of system outputs remains unclear.  We present metric preference checklist as a framework to assess the discriminative power of automatic metrics in three NLG tasks: Text Summarization, Dialogue Response Generation, and Controlled Generation. We show that multi-aspect human-aligned metric (UniEval) is not necessarily dominant over single-aspect human-aligned metrics (CTC, CtrlEval) and task-agnostic metrics (BLEU, BER
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;PALR&#30340;&#26694;&#26550;&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07622</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;LMMs&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PALR: Personalization Aware LLMs for Recommendation. (arXiv:2305.07622v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;PALR&#30340;&#26694;&#26550;&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;PALR&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#29992;&#25143;/&#29289;&#21697;&#20114;&#21160;&#20316;&#20026;&#20505;&#36873;&#26816;&#32034;&#30340;&#25351;&#23548;&#65292;&#28982;&#21518;&#37319;&#29992;&#22522;&#20110;LLMs&#30340;&#25490;&#24207;&#27169;&#22411;&#29983;&#25104;&#25512;&#33616;&#29289;&#21697;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently received significant attention for their exceptional capabilities. Despite extensive efforts in developing general-purpose LLMs that can be utilized in various natural language processing (NLP) tasks, there has been less research exploring their potential in recommender systems. In this paper, we propose a novel framework, named PALR, which aiming to combine user history behaviors (such as clicks, purchases, ratings, etc.) with LLMs to generate user preferred items. Specifically, we first use user/item interactions as guidance for candidate retrieval. Then we adopt a LLM-based ranking model to generate recommended items. Unlike existing approaches that typically adopt general-purpose LLMs for zero/few-shot recommendation testing or training on small-sized language models (with less than 1 billion parameters), which cannot fully elicit LLMs' reasoning abilities and leverage rich item side parametric knowledge, we fine-tune a 7 billion parameter
&lt;/p&gt;</description></item><item><title>BanglaBook &#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23391;&#21152;&#25289;&#35821;&#20070;&#35780;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324; 158,065 &#20010;&#26679;&#26412;&#65292;&#38024;&#23545;&#24773;&#24863;&#20998;&#26512;&#20998;&#20026;&#19977;&#20010;&#22823;&#31867;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#21462;&#20195;&#25163;&#21160;&#26500;&#24314;&#29305;&#24449;&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#26174;&#30528;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06595</link><description>&lt;p&gt;
BanglaBook: &#19968;&#31181;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#22823;&#35268;&#27169;&#23391;&#21152;&#25289;&#35821;&#20070;&#35780;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from Book Reviews. (arXiv:2305.06595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06595
&lt;/p&gt;
&lt;p&gt;
BanglaBook &#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23391;&#21152;&#25289;&#35821;&#20070;&#35780;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324; 158,065 &#20010;&#26679;&#26412;&#65292;&#38024;&#23545;&#24773;&#24863;&#20998;&#26512;&#20998;&#20026;&#19977;&#20010;&#22823;&#31867;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#21462;&#20195;&#25163;&#21160;&#26500;&#24314;&#29305;&#24449;&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#26174;&#30528;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#36153;&#32773;&#24773;&#24863;&#20998;&#26512;&#21487;&#20197;&#36890;&#36807;&#35780;&#35770;&#34920;&#36798;&#25552;&#20379;&#26377;&#20851;&#20135;&#21697;&#36136;&#37327;&#30340;&#20016;&#23500;&#35265;&#35299;&#12290;&#23613;&#31649;&#24773;&#24863;&#20998;&#26512;&#30340;&#30740;&#31350;&#22312;&#35768;&#22810;&#27969;&#34892;&#35821;&#35328;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#30456;&#20851;&#25968;&#25454;&#21644;&#36328;&#39046;&#22495;&#36866;&#24212;&#24615;&#65292;&#30456;&#23545;&#36739;&#23569;&#20851;&#27880;&#23391;&#21152;&#25289;&#35821;&#35328;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; BanglaBook&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23391;&#21152;&#25289;&#35821;&#20070;&#35780;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324; 158,065 &#20010;&#26679;&#26412;&#65292;&#20998;&#20026;&#19977;&#20010;&#22823;&#31867;&#65306;&#31215;&#26497;&#12289;&#28040;&#26497;&#21644;&#20013;&#24615;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#24182;&#20351;&#29992;&#19968;&#31995;&#21015;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24314;&#31435;&#20102;&#22522;&#32447;&#65292;&#21253;&#25324; SVM&#12289;LSTM &#21644; Bangla-BERT&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20381;&#36182;&#25163;&#21160;&#26500;&#24314;&#29305;&#24449;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#24378;&#35843;&#20102;&#22312;&#27492;&#39046;&#22495;&#38656;&#35201;&#39069;&#22806;&#30340;&#22521;&#35757;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#24773;&#24863;&#38169;&#35823;&#20998;&#31867;&#26469;&#36827;&#34892;&#28145;&#20837;&#30340;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#23391;&#21152;&#25289;&#24773;&#24863;&#20998;&#26512;&#24615;&#36136;&#30340;&#36827;&#19968;&#27493;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of consumer sentiment, as expressed through reviews, can provide a wealth of insight regarding the quality of a product. While the study of sentiment analysis has been widely explored in many popular languages, relatively less attention has been given to the Bangla language, mostly due to a lack of relevant data and cross-domain adaptability. To address this limitation, we present BanglaBook, a large-scale dataset of Bangla book reviews consisting of 158,065 samples classified into three broad categories: positive, negative, and neutral. We provide a detailed statistical analysis of the dataset and employ a range of machine learning models to establish baselines including SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial performance advantage of pre-trained models over models that rely on manually crafted features, emphasizing the necessity for additional training resources in this domain. Additionally, we conduct an in-depth error analysis by examining se
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#36328;&#35821;&#35328;ICL&#20013;&#26080;&#27861;&#23545;&#20934;&#36755;&#20837;&#36755;&#20986;&#31354;&#38388;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26500;&#24314;&#31574;&#30053;X-InSTA&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#30340;&#35821;&#22659;&#36827;&#34892;&#32534;&#30721;&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#25552;&#39640;&#36328;&#35821;&#35328;ICL&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.05940</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;LLMs&#26159;&#26356;&#22909;&#30340;&#36328;&#35821;&#35328;&#19978;&#19979;&#25991;&#23398;&#20064;&#32773;&#19982;&#23545;&#40784;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment. (arXiv:2305.05940v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05940
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#36328;&#35821;&#35328;ICL&#20013;&#26080;&#27861;&#23545;&#20934;&#36755;&#20837;&#36755;&#20986;&#31354;&#38388;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26500;&#24314;&#31574;&#30053;X-InSTA&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#30340;&#35821;&#22659;&#36827;&#34892;&#32534;&#30721;&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#25552;&#39640;&#36328;&#35821;&#35328;ICL&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#20219;&#20309;&#26799;&#24230;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#20197;&#23569;&#25968;&#26631;&#35760;&#26679;&#26412;&#20026;&#26465;&#20214;&#30340;&#27979;&#35797;&#26631;&#31614;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#25104;&#20026;&#21487;&#33021;&#12290;&#21551;&#29992;ICL&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35268;&#36991;&#22797;&#21457;&#24615;&#27880;&#37322;&#25104;&#26412;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#21069;&#36827;&#27493;&#20240;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#21482;&#26377;&#23569;&#25968;&#20960;&#39033;&#30740;&#31350;&#25506;&#31350;&#20102;&#36328;&#35821;&#35328;&#35774;&#32622;&#19979;&#30340;ICL&#65292;&#36825;&#22312;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#36716;&#31227;&#26631;&#31614;&#30693;&#35782;&#30340;&#38656;&#35201;&#19979;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;&#36328;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#30340;ICL&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36328;&#35821;&#35328;ICL&#30340;&#24773;&#20917;&#19979;&#65292;&#26222;&#36941;&#36873;&#25321;&#38543;&#26426;&#30340;&#36755;&#20837;-&#26631;&#31614;&#23545;&#26469;&#26500;&#24314;&#25552;&#31034;&#19978;&#19979;&#25991;&#30340;&#27169;&#24335;&#20005;&#37325;&#21463;&#38480;&#20110;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;&#30340;&#32570;&#20047;&#23545;&#20934;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26500;&#24314;&#31574;&#30053;&#8212;&#8212;&#36328;&#35821;&#35328;&#19978;&#19979;&#25991;&#28304;-&#30446;&#26631;&#23545;&#40784;&#65288;X-InSTA&#65289;&#12290;&#36890;&#36807;&#27880;&#20837;&#20849;&#21516;&#35757;&#32451;&#30340;&#38408;&#20540;&#20803;&#32032;&#65292;X-InSTA&#21487;&#20197;&#21516;&#26102;&#23545;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#30340;&#35821;&#22659;&#36827;&#34892;&#32534;&#30721;&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#25552;&#39640;&#36328;&#35821;&#35328;ICL&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) unfolds as large language models become capable of inferring test labels conditioned on a few labeled samples without any gradient update. ICL-enabled large language models provide a promising step forward toward bypassing recurrent annotation costs in a low-resource setting. Yet, only a handful of past studies have explored ICL in a cross-lingual setting, in which the need for transferring label-knowledge from a high-resource language to a low-resource one is immensely crucial. To bridge the gap, we provide the first in-depth analysis of ICL for cross-lingual text classification. We find that the prevalent mode of selecting random input-label pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces. To mitigate this, we propose a novel prompt construction strategy -- Cross-lingual In-context Source-Target Alignment (X-InSTA). With an injected co
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#25552;&#20986;&#35757;&#32451;&#25490;&#21517;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#36328;&#35821;&#35328;&#26816;&#32034;&#30340;&#25928;&#29575;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#20154;&#24037;&#20195;&#30721;&#20999;&#25442;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#23454;&#39564;&#34920;&#26126;&#22312;&#36328;&#35821;&#35328;&#26816;&#32034;&#21644;&#22810;&#35821;&#35328;&#26816;&#32034;&#20013;&#20250;&#24102;&#26469;&#26174;&#33879;&#25913;&#36827;&#65292;&#22312;&#19981;&#24433;&#21709;&#21333;&#35821;&#26816;&#32034;&#30340;&#22522;&#30784;&#19978;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36828;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2305.05295</link><description>&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#20154;&#24037;&#20195;&#30721;&#20999;&#25442;&#25968;&#25454;&#26469;&#25552;&#21319;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Boosting Zero-shot Cross-lingual Retrieval by Training on Artificially Code-Switched Data. (arXiv:2305.05295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05295
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#25552;&#20986;&#35757;&#32451;&#25490;&#21517;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#36328;&#35821;&#35328;&#26816;&#32034;&#30340;&#25928;&#29575;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#20154;&#24037;&#20195;&#30721;&#20999;&#25442;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#23454;&#39564;&#34920;&#26126;&#22312;&#36328;&#35821;&#35328;&#26816;&#32034;&#21644;&#22810;&#35821;&#35328;&#26816;&#32034;&#20013;&#20250;&#24102;&#26469;&#26174;&#33879;&#25913;&#36827;&#65292;&#22312;&#19981;&#24433;&#21709;&#21333;&#35821;&#26816;&#32034;&#30340;&#22522;&#30784;&#19978;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36828;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20197;&#33521;&#35821;&#20026;&#20195;&#34920;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#27169;&#22411;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#36801;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#24050;&#25104;&#20026;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#24403;&#26597;&#35810;&#21644;&#25991;&#26723;&#20197;&#19981;&#21516;&#35821;&#35328;&#23384;&#22312;&#26102;&#65292;&#38646;&#26679;&#26412;&#25490;&#21517;&#22120;&#30340;&#26377;&#25928;&#24615;&#20250;&#38477;&#20302;&#12290;&#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20154;&#24037;&#20195;&#30721;&#20999;&#25442;&#25968;&#25454;&#26469;&#35757;&#32451;&#25490;&#21517;&#27169;&#22411;&#65292;&#32780;&#25105;&#20204;&#29983;&#25104;&#36825;&#20123;&#25968;&#25454;&#26159;&#36890;&#36807;&#21033;&#29992;&#21452;&#35821;&#35789;&#34920;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#20174;&#65288;1&#65289;&#36328;&#35821;&#35328;&#35789;&#23884;&#20837;&#21644;&#65288;2&#65289;&#24179;&#34892;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#26631;&#39064;&#24471;&#20986;&#30340;&#35789;&#34920;&#12290;&#25105;&#20204;&#20351;&#29992;mMARCO&#25968;&#25454;&#38598;&#23545;&#28085;&#30422;&#21333;&#35821;IR&#65288;MoIR&#65289;&#12289;&#36328;&#35821;&#35328;IR&#65288;CLIR&#65289;&#21644;&#22810;&#35821;&#35328;IR&#65288;MLIR&#65289;&#30340;36&#31181;&#35821;&#35328;&#23545;&#30340;&#37325;&#25490;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20195;&#30721;&#20999;&#25442;&#21487;&#20197;&#22312;&#20445;&#25345;MoIR&#24615;&#33021;&#31283;&#23450;&#30340;&#21516;&#26102;&#65292;&#22312;CLIR&#20013;&#20135;&#29983;5.1 MRR@10&#30340;&#19968;&#33268;&#21644;&#26174;&#33879;&#22686;&#30410;&#65292;&#20197;&#21450;&#22312;MLIR&#20013;&#20135;&#29983;3.9 MRR@10&#30340;&#22686;&#30410;&#12290;&#20196;&#20154;&#40723;&#33310;&#30340;&#26159;&#65292;&#36828;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#22686;&#30410;&#29305;&#21035;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transferring information retrieval (IR) models from a high-resource language (typically English) to other languages in a zero-shot fashion has become a widely adopted approach. In this work, we show that the effectiveness of zero-shot rankers diminishes when queries and documents are present in different languages. Motivated by this, we propose to train ranking models on artificially code-switched data instead, which we generate by utilizing bilingual lexicons. To this end, we experiment with lexicons induced from (1) cross-lingual word embeddings and (2) parallel Wikipedia page titles. We use the mMARCO dataset to extensively evaluate reranking models on 36 language pairs spanning Monolingual IR (MoIR), Cross-lingual IR (CLIR), and Multilingual IR (MLIR). Our results show that code-switching can yield consistent and substantial gains of 5.1 MRR@10 in CLIR and 3.9 MRR@10 in MLIR, while maintaining stable performance in MoIR. Encouragingly, the gains are especially pronounced for distan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.05252</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#33050;&#26412;&#30693;&#35782;&#20197;&#36827;&#34892;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#36890;&#36807;&#36981;&#24490;&#30446;&#26631;&#23548;&#21521;&#30340;&#33050;&#26412;&#24418;&#24335;&#30340;&#36880;&#27493;&#35828;&#26126;&#26469;&#35268;&#21010;&#33258;&#24049;&#30340;&#34892;&#21160;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26469;&#20026;&#31435;&#20307;&#27963;&#21160;&#30340;&#25277;&#35937;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#36827;&#34892;&#35268;&#21010;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#22810;&#26041;&#38754;&#32422;&#26463;&#30340;&#26356;&#20855;&#20307;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#20026;&#31958;&#23615;&#30149;&#24739;&#32773;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#40092;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#24230;&#29983;&#25104;&#24182;&#36807;&#28388;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#25552;&#21462;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;CoScript&#65292;&#20854;&#20013;&#21253;&#25324;55,000&#20010;&#33050;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#22312;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;CoScript&#34987;&#35777;&#26126;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;LM&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., "make a cake"), but leaves more specific goals with multi-facet constraints understudied (e.g., "make a cake for diabetics"). In this paper, we define the task of constrained language planning for the first time. We propose an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, CoScript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;ANALOGICAL&#8221;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20197;&#20869;&#22312;&#35780;&#20272;LLMs&#22312;&#38271;&#25991;&#26412;&#31867;&#27604;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#20845;&#20010;&#22797;&#26434;&#32423;&#21035;&#30340;&#38271;&#25991;&#26412;&#31867;&#27604;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;13&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;8&#20010;LLMs&#22312;&#35821;&#20041;&#21521;&#37327;&#31354;&#38388;&#20013;&#35782;&#21035;&#31867;&#27604;&#23545;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05050</link><description>&lt;p&gt;
ANALOGICAL- &#19968;&#31181;&#26032;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#31867;&#27604;&#35780;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ANALOGICAL - A New Benchmark for Analogy of Long Text for Large Language Models. (arXiv:2305.05050v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;ANALOGICAL&#8221;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20197;&#20869;&#22312;&#35780;&#20272;LLMs&#22312;&#38271;&#25991;&#26412;&#31867;&#27604;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#20845;&#20010;&#22797;&#26434;&#32423;&#21035;&#30340;&#38271;&#25991;&#26412;&#31867;&#27604;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;13&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;8&#20010;LLMs&#22312;&#35821;&#20041;&#21521;&#37327;&#31354;&#38388;&#20013;&#35782;&#21035;&#31867;&#27604;&#23545;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#20197;&#35789;&#32423;&#21035;&#30340;&#31867;&#27604;&#20026;&#24418;&#24335;&#30340;&#31867;&#27604;&#22312;&#34913;&#37327;&#35832;&#22914;word2vec&#20043;&#31867;&#30340;&#35789;&#23884;&#20837;&#26041;&#27861;&#30340;&#36136;&#37327;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20027;&#35201;&#26681;&#25454;GLUE&#21644;SuperGLUE&#31561;&#22522;&#20934;&#30340;&#22806;&#22312;&#37327;&#24230;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#22312;LLMs&#26159;&#21542;&#33021;&#22815;&#22312;&#38271;&#25991;&#26412;&#20013;&#32472;&#21046;&#31867;&#27604;&#30340;&#26041;&#38754;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#39033;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;ANALOGICAL&#8221;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#20197;&#20845;&#20010;&#22797;&#26434;&#32423;&#21035;&#30340;&#38271;&#25991;&#26412;&#31867;&#27604;&#20998;&#31867;&#23545;LLMs&#36827;&#34892;&#20869;&#22312;&#35780;&#20272;&#65292;&#20998;&#21035;&#20026; (i)&#21333;&#35789;&#12289;(ii)&#21333;&#35789;vs&#21477;&#23376;&#12289;(iii)&#35821;&#27861;&#12289;(iv)&#21542;&#23450;&#12289;(v)&#34164;&#21547;&#21644;(vi)&#38544;&#21947;&#12290;&#21033;&#29992;13&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#19981;&#21516;&#30340;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;8&#20010;LLMs&#22312;&#35821;&#20041;&#21521;&#37327;&#31354;&#38388;&#20013;&#35782;&#21035;&#31867;&#27604;&#23545;&#30340;&#33021;&#21147;(&#20363;&#22914;&#65292;&#8220;&#25105;&#33021;&#35828;&#20004;&#31181;&#35821;&#35328;&#8221;&#24212;&#35813;&#26356;&#25509;&#36817;&#8220;&#25105;&#26159;&#21452;&#35821;&#30340;&#8221;&#65292;&#32780;&#8220;&#25105;&#21916;&#27426;&#24039;&#20811;&#21147;&#8221;&#21644;&#8220;&#25105;&#19981;&#21916;&#27426;&#24039;&#20811;&#21147;&#8221;&#24212;&#35813;&#26159;&#27491;&#20132;&#30340;)&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, analogies, in the form of word-level analogies, have played a significant role as an intrinsic measure of evaluating the quality of word embedding methods such as word2vec. Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on benchmarks such as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs can draw analogies between long texts. In this paper, we present ANALOGICAL, a new benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of long text with six levels of complexity -- (i) word, (ii) word vs. sentence, (iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using thirteen datasets and three different distance measures, we evaluate the abilities of eight LLMs in identifying analogical pairs in the semantic vector space (e.g., "I can speak two languages" should be closer to "I am bilingual" while "I like chocolate" and "I do not like chocolate" should be orthog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#8212;&#8212;&#35299;&#37322;&#24615;&#24494;&#35843;&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#22312;&#32473;&#20986;&#31572;&#26696;&#30340;&#21516;&#26102;&#29983;&#25104;&#25903;&#25345;&#35813;&#31572;&#26696;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#65292;&#26469;&#20943;&#36731;LLMs&#20381;&#36182;&#34394;&#20551;&#20851;&#32852;&#65292;&#20351;&#24471;&#27169;&#22411;&#23545;&#34394;&#20551;&#25552;&#31034;&#26356;&#21152;&#24378;&#38887;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04990</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#24494;&#35843;&#20351;&#27169;&#22411;&#23545;&#34394;&#20551;&#25552;&#31034;&#26356;&#24378;&#38887;
&lt;/p&gt;
&lt;p&gt;
Explanation-based Finetuning Makes Models More Robust to Spurious Cues. (arXiv:2305.04990v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#8212;&#8212;&#35299;&#37322;&#24615;&#24494;&#35843;&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#22312;&#32473;&#20986;&#31572;&#26696;&#30340;&#21516;&#26102;&#29983;&#25104;&#25903;&#25345;&#35813;&#31572;&#26696;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#65292;&#26469;&#20943;&#36731;LLMs&#20381;&#36182;&#34394;&#20551;&#20851;&#32852;&#65292;&#20351;&#24471;&#27169;&#22411;&#23545;&#34394;&#20551;&#25552;&#31034;&#26356;&#21152;&#24378;&#38887;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38750;&#24120;&#24378;&#22823;&#65292;&#26377;&#26102;&#20250;&#23398;&#20064;&#21040;&#26631;&#31614;&#21644;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#35299;&#37322;&#24615;&#24494;&#35843;&#20316;&#20026;&#20943;&#36731;LLMs&#20381;&#36182;&#34394;&#20551;&#20851;&#32852;&#30340;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#19982;&#26631;&#20934;&#24494;&#35843;&#21482;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#31572;&#26696;&#19981;&#21516;&#65292;&#25105;&#20204;&#24494;&#35843;&#27169;&#22411;&#20197;&#29983;&#25104;&#25903;&#25345;&#20854;&#31572;&#26696;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#20154;&#24037;&#26500;&#24314;&#30340;&#35757;&#32451;&#38598;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#35813;&#35757;&#32451;&#38598;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#30340;&#34394;&#20551;&#25552;&#31034;&#65292;&#24182;&#22312;&#27809;&#26377;&#36825;&#20123;&#25552;&#31034;&#30340;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#19982;&#26631;&#20934;&#24494;&#35843;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#26041;&#38754;&#20351;&#27169;&#22411;&#26497;&#20854;&#24378;&#38887;&#65306;ComVE&#65288;+1.2&#65289;&#65292;CREAK&#65288;+9.1&#65289;&#65292;e-SNLI&#65288;+15.4&#65289;&#21644;SBIC&#65288;+6.5&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#37322;&#21516;&#26679;&#26377;&#25928;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data. We propose explanation-based finetuning as a novel and general approach to mitigate LLMs' reliance on spurious correlations. Unlike standard finetuning where the model only predicts the answer given the input, we finetune the model to additionally generate a free-text explanation supporting its answer. To evaluate our method, we finetune the model on artificially constructed training sets containing different types of spurious cues, and test it on a test set without these cues. Compared to standard finetuning, our method makes models remarkably more robust against spurious cues in terms of accuracy drop across four classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC (+6.5). Moreover, our method works equally well with explanations generated by the model, implyin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#21010;&#21644;&#35299;&#20915;&#30340;&#25552;&#31034;&#26041;&#27861;&#26469;&#25913;&#21892;&#38646;&#26679;&#26412;&#24605;&#32771;&#38142;&#25512;&#29702;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#21046;&#23450;&#35745;&#21010;&#23558;&#20219;&#21153;&#21010;&#20998;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#25353;&#35745;&#21010;&#25191;&#34892;&#23376;&#20219;&#21153;&#65307;&#23558;&#36755;&#20837;&#25552;&#31034;&#25193;&#23637;&#21040;&#21253;&#25324;&#31616;&#21333;&#31639;&#26415;&#35745;&#31639;&#30340;&#31034;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#20102;&#38646;&#26679;&#26412;-CoT&#12290;</title><link>http://arxiv.org/abs/2305.04091</link><description>&lt;p&gt;
&#35745;&#21010;&#21644;&#35299;&#20915;&#25552;&#31034;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#21892;&#38646;&#26679;&#26412;&#24605;&#32771;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models. (arXiv:2305.04091v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#21010;&#21644;&#35299;&#20915;&#30340;&#25552;&#31034;&#26041;&#27861;&#26469;&#25913;&#21892;&#38646;&#26679;&#26412;&#24605;&#32771;&#38142;&#25512;&#29702;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#21046;&#23450;&#35745;&#21010;&#23558;&#20219;&#21153;&#21010;&#20998;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#25353;&#35745;&#21010;&#25191;&#34892;&#23376;&#20219;&#21153;&#65307;&#23558;&#36755;&#20837;&#25552;&#31034;&#25193;&#23637;&#21040;&#21253;&#25324;&#31616;&#21333;&#31639;&#26415;&#35745;&#31639;&#30340;&#31034;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#20102;&#38646;&#26679;&#26412;-CoT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#22810;&#27493;&#39588;&#25512;&#29702;&#20219;&#21153;&#65292;&#23569;&#26679;&#26412;&#30340;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#21253;&#25324;&#19968;&#20123;&#25163;&#24037;&#21046;&#20316;&#30340;&#36880;&#27493;&#25512;&#29702;&#28436;&#31034;&#65292;&#20351;LLMs&#33021;&#22815;&#26126;&#30830;&#29983;&#25104;&#25512;&#29702;&#27493;&#39588;&#24182;&#25552;&#39640;&#20854;&#25512;&#29702;&#20219;&#21153;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#28040;&#38500;&#25163;&#21160;&#21171;&#21160;&#65292;&#38646;&#26679;&#26412;&#24605;&#32500;&#38142;&#23558;&#30446;&#26631;&#38382;&#39064;&#38472;&#36848;&#19982;&#8220;&#35753;&#25105;&#20204;&#36880;&#27493;&#24605;&#32771;&#8221;&#36830;&#25509;&#36215;&#26469;&#20316;&#20026;&#36755;&#20837;&#25552;&#31034;LLMs&#12290;&#23613;&#31649;&#38646;&#26679;&#26412;-CoT&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20173;&#23384;&#22312;&#35745;&#31639;&#38169;&#35823;&#12289;&#32570;&#22833;&#27493;&#39588;&#38169;&#35823;&#21644;&#35821;&#20041;&#35823;&#35299;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#32570;&#22833;&#27493;&#39588;&#38169;&#35823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35745;&#21010;&#21644;&#35299;&#20915;&#65288;PS&#65289;&#25552;&#31034;&#12290;&#23427;&#21253;&#21547;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#39318;&#20808;&#65292;&#21046;&#23450;&#35745;&#21010;&#23558;&#25972;&#20010;&#20219;&#21153;&#21010;&#20998;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#25353;&#29031;&#35745;&#21010;&#25191;&#34892;&#23376;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#35745;&#31639;&#38169;&#35823;&#24182;&#25552;&#39640;&#29983;&#25104;&#25512;&#29702;&#27493;&#39588;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#23558;&#36755;&#20837;&#25552;&#31034;&#25193;&#23637;&#21040;&#21253;&#25324;&#31616;&#21333;&#31639;&#26415;&#35745;&#31639;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PS&#25552;&#31034;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#32988;&#36807;&#20102;&#38646;&#26679;&#26412;CoT&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual effort, Zero-shot-CoT concatenates the target problem statement with "Let's think step by step" as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#25191;&#34892;&#29983;&#25104;&#30340;&#20195;&#30721;&#24182;&#23558;&#25191;&#34892;&#32467;&#26524;&#21253;&#21547;&#22312;&#22312;&#27880;&#37322;&#20013;&#26469;&#20248;&#21270;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#30340;&#20195;&#30721;&#36136;&#37327;&#65292;&#36890;&#36807;&#19982;&#20061;&#20010;&#19981;&#21516;&#30340;LLMs&#36827;&#34892;&#27604;&#36739;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#22312;&#20004;&#20010;&#31454;&#25216;&#32534;&#31243;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04087</link><description>&lt;p&gt;
&#33258;&#25105;&#32534;&#36753;&#65306;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#30340;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;
&lt;/p&gt;
&lt;p&gt;
Self-Edit: Fault-Aware Code Editor for Code Generation. (arXiv:2305.04087v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#25191;&#34892;&#29983;&#25104;&#30340;&#20195;&#30721;&#24182;&#23558;&#25191;&#34892;&#32467;&#26524;&#21253;&#21547;&#22312;&#22312;&#27880;&#37322;&#20013;&#26469;&#20248;&#21270;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#30340;&#20195;&#30721;&#36136;&#37327;&#65292;&#36890;&#36807;&#19982;&#20061;&#20010;&#19981;&#21516;&#30340;LLMs&#36827;&#34892;&#27604;&#36739;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#22312;&#20004;&#20010;&#31454;&#25216;&#32534;&#31243;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#20013;&#29983;&#25104;&#20195;&#30721;&#30340;&#33021;&#21147;&#24050;&#32463;&#24471;&#21040;&#35777;&#26126;&#65292;&#20294;&#30001;&#20110;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#21463;&#20154;&#31867;&#32534;&#31243;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#25191;&#34892;&#32467;&#26524;&#26469;&#25552;&#39640;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#30340;&#20195;&#30721;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#38382;&#39064;&#20013;&#25552;&#20379;&#30340;&#31034;&#20363;&#27979;&#35797;&#29992;&#20363;&#19978;&#25191;&#34892;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#24182;&#23558;&#25191;&#34892;&#32467;&#26524;&#21253;&#21547;&#22312;&#34917;&#20805;&#24615;&#27880;&#37322;&#20013;&#12290;&#21033;&#29992;&#36825;&#20010;&#27880;&#37322;&#20316;&#20026;&#25351;&#23548;&#65292;&#25105;&#20204;&#30340;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;&#29992;&#20110;&#32416;&#27491;&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#31454;&#25216;&#32534;&#31243;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20061;&#20010;&#19981;&#21516;&#30340;LLMs&#12290;&#19982;&#30452;&#25509;&#20174;LLMs&#29983;&#25104;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;APPS-dev&#19978;&#23558;pass@1&#30340;&#24179;&#22343;&#20540;&#25552;&#39640;89&#65285;&#65292;&#22312;APPS-test&#19978;&#25552;&#39640;31&#65285;&#65292;&#22312;HumanEval&#19978;&#25552;&#39640;48&#65285;&#65292;&#36229;&#36807;&#20102;&#20061;&#20010;&#27969;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;LLMs&#65292;&#21442;&#25968;&#22823;&#23567;&#33539;&#22260;&#20026;110M-t&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89\% on APPS-dev, 31\% on APPS-test, and 48\% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26816;&#32034;&#22120;&#21487;&#20197;&#22312;&#25968;&#25454;&#38598;&#38388;&#37325;&#22797;&#20351;&#29992;&#65292;&#25903;&#25345;&#38024;&#23545;&#30446;&#26631;&#39046;&#22495;&#30340;&#28789;&#27963;&#25216;&#33021;&#37197;&#32622;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22810;&#20010; ODQA &#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#26368;&#26032;&#39062;&#30340;&#24494;&#35843;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03130</link><description>&lt;p&gt;
Chain-of-Skills: &#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Skills: A Configurable Model for Open-domain Question Answering. (arXiv:2305.03130v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26816;&#32034;&#22120;&#21487;&#20197;&#22312;&#25968;&#25454;&#38598;&#38388;&#37325;&#22797;&#20351;&#29992;&#65292;&#25903;&#25345;&#38024;&#23545;&#30446;&#26631;&#39046;&#22495;&#30340;&#28789;&#27963;&#25216;&#33021;&#37197;&#32622;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22810;&#20010; ODQA &#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#26368;&#26032;&#39062;&#30340;&#24494;&#35843;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#30340;&#30693;&#35782;&#23494;&#38598;&#20219;&#21153;&#20013;&#65292;&#22914;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#65292;&#26816;&#32034;&#27169;&#22411;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#20214;&#12290;&#30001;&#20110;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#26377;&#30528;&#19981;&#21516;&#30340;&#26816;&#32034;&#25216;&#33021;&#65292;&#36817;&#26399;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#23450;&#21046;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26816;&#32034;&#22120;&#65292;&#20854;&#20013;&#21508;&#20010;&#27169;&#22359;&#23545;&#24212;&#20110;&#21487;&#20197;&#22312;&#25968;&#25454;&#38598;&#20043;&#38388;&#37325;&#22797;&#20351;&#29992;&#30340;&#20851;&#38190;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25903;&#25345;&#22522;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#28789;&#27963;&#25216;&#33021;&#37197;&#32622;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#20026;&#20102;&#20943;&#36731;&#20219;&#21153;&#24178;&#25200;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21463;&#31232;&#30095; Transformer &#21551;&#21457;&#30340;&#26032;&#22411;&#27169;&#22359;&#21270;&#21442;&#25968;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#32500;&#22522;&#30334;&#31185;&#19978;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#22810;&#20010; ODQA &#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20855;&#26377;&#22810;&#20219;&#21153;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38646;&#26679;&#20363;&#35780;&#20272;&#20013;&#20248;&#20110;&#26368;&#36817;&#30340;&#33258;&#25105;&#30417;&#30563;&#26816;&#32034;&#22120;&#65292;&#24182;&#22312; NQ&#12289;HotpotQA &#21644; OTT-QA &#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24494;&#35843;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The retrieval model is an indispensable component for real-world knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As separate retrieval skills are annotated for different datasets, recent work focuses on customized methods, limiting the model transferability and scalability. In this work, we propose a modular retriever where individual modules correspond to key skills that can be reused across datasets. Our approach supports flexible skill configurations based on the target domain to boost performance. To mitigate task interference, we design a novel modularization parameterization inspired by sparse Transformer. We demonstrate that our model can benefit from self-supervised pretraining on Wikipedia and fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our approach outperforms recent self-supervised retrievers in zero-shot evaluations and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA.
&lt;/p&gt;</description></item><item><title>PeaCoK&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35282;&#33394;&#24120;&#35782;&#30693;&#35782;&#22270;&#65292;&#21253;&#21547;&#32422;10&#19975;&#20010;&#32463;&#36807;&#20154;&#31867;&#39564;&#35777;&#30340;&#35282;&#33394;&#20107;&#23454;&#12290;&#35813;&#30693;&#35782;&#22270;&#23637;&#29616;&#20102;&#22312;&#20197;&#21069;&#30340;&#20154;&#31867;&#20132;&#20114;&#34892;&#20026;&#30740;&#31350;&#20013;&#30830;&#23450;&#30340;&#20116;&#20010;&#35282;&#33394;&#30693;&#35782;&#32500;&#24230;&#65292;&#24182;&#21306;&#20998;&#20102;&#24120;&#35782;&#21644;&#24773;&#24863;&#23618;&#38754;&#12290;</title><link>http://arxiv.org/abs/2305.02364</link><description>&lt;p&gt;
PeaCoK: &#32500;&#25345;&#19968;&#33268;&#24182;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#36848;&#25152;&#38656;&#30340;&#35282;&#33394;&#24120;&#35782;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives. (arXiv:2305.02364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02364
&lt;/p&gt;
&lt;p&gt;
PeaCoK&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35282;&#33394;&#24120;&#35782;&#30693;&#35782;&#22270;&#65292;&#21253;&#21547;&#32422;10&#19975;&#20010;&#32463;&#36807;&#20154;&#31867;&#39564;&#35777;&#30340;&#35282;&#33394;&#20107;&#23454;&#12290;&#35813;&#30693;&#35782;&#22270;&#23637;&#29616;&#20102;&#22312;&#20197;&#21069;&#30340;&#20154;&#31867;&#20132;&#20114;&#34892;&#20026;&#30740;&#31350;&#20013;&#30830;&#23450;&#30340;&#20116;&#20010;&#35282;&#33394;&#30693;&#35782;&#32500;&#24230;&#65292;&#24182;&#21306;&#20998;&#20102;&#24120;&#35782;&#21644;&#24773;&#24863;&#23618;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#25345;&#19968;&#33268;&#19988;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#36848;&#38656;&#35201;&#23545;&#35805;&#25110;&#25925;&#20107;&#20195;&#29702;&#20154;&#29702;&#35299;&#35828;&#35805;&#32773;&#25110;&#21548;&#20247;&#30340;&#35282;&#33394;&#22914;&#20309;&#19982;&#21465;&#36848;&#30456;&#20851;&#32852;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#20123;&#20195;&#29702;&#20154;&#24517;&#39035;&#25512;&#26029;&#20182;&#20204;&#21548;&#20247;&#30340;&#35282;&#33394;&#65292;&#20197;&#20135;&#29983;&#31526;&#21512;&#20182;&#20204;&#20852;&#36259;&#30340;&#38472;&#36848;&#12290;&#20182;&#20204;&#36824;&#24517;&#39035;&#23398;&#20064;&#22312;&#25972;&#20010;&#21465;&#36848;&#20013;&#20445;&#25345;&#19968;&#33268;&#30340;&#35828;&#35805;&#32773;&#35282;&#33394;&#65292;&#20197;&#20415;&#20182;&#20204;&#30340;&#23545;&#31561;&#26041;&#24863;&#21040;&#21442;&#19982;&#20854;&#20013;&#24182;&#19988;&#26159;&#19968;&#27573;&#36924;&#30495;&#30340;&#23545;&#35805;&#25110;&#25925;&#20107;&#12290;&#28982;&#32780;&#65292;&#35282;&#33394;&#26159;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#30340;&#65306;&#23427;&#20204;&#21253;&#21547;&#22823;&#37327;&#20016;&#23500;&#30340;&#30456;&#20114;&#20851;&#32852;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#24378;&#22823;&#30340;&#19968;&#33324;&#21465;&#36848;&#31995;&#32479;&#32780;&#35328;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65288;&#20363;&#22914;&#65292;&#27468;&#25163;&#25797;&#38271;&#21809;&#27468;&#65292;&#24182;&#21487;&#33021;&#26366;&#21442;&#21152;&#36807;&#38899;&#20048;&#23398;&#38498;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#35282;&#33394;&#24120;&#35782;&#30693;&#35782;&#22270;PeaCoK&#65292;&#20854;&#20013;&#21253;&#21547;&#32422;10&#19975;&#20010;&#32463;&#36807;&#20154;&#31867;&#39564;&#35777;&#30340;&#35282;&#33394;&#20107;&#23454;&#12290;&#25105;&#20204;&#30340;&#30693;&#35782;&#22270;&#34920;&#29616;&#20102;&#22312;&#20197;&#21069;&#30340;&#20154;&#31867;&#20132;&#20114;&#34892;&#20026;&#30740;&#31350;&#20013;&#30830;&#23450;&#30340;&#20116;&#20010;&#35282;&#33394;&#30693;&#35782;&#32500;&#24230;&#65292;&#24182;&#21306;&#20998;&#20102;&#24120;&#35782;&#21644;&#24773;&#24863;&#23618;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sustaining coherent and engaging narratives requires dialogue or storytelling agents to understand how the personas of speakers or listeners ground the narrative. Specifically, these agents must infer personas of their listeners to produce statements that cater to their interests. They must also learn to maintain consistent speaker personas for themselves throughout the narrative, so that their counterparts feel involved in a realistic conversation or story.  However, personas are diverse and complex: they entail large quantities of rich interconnected world knowledge that is challenging to robustly represent in general narrative systems (e.g., a singer is good at singing, and may have attended conservatoire). In this work, we construct a new large-scale persona commonsense knowledge graph, PeaCoK, containing ~100K human-validated persona facts. Our knowledge graph schematizes five dimensions of persona knowledge identified in previous studies of human interactive behaviours, and disti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#21387;&#32553;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#20197;&#36866;&#24212;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#20266;&#30446;&#26631;&#35757;&#32451;&#25216;&#26415;&#38024;&#23545;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02031</link><description>&lt;p&gt;
&#31995;&#32479;&#30740;&#31350;&#22522;&#20110;&#20266;&#30446;&#26631;&#35757;&#32451;&#30340;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training. (arXiv:2305.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#21387;&#32553;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#20197;&#36866;&#24212;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#20266;&#30446;&#26631;&#35757;&#32451;&#25216;&#26415;&#38024;&#23545;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#12290;&#26412;&#25991;&#30740;&#31350;&#21387;&#32553;&#36825;&#20123;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#36825;&#23545;&#20110;&#26381;&#21153;&#25968;&#30334;&#19975;&#29992;&#25143;&#30340;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#25216;&#26415;&#65292;&#20854;&#20013;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#23398;&#20064;&#27169;&#20223;&#22823;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#20351;&#24471;&#21487;&#20197;&#20174;&#25945;&#24072;&#21521;&#23398;&#29983;&#20256;&#36882;&#30693;&#35782;&#12290;&#19982;&#20043;&#21069;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#38024;&#23545;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#20248;&#21270;&#27169;&#22411;&#12290;&#36890;&#24120;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#38500;&#20102;&#26377;&#26631;&#35760;&#25968;&#25454;&#22806;&#65292;&#36824;&#26377;&#22823;&#37327;&#30340;&#26410;&#26631;&#35760;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#65292;&#36825;&#23545;&#20110;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#33719;&#24471;&#39640;&#21387;&#32553;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#29616;&#23454;&#30340;&#20551;&#35774;&#19979;&#65292;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#33976;&#39311;&#30740;&#31350;&#65292;&#24182;&#35752;&#35770;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#33976;&#39311;&#30340;&#29305;&#27530;&#29305;&#24449;&#65292;&#23588;&#20854;&#26159;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#31995;&#21015;&#20266;&#30446;&#26631;&#35757;&#32451;&#65288;PTT&#65289;&#25216;&#26415;&#65292;&#32531;&#35299;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern Natural Language Generation (NLG) models come with massive computational and storage requirements. In this work, we study the potential of compressing them, which is crucial for real-world applications serving millions of users. We focus on Knowledge Distillation (KD) techniques, in which a small student model learns to imitate a large teacher model, allowing to transfer knowledge from the teacher to the student. In contrast to much of the previous work, our goal is to optimize the model for a specific NLG task and a specific dataset. Typically, in real-world applications, in addition to labeled data there is abundant unlabeled task-specific data, which is crucial for attaining high compression rates via KD. In this work, we conduct a systematic study of task-specific KD techniques for various NLG tasks under realistic assumptions. We discuss the special characteristics of NLG distillation and particularly the exposure bias problem. Following, we derive a family of Pseudo-Target
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21253;&#21547;&#30495;&#23454;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;Dungeons &amp; Dragons&#23454;&#38469;&#28216;&#25103;&#25968;&#25454;&#38598;FIREBALL&#65292;&#23427;&#21487;&#20197;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#20197;&#20351;&#29992;FIREBALL&#20013;&#30340;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#26469;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#28216;&#25103;&#22238;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.01528</link><description>&lt;p&gt;
FIREBALL&#65306;&#19968;&#20221;&#21253;&#21547;&#32467;&#26500;&#21270;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;Dungeons &amp; Dragons&#23454;&#38469;&#28216;&#25103;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information. (arXiv:2305.01528v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21253;&#21547;&#30495;&#23454;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;Dungeons &amp; Dragons&#23454;&#38469;&#28216;&#25103;&#25968;&#25454;&#38598;FIREBALL&#65292;&#23427;&#21487;&#20197;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#20197;&#20351;&#29992;FIREBALL&#20013;&#30340;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#26469;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#28216;&#25103;&#22238;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Dungeons &amp; Dragons&#65288;D&#65286;D&#65289;&#26159;&#19968;&#27454;&#26700;&#38754;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#65292;&#20854;&#29609;&#23478;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#21644;&#38544;&#34255;&#30340;&#29366;&#24577;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25317;&#26377;&#29366;&#24577;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#28216;&#25103;&#22238;&#21512;&#27604;&#20165;&#20351;&#29992;&#23545;&#35805;&#21382;&#21490;&#30340;LLMs&#26356;&#20855;&#39640;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20351;&#29992;&#30340;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#26159;&#21551;&#21457;&#24335;&#21019;&#24314;&#30340;&#65292;&#24182;&#19981;&#26159;&#30495;&#27491;&#30340;&#40644;&#37329;&#26631;&#20934;&#28216;&#25103;&#29366;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FIREBALL&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#30495;&#23454;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;Discord&#30340;&#36817;25,000&#20010;&#30495;&#23454;D&#65286;D&#28216;&#25103;&#20250;&#35805;&#12290;&#25105;&#20204;&#35760;&#24405;&#20102;&#20351;&#29992;Avrae&#26426;&#22120;&#20154;&#30340;&#29609;&#23478;&#30340;&#28216;&#25103;&#20250;&#35805;&#65292;&#35813;&#26426;&#22120;&#20154;&#26159;&#20026;&#20102;&#24110;&#21161;&#20154;&#20204;&#22312;&#32447;&#29609;D&#65286;D&#32780;&#24320;&#21457;&#30340;&#65292;&#24182;&#25429;&#33719;&#20102;&#35821;&#35328;&#12289;&#28216;&#25103;&#21629;&#20196;&#21644;&#22522;&#30784;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;Avrae&#29366;&#24577;&#20449;&#24687;&#65292;FIREBALL&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#30340;&#36136;&#37327;&#35780;&#21028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LLMs&#21487;&#20197;&#29983;&#25104;&#8230;
&lt;/p&gt;
&lt;p&gt;
Dungeons &amp; Dragons (D&amp;D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information. Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone. However, previous work used game state information that was heuristically created and was not a true gold standard game state. We present FIREBALL, a large dataset containing nearly 25,000 unique sessions from real D\&amp;D gameplay on Discord with true game state info. We recorded game play sessions of players who used the Avrae bot, which was developed to aid people in playing D&amp;D online, capturing language, game commands and underlying game state information. We demonstrate that FIREBALL can improve natural language generation (NLG) by using Avrae state information, improving both automated metrics and human judgments of quality. Additionally, we show that LLMs can generate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#26694;&#26550;&#65292;&#36890;&#36807;&#22238;&#35793;&#26041;&#27861;&#25552;&#39640;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;</title><link>http://arxiv.org/abs/2304.00483</link><description>&lt;p&gt;
&#19968;&#31181;&#25913;&#36827;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#20013;&#24515;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Data-centric Framework for Improving Domain-specific Machine Reading Comprehension Datasets. (arXiv:2304.00483v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#26694;&#26550;&#65292;&#36890;&#36807;&#22238;&#35793;&#26041;&#27861;&#25552;&#39640;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#30340;&#36136;&#37327;&#20302;&#21487;&#33021;&#20250;&#23548;&#33268;&#39640;&#39118;&#38505;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#38382;&#39064;&#12290;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#24378;&#35843;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#23545;&#36890;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35757;&#32451;&#20197;&#21450;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#37117;&#26159;&#24517;&#38656;&#30340;&#65292;&#30001;&#20110;&#21560;&#24341;&#22823;&#37327;&#39046;&#22495;&#19987;&#23478;&#21442;&#19982;&#21019;&#24314;&#25968;&#25454;&#38598;&#20250;&#24456;&#26114;&#36149;&#65292;&#22240;&#27492;&#39046;&#22495;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#30340;&#39640;&#36136;&#37327;&#23545;&#20110;&#27169;&#22411;&#24615;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#21407;&#22987;&#25968;&#25454;&#38598;&#25968;&#25454;&#36136;&#37327;&#30340;&#26694;&#26550;&#65292;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#22235;&#20010;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20351;&#29992;&#22238;&#35793;&#26469;&#25552;&#39640;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#22312;BioASQ&#25968;&#25454;&#38598;&#19978;&#23545;&#26816;&#32034;/&#35835;&#32773;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#26368;&#22810;33&#65285;/40&#65285;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-quality data can cause downstream problems in high-stakes applications. Data-centric approach emphasizes on improving dataset quality to enhance model performance. High-quality datasets are needed for general-purpose Large Language Models (LLMs) training, as well as for domain-specific models, which are usually small in size as it is costly to engage a large number of domain experts for their creation. Thus, it is vital to ensure high-quality domain-specific training data. In this paper, we propose a framework for enhancing the data quality of original datasets. We applied the proposed framework to four biomedical datasets and showed relative improvement of up to 33%/40% for fine-tuning of retrieval/reader models on the BioASQ dataset when using back translation to enhance the original dataset quality.
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#26159;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#23398;&#20064;&#25110;&#21152;&#24378;&#23398;&#20064;&#30340;LLMs&#21021;&#22987;&#36755;&#20986;&#20248;&#21270;&#26041;&#27861;&#65292;&#20248;&#20110;&#30452;&#25509;&#29983;&#25104;&#65292;&#34987;&#35777;&#23454;&#22312;7&#20010;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.17651</link><description>&lt;p&gt;
&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#65306;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#23398;&#20064;&#25110;&#21152;&#24378;&#23398;&#20064;&#30340;LM&#25913;&#36827;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Self-Refine: Iterative Refinement with Self-Feedback. (arXiv:2303.17651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17651
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#26159;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#23398;&#20064;&#25110;&#21152;&#24378;&#23398;&#20064;&#30340;LLMs&#21021;&#22987;&#36755;&#20986;&#20248;&#21270;&#26041;&#27861;&#65292;&#20248;&#20110;&#30452;&#25509;&#29983;&#25104;&#65292;&#34987;&#35777;&#23454;&#22312;7&#20010;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19981;&#24635;&#26159;&#33021;&#22312;&#31532;&#19968;&#27425;&#33391;&#22909;&#22320;&#35299;&#20915;&#29983;&#25104;&#38382;&#39064;&#65288;&#22914;&#25688;&#35201;&#12289;&#31572;&#26696;&#12289;&#35299;&#37322;&#31561;&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#65288;SELF-REFINE&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#21453;&#39304;&#21644;&#31934;&#28860;&#30456;&#20284;&#22320;&#20248;&#21270;LLMs&#30340;&#21021;&#22987;&#36755;&#20986;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#65306;&#20351;&#29992;LLM&#29983;&#25104;&#36755;&#20986;&#65292;&#28982;&#21518;&#20801;&#35768;&#21516;&#19968;&#27169;&#22411;&#25552;&#20379;&#20854;&#33258;&#36523;&#36755;&#20986;&#30340;&#22810;&#26041;&#38754;&#21453;&#39304;&#65292;&#26368;&#21518;&#21033;&#29992;&#21453;&#39304;&#20351;&#30456;&#21516;&#27169;&#22411;&#31934;&#28860;&#20808;&#21069;&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#36845;&#20195;&#31934;&#28860;&#26694;&#26550;&#19982;&#26089;&#26399;&#24037;&#20316;&#19981;&#21516;&#65292;&#26080;&#38656;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#25110;&#21152;&#24378;&#23398;&#20064;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#21333;&#20010;LLM&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#23545;&#19971;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#33539;&#22260;&#20174;&#35780;&#35770;&#37325;&#20889;&#21040;&#25968;&#23398;&#25512;&#29702;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#30452;&#25509;&#29983;&#25104;&#12290;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;SELF-REFINE&#29983;&#25104;&#30340;&#36755;&#20986;&#34987;&#20154;&#31867;&#21644;&#33258;&#21160;&#21270;&#25351;&#26631;&#20248;&#20808;&#20110;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#30452;&#25509;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like people, LLMs do not always generate the best text for a given generation problem on their first try (e.g., summaries, answers, explanations). Just as people then refine their text, we introduce SELF-REFINE, a framework for similarly improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an output using an LLM, then allow the same model to provide multi-aspect feedback for its own output; finally, the same model refines its previously generated output given its own feedback. Unlike earlier work, our iterative refinement framework does not require supervised training data or reinforcement learning, and works with a single LLM. We experiment with 7 diverse tasks, ranging from review rewriting to math reasoning, demonstrating that our approach outperforms direct generation. In all tasks, outputs generated with SELF-REFINE are preferred by humans and by automated metrics over those generated directly with GPT-3.5 and GPT-4, improving
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#37327;&#21270;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#32467;&#26524;&#21457;&#29616;&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#24456;&#37325;&#35201;&#65292;&#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2303.08302</link><description>&lt;p&gt;
&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study on Post-Training Quantization for Large Language Models. (arXiv:2303.08302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#37327;&#21270;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#32467;&#26524;&#21457;&#29616;&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#24456;&#37325;&#35201;&#65292;&#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#26159;&#19968;&#31181;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#23384;&#28040;&#32791;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#30340;&#26435;&#34913;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#19981;&#21516;&#37327;&#21270;&#26041;&#26696;&#12289;&#19981;&#21516;&#27169;&#22411;&#26063;&#12289;&#19981;&#21516;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12289;&#19981;&#21516;&#37327;&#21270;&#20301;&#31934;&#24230;&#31561;&#30340;&#24433;&#21709;&#30340;&#20840;&#38754;&#30740;&#31350;&#20173;&#32570;&#22833;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#36825;&#20123;&#32452;&#20214;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;(1)&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;(&#32780;&#19981;&#26159;&#26420;&#32032;&#30340;&#26368;&#36817;&#33293;&#20837;&#37327;&#21270;)&#26159;&#23454;&#29616;&#33391;&#22909;&#31934;&#24230;&#30340;&#24517;&#35201;&#26465;&#20214;&#65307;(2) &#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#65288;&#22914;5&#20301;&#65289;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#65288;&#22914;4&#20301;&#65289;&#65288;&#20854;&#26377;&#25928;&#20301;&#25968;&#19982;5&#20301;&#30456;&#20284;&#65289;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#65292;&#24182;&#30041;&#19979;&#26410;&#26469;&#26426;&#20250;&#21644;&#31995;&#32479;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (\ptq) had been recently shown as a compromising method to reduce the memory consumption and/or compute cost for large language models. However, a comprehensive study about the effect of different quantization schemes, different model families, different \ptq methods, different quantization bit precision, etc, is still missing. In this work, we provide an extensive study on those components over tens of thousands of zero-shot experiments. Our results show that (1) Fine-grained quantization and \ptq methods (instead of naive round-to-nearest quantization) are necessary to achieve good accuracy and (2) Higher bits (e.g., 5 bits) with coarse-grained quantization is more powerful than lower bits (e.g., 4 bits) with very fine-grained quantization (whose effective bits is similar to 5-bits). We also present recommendations about how to utilize quantization for \llms with different sizes, and leave suggestions of future opportunities and system work that are not res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;BERT&#30340;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;BLINKout&#65292;&#36890;&#36807;&#19982;&#29305;&#27530;NIL&#23454;&#20307;&#21305;&#37197;&#26469;&#35782;&#21035;&#27809;&#26377;&#30456;&#24212;KB&#23454;&#20307;&#30340;&#23454;&#20307;&#25552;&#21450;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.07189</link><description>&lt;p&gt;
&#25581;&#31034;&#26410;&#30693;&#65306;&#22522;&#20110;&#23454;&#20307;&#38142;&#25509;&#30340;&#30693;&#35782;&#24211;&#22806;&#25552;&#21450;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking. (arXiv:2302.07189v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;BERT&#30340;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;BLINKout&#65292;&#36890;&#36807;&#19982;&#29305;&#27530;NIL&#23454;&#20307;&#21305;&#37197;&#26469;&#35782;&#21035;&#27809;&#26377;&#30456;&#24212;KB&#23454;&#20307;&#30340;&#23454;&#20307;&#25552;&#21450;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25991;&#26412;&#20013;&#21457;&#29616;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#22806;&#30340;&#23454;&#20307;&#25552;&#21450;&#65292;&#22312;KB&#32500;&#25252;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#24182;&#26410;&#34987;&#23436;&#20840;&#24320;&#21457;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#23616;&#38480;&#20110;&#31616;&#21333;&#30340;&#22522;&#20110;&#38408;&#20540;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#29305;&#24449;&#30340;&#20998;&#31867;&#65292;&#24182;&#19988;&#29992;&#20110;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BLINKout&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;BERT&#30340;&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#25552;&#21450;&#19982;&#29305;&#27530;&#30340;NIL&#23454;&#20307;&#36827;&#34892;&#21305;&#37197;&#26469;&#35782;&#21035;&#27809;&#26377;&#30456;&#24212;KB&#23454;&#20307;&#30340;&#25552;&#21450;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;BERT&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21253;&#25324;NIL&#23454;&#20307;&#34920;&#31034;&#21644;&#20998;&#31867;&#22312;&#20869;&#30340;&#26032;&#25216;&#26415;&#65292;&#24182;&#22686;&#24378;&#20102;&#20854;&#21516;&#20041;&#35789;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;KB&#20462;&#21098;&#21644;&#29256;&#26412;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#33258;&#21160;&#20174;&#24120;&#35265;&#30340;KB EL&#25968;&#25454;&#38598;&#26500;&#24314;&#20986;KB&#22806;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#21307;&#23398;&#26412;&#20307;&#35770;&#12289;UMLS&#12289;SNOMED CT&#31561;&#20116;&#20010;&#19981;&#21516;&#39046;&#22495;&#20013;&#65292;&#23545;&#20020;&#24202;&#31508;&#35760;&#12289;&#29983;&#29289;&#21307;&#23398;&#20986;&#29256;&#29289;&#21644;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;BLINKout&#22312;&#35782;&#21035;&#30693;&#35782;&#24211;&#22806;&#25552;&#21450;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering entity mentions that are out of a Knowledge Base (KB) from texts plays a critical role in KB maintenance, but has not yet been fully explored. The current methods are mostly limited to the simple threshold-based approach and feature-based classification, and the datasets for evaluation are relatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL) method which can identify mentions that do not have corresponding KB entities by matching them to a special NIL entity. To better utilize BERT, we propose new techniques including NIL entity representation and classification, with synonym enhancement. We also propose KB Pruning and Versioning strategies to automatically construct out-of-KB datasets from common in-KB EL datasets. Results on five datasets of clinical notes, biomedical publications, and Wikipedia articles in various domains show the advantages of BLINKout over existing methods to identify out-of-KB mentions for the medical ontologies, UMLS, SNOMED CT,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#21313;&#20998;&#20248;&#24322;&#65292;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#35270;&#35273;&#22330;&#26223;&#19979;&#20132;&#20114;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.13823</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#20197;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Grounding Language Models to Images for Multimodal Inputs and Outputs. (arXiv:2301.13823v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13823
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#21313;&#20998;&#20248;&#24322;&#65292;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#35270;&#35273;&#22330;&#26223;&#19979;&#20132;&#20114;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#20165;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#39046;&#22495;&#32852;&#31995;&#36215;&#26469;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#20174;&#22823;&#35268;&#27169;&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#23398;&#21040;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#20445;&#25345;&#35821;&#35328;&#27169;&#22411;&#20923;&#32467;&#65292;&#24182;&#24494;&#35843;&#36755;&#20837;&#21644;&#36755;&#20986;&#32447;&#24615;&#23618;&#20197;&#23454;&#29616;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#24341;&#20154;&#20837;&#32988;&#30340;&#20132;&#20114;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#22312;&#35270;&#35273;&#22330;&#26223;&#19979;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#24615;&#21035;&#20013;&#24615;&#21270;&#32763;&#35793;&#65288;GNT&#65289;&#20316;&#20026;&#19968;&#31181;&#24615;&#21035;&#21253;&#23481;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#33521;&#35821;&#21040;&#24847;&#22823;&#21033;&#35821;&#30340;&#32763;&#35793;&#26159;&#19968;&#20010;&#31361;&#20986;&#30340;&#24615;&#21035;&#30456;&#20851;&#30340;&#35821;&#35328;&#32763;&#35793;&#38382;&#39064;&#12290;&#30740;&#31350;&#22238;&#39038;&#20102;&#19968;&#20123;&#30456;&#20851;&#30340;&#24615;&#21035;&#21253;&#23481;&#24615;&#35821;&#35328;&#25351;&#21335;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;GNT&#30340;&#24773;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;MT&#20013;&#25191;&#34892;GNT&#30340;&#25216;&#26415;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.10075</link><description>&lt;p&gt;
&#20855;&#26377;&#21253;&#23481;&#24615;&#30340;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#21035;&#20013;&#24615;&#21270;&#65306;&#20174;&#29702;&#35770;&#22522;&#30784;&#21040;&#24320;&#25918;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Gender Neutralization for an Inclusive Machine Translation: from Theoretical Foundations to Open Challenges. (arXiv:2301.10075v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10075
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#24615;&#21035;&#20013;&#24615;&#21270;&#32763;&#35793;&#65288;GNT&#65289;&#20316;&#20026;&#19968;&#31181;&#24615;&#21035;&#21253;&#23481;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#33521;&#35821;&#21040;&#24847;&#22823;&#21033;&#35821;&#30340;&#32763;&#35793;&#26159;&#19968;&#20010;&#31361;&#20986;&#30340;&#24615;&#21035;&#30456;&#20851;&#30340;&#35821;&#35328;&#32763;&#35793;&#38382;&#39064;&#12290;&#30740;&#31350;&#22238;&#39038;&#20102;&#19968;&#20123;&#30456;&#20851;&#30340;&#24615;&#21035;&#21253;&#23481;&#24615;&#35821;&#35328;&#25351;&#21335;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;GNT&#30340;&#24773;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;MT&#20013;&#25191;&#34892;GNT&#30340;&#25216;&#26415;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#25216;&#26415;&#20013;&#30340;&#24615;&#21035;&#21253;&#23481;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#24615;&#21035;&#20013;&#24615;&#21270;&#32763;&#35793;&#65288;GNT&#65289;&#20316;&#20026;&#19968;&#31181;&#24615;&#21035;&#21253;&#23481;&#24615;&#21644;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#27169;&#22411;&#25152;&#35201;&#23454;&#29616;&#30340;&#30446;&#26631;&#65292;&#36825;&#20123;&#27169;&#22411;&#34987;&#21457;&#29616;&#20855;&#26377;&#24310;&#32493;&#24615;&#21035;&#20559;&#35265;&#21644;&#27495;&#35270;&#30340;&#20542;&#21521;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#33521;&#35793;&#24847;&#36825;&#23545;&#35821;&#35328;&#65292;&#23427;&#20195;&#34920;&#20102;&#31361;&#20986;&#30340;&#19982;&#24615;&#21035;&#26377;&#20851;&#30340;&#35821;&#35328;&#36716;&#31227;&#38382;&#39064;&#12290;&#20026;&#20102;&#23450;&#20041;GNT&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#19968;&#20123;&#30456;&#20851;&#30340;&#26426;&#26500;&#24615;&#21035;&#21253;&#23481;&#24615;&#35821;&#35328;&#25351;&#21335;&#65292;&#35752;&#35770;&#20102;&#20351;&#29992;GNT&#30340;&#24773;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;MT&#20013;&#25191;&#34892;GNT&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#40723;&#21169;&#26397;&#30528;&#26356;&#22823;&#30340;&#21253;&#23481;&#24615;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gender inclusivity in language technologies has become a prominent research topic. In this study, we explore gender-neutral translation (GNT) as a form of gender inclusivity and a goal to be achieved by machine translation (MT) models, which have been found to perpetuate gender bias and discrimination. Specifically, we focus on translation from English into Italian, a language pair representative of salient gender-related linguistic transfer problems. To define GNT, we review a selection of relevant institutional guidelines for gender-inclusive language, discuss its scenarios of use, and examine the technical challenges of performing GNT in MT, concluding with a discussion of potential solutions to encourage advancements toward greater inclusivity in MT.
&lt;/p&gt;</description></item><item><title>MoralDial&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#27169;&#25311;&#29305;&#23450;&#29992;&#25143;&#21644;&#23545;&#35805;&#31995;&#32479;&#20043;&#38388;&#30340;&#36947;&#24503;&#35752;&#35770;&#26469;&#35757;&#32451;&#21644;&#35780;&#20272;&#36947;&#24503;&#23545;&#35805;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#21028;&#26029;&#23545;&#35805;&#21709;&#24212;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#35780;&#20272;&#36947;&#24503;&#30340;&#22810;&#20010;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2212.10720</link><description>&lt;p&gt;
MoralDial&#65306;&#36890;&#36807;&#36947;&#24503;&#35752;&#35770;&#26469;&#35757;&#32451;&#21644;&#35780;&#20272;&#36947;&#24503;&#23545;&#35805;&#31995;&#32479;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MoralDial: A Framework to Train and Evaluate Moral Dialogue Systems via Moral Discussions. (arXiv:2212.10720v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10720
&lt;/p&gt;
&lt;p&gt;
MoralDial&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#27169;&#25311;&#29305;&#23450;&#29992;&#25143;&#21644;&#23545;&#35805;&#31995;&#32479;&#20043;&#38388;&#30340;&#36947;&#24503;&#35752;&#35770;&#26469;&#35757;&#32451;&#21644;&#35780;&#20272;&#36947;&#24503;&#23545;&#35805;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#21028;&#26029;&#23545;&#35805;&#21709;&#24212;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#35780;&#20272;&#36947;&#24503;&#30340;&#22810;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#36947;&#24503;&#38382;&#39064;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#19982;&#29992;&#25143;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#36947;&#24503;&#23545;&#35805;&#31995;&#32479;&#21487;&#20197;&#22686;&#24378;&#23545;&#35805;&#30340;&#21442;&#19982;&#24230;&#21644;&#29992;&#25143;&#36830;&#25509;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;MoralDial&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#36947;&#24503;&#23545;&#35805;&#31995;&#32479;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#35752;&#20102;&#36947;&#24503;&#30340;&#20132;&#27969;&#26426;&#21046;&#65292;&#24182;&#23558;&#34920;&#36798;&#30340;&#36947;&#24503;&#20998;&#20026;&#19977;&#20010;&#37096;&#20998;&#65292;&#36825;&#25351;&#26126;&#20102;&#26500;&#24314;&#36947;&#24503;&#23545;&#35805;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65306;&#26500;&#24314;&#27169;&#25311;&#29305;&#23450;&#29992;&#25143;&#21644;&#23545;&#35805;&#31995;&#32479;&#20043;&#38388;&#30340;&#36947;&#24503;&#35752;&#35770;&#12290;&#26500;&#24314;&#30340;&#35752;&#35770;&#21253;&#25324;&#22312;&#23545;&#35805;&#20132;&#25442;&#20013;&#34920;&#36798;&#12289;&#35299;&#37322;&#12289;&#20462;&#35746;&#21644;&#25512;&#26029;&#36947;&#24503;&#35266;&#28857;&#65292;&#36825;&#20351;&#24471;&#23545;&#35805;&#27169;&#22411;&#21487;&#20197;&#20197;&#33258;&#28982;&#30340;&#26041;&#24335;&#23398;&#20064;&#36947;&#24503;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#21028;&#26029;&#23545;&#35805;&#21709;&#24212;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#35780;&#20272;&#36947;&#24503;&#30340;&#22810;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Morality in dialogue systems has raised great attention in research recently. A moral dialogue system aligned with users' values could enhance conversation engagement and user connections. In this paper, we propose a framework, MoralDial to train and evaluate moral dialogue systems. In our framework, we first explore the communication mechanisms of morality and resolve expressed morality into three parts, which indicate the roadmap for building a moral dialogue system. Based on that, we design a simple yet effective method: constructing moral discussions between simulated specific users and the dialogue system. The constructed discussions consist of expressing, explaining, revising, and inferring moral views in dialogue exchanges, which makes conversational models learn morality well in a natural manner. Furthermore, we propose a novel evaluation method under the framework. We evaluate the multiple aspects of morality by judging the relation between dialogue responses and human values 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Instruct&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36523;&#29983;&#25104;&#25351;&#23548;&#20449;&#24687;&#26469;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#12290;&#22312;&#36229;&#33258;&#28982;&#25351;&#20196;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;InstructGPT-001&#30456;&#21516;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#22312;&#21407;&#22987;&#27169;&#22411;&#19978;&#33719;&#24471;&#20102;33%&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2212.10560</link><description>&lt;p&gt;
&#33258;&#25105;&#25351;&#23548;: &#29992;&#33258;&#29983;&#25104;&#30340;&#25351;&#31034;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Instruct: Aligning Language Models with Self-Generated Instructions. (arXiv:2212.10560v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Instruct&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36523;&#29983;&#25104;&#25351;&#23548;&#20449;&#24687;&#26469;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#12290;&#22312;&#36229;&#33258;&#28982;&#25351;&#20196;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;InstructGPT-001&#30456;&#21516;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#22312;&#21407;&#22987;&#27169;&#22411;&#19978;&#33719;&#24471;&#20102;33%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#30340;"&#25351;&#20196;&#35843;&#25972;"&#35821;&#35328;&#27169;&#22411;(&#21363;&#65292;&#35843;&#25972;&#20026;&#21709;&#24212;&#25351;&#20196;)&#24050;&#32463;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#38646;-shot&#25512;&#24191;&#21040;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#36890;&#24120;&#22312;&#25968;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#21019;&#36896;&#21147;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#35843;&#25972;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Self-Instruct&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36523;&#30340;&#29983;&#25104;&#26469;&#24341;&#23548;&#23427;&#20204;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27969;&#31243;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#29983;&#25104;&#25351;&#20196;&#12289;&#36755;&#20837;&#21644;&#36755;&#20986;&#26679;&#26412;&#65292;&#28982;&#21518;&#36807;&#28388;&#25481;&#26080;&#25928;&#25110;&#30456;&#20284;&#30340;&#26679;&#26412;&#65292;&#28982;&#21518;&#20877;&#23558;&#23427;&#20204;&#29992;&#20110;&#35843;&#25972;&#21407;&#22987;&#27169;&#22411;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26222;&#36890;&#30340;GPT3&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36229;&#33258;&#28982;&#25351;&#20196;&#19978;&#19982;InstructGPT-001&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#65292;&#24182;&#27604;&#21407;&#22987;&#27169;&#22411;&#33719;&#24471;&#20102;33%&#30340;&#32477;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large "instruction-tuned" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written inst
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#38382;&#39064;&#39537;&#21160;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;Socratic&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#25552;&#39640;&#25688;&#35201;&#20219;&#21153;&#30340;&#21487;&#25511;&#24615;&#65292;&#24182;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#22810;&#20010;&#25511;&#21046;&#31574;&#30053;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#24471;&#20986;&#30340;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.10449</link><description>&lt;p&gt;
Socratic&#39044;&#35757;&#32451;&#65306;&#38754;&#21521;&#21487;&#25511;&#25688;&#35201;&#30340;&#38382;&#39064;&#39537;&#21160;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization. (arXiv:2212.10449v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#38382;&#39064;&#39537;&#21160;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;Socratic&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#25552;&#39640;&#25688;&#35201;&#20219;&#21153;&#30340;&#21487;&#25511;&#24615;&#65292;&#24182;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#22810;&#20010;&#25511;&#21046;&#31574;&#30053;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#24471;&#20986;&#30340;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#38271;&#31687;&#25991;&#26723;&#30340;&#21487;&#25511;&#25688;&#35201;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#24456;&#38590;&#36866;&#24212;&#20219;&#21153;&#24182;&#26377;&#25928;&#22320;&#21709;&#24212;&#29992;&#25143;&#26597;&#35810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Socratic&#39044;&#35757;&#32451;&#65292;&#19968;&#31181;&#38754;&#21521;&#38382;&#39064;&#39537;&#21160;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#25688;&#35201;&#20219;&#21153;&#30340;&#21487;&#25511;&#24615;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#21644;&#22238;&#31572;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#30456;&#20851;&#38382;&#39064;&#65292;Socratic&#39044;&#35757;&#32451;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#36981;&#24490;&#29992;&#25143;&#25552;&#20379;&#30340;&#26597;&#35810;&#65292;&#24182;&#30830;&#23450;&#38656;&#35201;&#25688;&#35201;&#30340;&#30456;&#20851;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20004;&#20010;&#25688;&#35201;&#22495;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#21363;&#30701;&#31687;&#25925;&#20107;&#21644;&#23545;&#35805;&#65292;&#24182;&#20351;&#29992;&#20851;&#38190;&#35789;&#12289;&#38382;&#39064;&#21644;&#20107;&#23454;QA&#23545;&#22810;&#20010;&#25511;&#21046;&#31574;&#30053;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#21482;&#20381;&#36182;&#20110;&#26080;&#26631;&#27880;&#25991;&#26723;&#21644;&#38382;&#39064;&#29983;&#25104;&#31995;&#32479;&#65292;&#34920;&#29616;&#20248;&#20110;&#20351;&#29992;&#39069;&#22806;&#30340;&#30417;&#30563;&#25968;&#25454;&#30340;&#39044;&#31934;&#35843;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Socratic&#39044;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#29983;&#25104;&#36981;&#23432;&#29992;&#25143;&#25351;&#23450;&#32422;&#26463;&#26465;&#20214;&#30340;&#25688;&#35201;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In long document controllable summarization, where labeled data is scarce, pretrained models struggle to adapt to the task and effectively respond to user queries. In this paper, we introduce Socratic pretraining, a question-driven, unsupervised pretraining objective specifically designed to improve controllability in summarization tasks. By training a model to generate and answer relevant questions in a given context, Socratic pretraining enables the model to more effectively adhere to user-provided queries and identify relevant content to be summarized. We demonstrate the effectiveness of this approach through extensive experimentation on two summarization domains, short stories and dialogue, and multiple control strategies: keywords, questions, and factoid QA pairs. Our pretraining method relies only on unlabeled documents and a question generation system and outperforms pre-finetuning approaches that use additional supervised data. Furthermore, our results show that Socratic pretra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25512;&#29702;&#30740;&#31350;&#30340;&#29616;&#29366;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#21253;&#25324;&#25552;&#39640;&#21644;&#35825;&#23548;&#25512;&#29702;&#33021;&#21147;&#30340;&#25216;&#26415;&#12289;&#35780;&#20272;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#21644;&#22522;&#20934;&#65292;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#35814;&#32454;&#21644;&#26368;&#26032;&#30340;&#32508;&#36848;&#65292;&#21050;&#28608;&#26377;&#24847;&#20041;&#30340;&#35752;&#35770;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2212.10403</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25512;&#29702;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Towards Reasoning in Large Language Models: A Survey. (arXiv:2212.10403v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25512;&#29702;&#30740;&#31350;&#30340;&#29616;&#29366;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#21253;&#25324;&#25552;&#39640;&#21644;&#35825;&#23548;&#25512;&#29702;&#33021;&#21147;&#30340;&#25216;&#26415;&#12289;&#35780;&#20272;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#21644;&#22522;&#20934;&#65292;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#35814;&#32454;&#21644;&#26368;&#26032;&#30340;&#32508;&#36848;&#65292;&#21050;&#28608;&#26377;&#24847;&#20041;&#30340;&#35752;&#35770;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#26412;&#26041;&#38754;&#65292;&#22312;&#38382;&#39064;&#35299;&#20915;&#12289;&#20915;&#31574;&#21644;&#25209;&#21028;&#24615;&#24605;&#32500;&#31561;&#27963;&#21160;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#35266;&#23519;&#21040;&#24403;&#36825;&#20123;&#27169;&#22411;&#36275;&#22815;&#22823;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#23637;&#29616;&#20986;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;LLMs&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#21040;&#24213;&#22914;&#20309;&#12290;&#26412;&#25991;&#20840;&#38754;&#38416;&#36848;&#20102;LLMs&#20013;&#25512;&#29702;&#30740;&#31350;&#30340;&#24403;&#21069;&#29366;&#20917;&#65292;&#21253;&#25324;&#25552;&#39640;&#21644;&#35825;&#23548;&#36825;&#20123;&#27169;&#22411;&#25512;&#29702;&#30340;&#25216;&#26415;&#12289;&#35780;&#20272;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#21644;&#22522;&#20934;&#12289;&#20197;&#21450;&#20197;&#24448;&#30740;&#31350;&#30340;&#32467;&#26524;&#21644;&#24847;&#20041;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26410;&#26469;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#30446;&#30340;&#26159;&#25552;&#20379;&#19968;&#20010;&#35814;&#32454;&#21644;&#26368;&#26032;&#30340;&#32508;&#36848;&#65292;&#21050;&#28608;&#26377;&#24847;&#20041;&#30340;&#35752;&#35770;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#22810;&#27169;&#24335;&#32763;&#35793;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#20687;&#35299;&#20915;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;CoMMuTE&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#23545;&#27604;&#22810;&#35821;&#35328;&#22810;&#27169;&#24335;&#32763;&#35793;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#19982;&#24378;&#25991;&#26412;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.10140</link><description>&lt;p&gt;
&#22788;&#29702;&#22270;&#20687;&#27495;&#20041;&#65306;&#25913;&#21892;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;&#21644;&#23545;&#27604;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Tackling Ambiguity with Images: Improved Multimodal Machine Translation and Contrastive Evaluation. (arXiv:2212.10140v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#22810;&#27169;&#24335;&#32763;&#35793;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#20687;&#35299;&#20915;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;CoMMuTE&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#23545;&#27604;&#22810;&#35821;&#35328;&#22810;&#27169;&#24335;&#32763;&#35793;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#19982;&#24378;&#25991;&#26412;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#27495;&#20041;&#65292;&#32780;&#22270;&#20687;&#31561;&#19978;&#19979;&#25991;&#20449;&#24687;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#22270;&#20687;&#25552;&#39640;&#32763;&#35793;&#25928;&#26524;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#19981;&#20165;&#21463;&#21040;&#36328;&#27169;&#24577;&#34920;&#36798;&#26377;&#25928;&#24615;&#30340;&#22256;&#38590;&#38480;&#21046;&#65292;&#36824;&#21463;&#21040;&#29305;&#23450;&#35780;&#20272;&#21644;&#35757;&#32451;&#25968;&#25454;&#32570;&#20047;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24378;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#22810;&#27169;&#24335;&#32763;&#35793;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#31070;&#32463;&#36866;&#37197;&#22120;&#21644;&#26032;&#39062;&#30340;&#24341;&#23548;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#22312;&#35270;&#35273;&#26465;&#20214;&#25513;&#34109;&#21644;&#22810;&#27169;&#24335;&#32763;&#35793;&#19978;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;CoMMuTE&#65292;&#19968;&#32452;&#21253;&#21547;&#27169;&#31946;&#21477;&#23376;&#21450;&#20854;&#21487;&#33021;&#30340;&#32763;&#35793;&#26041;&#26696;&#21644;&#23545;&#24212;&#22270;&#20687;&#30340;&#23545;&#27604;&#22810;&#35821;&#35328;&#22810;&#27169;&#24335;&#32763;&#35793;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#30340;&#33521;&#27861;&#12289;&#33521;&#24503;&#21644;&#33521;&#25463;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#19982;&#24378;&#25991;&#26412;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the major challenges of machine translation (MT) is ambiguity, which can in some cases be resolved by accompanying context such as images. However, recent work in multimodal MT (MMT) has shown that obtaining improvements from images is challenging, limited not only by the difficulty of building effective cross-modal representations, but also by the lack of specific evaluation and training data. We present a new MMT approach based on a strong text-only MT model, which uses neural adapters, a novel guided self-attention mechanism and which is jointly trained on both visually-conditioned masking and MMT. We also introduce CoMMuTE, a Contrastive Multilingual Multimodal Translation Evaluation set of ambiguous sentences and their possible translations, accompanied by disambiguating images corresponding to each translation. Our approach obtains competitive results compared to strong text-only models on standard English-to-French, English-to-German and English-to-Czech benchmarks and ou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIONYSUS&#30340;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#20302;&#36164;&#28304;&#30340;&#23545;&#35805;&#25688;&#35201;&#65292;&#33258;&#30417;&#30563;&#30340;&#26041;&#27861;&#29992;&#20110;&#39044;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#30340;&#21019;&#26032;&#28857;&#22312;&#20110;&#21033;&#29992;&#19981;&#21516;&#30340;&#20266;&#25688;&#35201;&#65292;&#24182;&#22522;&#20110;&#23545;&#35805;&#20013;&#20449;&#24687;&#20998;&#24067;&#30340;&#20998;&#26512;&#26469;&#36873;&#25321;&#26368;&#20339;&#30340;&#20266;&#25688;&#35201;&#65292;&#25552;&#39640;&#20102;&#22312;&#26032;&#39046;&#22495;&#20013;&#23545;&#35805;&#25688;&#35201;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.10018</link><description>&lt;p&gt;
DIONYSUS&#65306;&#29992;&#20110;&#20302;&#36164;&#28304;&#23545;&#35805;&#25688;&#35201;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DIONYSUS: A Pre-trained Model for Low-Resource Dialogue Summarization. (arXiv:2212.10018v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10018
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIONYSUS&#30340;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#20302;&#36164;&#28304;&#30340;&#23545;&#35805;&#25688;&#35201;&#65292;&#33258;&#30417;&#30563;&#30340;&#26041;&#27861;&#29992;&#20110;&#39044;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#30340;&#21019;&#26032;&#28857;&#22312;&#20110;&#21033;&#29992;&#19981;&#21516;&#30340;&#20266;&#25688;&#35201;&#65292;&#24182;&#22522;&#20110;&#23545;&#35805;&#20013;&#20449;&#24687;&#20998;&#24067;&#30340;&#20998;&#26512;&#26469;&#36873;&#25321;&#26368;&#20339;&#30340;&#20266;&#25688;&#35201;&#65292;&#25552;&#39640;&#20102;&#22312;&#26032;&#39046;&#22495;&#20013;&#23545;&#35805;&#25688;&#35201;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25688;&#35201;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#36817;&#26399;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#35805;&#25688;&#35201;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#23545;&#35805;&#30340;&#22266;&#26377;&#32467;&#26500;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#26032;&#39046;&#22495;&#20013;&#25928;&#26524;&#27424;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIONYSUS&#65288;&#29992;&#20110;&#20219;&#20309;&#26032;&#39046;&#22495;&#20013;&#23545;&#35805;&#25688;&#35201;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65289;&#65292;&#20854;&#22522;&#20110;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#23545;&#35805;&#31034;&#20363;&#21019;&#24314;&#20102;&#20004;&#20010;&#20266;&#25688;&#35201;&#65306;&#19968;&#20010;&#26159;&#36890;&#36807;&#24494;&#35843;&#25688;&#35201;&#27169;&#22411;&#29983;&#25104;&#30340;&#65292;&#21478;&#19968;&#20010;&#26159;&#21253;&#21547;&#37325;&#35201;&#20449;&#24687;&#30340;&#23545;&#35805;&#36716;&#25442;&#38598;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#35805;&#20013;&#30340;&#20449;&#24687;&#20998;&#24067;&#24046;&#24322;&#36873;&#25321;&#20854;&#20013;&#19968;&#20010;&#20266;&#25688;&#35201;&#12290;&#36825;&#20010;&#25152;&#36873;&#30340;&#20266;&#25688;&#35201;&#20316;&#20026;&#30446;&#26631;&#65292;&#29992;&#33258;&#30417;&#30563;&#30340;&#26041;&#27861;&#22312;&#22823;&#22411;&#23545;&#35805;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;DIONYSUS&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue summarization has recently garnered significant attention due to its wide range of applications. However, existing methods for summarizing dialogues have limitations because they do not take into account the inherent structure of dialogue and rely heavily on labeled data, which can lead to poor performance in new domains. In this work, we propose DIONYSUS (dynamic input optimization in pre-training for dialogue summarization), a pre-trained encoder-decoder model for summarizing dialogues in any new domain. To pre-train DIONYSUS, we create two pseudo summaries for each dialogue example: one is produced by a fine-tuned summarization model, and the other is a collection of dialogue turns that convey important information. We then choose one of these pseudo summaries based on the difference in information distribution across different types of dialogues. This selected pseudo summary serves as the objective for pre-training DIONYSUS using a self-supervised approach on a large dialo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#23384;&#22312;&#27495;&#20041;&#30340;&#38382;&#39064;--&#36890;&#36807;&#25552;&#38382;&#28548;&#28165;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2212.09885</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#38382;&#28548;&#28165;&#38382;&#39064;&#23454;&#29616;Python&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Python Code Generation by Asking Clarification Questions. (arXiv:2212.09885v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#23384;&#22312;&#27495;&#20041;&#30340;&#38382;&#39064;--&#36890;&#36807;&#25552;&#38382;&#28548;&#28165;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25991;&#26412;&#20013;&#29983;&#25104;&#20195;&#30721;&#38656;&#35201;&#29702;&#35299;&#29992;&#25143;&#30340;&#24847;&#22270;&#65292;&#24182;&#29983;&#25104;&#28385;&#36275;&#27492;&#24847;&#22270;&#30340;&#21487;&#25191;&#34892;&#20195;&#30721;&#29255;&#27573;&#12290;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#65292;&#24403;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19981;&#22815;&#26126;&#30830;&#26102;&#65292;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#26356;&#29616;&#23454;&#30340;&#20219;&#21153;&#35774;&#32622;&#65292;&#20551;&#35774;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#19981;&#26126;&#30830;&#24615;&#21487;&#20197;&#36890;&#36807;&#25552;&#38382;&#28548;&#28165;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026; CodeClarQA &#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#20195;&#30721;&#30340;&#25104;&#23545;&#25968;&#25454;&#20197;&#21450;&#21019;&#24314;&#30340;&#21512;&#25104;&#28548;&#28165;&#38382;&#39064;&#21644;&#31572;&#26696;&#12290;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#23384;&#22312;&#27495;&#20041;&#26102;&#65292;&#36890;&#36807;&#28548;&#28165;&#38382;&#39064;&#21487;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#20195;&#30721;&#65292;&#36825;&#20307;&#29616;&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#30340;&#27169;&#22411;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#12290;&#38500;&#20102;&#32763;&#35793;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#25552;&#38382;&#28548;&#28165;&#38382;&#39064;&#20197;&#28040;&#38500;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#30340;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code generation from text requires understanding the user's intent from a natural language description and generating an executable code snippet that satisfies this intent. While recent pretrained language models demonstrate remarkable performance for this task, these models fail when the given natural language description is under-specified. In this work, we introduce a novel and more realistic setup for this task. We hypothesize that the under-specification of a natural language description can be resolved by asking clarification questions. Therefore, we collect and introduce a new dataset named CodeClarQA containing pairs of natural language descriptions and code with created synthetic clarification questions and answers. The empirical results of our evaluation of pretrained language model performance on code generation show that clarifications result in more precisely generated code, as shown by the substantial improvement of model performance in all evaluation metrics. Alongside t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25688;&#35201;&#20877;&#25490;&#24207;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26080;&#30417;&#30563;&#27169;&#22411;&#30340;&#25688;&#35201;&#34920;&#29616;&#25552;&#39640;&#65292;&#32553;&#23567;&#20854;&#19982;&#26377;&#30417;&#30563;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2212.09593</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#25688;&#35201;&#20877;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Summarization Re-ranking. (arXiv:2212.09593v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09593
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25688;&#35201;&#20877;&#25490;&#24207;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26080;&#30417;&#30563;&#27169;&#22411;&#30340;&#25688;&#35201;&#34920;&#29616;&#25552;&#39640;&#65292;&#32553;&#23567;&#20854;&#19982;&#26377;&#30417;&#30563;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20219;&#21153;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#20852;&#36215;&#65292;&#20687;PEGASUS&#36825;&#26679;&#30340;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#22312;&#19979;&#28216;&#25688;&#35201;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26080;&#30417;&#30563;&#27169;&#22411;&#30340;&#24615;&#33021;&#20173;&#28982;&#26126;&#26174;&#33853;&#21518;&#20110;&#23427;&#20204;&#30340;&#26377;&#30417;&#30563;&#23545;&#24212;&#29289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25688;&#35201;&#20877;&#25490;&#24207;&#26041;&#27861;&#65292;&#26088;&#22312;&#32553;&#23567;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#25688;&#35201;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#23558;PEGASUS&#30340;&#30456;&#23545;&#24179;&#22343;ROUGE&#25552;&#39640;&#20102;&#26368;&#22810;7.27&#65285;&#65292;ChatGPT&#25552;&#39640;&#20102;&#26368;&#22810;6.86&#65285;&#65307;&#24182;&#19988;&#22312;30&#31181;&#38646;&#26679;&#26412;&#36716;&#31227;&#35774;&#32622;&#65288;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#65292;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#65289;&#20013;&#65292;&#24179;&#22343;&#33719;&#24471;&#20102;7.51&#65285;&#30340;&#30456;&#23545;&#22686;&#30410;&#65288;&#20174;XSum&#21040;WikiHow&#26368;&#39640;&#21487;&#36798;23.73&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of task-specific pre-training objectives, abstractive summarization models like PEGASUS offer appealing zero-shot performance on downstream summarization tasks. However, the performance of such unsupervised models still lags significantly behind their supervised counterparts. Similarly to the supervised setup, we notice a very high variance in quality among summary candidates from these models while only one candidate is kept as the summary output. In this paper, we propose to re-rank summary candidates in an unsupervised manner, aiming to close the performance gap between unsupervised and supervised models. Our approach improves the unsupervised PEGASUS by up to 7.27% and ChatGPT by up to 6.86% relative mean ROUGE across four widely-adopted summarization benchmarks ; and achieves relative gains of 7.51% (up to 23.73% from XSum to WikiHow) averaged over 30 zero-shot transfer setups (finetuning on a dataset, evaluating on another).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#26597;&#35810;&#22686;&#24378;&#26041;&#27861;QKConv&#65292;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#23545;&#35805;&#65292;&#24182;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;QKConv&#30340;&#34920;&#29616;&#27604;&#25152;&#26377;&#26080;&#30417;&#30563;&#26041;&#27861;&#37117;&#35201;&#22909;&#65292;&#24182;&#19988;&#19982;&#26377;&#30417;&#30563;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.09588</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#30417;&#30563;&#32852;&#21512;&#24314;&#27169;&#30340;&#26597;&#35810;&#22686;&#24378;&#30693;&#35782;&#23494;&#38598;&#22411;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Query Enhanced Knowledge-Intensive Conversation via Unsupervised Joint Modeling. (arXiv:2212.09588v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#26597;&#35810;&#22686;&#24378;&#26041;&#27861;QKConv&#65292;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#23545;&#35805;&#65292;&#24182;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;QKConv&#30340;&#34920;&#29616;&#27604;&#25152;&#26377;&#26080;&#30417;&#30563;&#26041;&#27861;&#37117;&#35201;&#22909;&#65292;&#24182;&#19988;&#19982;&#26377;&#30417;&#30563;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26597;&#35810;&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;QKConv&#65292;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#23545;&#35805;&#12290;QKConv&#21253;&#21547;&#19977;&#20010;&#27169;&#22359;&#65306;&#26597;&#35810;&#29983;&#25104;&#22120;&#12289;&#36890;&#29992;&#30693;&#35782;&#36873;&#25321;&#22120;&#21644;&#21709;&#24212;&#29983;&#25104;&#22120;&#12290;QKConv&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#25506;&#32034;&#22810;&#20010;&#20505;&#36873;&#26597;&#35810;&#24182;&#21033;&#29992;&#30456;&#24212;&#30340;&#36873;&#25321;&#30340;&#30693;&#35782;&#26469;&#29983;&#25104;&#21709;&#24212;&#12290;&#32852;&#21512;&#35757;&#32451;&#20165;&#20381;&#36182;&#20110;&#23545;&#35805;&#19978;&#19979;&#25991;&#21644;&#30446;&#26631;&#21709;&#24212;&#65292;&#20813;&#38500;&#20102;&#39069;&#22806;&#30340;&#26597;&#35810;&#27880;&#37322;&#25110;&#30693;&#35782;&#26469;&#28304;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;QKConv&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65306;&#23545;&#35805;&#38382;&#31572;&#12289;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;QKConv&#30340;&#34920;&#29616;&#27604;&#25152;&#26377;&#26080;&#30417;&#30563;&#26041;&#27861;&#37117;&#35201;&#22909;&#65292;&#24182;&#19988;&#19982;&#26377;&#30417;&#30563;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an unsupervised query enhanced approach for knowledge-intensive conversations, namely QKConv. There are three modules in QKConv: a query generator, an off-the-shelf knowledge selector, and a response generator. QKConv is optimized through joint training, which produces the response by exploring multiple candidate queries and leveraging corresponding selected knowledge. The joint training solely relies on the dialogue context and target response, getting exempt from extra query annotations or knowledge provenances. To evaluate the effectiveness of the proposed QKConv, we conduct experiments on three representative knowledge-intensive conversation datasets: conversational question-answering, task-oriented dialogue, and knowledge-grounded conversation. Experimental results reveal that QKConv performs better than all unsupervised methods across three datasets and achieves competitive performance compared to supervised methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#36890;&#36807;&#24120;&#35782;&#33976;&#39311;&#31639;&#27861;&#24378;&#21270;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25361;&#25112;&#22823;&#22411;&#27169;&#22411;&#30340;&#24120;&#35782;&#33719;&#21462;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#35268;&#27169;&#30340;&#23398;&#20064;&#31639;&#27861;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2212.09246</link><description>&lt;p&gt;
I2D2: &#22522;&#20110;NeuroLogic&#21644;&#33258;&#25105;&#27169;&#20223;&#30340;&#24402;&#32435;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation. (arXiv:2212.09246v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#36890;&#36807;&#24120;&#35782;&#33976;&#39311;&#31639;&#27861;&#24378;&#21270;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25361;&#25112;&#22823;&#22411;&#27169;&#22411;&#30340;&#24120;&#35782;&#33719;&#21462;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#35268;&#27169;&#30340;&#23398;&#20064;&#31639;&#27861;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#35268;&#27169;&#26041;&#38754;&#19981;&#26029;&#24378;&#21270;&#65292;&#20294;&#20173;&#32570;&#20047;&#22362;&#23454;&#30340;&#24120;&#35782;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#35268;&#27169;&#20284;&#20046;&#26159;&#21046;&#32988;&#27861;&#23453;&#65307;&#27605;&#31455;&#65292;&#26368;&#22823;&#30340;&#27169;&#22411;&#20284;&#20046;&#24050;&#32463;&#33719;&#24471;&#20102;&#26368;&#22810;&#30340;&#24120;&#35782;&#21151;&#33021;&#12290;&#36825;&#31687;&#35770;&#25991;&#25506;&#31350;&#20102;&#20284;&#20046;&#19981;&#21487;&#33021;&#23454;&#29616;&#30340;&#21305;&#37197;&#65306;&#22914;&#26524;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-2&#65289;&#36890;&#36807;&#26032;&#39062;&#30340;&#24120;&#35782;&#33976;&#39311;&#31639;&#27861;&#24471;&#21040;&#21160;&#21147;&#65292;&#23427;&#20204;&#26159;&#21542;&#33021;&#36194;&#36807;&#27604;&#23427;&#20204;&#22823;&#25968;&#20010;&#25968;&#37327;&#32423;&#24182;&#19988;&#26356;&#20248;&#31168;&#30340;&#27169;&#22411;&#65288;&#22914;GPT-3&#65289;&#65311;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#20851;&#38190;&#26234;&#21147;&#38382;&#39064;&#26159;&#65292;&#26159;&#21542;&#21487;&#33021;&#35774;&#35745;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#24182;&#19981;&#21463;&#21040;&#35268;&#27169;&#30340;&#22909;&#22788;&#65292;&#32780;&#21364;&#26377;&#31454;&#20105;&#21147;&#30340;&#24120;&#35782;&#33719;&#21462;&#27700;&#24179;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24120;&#35782;&#30693;&#35782;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#29983;&#25104;&#36890;&#29992;&#35821;&#21477;&#30340;&#20219;&#21153;&#65292;&#21363;&#20851;&#20110;&#26085;&#24120;&#27010;&#24565;&#30340;&#24120;&#35782;&#20107;&#23454;&#38472;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models, despite their rapid advancements powered by scale, still fall short of robust commonsense capabilities. And yet, scale appears to be the winning recipe; after all, the largest models seem to have acquired the largest amount of commonsense capabilities. Or is it?  In this paper, we investigate the possibility of a seemingly impossible match: can smaller language models with dismal commonsense capabilities (i.e., GPT-2), ever win over models that are orders of magnitude larger and better (i.e., GPT-3), if the smaller models are powered with novel commonsense distillation algorithms? The key intellectual question we ask here is whether it is possible, if at all, to design a learning algorithm that does not benefit from scale, yet leads to a competitive level of commonsense acquisition. In this work, we study the generative models of commonsense knowledge, focusing on the task of generating generics, statements of commonsense facts about everyday concepts, e.g.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#24320;&#25918;&#39046;&#22495;&#22522;&#20110;&#29305;&#23450;&#26041;&#38754;&#30340;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#32500;&#22522;&#30334;&#31185;&#36827;&#34892;&#20247;&#21253;&#30693;&#35782;&#30340;&#21033;&#29992;&#65292;&#26500;&#24314;&#20102;&#39640;&#36136;&#37327;&#30340;OASum&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#19971;&#20010;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36827;&#34892;&#22810;&#26679;&#21270;&#30340;&#26041;&#38754;&#22522;&#30784;&#30340;&#24635;&#32467;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2212.09233</link><description>&lt;p&gt;
OASum&#65306;&#22823;&#35268;&#27169;&#24320;&#25918;&#39046;&#22495;&#22522;&#20110;&#29305;&#23450;&#26041;&#38754;&#30340;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
OASum: Large-Scale Open Domain Aspect-based Summarization. (arXiv:2212.09233v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#24320;&#25918;&#39046;&#22495;&#22522;&#20110;&#29305;&#23450;&#26041;&#38754;&#30340;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#32500;&#22522;&#30334;&#31185;&#36827;&#34892;&#20247;&#21253;&#30693;&#35782;&#30340;&#21033;&#29992;&#65292;&#26500;&#24314;&#20102;&#39640;&#36136;&#37327;&#30340;OASum&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#19971;&#20010;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36827;&#34892;&#22810;&#26679;&#21270;&#30340;&#26041;&#38754;&#22522;&#30784;&#30340;&#24635;&#32467;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#26041;&#38754;&#25110;&#22522;&#20110;&#26597;&#35810;&#30340;&#24635;&#32467;&#24341;&#36215;&#20102;&#26356;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22522;&#20110;&#29992;&#25143;&#30340;&#20852;&#36259;&#29983;&#25104;&#19981;&#21516;&#30340;&#24635;&#32467;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#38754;&#25110;&#22522;&#20110;&#26597;&#35810;&#30340;&#24635;&#32467;&#25968;&#25454;&#38598;&#36890;&#24120;&#20391;&#37325;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#21253;&#21547;&#30340;&#23454;&#20363;&#35268;&#27169;&#30456;&#23545;&#36739;&#23567;&#65292;&#25110;&#32773;&#20165;&#21253;&#21547;&#23569;&#37327;&#26041;&#38754;&#31867;&#22411;&#12290;&#36825;&#20123;&#38480;&#21046;&#38459;&#30861;&#20102;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;.org&#19978;&#30340;&#20247;&#21253;&#30693;&#35782;&#65292;&#24182;&#33258;&#21160;&#29983;&#25104;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#12289;&#22823;&#35268;&#27169;&#30340;&#24320;&#25918;&#39046;&#22495;&#22522;&#20110;&#29305;&#23450;&#26041;&#38754;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;OASum&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#36229;&#36807;370&#19975;&#20010;&#23454;&#20363;&#65292;&#22312;200&#19975;&#20010;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#20013;&#28041;&#21450;&#20102;&#32422;100&#19975;&#20010;&#19981;&#21516;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#22312;OASum&#19978;&#25552;&#20379;&#20102;&#22522;&#20934;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#36827;&#34892;&#22810;&#26679;&#21270;&#26041;&#38754;&#22522;&#30784;&#30340;&#24635;&#32467;&#29983;&#25104;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#19971;&#20010;&#19979;&#28216;&#25968;&#25454;&#38598;&#36827;&#34892;&#38646;-shot&#12289;&#23569;-shot&#21644;&#24494;&#35843;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect or query-based summarization has recently caught more attention, as it can generate differentiated summaries based on users' interests. However, the current dataset for aspect or query-based summarization either focuses on specific domains, contains relatively small-scale instances, or includes only a few aspect types. Such limitations hinder further explorations in this direction. In this work, we take advantage of crowd-sourcing knowledge on Wikipedia.org and automatically create a high-quality, large-scale open-domain aspect-based summarization dataset named OASum, which contains more than 3.7 million instances with around 1 million different aspects on 2 million Wikipedia pages. We provide benchmark results on OASum and demonstrate its ability for diverse aspect-based summarization generation. To overcome the data scarcity problem on specific domains, we also perform zero-shot, few-shot, and fine-tuning on seven downstream datasets. Specifically, zero/few-shot and fine-tunin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#32500;&#24230;&#21270;&#35780;&#20272;&#20154;&#26426;&#32842;&#22825;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#19968;&#32452;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20026;&#26410;&#26469;&#32842;&#22825;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2212.09180</link><description>&lt;p&gt;
&#21035;&#24536;&#20102;&#20320;&#30340;ABC&#65306;&#35780;&#20272;&#32842;&#22825;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems. (arXiv:2212.09180v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#32500;&#24230;&#21270;&#35780;&#20272;&#20154;&#26426;&#32842;&#22825;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#19968;&#32452;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20026;&#26410;&#26469;&#32842;&#22825;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#32842;&#22825;&#20132;&#20114;&#39046;&#22495;&#36817;&#26469;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#36866;&#24403;&#30340;&#35780;&#20272;&#20173;&#38656;&#35201;&#20154;&#31867;&#20027;&#35266;&#21028;&#26029;&#65292;&#22240;&#27492;&#35780;&#27979;&#25351;&#26631;&#26131;&#20986;&#29616;&#39640;&#26041;&#24046;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#35780;&#20272;&#26041;&#27861;&#21644;&#26631;&#20934;&#32570;&#20047;&#35268;&#33539;&#24615;&#65292;&#32570;&#20047;&#29992;&#20110;&#35780;&#20272;&#26377;&#25928;&#24615;&#30340;&#24037;&#20316;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#32467;&#26524;&#21487;&#33021;&#26080;&#27861;&#23436;&#25972;&#21453;&#26144;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20248;&#28857;&#21644;&#32570;&#38519;&#12290;&#25105;&#20204;&#26088;&#22312;&#23454;&#29616;&#23545;&#20154;&#26426;&#32842;&#22825;&#30340;&#32500;&#24230;&#21270;&#35780;&#20272;&#65292;&#21487;&#21487;&#38752;&#22320;&#27979;&#37327;&#32842;&#22825;&#36136;&#37327;&#30340;&#20960;&#20010;&#19981;&#21516;&#26041;&#38754;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#65292;&#37327;&#21270;&#20102;&#20960;&#31181;&#19982;&#36136;&#37327;&#30456;&#20851;&#30340;&#26426;&#22120;&#20154;&#32842;&#22825;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#26367;&#20195;&#30340;Likert-style&#25110;&#27604;&#36739;&#26041;&#27861;&#26356;&#36866;&#21512;&#35780;&#20272;&#32500;&#24230;&#21270;&#32842;&#22825;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#39564;&#35777;&#30340;&#26041;&#27861;&#21644;&#29616;&#26377;&#26041;&#27861;&#26469;&#35780;&#20272;&#19968;&#32452;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#24182;&#20840;&#38754;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;&#20960;&#20010;&#36136;&#37327;&#32500;&#24230;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#29616;&#26377;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#25345;&#32493;&#20248;&#21183;&#21644;&#38480;&#21046;&#65292;&#24182;&#20026;&#32842;&#22825;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been great recent advancement in human-computer chat. However, proper evaluation currently requires human judgements that produce notoriously high-variance metrics due to their inherent subjectivity. Furthermore, there is little standardization in the methods and labels used for evaluation, with an overall lack of work to compare and assess the validity of various evaluation approaches. As a consequence, existing evaluation results likely leave an incomplete picture of the strengths and weaknesses of open-domain chatbots. We aim towards a dimensional evaluation of human-computer chat that can reliably measure several distinct aspects of chat quality. To this end, we present our novel human evaluation method that quantifies the rate of several quality-related chatbot behaviors. Our results demonstrate our method to be more suitable for dimensional chat evaluation than alternative likert-style or comparative methods. We then use our validated method and existing methods to eval
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#8220;few-&#8221;&#31867;&#22411;&#37327;&#35789;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#25152;&#26377;&#27169;&#22411;&#23545;&#36825;&#31181;&#37327;&#35789;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#36739;&#22823;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#24046;&#12290;&#36825;&#31181;&#21453;&#27604;&#20363;&#32553;&#25918;&#30340;&#29616;&#35937;&#34920;&#26126;&#22823;&#22411;&#27169;&#22411;&#36234;&#26469;&#24840;&#21453;&#26144;&#22312;&#32447;&#20154;&#31867;&#22788;&#29702;&#65292;&#32780;&#19981;&#26159;&#31163;&#32447;&#22788;&#29702;&#12290;&#36825;&#21487;&#33021;&#25361;&#25112;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#22522;&#30784;&#30340;&#20570;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.08700</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#37327;&#35789;&#26102;&#34920;&#29616;&#30053;&#26377;&#38382;&#39064;&#65311;&#20351;&#29992;&#23569;&#37327;&#31867;&#22411;&#30340;&#37327;&#35789;&#20250;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#21576;&#29616;&#21453;&#27604;&#20363;&#32553;&#25918;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rarely a problem? Language models exhibit inverse scaling in their predictions following few-type quantifiers. (arXiv:2212.08700v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#8220;few-&#8221;&#31867;&#22411;&#37327;&#35789;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#25152;&#26377;&#27169;&#22411;&#23545;&#36825;&#31181;&#37327;&#35789;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#36739;&#22823;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#24046;&#12290;&#36825;&#31181;&#21453;&#27604;&#20363;&#32553;&#25918;&#30340;&#29616;&#35937;&#34920;&#26126;&#22823;&#22411;&#27169;&#22411;&#36234;&#26469;&#24840;&#21453;&#26144;&#22312;&#32447;&#20154;&#31867;&#22788;&#29702;&#65292;&#32780;&#19981;&#26159;&#31163;&#32447;&#22788;&#29702;&#12290;&#36825;&#21487;&#33021;&#25361;&#25112;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#22522;&#30784;&#30340;&#20570;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#37327;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#20197;&#8220;few-&#8221;&#31867;&#22411;&#30340;&#37327;&#35789;&#20026;&#37325;&#28857;&#65292;&#27604;&#22914;&#8220;few children like toys&#8221;&#65292;&#22240;&#20026;&#36825;&#31181;&#31867;&#22411;&#30340;&#21477;&#23376;&#32452;&#25104;&#37096;&#20998;&#36890;&#24120;&#20250;&#20849;&#29616;&#65292;&#32780;&#8220;few-&#8221;&#31867;&#22411;&#30340;&#37327;&#35789;&#36739;&#20026;&#32597;&#35265;&#65292;&#36825;&#21487;&#33021;&#23545;&#35821;&#35328;&#27169;&#22411;&#26500;&#25104;&#29305;&#21035;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#26469;&#33258;&#20004;&#39033;&#20154;&#31867;&#31070;&#32463;&#35821;&#35328;&#23398;&#23454;&#39564;&#30340;960&#20010;&#33521;&#35821;&#21477;&#23376;&#36827;&#34892;&#20102;&#35797;&#39564;&#65292;&#24182;&#23558;&#23427;&#20204;&#25552;&#20379;&#32473;22&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#27169;&#22411;&#12290;&#19981;&#20165;&#25152;&#26377;&#27169;&#22411;&#23545;&#8220;few-&#8221;&#31867;&#22411;&#30340;&#37327;&#35789;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#19988;&#24635;&#20307;&#19978;&#65292;&#27169;&#22411;&#36234;&#22823;&#65292;&#20854;&#34920;&#29616;&#36234;&#24046;&#12290;&#36825;&#31181;&#21453;&#27604;&#20363;&#32553;&#25918;&#30340;&#29616;&#35937;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#19968;&#33268;&#65292;&#34920;&#26126;&#36739;&#22823;&#30340;&#27169;&#22411;&#36234;&#26469;&#36234;&#21453;&#26144;&#22312;&#32447;&#20154;&#31867;&#22788;&#29702;&#65292;&#32780;&#19981;&#26159;&#31163;&#32447;&#22788;&#29702;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#19979;&#38477;&#21487;&#33021;&#20250;&#25361;&#25112;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#22522;&#30784;&#30340;&#20570;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
How well do language models deal with quantification? In this study, we focus on 'few'-type quantifiers, as in 'few children like toys', which might pose a particular challenge for language models because the sentence components with out the quantifier are likely to co-occur, and 'few'-type quantifiers are rare. We present 960 English sentence stimuli from two human neurolinguistic experiments to 22 autoregressive transformer models of differing sizes. Not only do all the models perform poorly on 'few'-type quantifiers, but overall the larger the model, the worse its performance. This inverse scaling is consistent with previous work suggesting that larger models increasingly reflect online rather than offline human processing, and we argue that the decreasing performance of larger models may challenge uses of language models as the basis for natural language systems.
&lt;/p&gt;</description></item><item><title>UnitY&#26159;&#19968;&#31181;&#36890;&#36807;&#20004;&#36941;&#32763;&#35793;&#29983;&#25104;&#26368;&#20339;&#32467;&#26524;&#30340;&#35821;&#38899;&#32763;&#35793;&#26041;&#27861;&#65292;&#33021;&#22815;&#27604;&#21333;&#36941;&#35821;&#38899;&#21040;&#21333;&#20803;&#32763;&#35793;&#27169;&#22411;&#22312;ASR-BLEU&#20540;&#21644;&#35299;&#30721;&#36895;&#24230;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2212.08055</link><description>&lt;p&gt;
UnitY: &#29992;&#31163;&#25955;&#21333;&#20803;&#36827;&#34892;&#20004;&#36941;&#30452;&#25509;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units. (arXiv:2212.08055v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08055
&lt;/p&gt;
&lt;p&gt;
UnitY&#26159;&#19968;&#31181;&#36890;&#36807;&#20004;&#36941;&#32763;&#35793;&#29983;&#25104;&#26368;&#20339;&#32467;&#26524;&#30340;&#35821;&#38899;&#32763;&#35793;&#26041;&#27861;&#65292;&#33021;&#22815;&#27604;&#21333;&#36941;&#35821;&#38899;&#21040;&#21333;&#20803;&#32763;&#35793;&#27169;&#22411;&#22312;ASR-BLEU&#20540;&#21644;&#35299;&#30721;&#36895;&#24230;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#20855;&#26377;&#20248;&#21270;&#30340;&#32452;&#20214;&#21644;&#31616;&#21270;&#30340;&#27969;&#31243;&#65292;&#27604;&#32423;&#32852;&#26041;&#27861;&#26356;&#20855;&#20248;&#21183;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#36941;&#30452;&#25509;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#26550;&#26500;UnitY&#65292;&#39318;&#20808;&#29983;&#25104;&#25991;&#26412;&#34920;&#31034;&#65292;&#20854;&#27425;&#39044;&#27979;&#31163;&#25955;&#30340;&#22768;&#23398;&#21333;&#20803;&#12290;&#36890;&#36807;&#31532;&#19968;&#36941;&#35299;&#30721;&#22120;&#30340;&#23376;&#35789;&#39044;&#27979;&#12289;&#39640;&#32423;&#30340;&#20004;&#36941;&#35299;&#30721;&#22120;&#26550;&#26500;&#35774;&#35745;&#21644;&#25628;&#32034;&#31574;&#30053;&#20197;&#21450;&#26356;&#22909;&#30340;&#35757;&#32451;&#27491;&#21017;&#21270;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#25105;&#20204;&#22522;&#20110;&#33258;&#30417;&#30563;&#21435;&#22122;&#33258;&#32534;&#30721;&#20219;&#21153;&#23545;&#31532;&#19968;&#36941;&#25991;&#26412;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#35268;&#27169;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#65292;UnitY&#27604;&#21333;&#36941;&#35821;&#38899;&#21040;&#21333;&#20803;&#32763;&#35793;&#27169;&#22411;&#30340;ASR-BLEU&#25552;&#39640;&#20102;2.5-4.2&#20010;&#65292;&#24182;&#19988;&#35299;&#30721;&#36895;&#24230;&#25552;&#39640;&#20102;2.83&#20493;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#31532;&#20108;&#36941;&#39044;&#27979;&#39057;&#35889;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Direct speech-to-speech translation (S2ST), in which all components can be optimized jointly, is advantageous over cascaded approaches to achieve fast inference with a simplified pipeline. We present a novel two-pass direct S2ST architecture, UnitY, which first generates textual representations and predicts discrete acoustic units subsequently. We enhance the model performance by subword prediction in the first-pass decoder, advanced two-pass decoder architecture design and search strategy, and better training regularization. To leverage large amounts of unlabeled text data, we pre-train the first-pass text decoder based on the self-supervised denoising auto-encoding task. Experimental evaluations on benchmark datasets at various data scales demonstrate that UnitY outperforms a single-pass speech-to-unit translation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up. We show that the proposed methods boost the performance even when predicting spectrogram in the second pass. However
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#30340;&#21452;&#37325;&#23545;&#40784;&#22810;&#35821;&#35328;&#35299;&#26512;&#22120;&#65292;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#22810;&#35821;&#35328;&#21644;&#20195;&#30721;&#20999;&#25442;&#35821;&#20041;&#35299;&#26512;&#31995;&#32479;&#30340;&#38646;-shot&#24615;&#33021;&#65292;&#25552;&#39640;mBERT&#36716;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.08054</link><description>&lt;p&gt;
DAMP&#65306;&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#30340;&#21452;&#37325;&#23545;&#40784;&#22810;&#35821;&#35328;&#35299;&#26512;&#22120;
&lt;/p&gt;
&lt;p&gt;
DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue. (arXiv:2212.08054v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#30340;&#21452;&#37325;&#23545;&#40784;&#22810;&#35821;&#35328;&#35299;&#26512;&#22120;&#65292;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#22810;&#35821;&#35328;&#21644;&#20195;&#30721;&#20999;&#25442;&#35821;&#20041;&#35299;&#26512;&#31995;&#32479;&#30340;&#38646;-shot&#24615;&#33021;&#65292;&#25552;&#39640;mBERT&#36716;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#34394;&#25311;&#21161;&#25163;&#20351;&#29992;&#20869;&#37096;&#35821;&#20041;&#35299;&#26512;&#24341;&#25806;&#23558;&#29992;&#25143;&#35805;&#35821;&#36716;&#21270;&#20026;&#21487;&#25805;&#20316;&#21629;&#20196;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#20041;&#35299;&#26512;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#22810;&#35821;&#35328;&#36716;&#31227;&#20219;&#21153;&#65292;&#20854;&#36716;&#31227;&#25928;&#29575;&#27604;&#20854;&#20182;&#20219;&#21153;&#20302;&#12290;&#22312;&#20840;&#29699;&#24066;&#22330;&#65288;&#22914;&#21360;&#24230;&#21644;&#25289;&#19969;&#32654;&#27954;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22240;&#20026;&#21452;&#35821;&#29992;&#25143;&#39057;&#32321;&#20999;&#25442;&#35821;&#35328;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#30340;&#22810;&#35821;&#35328;&#23545;&#40784;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22810;&#35821;&#35328;&#21644;&#20195;&#30721;&#20999;&#25442;&#35821;&#20041;&#35299;&#26512;&#31995;&#32479;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#23545;&#27604;&#23545;&#40784;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#33521;&#25991;&#24615;&#33021;&#21644;&#36716;&#31227;&#25928;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#21463;&#38480;&#21046;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#36229;&#21442;&#25968;&#30340;&#23545;&#25239;&#24615;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#21452;&#37325;&#23545;&#40784;&#22810;&#35821;&#35328;&#35299;&#26512;&#22120;&#65288;DAMP&#65289;&#20998;&#21035;&#23558;Spanglish&#12289;Hinglish&#21644;&#22810;&#35821;&#35328;&#20219;&#21153;&#23548;&#21521;&#35299;&#26512;&#22522;&#20934;&#30340;mBERT&#36716;&#31227;&#24615;&#33021;&#25552;&#39640;&#20102;3&#20493;&#12289;6&#20493;&#21644;81&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern virtual assistants use internal semantic parsing engines to convert user utterances to actionable commands. However, prior work has demonstrated that semantic parsing is a difficult multilingual transfer task with low transfer efficiency compared to other tasks. In global markets such as India and Latin America, this is a critical issue as switching between languages is prevalent for bilingual users. In this work we dramatically improve the zero-shot performance of a multilingual and codeswitched semantic parsing system using two stages of multilingual alignment. First, we show that constrastive alignment pretraining improves both English performance and transfer efficiency. We then introduce a constrained optimization approach for hyperparameter-free adversarial alignment during finetuning. Our Doubly Aligned Multilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and 81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing benchmarks respectively
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36328;&#26041;&#35328;&#30340;&#33521;&#25991;NLP&#26694;&#26550;Multi-VALUE&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23558;&#26631;&#20934;&#32654;&#24335;&#33521;&#35821;&#26144;&#23556;&#20026;50&#31181;&#33521;&#35821;&#26041;&#35328;&#30340;&#21512;&#25104;&#24418;&#24335;&#65292;&#29992;&#20110;&#35780;&#20272;&#12289;&#23454;&#29616;&#33521;&#24335;&#26041;&#35328;&#20020;&#36817;&#24615;&#65292;&#24182;&#22312;&#38750;&#26631;&#20934;&#26041;&#35328;&#19978;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2212.08011</link><description>&lt;p&gt;
&#22810;&#20803;&#20215;&#20540;&#65306;&#36328;&#26041;&#35328;&#30340;&#33521;&#25991;NLP&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Multi-VALUE: A Framework for Cross-Dialectal English NLP. (arXiv:2212.08011v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08011
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36328;&#26041;&#35328;&#30340;&#33521;&#25991;NLP&#26694;&#26550;Multi-VALUE&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23558;&#26631;&#20934;&#32654;&#24335;&#33521;&#35821;&#26144;&#23556;&#20026;50&#31181;&#33521;&#35821;&#26041;&#35328;&#30340;&#21512;&#25104;&#24418;&#24335;&#65292;&#29992;&#20110;&#35780;&#20272;&#12289;&#23454;&#29616;&#33521;&#24335;&#26041;&#35328;&#20020;&#36817;&#24615;&#65292;&#24182;&#22312;&#38750;&#26631;&#20934;&#26041;&#35328;&#19978;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22495;&#12289;&#31038;&#20250;&#21644;&#32463;&#27982;&#22240;&#32032;&#24341;&#36215;&#30340;&#26041;&#35328;&#24046;&#24322;&#23545;&#35768;&#22810;&#35821;&#35328;&#25216;&#26415;&#29992;&#25143;&#36896;&#25104;&#20102;&#24615;&#33021;&#24046;&#24322;&#12290;&#26222;&#24800;&#21644;&#20844;&#24179;&#30340;&#35821;&#35328;&#25216;&#26415;&#24517;&#39035;&#26159;&#20020;&#36817;&#26041;&#35328;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;&#26041;&#35328;&#36716;&#25442;&#26102;&#24615;&#33021;&#20445;&#25345;&#19981;&#21464;&#12290;&#30001;&#20110;&#23427;&#20204;&#30340;&#35774;&#35745;&#21644;&#27979;&#35797;&#37117;&#26159;&#22522;&#20110;&#26631;&#20934;&#32654;&#24335;&#33521;&#35821;&#65288;SAE&#65289;&#65292;&#30446;&#21069;&#30340;&#31995;&#32479;&#24448;&#24448;&#19981;&#33021;&#36798;&#21040;&#36825;&#20010;&#29702;&#24819;&#29366;&#24577;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#36164;&#28304;&#22871;&#20214;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#23454;&#29616;&#33521;&#24335;&#26041;&#35328;&#20020;&#36817;&#24615;&#65292;&#31216;&#20043;&#20026;Multi-VALUE&#65292;&#23427;&#26159;&#19968;&#20010;&#21487;&#25511;&#21046;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#32763;&#35793;&#31995;&#32479;&#65292;&#35206;&#30422;&#20102;50&#31181;&#33521;&#35821;&#26041;&#35328;&#21644;189&#20010;&#29420;&#29305;&#30340;&#35821;&#35328;&#29305;&#24449;&#12290;Multi-VALUE&#23558;SAE&#26144;&#23556;&#21040;&#27599;&#31181;&#26041;&#35328;&#30340;&#21512;&#25104;&#24418;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#36825;&#20010;&#31995;&#32479;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#65292;&#27979;&#35797;&#38382;&#31572;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#20041;&#35299;&#26512;&#12290;&#21387;&#21147;&#27979;&#35797;&#25581;&#31034;&#20102;&#22312;&#38750;&#26631;&#20934;&#26041;&#35328;&#19978;&#30340;&#39046;&#20808;&#27169;&#22411;&#30340;&#26174;&#30528;&#24615;&#33021;&#24046;&#36317;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#31995;&#32479;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialect differences caused by regional, social, and economic factors cause performance discrepancies for many groups of language technology users. Inclusive and equitable language technology must critically be dialect invariant, meaning that performance remains constant over dialectal shifts. Current systems often fall short of this ideal since they are designed and tested on a single dialect: Standard American English (SAE). We introduce a suite of resources for evaluating and achieving English dialect invariance. The resource is called Multi-VALUE, a controllable rule-based translation system spanning 50 English dialects and 189 unique linguistic features. Multi-VALUE maps SAE to synthetic forms of each dialect. First, we use this system to stress tests question answering, machine translation, and semantic parsing. Stress tests reveal significant performance disparities for leading models on non-standard dialects. Second, we use this system as a data augmentation technique to improve
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26816;&#32034;&#25110;&#29983;&#25104;&#22270;&#20687;&#30340;&#35270;&#35273;&#22686;&#24378;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#26222;&#36941;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;PLM&#25110;NLP&#20219;&#21153;&#65292;&#24182;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;BERT&#12289;RoBERTa&#12289;BART&#21644;T5&#19978;&#25345;&#32493;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.07937</link><description>&lt;p&gt;
&#26080;&#38656;&#22270;&#20687;&#30340;&#35270;&#35273;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Visually-augmented pretrained language models for NLP tasks without images. (arXiv:2212.07937v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07937
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26816;&#32034;&#25110;&#29983;&#25104;&#22270;&#20687;&#30340;&#35270;&#35273;&#22686;&#24378;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#26222;&#36941;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;PLM&#25110;NLP&#20219;&#21153;&#65292;&#24182;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;BERT&#12289;RoBERTa&#12289;BART&#21644;T5&#19978;&#25345;&#32493;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#36890;&#36807;&#32431;&#25991;&#26412;&#33258;&#30417;&#30563;&#35757;&#32451;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20182;&#20204;&#32570;&#20047;&#35270;&#35273;&#35821;&#20041;&#25110;&#24120;&#35782;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#24120;&#24120;&#20381;&#36182;&#20110;&#26126;&#30830;&#30340;&#22270;&#20687;&#26469;&#36827;&#34892;&#35270;&#35273;&#30693;&#35782;&#22686;&#24378;(&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#30340;&#26816;&#32034;&#25110;&#29983;&#25104;)&#65292;&#24182;&#19988;&#20182;&#20204;&#20063;&#20250;&#20026;&#25972;&#20010;&#36755;&#20837;&#25991;&#26412;&#36827;&#34892;&#22686;&#24378;&#65292;&#32780;&#19981;&#32771;&#34385;&#26159;&#21542;&#23454;&#38469;&#19978;&#38656;&#35201;&#22312;&#29305;&#23450;&#30340;&#36755;&#20837;&#25110;&#20219;&#21153;&#20013;&#36827;&#34892;&#22686;&#24378;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;&#22686;&#24378;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#26222;&#36941;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;PLM&#25110;NLP&#20219;&#21153;&#65292;&#26080;&#38656;&#20351;&#29992;&#20219;&#20309;&#26816;&#32034;&#25110;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#31216;&#20026;VAWI&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;BERT&#12289;RoBERTa&#12289;BART&#21644;T5&#19978;&#25345;&#32493;&#22320;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#21313;&#20010;&#20219;&#21153;&#19978;&#32988;&#36807;&#20960;&#20010;&#31454;&#20105;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#20844;&#24320;&#21487;&#29992;&#20110;\url{https://github.com/RUCAIBox/VAWI}&#12290;
&lt;/p&gt;
&lt;p&gt;
Although pre-trained language models~(PLMs) have shown impressive performance by text-only self-supervised training, they are found lack of visual semantics or commonsense. Existing solutions often rely on explicit images for visual knowledge augmentation (requiring time-consuming retrieval or generation), and they also conduct the augmentation for the whole input text, without considering whether it is actually needed in specific inputs or tasks. To address these issues, we propose a novel \textbf{V}isually-\textbf{A}ugmented fine-tuning approach that can be generally applied to various PLMs or NLP tasks, \textbf{W}ithout using any retrieved or generated \textbf{I}mages, namely \textbf{VAWI}. Experimental results show that our approach can consistently improve the performance of BERT, RoBERTa, BART, and T5 at different scales, and outperform several competitive baselines on ten tasks. Our codes and data are publicly available at~\url{https://github.com/RUCAIBox/VAWI}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NPPrompt&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#23436;&#20840;&#38646;-shot&#23398;&#20064;&#22120;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;NPPrompt&#19981;&#38656;&#35201;&#20351;&#29992;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#25110;&#32773;&#26500;&#24314;&#25552;&#31034;&#65292;&#21482;&#38656;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;NPPrompt&#22312;&#25991;&#26412;&#20998;&#31867;&#12289;&#25991;&#26412;&#34164;&#21547;&#12289;&#30456;&#20284;&#25991;&#26412;&#26816;&#32034;&#21644;&#25913;&#20889;&#31561;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#22823;&#24133;&#20248;&#20110;&#20197;&#21069;&#26368;&#22909;&#30340;&#23436;&#20840;&#38646;-shot&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.06950</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#23436;&#20840;&#38646;-shot&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models Can be Fully Zero-Shot Learners. (arXiv:2212.06950v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NPPrompt&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#23436;&#20840;&#38646;-shot&#23398;&#20064;&#22120;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;NPPrompt&#19981;&#38656;&#35201;&#20351;&#29992;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#25110;&#32773;&#26500;&#24314;&#25552;&#31034;&#65292;&#21482;&#38656;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;NPPrompt&#22312;&#25991;&#26412;&#20998;&#31867;&#12289;&#25991;&#26412;&#34164;&#21547;&#12289;&#30456;&#20284;&#25991;&#26412;&#26816;&#32034;&#21644;&#25913;&#20889;&#31561;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#22823;&#24133;&#20248;&#20110;&#20197;&#21069;&#26368;&#22909;&#30340;&#23436;&#20840;&#38646;-shot&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#26631;&#35760;&#25110;&#39069;&#22806;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22914;&#20309;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#25193;&#23637;&#21040;&#35768;&#22810;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#65311;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#32463;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#23545;&#19979;&#28216;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#35201;&#20040;&#38656;&#35201;&#25163;&#21160;&#26500;&#24314;&#36866;&#24403;&#30340;&#25552;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#23436;&#20840;&#38646;-shot&#35821;&#35328;&#29702;&#35299;&#30340;&#38750;&#21442;&#25968;&#25552;&#31034;PLM&#65288;NPPrompt&#65289;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;NPPrompt&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#25110;&#39069;&#22806;&#30340;&#21407;&#22987;&#35821;&#26009;&#24211;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#20154;&#31867;&#26500;&#24314;&#20840;&#38754;&#30340;&#25552;&#31034;&#26631;&#31614;&#35789;&#38598;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;NPPrompt&#21644;&#20197;&#21069;&#30340;&#20027;&#35201;&#23569;&#37327;&#26679;&#26412;&#21644;&#38646;&#23556;&#26041;&#27861;&#65306;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#12289;&#25991;&#26412;&#34164;&#21547;&#12289;&#30456;&#20284;&#25991;&#26412;&#26816;&#32034;&#21644;&#25913;&#20889;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;NPPrompt&#26174;&#31034;&#20986;&#22823;&#24133;&#20248;&#20110;&#20197;&#21069;&#26368;&#22909;&#30340;&#23436;&#20840;&#38646;-shot&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we extend a pre-trained model to many language understanding tasks, without labeled or additional unlabeled data? Pre-trained language models (PLMs) have been effective for a wide range of NLP tasks. However, existing approaches either require fine-tuning on downstream labeled datasets or manually constructing proper prompts. In this paper, we propose nonparametric prompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike previous methods, NPPrompt uses only pre-trained language models and does not require any labeled data or additional raw corpus for further fine-tuning, nor does it rely on humans to construct a comprehensive set of prompt label words. We evaluate NPPrompt against previous major few-shot and zero-shot learning methods on diverse NLP tasks: including text classification, text entailment, similar text retrieval, and paraphrasing. Experimental results demonstrate that our NPPrompt outperforms the previous best fully zero-shot method by big margi
&lt;/p&gt;</description></item><item><title>NPM&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#38750;&#21442;&#25968;&#20998;&#24067;&#26367;&#25442;softmax&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#31232;&#26377;&#27169;&#24335;&#21644;&#39044;&#27979;&#32597;&#35265;&#25110;&#20960;&#20046;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#65292;&#24182;&#22312;16&#39033;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#26356;&#22823;&#30340;&#21442;&#25968;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.01349</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Masked Language Modeling. (arXiv:2212.01349v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01349
&lt;/p&gt;
&lt;p&gt;
NPM&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#38750;&#21442;&#25968;&#20998;&#24067;&#26367;&#25442;softmax&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#31232;&#26377;&#27169;&#24335;&#21644;&#39044;&#27979;&#32597;&#35265;&#25110;&#20960;&#20046;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#65292;&#24182;&#22312;16&#39033;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#26356;&#22823;&#30340;&#21442;&#25968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36890;&#36807;&#26377;&#38480;&#35789;&#27719;&#34920;&#19978;&#30340; softmax &#26469;&#39044;&#27979;&#26631;&#35760;&#65292;&#36825;&#21487;&#33021;&#20351;&#24471;&#39044;&#27979;&#31232;&#26377;&#26631;&#35760;&#25110;&#30701;&#35821;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102; NPM&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#23545;&#27599;&#20010;&#21442;&#32771;&#35821;&#26009;&#24211;&#20013;&#30701;&#35821;&#30340;&#38750;&#21442;&#25968;&#20998;&#24067;&#26367;&#25442;&#27492; softmax &#30340;&#38750;&#21442;&#25968;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#12290;NPM &#20165;&#36890;&#36807;&#20174;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#26631;&#35760;&#26469;&#22635;&#20889; [MASK]&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; NPM &#21487;&#20197;&#36890;&#36807;&#23545;&#27604;&#24615;&#30446;&#26631;&#21644;&#25209;&#37327;&#36817;&#20284;&#20840;&#35821;&#26009;&#24211;&#26816;&#32034;&#26377;&#25928;&#22320;&#35757;&#32451;&#12290;&#23545; 16 &#39033;&#20219;&#21153;&#36827;&#34892;&#38646;&#26679;&#26412;&#35780;&#20272;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#20107;&#23454;&#25506;&#38024;&#21644;&#38382;&#39064;&#22238;&#31572;&#65292;&#35777;&#26126; NPM &#36229;&#36807;&#20102;&#26174;&#30528;&#26356;&#22823;&#30340;&#21442;&#25968;&#27169;&#22411;&#65292;&#26080;&#35770;&#20351;&#29992;&#25110;&#19981;&#20351;&#29992;&#26816;&#32034;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#22312;&#22788;&#29702;&#31232;&#26377;&#27169;&#24335;&#65288;&#35789;&#20041;&#25110;&#20107;&#23454;&#65289;&#21644;&#39044;&#27979;&#32597;&#35265;&#25110;&#20960;&#20046;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#65288;&#22914;&#38750;&#25289;&#19969;&#25991;&#33050;&#26412;&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312; github.com/facebookresearch/NPM &#19978;&#21457;&#24067;&#20102;&#27169;&#22411;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases. We introduce NPM, the first nonparametric masked language model that replaces this softmax with a nonparametric distribution over every phrase in a reference corpus. NPM fills in the [MASK] solely from retrieving a token from a text corpus. We show that NPM can be efficiently trained with a contrastive objective and an in-batch approximation to full corpus retrieval. Zero-shot evaluation on 16 tasks including classification, fact probing and question answering demonstrates that NPM outperforms significantly larger parametric models, either with or without a retrieve-and-generate approach. It is particularly better at dealing with rare patterns (word senses or facts) and predicting rare or nearly unseen words (e.g., non-Latin script). We release the model and code at github.com/facebookresearch/NPM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt&#23398;&#20064;&#21644;&#20256;&#25773;&#32467;&#26500;&#30340;&#38646;&#26679;&#26412;&#35875;&#35328;&#26816;&#27979;&#26694;&#26550;&#65292;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#19981;&#21516;&#39046;&#22495;&#21644;&#35821;&#35328;&#30340;&#35875;&#35328;&#65292;&#24182;&#21487;&#36866;&#24212;&#38750;&#39044;&#26009;&#20013;&#26029;&#20107;&#20214;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2212.01117</link><description>&lt;p&gt;
&#22522;&#20110;Prompt&#23398;&#20064;&#19982;&#20256;&#25773;&#32467;&#26500;&#30340;&#38646;&#26679;&#26412;&#35875;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning. (arXiv:2212.01117v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt&#23398;&#20064;&#21644;&#20256;&#25773;&#32467;&#26500;&#30340;&#38646;&#26679;&#26412;&#35875;&#35328;&#26816;&#27979;&#26694;&#26550;&#65292;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#19981;&#21516;&#39046;&#22495;&#21644;&#35821;&#35328;&#30340;&#35875;&#35328;&#65292;&#24182;&#21487;&#36866;&#24212;&#38750;&#39044;&#26009;&#20013;&#26029;&#20107;&#20214;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#26102;&#20195;&#65292;&#35875;&#35328;&#38543;&#30528;&#20107;&#20214;&#30340;&#21457;&#29983;&#32780;&#20256;&#25773;&#65292;&#20005;&#37325;&#24433;&#21709;&#20102;&#30495;&#30456;&#30340;&#20256;&#25773;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#27880;&#36164;&#28304;&#65292;&#24456;&#38590;&#26816;&#27979;&#20986;&#20351;&#29992;&#23569;&#25968;&#35821;&#35328;&#30340;&#35875;&#35328;&#12290;&#32780;&#19988;&#65292;&#26152;&#22825;&#27809;&#26377;&#28041;&#21450;&#21040;&#30340;&#38750;&#39044;&#26009;&#20013;&#26029;&#20107;&#20214;&#21152;&#21095;&#20102;&#25968;&#25454;&#36164;&#28304;&#30340;&#31232;&#32570;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt&#23398;&#20064;&#30340;&#26032;&#22411;&#38646;&#26679;&#26412;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#19981;&#21516;&#39046;&#22495;&#25110;&#29992;&#19981;&#21516;&#35821;&#35328;&#23637;&#29616;&#30340;&#35875;&#35328;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#31038;&#20132;&#23186;&#20307;&#19978;&#20256;&#25773;&#30340;&#35875;&#35328;&#34920;&#31034;&#20026;&#22810;&#26679;&#30340;&#20256;&#25773;&#32447;&#31243;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#23618;Prompt&#32534;&#30721;&#26426;&#21046;&#65292;&#23398;&#20064;&#20102;&#26080;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#20197;&#29992;&#20110;&#20419;&#36827;&#23545;&#19981;&#21516;&#39046;&#22495;&#21644;&#35821;&#35328;&#19979;&#30340;&#35875;&#35328;&#25968;&#25454;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#20256;&#25773;&#32447;&#31243;&#20013;&#24314;&#27169;&#39046;&#22495;&#19981;&#21464;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#20197;&#25972;&#21512;&#26377;&#24433;&#21709;&#21147;&#30340;&#31038;&#21306;&#21453;&#24212;&#30340;&#32467;&#26500;&#20301;&#32622;&#34920;&#31034;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#39046;&#22495;&#36866;&#24212;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#26032;&#30340;&#34394;&#25311;&#21709;&#24212;&#26426;&#21046;&#65292;&#20197;&#21152;&#24378;&#27169;&#22411;&#30340;&#25512;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of rumors along with breaking events seriously hinders the truth in the era of social media. Previous studies reveal that due to the lack of annotated resources, rumors presented in minority languages are hard to be detected. Furthermore, the unforeseen breaking events not involved in yesterday's news exacerbate the scarcity of data resources. In this work, we propose a novel zero-shot framework based on prompt learning to detect rumors falling in different domains or presented in different languages. More specifically, we firstly represent rumor circulated on social media as diverse propagation threads, then design a hierarchical prompt encoding mechanism to learn language-agnostic contextual representations for both prompts and rumor data. To further enhance domain adaptation, we model the domain-invariant structural features from the propagation threads, to incorporate structural position representations of influential community response. In addition, a new virtual respon
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#20196;&#29260;&#19982;&#21442;&#32771;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#30446;&#26631;&#65292;&#21487;&#20197;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#21448;&#21487;&#20197;&#20445;&#25345;&#35843;&#25972;&#36136;&#37327;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#22686;&#21152;&#21487;&#24573;&#30053;&#19981;&#35745;&#12290;</title><link>http://arxiv.org/abs/2211.16550</link><description>&lt;p&gt;
&#29992;&#20110;&#29983;&#25104;&#35821;&#35328;&#30340;&#36719;&#23545;&#40784;&#30446;&#26631;&#30340;&#40065;&#26834;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Soft Alignment Objectives for Robust Adaptation of Language Generation. (arXiv:2211.16550v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#20196;&#29260;&#19982;&#21442;&#32771;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#30446;&#26631;&#65292;&#21487;&#20197;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#21448;&#21487;&#20197;&#20445;&#25345;&#35843;&#25972;&#36136;&#37327;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#22686;&#21152;&#21487;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#20801;&#35768;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#24212;&#29992;&#39046;&#22495;&#36716;&#31227;&#36896;&#25104;&#30340;&#29305;&#23450;&#32570;&#38519;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#22312;&#39046;&#22495;&#20869;&#25968;&#25454;&#19978;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#26469;&#36827;&#34892;&#20256;&#32479;&#36866;&#24212;&#20250;&#36805;&#36895;&#21066;&#24369;&#27169;&#22411;&#25512;&#24191;&#21040;&#20854;&#20182;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#20351;&#24471;&#35843;&#25972;&#21518;&#27169;&#22411;&#30340;&#26080;&#38480;&#37096;&#32626;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#12290;&#26412;&#24037;&#20316;&#20171;&#32461;&#20102;&#24314;&#31435;&#22312;&#39044;&#27979;&#20196;&#29260;&#19982;&#21442;&#32771;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36991;&#20813;&#21333;&#20010;&#27491;&#30830;&#39044;&#27979;&#30340;&#24120;&#35265;&#20551;&#35774;&#65292;&#36890;&#36807;&#26500;&#24314;&#26469;&#33258;&#20196;&#29260;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#35757;&#32451;&#30446;&#26631;&#21487;&#20197;&#32531;&#35299;&#39046;&#22495;&#36866;&#24212;&#26399;&#38388;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#22312;&#20445;&#25345;&#35843;&#25972;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#21487;&#24573;&#30053;&#30340;&#35745;&#31639;&#25104;&#26412;&#22686;&#21152;&#12290;&#22312;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#65292;&#22522;&#20110;&#36830;&#32493;&#30340;&#20196;&#29260;&#30456;&#20284;&#24230;&#30340;&#30446;&#26631;&#24341;&#39046;&#20102;&#39640;&#25928;&#20294;&#26174;&#24335;&#20196;&#29260;&#32423;&#30446;&#26631;&#21644;&#20855;&#26377;&#34920;&#29616;&#21147;&#30340;&#22522;&#20110;&#36830;&#32493;&#20196;&#29260;&#34920;&#31034;&#30340;&#30446;&#26631;&#20043;&#38388;&#20013;&#38388;&#22320;&#24102;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation allows generative language models to address specific flaws caused by the domain shift of their application. However, the traditional adaptation by further training on in-domain data rapidly weakens the model's ability to generalize to other domains, making the open-ended deployments of the adapted models prone to errors. This work introduces novel training objectives built upon a semantic similarity of the predicted tokens to the reference.  Our results show that (1) avoiding the common assumption of a single correct prediction by constructing the training target from tokens' semantic similarity can mitigate catastrophic forgetting during domain adaptation, while (2) preserving the quality of the adaptation, (3) with negligible additions to compute costs.  In the broader context, the objectives grounded in a continuous token similarity pioneer the exploration of the middle ground between the efficient but na\"{\i}ve exact-match token-level objectives and expressive b
&lt;/p&gt;</description></item><item><title>CoNAL&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20998;&#31867;&#27169;&#22411;&#38477;&#20302;&#22312;&#26032;&#39062;&#31867;&#21035;&#19978;&#30340;&#36807;&#24230;&#33258;&#20449;&#65292;&#25552;&#39640;&#26816;&#27979;&#21644;&#25918;&#24323;&#36825;&#20123;&#31867;&#21035;&#31034;&#20363;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.15718</link><description>&lt;p&gt;
&#23545;&#27604;&#26032;&#39062;&#24615;&#22686;&#24378;&#23398;&#20064;: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#31163;&#32676;&#20540;
&lt;/p&gt;
&lt;p&gt;
Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models. (arXiv:2211.15718v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15718
&lt;/p&gt;
&lt;p&gt;
CoNAL&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20998;&#31867;&#27169;&#22411;&#38477;&#20302;&#22312;&#26032;&#39062;&#31867;&#21035;&#19978;&#30340;&#36807;&#24230;&#33258;&#20449;&#65292;&#25552;&#39640;&#26816;&#27979;&#21644;&#25918;&#24323;&#36825;&#20123;&#31867;&#21035;&#31034;&#20363;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#65292;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#21487;&#33021;&#20250;&#36935;&#21040;&#26080;&#27861;&#27491;&#30830;&#39044;&#27979;&#30340;&#26032;&#39062;&#31867;&#21035;&#30340;&#31034;&#20363;&#12290;&#26377;&#36873;&#25321;&#22320;&#39044;&#27979;&#22312;&#20302;&#32622;&#20449;&#24230;&#31034;&#20363;&#19978;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#24120;&#24120;&#36807;&#20110;&#33258;&#20449;&#12290;&#20026;&#20102;&#32416;&#27491;&#36825;&#31181;&#36807;&#24230;&#33258;&#20449;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#27604;&#26032;&#39062;&#24615;&#22686;&#24378;&#23398;&#20064;&#65288;CoNAL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20004;&#27493;&#26041;&#27861;&#65292;&#23427;&#29983;&#25104;&#20195;&#34920;&#26032;&#39062;&#31867;&#21035;&#30340;OOD&#31034;&#20363;&#65292;&#28982;&#21518;&#35757;&#32451;&#20197;&#38477;&#20302;&#32622;&#20449;&#24230;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20004;&#27425;&#65292;&#25105;&#20204;&#29983;&#25104;OOD&#31034;&#20363;&#65306;&#25105;&#20204;&#25552;&#31034;&#23427;&#26522;&#20030;&#30456;&#20851;&#30340;&#26032;&#39062;&#31867;&#21035;&#65292;&#28982;&#21518;&#29983;&#25104;&#27599;&#20010;&#26032;&#39062;&#31867;&#21035;&#21305;&#37197;&#20219;&#21153;&#26684;&#24335;&#30340;&#31034;&#20363;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#26032;&#39062;&#30340;&#23545;&#27604;&#30446;&#26631;&#26469;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#35813;&#30446;&#26631;&#40723;&#21169;&#22312;&#29983;&#25104;&#30340;OOD&#31034;&#20363;&#19978;&#20855;&#26377;&#27604;&#35757;&#32451;&#31034;&#20363;&#26356;&#20302;&#30340;&#32622;&#20449;&#24230;&#12290;&#24403;&#20351;&#29992;CoNAL&#35757;&#32451;&#26102;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20998;&#31867;&#22120;&#22312;&#26816;&#27979;&#21644;&#25918;&#24323;&#26032;&#39062;&#31867;&#21035;&#31034;&#20363;&#26041;&#38754;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#22823;&#24133;&#25552;&#39640;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#24050;&#30693;&#31867;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many task settings, text classification models are likely to encounter examples from novel classes on which they cannot predict correctly. Selective prediction, in which models abstain on low-confidence examples, provides a possible solution, but existing models are often overly confident on unseen classes. To remedy this overconfidence, we introduce Contrastive Novelty-Augmented Learning (CoNAL), a two-step method that generates OOD examples representative of novel classes, then trains to decrease confidence on them. First, we generate OOD examples by prompting a large language model twice: we prompt it to enumerate relevant novel classes, then generate examples from each novel class matching the task format. Second, we train a classifier with a novel contrastive objective that encourages lower confidence on generated OOD examples than training examples. When trained with CoNAL, classifiers improve in their ability to detect and abstain on novel class examples over prior methods by
&lt;/p&gt;</description></item><item><title>SongRewriter&#26159;&#19968;&#31181;&#20013;&#25991;&#27468;&#26354;&#25913;&#32534;&#31995;&#32479;&#65292;&#21487;&#20197;&#37325;&#26032;&#32534;&#20889;&#29616;&#26377;&#27468;&#26354;&#30340;&#27468;&#35789;&#29983;&#25104;&#19982;&#26059;&#24459;&#30456;&#37197;&#30340;&#27468;&#35789;&#65292;&#21487;&#24110;&#21161;&#27809;&#26377;&#26059;&#24459;&#32452;&#25104;&#30693;&#35782;&#30340;&#29992;&#25143;&#65292;&#26131;&#20110;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2211.15037</link><description>&lt;p&gt;
SongRewriter: &#19968;&#31181;&#20855;&#26377;&#21487;&#25511;&#20869;&#23481;&#21644;&#38901;&#24459;&#26041;&#26696;&#30340;&#20013;&#25991;&#27468;&#26354;&#25913;&#32534;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SongRewriter: A Chinese Song Rewriting System with Controllable Content and Rhyme Scheme. (arXiv:2211.15037v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15037
&lt;/p&gt;
&lt;p&gt;
SongRewriter&#26159;&#19968;&#31181;&#20013;&#25991;&#27468;&#26354;&#25913;&#32534;&#31995;&#32479;&#65292;&#21487;&#20197;&#37325;&#26032;&#32534;&#20889;&#29616;&#26377;&#27468;&#26354;&#30340;&#27468;&#35789;&#29983;&#25104;&#19982;&#26059;&#24459;&#30456;&#37197;&#30340;&#27468;&#35789;&#65292;&#21487;&#24110;&#21161;&#27809;&#26377;&#26059;&#24459;&#32452;&#25104;&#30693;&#35782;&#30340;&#29992;&#25143;&#65292;&#26131;&#20110;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#27468;&#35789;&#29983;&#25104;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#26080;&#27861;&#29983;&#25104;&#19982;&#30456;&#37197;&#30340;&#26059;&#24459;&#36827;&#34892;&#28436;&#21809;&#65292;&#22240;&#27492;&#20855;&#26377;&#38480;&#21046;&#24615;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27468;&#26354;&#25913;&#32534;&#31995;&#32479;&#65292;&#21487;&#36890;&#36807;&#37325;&#26032;&#32534;&#20889;&#29616;&#26377;&#27468;&#26354;&#30340;&#27468;&#35789;&#26469;&#29983;&#25104;&#19982;&#35813;&#26059;&#24459;&#30456;&#23481;&#30340;&#27468;&#35789;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SongRewriter&#65292;&#19968;&#31181;&#21487;&#25511;&#30340;&#20013;&#25991;&#27468;&#35789;&#29983;&#25104;&#21644;&#32534;&#36753;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#27809;&#26377;&#26059;&#24459;&#32452;&#25104;&#30693;&#35782;&#30340;&#29992;&#25143;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#38543;&#26426;&#30340;&#22810;&#32423;&#23631;&#34109;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#29983;&#25104;&#36866;&#29992;&#20110;&#29983;&#25104;&#20840;&#26032;&#27468;&#35789;&#25110;&#32534;&#36753;&#23569;&#37327;&#29255;&#27573;&#30340;&#32479;&#19968;&#27169;&#22411;&#12290;&#20026;&#20102;&#25913;&#21892;&#29983;&#25104;&#36807;&#31243;&#30340;&#21487;&#25511;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21152;&#20837;&#20102;&#20851;&#38190;&#35789;&#25552;&#31034;&#26469;&#25511;&#21046;&#20869;&#23481;&#30340;&#35789;&#27719;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#35299;&#30721;&#32422;&#26463;&#21644;&#20803;&#38899;&#24314;&#27169;&#20219;&#21153;&#65292;&#20197;&#23454;&#29616;&#28789;&#27963;&#30340;&#21069;&#21518;&#32467;&#26500;&#21644;&#20869;&#38901;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although lyrics generation has achieved significant progress in recent years, it has limited practical applications because the generated lyrics cannot be performed without composing compatible melodies. In this work, we bridge this practical gap by proposing a song rewriting system which rewrites the lyrics of an existing song such that the generated lyrics are compatible with the rhythm of the existing melody and thus singable. In particular, we propose SongRewriter,a controllable Chinese lyrics generation and editing system which assists users without prior knowledge of melody composition. The system is trained by a randomized multi-level masking strategy which produces a unified model for generating entirely new lyrics or editing a few fragments. To improve the controllabiliy of the generation process, we further incorporate a keyword prompt to control the lexical choices of the content and propose novel decoding constraints and a vowel modeling task to enable flexible end and inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#38477;&#20302;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#27979;&#35797;&#22312;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.08794</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#20302;&#36164;&#28304;&#24494;&#35843;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations. (arXiv:2211.08794v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#38477;&#20302;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#27979;&#35797;&#22312;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21442;&#25968;&#30340;&#24040;&#22823;&#25968;&#37327;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24494;&#35843;&#23481;&#26131;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#20986;&#29616;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;PLM&#30340;&#38544;&#34255;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#20197;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;PLM&#30340;&#38544;&#34255;&#23618;&#20043;&#38388;&#25554;&#20837;&#38543;&#26426;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#26469;&#33258;&#21069;&#19968;&#23618;&#30340;&#28608;&#27963;&#36716;&#25442;&#20026;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#39304;&#36865;&#21040;&#19978;&#23618;&#12290;&#24494;&#35843;&#32467;&#26463;&#21518;&#65292;&#33258;&#32534;&#30721;&#22120;&#20250;&#34987;&#31227;&#38500;&#25481;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#21442;&#25968;&#25110;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#24207;&#21015;&#21644;&#26631;&#35760;&#32423;&#21035;&#30340;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the huge amount of parameters, fine-tuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts random autoencoders between the hidden layers of a PLM, which transform activations from the previous layers into a multi-view compressed representation before feeding it into the upper layers. The autoencoders are plugged out after fine-tuning, so our method does not add extra parameters or increase computation cost during inference. Our method demonstrates promising performance improvement across a wide range of sequence- and token-level low-resource NLP tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#27425;&#22810;&#26041;&#38754;&#27880;&#24847;&#21147;&#30340;&#21457;&#38899;&#35780;&#20272;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#38899;&#32032;&#12289;&#21333;&#35789;&#21644;&#35805;&#35821;&#30340;&#35821;&#35328;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#22312;&#35780;&#20272;&#20934;&#30830;&#24615;&#12289;&#27969;&#30021;&#24230;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.08102</link><description>&lt;p&gt;
&#22810;&#23618;&#27425;&#22810;&#26041;&#38754;&#27880;&#24847;&#21147;&#30340;&#21457;&#38899;&#35780;&#20272;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Pronunciation Assessment with Multi-Aspect Attention. (arXiv:2211.08102v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#27425;&#22810;&#26041;&#38754;&#27880;&#24847;&#21147;&#30340;&#21457;&#38899;&#35780;&#20272;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#38899;&#32032;&#12289;&#21333;&#35789;&#21644;&#35805;&#35821;&#30340;&#35821;&#35328;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#22312;&#35780;&#20272;&#20934;&#30830;&#24615;&#12289;&#27969;&#30021;&#24230;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#26159;&#35745;&#31639;&#26426;&#36741;&#21161;&#21457;&#38899;&#35757;&#32451;&#31995;&#32479;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#20026;&#20102;&#25552;&#20379;&#28145;&#20837;&#30340;&#21453;&#39304;&#65292;&#23545;&#21508;&#31181;&#31890;&#24230;&#65288;&#22914;&#38899;&#32032;&#65292;&#21333;&#35789;&#21644;&#35805;&#35821;&#65289;&#21644;&#21508;&#31181;&#26041;&#38754;&#65288;&#22914;&#20934;&#30830;&#24615;&#65292;&#27969;&#30021;&#24230;&#21644;&#23436;&#25972;&#24615;&#65289;&#36827;&#34892;&#21457;&#38899;&#35780;&#20998;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#26041;&#38754;&#22810;&#31890;&#24230;&#26041;&#27861;&#21516;&#26102;&#39044;&#27979;&#25152;&#26377;&#31890;&#24230;&#32423;&#21035;&#30340;&#25152;&#26377;&#26041;&#38754;&#65307;&#22240;&#27492;&#65292;&#23427;&#20204;&#38590;&#20197;&#25429;&#25417;&#38899;&#32032;&#65292;&#21333;&#35789;&#21644;&#35805;&#35821;&#30340;&#35821;&#35328;&#23618;&#27425;&#32467;&#26500;&#12290;&#35813;&#38480;&#21046;&#36827;&#19968;&#27493;&#23548;&#33268;&#24573;&#30053;&#22312;&#21516;&#19968;&#35821;&#35328;&#21333;&#20301;&#20869;&#30340;&#20146;&#23494;&#36328;&#26041;&#38754;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#27425;&#22810;&#26041;&#38754;&#27880;&#24847;&#21147;&#30340;&#21457;&#38899;&#35780;&#20272;&#27169;&#22411;&#65288;HiPAMA&#65289;&#65292;&#35813;&#27169;&#22411;&#25353;&#23618;&#27425;&#34920;&#31034;&#31890;&#24230;&#32423;&#21035;&#65292;&#20197;&#30452;&#25509;&#25429;&#33719;&#20854;&#35821;&#35328;&#32467;&#26500;&#65292;&#24182;&#24341;&#20837;&#22810;&#26041;&#38754;&#27880;&#24847;&#21147;&#65292;&#20197;&#21453;&#26144;&#22312;&#21516;&#19968;&#32423;&#21035;&#19978;&#21508;&#26041;&#38754;&#20043;&#38388;&#30340;&#20851;&#32852;&#20197;&#21019;&#24314;&#26356;&#20855;&#20869;&#28085;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#22312;&#20013;&#33521;&#25991;&#20960;&#20010;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35780;&#20272;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#30340;HiPAMA&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic pronunciation assessment is a major component of a computer-assisted pronunciation training system. To provide in-depth feedback, scoring pronunciation at various levels of granularity such as phoneme, word, and utterance, with diverse aspects such as accuracy, fluency, and completeness, is essential. However, existing multi-aspect multi-granularity methods simultaneously predict all aspects at all granularity levels; therefore, they have difficulty in capturing the linguistic hierarchy of phoneme, word, and utterance. This limitation further leads to neglecting intimate cross-aspect relations at the same linguistic unit. In this paper, we propose a Hierarchical Pronunciation Assessment with Multi-aspect Attention (HiPAMA) model, which hierarchically represents the granularity levels to directly capture their linguistic structures and introduces multi-aspect attention that reflects associations across aspects at the same level to create more connotative representations. By ob
&lt;/p&gt;</description></item><item><title>GreenPLM&#26159;&#19968;&#20010;&#33410;&#33021;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21487;&#21033;&#29992;&#21452;&#35821;&#35789;&#20856;&#23454;&#29616;&#20960;&#20046;&#26080;&#25104;&#26412;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#35775;&#38382;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#20943;&#23569;&#22823;&#35268;&#27169;&#27169;&#22411;&#35757;&#32451;&#33021;&#28304;&#28040;&#32791;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;18&#31181;&#35821;&#35328;&#30340;BERT&#27169;&#22411;&#20013;&#39564;&#35777;&#20102;&#20854;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.06993</link><description>&lt;p&gt;
GreenPLM&#65306;&#20960;&#20046;&#19981;&#38656;&#35201;&#25104;&#26412;&#30340;&#36328;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost. (arXiv:2211.06993v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06993
&lt;/p&gt;
&lt;p&gt;
GreenPLM&#26159;&#19968;&#20010;&#33410;&#33021;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21487;&#21033;&#29992;&#21452;&#35821;&#35789;&#20856;&#23454;&#29616;&#20960;&#20046;&#26080;&#25104;&#26412;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#35775;&#38382;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#20943;&#23569;&#22823;&#35268;&#27169;&#27169;&#22411;&#35757;&#32451;&#33021;&#28304;&#28040;&#32791;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;18&#31181;&#35821;&#35328;&#30340;BERT&#27169;&#22411;&#20013;&#39564;&#35777;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#20294;&#39640;&#26114;&#30340;&#35757;&#32451;&#25104;&#26412;&#21644;&#26377;&#38480;&#30340;&#25968;&#25454;&#36164;&#28304;&#38459;&#30861;&#20102;&#25152;&#26377;&#19990;&#30028;&#35821;&#35328;&#30340;&#20351;&#29992;&#32773;&#24179;&#31561;&#20998;&#20139;&#20854;&#20013;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20943;&#23569;&#22823;&#35268;&#27169;&#27169;&#22411;&#35757;&#32451;&#30340;&#33021;&#28304;&#28040;&#32791;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#12289;&#33410;&#33021;&#30340;&#26694;&#26550;&#8212;&#8212;GreenPLM&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#21452;&#35821;&#35789;&#20856;&#30452;&#25509;&#8220;&#32763;&#35793;&#8221;&#19968;&#20010;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21040;&#21478;&#19968;&#31181;&#35821;&#35328;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#36153;&#29992;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;18&#31181;&#35821;&#35328;&#30340;BERT&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#35813;&#26694;&#26550;&#19982;&#20854;&#20182;&#35757;&#32451;&#25104;&#26412;&#39640;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#30456;&#20284;&#29978;&#33267;&#26356;&#20248;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#26377;&#38480;&#25968;&#25454;&#30340;&#36731;&#37327;&#32423;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#36825;&#20010;&#26694;&#26550;&#22312;&#19971;&#31181;&#34987;&#27979;&#35797;&#30340;&#35821;&#35328;&#20013;&#26377;&#20845;&#31181;&#27604;&#21407;&#26469;&#30340;&#21333;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#25928;&#29575;&#39640;&#36798;200&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained models have revolutionized natural language processing (NLP) research and applications, but high training costs and limited data resources have prevented their benefits from being shared equally amongst speakers of all the world's languages. To address issues of cross-linguistic access to such models and reduce energy consumption for sustainability during large-scale model training, this study proposes an effective and energy-efficient framework called GreenPLM that uses bilingual lexicons to directly "translate" pre-trained language models of one language into another at almost no additional cost. We validate this approach in 18 languages' BERT models and show that this framework is comparable to, if not better than, other heuristics with high training costs. In addition, given lightweight continued pre-training on limited data where available, this framework outperforms the original monolingual language models in six out of seven tested languages with up to 200x les
&lt;/p&gt;</description></item><item><title>RQUGE&#26159;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#20505;&#36873;&#38382;&#39064;&#26159;&#21542;&#21487;&#20197;&#22238;&#31572;&#26469;&#35780;&#20272;&#38382;&#39064;&#29983;&#25104;&#36136;&#37327;, &#27604;&#29616;&#26377;&#25351;&#26631;&#26356;&#21152;&#31283;&#20581;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#25552;&#20379;&#21442;&#32771;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.01482</link><description>&lt;p&gt;
RQUGE&#65306;&#19968;&#31181;&#22522;&#20110;&#22238;&#31572;&#38382;&#39064;&#35780;&#20272;&#38382;&#39064;&#29983;&#25104;&#30340;&#26080;&#21442;&#32771;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question. (arXiv:2211.01482v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01482
&lt;/p&gt;
&lt;p&gt;
RQUGE&#26159;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#20505;&#36873;&#38382;&#39064;&#26159;&#21542;&#21487;&#20197;&#22238;&#31572;&#26469;&#35780;&#20272;&#38382;&#39064;&#29983;&#25104;&#36136;&#37327;, &#27604;&#29616;&#26377;&#25351;&#26631;&#26356;&#21152;&#31283;&#20581;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#25552;&#20379;&#21442;&#32771;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#36136;&#37327;&#30340;&#25351;&#26631;&#65288;&#22914;BLEU&#12289;ROUGE&#12289;BERTScore&#21644;BLEURT&#65289;&#23558;&#21442;&#32771;&#21644;&#39044;&#27979;&#38382;&#39064;&#36827;&#34892;&#27604;&#36739;&#65292;&#24403;&#20505;&#36873;&#38382;&#39064;&#21644;&#21442;&#32771;&#38382;&#39064;&#20043;&#38388;&#23384;&#22312;&#30456;&#24403;&#30340;&#35789;&#27719;&#37325;&#21472;&#25110;&#35821;&#20041;&#30456;&#20284;&#24615;&#26102;&#65292;&#25552;&#20379;&#39640;&#20998;&#12290;&#35813;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#25552;&#20379;&#21442;&#32771;&#38382;&#39064;&#65307;&#20854;&#27425;&#65292;&#23427;&#24809;&#32602;&#37027;&#20123;&#21487;&#33021;&#19982;&#21442;&#32771;&#38382;&#39064;&#27809;&#26377;&#39640;&#35789;&#27719;&#25110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26377;&#25928;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;RQUGE&#65292;&#22522;&#20110;&#32473;&#23450;&#19978;&#19979;&#25991;&#30340;&#20505;&#36873;&#38382;&#39064;&#30340;&#21487;&#22238;&#31572;&#24615;&#12290;&#35813;&#24230;&#37327;&#26631;&#20934;&#30001;&#19968;&#20010;&#38382;&#31572;&#27169;&#22359;&#21644;&#19968;&#20010;&#36328;&#24230;&#35780;&#20998;&#22120;&#27169;&#22359;&#32452;&#25104;&#65292;&#20351;&#29992;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;RQUGE&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#21442;&#32771;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;RQUGE&#26174;&#31034;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing metrics for evaluating the quality of automatically generated questions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and predicted questions, providing a high score when there is a considerable lexical overlap or semantic similarity between the candidate and the reference questions. This approach has two major shortcomings. First, we need expensive human-provided reference questions. Second, it penalises valid questions that may not have high lexical or semantic similarity to the reference questions. In this paper, we propose a new metric, RQUGE, based on the answerability of the candidate question given the context. The metric consists of a question-answering and a span scorer modules, using pre-trained models from existing literature, thus it can be used without any further training. We demonstrate that RQUGE has a higher correlation with human judgment without relying on the reference question. Additionally, RQUGE is shown to be more robust to several ad
&lt;/p&gt;</description></item><item><title>&#38450;&#27490;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#36880;&#23383;&#35760;&#24518;&#26080;&#27861;&#30495;&#27491;&#20445;&#25252;&#38544;&#31169;&#65292;&#26412;&#25991;&#35774;&#35745;&#30340;&#24067;&#38534;&#36807;&#28388;&#22120;&#34429;&#28982;&#38450;&#27490;&#20102;&#25152;&#26377;&#36880;&#23383;&#35760;&#24518;&#65292;&#20294;&#20173;&#28982;&#26080;&#27861;&#38450;&#27490;&#35757;&#32451;&#25968;&#25454;&#27844;&#38706;&#65292;&#23481;&#26131;&#34987;&#21512;&#29702;&#20462;&#25913;&#30340;&#8220;&#26679;&#24335;&#36716;&#25442;&#8221;&#25552;&#31034;&#32469;&#36807;&#12290;</title><link>http://arxiv.org/abs/2210.17546</link><description>&lt;p&gt;
&#38450;&#27490;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#36880;&#23383;&#35760;&#24518;&#20250;&#20135;&#29983;&#34394;&#20551;&#38544;&#31169;&#20445;&#25252;&#24863;
&lt;/p&gt;
&lt;p&gt;
Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy. (arXiv:2210.17546v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17546
&lt;/p&gt;
&lt;p&gt;
&#38450;&#27490;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#36880;&#23383;&#35760;&#24518;&#26080;&#27861;&#30495;&#27491;&#20445;&#25252;&#38544;&#31169;&#65292;&#26412;&#25991;&#35774;&#35745;&#30340;&#24067;&#38534;&#36807;&#28388;&#22120;&#34429;&#28982;&#38450;&#27490;&#20102;&#25152;&#26377;&#36880;&#23383;&#35760;&#24518;&#65292;&#20294;&#20173;&#28982;&#26080;&#27861;&#38450;&#27490;&#35757;&#32451;&#25968;&#25454;&#27844;&#38706;&#65292;&#23481;&#26131;&#34987;&#21512;&#29702;&#20462;&#25913;&#30340;&#8220;&#26679;&#24335;&#36716;&#25442;&#8221;&#25552;&#31034;&#32469;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20013;&#25968;&#25454;&#35760;&#24518;&#30340;&#29616;&#35937;&#65292;&#26412;&#30740;&#31350;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#19982;&#38544;&#31169;&#25110;&#29256;&#26435;&#30456;&#20851;&#30340;&#39118;&#38505;&#65292;&#24182;&#26377;&#21161;&#20110;&#35780;&#20272;&#23545;&#31574;&#12290;&#28982;&#32780;&#36880;&#23383;&#35760;&#24518;&#23450;&#20041;&#36807;&#20110;&#20005;&#26684;&#65292;&#26410;&#33021;&#25429;&#25417;&#26356;&#20026;&#24494;&#22937;&#30340;&#35760;&#24518;&#24418;&#24335;&#12290;&#26412;&#25991;&#22522;&#20110;&#24067;&#38534;&#36807;&#28388;&#22120;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#35813;&#8220;&#23436;&#32654;&#8221;&#36807;&#28388;&#22120;&#24182;&#19981;&#33021;&#38450;&#27490;&#35757;&#32451;&#25968;&#25454;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data, and aids in the evaluation of potential countermeasures. Many prior works -- and some recently deployed defenses -- focus on "verbatim memorization", defined as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization definitions are too restrictive and fail to capture more subtle forms of memorization. Specifically, we design and implement an efficient defense based on Bloom filters that perfectly prevents all verbatim memorization. And yet, we demonstrate that this "perfect" filter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and minimally modified "style-transfer" prompts -- and in some cases even the non-modified original prompts -- to extract memorized information. For example, instructing the model to output ALL-CA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;JECC&#65292;&#22522;&#20110;&#20154;&#31867;&#20114;&#21160;&#23567;&#35828;&#28216;&#25103;&#30340;&#28436;&#31034;&#27493;&#39588;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#30340;&#26159;&#21151;&#33021;&#24615;&#30340;&#24120;&#35782;&#30693;&#35782;&#35268;&#21017;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#27169;&#22411;&#38656;&#35201;&#21033;&#29992;&#36825;&#31181;&#24120;&#35782;&#30693;&#35782;&#26469;&#25512;&#26029;&#34892;&#21160;&#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#36182;&#20110;&#35760;&#24518;&#20107;&#23454;&#12290;</title><link>http://arxiv.org/abs/2210.15456</link><description>&lt;p&gt;
JECC&#65306;&#20174;&#20114;&#21160;&#23567;&#35828;&#20013;&#25512;&#23548;&#20986;&#30340;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
JECC: Commonsense Reasoning Tasks Derived from Interactive Fictions. (arXiv:2210.15456v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;JECC&#65292;&#22522;&#20110;&#20154;&#31867;&#20114;&#21160;&#23567;&#35828;&#28216;&#25103;&#30340;&#28436;&#31034;&#27493;&#39588;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#30340;&#26159;&#21151;&#33021;&#24615;&#30340;&#24120;&#35782;&#30693;&#35782;&#35268;&#21017;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#27169;&#22411;&#38656;&#35201;&#21033;&#29992;&#36825;&#31181;&#24120;&#35782;&#30693;&#35782;&#26469;&#25512;&#26029;&#34892;&#21160;&#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#36182;&#20110;&#35760;&#24518;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#25512;&#29702;&#27169;&#25311;&#20102;&#20154;&#31867;&#23545;&#25105;&#20204;&#29289;&#29702;&#19990;&#30028;&#30340;&#25512;&#26029;&#33021;&#21147;&#65292;&#26159;&#26500;&#24314;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22522;&#30707;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#20154;&#31867;&#20114;&#21160;&#23567;&#35828;&#28216;&#25103;&#30340;&#27493;&#39588;&#28436;&#31034;&#65292;&#22240;&#20026;&#20154;&#31867;&#29609;&#23478;&#23637;&#31034;&#20102;&#20016;&#23500;&#21644;&#22810;&#26679;&#30340;&#24120;&#35782;&#25512;&#29702;&#12290;&#35813;&#26032;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#21508;&#31181;&#25512;&#29702;&#31867;&#22411;&#30340;&#33258;&#28982;&#28151;&#21512;&#65292;&#24182;&#38656;&#35201;&#22810;&#36339;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;IF&#28216;&#25103;&#26500;&#24314;&#36807;&#31243;&#38656;&#35201;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#23569;&#30340;&#20154;&#31867;&#24178;&#39044;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#35780;&#20272;&#21151;&#33021;&#24615;&#30340;&#24120;&#35782;&#30693;&#35782;&#35268;&#21017;&#65292;&#32780;&#19981;&#26159;&#20107;&#23454;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#25105;&#20204;&#30340;&#20219;&#21153;&#19978;&#33719;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#27169;&#22411;&#38656;&#35201;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#31181;&#21151;&#33021;&#24615;&#30693;&#35782;&#26469;&#25512;&#26029;&#34892;&#21160;&#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#20165;&#20381;&#38752;&#35760;&#24518;&#20107;&#23454;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#24341;&#20837;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#20808;&#21069;&#30340;&#26426;&#22120;&#38405;&#35835;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense reasoning simulates the human ability to make presumptions about our physical world, and it is an essential cornerstone in building general AI systems. We propose a new commonsense reasoning dataset based on human's Interactive Fiction (IF) gameplay walkthroughs as human players demonstrate plentiful and diverse commonsense reasoning. The new dataset provides a natural mixture of various reasoning types and requires multi-hop reasoning. Moreover, the IF game-based construction procedure requires much less human interventions than previous ones. Different from existing benchmarks, our dataset focuses on the assessment of functional commonsense knowledge rules rather than factual knowledge. Hence, in order to achieve higher performance on our tasks, models need to effectively utilize such functional knowledge to infer the outcomes of actions, rather than relying solely on memorizing facts. Experiments show that the introduced dataset is challenging to previous machine reading
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#23558;&#28966;&#28857;&#32553;&#23567;&#21040;&#20107;&#20214;&#30340;&#32463;&#21382;&#32773;&#65292;&#23558;&#24773;&#24863;&#65288;&#22914;&#26524;&#26377;&#65289;&#20998;&#37197;&#32473;&#27599;&#20010;&#32463;&#21382;&#32773;&#65292;&#25552;&#20986;&#20102;&#34920;&#31034;&#27599;&#31181;&#24773;&#24863;&#20998;&#31867;&#30340;&#24515;&#29702;&#35780;&#20215;&#21464;&#37327;&#65292;&#20174;&#32780;&#20351;&#24471;&#24773;&#24863;&#20998;&#31867;&#26356;&#20026;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2210.12078</link><description>&lt;p&gt;
&#24773;&#24863;&#21644;&#35780;&#20215;&#30340;&#39044;&#27979;&#19982;&#20010;&#20307;&#32463;&#21382;&#26377;&#20851;
&lt;/p&gt;
&lt;p&gt;
Experiencer-Specific Emotion and Appraisal Prediction. (arXiv:2210.12078v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#23558;&#28966;&#28857;&#32553;&#23567;&#21040;&#20107;&#20214;&#30340;&#32463;&#21382;&#32773;&#65292;&#23558;&#24773;&#24863;&#65288;&#22914;&#26524;&#26377;&#65289;&#20998;&#37197;&#32473;&#27599;&#20010;&#32463;&#21382;&#32773;&#65292;&#25552;&#20986;&#20102;&#34920;&#31034;&#27599;&#31181;&#24773;&#24863;&#20998;&#31867;&#30340;&#24515;&#29702;&#35780;&#20215;&#21464;&#37327;&#65292;&#20174;&#32780;&#20351;&#24471;&#24773;&#24863;&#20998;&#31867;&#26356;&#20026;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24773;&#24863;&#20998;&#31867;&#26159;&#20026;&#25991;&#26412;&#65288;&#22914;&#21477;&#23376;&#25110;&#27573;&#33853;&#65289;&#20998;&#37197;&#24773;&#24863;&#12290;&#23545;&#20110;&#20687;&#8220;&#24403;&#20182;&#21741;&#27875;&#26102;&#65292;&#25105;&#24863;&#21040;&#20869;&#30106;&#8221;&#36825;&#26679;&#30340;&#25991;&#26412;&#65292;&#20851;&#27880;&#21477;&#23376;&#32423;&#21035;&#24573;&#30053;&#20102;&#27599;&#20010;&#21442;&#19982;&#32773;&#30340;&#31435;&#22330;&#65306;&#20316;&#32773;&#65288;&#8220;&#25105;&#8221;&#65289;&#21644;&#21478;&#19968;&#20010;&#23454;&#20307;&#65288;&#8220;&#20182;&#8221;&#65289;&#23454;&#38469;&#19978;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#24773;&#24863;&#29366;&#24577;&#12290;&#19981;&#21516;&#23454;&#20307;&#30340;&#24773;&#24863;&#21482;&#22312;&#24773;&#24863;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#31561;&#20219;&#21153;&#20013;&#37096;&#20998;&#32771;&#34385;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#23558;&#28966;&#28857;&#32553;&#23567;&#21040;&#20107;&#20214;&#30340;&#32463;&#21382;&#32773;&#65292;&#23558;&#24773;&#24863;&#65288;&#22914;&#26524;&#26377;&#65289;&#20998;&#37197;&#32473;&#27599;&#20010;&#32463;&#21382;&#32773;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#27599;&#31181;&#24773;&#24863;&#20998;&#31867;&#21644;&#35780;&#20215;&#21464;&#37327;&#37117;&#34920;&#31034;&#20986;&#26469;&#65292;&#20316;&#20026;&#35299;&#37322;&#20154;&#20204;&#20026;&#20160;&#20040;&#20250;&#20135;&#29983;&#29305;&#23450;&#24773;&#24863;&#30340;&#24515;&#29702;&#36884;&#24452;&#12290;&#22312;&#20107;&#20214;&#25551;&#36848;&#35821;&#26009;&#24211;&#19978;&#65292;&#25105;&#20204;&#23545;&#32463;&#21382;&#32773;&#24863;&#30693;&#30340;&#24773;&#24863;&#21644;&#35780;&#20215;&#27169;&#22411;&#20248;&#20110;&#32463;&#21382;&#32773;&#26080;&#20851;&#30340;&#22522;&#32447;&#65292;&#34920;&#26126;&#24573;&#30053;&#20107;&#20214;&#21442;&#19982;&#32773;&#26159;&#22312;&#35821;&#35328;&#20013;&#20256;&#36798;&#24773;&#24863;&#30340;&#19968;&#31181;&#36807;&#24230;&#31616;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion classification in NLP assigns emotions to texts, such as sentences or paragraphs. With texts like "I felt guilty when he cried", focusing on the sentence level disregards the standpoint of each participant in the situation: the writer ("I") and the other entity ("he") could in fact have different affective states. The emotions of different entities have been considered only partially in emotion semantic role labeling, a task that relates semantic roles to emotion cue words. Proposing a related task, we narrow the focus on the experiencers of events, and assign an emotion (if any holds) to each of them. To this end, we represent each emotion both categorically and with appraisal variables, as a psychological access to explaining why a person develops a particular emotion. On an event description corpus, our experiencer-aware models of emotions and appraisals outperform the experiencer-agnostic baselines, showing that disregarding event participants is an oversimplification for t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23454;&#20307;&#19982;&#25991;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65288;EnTDA&#65289;&#65292;&#21487;&#20026;&#21508;&#31181;&#25153;&#24179;&#12289;&#23884;&#22871;&#21644;&#19981;&#36830;&#32493; NER &#20219;&#21153;&#29983;&#25104;&#35821;&#20041;&#36830;&#36143;&#21644;&#20445;&#30041;&#23454;&#20307;&#30340;&#25991;&#26412;&#65292;&#24182;&#24341;&#20837;&#22810;&#26679;&#24615;&#26463;&#25628;&#32034;&#20197;&#22686;&#21152;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.10343</link><description>&lt;p&gt;
&#22522;&#20110;&#23454;&#20307;&#19982;&#25991;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#23545;&#21508;&#31181;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Entity-to-Text based Data Augmentation for various Named Entity Recognition Tasks. (arXiv:2210.10343v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23454;&#20307;&#19982;&#25991;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65288;EnTDA&#65289;&#65292;&#21487;&#20026;&#21508;&#31181;&#25153;&#24179;&#12289;&#23884;&#22871;&#21644;&#19981;&#36830;&#32493; NER &#20219;&#21153;&#29983;&#25104;&#35821;&#20041;&#36830;&#36143;&#21644;&#20445;&#30041;&#23454;&#20307;&#30340;&#25991;&#26412;&#65292;&#24182;&#24341;&#20837;&#22810;&#26679;&#24615;&#26463;&#25628;&#32034;&#20197;&#22686;&#21152;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#34987;&#29992;&#20110;&#32531;&#35299;&#21508;&#31181; NER&#65288;&#25153;&#24179;&#12289;&#23884;&#22871;&#21644;&#19981;&#36830;&#32493;&#30340; NER&#65289;&#20219;&#21153;&#20013;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22686;&#24378;&#25216;&#26415;&#35201;&#20040;&#20462;&#25913;&#21407;&#22987;&#25991;&#26412;&#20013;&#30340;&#21333;&#35789;&#20174;&#32780;&#30772;&#22351;&#25991;&#26412;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#65292;&#35201;&#20040;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#32780;&#24573;&#30053;&#25658;&#24102;&#21407;&#22987;&#25991;&#26412;&#20013;&#30340;&#23454;&#20307;&#65292;&#36825;&#38459;&#30861;&#20102;&#22686;&#24191;&#25216;&#26415;&#22312;&#23884;&#22871;&#21644;&#19981;&#36830;&#32493;&#30340; NER &#20219;&#21153;&#19978;&#30340;&#20351;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23454;&#20307;&#19982;&#25991;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65288;EnTDA&#65289;&#65292;&#23545;&#21407;&#22987;&#25991;&#26412;&#20013;&#30340;&#23454;&#20307;&#21015;&#34920;&#36827;&#34892;&#28155;&#21152;&#12289;&#21024;&#38500;&#12289;&#26367;&#25442;&#25110;&#20132;&#25442;&#65292;&#37319;&#29992;&#36825;&#20123;&#22686;&#24191;&#21518;&#30340;&#23454;&#20307;&#21015;&#34920;&#20026;&#21508;&#31181; NER &#20219;&#21153;&#29983;&#25104;&#35821;&#20041;&#36830;&#36143;&#21644;&#20445;&#30041;&#23454;&#20307;&#30340;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#26679;&#24615;&#26463;&#25628;&#32034;&#65292;&#20197;&#22686;&#21152;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#22810;&#26679;&#24615;&#12290;&#22312;&#19977;&#20010;&#20219;&#21153;&#65288;&#25153;&#24179;&#12289;&#23884;&#22871;&#21644;&#19981;&#36830;&#32493; NER &#20219;&#21153;&#65289;&#21644;&#20004;&#20010;&#35774;&#32622;&#65288;&#23436;&#25972;&#25968;&#25454;&#20197;&#21450;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#65289;&#30340;&#21313;&#19977;&#20010; NER &#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation techniques have been used to alleviate the problem of scarce labeled data in various NER tasks (flat, nested, and discontinuous NER tasks). Existing augmentation techniques either manipulate the words in the original text that break the semantic coherence of the text, or exploit generative models that ignore preserving entities in the original text, which impedes the use of augmentation techniques on nested and discontinuous NER tasks. In this work, we propose a novel Entity-to-Text based data augmentation technique named EnTDA to add, delete, replace or swap entities in the entity list of the original texts, and adopt these augmented entity lists to generate semantically coherent and entity preserving texts for various NER tasks. Furthermore, we introduce a diversity beam search to increase the diversity during the text generation process. Experiments on thirteen NER datasets across three tasks (flat, nested, and discontinuous NER tasks) and two settings (full data a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SummaFusion&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#24635;&#32467;&#20505;&#36873;&#39033;&#26469;&#20135;&#29983;&#19968;&#20010;&#26032;&#30340;&#25277;&#35937;&#24635;&#32467;&#65292;&#20197;&#25913;&#21892;&#31532;&#19968;&#38454;&#27573;&#20505;&#36873;&#39033;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#12290;</title><link>http://arxiv.org/abs/2210.08779</link><description>&lt;p&gt;
&#21521;&#24635;&#32467;&#20505;&#36873;&#39033;&#34701;&#21512;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Summary Candidates Fusion. (arXiv:2210.08779v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SummaFusion&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#24635;&#32467;&#20505;&#36873;&#39033;&#26469;&#20135;&#29983;&#19968;&#20010;&#26032;&#30340;&#25277;&#35937;&#24635;&#32467;&#65292;&#20197;&#25913;&#21892;&#31532;&#19968;&#38454;&#27573;&#20505;&#36873;&#39033;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#29992;&#20110;&#25277;&#35937;&#24635;&#32467;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#21487;&#20197;&#22312;&#20855;&#26377;&#36275;&#22815;&#20154;&#20026;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#23578;&#26410;&#36798;&#21040;&#20854;&#20840;&#37096;&#28508;&#21147;&#65292;&#26368;&#20339;&#26463;&#25628;&#32034;&#36755;&#20986;&#19982;&#23436;&#32654;&#32467;&#26524;&#20043;&#38388;&#23384;&#22312;&#30528;&#24456;&#22823;&#24046;&#36317;&#12290;&#26368;&#36817;&#20986;&#29616;&#20102;&#37325;&#26032;&#25490;&#21517;&#26041;&#27861;&#65292;&#23398;&#20064;&#36873;&#25321;&#26356;&#22909;&#30340;&#25688;&#35201;&#20505;&#36873;&#39033;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21463;&#31532;&#19968;&#38454;&#27573;&#20505;&#36873;&#39033;&#25429;&#33719;&#30340;&#25688;&#35201;&#36136;&#37327;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32469;&#36807;&#36825;&#31181;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#33539;&#24335;&#65292;&#31216;&#20026;&#8220;SummaFusion&#8221;&#65292;&#23427;&#22312;&#31532;&#20108;&#38454;&#27573;&#30340;&#25277;&#35937;&#24635;&#32467;&#20013;&#34701;&#21512;&#20102;&#22810;&#20010;&#24635;&#32467;&#20505;&#36873;&#39033;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#26032;&#30340;&#25277;&#35937;&#24635;&#32467;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#25552;&#39640;&#20102;&#34701;&#21512;&#24635;&#32467;&#30340;ROUGE&#20998;&#25968;&#21644;&#36136;&#37327;&#29305;&#24615;&#12290;&#24403;&#38656;&#35201;&#34701;&#21512;&#30340;&#20505;&#36873;&#39033;&#36739;&#24046;&#26102;&#65292;&#29305;&#21035;&#26159;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#27700;&#24179;&#12290;&#25105;&#20204;&#23558;&#22312;&#21457;&#34920;&#21518;&#20844;&#24320;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-sequence deep neural models fine-tuned for abstractive summarization can achieve great performance on datasets with enough human annotations. Yet, it has been shown that they have not reached their full potential, with a wide gap between the top beam search output and the oracle beam. Recently, re-ranking methods have been proposed, to learn to select a better summary candidate. However, such methods are limited by the summary quality aspects captured by the first-stage candidates. To bypass this limitation, we propose a new paradigm in second-stage abstractive summarization called SummaFusion that fuses several summary candidates to produce a novel abstractive second-stage summary. Our method works well on several summarization datasets, improving both the ROUGE scores and qualitative properties of fused summaries. It is especially good when the candidates to fuse are worse, such as in the few-shot setup where we set a new state-of-the-art. We will make our code and checkp
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HighGEN&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#30701;&#35821;&#23884;&#20837;&#25628;&#32034;&#26041;&#27861;&#29983;&#25104;&#23454;&#20307;&#20016;&#23500;&#30340;&#20266;&#23383;&#20856;&#65292;&#22312;&#20351;&#29992;&#23884;&#20837;&#36317;&#31163;&#39564;&#35777;&#36807;&#31243;&#20943;&#23569;&#35823;&#25253;&#30340;&#22522;&#30784;&#19978;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;NER&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2210.07586</link><description>&lt;p&gt;
&#20351;&#29992;&#30701;&#35821;&#34920;&#31034;&#26597;&#35810;&#33258;&#21160;&#29983;&#25104;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations. (arXiv:2210.07586v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HighGEN&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#30701;&#35821;&#23884;&#20837;&#25628;&#32034;&#26041;&#27861;&#29983;&#25104;&#23454;&#20307;&#20016;&#23500;&#30340;&#20266;&#23383;&#20856;&#65292;&#22312;&#20351;&#29992;&#23884;&#20837;&#36317;&#31163;&#39564;&#35777;&#36807;&#31243;&#20943;&#23569;&#35823;&#25253;&#30340;&#22522;&#30784;&#19978;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;NER&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24369;&#30417;&#30563;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#20381;&#36182;&#20110;&#30001;&#19987;&#23478;&#25552;&#20379;&#30340;&#39046;&#22495;&#29305;&#23450;&#35789;&#20856;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#27809;&#26377;&#23383;&#20856;&#30340;&#39046;&#22495;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#21487;&#34892;&#12290;&#22312;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#30701;&#35821;&#26816;&#32034;&#27169;&#22411;&#33258;&#21160;&#20174;&#32500;&#22522;&#30334;&#31185;&#20013;&#25552;&#21462;&#23454;&#20307;&#26500;&#24314;&#20102;&#20266;&#23383;&#20856;&#65292;&#20294;&#36825;&#20123;&#23383;&#20856;&#30340;&#35206;&#30422;&#38754;&#24448;&#24448;&#26377;&#38480;&#65292;&#22240;&#20026;&#26816;&#32034;&#22120;&#24456;&#21487;&#33021;&#20250;&#26816;&#32034;&#21040;&#27969;&#34892;&#30340;&#23454;&#20307;&#32780;&#19981;&#26159;&#32597;&#35265;&#30340;&#23454;&#20307;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;HighGEN&#65292;&#23427;&#20351;&#29992;&#20855;&#26377;&#39640;&#35206;&#30422;&#29575;&#30340;&#20266;&#23383;&#20856;&#29983;&#25104;NER&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#26041;&#27861;&#8212;&#8212;&#30701;&#35821;&#23884;&#20837;&#25628;&#32034;&#26469;&#21019;&#24314;&#23500;&#23454;&#20307;&#23383;&#20856;&#65292;&#35813;&#26041;&#27861;&#40723;&#21169;&#26816;&#32034;&#22120;&#22312;&#19968;&#20010;&#23494;&#38598;&#30340;&#21508;&#31181;&#23454;&#20307;&#30340;&#31354;&#38388;&#20013;&#25628;&#32034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#23454;&#20307;&#25552;&#21450;&#21644;&#23454;&#20307;&#31867;&#22411;&#20043;&#38388;&#23884;&#20837;&#36317;&#31163;&#30340;&#26032;&#30340;&#39564;&#35777;&#36807;&#31243;&#65292;&#20197;&#20943;&#23569;&#39640;&#35206;&#30422;&#29575;&#20266;&#26631;&#31614;&#20013;&#30340;&#35823;&#25253;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most weakly supervised named entity recognition (NER) models rely on domain-specific dictionaries provided by experts. This approach is infeasible in many domains where dictionaries do not exist. While a phrase retrieval model was used to construct pseudo-dictionaries with entities retrieved from Wikipedia automatically in a recent study, these dictionaries often have limited coverage because the retriever is likely to retrieve popular entities rather than rare ones. In this study, we present a novel framework, HighGEN, that generates NER datasets with high-coverage pseudo-dictionaries. Specifically, we create entity-rich dictionaries with a novel search method, called phrase embedding search, which encourages the retriever to search a space densely populated with various entities. In addition, we use a new verification process based on the embedding distance between candidate entity mentions and entity types to reduce the false-positive noise in weak labels generated by high-coverage 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22522;&#20110;&#25945;&#24072;-&#23398;&#29983;&#30693;&#35782;&#33976;&#39311;&#30340;&#22810;&#35821;&#35328;&#35757;&#32451;&#25216;&#26415;&#65292;&#21033;&#29992;&#36866;&#29992;&#20110;&#27599;&#31181;&#35821;&#35328;&#20248;&#21270;&#30340;&#19987;&#19994;&#35821;&#35328;&#25945;&#24072;&#27169;&#22411;&#21644;&#24179;&#34913;&#25968;&#25454;&#65292;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#34920;&#29616;&#26041;&#38754;&#32988;&#36807;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#22312;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20013;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2210.07135</link><description>&lt;p&gt;
&#20320;&#21487;&#20197;&#25317;&#26377;&#20320;&#30340;&#25968;&#25454;&#24182;&#19988;&#24179;&#34913;&#20351;&#29992;&#65306;&#36208;&#21521;&#24179;&#34913;&#39640;&#25928;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
You Can Have Your Data and Balance It Too: Towards Balanced and Efficient Multilingual Models. (arXiv:2210.07135v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07135
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#25945;&#24072;-&#23398;&#29983;&#30693;&#35782;&#33976;&#39311;&#30340;&#22810;&#35821;&#35328;&#35757;&#32451;&#25216;&#26415;&#65292;&#21033;&#29992;&#36866;&#29992;&#20110;&#27599;&#31181;&#35821;&#35328;&#20248;&#21270;&#30340;&#19987;&#19994;&#35821;&#35328;&#25945;&#24072;&#27169;&#22411;&#21644;&#24179;&#34913;&#25968;&#25454;&#65292;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#34920;&#29616;&#26041;&#38754;&#32988;&#36807;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#22312;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20013;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#27169;&#22411;&#34987;&#24191;&#27867;&#29992;&#20110;&#36328;&#35821;&#35328;&#30340;&#20302;&#36164;&#28304;&#36716;&#31227;&#65292;&#20294;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#19981;&#20805;&#20998;&#30340;&#38382;&#39064;&#65292;&#36825;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25945;&#24072;-&#23398;&#29983;&#30693;&#35782;&#33976;&#39311;&#30340;&#22810;&#35821;&#35328;&#35757;&#32451;&#25216;&#26415;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21033;&#29992;&#36866;&#29992;&#20110;&#27599;&#31181;&#35821;&#35328;&#20248;&#21270;&#30340;&#19987;&#19994;&#35821;&#35328;&#25945;&#24072;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25945;&#24072;&#21644;&#24179;&#34913;&#65288;&#23376;&#37319;&#26679;&#65289;&#25968;&#25454;&#26469;&#33976;&#39311;&#25945;&#24072;&#30340;&#30693;&#35782;&#21040;&#21333;&#19968;&#30340;&#22810;&#35821;&#35328;&#23398;&#29983;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#20248;&#20110;&#26631;&#20934;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#22312;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#24615;&#33021;&#12290;&#22914;&#26524;&#24191;&#27867;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20013;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual models have been widely used for cross-lingual transfer to low-resource languages. However, the performance on these languages is hindered by their underrepresentation in the pretraining data. To alleviate this problem, we propose a novel multilingual training technique based on teacher-student knowledge distillation. In this setting, we utilize monolingual teacher models optimized for their language. We use those teachers along with balanced (sub-sampled) data to distill the teachers' knowledge into a single multilingual student. Our method outperforms standard training methods in low-resource languages and retrains performance on high-resource languages while using the same amount of data. If applied widely, our approach can increase the representation of low-resource languages in NLP systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#24179;&#34892;&#21644;&#38750;&#24179;&#34892;&#35821;&#26009;&#24211;&#26469;&#25552;&#39640;&#22810;&#35821;&#31181;&#20449;&#24687;&#26816;&#32034;&#30340;&#25928;&#26524;&#65292;&#20165;&#20351;&#29992;&#33521;&#35821;IR&#35757;&#32451;&#25968;&#25454;&#21644;&#19968;&#20123;&#24179;&#34892;&#35821;&#26009;&#24211;&#21363;&#21487;&#22312;&#38750;&#33521;&#35821;&#25968;&#25454;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#26816;&#32034;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.06633</link><description>&lt;p&gt;
&#26080;&#20851;&#35821;&#35328;&#30340;&#22810;&#35821;&#31181;&#20449;&#24687;&#26816;&#32034;&#19982;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Language Agnostic Multilingual Information Retrieval with Contrastive Learning. (arXiv:2210.06633v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06633
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#24179;&#34892;&#21644;&#38750;&#24179;&#34892;&#35821;&#26009;&#24211;&#26469;&#25552;&#39640;&#22810;&#35821;&#31181;&#20449;&#24687;&#26816;&#32034;&#30340;&#25928;&#26524;&#65292;&#20165;&#20351;&#29992;&#33521;&#35821;IR&#35757;&#32451;&#25968;&#25454;&#21644;&#19968;&#20123;&#24179;&#34892;&#35821;&#26009;&#24211;&#21363;&#21487;&#22312;&#38750;&#33521;&#35821;&#25968;&#25454;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#26816;&#32034;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#31181;&#20449;&#24687;&#26816;&#32034;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#22312;&#35768;&#22810;&#35821;&#35328;&#20013;&#33719;&#21462;&#32463;&#36807;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#25104;&#26412;&#24456;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#21482;&#26377;&#33521;&#35821;IR&#35757;&#32451;&#25968;&#25454;&#21644;&#33521;&#35821;&#19982;&#20854;&#20182;&#35821;&#35328;&#20043;&#38388;&#30340;&#19968;&#20123;&#24179;&#34892;&#35821;&#26009;&#24211;&#21487;&#29992;&#26102;&#35757;&#32451;&#22810;&#35821;&#31181;IR&#31995;&#32479;&#12290;&#25105;&#20204;&#21033;&#29992;&#24179;&#34892;&#21644;&#38750;&#24179;&#34892;&#35821;&#26009;&#24211;&#26469;&#25552;&#39640;&#39044;&#35757;&#32451;&#22810;&#35821;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#33021;&#21147;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35821;&#20041;&#23545;&#27604;&#25439;&#22833;&#65292;&#20197;&#23545;&#40784;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#30340;&#24179;&#34892;&#21477;&#23376;&#30340;&#34920;&#31034;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#23545;&#27604;&#25439;&#22833;&#65292;&#21033;&#29992;&#24179;&#34892;&#21477;&#23376;&#23545;&#20174;&#38750;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#30340;&#21477;&#23376;&#34920;&#31034;&#20013;&#21024;&#38500;&#35821;&#35328;&#29305;&#23450;&#20449;&#24687;&#12290;&#22312;&#20351;&#29992;&#36825;&#20123;&#25439;&#22833;&#23545;&#33521;&#35821;IR&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#24182;&#22312;&#38750;&#33521;&#35821;&#25968;&#25454;&#19978;&#36827;&#34892;&#38646;-shot&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#25913;&#36827;&#65292;&#21516;&#26102;&#38656;&#35201;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual information retrieval (IR) is challenging since annotated training data is costly to obtain in many languages. We present an effective method to train multilingual IR systems when only English IR training data and some parallel corpora between English and other languages are available. We leverage parallel and non-parallel corpora to improve the pretrained multilingual language models' cross-lingual transfer ability. We design a semantic contrastive loss to align representations of parallel sentences that share the same semantics in different languages, and a new language contrastive loss to leverage parallel sentence pairs to remove language-specific information in sentence representations from non-parallel corpora. When trained on English IR data with these losses and evaluated zero-shot on non-English data, our model demonstrates significant improvement to prior work on retrieval performance, while it requires much less computational effort. We also demonstrate the valu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#20307;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#20855;&#20307;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#30340;&#20855;&#20307;&#24615;&#21487;&#20197;&#24471;&#21040;&#25913;&#21892;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2210.05159</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20855;&#20307;&#21270;&#21527;&#65311;&#22914;&#20309;&#23454;&#29616;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Language Models Be Specific? How?. (arXiv:2210.05159v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#20307;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#20855;&#20307;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#30340;&#20855;&#20307;&#24615;&#21487;&#20197;&#24471;&#21040;&#25913;&#21892;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#20182;&#26159;&#19968;&#20010;&#20154;&#8221;&#12289;&#8220;&#24052;&#40654;&#20301;&#20110;&#22320;&#29699;&#19978;&#8221;&#12290;&#36825;&#20123;&#35821;&#21477;&#37117;&#26159;&#27491;&#30830;&#30340;&#65292;&#20294;&#27809;&#26377;&#20855;&#20307;&#24615;&#8212;&#8212;&#22240;&#20026;&#32570;&#20047;&#26126;&#30830;&#30340;&#20869;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#20855;&#20307;&#24615;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#24314;&#31435;&#20855;&#20307;&#24615;&#27979;&#35797;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#24102;&#26377;&#25552;&#31034;&#30340;&#25513;&#30721;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#26469;&#23454;&#29616;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#8220;&#22810;&#20262;&#22810;&#20301;&#20110;[MASK]&#20013;&#8221;&#65292;&#25105;&#20204;&#24819;&#27979;&#35797;PLMs&#26159;&#21542;&#33021;&#26356;&#22909;&#22320;&#22635;&#20889;&#26356;&#20855;&#20307;&#30340;&#31572;&#26696;&#65292;&#20363;&#22914;&#23433;&#22823;&#30053;&#30465;&#32780;&#19981;&#26159;&#21152;&#25343;&#22823;&#12290;&#20174;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;PLMs&#21482;&#23545;&#26356;&#20855;&#20307;&#30340;&#31572;&#26696;&#30053;&#24494;&#26356;&#26377;&#20559;&#22909;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#24433;&#21709;&#20855;&#20307;&#24615;&#30340;&#28508;&#22312;&#22240;&#32032;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#20197;&#25913;&#21892;&#20855;&#20307;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#27169;&#22411;&#30340;&#20855;&#20307;&#24615;&#21487;&#20197;&#24471;&#21040;&#25913;&#21892;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#24102;&#26469;&#35821;&#35328;&#27169;&#22411;&#20855;&#20307;&#24615;&#30340;&#35748;&#35782;&#65292;&#24182;&#40723;&#21169;&#30456;&#20851;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
"He is a person", "Paris is located on the earth". Both statements are correct but meaningless - due to lack of specificity. In this paper, we propose to measure how specific the language of pre-trained language models (PLMs) is. To achieve this, we introduce a novel approach to build a benchmark for specificity testing by forming masked token prediction tasks with prompts. For instance, given "Toronto is located in [MASK].", we want to test whether a more specific answer will be better filled in by PLMs, e.g., Ontario instead of Canada. From our evaluations, we show that existing PLMs have only a slight preference for more specific answers. We identify underlying factors affecting the specificity and design two prompt-based methods to improve the specificity. Results show that the specificity of the models can be improved by the proposed methods without additional training. We hope this work can bring to awareness the notion of specificity of language models and encourage the research
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REV&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#20013;&#26032;&#39062;&#12289;&#19982;&#26631;&#31614;&#30456;&#20851;&#30340;&#20449;&#24687;&#30340;&#25968;&#37327;&#65292;&#36890;&#36807;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#36827;&#34892;&#30740;&#31350;&#12290;&#23454;&#39564;&#35777;&#26126;REV&#22312;&#35780;&#20272;&#35299;&#37322;-&#26631;&#31614;&#23545;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#30452;&#35273;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2210.04982</link><description>&lt;p&gt;
&#29992;&#20449;&#24687;&#35770;&#35780;&#20272;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
REV: Information-Theoretic Evaluation of Free-Text Rationales. (arXiv:2210.04982v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REV&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#20013;&#26032;&#39062;&#12289;&#19982;&#26631;&#31614;&#30456;&#20851;&#30340;&#20449;&#24687;&#30340;&#25968;&#37327;&#65292;&#36890;&#36807;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#36827;&#34892;&#30740;&#31350;&#12290;&#23454;&#39564;&#35777;&#26126;REV&#22312;&#35780;&#20272;&#35299;&#37322;-&#26631;&#31614;&#23545;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#30452;&#35273;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#26159;&#36808;&#21521;&#21487;&#35299;&#37322; NLP &#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#27493;&#39588;&#65292;&#28982;&#32780;&#35780;&#20272;&#36825;&#26679;&#30340;&#35299;&#37322;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#24230;&#37327;&#20027;&#35201;&#38598;&#20013;&#22312;&#27979;&#37327;&#35299;&#37322;&#21644;&#32473;&#23450;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#19978;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#29702;&#24819;&#30340;&#24230;&#37327;&#24212;&#35813;&#38598;&#20013;&#20110;&#35299;&#37322;&#20013;&#25552;&#20379;&#30340;&#26032;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#22312;&#36755;&#20837;&#25110;&#26631;&#31614;&#20013;&#37117;&#27809;&#26377;&#25552;&#20379;&#12290;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#20351;&#29992;&#26465;&#20214;V-&#20449;&#24687;&#65288;Hewitt et al&#12290;&#65292;2021&#65289;&#30740;&#31350;&#20102;&#36825;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;REV&#65288;&#21033;&#29992;&#26465;&#20214;V-&#20449;&#24687;&#35780;&#20272;&#35299;&#37322;&#65289;&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#37327;&#21270;&#29702;&#24615;&#20013;&#38500;&#20102;&#36755;&#20837;&#25110;&#26631;&#31614;&#20013;&#24050;&#26377;&#20449;&#24687;&#20043;&#22806;&#30340;&#26032;&#26631;&#31614;&#30456;&#20851;&#20449;&#24687;&#30340;&#25968;&#37327;&#12290;&#22312;&#28041;&#21450;&#25512;&#29702;&#20219;&#21153;&#30340;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;REV&#22312;&#35780;&#20272;&#35299;&#37322;-&#26631;&#31614;&#23545;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#19982;&#29616;&#26377;&#30340;&#24230;&#37327;&#30456;&#27604;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;REV&#19982;&#20154;&#31867;&#30452;&#35273;&#19968;&#33268;&#65292;&#32780;&#19968;&#20123;&#29616;&#26377;&#30340;&#24230;&#37327;&#21017;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating free-text rationales is a promising step towards explainable NLP, yet evaluating such rationales remains a challenge. Existing metrics have mostly focused on measuring the association between the rationale and a given label. We argue that an ideal metric should focus on the new information uniquely provided in the rationale that is otherwise not provided in the input or the label. We investigate this research problem from an information-theoretic perspective using conditional V-information (Hewitt et al., 2021). More concretely, we propose a metric called REV (Rationale Evaluation with conditional V-information), to quantify the amount of new, label-relevant information in a rationale beyond the information already available in the input or the label. Experiments across four benchmarks with reasoning tasks, including chain-of-thought, demonstrate the effectiveness of REV in evaluating rationale-label pairs, compared to existing metrics. We further demonstrate REV is consiste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#30340;&#22522;&#20110;&#36328;&#24230;&#30340;&#23884;&#22871;&#24335;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(n^2)&#65292;&#20854;&#22312;&#33521;&#35821;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19977;&#20010;&#26631;&#20934;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2210.04738</link><description>&lt;p&gt;
&#19968;&#31181;O(n^2)&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#29992;&#20110;&#22522;&#20110;&#36328;&#24230;&#30340;&#23884;&#22871;&#24335;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
A dynamic programming algorithm for span-based nested named-entity recognition in O(n^2). (arXiv:2210.04738v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#30340;&#22522;&#20110;&#36328;&#24230;&#30340;&#23884;&#22871;&#24335;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(n^2)&#65292;&#20854;&#22312;&#33521;&#35821;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19977;&#20010;&#26631;&#20934;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36328;&#24230;&#30340;&#23884;&#22871;&#24335;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#20351;&#29992;&#20462;&#25913;&#29256;CYK&#31639;&#27861;&#30340;&#31435;&#26041;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#25628;&#32034;&#31354;&#38388;&#19978;&#22686;&#21152;&#19968;&#31181;&#34917;&#20805;&#30340;&#32467;&#26500;&#32422;&#26463;&#65292;&#23637;&#31034;&#20102;&#23884;&#22871;&#24335;NER&#20855;&#26377;&#20108;&#27425;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#19982;&#38750;&#23884;&#22871;&#24335;&#24773;&#20917;&#30340;&#28176;&#36817;&#22797;&#26434;&#24230;&#30456;&#21516;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#35206;&#30422;&#20102;&#33521;&#35821;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19977;&#20010;&#26631;&#20934;&#65292;&#19988;&#23454;&#39564;&#32467;&#26524;&#20063;&#21487;&#20197;&#19982;&#20043;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Span-based nested named-entity recognition (NER) has a cubic-time complexity using a variant of the CYK algorithm. We show that by adding a supplementary structural constraint on the search space, nested NER has a quadratic-time complexity, that is the same asymptotic complexity than the non-nested case. The proposed algorithm covers a large part of three standard English benchmarks and delivers comparable experimental results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19979;&#28216;&#25968;&#25454;&#38598;&#36827;&#34892;&#33258;&#25105;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#19982;&#20351;&#29992;&#22823;&#22411;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#26631;&#20934;&#26041;&#27861;&#30456;&#23218;&#32654;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#26356;&#21152;&#20248;&#31168;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#33258;&#25105;&#39044;&#35757;&#32451;&#27169;&#22411;&#36824;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.14389</link><description>&lt;p&gt;
&#19979;&#28216;&#25968;&#25454;&#38598;&#24847;&#22806;&#22320;&#25104;&#20026;&#33391;&#22909;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Downstream Datasets Make Surprisingly Good Pretraining Corpora. (arXiv:2209.14389v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19979;&#28216;&#25968;&#25454;&#38598;&#36827;&#34892;&#33258;&#25105;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#19982;&#20351;&#29992;&#22823;&#22411;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#26631;&#20934;&#26041;&#27861;&#30456;&#23218;&#32654;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#26356;&#21152;&#20248;&#31168;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#33258;&#25105;&#39044;&#35757;&#32451;&#27169;&#22411;&#36824;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20027;&#35201;&#30340;&#20570;&#27861;&#26159;&#20351;&#29992;&#26356;&#23567;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65288;&#20363;&#22914;BERT&#65289;&#36827;&#34892;&#24494;&#35843;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#25910;&#30410;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#65292;&#32780;&#19981;&#26159;&#39044;&#35757;&#32451;&#30446;&#26631;&#26412;&#36523;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#33258;&#25105;&#39044;&#35757;&#32451;&#65288;self-pretraining&#65289;&#30340;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#20854;&#20013;&#30456;&#21516;&#30340;&#65288;&#19979;&#28216;&#65289;&#35757;&#32451;&#25968;&#25454;&#29992;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#22312;&#38024;&#23545;ELECTRA&#21644;RoBERTa&#27169;&#22411;&#20197;&#21450;10&#20010;&#19981;&#21516;&#30340;&#19979;&#28216;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#33258;&#25105;&#39044;&#35757;&#32451;&#19982;&#20351;&#29992;BookWiki&#35821;&#26009;&#24211;&#36827;&#34892;&#26631;&#20934;&#39044;&#35757;&#32451;&#30456;&#23218;&#32654;&#65288;&#23613;&#31649;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#20165;&#20026;&#21518;&#32773;&#30340;$10$&#20493;&#21040;$500$&#20493;&#19981;&#31561;&#65289;&#65292;&#24182;&#19988;&#22312;$7$&#20010;&#21644;$5$&#20010;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#20248;&#20110;&#21518;&#32773;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#21253;&#25324;GLUE&#22522;&#20934;&#27979;&#35797;&#12290;&#38500;&#20102;&#20998;&#31867;&#20219;&#21153;&#65292;&#33258;&#25105;&#39044;&#35757;&#32451;&#27169;&#22411;&#36824;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#21644;&#25277;&#21462;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
For most natural language processing tasks, the dominant practice is to finetune large pretrained transformer models (e.g., BERT) using smaller downstream datasets. Despite the success of this approach, it remains unclear to what extent these gains are attributable to the massive background corpora employed for pretraining versus to the pretraining objectives themselves. This paper introduces a large-scale study of self-pretraining, where the same (downstream) training data is used for both pretraining and finetuning. In experiments addressing both ELECTRA and RoBERTa models and 10 distinct downstream classification datasets, we observe that self-pretraining rivals standard pretraining on the BookWiki corpus (despite using around $10\times$--$500\times$ less data), outperforming the latter on $7$ and $5$ datasets, respectively. Surprisingly, these task-specific pretrained models often perform well on other tasks, including the GLUE benchmark. Besides classification tasks, self-pretrain
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#25991;&#26723;&#32423;&#21035;&#19978;&#25429;&#25417;&#20107;&#20214;&#35770;&#20803;&#30340;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#26032;&#26041;&#27861;FewDocAE&#65292;&#24182;&#19988;&#36890;&#36807;N-Way-D-Doc&#25277;&#26679;&#26041;&#27861;&#37325;&#26500;&#20102;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#23558;&#24403;&#21069;&#25991;&#26723;&#32423;&#31070;&#32463;&#27169;&#22411;&#35843;&#25972;&#21040;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#20026;&#36328;&#39046;&#22495;&#21644;&#39046;&#22495;&#20869;&#25552;&#20379;&#22522;&#32447;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.02203</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#20803;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Document-Level Event Argument Extraction. (arXiv:2209.02203v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#25991;&#26723;&#32423;&#21035;&#19978;&#25429;&#25417;&#20107;&#20214;&#35770;&#20803;&#30340;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#26032;&#26041;&#27861;FewDocAE&#65292;&#24182;&#19988;&#36890;&#36807;N-Way-D-Doc&#25277;&#26679;&#26041;&#27861;&#37325;&#26500;&#20102;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#23558;&#24403;&#21069;&#25991;&#26723;&#32423;&#31070;&#32463;&#27169;&#22411;&#35843;&#25972;&#21040;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#20026;&#36328;&#39046;&#22495;&#21644;&#39046;&#22495;&#20869;&#25552;&#20379;&#22522;&#32447;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#35770;&#20803;&#25552;&#21462;(EAE)&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#20294;&#22312;&#25991;&#26723;&#32423;&#21035;&#19978;&#21364;&#24456;&#23569;&#12290;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#25429;&#25417;&#25991;&#26723;&#20013;&#36328;&#36234;&#21477;&#23376;&#30340;&#20107;&#20214;&#35770;&#20803;&#12290;&#20026;&#20102;&#22635;&#34917;&#27492;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FewDocAE&#65292;&#19968;&#31181;&#22522;&#20110;&#29616;&#26377;&#25991;&#26723;&#32423;&#20107;&#20214;&#25552;&#21462;&#25968;&#25454;&#38598;&#30340;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#20803;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#26032;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;N-Way-D-Doc&#25277;&#26679;&#26041;&#27861;&#37325;&#26500;&#20102;&#35821;&#26009;&#24211;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;N-Way-K-Shot&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#24403;&#21069;&#25991;&#26723;&#32423;&#31070;&#32463;&#27169;&#22411;&#35843;&#25972;&#21040;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#20026;&#36328;&#39046;&#22495;&#21644;&#39046;&#22495;&#20869;&#25552;&#20379;&#22522;&#32447;&#32467;&#26524;&#12290;&#30001;&#20110;&#35770;&#20803;&#25552;&#21462;&#21462;&#20915;&#20110;&#22810;&#20010;&#21477;&#23376;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#23398;&#20064;&#36807;&#31243;&#20165;&#38480;&#20110;&#24456;&#23569;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#39033;&#26032;&#20219;&#21153;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event argument extraction (EAE) has been well studied at the sentence level but under-explored at the document level. In this paper, we study to capture event arguments that actually spread across sentences in documents. Prior works usually assume full access to rich document supervision, ignoring the fact that the available argument annotation is usually limited. To fill this gap, we present FewDocAE, a Few-Shot Document-Level Event Argument Extraction benchmark, based on the existing document-level event extraction dataset. We first define the new problem and reconstruct the corpus by a novel N -Way-D-Doc sampling instead of the traditional N -Way-K-Shot strategy. Then we adjust the current document-level neural models into the few-shot setting to provide baseline results under in- and cross-domain settings. Since the argument extraction depends on the context from multiple sentences and the learning process is limited to very few examples, we find this novel task to be very challeng
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#40784;&#30340;&#31616;&#26131;&#24503;&#35821;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#36741;&#21161;&#19981;&#21516;&#20154;&#32676;&#29702;&#35299;&#22797;&#26434;&#30340;&#24503;&#35821;&#20070;&#38754;&#35821;&#35328;&#65307;&#35813;&#35821;&#26009;&#24211;&#36890;&#36807;&#33258;&#21160;&#21477;&#23376;&#23545;&#40784;&#26041;&#27861;&#20351;&#22810;&#20010;&#25991;&#26723;&#23545;&#40784;&#65292;&#19988;&#36136;&#37327;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2209.01106</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#30340;&#23545;&#40784;&#30340;&#31616;&#26131;&#24503;&#35821;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
A New Aligned Simple German Corpus. (arXiv:2209.01106v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#40784;&#30340;&#31616;&#26131;&#24503;&#35821;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#36741;&#21161;&#19981;&#21516;&#20154;&#32676;&#29702;&#35299;&#22797;&#26434;&#30340;&#24503;&#35821;&#20070;&#38754;&#35821;&#35328;&#65307;&#35813;&#35821;&#26009;&#24211;&#36890;&#36807;&#33258;&#21160;&#21477;&#23376;&#23545;&#40784;&#26041;&#27861;&#20351;&#22810;&#20010;&#25991;&#26723;&#23545;&#40784;&#65292;&#19988;&#36136;&#37327;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"Leichte Sprache"&#26159;&#24503;&#35821;&#29256;&#30340;&#31616;&#26131;&#33521;&#35821;&#65292;&#26088;&#22312;&#20026;&#19981;&#21516;&#20154;&#32676;&#25552;&#20379;&#22797;&#26434;&#30340;&#20070;&#38754;&#35821;&#35328;&#65292;&#20197;&#20415;&#20351;&#36825;&#20123;&#20869;&#23481;&#26131;&#20110;&#29702;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#40784;&#21477;&#23376;&#30340;&#21333;&#35821;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#31616;&#26131;&#24503;&#35821;--&#24503;&#35821;&#12290;&#23427;&#21253;&#21547;&#22810;&#20010;&#25991;&#26723;&#23545;&#40784;&#26469;&#28304;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#21477;&#23376;&#23545;&#40784;&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#20102;&#23545;&#40784;&#12290;&#25105;&#20204;&#22522;&#20110;&#25163;&#21160;&#26631;&#35760;&#30340;&#23545;&#40784;&#25991;&#26723;&#30340;&#23376;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#23545;&#40784;&#36136;&#37327;&#12290;&#26681;&#25454;F1&#20998;&#25968;&#27979;&#37327;&#65292;&#25105;&#20204;&#30340;&#21477;&#23376;&#23545;&#40784;&#36136;&#37327;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#22312;CC BY-SA&#19979;&#21457;&#24067;&#25968;&#25454;&#38598;&#65292;&#22312;MIT&#35768;&#21487;&#19979;&#21457;&#24067;&#38468;&#24102;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
"Leichte Sprache", the German counterpart to Simple English, is a regulated language aiming to facilitate complex written language that would otherwise stay inaccessible to different groups of people. We present a new sentence-aligned monolingual corpus for Simple German -- German. It contains multiple document-aligned sources which we have aligned using automatic sentence-alignment methods. We evaluate our alignments based on a manually labelled subset of aligned documents. The quality of our sentence alignments, as measured by F1-score, surpasses previous work. We publish the dataset under CC BY-SA and the accompanying code under MIT license.
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#23454;&#29616;&#32511;&#33394;&#32463;&#27982;&#65292;&#38656;&#35201;&#21487;&#38752;&#12289;&#21487;&#27604;&#36739;&#21644;&#21487;&#39564;&#35777;&#30340;&#29615;&#22659;&#22768;&#26126;&#12290;&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29615;&#22659;&#22768;&#26126;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#19987;&#23478;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#26816;&#27979;&#29615;&#22659;&#22768;&#26126;&#22312;&#23395;&#24230;&#30005;&#35805;&#20250;&#35758;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#24182;&#21457;&#29616;&#35813;&#20351;&#29992;&#24773;&#20917;&#33258;2015&#24180;&#20197;&#26469;&#26377;&#31283;&#27493;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2209.00507</link><description>&lt;p&gt;
&#29615;&#22659;&#22768;&#26126;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Environmental Claim Detection. (arXiv:2209.00507v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00507
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#32511;&#33394;&#32463;&#27982;&#65292;&#38656;&#35201;&#21487;&#38752;&#12289;&#21487;&#27604;&#36739;&#21644;&#21487;&#39564;&#35777;&#30340;&#29615;&#22659;&#22768;&#26126;&#12290;&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29615;&#22659;&#22768;&#26126;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#19987;&#23478;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#26816;&#27979;&#29615;&#22659;&#22768;&#26126;&#22312;&#23395;&#24230;&#30005;&#35805;&#20250;&#35758;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#24182;&#21457;&#29616;&#35813;&#20351;&#29992;&#24773;&#20917;&#33258;2015&#24180;&#20197;&#26469;&#26377;&#31283;&#27493;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#23454;&#29616;&#32511;&#33394;&#32463;&#27982;&#65292;&#20844;&#21496;&#25152;&#20316;&#20986;&#30340;&#29615;&#22659;&#22768;&#26126;&#24517;&#39035;&#26159;&#21487;&#38752;&#12289;&#21487;&#27604;&#36739;&#21644;&#21487;&#39564;&#35777;&#30340;&#12290;&#20026;&#20102;&#22823;&#35268;&#27169;&#20998;&#26512;&#36825;&#20123;&#22768;&#26126;&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#26469;&#39318;&#20808;&#26816;&#27979;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#23384;&#22312;&#27492;&#31867;&#25968;&#25454;&#38598;&#25110;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#29615;&#22659;&#22768;&#26126;&#26816;&#27979;&#20219;&#21153;&#12290;&#20026;&#20102;&#37197;&#21512;&#35813;&#20219;&#21153;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#30001;&#19987;&#23478;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#21644;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#39044;&#35272;&#20102;&#27492;&#31867;&#27169;&#22411;&#30340;&#19968;&#20010;&#28508;&#22312;&#24212;&#29992;&#65306;&#25105;&#20204;&#26816;&#27979;&#23395;&#24230;&#30005;&#35805;&#20250;&#35758;&#20013;&#25152;&#20316;&#20986;&#30340;&#29615;&#22659;&#22768;&#26126;&#65292;&#24182;&#21457;&#29616;&#33258;2015&#24180;&#24052;&#40654;&#21327;&#35758;&#20197;&#26469;&#29615;&#22659;&#22768;&#26126;&#30340;&#25968;&#37327;&#36880;&#28176;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
To transition to a green economy, environmental claims made by companies must be reliable, comparable, and verifiable. To analyze such claims at scale, automated methods are needed to detect them in the first place. However, there exist no datasets or models for this. Thus, this paper introduces the task of environmental claim detection. To accompany the task, we release an expert-annotated dataset and models trained on this dataset. We preview one potential application of such models: We detect environmental claims made in quarterly earning calls and find that the number of environmental claims has steadily increased since the Paris Agreement in 2015.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22810;&#23186;&#20307;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#36319;&#36394;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#20013;&#30340;&#21382;&#21490;&#29366;&#24577;&#26469;&#29983;&#25104;&#21518;&#32493;&#27493;&#39588;&#65292;&#33021;&#23545;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#20855;&#26377;&#24402;&#32435;&#33021;&#21147;&#24182;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.12306</link><description>&lt;p&gt;
&#22810;&#23186;&#20307;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#29992;&#20110;&#20219;&#21153;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Multimedia Generative Script Learning for Task Planning. (arXiv:2208.12306v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12306
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22810;&#23186;&#20307;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#36319;&#36394;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#20013;&#30340;&#21382;&#21490;&#29366;&#24577;&#26469;&#29983;&#25104;&#21518;&#32493;&#27493;&#39588;&#65292;&#33021;&#23545;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#20855;&#26377;&#24402;&#32435;&#33021;&#21147;&#24182;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#30340;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#26088;&#22312;&#22522;&#20110;&#30446;&#26631;&#29983;&#25104;&#21518;&#32493;&#27493;&#39588;&#65292;&#36825;&#26159;&#24110;&#21161;&#26426;&#22120;&#20154;&#25191;&#34892;&#26085;&#24120;&#29983;&#27963;&#20013;&#20856;&#22411;&#27963;&#21160;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#21382;&#21490;&#29366;&#24577;&#19981;&#20165;&#30001;&#32473;&#20154;&#30340;&#35821;&#35328;&#25351;&#31034;&#25429;&#33719;&#65292;&#32780;&#19988;&#36824;&#36890;&#36807;&#30456;&#20276;&#30340;&#22270;&#20687;&#25552;&#20379;&#20102;&#38468;&#21152;&#20449;&#24687;&#65292;&#37027;&#20040;&#27492;&#20219;&#21153;&#30340;&#34920;&#29616;&#21487;&#20197;&#25913;&#21892;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#22810;&#23186;&#20307;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#65292;&#20197;&#36890;&#36807;&#36319;&#36394;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#20013;&#30340;&#21382;&#21490;&#29366;&#24577;&#26469;&#29983;&#25104;&#21518;&#32493;&#27493;&#39588;&#65292;&#24182;&#25552;&#20379;&#20102;&#21253;&#21547;2,338&#20010;&#20219;&#21153;&#21644;31,496&#20010;&#27493;&#39588;&#21450;&#20854;&#25551;&#36848;&#24615;&#22270;&#20687;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#21487;&#35270;&#29366;&#24577;&#21487;&#36319;&#36394;&#30340;&#33050;&#26412;&#65292;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#20855;&#26377;&#24402;&#32435;&#33021;&#21147;&#65292;&#24182;&#19988;&#20854;&#27493;&#39588;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#22810;&#23186;&#20307;&#36873;&#25321;&#24615;&#32534;&#30721;&#22120;&#23545;&#35270;&#35273;&#29366;&#24577;&#21464;&#21270;&#36827;&#34892;&#32534;&#30721;&#65292;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#35299;&#30721;&#22120;&#20256;&#36882;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#25277;&#26679;&#21644;&#27874;&#26463;&#25628;&#32034;&#35299;&#30721;&#29983;&#25104;&#22810;&#26679;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal-oriented generative script learning aims to generate subsequent steps based on a goal, which is an essential task to assist robots in performing stereotypical activities of daily life. We show that the performance of this task can be improved if historical states are not just captured by the linguistic instructions given to people, but are augmented with the additional information provided by accompanying images. Therefore, we propose a new task, Multimedia Generative Script Learning, to generate subsequent steps by tracking historical states in both text and vision modalities, as well as presenting the first benchmark containing 2,338 tasks and 31,496 steps with descriptive images. We aim to generate scripts that are visual-state trackable, inductive for unseen tasks, and diverse in their individual steps. We propose to encode visual state changes through a multimedia selective encoder, transferring knowledge from previously observed tasks using a retrieval-augmented decoder, and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24402;&#32435;&#20559;&#35265;&#24378;&#21046;&#25191;&#34892;&#26126;&#30830;&#30340;&#20851;&#31995;&#32467;&#26500;&#65292;&#20174;&#32780;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#34920;&#31034;&#26174;&#24335;&#22320;&#32452;&#25104;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#38543;&#26426;&#26631;&#35760;&#24207;&#21015;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#38544;&#21547;&#30340;&#30495;&#23454;&#22270;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#20986;&#31526;&#21495;&#32593;&#32476;&#65288;&#27169;&#24335;&#65289;&#65292;&#30452;&#25509;&#21453;&#26144;&#20102;&#35821;&#35328;&#30340;&#22522;&#30784;&#21477;&#27861;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2207.03777</link><description>&lt;p&gt;
&#38544;&#34255;&#32467;&#26500;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hidden Schema Networks. (arXiv:2207.03777v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24402;&#32435;&#20559;&#35265;&#24378;&#21046;&#25191;&#34892;&#26126;&#30830;&#30340;&#20851;&#31995;&#32467;&#26500;&#65292;&#20174;&#32780;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#34920;&#31034;&#26174;&#24335;&#22320;&#32452;&#25104;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#38543;&#26426;&#26631;&#35760;&#24207;&#21015;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#38544;&#21547;&#30340;&#30495;&#23454;&#22270;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#20986;&#31526;&#21495;&#32593;&#32476;&#65288;&#27169;&#24335;&#65289;&#65292;&#30452;&#25509;&#21453;&#26144;&#20102;&#35821;&#35328;&#30340;&#22522;&#30784;&#21477;&#27861;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25512;&#26029;&#20986;&#24378;&#22823;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#20854;&#20013;&#34164;&#21547;&#30528;&#20016;&#23500;&#30340;&#35821;&#20041;&#21644;&#21477;&#27861;&#20869;&#23481;&#65292;&#23613;&#31649;&#26159;&#38544;&#21547;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24402;&#32435;&#20559;&#35265;&#24378;&#21046;&#25191;&#34892;&#26126;&#30830;&#30340;&#20851;&#31995;&#32467;&#26500;&#65292;&#20174;&#32780;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#34920;&#31034;&#26174;&#24335;&#22320;&#32452;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#27169;&#22411;&#23558;&#21477;&#23376;&#32534;&#30721;&#20026;&#31526;&#21495;&#24207;&#21015;&#65288;&#32452;&#21512;&#34920;&#31034;&#65289;&#65292;&#36825;&#20123;&#31526;&#21495;&#23545;&#24212;&#20110;&#20840;&#23616;&#28508;&#22312;&#22270;&#19978;&#24102;&#20559;&#32622;&#30340;&#38543;&#26426;&#28216;&#36208;&#22120;&#35775;&#38382;&#30340;&#33410;&#28857;&#65292;&#24182;&#25512;&#26029;&#20854;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;&#20154;&#24037;&#29983;&#25104;&#30340;&#38543;&#26426;&#26631;&#35760;&#24207;&#21015;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#38544;&#21547;&#30340;&#30495;&#23454;&#22270;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#21644;GPT-2&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#20986;&#31526;&#21495;&#32593;&#32476;&#65288;&#27169;&#24335;&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#65306;&#65288;i&#65289;&#25512;&#26029;&#20986;&#30340;&#31526;&#21495;&#21487;&#20197;&#35299;&#37322;&#20026;&#32534;&#30721;&#19981;&#21516;&#26041;&#38754;&#30340;&#21547;&#20041;&#65292;&#65288;ii&#65289;&#23427;&#20204;&#30340;&#22797;&#21512;&#24615;&#30452;&#25509;&#21453;&#26144;&#20102;&#35821;&#35328;&#30340;&#22522;&#30784;&#21477;&#27861;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large, pretrained language models infer powerful representations that encode rich semantic and syntactic content, albeit implicitly. In this work we introduce a novel neural language model that enforces, via inductive biases, explicit relational structures which allow for compositionality onto the output representations of pretrained language models. Specifically, the model encodes sentences into sequences of symbols (composed representations), which correspond to the nodes visited by biased random walkers on a global latent graph, and infers the posterior distribution of the latter. We first demonstrate that the model is able to uncover ground-truth graphs from artificially generated datasets of random token sequences. Next, we leverage pretrained BERT and GPT-2 language models as encoder and decoder, respectively, to infer networks of symbols (schemata) from natural language datasets. Our experiments show that (i) the inferred symbols can be interpreted as encoding different aspects 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; B2T &#36830;&#25509;&#30340;&#26041;&#27861;&#65292;&#36830;&#25509;&#20102; Pre-LN &#21644; Post-LN &#23618;&#30340;&#36755;&#20986;&#65292;&#20026;&#28145;&#24230; Transformer &#25552;&#20379;&#20102;&#39640;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#30340;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.00330</link><description>&lt;p&gt;
B2T &#36830;&#25509;&#65306;&#26381;&#21153;&#20110;&#28145;&#24230; Transformer &#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
B2T Connection: Serving Stability and Performance in Deep Transformers. (arXiv:2206.00330v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; B2T &#36830;&#25509;&#30340;&#26041;&#27861;&#65292;&#36830;&#25509;&#20102; Pre-LN &#21644; Post-LN &#23618;&#30340;&#36755;&#20986;&#65292;&#20026;&#28145;&#24230; Transformer &#25552;&#20379;&#20102;&#39640;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#30340;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23618;&#24402;&#19968;&#21270;&#65288;LN&#65289;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;Transformer &#30340;&#26550;&#26500;&#21487;&#20197;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;Post-LN &#21644; Pre-LN&#12290;&#26368;&#36817;&#30340; Transformers &#20542;&#21521;&#20110;&#37319;&#29992; Pre-LN&#65292;&#22240;&#20026;&#22312; Post-LN &#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#28145;&#24230; Transformers &#20013;&#65288;&#20363;&#22914;&#26377;&#21313;&#20010;&#25110;&#26356;&#22810;&#23618;&#30340;&#27169;&#22411;&#65289;&#65292;&#35757;&#32451;&#32463;&#24120;&#19981;&#31283;&#23450;&#65292;&#23548;&#33268;&#24471;&#21040;&#26080;&#29992;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#19982; Pre-LN &#30456;&#27604;&#65292;&#22312;&#30456;&#23545;&#36739;&#27973;&#30340; Transformers&#65288;&#20363;&#22914;&#26377;&#20845;&#20010;&#25110;&#26356;&#23569;&#30340;&#23618;&#65289;&#20013;&#65292;Post-LN &#19968;&#30452;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#36825;&#20123;&#19981;&#19968;&#33268;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;&#20102;&#20197;&#19979;&#21457;&#29616;&#65306;1&#65289;Post-LN &#20013;&#30340; LN &#26159;&#23548;&#33268;&#19981;&#31283;&#23450;&#35757;&#32451;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#32780; Pre-LN &#21487;&#20197;&#36991;&#20813;&#36825;&#31181;&#38382;&#39064;&#65307;2&#65289;Post-LN &#24448;&#24448;&#20250;&#22312;&#21453;&#21521;&#20256;&#25773;&#30340;&#39640;&#23618;&#20445;&#30041;&#26356;&#22823;&#30340;&#26799;&#24230;&#33539;&#25968;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;&#21033;&#29992;&#36825;&#20123;&#26032;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21516;&#26102;&#25552;&#20379;&#39640;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#30340; Transformer &#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; B2T Connection&#65292;&#23427;&#36830;&#25509;&#20102; Pre-LN &#21644; Post-LN &#23618;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;B2T Connection &#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#28145;&#24230; Transformers&#65288;&#26377;&#21313;&#20010;&#25110;&#26356;&#22810;&#23618;&#65289;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
From the perspective of the layer normalization (LN) positions, the architectures of Transformers can be categorized into two types: Post-LN and Pre-LN. Recent Transformers tend to be Pre-LN because, in Post-LN with deep Transformers (e.g., those with ten or more layers), the training is often unstable, resulting in useless models. However, Post-LN has consistently achieved better performance than Pre-LN in relatively shallow Transformers (e.g., those with six or fewer layers). This study first investigates the reason for these discrepant observations empirically and theoretically and made the following discoveries: 1, the LN in Post-LN is the main source of the vanishing gradient problem that leads to unstable training, whereas Pre-LN prevents it, and 2, Post-LN tends to preserve larger gradient norms in higher layers during the back-propagation, which may lead to effective training. Exploiting the new findings, we propose a method that can provide both high stability and effective tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32858;&#21512;&#20102;&#26469;&#33258;&#20061;&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#20107;&#23454;&#38169;&#35823;&#27880;&#37322;&#65292;&#38024;&#23545;&#24213;&#23618;&#30340;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;&#20107;&#23454;&#24230;&#37327;&#26631;&#20934;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24230;&#37327;&#26631;&#20934;&#30340;&#24615;&#33021;&#22240;&#19981;&#21516;&#30340;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#32780;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2205.12854</link><description>&lt;p&gt;
&#29702;&#35299;&#24635;&#32467;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65306;&#38169;&#35823;&#65292;&#25688;&#35201;&#29983;&#25104;&#22120;&#65292;&#25968;&#25454;&#38598;&#21644;&#38169;&#35823;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors. (arXiv:2205.12854v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32858;&#21512;&#20102;&#26469;&#33258;&#20061;&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#20107;&#23454;&#38169;&#35823;&#27880;&#37322;&#65292;&#38024;&#23545;&#24213;&#23618;&#30340;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;&#20107;&#23454;&#24230;&#37327;&#26631;&#20934;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24230;&#37327;&#26631;&#20934;&#30340;&#24615;&#33021;&#22240;&#19981;&#21516;&#30340;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#32780;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#21046;&#36896;&#20107;&#23454;&#38169;&#35823;&#30340;&#20542;&#21521;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#35774;&#35745;&#29992;&#20110;&#26816;&#27979;&#20107;&#23454;&#38169;&#35823;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#27880;&#37322;&#24403;&#21069;&#31995;&#32479;&#36755;&#20986;&#20013;&#30340;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#24635;&#32467;&#31995;&#32479;&#12289;&#24230;&#37327;&#26631;&#20934;&#21644;&#27880;&#37322;&#22522;&#20934;&#30340;&#19981;&#26029;&#21457;&#23637;&#20351;&#24471;&#20107;&#23454;&#35780;&#20215;&#25104;&#20026;&#19968;&#20010;&#31227;&#21160;&#30340;&#30446;&#26631;&#65292;&#24182;&#19988;&#22312;&#24230;&#37327;&#26631;&#20934;&#20043;&#38388;&#36827;&#34892;&#28165;&#26224;&#30340;&#27604;&#36739;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32858;&#21512;&#20102;&#26469;&#33258;&#20061;&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#20107;&#23454;&#38169;&#35823;&#27880;&#37322;&#65292;&#24182;&#26681;&#25454;&#24213;&#23618;&#30340;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;&#20107;&#23454;&#24230;&#37327;&#26631;&#20934;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#26368;&#36817;&#30340;ChatGPT-based&#24230;&#37327;&#26631;&#20934;&#65292;&#22312;&#36825;&#20010;&#20998;&#23618;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#26368;&#36817;&#22312;&#20107;&#23454;&#26816;&#27979;&#31354;&#38388;&#30340;&#24456;&#22823;&#25913;&#36827;&#26159;&#38024;&#23545;&#26087;&#30340;(&#21069;Transformer) &#27169;&#22411;&#30340;&#24635;&#32467;&#65292;&#32780;&#19981;&#26159;&#26356;&#30456;&#20851;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The propensity of abstractive summarization models to make factual errors has been studied extensively, including design of metrics to detect factual errors and annotation of errors in current systems' outputs. However, the ever-evolving nature of summarization systems, metrics, and annotated benchmarks makes factuality evaluation a moving target, and drawing clear comparisons among metrics has become increasingly difficult. In this work, we aggregate factuality error annotations from nine existing datasets and stratify them according to the underlying summarization model. We compare performance of state-of-the-art factuality metrics, including recent ChatGPT-based metrics, on this stratified benchmark and show that their performance varies significantly across different types of summarization models. Critically, our analysis shows that much of the recent improvement in the factuality detection space has been on summaries from older (pre-Transformer) models instead of more relevant rec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TAGPRIME&#30340;&#32479;&#19968;&#20851;&#31995;&#32467;&#26500;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#32473;&#23450;&#26465;&#20214;&#20449;&#24687;&#28155;&#21152;&#21040;&#36755;&#20837;&#25991;&#26412;&#20013;&#65292;&#20351;&#24471;&#36755;&#20986;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#26356;&#36866;&#21512;&#25552;&#21462;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#20851;&#31995;&#12290;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#26032;&#30340;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2205.12585</link><description>&lt;p&gt;
TAGPRIME&#65306;&#19968;&#31181;&#29992;&#20110;&#20851;&#31995;&#32467;&#26500;&#25552;&#21462;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TAGPRIME: A Unified Framework for Relational Structure Extraction. (arXiv:2205.12585v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TAGPRIME&#30340;&#32479;&#19968;&#20851;&#31995;&#32467;&#26500;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#32473;&#23450;&#26465;&#20214;&#20449;&#24687;&#28155;&#21152;&#21040;&#36755;&#20837;&#25991;&#26412;&#20013;&#65292;&#20351;&#24471;&#36755;&#20986;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#26356;&#36866;&#21512;&#25552;&#21462;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#20851;&#31995;&#12290;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#26032;&#30340;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#35768;&#22810;&#20219;&#21153;&#38656;&#35201;&#25552;&#21462;&#32473;&#23450;&#26465;&#20214;&#19979;&#30340;&#20851;&#31995;&#20449;&#24687;&#65292;&#20363;&#22914;&#20107;&#20214;&#21442;&#25968;&#25552;&#21462;&#12289;&#20851;&#31995;&#25552;&#21462;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#35821;&#20041;&#35299;&#26512;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#24120;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#20998;&#21035;&#25552;&#20986;&#22797;&#26434;&#30340;&#27169;&#22411;&#65292;&#36739;&#23569;&#20851;&#27880;&#36825;&#20123;&#20219;&#21153;&#30340;&#20849;&#24615;&#65292;&#24182;&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TAGPRIME&#65292;&#26088;&#22312;&#20174;&#32479;&#19968;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#36825;&#20123;&#20851;&#31995;&#32467;&#26500;&#25552;&#21462;&#38382;&#39064;&#12290;TAGPRIME&#26159;&#19968;&#31181;&#24207;&#21015;&#26631;&#35760;&#27169;&#22411;&#65292;&#23558;&#32473;&#23450;&#26465;&#20214;&#65288;&#20363;&#22914;&#20107;&#20214;&#35302;&#21457;&#22120;&#65289;&#30340;&#20449;&#24687;&#28155;&#21152;&#21040;&#36755;&#20837;&#25991;&#26412;&#20013;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#36825;&#20123;&#26465;&#20214;&#20449;&#24687;&#20351;&#24471;&#36755;&#20986;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#21253;&#21547;&#26356;&#22810;&#20851;&#20110;&#32473;&#23450;&#26465;&#20214;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#26356;&#36866;&#21512;&#25552;&#21462;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#20851;&#31995;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#20013;&#34920;&#26126;&#65292;TAGPRIME&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#26032;&#30340;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many tasks in natural language processing require the extraction of relationship information for a given condition, such as event argument extraction, relation extraction, and task-oriented semantic parsing. Recent works usually propose sophisticated models for each task independently and pay less attention to the commonality of these tasks and to have a unified framework for all the tasks. In this work, we propose to take a unified view of all these tasks and introduce TAGPRIME to address relational structure extraction problems. TAGPRIME is a sequence tagging model that appends priming words about the information of the given condition (such as an event trigger) to the input text. With the self-attention mechanism in pre-trained language models, the priming words make the output contextualized representations contain more information about the given condition, and hence become more suitable for extracting specific relationships for the condition. Extensive experiments and analyses on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#32780;&#20840;&#30340;EAE&#26412;&#20307;&#35770;&#65292;105&#20010;&#20107;&#20214;&#21644;220&#20010;&#35770;&#20803;&#35282;&#33394;&#30340;&#21253;&#21547;&#22312;&#20869;&#65292;&#21033;&#29992;&#36825;&#20010;&#26412;&#20307;&#35770;&#21019;&#24314;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#30340;&#36890;&#29992;&#24615;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GENEVA&#65292;&#20849;&#21253;&#21547;&#22235;&#20010;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2205.12505</link><description>&lt;p&gt;
GENEVA&#65306;&#8220;&#36890;&#29992;&#24615;&#22522;&#20934;&#27979;&#35797;&#8221;&#20107;&#20214;&#35770;&#20803;&#25552;&#21462;&#65292;&#28085;&#30422;&#25968;&#30334;&#31181;&#20107;&#20214;&#31867;&#22411;&#21644;&#35770;&#20803;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles. (arXiv:2205.12505v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#32780;&#20840;&#30340;EAE&#26412;&#20307;&#35770;&#65292;105&#20010;&#20107;&#20214;&#21644;220&#20010;&#35770;&#20803;&#35282;&#33394;&#30340;&#21253;&#21547;&#22312;&#20869;&#65292;&#21033;&#29992;&#36825;&#20010;&#26412;&#20307;&#35770;&#21019;&#24314;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#30340;&#36890;&#29992;&#24615;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GENEVA&#65292;&#20849;&#21253;&#21547;&#22235;&#20010;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20107;&#20214;&#35770;&#20803;&#25552;&#21462;&#65288;EAE&#65289;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#20197;&#36866;&#24212;&#26032;&#30340;&#20107;&#20214;&#31867;&#22411;&#21644;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#22914;ACE&#21644;ERE&#21482;&#28085;&#30422;&#19981;&#21040;40&#31181;&#20107;&#20214;&#31867;&#22411;&#21644;25&#31181;&#38754;&#21521;&#23454;&#20307;&#30340;&#35770;&#20803;&#35282;&#33394;&#12290;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#22810;&#26679;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#24433;&#21709;&#20102;&#36825;&#20123;&#25968;&#25454;&#38598;&#23545;EAE&#27169;&#22411;&#36890;&#29992;&#24615;&#30340;&#20805;&#20998;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#32780;&#20840;&#30340;EAE&#26412;&#20307;&#35770;&#65292;&#22312;FrameNet&#30340;&#22522;&#30784;&#19978;&#21019;&#24314;&#20102;&#21253;&#21547;115&#20010;&#20107;&#20214;&#21644;220&#20010;&#35770;&#20803;&#35282;&#33394;&#30340;&#26412;&#20307;&#35770;&#65292;&#20854;&#20013;&#35768;&#22810;&#35282;&#33394;&#19981;&#26159;&#23454;&#20307;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26412;&#20307;&#35770;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;GENEVA&#65292;&#19968;&#31181;&#22810;&#26679;&#21270;&#30340;&#36890;&#29992;&#24615;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22235;&#20010;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works in Event Argument Extraction (EAE) have focused on improving model generalizability to cater to new events and domains. However, standard benchmarking datasets like ACE and ERE cover less than 40 event types and 25 entity-centric argument roles. Limited diversity and coverage hinder these datasets from adequately evaluating the generalizability of EAE models. In this paper, we first contribute by creating a large and diverse EAE ontology. This ontology is created by transforming FrameNet, a comprehensive semantic role labeling (SRL) dataset for EAE, by exploiting the similarity between these two tasks. Then, exhaustive human expert annotations are collected to build the ontology, concluding with 115 events and 220 argument roles, with a significant portion of roles not being entities. We utilize this ontology to further introduce GENEVA, a diverse generalizability benchmarking dataset comprising four test suites, aimed at evaluating models' ability to handle limited data a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#35789;&#34955;&#12289;&#24207;&#21015;&#12289;&#22270;&#24418;&#21644;&#20998;&#23618;&#26041;&#27861;&#65292;&#21457;&#29616;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#26080;&#27861;&#36229;&#36234;&#29616;&#20195;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#19988;&#29978;&#33267;&#26377;&#26102;&#34920;&#29616;&#19981;&#22914;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36136;&#30097;&#20102;&#36807;&#21435;&#20960;&#24180;&#20013;&#20026;&#24320;&#21457;&#26032;&#30340;&#22270;&#24418;&#26041;&#27861;&#25237;&#20837;&#30340;&#24040;&#22823;&#21162;&#21147;&#20197;&#21450;&#23427;&#20204;&#20026;&#25991;&#26412;&#20998;&#31867;&#24102;&#26469;&#30340;&#25215;&#35834;&#12290;</title><link>http://arxiv.org/abs/2204.03954</link><description>&lt;p&gt;
&#25105;&#20204;&#30495;&#30340;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#21527;&#65311;&#38024;&#23545;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#35789;&#34955;&#12289;&#24207;&#21015;&#12289;&#22270;&#21644;&#23618;&#27425;&#32467;&#26500;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Are We Really Making Much Progress? Bag-of-Words vs. Sequence vs. Graph vs. Hierarchy for Single- and Multi-Label Text Classification. (arXiv:2204.03954v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#35789;&#34955;&#12289;&#24207;&#21015;&#12289;&#22270;&#24418;&#21644;&#20998;&#23618;&#26041;&#27861;&#65292;&#21457;&#29616;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#26080;&#27861;&#36229;&#36234;&#29616;&#20195;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#19988;&#29978;&#33267;&#26377;&#26102;&#34920;&#29616;&#19981;&#22914;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36136;&#30097;&#20102;&#36807;&#21435;&#20960;&#24180;&#20013;&#20026;&#24320;&#21457;&#26032;&#30340;&#22270;&#24418;&#26041;&#27861;&#25237;&#20837;&#30340;&#24040;&#22823;&#21162;&#21147;&#20197;&#21450;&#23427;&#20204;&#20026;&#25991;&#26412;&#20998;&#31867;&#24102;&#26469;&#30340;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#24341;&#21457;&#20102;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#22270;&#24418;&#26041;&#27861;&#30340;&#22797;&#33487;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#26159;&#21542;&#27604;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#29616;&#20195;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26356;&#26377;&#30410;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#20016;&#23500;&#30340;&#35789;&#34955;&#12289;&#22522;&#20110;&#24207;&#21015;&#12289;&#22522;&#20110;&#22270;&#24418;&#21644;&#20998;&#23618;&#26041;&#27861;&#12290;&#25105;&#20204;&#32858;&#21512;&#20102;&#26469;&#33258;&#25991;&#29486;&#30340;&#32467;&#26524;&#65292;&#22312;5&#20010;&#21333;&#26631;&#31614;&#21644;7&#20010;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#20102;&#25105;&#20204;&#33258;&#24049;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26126;&#30830;&#34920;&#26126;&#65292;&#22312;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#26080;&#27861;&#36229;&#36234;&#31934;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26377;&#26102;&#29978;&#33267;&#34920;&#29616;&#19981;&#22914;&#35789;&#34955;&#19978;&#30340;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#36136;&#30097;&#20102;&#36807;&#21435;&#20960;&#24180;&#20013;&#20026;&#24320;&#21457;&#26032;&#30340;&#22270;&#24418;&#26041;&#27861;&#25237;&#20837;&#30340;&#24040;&#22823;&#21162;&#21147;&#20197;&#21450;&#23427;&#20204;&#20026;&#25991;&#26412;&#20998;&#31867;&#24102;&#26469;&#30340;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of graph neural networks has triggered a resurgence of graph-based methods for single-label and multi-label text classification. However, it is unclear whether these graph-based methods are beneficial compared to standard machine learning methods and modern pretrained language models. We compare a rich selection of bag-of-words, sequence-based, graph-based, and hierarchical methods for text classification. We aggregate results from the literature over 5 single-label and 7 multi-label datasets and run our own experiments. Our findings unambiguously demonstrate that for single-label and multi-label classification tasks, the graph-based methods fail to outperform fine-tuned language models and sometimes even perform worse than standard machine learning methods like multilayer perceptron (MLP) on a bag-of-words. This questions the enormous amount of effort put into the development of new graph-based methods in the last years and the promises they make for text classification
&lt;/p&gt;</description></item><item><title>SummaReranker&#26159;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#30340;&#20108;&#27425;&#25490;&#24207;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#25277;&#35937;&#25688;&#35201;&#12290;&#23427;&#33021;&#22815;&#30452;&#25509;&#22312;&#19968;&#32452;&#25688;&#35201;&#20505;&#36873;&#19978;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#20174;&#32780;&#20248;&#21270;&#22522;&#26412;&#27169;&#22411;&#30340;ROUGE&#20998;&#25968;&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;SOTA&#12290;</title><link>http://arxiv.org/abs/2203.06569</link><description>&lt;p&gt;
SummaReranker&#65306;&#19968;&#31181;&#22810;&#20219;&#21153;&#19987;&#23478;&#28151;&#21512;&#20108;&#27425;&#25490;&#24207;&#26694;&#26550;&#65292;&#29992;&#20110;&#25277;&#35937;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization. (arXiv:2203.06569v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.06569
&lt;/p&gt;
&lt;p&gt;
SummaReranker&#26159;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#30340;&#20108;&#27425;&#25490;&#24207;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#25277;&#35937;&#25688;&#35201;&#12290;&#23427;&#33021;&#22815;&#30452;&#25509;&#22312;&#19968;&#32452;&#25688;&#35201;&#20505;&#36873;&#19978;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#20174;&#32780;&#20248;&#21270;&#22522;&#26412;&#27169;&#22411;&#30340;ROUGE&#20998;&#25968;&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;SOTA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#25277;&#35937;&#25688;&#35201;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#22312;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#27874;&#26463;&#25628;&#32034;&#36827;&#34892;&#35299;&#30721;&#20197;&#29983;&#25104;&#21807;&#19968;&#30340;&#25688;&#35201;&#12290;&#28982;&#32780;&#65292;&#25628;&#32034;&#31354;&#38388;&#38750;&#24120;&#22823;&#65292;&#24182;&#19988;&#30001;&#20110;&#26333;&#20809;&#20559;&#24046;&#65292;&#36825;&#31181;&#35299;&#30721;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#30452;&#25509;&#35757;&#32451;&#31532;&#20108;&#38454;&#27573;&#27169;&#22411;&#22312;&#19968;&#32452;&#25688;&#35201;&#20505;&#36873;&#19978;&#25191;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#19987;&#23478;&#28151;&#21512;&#26694;&#26550;SummaReranker&#23398;&#20064;&#36873;&#25321;&#26356;&#22909;&#30340;&#20505;&#36873;&#32773;&#65292;&#24182;&#25345;&#32493;&#25552;&#39640;&#22522;&#26412;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20197;&#22522;&#26412;PEGASUS&#20026;&#20363;&#65292;&#25105;&#20204;&#22312;CNN-DailyMail&#65288;47.16 ROUGE-1&#65289;&#19978;&#25552;&#39640;&#20102;5.44&#65285;&#65292;&#22312;XSum&#65288;48.12 ROUGE-1&#65289;&#19978;&#25552;&#39640;&#20102;1.31&#65285;&#65292;&#22312;Reddit TIFU&#65288;29.83 ROUGE-1&#65289;&#19978;&#25552;&#39640;&#20102;9.34&#65285;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#26816;&#26597;&#28857;&#23558;&#22312;https://github.com/ntunlp/SummaReranker&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-sequence neural networks have recently achieved great success in abstractive summarization, especially through fine-tuning large pre-trained language models on the downstream dataset. These models are typically decoded with beam search to generate a unique summary. However, the search space is very large, and with the exposure bias, such decoding is not optimal. In this paper, we show that it is possible to directly train a second-stage model performing re-ranking on a set of summary candidates. Our mixture-of-experts SummaReranker learns to select a better candidate and consistently improves the performance of the base model. With a base PEGASUS, we push ROUGE scores by 5.44% on CNN-DailyMail (47.16 ROUGE-1), 1.31% on XSum (48.12 ROUGE-1) and 9.34% on Reddit TIFU (29.83 ROUGE-1), reaching a new state-of-the-art. Our code and checkpoints will be available at https://github.com/ntunlp/SummaReranker.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CrossSum - &#19968;&#20010;1500&#22810;&#31181;&#35821;&#35328;&#23545;&#20013;&#30340;&#36328;&#35821;&#35328;&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#31181;&#22810;&#38454;&#27573;&#25968;&#25454;&#37319;&#26679;&#31639;&#27861;&#21644;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#24230;&#37327;LaSE&#12290;&#35813;&#27169;&#22411;&#22312;&#25688;&#35201;&#29983;&#25104;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#26159;&#30446;&#21069;&#24050;&#30693;&#26368;&#22823;&#30340;&#36328;&#35821;&#35328;&#25688;&#35201;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2112.08804</link><description>&lt;p&gt;
CrossSum&#65306;&#36229;&#36234;&#33521;&#35821;&#20013;&#24515;&#30340;1500&#22810;&#31181;&#35821;&#35328;&#23545;&#30340;&#36328;&#35821;&#35328;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
CrossSum: Beyond English-Centric Cross-Lingual Summarization for 1,500+ Language Pairs. (arXiv:2112.08804v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CrossSum - &#19968;&#20010;1500&#22810;&#31181;&#35821;&#35328;&#23545;&#20013;&#30340;&#36328;&#35821;&#35328;&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#31181;&#22810;&#38454;&#27573;&#25968;&#25454;&#37319;&#26679;&#31639;&#27861;&#21644;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#24230;&#37327;LaSE&#12290;&#35813;&#27169;&#22411;&#22312;&#25688;&#35201;&#29983;&#25104;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#26159;&#30446;&#21069;&#24050;&#30693;&#26368;&#22823;&#30340;&#36328;&#35821;&#35328;&#25688;&#35201;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CrossSum&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;1500&#22810;&#31181;&#35821;&#35328;&#23545;&#20013;168&#19975;&#31687;&#25991;&#31456;-&#25688;&#35201;&#26679;&#26412;&#30340;&#22823;&#35268;&#27169;&#36328;&#35821;&#35328;&#25688;&#35201;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#22810;&#35821;&#35328;&#25277;&#35937;&#27010;&#25324;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#36328;&#35821;&#35328;&#26816;&#32034;&#23545;&#29992;&#19981;&#21516;&#35821;&#35328;&#32534;&#20889;&#30340;&#24179;&#34892;&#25991;&#31456;&#36827;&#34892;&#20102;&#23545;&#40784;&#65292;&#24182;&#36827;&#34892;&#20102;&#21463;&#25511;&#20154;&#24037;&#35780;&#20272;&#20197;&#39564;&#35777;&#20854;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#25968;&#25454;&#37319;&#26679;&#31639;&#27861;&#65292;&#20197;&#26377;&#25928;&#35757;&#32451;&#33021;&#22815;&#25688;&#35201;&#20219;&#20309;&#30446;&#26631;&#35821;&#35328;&#25991;&#31456;&#30340;&#36328;&#35821;&#35328;&#25688;&#35201;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;LaSE&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#30340;&#25688;&#35201;&#12290;LaSE&#19982;ROUGE&#24378;&#30456;&#20851;&#65292;&#24182;&#19988;&#65292;&#19982;ROUGE&#19981;&#21516;&#65292;&#22312;&#30446;&#26631;&#35821;&#35328;&#27809;&#26377;&#21442;&#32771;&#25991;&#29486;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#21487;&#38752;&#22320;&#27979;&#37327;&#12290;ROUGE&#21644;LaSE&#30340;&#34920;&#29616;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25311;&#35758;&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;CrossSum&#26159;&#26368;&#22823;&#30340;&#36328;&#35821;&#35328;&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#20063;&#26159;&#31532;&#19968;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present CrossSum, a large-scale cross-lingual summarization dataset comprising 1.68 million article-summary samples in 1,500+ language pairs. We create CrossSum by aligning parallel articles written in different languages via cross-lingual retrieval from a multilingual abstractive summarization dataset and perform a controlled human evaluation to validate its quality. We propose a multistage data sampling algorithm to effectively train a cross-lingual summarization model capable of summarizing an article in any target language. We also introduce LaSE, an embedding-based metric for automatically evaluating model-generated summaries. LaSE is strongly correlated with ROUGE and, unlike ROUGE, can be reliably measured even in the absence of references in the target language. Performance on ROUGE and LaSE indicate that our proposed model consistently outperforms baseline models. To the best of our knowledge, CrossSum is the largest cross-lingual summarization dataset and the first ever th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20107;&#23454;&#30340;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#23618;&#26041;&#24335;&#28085;&#30422;&#24120;&#35782;&#21644;&#20020;&#26102;&#30693;&#35782;&#32447;&#32034;&#65292;&#36890;&#36807;&#26500;&#24314;&#36229;&#22270;&#23454;&#29616;&#21477;&#23376;&#32423;&#21035;&#21644;&#23454;&#20307;&#32423;&#21035;&#30340;&#20132;&#20114;&#65292;&#22312;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#21644;&#22823;&#35268;&#27169;&#38405;&#35835;&#29702;&#35299;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2105.10334</link><description>&lt;p&gt;
&#22522;&#20110;&#20107;&#23454;&#30340;&#36923;&#36753;&#25512;&#29702;&#19982;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Fact-driven Logical Reasoning for Machine Reading Comprehension. (arXiv:2105.10334v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.10334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20107;&#23454;&#30340;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#23618;&#26041;&#24335;&#28085;&#30422;&#24120;&#35782;&#21644;&#20020;&#26102;&#30693;&#35782;&#32447;&#32034;&#65292;&#36890;&#36807;&#26500;&#24314;&#36229;&#22270;&#23454;&#29616;&#21477;&#23376;&#32423;&#21035;&#21644;&#23454;&#20307;&#32423;&#21035;&#30340;&#20132;&#20114;&#65292;&#22312;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#21644;&#22823;&#35268;&#27169;&#38405;&#35835;&#29702;&#35299;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#20851;&#27880;&#26426;&#22120;&#20855;&#22791;&#25512;&#29702;&#33021;&#21147;&#65292;&#36825;&#38656;&#35201;&#20934;&#30830;&#28165;&#26224;&#22320;&#25552;&#20379;&#32447;&#32034;&#12290;&#22312;&#29616;&#26377;&#30340;&#30740;&#31350;&#20013;&#65292;&#32447;&#32034;&#36890;&#24120;&#34987;&#24314;&#27169;&#20026;&#23454;&#20307;&#24863;&#30693;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23454;&#20307;&#24863;&#30693;&#32447;&#32034;&#20027;&#35201;&#38598;&#20013;&#20110;&#24120;&#35782;&#65292;&#23545;&#20110;&#38656;&#35201;&#20102;&#35299;&#20020;&#26102;&#20107;&#23454;&#25110;&#20107;&#20214;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#38405;&#35835;&#29702;&#35299;&#30340;&#36923;&#36753;&#25512;&#29702;&#20013;&#65292;&#36825;&#20123;&#32447;&#32034;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#26377;&#21160;&#21147;&#20197;&#20998;&#23618;&#26041;&#24335;&#28085;&#30422;&#24120;&#35782;&#21644;&#20020;&#26102;&#30693;&#35782;&#32447;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#21333;&#20803;&#30340;&#36890;&#29992;&#24418;&#24335;&#65292;&#36890;&#36807;&#25552;&#21462;&#21477;&#23376;&#30340;&#39592;&#24178;&#26500;&#25104;&#37096;&#20998;&#65292;&#22914;&#20027;&#35821;-&#35859;&#35821;-&#23486;&#35821;&#24418;&#25104;&#30340;&#8220;&#20107;&#23454;&#8221;&#12290;&#28982;&#21518;&#65292;&#22312;&#20107;&#23454;&#21333;&#20803;&#20043;&#19978;&#26500;&#24314;&#19968;&#20010;&#36229;&#22270;&#65292;&#20801;&#35768;&#21477;&#23376;&#32423;&#21035;&#65288;&#20107;&#23454;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#65289;&#21644;&#23454;&#20307;&#32423;&#21035;&#20132;&#20114;&#65288;&#20107;&#23454;&#20013;&#30340;&#27010;&#24565;&#25110;&#34892;&#21160;&#65289;&#30340;&#20114;&#21160;&#12290;&#22312;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#21644;&#22823;&#35268;&#27169;&#38405;&#35835;&#29702;&#35299;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an increasing interest in training machines with reasoning ability, which deeply relies on accurately and clearly presented clue forms. The clues are usually modeled as entity-aware knowledge in existing studies. However, those entity-aware clues are primarily focused on commonsense, making them insufficient for tasks that require knowledge of temporary facts or events, particularly in logical reasoning for reading comprehension. To address this challenge, we are motivated to cover both commonsense and temporary knowledge clues hierarchically. Specifically, we propose a general formalism of knowledge units by extracting backbone constituents of the sentence, such as the subject-verb-object formed ``facts''. We then construct a supergraph on top of the fact units, allowing for the benefit of sentence-level (relations among fact groups) and entity-level interactions (concepts or actions inside a fact). Experimental results on logical reasoning benchmarks and d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#26080;&#38656;&#20551;&#35774;&#25991;&#26723;&#25110;&#35770;&#25454;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#35770;&#36848;&#25366;&#25496;&#20219;&#21153;&#20013;&#65292;&#25104;&#20026;&#20102;&#19968;&#31181;&#26082;&#36890;&#29992;&#21448;&#39640;&#24615;&#33021;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2102.12227</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#29992;&#20110;&#35770;&#36848;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Attentive Residual Networks for Argument Mining. (arXiv:2102.12227v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.12227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#26080;&#38656;&#20551;&#35774;&#25991;&#26723;&#25110;&#35770;&#25454;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#35770;&#36848;&#25366;&#25496;&#20219;&#21153;&#20013;&#65292;&#25104;&#20026;&#20102;&#19968;&#31181;&#26082;&#36890;&#29992;&#21448;&#39640;&#24615;&#33021;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#22312;&#22810;&#20010;&#35770;&#36848;&#25366;&#25496;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27531;&#24046;&#26550;&#26500;&#65292;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#19981;&#23545;&#25991;&#26723;&#25110;&#35770;&#25454;&#32467;&#26500;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#19981;&#21516;&#30340;&#29992;&#25143;&#29983;&#25104;&#35780;&#35770;&#12289;&#31185;&#23398;&#20986;&#29256;&#29289;&#21644;&#21149;&#35828;&#24615;&#35770;&#25991;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38024;&#23545;&#20855;&#26377;&#26356;&#39640;&#35745;&#31639;&#21360;&#35760;&#25110;&#29305;&#23450;&#20110;&#35821;&#26009;&#24211;&#35774;&#35745;&#30340;&#26368;&#20808;&#36827;&#26550;&#26500;&#30340;&#24378;&#26377;&#21147;&#30340;&#31454;&#20105;&#23545;&#25163;&#65292;&#20195;&#34920;&#20102;&#36890;&#29992;&#24615;&#12289;&#24615;&#33021;&#31934;&#24230;&#21644;&#20943;&#23569;&#27169;&#22411;&#22823;&#23567;&#20043;&#38388;&#30340;&#26377;&#36259;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the use of residual networks and neural attention for multiple argument mining tasks. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble, without any assumption on document or argument structure. We present an extensive experimental evaluation on five different corpora of user-generated comments, scientific publications, and persuasive essays. Our results show that our approach is a strong competitor against state-of-the-art architectures with a higher computational footprint or corpus-specific design, representing an interesting compromise between generality, performance accuracy and reduced model size.
&lt;/p&gt;</description></item></channel></rss>