<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21477;&#23376;&#20869;&#37096;&#24773;&#24863;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#35757;&#32451;&#20102;&#39044;&#27979;&#22120;&#26469;&#20998;&#26512;&#21477;&#23376;&#30340;&#24773;&#24863;&#20998;&#24067;&#65292;&#24182;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#26222;&#36890;&#30340;&#36830;&#25509;&#35789;&#20063;&#21487;&#20197;&#26174;&#33879;&#25913;&#21464;&#35805;&#35821;&#30340;&#24773;&#24863;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2307.01784</link><description>&lt;p&gt;
&#19968;&#20010;&#24605;&#24819;&#30340;&#20869;&#22312;&#24773;&#24863;
&lt;/p&gt;
&lt;p&gt;
The Inner Sentiments of a Thought. (arXiv:2307.01784v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01784
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21477;&#23376;&#20869;&#37096;&#24773;&#24863;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#35757;&#32451;&#20102;&#39044;&#27979;&#22120;&#26469;&#20998;&#26512;&#21477;&#23376;&#30340;&#24773;&#24863;&#20998;&#24067;&#65292;&#24182;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#26222;&#36890;&#30340;&#36830;&#25509;&#35789;&#20063;&#21487;&#20197;&#26174;&#33879;&#25913;&#21464;&#35805;&#35821;&#30340;&#24773;&#24863;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#25991;&#26412;&#12290;&#23427;&#20204;&#33021;&#22815;&#34920;&#36798;&#24182;&#33267;&#23569;&#26263;&#31034;&#20986;&#19968;&#31995;&#21015;&#24773;&#24863;&#21644;&#33394;&#24425;&#65292;&#20174;&#26126;&#26174;&#30340;&#20215;&#20540;&#21644;&#21796;&#36215;&#21040;&#24494;&#22937;&#30340;&#20915;&#24515;&#21644;&#36190;&#36175;&#12290;&#25105;&#20204;&#39318;&#27425;&#25506;&#32034;&#20102;&#36825;&#20123;&#34920;&#31034;&#21450;&#20854;&#22914;&#20309;&#29992;&#20110;&#29702;&#35299;&#21333;&#20010;&#21477;&#23376;&#20869;&#37096;&#30340;&#24773;&#24863;&#36816;&#20316;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#20174;&#22686;&#38271;&#38271;&#24230;&#30340;&#21069;&#32512;&#20013;&#24212;&#29992;&#21040;LLM&#30340;&#38544;&#34255;&#34920;&#31034;&#30340;&#21477;&#23376;&#30340;&#26368;&#32456;&#24773;&#24863;&#30340;&#20998;&#24067;&#30340;&#23450;&#37327;&#39044;&#27979;&#22120;&#12290;&#22312;&#23637;&#31034;&#20102;&#20215;&#20540;&#12289;&#20915;&#24515;&#12289;&#36190;&#36175;&#12289;&#28966;&#34385;&#21644;&#28902;&#24700;&#30340;&#20998;&#24067;&#39044;&#27979;&#22120;&#26159;&#33391;&#22909;&#26657;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#39044;&#27979;&#22120;&#20998;&#26512;&#21477;&#23376;&#30340;&#31034;&#20363;&#65292;&#20363;&#22914;&#65292;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#26222;&#36890;&#30340;&#36830;&#25509;&#35789;&#65288;&#20363;&#22914;&#65292;&#8220;&#20294;&#26159;&#8221;&#65289;&#20063;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#21464;&#35805;&#35821;&#30340;&#24773;&#24863;&#36712;&#36857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#39044;&#27979;&#22120;&#26469;&#21033;&#29992;&#20998;&#24067;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based large-scale language models (LLMs) are able to generate highly realistic text. They are duly able to express, and at least implicitly represent, a wide range of sentiments and color, from the obvious, such as valence and arousal to the subtle, such as determination and admiration. We provide a first exploration of these representations and how they can be used for understanding the inner sentimental workings of single sentences. We train predictors of the quantiles of the distributions of final sentiments of sentences from the hidden representations of an LLM applied to prefixes of increasing lengths. After showing that predictors of distributions of valence, determination, admiration, anxiety and annoyance are well calibrated, we provide examples of using these predictors for analyzing sentences, illustrating, for instance, how even ordinary conjunctions (e.g., "but") can dramatically alter the emotional trajectory of an utterance. We then show how to exploit the dis
&lt;/p&gt;</description></item><item><title>&#22312;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24863;&#30693;&#30340;&#38899;&#39057;&#24341;&#23548;&#29983;&#25104;&#24335;&#25554;&#27133;&#22635;&#20805;&#26041;&#27861;&#65292;KA2G&#65292;&#23427;&#36890;&#36807;&#23558;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19982;&#38899;&#39057;&#24418;&#24335;&#30456;&#32467;&#21512;&#65292;&#24182;&#20381;&#36182;&#20110;&#22806;&#37096;&#30693;&#35782;&#26469;&#23454;&#29616;&#23545;&#35821;&#38899;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#31283;&#20581;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#25554;&#27133;&#22635;&#20805;&#12290;</title><link>http://arxiv.org/abs/2307.01764</link><description>&lt;p&gt;
&#26377;&#38480;&#24050;&#26631;&#27880;&#25968;&#25454;&#19979;&#22522;&#20110;&#30693;&#35782;&#24863;&#30693;&#30340;&#38899;&#39057;&#24341;&#23548;&#29983;&#25104;&#24335;&#25554;&#27133;&#22635;&#20805;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Aware Audio-Grounded Generative Slot Filling for Limited Annotated Data. (arXiv:2307.01764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01764
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24863;&#30693;&#30340;&#38899;&#39057;&#24341;&#23548;&#29983;&#25104;&#24335;&#25554;&#27133;&#22635;&#20805;&#26041;&#27861;&#65292;KA2G&#65292;&#23427;&#36890;&#36807;&#23558;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19982;&#38899;&#39057;&#24418;&#24335;&#30456;&#32467;&#21512;&#65292;&#24182;&#20381;&#36182;&#20110;&#22806;&#37096;&#30693;&#35782;&#26469;&#23454;&#29616;&#23545;&#35821;&#38899;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#31283;&#20581;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#25554;&#27133;&#22635;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#25163;&#21160;&#27880;&#37322;&#32454;&#31890;&#24230;&#25554;&#27133;-&#20540;&#26631;&#31614;&#26159;&#19968;&#39033;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#24037;&#20316;&#12290;&#36825;&#20419;&#20351;&#30740;&#31350;&#20165;&#20351;&#29992;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#25554;&#27133;&#22635;&#20805;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#20851;&#20110;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#20165;&#22522;&#20110;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#24418;&#24335;&#65292;&#24573;&#35270;&#20102;&#22312;&#22788;&#29702;&#21475;&#35821;&#26102;&#30340;&#19981;&#23436;&#32654;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#25152;&#24102;&#26469;&#30340;&#39069;&#22806;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KA2G&#30340;&#22522;&#20110;&#30693;&#35782;&#24863;&#30693;&#30340;&#38899;&#39057;&#24341;&#23548;&#29983;&#25104;&#24335;&#25554;&#27133;&#22635;&#20805;&#26694;&#26550;&#65292;&#19987;&#27880;&#20110;&#26377;&#35821;&#38899;&#36755;&#20837;&#30340;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#12290;KA2G&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#23454;&#29616;&#20102;&#35821;&#38899;&#23548;&#21521;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#31283;&#20581;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#25554;&#27133;&#22635;&#20805;&#65306;1&#65289;&#23558;&#20854;&#26500;&#36896;&#20026;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;2&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#39069;&#22806;&#20351;&#29992;&#38899;&#39057;&#24418;&#24335;&#65292;3&#65289;&#22312;&#22806;&#37096;&#21487;&#29992;&#30340;&#30693;&#35782;&#26465;&#20214;&#19979;&#65288;&#20363;&#22914;&#65292;&#39044;&#23450;&#20041;&#30340;&#21487;&#33021;&#25554;&#27133;&#20540;&#21015;&#34920;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;KA2G&#26694;&#26550;&#20869;&#32467;&#21512;&#20004;&#31181;&#24418;&#24335;&#21487;&#20197;&#25913;&#21892;&#25554;&#27133;&#22635;&#20805;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manually annotating fine-grained slot-value labels for task-oriented dialogue (ToD) systems is an expensive and time-consuming endeavour. This motivates research into slot-filling methods that operate with limited amounts of labelled data. Moreover, the majority of current work on ToD is based solely on text as the input modality, neglecting the additional challenges of imperfect automatic speech recognition (ASR) when working with spoken language. In this work, we propose a Knowledge-Aware Audio-Grounded generative slot-filling framework, termed KA2G, that focuses on few-shot and zero-shot slot filling for ToD with speech input. KA2G achieves robust and data-efficient slot filling for speech-based ToD by 1) framing it as a text generation task, 2) grounding text generation additionally in the audio modality, and 3) conditioning on available external knowledge (e.g. a predefined list of possible slot values). We show that combining both modalities within the KA2G framework improves the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25554;&#20837;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;CTC&#27169;&#22411;&#20013;&#30340;&#25152;&#38656;&#23646;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#34917;&#20805;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#26469;&#20248;&#20808;&#32771;&#34385;&#31526;&#21512;&#25152;&#38656;&#23646;&#24615;&#30340;&#23545;&#40784;&#65292;&#24182;&#19981;&#38656;&#35201;&#20462;&#25913;CTC&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.01715</link><description>&lt;p&gt;
&#31526;&#21512;&#30446;&#26631;&#65306;&#20351;&#29992;&#36890;&#29992;&#30340;&#25554;&#20837;&#24335;&#26694;&#26550;&#22312;CTC&#27169;&#22411;&#20013;&#20248;&#21270;&#25152;&#38656;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework. (arXiv:2307.01715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25554;&#20837;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;CTC&#27169;&#22411;&#20013;&#30340;&#25152;&#38656;&#23646;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#34917;&#20805;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#26469;&#20248;&#20808;&#32771;&#34385;&#31526;&#21512;&#25152;&#38656;&#23646;&#24615;&#30340;&#23545;&#40784;&#65292;&#24182;&#19981;&#38656;&#35201;&#20462;&#25913;CTC&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#26159;&#35757;&#32451;&#30417;&#30563;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#24191;&#27867;&#20351;&#29992;&#30340;&#20934;&#21017;&#12290;&#23427;&#36890;&#36807;&#23558;&#23436;&#32654;&#23545;&#40784;&#65288;&#20135;&#29983;&#22522;&#26412;&#20107;&#23454;&#65289;&#30340;&#36793;&#38469;&#21270;&#26469;&#23398;&#20064;&#36755;&#20837;&#21644;&#36755;&#20986;&#24207;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#31216;&#20026;&#23545;&#20854;&#65292;&#20197;&#20195;&#20215;&#19981;&#23436;&#32654;&#23545;&#40784;&#12290;&#36825;&#31181;&#23545;&#23436;&#32654;&#21644;&#19981;&#23436;&#32654;&#23545;&#40784;&#30340;&#20108;&#20803;&#21306;&#20998;&#26080;&#27861;&#25429;&#25417;&#21040;&#22312;&#20854;&#20182;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#20854;&#20182;&#20851;&#38190;&#23545;&#40784;&#23646;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{Align With Purpose}$&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;CTC&#26465;&#20214;&#19979;&#35757;&#32451;&#27169;&#22411;&#20013;&#25152;&#38656;&#23646;&#24615;&#30340;$\textbf{&#36890;&#29992;&#25554;&#20837;&#24335;&#26694;&#26550;}$&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#26469;&#34917;&#20805;CTC&#26469;&#20248;&#20808;&#32771;&#34385;&#31526;&#21512;&#25152;&#38656;&#23646;&#24615;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#24178;&#39044;CTC&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#36731;&#26494;&#20248;&#21270;&#21508;&#31181;&#23646;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#21306;&#20998;&#23436;&#32654;&#21644;&#19981;&#23436;&#32654;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Connectionist Temporal Classification (CTC) is a widely used criterion for training supervised sequence-to-sequence (seq2seq) models. It enables learning the relations between input and output sequences, termed alignments, by marginalizing over perfect alignments (that yield the ground truth), at the expense of imperfect alignments. This binary differentiation of perfect and imperfect alignments falls short of capturing other essential alignment properties that hold significance in other real-world applications. Here we propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play framework}$ for enhancing a desired property in models trained with the CTC criterion. We do that by complementing the CTC with an additional loss term that prioritizes alignments according to a desired property. Our method does not require any intervention in the CTC loss function, enables easy optimization of a variety of properties, and allows differentiation between both perfect and imperfect al
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CSProm-KG&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#36719;&#25552;&#31034;&#23454;&#29616;&#32467;&#26500;&#19982;&#25991;&#26412;&#30340;&#26377;&#25928;&#34701;&#21512;&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01709</link><description>&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#36719;&#25552;&#31034;&#65292;&#23558;&#32467;&#26500;&#21644;&#25991;&#26412;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting. (arXiv:2307.01709v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01709
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CSProm-KG&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#36719;&#25552;&#31034;&#23454;&#29616;&#32467;&#26500;&#19982;&#25991;&#26412;&#30340;&#26377;&#25928;&#34701;&#21512;&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#36890;&#24120;&#38656;&#35201;&#32467;&#26500;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#20849;&#21516;&#20316;&#29992;&#12290;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(PLMs)&#24050;&#34987;&#29992;&#20110;&#23398;&#20064;&#25991;&#26412;&#20449;&#24687;&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#20013;&#36890;&#24120;&#37319;&#29992;&#31934;&#35843;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#31934;&#35843;&#21518;&#30340;PLMs&#24448;&#24448;&#36807;&#20110;&#27880;&#37325;&#25991;&#26412;&#20449;&#24687;&#65292;&#24573;&#35270;&#20102;&#32467;&#26500;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;CSProm-KG(&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26465;&#20214;&#36719;&#25552;&#31034;)&#65292;&#22312;&#32467;&#26500;&#20449;&#24687;&#21644;&#25991;&#26412;&#30693;&#35782;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#12290;CSProm-KG&#21482;&#35843;&#25972;&#30001;&#23454;&#20307;&#21644;&#20851;&#31995;&#34920;&#31034;&#29983;&#25104;&#30340;&#26465;&#20214;&#36719;&#25552;&#31034;&#21442;&#25968;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;CSProm-KG&#22312;&#19977;&#20010;&#27969;&#34892;&#30340;&#38745;&#24577;KGC&#22522;&#20934;WN18RR&#12289;FB15K-237&#21644;Wikidata5M&#65292;&#20197;&#21450;&#20004;&#20010;&#26102;&#24577;KGC&#22522;&#20934;ICEWS14&#21644;ICEWS05-15&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;CSProm-KG&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#31454;&#20105;&#24615;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20248;&#27700;&#24179;&#12290;&#25105;&#20204;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Completion (KGC) often requires both KG structural and textual information to be effective. Pre-trained Language Models (PLMs) have been used to learn the textual information, usually under the fine-tune paradigm for the KGC task. However, the fine-tuned PLMs often overwhelmingly focus on the textual information and overlook structural knowledge. To tackle this issue, this paper proposes CSProm-KG (Conditional Soft Prompts for KGC) which maintains a balance between structural information and textual knowledge. CSProm-KG only tunes the parameters of Conditional Soft Prompts that are generated by the entities and relations representations. We verify the effectiveness of CSProm-KG on three popular static KGC benchmarks WN18RR, FB15K-237 and Wikidata5M, and two temporal KGC benchmarks ICEWS14 and ICEWS05-15. CSProm-KG outperforms competitive baseline models and sets new state-of-the-art on these benchmarks. We conduct further analysis to show (i) the effectiveness of our pr
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#32654;&#22269;&#27861;&#24459;&#25991;&#20070;&#20013;&#23384;&#22312;&#24191;&#27867;&#30340;&#31181;&#26063;&#20559;&#35265;&#65292;&#20256;&#32479;&#19978;&#19982;&#40657;&#20154;&#30456;&#20851;&#30340;&#21517;&#23383;&#26356;&#19982;&#8220;&#19981;&#24841;&#24555;&#8221;&#26415;&#35821;&#30456;&#20851;&#65292;&#32780;&#19982;&#30333;&#20154;&#30456;&#20851;&#30340;&#21517;&#23383;&#21017;&#26356;&#19982;&#8220;&#24841;&#24555;&#8221;&#26415;&#35821;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#24182;&#26410;&#21457;&#29616;1950&#24180;&#21069;&#30340;&#27861;&#24459;&#24847;&#35265;&#23384;&#22312;&#26356;&#39640;&#31243;&#24230;&#30340;&#20559;&#35265;&#65292;&#20063;&#26410;&#21457;&#29616;&#21335;&#26041;&#24030;&#30340;&#24847;&#35265;&#27604;&#19996;&#21271;&#24030;&#30340;&#24847;&#35265;&#22312;&#31181;&#26063;&#20559;&#35265;&#26041;&#38754;&#21464;&#21270;&#36739;&#23567;&#30340;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2307.01693</link><description>&lt;p&gt;
&#32654;&#22269;&#27861;&#24459;&#25991;&#20070;&#20013;&#30340;&#31181;&#26063;&#20559;&#35265;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Racial Bias Trends in the Text of US Legal Opinions. (arXiv:2307.01693v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01693
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#32654;&#22269;&#27861;&#24459;&#25991;&#20070;&#20013;&#23384;&#22312;&#24191;&#27867;&#30340;&#31181;&#26063;&#20559;&#35265;&#65292;&#20256;&#32479;&#19978;&#19982;&#40657;&#20154;&#30456;&#20851;&#30340;&#21517;&#23383;&#26356;&#19982;&#8220;&#19981;&#24841;&#24555;&#8221;&#26415;&#35821;&#30456;&#20851;&#65292;&#32780;&#19982;&#30333;&#20154;&#30456;&#20851;&#30340;&#21517;&#23383;&#21017;&#26356;&#19982;&#8220;&#24841;&#24555;&#8221;&#26415;&#35821;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#24182;&#26410;&#21457;&#29616;1950&#24180;&#21069;&#30340;&#27861;&#24459;&#24847;&#35265;&#23384;&#22312;&#26356;&#39640;&#31243;&#24230;&#30340;&#20559;&#35265;&#65292;&#20063;&#26410;&#21457;&#29616;&#21335;&#26041;&#24030;&#30340;&#24847;&#35265;&#27604;&#19996;&#21271;&#24030;&#30340;&#24847;&#35265;&#22312;&#31181;&#26063;&#20559;&#35265;&#26041;&#38754;&#21464;&#21270;&#36739;&#23567;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24191;&#27867;&#35748;&#35782;&#21040;&#32654;&#22269;&#27861;&#24459;&#20013;&#23384;&#22312;&#31181;&#26063;&#20559;&#35265;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#31181;&#20559;&#35265;&#22312;&#27861;&#24459;&#35821;&#35328;&#20013;&#30340;&#34920;&#29616;&#26041;&#24335;&#65292;&#21363;&#21496;&#27861;&#24847;&#35265;&#20013;&#26159;&#21542;&#23384;&#22312;&#26102;&#26399;&#25110;&#22320;&#21306;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#30340;&#38544;&#24615;&#31181;&#26063;&#20559;&#35265;&#27979;&#37327;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#23545;1860&#24180;&#33267;2009&#24180;&#30340;600&#22810;&#19975;&#20010;&#32654;&#22269;&#32852;&#37030;&#21644;&#24030;&#32423;&#27861;&#38498;&#26696;&#20214;&#36827;&#34892;&#20102;GloVe&#35789;&#21521;&#37327;&#20272;&#31639;&#12290;&#25105;&#20204;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#22320;&#21306;&#21644;&#26102;&#26399;&#37117;&#23384;&#22312;&#26126;&#26174;&#30340;&#31181;&#26063;&#20559;&#35265;&#65292;&#20256;&#32479;&#19978;&#19982;&#40657;&#20154;&#30456;&#20851;&#30340;&#21517;&#23383;&#26356;&#19982;&#39044;&#20998;&#31867;&#30340;&#8220;&#19981;&#24841;&#24555;&#8221;&#26415;&#35821;&#23494;&#20999;&#30456;&#20851;&#65292;&#32780;&#20256;&#32479;&#19978;&#19982;&#30333;&#20154;&#30456;&#20851;&#30340;&#21517;&#23383;&#26356;&#19982;&#39044;&#20998;&#31867;&#30340;&#8220;&#24841;&#24555;&#8221;&#26415;&#35821;&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#36824;&#27979;&#35797;&#20102;1950&#24180;&#21069;&#30340;&#27861;&#24459;&#24847;&#35265;&#26159;&#21542;&#27604;1950&#24180;&#21518;&#30340;&#24847;&#35265;&#26356;&#20855;&#38544;&#24615;&#31181;&#26063;&#20559;&#35265;&#65292;&#20197;&#21450;&#21335;&#26041;&#24030;&#30340;&#24847;&#35265;&#26159;&#21542;&#27604;&#19996;&#21271;&#24030;&#30340;&#24847;&#35265;&#22312;&#31181;&#26063;&#20559;&#35265;&#26041;&#38754;&#21464;&#21270;&#36739;&#23567;&#12290;&#25105;&#20204;&#24182;&#26410;&#21457;&#29616;1950&#24180;&#21069;&#30340;&#27861;&#24459;&#24847;&#35265;&#23384;&#22312;&#26356;&#39640;&#31243;&#24230;&#30340;&#20559;&#35265;&#65292;&#20063;&#26410;&#21457;&#29616;&#21335;&#26041;&#24030;&#30340;&#24847;&#35265;&#27604;&#19996;&#21271;&#24030;&#30340;&#24847;&#35265;&#22312;&#31181;&#26063;&#20559;&#35265;&#26041;&#38754;&#21464;&#21270;&#36739;&#23567;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although there is widespread recognition of racial bias in US law, it is unclear how such bias appears in the language of law, namely judicial opinions, and whether it varies across time period or region. Building upon approaches for measuring implicit racial bias in large-scale corpora, we approximate GloVe word embeddings for over 6 million US federal and state court cases from 1860 to 2009. We find strong evidence of racial bias across nearly all regions and time periods, as traditionally Black names are more closely associated with pre-classified "unpleasant" terms whereas traditionally White names are more closely associated with pre-classified "pleasant" terms. We also test whether legal opinions before 1950 exhibit more implicit racial bias than those after 1950, as well as whether opinions from Southern states exhibit less change in racial bias than those from Northeastern states. We do not find evidence of elevated bias in legal opinions before 1950, or evidence that legal opi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36328;&#25968;&#25454;&#38598;&#27604;&#36739;&#21457;&#29616;&#65292;&#32467;&#21512;&#22810;&#20010;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#25968;&#25454;&#38598;&#21487;&#20197;&#24320;&#21457;&#20986;&#40065;&#26834;&#30340;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#25511;&#21046;&#25968;&#25454;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2307.01680</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#20013;&#23545;&#24694;&#24847;&#35328;&#35770;&#30340;&#40065;&#26834;&#24615;&#26816;&#27979;&#65306;&#36328;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation. (arXiv:2307.01680v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36328;&#25968;&#25454;&#38598;&#27604;&#36739;&#21457;&#29616;&#65292;&#32467;&#21512;&#22810;&#20010;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#25968;&#25454;&#38598;&#21487;&#20197;&#24320;&#21457;&#20986;&#40065;&#26834;&#30340;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#25511;&#21046;&#25968;&#25454;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#23545;&#22312;&#32447;&#24694;&#24847;&#35328;&#35770;&#30340;&#33258;&#21160;&#26816;&#27979;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26377;&#21161;&#20110;&#35757;&#32451;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#21019;&#24314;&#36807;&#31243;&#20013;&#23384;&#22312;&#33258;&#36523;&#30340;&#20559;&#35265;&#65292;&#24182;&#19988;&#27169;&#22411;&#22312;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#19979;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#36328;&#25968;&#25454;&#38598;&#27604;&#36739;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#36825;&#39033;&#20998;&#26512;&#26174;&#31034;&#20102;&#24403;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#19968;&#20123;&#25968;&#25454;&#38598;&#27604;&#20854;&#20182;&#25968;&#25454;&#38598;&#26356;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#32467;&#21512;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#25968;&#25454;&#38598;&#21487;&#20197;&#26377;&#21161;&#20110;&#24320;&#21457;&#20986;&#40065;&#26834;&#30340;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#12290;&#36825;&#31181;&#40065;&#26834;&#24615;&#29978;&#33267;&#22312;&#25511;&#21046;&#25968;&#25454;&#22823;&#23567;&#12289;&#19982;&#26368;&#20339;&#21333;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#26102;&#20063;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automatic detection of hate speech online is an active research area in NLP. Most of the studies to date are based on social media datasets that contribute to the creation of hate speech detection models trained on them. However, data creation processes contain their own biases, and models inherently learn from these dataset-specific biases. In this paper, we perform a large-scale cross-dataset comparison where we fine-tune language models on different hate speech detection datasets. This analysis shows how some datasets are more generalisable than others when used as training data. Crucially, our experiments show how combining hate speech detection datasets can contribute to the development of robust hate speech detection models. This robustness holds even when controlling by data size and compared with the best individual datasets.
&lt;/p&gt;</description></item><item><title>&#22312;&#26080;&#26465;&#20214;&#35821;&#38899;&#21512;&#25104;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ASGAN&#30340;&#35299;&#32806;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20511;&#37492;&#20102;StyleGAN&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20123;&#26032;&#25216;&#26415;&#12290;&#36890;&#36807;&#23398;&#20064;&#35299;&#32806;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;ASGAN&#33021;&#22815;&#20174;&#28508;&#22312;&#31354;&#38388;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#35821;&#38899;&#65292;&#21363;&#20351;&#22312;&#23567;&#23383;&#20856;&#25968;&#25454;&#38598;&#19978;&#20063;&#33021;&#21462;&#24471;&#26368;&#26032;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.01673</link><description>&lt;p&gt;
&#26080;&#26465;&#20214;&#35821;&#38899;&#21512;&#25104;&#20013;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#35299;&#32806;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Disentanglement in a GAN for Unconditional Speech Synthesis. (arXiv:2307.01673v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01673
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#26465;&#20214;&#35821;&#38899;&#21512;&#25104;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ASGAN&#30340;&#35299;&#32806;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20511;&#37492;&#20102;StyleGAN&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20123;&#26032;&#25216;&#26415;&#12290;&#36890;&#36807;&#23398;&#20064;&#35299;&#32806;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;ASGAN&#33021;&#22815;&#20174;&#28508;&#22312;&#31354;&#38388;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#35821;&#38899;&#65292;&#21363;&#20351;&#22312;&#23567;&#23383;&#20856;&#25968;&#25454;&#38598;&#19978;&#20063;&#33021;&#21462;&#24471;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#24320;&#21457;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#30452;&#25509;&#20174;&#28508;&#22312;&#31354;&#38388;&#21512;&#25104;&#36924;&#30495;&#30340;&#35821;&#38899;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#26465;&#20214;&#65311;&#23613;&#31649;&#36807;&#21435;&#21313;&#24180;&#20013;&#36827;&#34892;&#20102;&#20960;&#27425;&#23581;&#35797;&#65292;&#20043;&#21069;&#30340;&#23545;&#25239;&#24615;&#21644;&#25193;&#25955;&#24615;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#65292;&#21363;&#20351;&#22312;&#23567;&#23383;&#20856;&#25968;&#25454;&#38598;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AudioStyleGAN(ASGAN)&#8212;&#8212;&#19968;&#31181;&#29992;&#20110;&#26080;&#26465;&#20214;&#35821;&#38899;&#21512;&#25104;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#35299;&#32806;&#28508;&#22312;&#31354;&#38388;&#12290;&#22312;StyleGAN&#31995;&#21015;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;ASGAN&#65292;&#23427;&#23558;&#37319;&#26679;&#22122;&#22768;&#26144;&#23556;&#21040;&#19968;&#20010;&#35299;&#32806;&#28508;&#22312;&#21521;&#37327;&#65292;&#28982;&#21518;&#23558;&#20854;&#26144;&#23556;&#21040;&#19968;&#20010;&#38899;&#39057;&#29305;&#24449;&#24207;&#21015;&#65292;&#20197;&#22312;&#27599;&#19968;&#23618;&#20013;&#25233;&#21046;&#20449;&#21495;&#28151;&#21472;&#12290;&#20026;&#20102;&#25104;&#21151;&#35757;&#32451;ASGAN&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20123;&#26032;&#25216;&#26415;&#65292;&#21253;&#25324;&#23545;&#33258;&#36866;&#24212;&#37492;&#21035;&#22120;&#22686;&#24378;&#30340;&#20462;&#25913;&#65292;&#20351;&#20854;&#20197;&#27010;&#29575;&#26041;&#24335;&#36339;&#36807;&#37492;&#21035;&#22120;&#26356;&#26032;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#23567;&#23383;&#20856;&#30340;&#35895;&#27468;&#35821;&#38899;&#21629;&#20196;&#25968;&#23383;&#25968;&#25454;&#38598;&#19978;&#65292;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26080;&#26465;&#20214;&#35821;&#38899;&#21512;&#25104;&#26041;&#38754;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can we develop a model that can synthesize realistic speech directly from a latent space, without explicit conditioning? Despite several efforts over the last decade, previous adversarial and diffusion-based approaches still struggle to achieve this, even on small-vocabulary datasets. To address this, we propose AudioStyleGAN (ASGAN) -- a generative adversarial network for unconditional speech synthesis tailored to learn a disentangled latent space. Building upon the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer. To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation which probabilistically skips discriminator updates. We apply it on the small-vocabulary Google Speech Commands digits dataset, where it achieves state-of-the-art results in unconditional
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25386;&#23041;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#25386;&#23041;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#35752;&#35770;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#25386;&#23041;&#35821;ASR&#27169;&#22411;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.01672</link><description>&lt;p&gt;
&#25552;&#21319;&#25386;&#23041;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Boosting Norwegian Automatic Speech Recognition. (arXiv:2307.01672v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25386;&#23041;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#25386;&#23041;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#35752;&#35770;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#25386;&#23041;&#35821;ASR&#27169;&#22411;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20960;&#31181;&#38754;&#21521;&#25386;&#23041;&#20004;&#31181;&#23448;&#26041;&#20070;&#38754;&#35821;&#35328;&#65288;Bokm&#229;l&#21644;Nynorsk&#65289;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25386;&#23041;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#20043;&#21069;&#30340;&#26368;&#26032;ASR&#27169;&#22411;&#20197;&#21450;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#12290;&#25105;&#20204;&#23558;&#22312;&#25386;&#23041;&#35758;&#20250;&#28436;&#35762;&#35821;&#26009;&#24211;&#65288;NPSC&#65289;&#19978;&#30340;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20174;17.10&#65285;&#38477;&#20302;&#21040;7.60&#65285;&#65292;Bokm&#229;l&#21644;Nynorsk&#20998;&#21035;&#36798;&#21040;5.81&#65285;&#21644;11.54&#65285;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#25386;&#23041;&#35821;ASR&#27169;&#22411;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present several baselines for automatic speech recognition (ASR) models for the two official written languages in Norway: Bokm{\aa}l and Nynorsk. We compare the performance of models of varying sizes and pre-training approaches on multiple Norwegian speech datasets. Additionally, we measure the performance of these models against previous state-of-the-art ASR models, as well as on out-of-domain datasets. We improve the state of the art on the Norwegian Parliamentary Speech Corpus (NPSC) from a word error rate (WER) of 17.10\% to 7.60\%, with models achieving 5.81\% for Bokm{\aa}l and 11.54\% for Nynorsk. We also discuss the challenges and potential solutions for further improving ASR models for Norwegian.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32479;&#19968;&#23545;&#35805;&#27169;&#22411;&#20013;&#31995;&#32479;&#21457;&#36215;&#30340;&#38386;&#32842;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20043;&#38388;&#30340;&#36716;&#25442;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#25552;&#31034;&#27169;&#22411;&#26469;&#20027;&#21160;&#35302;&#21457;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2307.01664</link><description>&lt;p&gt;
&#20855;&#26377;&#31995;&#32479;&#21457;&#36215;&#30340;&#38386;&#32842;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20043;&#38388;&#36716;&#25442;&#30340;&#32479;&#19968;&#23545;&#35805;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unified Conversational Models with System-Initiated Transitions between Chit-Chat and Task-Oriented Dialogues. (arXiv:2307.01664v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32479;&#19968;&#23545;&#35805;&#27169;&#22411;&#20013;&#31995;&#32479;&#21457;&#36215;&#30340;&#38386;&#32842;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20043;&#38388;&#30340;&#36716;&#25442;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#25552;&#31034;&#27169;&#22411;&#26469;&#20027;&#21160;&#35302;&#21457;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25506;&#32034;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#38386;&#32842;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#32479;&#19968;&#23545;&#35805;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#35805;&#27169;&#24335;&#20043;&#38388;&#21457;&#29983;&#21464;&#21270;&#26102;&#30340;&#8220;&#20027;&#21160;&#24615;&#8221;&#28508;&#21147;&#24456;&#23569;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20004;&#31181;&#23545;&#35805;&#22330;&#26223;&#65292;&#19968;&#31181;&#26159;&#20174;&#38386;&#32842;&#24320;&#22987;&#65292;&#38544;&#21547;&#22320;&#28041;&#21450;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#24182;&#26368;&#32456;&#36716;&#20026;&#20219;&#21153;&#23548;&#21521;&#30340;&#35831;&#27714;&#65307;&#21478;&#19968;&#31181;&#26159;&#20174;&#20219;&#21153;&#23548;&#21521;&#30340;&#20114;&#21160;&#24320;&#22987;&#65292;&#26368;&#32456;&#22312;&#25552;&#20379;&#25152;&#26377;&#35831;&#27714;&#30340;&#20449;&#24687;&#21518;&#36716;&#20026;&#38543;&#24847;&#32842;&#22825;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#20004;&#31181;&#39640;&#25928;&#30340;&#25552;&#31034;&#27169;&#22411;&#65292;&#21487;&#20197;&#20027;&#21160;&#29983;&#25104;&#19968;&#20010;&#36807;&#28193;&#21477;&#26469;&#35302;&#21457;&#31995;&#32479;&#21457;&#36215;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken dialogue systems (SDSs) have been separately developed under two different categories, task-oriented and chit-chat. The former focuses on achieving functional goals and the latter aims at creating engaging social conversations without special goals. Creating a unified conversational model that can engage in both chit-chat and task-oriented dialogue is a promising research topic in recent years. However, the potential ``initiative'' that occurs when there is a change between dialogue modes in one dialogue has rarely been explored. In this work, we investigate two kinds of dialogue scenarios, one starts from chit-chat implicitly involving task-related topics and finally switching to task-oriented requests; the other starts from task-oriented interaction and eventually changes to casual chat after all requested information is provided. We contribute two efficient prompt models which can proactively generate a transition sentence to trigger system-initiated transitions in a unified 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29305;&#23450;&#24037;&#20855;&#25903;&#25345;&#19979;&#30340;&#23545;&#35805;&#24335;&#26234;&#33021;&#21161;&#25163;&#30340;&#25554;&#20837;&#25193;&#23637;&#12290;&#36890;&#36807;&#23558;&#29992;&#25143;&#20316;&#20026;&#24037;&#20855;&#65292;&#22312;&#29983;&#25104;&#26126;&#30830;&#25512;&#29702;&#36335;&#24452;&#26102;&#25552;&#20379;&#26356;&#22810;&#32454;&#33410;&#21644;&#26356;&#31934;&#30830;&#30340;&#35831;&#27714;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24037;&#20855;&#24178;&#25200;&#29992;&#25143;&#24847;&#22270;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20004;&#20010;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#25512;&#33616;&#39046;&#22495;&#20013;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#24102;&#26469;&#20102;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.01644</link><description>&lt;p&gt;
&#29305;&#23450;&#24037;&#20855;&#25903;&#25345;&#19979;&#30340;&#23545;&#35805;&#24335;&#26234;&#33021;&#21161;&#25163;&#30340;&#25554;&#20837;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Insert-expansions for Tool-enabled Conversational Agents. (arXiv:2307.01644v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29305;&#23450;&#24037;&#20855;&#25903;&#25345;&#19979;&#30340;&#23545;&#35805;&#24335;&#26234;&#33021;&#21161;&#25163;&#30340;&#25554;&#20837;&#25193;&#23637;&#12290;&#36890;&#36807;&#23558;&#29992;&#25143;&#20316;&#20026;&#24037;&#20855;&#65292;&#22312;&#29983;&#25104;&#26126;&#30830;&#25512;&#29702;&#36335;&#24452;&#26102;&#25552;&#20379;&#26356;&#22810;&#32454;&#33410;&#21644;&#26356;&#31934;&#30830;&#30340;&#35831;&#27714;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24037;&#20855;&#24178;&#25200;&#29992;&#25143;&#24847;&#22270;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20004;&#20010;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#25512;&#33616;&#39046;&#22495;&#20013;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#24102;&#26469;&#20102;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;Chain-of-Thought-Prompting&#30340;&#39640;&#32423;&#26041;&#24335;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#22312;&#36825;&#31181;&#25552;&#31034;&#26041;&#27861;&#29983;&#25104;&#30340;&#26126;&#30830;&#25512;&#29702;&#36335;&#24452;&#20013;&#20351;&#29992;&#24037;&#20855;&#65288;&#25110;&#8220;&#25554;&#20214;&#8221;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#24037;&#20855;&#30340;&#23545;&#35805;&#24335;&#26234;&#33021;&#21161;&#25163;&#24448;&#24448;&#20250;&#36208;&#20559;&#65292;&#22240;&#20026;&#26469;&#33258;&#25628;&#32034;&#24341;&#25806;&#25110;&#35745;&#31639;&#22120;&#31561;&#24037;&#20855;&#30340;&#39069;&#22806;&#19978;&#19979;&#25991;&#20250;&#20559;&#31163;&#21407;&#22987;&#29992;&#25143;&#24847;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#27010;&#24565;&#65292;&#21363;&#29992;&#25143;&#25104;&#20026;&#24037;&#20855;&#65292;&#25552;&#20379;&#24517;&#35201;&#30340;&#32454;&#33410;&#24182;&#23436;&#21892;&#20182;&#20204;&#30340;&#35831;&#27714;&#12290;&#36890;&#36807;&#23545;&#35805;&#20998;&#26512;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#20132;&#20114;&#31216;&#20026;&#25554;&#20837;&#25193;&#23637;-&#19968;&#31181;&#26088;&#22312;&#20419;&#36827;&#25152;&#38656;&#21709;&#24212;&#30340;&#20013;&#38388;&#23545;&#35805;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#23454;&#35777;&#30740;&#31350;&#20351;&#29992;&#30452;&#25509;&#27604;&#36739;&#30340;&#26041;&#27861;&#25506;&#32034;&#20102;&#20174;&#36825;&#31181;&#8220;&#29992;&#25143;&#20316;&#20026;&#24037;&#20855;&#8221;&#30340;&#26041;&#27861;&#20013;&#20135;&#29983;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#25512;&#33616;&#39046;&#22495;&#20013;&#21457;&#29616;&#20102;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into an advanced implementation of Chain-of-Thought-Prompting in Large Language Models, focusing on the use of tools (or "plug-ins") within the explicit reasoning paths generated by this prompting method. We find that tool-enabled conversational agents often become sidetracked, as additional context from tools like search engines or calculators diverts from original user intents. To address this, we explore a concept wherein the user becomes the tool, providing necessary details and refining their requests. Through Conversation Analysis, we characterize this interaction as insert-expansion - an intermediary conversation designed to facilitate the preferred response. We explore possibilities arising from this 'user-as-a-tool' approach in two empirical studies using direct comparison, and find benefits in the recommendation domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Chain-of-Thought-based&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;CoT-KA&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#22686;&#24378;&#12290;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#20256;&#32479;&#22686;&#24378;&#26041;&#27861;&#20013;&#38656;&#35201;&#39069;&#22806;&#30340;&#30693;&#35782;&#26816;&#32034;&#25110;&#30693;&#35782;&#25512;&#29702;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.01640</link><description>&lt;p&gt;
&#38142;&#24335;&#24605;&#32500;&#21551;&#21457;&#20419;&#36827;&#20102;&#30693;&#35782;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Chain of Thought Prompting Elicits Knowledge Augmentation. (arXiv:2307.01640v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Chain-of-Thought-based&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;CoT-KA&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#22686;&#24378;&#12290;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#20256;&#32479;&#22686;&#24378;&#26041;&#27861;&#20013;&#38656;&#35201;&#39069;&#22806;&#30340;&#30693;&#35782;&#26816;&#32034;&#25110;&#30693;&#35782;&#25512;&#29702;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#26159;&#25351;&#23558;&#39046;&#22495;&#30693;&#35782;&#35782;&#21035;&#24182;&#25972;&#21512;&#21040;&#28145;&#24230;&#27169;&#22411;&#20013;&#30340;&#19968;&#31181;&#33539;&#24335;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#26041;&#27861;&#20174;&#21508;&#31181;&#26469;&#28304;&#25910;&#38598;&#22806;&#37096;&#30693;&#35782;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#20316;&#20026;&#20840;&#38754;&#30340;&#22806;&#37096;&#30693;&#35782;&#26469;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38142;&#24335;&#24605;&#32500;&#30340;CoT-KA&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#22686;&#24378;&#12290;CoT-KA&#36991;&#20813;&#20102;&#20256;&#32479;&#22686;&#24378;&#26041;&#27861;&#20013;&#25152;&#38656;&#30340;&#39069;&#22806;&#30693;&#35782;&#26816;&#32034;&#25110;&#30693;&#35782;&#25512;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#30340;&#21313;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;CoT-KA&#22312;&#32431;CoT&#26041;&#27861;&#21644;&#38750;&#22686;&#24378;&#26041;&#27861;&#20043;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The knowledge-augmented deep learning paradigm refers to a paradigm in which domain knowledge is identified and integrated into deep models. Conventional methods typically employ task-specific approaches to gather external knowledge from various sources. In contrast, large language models are extensively pre-trained and can serve as a comprehensive source of external knowledge. In this paper, we propose CoT-KA, a Chain-of-Thought-based method that augments knowledge for deep learning. CoT-KA avoids the need for additional knowledge retrieval or knowledge reasoning models, as required in conventional augmentation methods. Our results demonstrate that CoT-KA outperforms both pure CoT-based methods and the non-augmented method across the majority of eleven publicly available benchmarks for various reasoning tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;L2&#20420;&#35821;&#20889;&#20316;&#38169;&#35823;&#20462;&#27491;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#20420;&#32599;&#26031;&#22269;&#23478;&#35821;&#26009;&#24211;&#30340;&#26032;&#38395;&#23376;&#35821;&#26009;&#24211;&#26410;&#26631;&#27880;&#25991;&#26412;&#19978;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;RULEC-GEC&#35821;&#26009;&#24211;&#39564;&#35777;&#20102;&#20854;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.01609</link><description>&lt;p&gt;
&#29992;&#20110;L2&#20420;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Language Model for Grammatical Error Correction in L2 Russian. (arXiv:2307.01609v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01609
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;L2&#20420;&#35821;&#20889;&#20316;&#38169;&#35823;&#20462;&#27491;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#20420;&#32599;&#26031;&#22269;&#23478;&#35821;&#26009;&#24211;&#30340;&#26032;&#38395;&#23376;&#35821;&#26009;&#24211;&#26410;&#26631;&#27880;&#25991;&#26412;&#19978;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;RULEC-GEC&#35821;&#26009;&#24211;&#39564;&#35777;&#20102;&#20854;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#12290;&#23545;&#20110;&#20420;&#35821;&#65292;&#22823;&#22810;&#25968;&#21487;&#29992;&#30340;&#25340;&#20889;&#26816;&#26597;&#31243;&#24207;&#21487;&#20197;&#39640;&#20934;&#30830;&#29575;&#22320;&#32416;&#27491;&#25340;&#20889;&#38169;&#35823;&#21644;&#20854;&#20182;&#31616;&#21333;&#38169;&#35823;&#65292;&#20294;&#22312;&#38754;&#23545;&#38750;&#27597;&#35821;&#65288;L2&#65289;&#20889;&#20316;&#26102;&#24448;&#24448;&#22833;&#25928;&#65292;&#22240;&#20026;&#21518;&#32773;&#21547;&#26377;&#23545;&#20110;&#27597;&#35821;&#32773;&#26469;&#35828;&#19981;&#20856;&#22411;&#30340;&#38169;&#35823;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20462;&#27491;L2&#20420;&#35821;&#20889;&#20316;&#38169;&#35823;&#30340;&#27969;&#31243;&#12290;&#25152;&#25552;&#20986;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#22312;&#20420;&#32599;&#26031;&#22269;&#23478;&#35821;&#26009;&#24211;&#26032;&#38395;&#23376;&#35821;&#26009;&#24211;&#30340;&#26410;&#26631;&#27880;&#25991;&#26412;&#19978;&#35757;&#32451;&#30340;&#65292;&#24182;&#36890;&#36807;RULEC-GEC&#35821;&#26009;&#24211;&#39564;&#35777;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grammatical error correction is one of the fundamental tasks in Natural Language Processing. For the Russian language, most of the spellcheckers available correct typos and other simple errors with high accuracy, but often fail when faced with non-native (L2) writing, since the latter contains errors that are not typical for native speakers. In this paper, we propose a pipeline involving a language model intended for correcting errors in L2 Russian writing. The language model proposed is trained on untagged texts of the Newspaper subcorpus of the Russian National Corpus, and the quality of the model is validated against the RULEC-GEC corpus.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#25552;&#31034;&#22686;&#24378;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#21435;&#20559;&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#25512;&#36827;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2307.01595</link><description>&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#36827;&#19968;&#27493;&#25512;&#36827;&#65292;&#23545;&#27604;&#23398;&#20064;&#25289;&#36817;&#36317;&#31163;&#65306;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#26469;&#20943;&#36731;&#31038;&#20250;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases. (arXiv:2307.01595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01595
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#25552;&#31034;&#22686;&#24378;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#21435;&#20559;&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#25512;&#36827;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#25552;&#39640;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#25285;&#24515;&#23427;&#20204;&#20250;&#32487;&#25215;&#26410;&#32463;&#22788;&#29702;&#30340;&#35821;&#26009;&#24211;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#21435;&#20559;&#25216;&#26415;&#20351;&#29992;&#23545;&#27604;&#25968;&#25454;&#22686;&#24378;&#65288;CDA&#65289;&#26469;&#24179;&#34913;&#35757;&#32451;&#35821;&#26009;&#24211;&#12290;&#28982;&#32780;&#65292;CDA&#30053;&#24494;&#20462;&#25913;&#20102;&#21407;&#22987;&#35821;&#26009;&#24211;&#65292;&#38480;&#21046;&#20102;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#22312;&#19968;&#20010;&#29421;&#31364;&#33539;&#22260;&#20869;&#12290;&#32467;&#26524;&#65292;&#21435;&#20559;&#27169;&#22411;&#23481;&#26131;&#36866;&#24212;&#23545;&#27604;&#20107;&#23454;&#23545;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36825;&#24433;&#21709;&#20102;&#23427;&#22312;&#26377;&#38480;&#30340;&#25991;&#26412;&#36164;&#28304;&#19979;&#30340;&#21435;&#20559;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#23545;&#25239;&#35757;&#32451;&#21551;&#21457;&#30340;&#20004;&#38454;&#27573;&#21435;&#20559;&#27169;&#22411;&#65292;&#20351;&#29992;&#36830;&#32493;&#25552;&#31034;&#22686;&#24378;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;&#31216;&#20026;CCPA&#65289;&#26469;&#20943;&#36731;PLMs&#32534;&#30721;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#25552;&#31034;&#35843;&#25972;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25512;&#36827;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;
&lt;/p&gt;
&lt;p&gt;
As the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora. Most previous debiasing techniques used Counterfactual Data Augmentation (CDA) to balance the training corpus. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model easily fits the differences between counterfactual pairs, which affects its debiasing performance with limited text resources. In this paper, we propose an adversarial training-inspired two-stage debiasing model using Contrastive learning with Continuous Prompt Augmentation (named CCPA) to mitigate social biases in PLMs' encoding. In the first stage, we propose a data augmentation method based on continuous prompt tuning to push farther the representation distance between sample pairs along different demographic groups. In the second sta
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#23545;&#27604;&#35757;&#32451;&#65292;&#25105;&#20204;&#25104;&#21151;&#20943;&#36731;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#29983;&#25104;&#20013;&#23545;&#37325;&#22797;&#30340;&#23398;&#20064;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#37325;&#22797;&#38382;&#39064;&#65292;&#24182;&#20445;&#25345;&#20102;&#27969;&#30021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01542</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#23545;&#27604;&#35757;&#32451;&#20943;&#36731;&#24320;&#25918;&#24335;&#29983;&#25104;&#20013;&#23545;&#37325;&#22797;&#30340;&#23398;&#20064;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation. (arXiv:2307.01542v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01542
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#23545;&#27604;&#35757;&#32451;&#65292;&#25105;&#20204;&#25104;&#21151;&#20943;&#36731;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#29983;&#25104;&#20013;&#23545;&#37325;&#22797;&#30340;&#23398;&#20064;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#37325;&#22797;&#38382;&#39064;&#65292;&#24182;&#20445;&#25345;&#20102;&#27969;&#30021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT2&#65289;&#20173;&#20542;&#21521;&#20110;&#20351;&#29992;&#22522;&#20110;&#26368;&#22823;&#21270;&#30340;&#35299;&#30721;&#31639;&#27861;&#29983;&#25104;&#37325;&#22797;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#23558;&#20854;&#23545;&#20196;&#29260;&#32423;&#21035;&#37325;&#22797;&#27010;&#29575;&#30340;&#36807;&#24230;&#20272;&#35745;&#24402;&#22240;&#20110;&#23398;&#20064;&#20559;&#24046;&#65306;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;MLE&#25439;&#22833;&#26356;&#24555;&#22320;&#25429;&#25417;&#21040;&#31616;&#21333;&#30340;&#37325;&#22797;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#23545;&#27604;&#35757;&#32451;&#65292;&#20197;&#24809;&#32602;&#21516;&#19968;&#27169;&#22411;&#36807;&#26089;&#26816;&#26597;&#28857;&#30340;&#36755;&#20986;&#65292;&#24403;&#23427;&#38169;&#35823;&#22320;&#39044;&#27979;&#37325;&#22797;&#26102;&#65292;&#36825;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#26377;&#25928;&#20943;&#36731;&#37325;&#22797;&#21516;&#26102;&#20445;&#25345;&#27969;&#30021;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#37325;&#22797;&#20196;&#29260;&#26102;&#20351;&#29992;&#30340;&#26159;&#26356;&#38271;&#33539;&#22260;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#38750;&#37325;&#22797;&#20196;&#29260;&#21017;&#19981;&#28982;&#65292;&#36825;&#21487;&#33021;&#26159;&#36896;&#25104;&#21477;&#23376;&#32423;&#21035;&#37325;&#22797;&#24490;&#29615;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the huge progress in myriad generation tasks, pretrained language models (LMs) such as GPT2 still tend to generate repetitive texts with maximization-based decoding algorithms for open-ended generation. We attribute their overestimation of token-level repetition probabilities to the learning bias: LMs capture simple repetitive patterns faster with the MLE loss. We propose self-contrastive training to penalize the output of a premature checkpoint of the same model when it incorrectly predicts repetition, which is shown to mitigate repetition effectively while maintaining fluency on two datasets. Furthermore, we find that LMs use longer-range dependencies to predict repetitive tokens than non-repetitive ones, which may be the cause of sentence-level repetition loops.
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#23398;&#20064;&#25552;&#31034;&#65292;&#35797;&#22270;&#22312;&#35838;&#22530;&#29615;&#22659;&#20013;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#30340;&#38480;&#21046;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#36127;&#38754;&#24773;&#32490;&#12290;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#38480;&#21046;&#34987;&#24573;&#35270;&#65292;&#23548;&#33268;&#20102;&#38169;&#35823;&#30340;&#33258;&#20449;&#21644;&#19981;&#20934;&#30830;&#30340;&#24314;&#35758;&#12290;&#25215;&#35748;&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#21487;&#38752;&#24615;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2307.01540</link><description>&lt;p&gt;
&#22312;&#35838;&#22530;&#19978;&#23398;&#20064;&#25552;&#31034;&#20197;&#20102;&#35299;&#20154;&#24037;&#26234;&#33021;&#30340;&#38480;&#21046;&#65306;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning to Prompt in the Classroom to Understand AI Limits: A pilot study. (arXiv:2307.01540v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01540
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#23398;&#20064;&#25552;&#31034;&#65292;&#35797;&#22270;&#22312;&#35838;&#22530;&#29615;&#22659;&#20013;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#30340;&#38480;&#21046;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#36127;&#38754;&#24773;&#32490;&#12290;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#38480;&#21046;&#34987;&#24573;&#35270;&#65292;&#23548;&#33268;&#20102;&#38169;&#35823;&#30340;&#33258;&#20449;&#21644;&#19981;&#20934;&#30830;&#30340;&#24314;&#35758;&#12290;&#25215;&#35748;&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#21487;&#38752;&#24615;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#22312;&#24110;&#21161;&#31038;&#20250;&#35299;&#20915;&#32039;&#36843;&#30340;&#31038;&#20250;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#27966;&#29983;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22914;ChatGPT&#65292;&#22823;&#22823;&#25913;&#36827;&#20102;AI&#31995;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#21069;&#25152;&#26410;&#26377;&#30340;&#22823;&#37327;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#28818;&#20316;&#20063;&#20135;&#29983;&#20102;&#36127;&#38754;&#24773;&#32490;&#65292;&#21363;&#20351;&#22312;&#26032;&#39062;&#30340;AI&#26041;&#27861;&#21462;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#36129;&#29486;&#20043;&#21518;&#12290;&#36896;&#25104;&#36825;&#31181;&#24773;&#20917;&#30340;&#21407;&#22240;&#20043;&#19968;&#65292;&#20294;&#20063;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#26412;&#36523;&#65292;&#26159;&#36234;&#26469;&#36234;&#22810;&#20154;&#38169;&#35823;&#22320;&#35748;&#20026;&#33258;&#24049;&#33021;&#22815;&#36731;&#26494;&#35775;&#38382;&#21644;&#22788;&#29702;&#20219;&#20309;&#24418;&#24335;&#30340;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#20219;&#20309;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#26080;&#38656;&#23545;AI&#25110;&#38382;&#39064;&#39046;&#22495;&#26377;&#20219;&#20309;&#19987;&#19994;&#30693;&#35782;&#65292;&#32780;&#24573;&#35270;&#20102;&#24403;&#21069;LLMs&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#24187;&#35273;&#21644;&#25512;&#29702;&#38480;&#21046;&#12290;&#25215;&#35748;&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#21487;&#38752;&#24615;&#23545;&#20110;&#35299;&#20915;&#30001;LLMs&#29983;&#25104;&#30340;&#21487;&#33021;&#38169;&#35823;&#24314;&#35758;&#21487;&#33021;&#20135;&#29983;&#30340;&#30450;&#30446;&#36807;&#24230;&#33258;&#20449;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#36825;&#21487;&#20197;&#20943;&#23569;&#24656;&#24807;&#21644;&#20854;&#20182;&#36127;&#38754;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence's progress holds great promise in assisting society in addressing pressing societal issues. In particular Large Language Models (LLM) and the derived chatbots, like ChatGPT, have highly improved the natural language processing capabilities of AI systems allowing them to process an unprecedented amount of unstructured data. The consequent hype has also backfired, raising negative sentiment even after novel AI methods' surprising contributions. One of the causes, but also an important issue per se, is the rising and misleading feeling of being able to access and process any form of knowledge to solve problems in any domain with no effort or previous expertise in AI or problem domain, disregarding current LLMs limits, such as hallucinations and reasoning limits. Acknowledging AI fallibility is crucial to address the impact of dogmatic overconfidence in possibly erroneous suggestions generated by LLMs. At the same time, it can reduce fear and other negative attitude
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#35780;&#20272;&#21644;&#32531;&#35299;&#24615;&#21035;&#20559;&#35265;&#30340;&#25361;&#25112;&#65292;&#25193;&#23637;&#20102;&#35780;&#20272;&#24615;&#21035;&#20559;&#35265;&#30340;&#22522;&#20934;&#21644;&#36164;&#28304;&#65292;&#24182;&#22312;&#21360;&#24230;&#35821;&#35328;&#19978;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#22522;&#20934;&#12290;&#30740;&#31350;&#36824;&#23558;&#21435;&#20559;&#26041;&#27861;&#25193;&#23637;&#21040;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#65292;&#24182;&#22312;&#25552;&#35758;&#30340;&#24230;&#37327;&#26631;&#20934;&#19978;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01503</link><description>&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#35780;&#20272;&#21644;&#32531;&#35299;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
On Evaluating and Mitigating Gender Biases in Multilingual Settings. (arXiv:2307.01503v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#35780;&#20272;&#21644;&#32531;&#35299;&#24615;&#21035;&#20559;&#35265;&#30340;&#25361;&#25112;&#65292;&#25193;&#23637;&#20102;&#35780;&#20272;&#24615;&#21035;&#20559;&#35265;&#30340;&#22522;&#20934;&#21644;&#36164;&#28304;&#65292;&#24182;&#22312;&#21360;&#24230;&#35821;&#35328;&#19978;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#22522;&#20934;&#12290;&#30740;&#31350;&#36824;&#23558;&#21435;&#20559;&#26041;&#27861;&#25193;&#23637;&#21040;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#65292;&#24182;&#22312;&#25552;&#35758;&#30340;&#24230;&#37327;&#26631;&#20934;&#19978;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#29702;&#35299;&#21644;&#28040;&#38500;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20294;&#20808;&#21069;&#30340;&#30740;&#31350;&#24037;&#20316;&#20027;&#35201;&#23616;&#38480;&#20110;&#33521;&#35821;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#35780;&#20272;&#21644;&#32531;&#35299;&#20559;&#35265;&#25152;&#38754;&#20020;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#36825;&#20123;&#25361;&#25112;&#28304;&#20110;&#32570;&#20047;&#29992;&#20110;&#38750;&#35199;&#26041;&#32972;&#26223;&#30340;&#24615;&#21035;&#20559;&#35265;&#35780;&#20272;&#30340;&#29616;&#26377;&#22522;&#20934;&#21644;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#65292;&#23558;DisCo&#25193;&#23637;&#21040;&#19981;&#21516;&#30340;&#21360;&#24230;&#35821;&#35328;&#65292;&#20026;&#35780;&#20272;&#39044;&#20808;&#35757;&#32451;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#12290;&#25105;&#20204;&#23558;&#21508;&#31181;&#21435;&#20559;&#26041;&#27861;&#25193;&#23637;&#21040;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#65292;&#24182;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#19978;&#35780;&#20272;&#23427;&#20204;&#23545;SOTA&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#30740;&#31350;&#31038;&#20250;&#20559;&#35265;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#36164;&#28304;&#21644;&#32531;&#35299;&#25216;&#26415;&#65292;&#20197;&#25512;&#36827;&#26356;&#22810;&#35821;&#35328;&#30340;&#35268;&#27169;&#21270;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
While understanding and removing gender biases in language models has been a long-standing problem in Natural Language Processing, prior research work has primarily been limited to English. In this work, we investigate some of the challenges with evaluating and mitigating biases in multilingual settings which stem from a lack of existing benchmarks and resources for bias evaluation beyond English especially for non-western context. In this paper, we first create a benchmark for evaluating gender biases in pre-trained masked language models by extending DisCo to different Indian languages using human annotations. We extend various debiasing methods to work beyond English and evaluate their effectiveness for SOTA massively multilingual models on our proposed metric. Overall, our work highlights the challenges that arise while studying social biases in multilingual settings and provides resources as well as mitigation techniques to take a step toward scaling to more languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCAT&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#26469;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#19981;&#20381;&#36182;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#40065;&#26834;&#30340;&#34920;&#31034;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;SCAT&#22312;&#20004;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01488</link><description>&lt;p&gt;
SCAT&#65306;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
SCAT: Robust Self-supervised Contrastive Learning via Adversarial Training for Text Classification. (arXiv:2307.01488v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCAT&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#26469;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#19981;&#20381;&#36182;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#40065;&#26834;&#30340;&#34920;&#31034;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;SCAT&#22312;&#20004;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#24403;&#21069;&#30340;NLP&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#38450;&#24481;&#36825;&#20123;&#25915;&#20987;&#65292;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#25968;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#26469;&#24341;&#20837;&#23545;&#25239;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20381;&#36182;&#20110;&#26631;&#20934;&#26631;&#31614;&#26469;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#36825;&#22312;&#22914;&#20170;&#24120;&#29992;&#20110;NLP&#21644;&#20854;&#20182;&#20219;&#21153;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;SCAT&#65288;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#40065;&#26834;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SCAT&#36890;&#36807;&#23545;&#25968;&#25454;&#36827;&#34892;&#23436;&#20840;&#26080;&#26631;&#35760;&#30340;&#38543;&#26426;&#22686;&#24378;&#26469;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#22686;&#24378;&#21644;&#20854;&#23545;&#24212;&#30340;&#23545;&#25239;&#26679;&#26412;&#20043;&#38388;&#30340;&#23545;&#27604;&#25439;&#22833;&#26469;&#23454;&#29616;&#23545;&#25239;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;SCAT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their promising performance across various natural language processing (NLP) tasks, current NLP systems are vulnerable to textual adversarial attacks. To defend against these attacks, most existing methods apply adversarial training by incorporating adversarial examples. However, these methods have to rely on ground-truth labels to generate adversarial examples, rendering it impractical for large-scale model pre-training which is commonly used nowadays for NLP and many other tasks. In this paper, we propose a novel learning framework called SCAT (Self-supervised Contrastive Learning via Adversarial Training), which can learn robust representations without requiring labeled data. Specifically, SCAT modifies random augmentations of the data in a fully labelfree manner to generate adversarial examples. Adversarial training is achieved by minimizing the contrastive loss between the augmentations and their adversarial counterparts. We evaluate SCAT on two text classification dataset
&lt;/p&gt;</description></item><item><title>CARE-MI&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#22269;&#23381;&#23156;&#25252;&#29702;&#39046;&#22495;LLM&#34394;&#20551;&#20449;&#24687;&#30340;&#22522;&#20934;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#38271;&#31687;&#29983;&#25104;&#35780;&#20272;&#22522;&#20934;&#30340;&#21019;&#26032;&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.01458</link><description>&lt;p&gt;
CARE-MI: &#20013;&#22269;&#23381;&#23156;&#25252;&#29702;&#39046;&#22495;&#30340;&#34394;&#20551;&#20449;&#24687;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care. (arXiv:2307.01458v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01458
&lt;/p&gt;
&lt;p&gt;
CARE-MI&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#22269;&#23381;&#23156;&#25252;&#29702;&#39046;&#22495;LLM&#34394;&#20551;&#20449;&#24687;&#30340;&#22522;&#20934;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#38271;&#31687;&#29983;&#25104;&#35780;&#20272;&#22522;&#20934;&#30340;&#21019;&#26032;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#23548;&#33268;&#20102;&#23558;LLM&#24212;&#29992;&#20110;&#29616;&#23454;&#22330;&#26223;&#30340;&#26032;&#36235;&#21183;&#12290;&#23613;&#31649;&#26368;&#26032;&#30340;LLM&#22312;&#19982;&#20154;&#31867;&#20114;&#21160;&#26102;&#20196;&#20154;&#24778;&#21497;&#22320;&#27969;&#21033;&#65292;&#20294;&#23427;&#20204;&#22312;&#29983;&#25104;&#38169;&#35823;&#20107;&#23454;&#38472;&#36848;&#26102;&#20250;&#24847;&#22806;&#20135;&#29983;&#34394;&#20551;&#20449;&#24687;&#38382;&#39064;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#26377;&#23475;&#21518;&#26524;&#65292;&#23588;&#20854;&#26159;&#22312;&#25935;&#24863;&#29615;&#22659;&#19979;&#65292;&#27604;&#22914;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#35780;&#20272;LLM&#38271;&#31687;&#29983;&#25104;&#20013;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#30693;&#35782;&#23494;&#38598;&#22411;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;LLM&#22312;&#19981;&#21516;&#35821;&#35328;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#34394;&#20551;&#20449;&#24687;&#35780;&#20272;&#20027;&#35201;&#22312;&#33521;&#35821;&#20013;&#36827;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;CARE-MI&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#34394;&#20551;&#20449;&#24687;&#22312;&#65306;1&#65289;&#19968;&#20010;&#25935;&#24863;&#20027;&#39064;&#65292;&#20855;&#20307;&#26159;&#23381;&#23156;&#25252;&#29702;&#39046;&#22495;&#65307;&#21644;2&#65289;&#19968;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#65292;&#21363;&#20013;&#25991;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#26500;&#24314;&#38271;&#31687;&#29983;&#25104;&#35780;&#20272;&#22522;&#20934;&#65292;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
The recent advances in NLP, have led to a new trend of applying LLMs to real-world scenarios. While the latest LLMs are astonishingly fluent when interacting with humans, they suffer from the misinformation problem by unintentionally generating factually false statements. This can lead to harmful consequences, especially when produced within sensitive contexts, such as healthcare. Yet few previous works have focused on evaluating misinformation in the long-form generation of LLMs, especially for knowledge-intensive topics. Moreover, although LLMs have been shown to perform well in different languages, misinformation evaluation has been mostly conducted in English. To this end, we present a benchmark, CARE-MI, for evaluating LLM misinformation in: 1) a sensitive topic, specifically the maternity and infant care domain; and 2) a language other than English, namely Chinese. Most importantly, we provide an innovative paradigm for building long-form generation evaluation benchmarks that can
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;RefPyDST&#65292;&#36890;&#36807;&#22312;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24341;&#20837;Python&#32534;&#31243;&#20219;&#21153;&#12289;&#22810;&#26679;&#24615;&#26816;&#32034;&#30456;&#20851;&#31034;&#20363;&#21644;&#37325;&#26032;&#26435;&#37325;&#35299;&#30721;&#26041;&#27861;&#30340;&#25913;&#36827;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01453</link><description>&lt;p&gt;
&#22810;&#26679;&#30340;&#26816;&#32034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#29992;&#20110;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking. (arXiv:2307.01453v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01453
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;RefPyDST&#65292;&#36890;&#36807;&#22312;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24341;&#20837;Python&#32534;&#31243;&#20219;&#21153;&#12289;&#22810;&#26679;&#24615;&#26816;&#32034;&#30456;&#20851;&#31034;&#20363;&#21644;&#37325;&#26032;&#26435;&#37325;&#35299;&#30721;&#26041;&#27861;&#30340;&#25913;&#36827;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25910;&#38598;&#21644;&#27880;&#37322;&#38754;&#21521;&#20219;&#21153;&#23545;&#35805;&#30340;&#25104;&#26412;&#36739;&#39640;&#65292;&#23545;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394; (DST) &#24341;&#36215;&#20102;&#37325;&#22823;&#20852;&#36259;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#21482;&#38656;&#35201;&#24456;&#23569;&#30340;&#25968;&#25454;&#21644;&#38646;&#20010;&#21442;&#25968;&#26356;&#26032;&#65292;&#29978;&#33267;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#20248;&#20110;&#35757;&#32451;&#26041;&#27861; (Hu&#31561;&#65292;2022)&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RefPyDST&#65292;&#23427;&#36890;&#36807;&#19977;&#20010;&#25913;&#36827;&#25512;&#21160;&#20102;&#23545;DST&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;DST&#24418;&#24335;&#21270;&#20026;Python&#32534;&#31243;&#20219;&#21153;&#65292;&#26126;&#30830;&#22320;&#23558;&#35821;&#35328;&#25351;&#20195;&#24314;&#27169;&#20026;Python&#20013;&#30340;&#21464;&#37327;&#24341;&#29992;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#39640;&#24230;&#20381;&#36182;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22810;&#26679;&#30340;&#30456;&#20851;&#31034;&#20363;&#20197;&#25552;&#39640;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#26032;&#26435;&#37325;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#31454;&#20105;&#34920;&#38754;&#24418;&#24335;&#30340;&#27010;&#29575;&#65292;&#24182;&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#35805;&#29366;&#24577;&#39044;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;MultiWOZ&#21644;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
There has been significant interest in zero and few-shot learning for dialogue state tracking (DST) due to the high cost of collecting and annotating task-oriented dialogues. Recent work has demonstrated that in-context learning requires very little data and zero parameter updates, and even outperforms trained methods in the few-shot setting (Hu et al. 2022). We propose RefPyDST, which advances the state of the art with three advancements to in-context learning for DST. First, we formulate DST as a Python programming task, explicitly modeling language coreference as variable reference in Python. Second, since in-context learning depends highly on the context examples, we propose a method to retrieve a diverse set of relevant examples to improve performance. Finally, we introduce a novel re-weighting method during decoding that takes into account probabilities of competing surface forms, and produces a more accurate dialogue state prediction. We evaluate our approach using MultiWOZ and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ReactIE&#65292;&#21033;&#29992;&#24369;&#30417;&#30563;&#26041;&#27861;&#25552;&#39640;&#20102;&#21270;&#23398;&#21453;&#24212;&#25552;&#21462;&#25216;&#26415;&#12290;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#20013;&#30340;&#39057;&#32321;&#27169;&#24335;&#21644;&#19987;&#21033;&#35760;&#24405;&#20013;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#23545;&#21270;&#23398;&#21453;&#24212;&#30340;&#29305;&#23450;&#29305;&#24449;&#35782;&#21035;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#24182;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.01448</link><description>&lt;p&gt;
ReactIE: &#20511;&#21161;&#24369;&#30417;&#30563;&#25552;&#21319;&#21270;&#23398;&#21453;&#24212;&#25552;&#21462;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision. (arXiv:2307.01448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ReactIE&#65292;&#21033;&#29992;&#24369;&#30417;&#30563;&#26041;&#27861;&#25552;&#39640;&#20102;&#21270;&#23398;&#21453;&#24212;&#25552;&#21462;&#25216;&#26415;&#12290;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#20013;&#30340;&#39057;&#32321;&#27169;&#24335;&#21644;&#19987;&#21033;&#35760;&#24405;&#20013;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#23545;&#21270;&#23398;&#21453;&#24212;&#30340;&#29305;&#23450;&#29305;&#24449;&#35782;&#21035;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#24182;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#21270;&#23398;&#21453;&#24212;&#20449;&#24687;&#23545;&#20174;&#20107;&#21270;&#23398;&#23454;&#39564;&#21644;&#35745;&#31639;&#36741;&#21161;&#33647;&#29289;&#35774;&#35745;&#31561;&#39046;&#22495;&#30340;&#21270;&#23398;&#23478;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#21453;&#24212;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#30001;&#20110;&#39046;&#22495;&#19987;&#23478;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#65292;&#23548;&#33268;&#20026;&#27492;&#30446;&#30340;&#36827;&#34892;&#25968;&#25454;&#26631;&#27880;&#25104;&#26412;&#36807;&#39640;&#12290;&#22240;&#27492;&#65292;&#32570;&#20047;&#36275;&#22815;&#30340;&#35757;&#32451;&#25968;&#25454;&#25104;&#20026;&#35813;&#39046;&#22495;&#30456;&#20851;&#27169;&#22411;&#36827;&#23637;&#30340;&#38556;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ReactIE&#65292;&#23427;&#32467;&#21512;&#20004;&#31181;&#24369;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#20013;&#30340;&#39057;&#32321;&#27169;&#24335;&#20316;&#20026;&#35821;&#35328;&#32447;&#32034;&#26469;&#35782;&#21035;&#21270;&#23398;&#21453;&#24212;&#30340;&#29305;&#23450;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#19987;&#21033;&#35760;&#24405;&#20013;&#30340;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#36828;&#31243;&#30417;&#30563;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReactIE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured chemical reaction information plays a vital role for chemists engaged in laboratory work and advanced endeavors such as computer-aided drug design. Despite the importance of extracting structured reactions from scientific literature, data annotation for this purpose is cost-prohibitive due to the significant labor required from domain experts. Consequently, the scarcity of sufficient training data poses an obstacle to the progress of related models in this domain. In this paper, we propose ReactIE, which combines two weakly supervised approaches for pre-training. Our method utilizes frequent patterns within the text as linguistic cues to identify specific characteristics of chemical reactions. Additionally, we adopt synthetic data from patent records as distant supervision to incorporate domain knowledge into the model. Experiments demonstrate that ReactIE achieves substantial improvements and outperforms all existing baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26465;&#20214;&#21644;&#32452;&#21512;&#30340;&#21487;&#24494;&#25552;&#31034;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;Prompt Production System&#65288;PRopS&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#35828;&#26126;&#25110;&#36755;&#20837;&#20803;&#25968;&#25454;&#36716;&#21270;&#20026;&#36830;&#32493;&#30340;&#25552;&#31034;&#65292;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#33021;&#22815;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#30340;&#36755;&#20986;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#21644;&#31163;&#25955;&#35268;&#21017;&#30340;&#23398;&#20064;&#65292;&#36866;&#29992;&#20110;&#32452;&#21512;&#24335;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;PRopS&#22312;PLM&#36866;&#24212;&#20013;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#25216;&#26415;&#65292;&#24182;&#19988;&#36890;&#24120;&#25913;&#36827;&#20102;&#23436;&#20840;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.01446</link><description>&lt;p&gt;
&#20851;&#20110;&#26465;&#20214;&#21644;&#32452;&#21512;&#35821;&#35328;&#27169;&#22411;&#21487;&#24494;&#25552;&#31034;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
On Conditional and Compositional Language Model Differentiable Prompting. (arXiv:2307.01446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26465;&#20214;&#21644;&#32452;&#21512;&#30340;&#21487;&#24494;&#25552;&#31034;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;Prompt Production System&#65288;PRopS&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#35828;&#26126;&#25110;&#36755;&#20837;&#20803;&#25968;&#25454;&#36716;&#21270;&#20026;&#36830;&#32493;&#30340;&#25552;&#31034;&#65292;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#33021;&#22815;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#30340;&#36755;&#20986;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#21644;&#31163;&#25955;&#35268;&#21017;&#30340;&#23398;&#20064;&#65292;&#36866;&#29992;&#20110;&#32452;&#21512;&#24335;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;PRopS&#22312;PLM&#36866;&#24212;&#20013;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#25216;&#26415;&#65292;&#24182;&#19988;&#36890;&#24120;&#25913;&#36827;&#20102;&#23436;&#20840;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25552;&#31034;&#21487;&#20197;&#30001;&#20154;&#24037;&#35774;&#35745;&#30340;&#35789;&#24207;&#21015;&#25110;&#23398;&#20064;&#24471;&#21040;&#30340;&#36830;&#32493;&#23884;&#20837;&#26469;&#34920;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26465;&#20214;&#21644;&#32452;&#21512;&#30340;&#21487;&#24494;&#25552;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;Prompt Production System&#65288;PRopS&#65289;&#65292;&#23427;&#23398;&#20064;&#23558;&#20219;&#21153;&#35828;&#26126;&#25110;&#36755;&#20837;&#20803;&#25968;&#25454;&#36716;&#21270;&#20026;&#36830;&#32493;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#28608;&#21457;PLM&#20135;&#29983;&#20219;&#21153;&#29305;&#23450;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#22522;&#20110;&#25105;&#20204;&#23545;&#20110;&#20135;&#21697;&#31995;&#32479;&#30340;&#31070;&#32463;&#24418;&#24335;&#21270;&#30340;&#27169;&#22359;&#21270;&#32593;&#32476;&#32467;&#26500;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#31163;&#25955;&#35268;&#21017;&#8212;&#8212;&#31070;&#32463;&#20989;&#25968;&#65292;&#36825;&#20123;&#20989;&#25968;&#23398;&#20064;&#19987;&#38376;&#23558;&#29305;&#23450;&#30340;&#25552;&#31034;&#36755;&#20837;&#27169;&#24335;&#36716;&#21270;&#20026;&#29305;&#23450;&#30340;&#36755;&#20986;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#32452;&#21512;&#24335;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;PRopS&#22987;&#32456;&#36229;&#36234;&#20854;&#20182;PLM&#36866;&#24212;&#25216;&#26415;&#65292;&#24182;&#19988;&#36890;&#24120;&#25913;&#36827;&#20102;&#23436;&#20840;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompts have been shown to be an effective method to adapt a frozen Pretrained Language Model (PLM) to perform well on downstream tasks. Prompts can be represented by a human-engineered word sequence or by a learned continuous embedding. In this work, we investigate conditional and compositional differentiable prompting. We propose a new model, Prompt Production System (PRopS), which learns to transform task instructions or input metadata, into continuous prompts that elicit task-specific outputs from the PLM. Our model uses a modular network structure based on our neural formulation of Production Systems, which allows the model to learn discrete rules -- neural functions that learn to specialize in transforming particular prompt input patterns, making it suitable for compositional transfer learning and few-shot learning. We present extensive empirical and theoretical analysis and show that PRopS consistently surpasses other PLM adaptation techniques, and often improves upon fully fine
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;17&#20010;StackExchange&#31038;&#21306;&#29992;&#25143;&#30340;&#26631;&#31614;&#34892;&#20026;&#36827;&#34892;&#20998;&#26512;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#26631;&#31614;&#39044;&#27979;&#26550;&#26500;&#65292;&#21487;&#20197;&#39044;&#27979;&#27599;&#20010;&#38382;&#39064;&#30340;&#28909;&#38376;&#26631;&#31614;&#21644;&#26356;&#32454;&#31890;&#24230;&#30340;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2307.01420</link><description>&lt;p&gt;
&#22522;&#20110;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#29992;&#25143;&#26631;&#31614;&#34892;&#20026;&#20998;&#26512;&#30340;&#26631;&#31614;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Modeling Tag Prediction based on Question Tagging Behavior Analysis of CommunityQA Platform Users. (arXiv:2307.01420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;17&#20010;StackExchange&#31038;&#21306;&#29992;&#25143;&#30340;&#26631;&#31614;&#34892;&#20026;&#36827;&#34892;&#20998;&#26512;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#26631;&#31614;&#39044;&#27979;&#26550;&#26500;&#65292;&#21487;&#20197;&#39044;&#27979;&#27599;&#20010;&#38382;&#39064;&#30340;&#28909;&#38376;&#26631;&#31614;&#21644;&#26356;&#32454;&#31890;&#24230;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#20013;&#65292;&#26631;&#31614;&#22312;&#26377;&#25928;&#30340;&#20449;&#24687;&#32452;&#32455;&#21644;&#26816;&#32034;&#12289;&#26356;&#22909;&#30340;&#38382;&#39064;&#36335;&#30001;&#12289;&#26356;&#24555;&#30340;&#38382;&#39064;&#21709;&#24212;&#20197;&#21450;&#35805;&#39064;&#28909;&#24230;&#35780;&#20272;&#26041;&#38754;&#37117;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#36825;&#31867;&#24179;&#21488;&#30340;&#29992;&#25143;&#26469;&#35828;&#65292;&#39044;&#27979;&#21644;&#24314;&#35758;&#24086;&#23376;&#30340;&#26631;&#31614;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#33258;&#21160;&#36741;&#21161;&#21151;&#33021;&#12290;&#20026;&#20102;&#22312;&#19981;&#21516;&#30340;&#31038;&#21306;&#21644;&#39046;&#22495;&#20013;&#24320;&#21457;&#26356;&#22909;&#30340;&#26631;&#31614;&#39044;&#27979;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;17&#20010;StackExchange&#31038;&#21306;&#30340;&#29992;&#25143;&#26631;&#31614;&#34892;&#20026;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#36825;&#20123;&#19981;&#21516;&#39046;&#22495;&#20013;&#29992;&#25143;&#26631;&#31614;&#34892;&#20026;&#30340;&#19968;&#20123;&#20849;&#21516;&#23646;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#24320;&#21457;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#26631;&#31614;&#39044;&#27979;&#26550;&#26500;&#65292;&#21487;&#20197;&#39044;&#27979;&#27599;&#20010;&#38382;&#39064;&#30340;&#28909;&#38376;&#26631;&#31614;&#21644;&#26356;&#32454;&#31890;&#24230;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#21644;&#33719;&#24471;&#30340;&#24615;&#33021;&#34920;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In community question-answering platforms, tags play essential roles in effective information organization and retrieval, better question routing, faster response to questions, and assessment of topic popularity. Hence, automatic assistance for predicting and suggesting tags for posts is of high utility to users of such platforms. To develop better tag prediction across diverse communities and domains, we performed a thorough analysis of users' tagging behavior in 17 StackExchange communities. We found various common inherent properties of this behavior in those diverse domains. We used the findings to develop a flexible neural tag prediction architecture, which predicts both popular tags and more granular tags for each question. Our extensive experiments and obtained performance show the effectiveness of our model
&lt;/p&gt;</description></item><item><title>&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#28145;&#24230;&#35770;&#35777;&#25366;&#25496;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#26500;&#24314;&#20849;&#20139;&#34920;&#31034;&#24182;&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#35770;&#35777;&#25366;&#25496;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.01401</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#25552;&#39640;&#28145;&#24230;&#35770;&#35777;&#25366;&#25496;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning Improves Performance In Deep Argument Mining Models. (arXiv:2307.01401v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01401
&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#28145;&#24230;&#35770;&#35777;&#25366;&#25496;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#26500;&#24314;&#20849;&#20139;&#34920;&#31034;&#24182;&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#35770;&#35777;&#25366;&#25496;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#22320;&#20174;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#20998;&#26512;&#35770;&#35777;&#25216;&#24039;&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#25919;&#27835;&#21644;&#24066;&#22330;&#20998;&#26512;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#35770;&#35777;&#25366;&#25496;&#24037;&#20855;&#20351;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20174;&#21508;&#31181;&#22312;&#32447;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#21644;&#27880;&#37322;&#35770;&#35777;&#25216;&#24039;&#65292;&#28982;&#32780;&#27599;&#20010;&#20219;&#21153;&#34987;&#35270;&#20026;&#29420;&#31435;&#30340;&#65292;&#19981;&#21516;&#30340;&#29305;&#23450;&#27169;&#22411;&#34987;&#38024;&#23545;&#27599;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#26045;&#22810;&#20219;&#21153;&#26041;&#27861;&#26469;&#35770;&#35777;&#25366;&#25496;&#65292;&#34920;&#26126;&#19981;&#21516;&#30340;&#35770;&#35777;&#25366;&#25496;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#24120;&#35265;&#30340;&#35821;&#20041;&#21644;&#36923;&#36753;&#32467;&#26500;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35299;&#20915;&#30456;&#21516;&#38382;&#39064;&#26102;&#36798;&#21040;&#20102;&#27604;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#20010;&#23545;&#25152;&#26377;&#20219;&#21153;&#37117;&#20849;&#26377;&#30340;&#36755;&#20837;&#25991;&#26412;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#21442;&#25968;&#20849;&#20139;&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;&#35770;&#35777;&#25366;&#25496;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#34920;&#26126;&#19981;&#21516;&#30340;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#20307;&#30340;&#35770;&#35777;&#25552;&#21462;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The successful analysis of argumentative techniques from user-generated text is central to many downstream tasks such as political and market analysis. Recent argument mining tools use state-of-the-art deep learning methods to extract and annotate argumentative techniques from various online text corpora, however each task is treated as separate and different bespoke models are fine-tuned for each dataset. We show that different argument mining tasks share common semantic and logical structure by implementing a multi-task approach to argument mining that achieves better performance than state-of-the-art methods for the same problems. Our model builds a shared representation of the input text that is common to all tasks and exploits similarities between tasks in order to further boost performance via parameter-sharing. Our results are important for argument mining as they show that different tasks share substantial similarities and suggest a holistic approach to the extraction of argume
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALBERTI&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#65292;&#22312;12&#31181;&#35821;&#35328;&#30340;1200&#19975;&#34892;&#35799;&#27468;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#32467;&#26500;&#24615;&#35799;&#27468;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#35780;&#20272;&#26174;&#31034;&#65292;ALBERTI&#20248;&#20110;&#22810;&#35821;&#35328;BERT&#21644;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#24503;&#35821;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#65292;&#35777;&#26126;&#20102;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#22312;&#35799;&#27468;&#39046;&#22495;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01387</link><description>&lt;p&gt;
ALBERTI,&#19968;&#31181;&#29992;&#20110;&#35799;&#27468;&#20998;&#26512;&#30340;&#22810;&#35821;&#35328;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;.
&lt;/p&gt;
&lt;p&gt;
ALBERTI, a Multilingual Domain Specific Language Model for Poetry Analysis. (arXiv:2307.01387v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALBERTI&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#65292;&#22312;12&#31181;&#35821;&#35328;&#30340;1200&#19975;&#34892;&#35799;&#27468;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#32467;&#26500;&#24615;&#35799;&#27468;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#35780;&#20272;&#26174;&#31034;&#65292;ALBERTI&#20248;&#20110;&#22810;&#35821;&#35328;BERT&#21644;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#24503;&#35821;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#65292;&#35777;&#26126;&#20102;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#22312;&#35799;&#27468;&#39046;&#22495;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35799;&#27468;&#30340;&#35745;&#31639;&#20998;&#26512;&#21463;&#21040;&#33258;&#21160;&#20998;&#26512;&#21644;&#25195;&#25551;&#24037;&#20855;&#30340;&#31232;&#32570;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#65292;&#30001;&#20110;&#20165;&#23384;&#22312;&#21333;&#20010;&#35821;&#35328;&#30340;&#25195;&#25551;&#21644;&#38901;&#24459;&#31995;&#32479;&#65292;&#22240;&#27492;&#27604;&#36739;&#24615;&#30740;&#31350;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;\textsc{Alberti}&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35799;&#27468;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;(DSP)&#65292;&#25105;&#20204;&#22312;12&#31181;&#35821;&#35328;&#30340;1,200&#19975;&#34892;&#35799;&#27468;&#35821;&#26009;&#24211;&#19978;&#23545;&#22810;&#35821;&#35328;BERT&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#32467;&#26500;&#24615;&#35799;&#27468;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;: &#35199;&#29677;&#29273;&#35799;&#27468;&#31687;&#31456;&#31867;&#22411;&#20998;&#31867;&#20197;&#21450;&#35199;&#29677;&#29273;&#35821;&#12289;&#33521;&#35821;&#21644;&#24503;&#35821;&#30340;&#38901;&#24459;&#27169;&#24335;&#39044;&#27979;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;\textsc{Alberti}&#20248;&#20110;&#22810;&#35821;&#35328;BERT&#21644;&#20854;&#20182;&#30456;&#20284;&#35268;&#27169;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#29978;&#33267;&#22312;&#19982;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;&#23545;&#20110;&#24503;&#35821;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#65292;&#23637;&#31034;&#20102;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#22312;&#35799;&#27468;&#39046;&#22495;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computational analysis of poetry is limited by the scarcity of tools to automatically analyze and scan poems. In a multilingual settings, the problem is exacerbated as scansion and rhyme systems only exist for individual languages, making comparative studies very challenging and time consuming. In this work, we present \textsc{Alberti}, the first multilingual pre-trained large language model for poetry. Through domain-specific pre-training (DSP), we further trained multilingual BERT on a corpus of over 12 million verses from 12 languages. We evaluated its performance on two structural poetry tasks: Spanish stanza type classification, and metrical pattern prediction for Spanish, English and German. In both cases, \textsc{Alberti} outperforms multilingual BERT and other transformers-based models of similar sizes, and even achieves state-of-the-art results for German when compared to rule-based systems, demonstrating the feasibility and effectiveness of DSP in the poetry domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#20869;&#23384;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#26032;&#30340;&#24038;&#19978;&#19979;&#25991;&#26041;&#27861;&#38544;&#24335;&#20445;&#30041;&#35760;&#24518;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35745;&#31639;&#39640;&#25928;&#30340;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#12290;&#22312;MuST-C&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#36895;&#24230;&#25552;&#21319;&#21644;&#23569;&#37327;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2307.01381</link><description>&lt;p&gt;
&#38544;&#24335;&#20869;&#23384;&#21464;&#25442;&#22120;&#29992;&#20110;&#35745;&#31639;&#39640;&#25928;&#30340;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation. (arXiv:2307.01381v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#20869;&#23384;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#26032;&#30340;&#24038;&#19978;&#19979;&#25991;&#26041;&#27861;&#38544;&#24335;&#20445;&#30041;&#35760;&#24518;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35745;&#31639;&#39640;&#25928;&#30340;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#12290;&#22312;MuST-C&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#36895;&#24230;&#25552;&#21319;&#21644;&#23569;&#37327;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20154;&#31867;&#20132;&#27969;&#20219;&#21153;&#65292;&#21363;&#22312;&#36827;&#34892;&#35821;&#38899;&#36755;&#20837;&#30340;&#21516;&#26102;&#29983;&#25104;&#32763;&#35793;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#27969;&#24335;&#20219;&#21153;&#65292;&#20351;&#29992;&#22359;&#22788;&#29702;&#23558;&#36755;&#20837;&#24207;&#21015;&#20998;&#21106;&#25104;&#29255;&#27573;&#30340;Transformer&#22312;&#38477;&#20302;&#25104;&#26412;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20801;&#35768;&#20449;&#24687;&#22312;&#29255;&#27573;&#20043;&#38388;&#20256;&#25773;&#65292;&#21253;&#25324;&#24038;&#19978;&#19979;&#25991;&#21644;&#23384;&#20648;&#22120;&#24211;&#65292;&#20294;&#23427;&#20204;&#26082;&#26159;&#19981;&#20805;&#20998;&#30340;&#34920;&#31034;&#21448;&#26159;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#20869;&#23384;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#24038;&#19978;&#19979;&#25991;&#26041;&#27861;&#38544;&#24335;&#20445;&#30041;&#35760;&#24518;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#38656;&#35201;&#29992;&#23384;&#20648;&#22120;&#24211;&#26174;&#24335;&#34920;&#31034;&#35760;&#24518;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#21069;&#19968;&#20010;&#29255;&#27573;&#30340;&#27880;&#24847;&#21147;&#36755;&#20986;&#29983;&#25104;&#24038;&#19978;&#19979;&#25991;&#65292;&#24182;&#23558;&#20854;&#21253;&#21547;&#22312;&#24403;&#21069;&#29255;&#27573;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#38190;&#21644;&#20540;&#20013;&#12290;&#23545;MuST-C&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#38544;&#24335;&#20869;&#23384;&#21464;&#25442;&#22120;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#36895;&#24230;&#25552;&#21319;&#21644;&#23569;&#37327;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous speech translation is an essential communication task difficult for humans whereby a translation is generated concurrently with oncoming speech inputs. For such a streaming task, transformers using block processing to break an input sequence into segments have achieved state-of-the-art performance at a reduced cost. Current methods to allow information to propagate across segments, including left context and memory banks, have faltered as they are both insufficient representations and unnecessarily expensive to compute. In this paper, we propose an Implicit Memory Transformer that implicitly retains memory through a new left context method, removing the need to explicitly represent memory with memory banks. We generate the left context from the attention output of the previous segment and include it in the keys and values of the current segment's attention calculation. Experiments on the MuST-C dataset show that the Implicit Memory Transformer provides a substantial speedu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#29983;&#25104;&#30340;&#20851;&#38190;&#35789;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#37325;&#35201;&#30340;&#20196;&#29260;&#21644;&#21547;&#26377;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#34987;&#21516;&#31561;&#25110;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01379</link><description>&lt;p&gt;
&#23558;&#20851;&#27880;&#28857;&#36716;&#31227;&#21040;&#30456;&#20851;&#24615;&#19978;: &#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models. (arXiv:2307.01379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#29983;&#25104;&#30340;&#20851;&#38190;&#35789;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#37325;&#35201;&#30340;&#20196;&#29260;&#21644;&#21547;&#26377;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#34987;&#21516;&#31561;&#25110;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#23545;&#20110;&#27169;&#22411;&#29983;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#29305;&#24449;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#29992;&#25143;&#20309;&#26102;&#21487;&#20197;&#20449;&#20219;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#19968;&#20123;&#21551;&#21457;&#24615;&#30340;&#20107;&#23454;&#65292;&#21363;&#22312;&#33258;&#22238;&#24402;&#30340;LLMs&#20013;&#65292;&#20196;&#29260;&#22312;&#21453;&#26144;&#29983;&#25104;&#30340;&#21547;&#20041;&#26041;&#38754;&#26159;&#19981;&#24179;&#31561;&#30340;&#65292;&#21363;&#19968;&#20123;&#20196;&#29260;&#27604;&#20854;&#20182;&#20196;&#29260;&#26356;&#30456;&#20851;&#65288;&#25110;&#26356;&#20855;&#20195;&#34920;&#24615;&#65289;&#65292;&#28982;&#32780;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#25152;&#26377;&#30340;&#20196;&#29260;&#34987;&#31561;&#20540;&#23545;&#24453;&#12290;&#36825;&#26159;&#30001;&#20110;&#35821;&#35328;&#20887;&#20313;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#65292;&#21482;&#38656;&#35201;&#20960;&#20010;&#20851;&#38190;&#35789;&#23601;&#36275;&#20197;&#20256;&#36798;&#19968;&#20010;&#38271;&#21477;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#19981;&#24179;&#31561;&#31216;&#20026;&#29983;&#25104;&#30340;&#19981;&#24179;&#31561;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#19981;&#30830;&#23450;&#24615;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#65292;&#30456;&#24403;&#25968;&#37327;&#30340;&#20196;&#29260;&#21644;&#21253;&#21547;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#65292;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#34987;&#21516;&#31561;&#25110;&#29978;&#33267;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#29983;&#25104;&#30340;&#19981;&#24179;&#31561;&#24341;&#36215;&#30340;&#36825;&#20123;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#21487;&#36716;&#31227;&#19978;&#19979;&#25991;"&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#20013;&#30340;&#35757;&#32451;-&#25512;&#29702;&#19978;&#19979;&#25991;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#36890;&#36807;&#20445;&#25345;&#19968;&#33268;&#30340;&#27573;&#33853;&#21644;&#19978;&#19979;&#25991;&#22823;&#23567;&#65292;&#21363;&#20351;&#23384;&#22312;&#37096;&#20998;&#22635;&#20805;&#30340;&#27573;&#33853;&#65292;&#35813;&#26041;&#26696;&#22312;&#27969;&#24335;&#20219;&#21153;&#30340;&#20998;&#27573;Transformer&#20013;&#20063;&#26159;&#24191;&#27867;&#36866;&#29992;&#30340;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24212;&#29992;&#20110;Augmented Memory Transformer&#21518;&#21487;&#20197;&#25552;&#39640;BLEU&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.01377</link><description>&lt;p&gt;
&#21487;&#36716;&#31227;&#19978;&#19979;&#25991;&#65306;&#35299;&#20915;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#20013;&#30340;&#35757;&#32451;-&#25512;&#29702;&#19978;&#19979;&#25991;&#19981;&#21305;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation. (arXiv:2307.01377v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01377
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#21487;&#36716;&#31227;&#19978;&#19979;&#25991;"&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#20013;&#30340;&#35757;&#32451;-&#25512;&#29702;&#19978;&#19979;&#25991;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#36890;&#36807;&#20445;&#25345;&#19968;&#33268;&#30340;&#27573;&#33853;&#21644;&#19978;&#19979;&#25991;&#22823;&#23567;&#65292;&#21363;&#20351;&#23384;&#22312;&#37096;&#20998;&#22635;&#20805;&#30340;&#27573;&#33853;&#65292;&#35813;&#26041;&#26696;&#22312;&#27969;&#24335;&#20219;&#21153;&#30340;&#20998;&#27573;Transformer&#20013;&#20063;&#26159;&#24191;&#27867;&#36866;&#29992;&#30340;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24212;&#29992;&#20110;Augmented Memory Transformer&#21518;&#21487;&#20197;&#25552;&#39640;BLEU&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#20013;&#65292;&#20351;&#29992;&#20998;&#27573;&#22788;&#29702;&#30340;Transformer&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#29615;&#22659;&#20043;&#38388;&#21019;&#24314;&#20102;&#19978;&#19979;&#25991;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#38459;&#30861;&#20102;&#28508;&#22312;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#21487;&#36716;&#31227;&#19978;&#19979;&#25991;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#26696;&#65292;&#21487;&#20197;&#30830;&#20445;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#22987;&#32456;&#32500;&#25345;&#19968;&#33268;&#30340;&#27573;&#33853;&#21644;&#19978;&#19979;&#25991;&#22823;&#23567;&#65292;&#21363;&#20351;&#30001;&#20110;&#27969;&#24335;&#32763;&#35793;&#30340;&#24615;&#36136;&#23548;&#33268;&#37096;&#20998;&#22635;&#20805;&#30340;&#27573;&#33853;&#23384;&#22312;&#12290;&#21487;&#36716;&#31227;&#19978;&#19979;&#25991;&#22312;&#27969;&#24335;&#20219;&#21153;&#30340;&#20998;&#27573;Transformer&#20013;&#20063;&#26159;&#24191;&#27867;&#36866;&#29992;&#30340;&#12290;&#25105;&#20204;&#22312;MUST-C&#25968;&#25454;&#38598;&#30340;&#33521;&#24503;&#12289;&#33521;&#27861;&#21644;&#33521;&#35199;&#35821;&#35328;&#23545;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#35813;&#26041;&#26696;&#24212;&#29992;&#20110;Augmented Memory Transformer&#65288;&#19968;&#31181;&#29992;&#20110;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65289;&#21487;&#20197;&#24179;&#22343;&#25552;&#39640;2.09&#12289;1.83&#21644;1.95&#20010;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models using segment-based processing have been an effective architecture for simultaneous speech translation. However, such models create a context mismatch between training and inference environments, hindering potential translation accuracy. We solve this issue by proposing Shiftable Context, a simple yet effective scheme to ensure that consistent segment and context sizes are maintained throughout training and inference, even with the presence of partially filled segments due to the streaming nature of simultaneous translation. Shiftable Context is also broadly applicable to segment-based transformers for streaming tasks. Our experiments on the English-German, English-French, and English-Spanish language pairs from the MUST-C dataset demonstrate that when applied to the Augmented Memory Transformer, a state-of-the-art model for simultaneous speech translation, the proposed scheme achieves an average increase of 2.09, 1.83, and 1.95 BLEU scores across each wait-k value f
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26410;&#33021;&#25104;&#21151;&#23398;&#20064;&#21040;&#25991;&#21270;&#36866;&#23452;&#24773;&#32490;&#30340;&#24494;&#22937;&#24046;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#32416;&#27491;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#33021;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.01370</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#24182;&#38750;&#22810;&#20803;&#25991;&#21270;&#30340;: &#20197;&#24773;&#32490;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multilingual Language Models are not Multicultural: A Case Study in Emotion. (arXiv:2307.01370v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01370
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26410;&#33021;&#25104;&#21151;&#23398;&#20064;&#21040;&#25991;&#21270;&#36866;&#23452;&#24773;&#32490;&#30340;&#24494;&#22937;&#24046;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#32416;&#27491;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#33021;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#22312;&#19990;&#30028;&#21508;&#22320;&#30340;&#32463;&#21382;&#21644;&#34920;&#36798;&#26041;&#24335;&#19981;&#21516;&#12290;&#20026;&#20102;&#22312;&#38656;&#35201;&#24773;&#24863;&#25935;&#24863;&#24615;&#30340;&#22810;&#35821;&#35328;&#20219;&#21153;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LMs)&#65292;LMs&#24517;&#39035;&#21453;&#26144;&#24773;&#32490;&#19978;&#30340;&#25991;&#21270;&#24046;&#24322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;2023&#24180;&#24191;&#27867;&#20351;&#29992;&#30340;&#22810;&#35821;&#35328;LMs&#26159;&#21542;&#21453;&#26144;&#20102;&#36328;&#25991;&#21270;&#21644;&#36328;&#35821;&#35328;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#20174;LMs(&#22914;XLM-RoBERTa)&#33719;&#24471;&#30340;&#23884;&#20837;&#26159;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#65292;&#29983;&#25104;&#22411;LMs(&#22914;ChatGPT)&#22312;&#22238;&#24212;&#20854;&#20182;&#35821;&#35328;&#30340;&#25552;&#31034;&#26102;&#20063;&#20307;&#29616;&#20102;&#35199;&#26041;&#35268;&#33539;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;LMs&#24182;&#27809;&#26377;&#25104;&#21151;&#22320;&#23398;&#20064;&#25991;&#21270;&#19978;&#36866;&#24403;&#30340;&#24773;&#32490;&#32454;&#24494;&#24046;&#21035;&#65292;&#24182;&#24378;&#35843;&#20102;&#21487;&#33021;&#30340;&#30740;&#31350;&#26041;&#21521;&#20197;&#32416;&#27491;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotions are experienced and expressed differently across the world. In order to use Large Language Models (LMs) for multilingual tasks that require emotional sensitivity, LMs must reflect this cultural variation in emotion. In this study, we investigate whether the widely-used multilingual LMs in 2023 reflect differences in emotional expressions across cultures and languages. We find that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric, and generative LMs (e.g., ChatGPT) reflect Western norms, even when responding to prompts in other languages. Our results show that multilingual LMs do not successfully learn the culturally appropriate nuances of emotion and we highlight possible research directions towards correcting this.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#19978;&#19987;&#27880;&#20110;&#23569;&#37327;&#36716;&#24405;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#20041;&#20016;&#23500;&#21270;&#65292;&#23545;SAMU-XLSR&#27169;&#22411;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#21516;&#26102;&#36824;&#25506;&#32034;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#21487;&#31227;&#26893;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#20016;&#23500;&#30340;SAMU-XLSR&#30340;&#36328;&#39046;&#22495;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.01323</link><description>&lt;p&gt;
&#21521;&#35821;&#20041;&#20016;&#23500;&#21270;&#30340;&#26377;&#25928;&#35821;&#38899;&#34920;&#31034;&#26041;&#27861;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Semantic enrichment towards efficient speech representations. (arXiv:2307.01323v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01323
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#19978;&#19987;&#27880;&#20110;&#23569;&#37327;&#36716;&#24405;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#20041;&#20016;&#23500;&#21270;&#65292;&#23545;SAMU-XLSR&#27169;&#22411;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#21516;&#26102;&#36824;&#25506;&#32034;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#21487;&#31227;&#26893;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#20016;&#23500;&#30340;SAMU-XLSR&#30340;&#36328;&#39046;&#22495;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35821;&#38899;&#34920;&#31034;&#22312;&#35299;&#20915;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#26102;&#24050;&#25104;&#20026;&#20256;&#32479;&#34920;&#38754;&#34920;&#31034;&#30340;&#26377;&#25928;&#26367;&#20195;&#21697;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#34987;&#24341;&#20837;&#20197;&#32534;&#30721;&#35821;&#35328;&#26080;&#20851;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#26368;&#36817;&#65292;SAMU-XLSR&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36825;&#31181;&#25991;&#26412;&#27169;&#22411;&#20174;&#32780;&#20351;&#22810;&#35821;&#35328;&#35821;&#38899;&#34920;&#31034;&#22686;&#21152;&#35821;&#35328;&#26080;&#20851;&#35821;&#20041;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#24182;&#32771;&#34385;&#35745;&#31639;&#25104;&#26412;&#65292;&#36890;&#36807;&#19987;&#27880;&#20110;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#23569;&#37327;&#36716;&#24405;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#20041;&#20016;&#23500;&#21270;&#65292;&#23545;SAMU-XLSR&#27169;&#22411;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#21487;&#31227;&#26893;&#24615;&#26041;&#38754;&#65292;&#20351;&#29992;&#21516;&#39046;&#22495;&#30340;&#27861;&#35821;&#21644;&#24847;&#22823;&#21033;&#35821;&#22522;&#20934;&#30340;&#22909;&#22788;&#65292;&#36824;&#25506;&#32034;&#20102;&#20016;&#23500;&#30340;SAMU-XLSR&#30340;&#36328;&#39046;&#22495;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, self-supervised learned speech representations have emerged as fruitful replacements for conventional surface representations when solving Spoken Language Understanding (SLU) tasks. Simultaneously, multilingual models trained on massive textual data were introduced to encode language agnostic semantics. Recently, the SAMU-XLSR approach introduced a way to make profit from such textual models to enrich multilingual speech representations with language agnostic semantics. By aiming for better semantic extraction on a challenging Spoken Language Understanding task and in consideration with computation costs, this study investigates a specific in-domain semantic enrichment of the SAMU-XLSR model by specializing it on a small amount of transcribed data from the downstream task. In addition, we show the benefits of the use of same-domain French and Italian benchmarks for low-resource language portability and explore cross-domain capacities of the enriched SAMU-XLSR.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21475;&#36848;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#36328;&#35821;&#35328;&#35270;&#35282;&#12290;&#36890;&#36807;&#20351;&#29992;&#33655;&#20848;&#35821;&#12289;&#33521;&#35821;&#21644;&#24503;&#35821;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#20197;&#21450;&#31649;&#36947;&#21644;&#31471;&#21040;&#31471;&#26041;&#26696;&#65292;&#21033;&#29992;&#33258;&#23450;&#20041;&#30340;&#20266;&#26631;&#27880;&#25968;&#25454;&#38598;&#21644;Wav2Vec2-XLS-R&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20960;&#31181;&#36866;&#24212;&#36328;&#35821;&#35328;&#31995;&#32479;&#30340;&#26550;&#26500;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#31471;&#21040;&#31471;&#21475;&#36848;NER&#22312;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#20248;&#20110;&#31649;&#36947;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20174;&#24503;&#35821;&#21040;&#33655;&#20848;&#35821;&#30340;&#36801;&#31227;&#23398;&#20064;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#36229;&#36807;&#20102;&#33655;&#20848;&#35821;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01310</link><description>&lt;p&gt;
&#25506;&#32034;&#21475;&#36848;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65306;&#36328;&#35821;&#35328;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Exploring Spoken Named Entity Recognition: A Cross-Lingual Perspective. (arXiv:2307.01310v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21475;&#36848;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#36328;&#35821;&#35328;&#35270;&#35282;&#12290;&#36890;&#36807;&#20351;&#29992;&#33655;&#20848;&#35821;&#12289;&#33521;&#35821;&#21644;&#24503;&#35821;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#20197;&#21450;&#31649;&#36947;&#21644;&#31471;&#21040;&#31471;&#26041;&#26696;&#65292;&#21033;&#29992;&#33258;&#23450;&#20041;&#30340;&#20266;&#26631;&#27880;&#25968;&#25454;&#38598;&#21644;Wav2Vec2-XLS-R&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20960;&#31181;&#36866;&#24212;&#36328;&#35821;&#35328;&#31995;&#32479;&#30340;&#26550;&#26500;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#31471;&#21040;&#31471;&#21475;&#36848;NER&#22312;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#20248;&#20110;&#31649;&#36947;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20174;&#24503;&#35821;&#21040;&#33655;&#20848;&#35821;&#30340;&#36801;&#31227;&#23398;&#20064;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#36229;&#36807;&#20102;&#33655;&#20848;&#35821;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#25968;&#25454;&#20013;&#23454;&#20307;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21475;&#36848;NER&#20316;&#20026;&#21475;&#36848;&#25991;&#26723;&#26816;&#32034;&#30340;&#19987;&#38376;&#39046;&#22495;&#65292;&#30001;&#20110;&#30740;&#31350;&#26377;&#38480;&#21644;&#25968;&#25454;&#31232;&#32570;&#32780;&#28382;&#21518;&#12290;&#27492;&#22806;&#65292;&#21475;&#36848;NER&#20013;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#21033;&#29992;&#33655;&#20848;&#35821;&#12289;&#33521;&#35821;&#21644;&#24503;&#35821;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#20351;&#29992;&#31649;&#36947;&#21644;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#26041;&#26696;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#23450;&#20041;&#30340;&#20266;&#26631;&#27880;&#25968;&#25454;&#38598;&#20351;&#29992;Wav2Vec2-XLS-R&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#20960;&#31181;&#36866;&#24212;&#36328;&#35821;&#35328;&#31995;&#32479;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31471;&#21040;&#31471;&#21475;&#36848;NER&#22312;&#25105;&#20204;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#19978;&#20248;&#20110;&#22522;&#20110;&#31649;&#36947;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20174;&#24503;&#35821;&#21040;&#33655;&#20848;&#35821;&#30340;&#36801;&#31227;&#23398;&#20064;&#36229;&#36807;&#20102;&#33655;&#20848;&#35821;E2E&#31995;&#32479;7%&#21644;&#33655;&#20848;&#35821;&#31649;&#36947;&#31995;&#32479;4%&#12290;&#36825;&#39033;&#30740;&#31350;&#19981;&#20165;&#31361;&#20986;&#20102;&#21475;&#36848;NER&#20013;&#36801;&#31227;&#23398;&#20064;&#30340;&#21487;&#34892;&#24615;&#65292;&#32780;&#19988;&#20026;&#26410;&#26469;&#30340;&#35780;&#20272;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Named Entity Recognition (NER) have significantly improved the identification of entities in textual data. However, spoken NER, a specialized field of spoken document retrieval, lags behind due to its limited research and scarce datasets. Moreover, cross-lingual transfer learning in spoken NER has remained unexplored. This paper utilizes transfer learning across Dutch, English, and German using pipeline and End-to-End (E2E) schemes. We employ Wav2Vec2-XLS-R models on custom pseudo-annotated datasets and investigate several architectures for the adaptability of cross-lingual systems. Our results demonstrate that End-to-End spoken NER outperforms pipeline-based alternatives over our limited annotations. Notably, transfer learning from German to Dutch surpasses the Dutch E2E system by 7% and the Dutch pipeline system by 4%. This study not only underscores the feasibility of transfer learning in spoken NER but also sets promising outcomes for future evaluations, hint
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#36153;&#22478;&#26597;&#38382;&#25253;&#21313;&#24180;&#38388;&#21457;&#34920;&#30340;157,476&#31687;&#25991;&#31456;&#65292;&#31361;&#20986;&#20102;&#23545;&#29289;&#36136;&#20351;&#29992;&#21644;&#25104;&#30270;&#30340;&#20934;&#30830;&#21644;&#21253;&#23481;&#24615;&#25551;&#36848;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01299</link><description>&lt;p&gt;
&#36153;&#22478;&#26597;&#38382;&#25253;&#20013;&#29289;&#36136;&#20351;&#29992;&#25253;&#36947;&#30340;&#28436;&#21464;
&lt;/p&gt;
&lt;p&gt;
The Evolution of Substance Use Coverage in the Philadelphia Inquirer. (arXiv:2307.01299v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01299
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#36153;&#22478;&#26597;&#38382;&#25253;&#21313;&#24180;&#38388;&#21457;&#34920;&#30340;157,476&#31687;&#25991;&#31456;&#65292;&#31361;&#20986;&#20102;&#23545;&#29289;&#36136;&#20351;&#29992;&#21644;&#25104;&#30270;&#30340;&#20934;&#30830;&#21644;&#21253;&#23481;&#24615;&#25551;&#36848;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23186;&#20307;&#23545;&#38750;&#27861;&#29289;&#36136;&#20351;&#29992;&#30340;&#34920;&#36848;&#21487;&#33021;&#23548;&#33268;&#23545;&#19982;&#25104;&#30270;&#26007;&#20105;&#30340;&#20010;&#20307;&#20135;&#29983;&#26377;&#23475;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#27745;&#21517;&#21270;&#65292;&#26368;&#32456;&#24433;&#21709;&#20844;&#20247;&#23545;&#24453;&#25104;&#30270;&#38382;&#39064;&#30340;&#30475;&#27861;&#12289;&#25919;&#31574;&#21644;&#20844;&#20849;&#21355;&#29983;&#32467;&#26524;&#12290;&#20026;&#20102;&#25506;&#32034;&#38750;&#27861;&#33647;&#29289;&#20351;&#29992;&#30340;&#35328;&#35770;&#21644;&#25253;&#36947;&#22914;&#20309;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#36153;&#22478;&#26597;&#38382;&#25253;&#22312;&#21313;&#24180;&#38388;&#21457;&#34920;&#30340;157,476&#31687;&#25991;&#31456;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#30740;&#31350;&#32858;&#28966;&#20110;&#25552;&#21450;&#33267;&#23569;&#19968;&#31181;&#24120;&#35265;&#28389;&#29992;&#29289;&#36136;&#30340;&#25991;&#31456;&#65292;&#24471;&#21040;&#20102;3,903&#31687;&#25991;&#31456;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#22823;&#40635;&#21644;&#40635;&#37257;&#33647;&#26159;&#26368;&#32463;&#24120;&#34987;&#35752;&#35770;&#30340;&#27602;&#21697;&#31867;&#21035;&#12290;&#24187;&#35273;&#33647;&#29289;&#27604;&#20854;&#20182;&#31867;&#21035;&#26356;&#27491;&#38754;&#22320;&#21576;&#29616;&#65292;&#32780;&#40635;&#37257;&#33647;&#21017;&#26159;&#26368;&#36127;&#38754;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#31361;&#20986;&#23186;&#20307;&#23545;&#29289;&#36136;&#20351;&#29992;&#21644;&#25104;&#30270;&#30340;&#20934;&#30830;&#21644;&#21253;&#23481;&#24615;&#25551;&#36848;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The media's representation of illicit substance use can lead to harmful stereotypes and stigmatization for individuals struggling with addiction, ultimately influencing public perception, policy, and public health outcomes. To explore how the discourse and coverage of illicit drug use changed over time, this study analyzes 157,476 articles published in the Philadelphia Inquirer over a decade. Specifically, the study focuses on articles that mentioned at least one commonly abused substance, resulting in a sample of 3,903 articles. Our analysis shows that cannabis and narcotics are the most frequently discussed classes of drugs. Hallucinogenic drugs are portrayed more positively than other categories, whereas narcotics are portrayed the most negatively. Our research aims to highlight the need for accurate and inclusive portrayals of substance use and addiction in the media.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#25991;&#26412;&#21040;&#19977;&#32500;&#27169;&#22411;&#22312;&#24037;&#31243;&#35774;&#35745;&#20248;&#21270;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#36827;&#21270;&#35774;&#35745;&#20248;&#21270;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2307.01230</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#21644;&#25991;&#26412;&#21040;&#19977;&#32500;&#27169;&#22411;&#29992;&#20110;&#24037;&#31243;&#35774;&#35745;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large Language and Text-to-3D Models for Engineering Design Optimization. (arXiv:2307.01230v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#25991;&#26412;&#21040;&#19977;&#32500;&#27169;&#22411;&#22312;&#24037;&#31243;&#35774;&#35745;&#20248;&#21270;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#36827;&#21270;&#35774;&#35745;&#20248;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#23398;&#20064;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#20855;&#26377;&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#35770;&#25991;&#12289;&#22270;&#20687;&#12289;&#38899;&#20048;&#29978;&#33267;&#19977;&#32500;&#36164;&#20135;&#30340;&#33021;&#21147;&#65292;&#20026;&#22810;&#23398;&#31185;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#25991;&#26412;&#21040;&#19977;&#32500;&#27169;&#22411;&#22312;&#24037;&#31243;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#35745;&#31639;&#27169;&#25311;&#35774;&#35745;&#20248;&#21270;&#20013;&#25972;&#21512;&#21644;&#20132;&#20114;&#19977;&#32500;&#36164;&#20135;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25968;&#20540;&#34920;&#31034;&#30340;&#19977;&#32500;&#20960;&#20309;&#35774;&#35745;&#20248;&#21270;&#19981;&#21516;&#65292;&#33258;&#28982;&#35821;&#35328;&#35201;&#27714;&#23545;&#21464;&#24322;&#31639;&#23376;&#26377;&#19981;&#21516;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#20943;&#36731;&#21644;&#28608;&#21457;&#20154;&#31867;&#29992;&#25143;&#30340;&#20132;&#20114;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#30340;&#36827;&#21270;&#35774;&#35745;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#20102;&#26368;&#36817;&#25512;&#20986;&#30340;Shap-E&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current advances in generative AI for learning large neural network models with the capability to produce essays, images, music and even 3D assets from text prompts create opportunities for a manifold of disciplines. In the present paper, we study the potential of deep text-to-3D models in the engineering domain, with focus on the chances and challenges when integrating and interacting with 3D assets in computational simulation-based design optimization. In contrast to traditional design optimization of 3D geometries that often searches for the optimum designs using numerical representations, such as B-Spline surface or deformation parameters in vehicle aerodynamic optimization, natural language challenges the optimization framework by requiring a different interpretation of variation operators while at the same time may ease and motivate the human user interaction. Here, we propose and realize a fully automated evolutionary design optimization framework using Shap-E, a recently pu
&lt;/p&gt;</description></item><item><title>vONTSS&#26159;&#19968;&#31181;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#27604;&#26368;&#36817;&#30340;NTM&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2307.01226</link><description>&lt;p&gt;
vONTSS&#65306;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01226
&lt;/p&gt;
&lt;p&gt;
vONTSS&#26159;&#19968;&#31181;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#27604;&#26368;&#36817;&#30340;NTM&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21463;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21551;&#21457;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;NTM&#65289;&#24341;&#36215;&#20102;&#24456;&#22810;&#30740;&#31350;&#20852;&#36259;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;vONTSS&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;von Mises-Fisher&#65288;vMF&#65289;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#26368;&#20248;&#20256;&#36755;&#12290;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#24403;&#25552;&#20379;&#27599;&#20010;&#20027;&#39064;&#30340;&#23569;&#37327;&#20851;&#38190;&#35789;&#26102;&#65292;vONTSS&#29983;&#25104;&#28508;&#22312;&#20027;&#39064;&#24182;&#20248;&#21270;&#20027;&#39064;-&#20851;&#38190;&#35789;&#36136;&#37327;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;vONTSS&#36824;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#22312;&#22810;&#20010;&#26041;&#38754;&#20248;&#20110;&#26368;&#36817;&#30340;NTM&#65306;vONTSS&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#39640;&#24230;&#32858;&#31867;&#21644;&#36830;&#36143;&#30340;&#20027;&#39064;&#12290;&#23427;&#20063;&#27604;&#29616;&#26377;-&#25163;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#26816;&#27979;&#21644;&#36716;&#25442;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#26041;&#38754;&#27880;&#37325;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#22270;&#12289;&#38598;&#25104;&#26799;&#24230;&#21644;&#27169;&#22411;&#21453;&#39304;&#31561;&#25216;&#26415;&#65292;&#22312;&#26816;&#27979;&#38454;&#27573;&#26377;&#21161;&#20110;&#35782;&#21035;&#23545;&#23545;&#25239;&#24615;&#20998;&#31867;&#26377;&#36129;&#29486;&#30340;&#26174;&#33879;&#29305;&#24449;&#21644;&#25200;&#21160;&#35789;&#35821;&#65292;&#24182;&#22312;&#36716;&#25442;&#38454;&#27573;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#27169;&#22411;&#21453;&#39304;&#26469;&#29983;&#25104;&#25200;&#21160;&#35789;&#35821;&#30340;&#26368;&#20339;&#26367;&#20195;&#65292;&#20197;&#23558;&#23545;&#25239;&#24615;&#31034;&#20363;&#36716;&#25442;&#20026;&#27491;&#24120;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.01225</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;
&lt;/p&gt;
&lt;p&gt;
Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT). (arXiv:2307.01225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01225
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#26816;&#27979;&#21644;&#36716;&#25442;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#26041;&#38754;&#27880;&#37325;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#22270;&#12289;&#38598;&#25104;&#26799;&#24230;&#21644;&#27169;&#22411;&#21453;&#39304;&#31561;&#25216;&#26415;&#65292;&#22312;&#26816;&#27979;&#38454;&#27573;&#26377;&#21161;&#20110;&#35782;&#21035;&#23545;&#23545;&#25239;&#24615;&#20998;&#31867;&#26377;&#36129;&#29486;&#30340;&#26174;&#33879;&#29305;&#24449;&#21644;&#25200;&#21160;&#35789;&#35821;&#65292;&#24182;&#22312;&#36716;&#25442;&#38454;&#27573;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#27169;&#22411;&#21453;&#39304;&#26469;&#29983;&#25104;&#25200;&#21160;&#35789;&#35821;&#30340;&#26368;&#20339;&#26367;&#20195;&#65292;&#20197;&#23558;&#23545;&#25239;&#24615;&#31034;&#20363;&#36716;&#25442;&#20026;&#27491;&#24120;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#22914;BERT&#12289;Roberta&#12289;T5&#21644;GPT-3&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20110;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#33030;&#24369;&#24615;&#25552;&#20986;&#20102;&#23433;&#20840;&#39118;&#38505;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#24456;&#38590;&#29702;&#35299;&#23545;&#25239;&#24615;&#20998;&#31867;&#24182;&#35782;&#21035;&#27169;&#22411;&#30340;&#28431;&#27934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;&#26694;&#26550;&#12290;&#23427;&#19987;&#27880;&#20110;&#22312;&#26816;&#27979;&#21644;&#36716;&#25442;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#26102;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#12290;IT-DT&#21033;&#29992;&#27880;&#24847;&#21147;&#22270;&#12289;&#38598;&#25104;&#26799;&#24230;&#21644;&#27169;&#22411;&#21453;&#39304;&#31561;&#25216;&#26415;&#36827;&#34892;&#35299;&#37322;&#24615;&#26816;&#27979;&#12290;&#36825;&#26377;&#21161;&#20110;&#35782;&#21035;&#23545;&#23545;&#25239;&#24615;&#20998;&#31867;&#26377;&#36129;&#29486;&#30340;&#26174;&#33879;&#29305;&#24449;&#21644;&#25200;&#21160;&#35789;&#35821;&#12290;&#22312;&#36716;&#25442;&#38454;&#27573;&#65292;IT-DT&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#27169;&#22411;&#21453;&#39304;&#26469;&#29983;&#25104;&#25200;&#21160;&#35789;&#35821;&#30340;&#26368;&#20339;&#26367;&#20195;&#12290;&#36890;&#36807;&#25214;&#21040;&#21512;&#36866;&#30340;&#26367;&#25442;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#23545;&#25239;&#24615;&#31034;&#20363;&#36716;&#25442;&#20026;&#27491;&#24120;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based text classifiers like BERT, Roberta, T5, and GPT-3 have shown impressive performance in NLP. However, their vulnerability to adversarial examples poses a security risk. Existing defense methods lack interpretability, making it hard to understand adversarial classifications and identify model vulnerabilities. To address this, we propose the Interpretability and Transparency-Driven Detection and Transformation (IT-DT) framework. It focuses on interpretability and transparency in detecting and transforming textual adversarial examples. IT-DT utilizes techniques like attention maps, integrated gradients, and model feedback for interpretability during detection. This helps identify salient features and perturbed words contributing to adversarial classifications. In the transformation phase, IT-DT uses pre-trained embeddings and model feedback to generate optimal replacements for perturbed words. By finding suitable substitutions, we aim to convert adversarial examples into
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#21457;&#29616;&#23450;&#20041;&#21644;&#26041;&#27861;&#27169;&#24335;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#22312;&#35821;&#20041;&#12289;&#21477;&#27861;&#21644;&#35789;&#27719;&#23618;&#38754;&#19978;&#20445;&#35777;&#20102;&#27169;&#24335;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01216</link><description>&lt;p&gt;
&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#21457;&#29616;&#23450;&#20041;&#21644;&#26041;&#27861;&#30340;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Discovering Patterns of Definitions and Methods from Scientific Documents. (arXiv:2307.01216v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#21457;&#29616;&#23450;&#20041;&#21644;&#26041;&#27861;&#27169;&#24335;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#22312;&#35821;&#20041;&#12289;&#21477;&#27861;&#21644;&#35789;&#27719;&#23618;&#38754;&#19978;&#20445;&#35777;&#20102;&#27169;&#24335;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#33258;&#21160;&#25552;&#21462;&#23450;&#20041;&#21644;&#26041;&#27861;&#30340;&#22256;&#38590;&#22312;&#20110;&#20004;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#35201;&#27714;&#19968;&#20010;&#20998;&#26512;&#26041;&#27861;&#26469;&#25903;&#25345;&#27169;&#24335;&#30340;&#21457;&#29616;&#65307;&#65288;2&#65289;&#31185;&#23398;&#35770;&#25991;&#20013;&#23436;&#25972;&#30340;&#23450;&#20041;&#25110;&#26041;&#27861;&#36890;&#24120;&#20998;&#24067;&#22312;&#25991;&#26412;&#20013;&#65292;&#22240;&#27492;&#19968;&#20010;&#26377;&#25928;&#30340;&#26041;&#27861;&#19981;&#20165;&#24212;&#35813;&#25552;&#21462;&#21333;&#20010;&#21477;&#23376;&#30340;&#23450;&#20041;&#21644;&#26041;&#27861;&#65292;&#36824;&#24212;&#35813;&#25972;&#21512;&#36825;&#20123;&#21477;&#23376;&#20197;&#33719;&#24471;&#23436;&#25972;&#30340;&#23450;&#20041;&#25110;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#23450;&#20041;&#21644;&#26041;&#27861;&#30340;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#35813;&#26041;&#27861;&#21457;&#29616;&#23450;&#20041;&#21644;&#26041;&#27861;&#30340;&#27169;&#24335;&#12290;&#35821;&#20041;&#23618;&#38754;&#19978;&#30340;&#27169;&#24335;&#30340;&#23436;&#25972;&#24615;&#30001;&#19968;&#32452;&#23436;&#25972;&#30340;&#35821;&#20041;&#20851;&#31995;&#20445;&#35777;&#65292;&#36825;&#20123;&#35821;&#20041;&#20851;&#31995;&#20998;&#21035;&#26631;&#35782;&#23450;&#20041;&#21644;&#26041;&#27861;&#12290;&#22312;&#21477;&#27861;&#21644;&#35789;&#27719;&#23618;&#38754;&#19978;&#65292;&#27169;&#24335;&#30340;&#23436;&#25972;&#24615;&#30001;&#21477;&#27861;&#21644;&#35789;&#27719;&#32422;&#26463;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The difficulties of automatic extraction of definitions and methods from scientific documents lie in two aspects: (1) the complexity and diversity of natural language texts, which requests an analysis method to support the discovery of pattern; and, (2) a complete definition or method represented by a scientific paper is usually distributed within text, therefore an effective approach should not only extract single sentence definitions and methods but also integrate the sentences to obtain a complete definition or method. This paper proposes an analysis method for discovering patterns of definition and method and uses the method to discover patterns of definition and method. Completeness of the patterns at the semantic level is guaranteed by a complete set of semantic relations that identify definitions and methods respectively. The completeness of the patterns at the syntactic and lexical levels is guaranteed by syntactic and lexical constraints. Experiments on the self-built dataset 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#35789;&#32452;&#25628;&#32034;&#30340;&#33258;&#21160;&#21453;&#20107;&#23454;&#25193;&#20805;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25429;&#25417;&#20851;&#38190;&#23383;&#32452;&#21512;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#25490;&#24207;&#26368;&#24433;&#21709;&#39044;&#27979;&#30340;&#32452;&#21512;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#30001;&#20110;&#21482;&#20851;&#27880;&#21333;&#20010;&#21333;&#35789;&#32780;&#23548;&#33268;&#30340;&#38169;&#35823;&#22240;&#26524;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.01214</link><description>&lt;p&gt;
&#22522;&#20110;&#21333;&#35789;&#32452;&#25628;&#32034;&#30340;&#40065;&#26834;&#25991;&#26412;&#20998;&#31867;&#30340;&#33258;&#21160;&#21453;&#20107;&#23454;&#25193;&#20805;
&lt;/p&gt;
&lt;p&gt;
Automatic Counterfactual Augmentation for Robust Text Classification Based on Word-Group Search. (arXiv:2307.01214v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01214
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#35789;&#32452;&#25628;&#32034;&#30340;&#33258;&#21160;&#21453;&#20107;&#23454;&#25193;&#20805;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25429;&#25417;&#20851;&#38190;&#23383;&#32452;&#21512;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#25490;&#24207;&#26368;&#24433;&#21709;&#39044;&#27979;&#30340;&#32452;&#21512;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#30001;&#20110;&#21482;&#20851;&#27880;&#21333;&#20010;&#21333;&#35789;&#32780;&#23548;&#33268;&#30340;&#38169;&#35823;&#22240;&#26524;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#23545;&#20110;&#25463;&#24452;&#23398;&#20064;&#30340;&#25361;&#25112;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#20851;&#38190;&#23383;&#19982;&#26631;&#31614;&#20135;&#29983;&#34920;&#38754;&#20851;&#32852;&#65292;&#20174;&#32780;&#23548;&#33268;&#38169;&#35823;&#39044;&#27979;&#65292;&#37027;&#20040;&#23427;&#34987;&#35270;&#20026;&#19968;&#31181;&#25463;&#24452;&#12290;&#30456;&#21453;&#65292;&#22914;&#26524;&#27169;&#22411;&#20381;&#36182;&#20110;&#33021;&#22815;&#20135;&#29983;&#20934;&#30830;&#39044;&#27979;&#30340;&#40065;&#26834;&#22240;&#26524;&#29305;&#24449;&#65292;&#23601;&#21487;&#20197;&#32531;&#35299;&#25463;&#24452;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#35768;&#22810;&#30740;&#31350;&#25506;&#32034;&#20102;&#20107;&#21518;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#25366;&#25496;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#25463;&#24452;&#21644;&#22240;&#26524;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#21482;&#20851;&#27880;&#21477;&#23376;&#20013;&#30340;&#21333;&#20010;&#21333;&#35789;&#65292;&#24573;&#35270;&#20102;&#21333;&#35789;&#32452;&#30340;&#32771;&#34385;&#65292;&#23548;&#33268;&#38169;&#35823;&#30340;&#22240;&#26524;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21333;&#35789;&#32452;&#25366;&#25496;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#20219;&#20309;&#20851;&#38190;&#23383;&#32452;&#21512;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#23545;&#26368;&#24433;&#21709;&#39044;&#27979;&#30340;&#32452;&#21512;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26377;&#25928;&#30340;&#20107;&#21518;&#20998;&#26512;&#21644;&#27874;&#26463;&#25628;&#32034;&#65292;&#30830;&#20445;&#20102;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite large-scale pre-trained language models have achieved striking results for text classificaion, recent work has raised concerns about the challenge of shortcut learning. In general, a keyword is regarded as a shortcut if it creates a superficial association with the label, resulting in a false prediction. Conversely, shortcut learning can be mitigated if the model relies on robust causal features that help produce sound predictions. To this end, many studies have explored post-hoc interpretable methods to mine shortcuts and causal features for robustness and generalization. However, most existing methods focus only on single word in a sentence and lack consideration of word-group, leading to wrong causal features. To solve this problem, we propose a new Word-Group mining approach, which captures the causal effect of any keyword combination and orders the combinations that most affect the prediction. Our approach bases on effective post-hoc analysis and beam search, which ensures
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#27861;&#24459;&#25991;&#20214;&#33258;&#21160;&#36716;&#21270;&#20026;&#26412;&#20307;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#26412;&#20307;&#21457;&#23637;&#21407;&#21017;&#30340;&#32467;&#21512;&#65292;&#23637;&#31034;&#20102;&#22312;&#27431;&#27954;&#32593;&#32476;&#21644;&#20449;&#24687;&#31995;&#32479;&#23433;&#20840;&#25351;&#20196;&#19978;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.01211</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#23433;&#20840;&#25351;&#20196;&#26412;&#20307;&#34920;&#31034;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An automated method for the ontological representation of security directives. (arXiv:2307.01211v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#27861;&#24459;&#25991;&#20214;&#33258;&#21160;&#36716;&#21270;&#20026;&#26412;&#20307;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#26412;&#20307;&#21457;&#23637;&#21407;&#21017;&#30340;&#32467;&#21512;&#65292;&#23637;&#31034;&#20102;&#22312;&#27431;&#27954;&#32593;&#32476;&#21644;&#20449;&#24687;&#31995;&#32479;&#23433;&#20840;&#25351;&#20196;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38590;&#20197;&#35299;&#37322;&#30340;&#22823;&#22411;&#27861;&#24459;&#25991;&#20214;&#65292;&#38271;&#21477;&#23548;&#33268;&#21517;&#35789;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#23558;&#36825;&#20010;&#38382;&#39064;&#25918;&#22312;&#26368;&#36817;&#27431;&#27954;&#23433;&#20840;&#25351;&#20196;&#30340;&#32972;&#26223;&#19979;&#12290;&#36890;&#36807;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#29305;&#23450;&#23450;&#21046;&#65292;&#33258;&#21160;&#21270;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#21363;&#27599;&#20010;&#20174;&#21477;&#30340;&#35789;&#31867;&#12290;&#36825;&#20123;&#19982;&#26412;&#20307;&#21457;&#23637;&#21407;&#21017;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20102;&#25105;&#20204;&#30340;&#23433;&#20840;&#25351;&#20196;&#26412;&#20307;&#34920;&#31034;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#23454;&#38469;&#38382;&#39064;&#19978;&#23637;&#31034;&#65292;&#21363;&#25512;&#23548;&#20986;&#34920;&#31034;&#27431;&#27954;&#23618;&#38754;&#19978;&#32593;&#32476;&#21644;&#20449;&#24687;&#31995;&#32479;&#23433;&#20840;&#25351;&#20196;&#30340;&#26412;&#20307;&#12290;&#23613;&#31649;&#37319;&#29992;&#30340;NLP&#25216;&#26415;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#24182;&#19988;&#38656;&#35201;&#36890;&#36807;&#25163;&#21160;&#20998;&#26512;&#36827;&#34892;&#34917;&#20805;&#65292;&#20294;&#24635;&#20307;&#32467;&#26524;&#20026;&#25351;&#20196;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large documents written in juridical language are difficult to interpret, with long sentences leading to intricate and intertwined relations between the nouns. The present paper frames this problem in the context of recent European security directives. The complexity of their language is here thwarted by automating the extraction of the relevant information, namely of the parts of speech from each clause, through a specific tailoring of Natural Language Processing (NLP) techniques. These contribute, in combination with ontology development principles, to the design of our automated method for the representation of security directives as ontologies. The method is showcased on a practical problem, namely to derive an ontology representing the NIS 2 directive, which is the peak of cybersecurity prescripts at the European level. Although the NLP techniques adopted showed some limitations and had to be complemented by manual analysis, the overall results provide valid support for directive 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27721;&#23383;&#38899;&#38901;&#23398;&#20013;&#33719;&#21462;&#22810;&#26041;&#35328;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#21644;&#24212;&#29992;&#26080;&#30417;&#30563;&#32858;&#31867;&#25216;&#26415;&#65292;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#25429;&#25417;&#36755;&#20837;&#26041;&#35328;&#30340;&#38899;&#20301;&#23545;&#27604;&#24182;&#23637;&#31034;&#21476;&#32769;&#30340;&#21407;&#22987;&#35821;&#35328;&#29305;&#24449;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.01209</link><description>&lt;p&gt;
&#27721;&#23383;&#38899;&#31995;&#30340;&#22810;&#26041;&#35328;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Dialectal Representation Learning of Sinitic Phonology. (arXiv:2307.01209v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01209
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27721;&#23383;&#38899;&#38901;&#23398;&#20013;&#33719;&#21462;&#22810;&#26041;&#35328;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#21644;&#24212;&#29992;&#26080;&#30417;&#30563;&#32858;&#31867;&#25216;&#26415;&#65292;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#25429;&#25417;&#36755;&#20837;&#26041;&#35328;&#30340;&#38899;&#20301;&#23545;&#27604;&#24182;&#23637;&#31034;&#21476;&#32769;&#30340;&#21407;&#22987;&#35821;&#35328;&#29305;&#24449;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#35821;&#35328;&#21644;&#38899;&#38901;&#31561;&#35937;&#24449;&#31995;&#32479;&#30340;&#34920;&#31034;&#21644;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#33258;&#24049;&#30340;&#33021;&#21147;&#12290;&#22312;&#27721;&#23383;&#21382;&#21490;&#38899;&#38901;&#23398;&#20013;&#65292;&#21487;&#20197;&#21463;&#30410;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#20219;&#21153;&#21253;&#25324;&#26041;&#35328;&#27604;&#36739;&#21644;&#21407;&#22987;&#35821;&#35328;&#31995;&#32479;&#30340;&#37325;&#24314;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33719;&#21462;&#27721;&#23383;&#38899;&#33410;&#30340;&#22810;&#26041;&#35328;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#32467;&#26500;&#21270;&#38899;&#38901;&#25968;&#25454;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#28982;&#21518;&#24212;&#29992;&#30693;&#35782;&#24211;&#23398;&#20064;&#20013;&#30340;BoxE&#25216;&#26415;&#12290;&#25105;&#20204;&#24212;&#29992;&#26080;&#30417;&#30563;&#30340;&#32858;&#31867;&#25216;&#26415;&#23545;&#25152;&#24471;&#21040;&#30340;&#34920;&#31034;&#36827;&#34892;&#35266;&#23519;&#65292;&#21457;&#29616;&#36825;&#20123;&#34920;&#31034;&#25429;&#25417;&#21040;&#36755;&#20837;&#26041;&#35328;&#30340;&#38899;&#20301;&#23545;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#20998;&#31867;&#22120;&#26469;&#36827;&#34892;&#26080;&#27861;&#35266;&#23519;&#30340;&#20013;&#21476;&#27721;&#35821;&#26631;&#31614;&#30340;&#25512;&#29702;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#34920;&#31034;&#25581;&#31034;&#21476;&#32769;&#30340;&#21407;&#22987;&#35821;&#35328;&#29305;&#24449;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#29992;&#20110;&#23545;&#30862;&#29255;&#21270;&#30340;&#27721;&#23383;&#38899;&#38901;&#36827;&#34892;&#34917;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques have shown their competence for representing and reasoning in symbolic systems such as language and phonology. In Sinitic Historical Phonology, notable tasks that could benefit from machine learning include the comparison of dialects and reconstruction of proto-languages systems. Motivated by this, this paper provides an approach for obtaining multi-dialectal representations of Sinitic syllables, by constructing a knowledge graph from structured phonological data, then applying the BoxE technique from knowledge base learning. We applied unsupervised clustering techniques to the obtained representations to observe that the representations capture phonemic contrast from the input dialects. Furthermore, we trained classifiers to perform inference of unobserved Middle Chinese labels, showing the representations' potential for indicating archaic, proto-language features. The representations can be used for performing completion of fragmented Sinitic phonological 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;ChatGPT&#25216;&#26415;&#65292;&#20197;OpenAI&#30340;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#23884;&#20837;&#20026;&#22522;&#30784;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#23454;&#29616;&#20102;&#23545;&#19987;&#21033;&#21019;&#26032;&#25104;&#21151;&#21644;&#20272;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#20026;&#19987;&#21033;&#20272;&#20540;&#25552;&#20379;&#20102;&#38761;&#21629;&#24615;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#39044;&#27979;&#25509;&#21463;&#29575;&#26500;&#24314;&#30340;&#22810;&#31354;&#25237;&#36164;&#32452;&#21512;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24322;&#24120;&#25910;&#30410;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.01202</link><description>&lt;p&gt;
&#39044;&#27979;&#24615;&#19987;&#21033;&#23398;&#65306;&#20351;&#29992;ChatGPT&#25216;&#26415;&#39044;&#27979;&#21019;&#26032;&#25104;&#21151;&#21644;&#20272;&#20540;
&lt;/p&gt;
&lt;p&gt;
Predictive Patentomics: Forecasting Innovation Success and Valuation with ChatGPT. (arXiv:2307.01202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;ChatGPT&#25216;&#26415;&#65292;&#20197;OpenAI&#30340;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#23884;&#20837;&#20026;&#22522;&#30784;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#23454;&#29616;&#20102;&#23545;&#19987;&#21033;&#21019;&#26032;&#25104;&#21151;&#21644;&#20272;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#20026;&#19987;&#21033;&#20272;&#20540;&#25552;&#20379;&#20102;&#38761;&#21629;&#24615;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#39044;&#27979;&#25509;&#21463;&#29575;&#26500;&#24314;&#30340;&#22810;&#31354;&#25237;&#36164;&#32452;&#21512;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24322;&#24120;&#25910;&#30410;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26041;&#27861;&#23545;&#20110;&#21019;&#26032;&#30340;&#20998;&#26512;&#22312;&#24191;&#27867;&#30340;&#32467;&#26500;&#21464;&#37327;&#26041;&#38754;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#24320;&#21019;&#24615;&#30340;ChatGPT&#25216;&#26415;&#37319;&#29992;LLM&#26041;&#27861;&#23545;&#19987;&#21033;&#36827;&#34892;&#20998;&#26512;&#65292;&#31361;&#30772;&#20102;&#36793;&#30028;&#12290;OpenAI&#30340;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#23884;&#20837;&#33021;&#22815;&#35775;&#38382;&#20851;&#20110;&#27599;&#20010;&#21457;&#26126;&#30340;&#36136;&#37327;&#21644;&#24433;&#21709;&#30340;&#22797;&#26434;&#20449;&#24687;&#65292;&#29992;&#20110;&#39537;&#21160;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#12290;&#32454;&#33268;&#30340;&#23884;&#20837;&#20351;&#39044;&#27979;&#19987;&#21033;&#20215;&#20540;&#30340;R-squared&#25552;&#39640;&#20102;24&#65285;&#65292;&#24182;&#26126;&#30830;&#22320;&#23558;&#26368;&#24046;&#21644;&#26368;&#20339;&#24212;&#29992;&#31243;&#24207;&#20998;&#31163;&#24320;&#26469;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#39044;&#27979;&#25509;&#21463;&#29575;&#26500;&#24314;&#30340;&#22810;&#31354;&#25237;&#36164;&#32452;&#21512;&#27599;&#24180;&#23454;&#29616;&#26174;&#33879;&#30340;&#24322;&#24120;&#25910;&#30410;&#29575;&#39640;&#36798;3.3%&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#24066;&#22330;&#26080;&#27861;&#21450;&#26102;&#25972;&#21512;&#26377;&#20851;&#24212;&#29992;&#31243;&#24207;&#30340;&#20449;&#24687;&#12290;&#36825;&#20123;&#27169;&#22411;&#20026;&#38761;&#21629;&#24615;&#25913;&#21464;&#31185;&#26681;&#12289;&#24085;&#24085;&#23612;&#31185;&#27931;&#12289;&#22622;&#40065;&#21644;&#26031;&#25176;&#22827;&#26364;&#65288;2017&#65289;&#23545;&#19987;&#21033;&#20272;&#20540;&#30340;&#26356;&#27491;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analysis of innovation has been fundamentally limited by conventional approaches to broad, structural variables. This paper pushes the boundaries, taking an LLM approach to patent analysis with the groundbreaking ChatGPT technology. OpenAI's state-of-the-art textual embedding accesses complex information about the quality and impact of each invention to power deep learning predictive models. The nuanced embedding drives a 24% incremental improvement in R-squared predicting patent value and clearly isolates the worst and best applications. These models enable a revision of the contemporary Kogan, Papanikolaou, Seru, and Stoffman (2017) valuation of patents by a median deviation of 1.5 times, accounting for potential institutional predictions. Furthermore, the market fails to incorporate timely information about applications; a long-short portfolio based on predicted acceptance rates achieves significant abnormal returns of 3.3% annually. The models provide an opportunity to revolutioniz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20351;&#29992;&#20811;&#38534;&#32467;&#26500;&#22240;&#26524;&#22270;&#21487;&#20197;&#23454;&#29616;&#19982;transformer-based&#35821;&#35328;&#27169;&#22411;&#30456;&#20284;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#37322;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.01201</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#20986;&#29616;&#20013;&#30340;&#27169;&#24335;&#23398;&#20064;&#21644;&#37325;&#26032;&#32465;&#23450;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Schema-learning and rebinding as mechanisms of in-context learning and emergence. (arXiv:2307.01201v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20351;&#29992;&#20811;&#38534;&#32467;&#26500;&#22240;&#26524;&#22270;&#21487;&#20197;&#23454;&#29616;&#19982;transformer-based&#35821;&#35328;&#27169;&#22411;&#30456;&#20284;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#37322;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26159;&#36817;&#26399;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#26368;&#24378;&#22823;&#19988;&#26368;&#20196;&#20154;&#24847;&#22806;&#30340;&#33021;&#21147;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#22522;&#30784;&#26426;&#21046;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20811;&#38534;&#32467;&#26500;&#22240;&#26524;&#22270;&#65288;CSCGs&#65289;&#36825;&#31181;&#26367;&#20195;&#30340;&#24207;&#21015;&#39044;&#27979;&#23398;&#20064;&#26041;&#27861;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;ICL&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;CSCGs&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#26159;&#65292;&#19982;&#22522;&#20110;Transformer&#30340;LLMs&#19981;&#21516;&#65292;&#23427;&#20204;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#36825;&#22823;&#22823;&#31616;&#21270;&#20102;&#35299;&#37322;ICL&#24037;&#20316;&#21407;&#29702;&#30340;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26174;&#31034;&#23427;&#20351;&#29992;&#20102;&#20197;&#19979;&#32452;&#21512;&#65306;&#65288;a&#65289;&#23398;&#20064;&#27169;&#26495;&#65288;&#27169;&#24335;&#65289;&#30005;&#36335;&#36827;&#34892;&#27169;&#24335;&#23436;&#25104;&#65292;&#65288;b&#65289;&#20197;&#19978;&#19979;&#25991;&#25935;&#24863;&#26041;&#24335;&#26816;&#32034;&#30456;&#20851;&#27169;&#26495;&#65292;&#20197;&#21450;&#65288;c&#65289;&#23558;&#26032;&#26631;&#35760;&#37325;&#26032;&#32465;&#23450;&#21040;&#27169;&#26495;&#30340;&#36866;&#24403;&#20301;&#32622;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;ICL&#22312;LLMs&#20013;&#37319;&#29992;&#31867;&#20284;&#26426;&#21046;&#30340;&#20551;&#35774;&#35777;&#25454;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;LLMs&#19968;&#26679;&#65292;&#20351;&#29992;CSCGs&#26102;&#20250;&#20986;&#29616;&#19981;&#21516;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) is one of the most powerful and most unexpected capabilities to emerge in recent transformer-based large language models (LLMs). Yet the mechanisms that underlie it are poorly understood. In this paper, we demonstrate that comparable ICL capabilities can be acquired by an alternative sequence prediction learning method using clone-structured causal graphs (CSCGs). Moreover, a key property of CSCGs is that, unlike transformer-based LLMs, they are {\em interpretable}, which considerably simplifies the task of explaining how ICL works. Specifically, we show that it uses a combination of (a) learning template (schema) circuits for pattern completion, (b) retrieving relevant templates in a context-sensitive manner, and (c) rebinding of novel tokens to appropriate slots in the templates. We go on to marshall evidence for the hypothesis that similar mechanisms underlie ICL in LLMs. For example, we find that, with CSCGs as with LLMs, different capabilities emerge at d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20027;&#21160;&#36951;&#24536;&#26426;&#21046;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#22609;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23450;&#26399;&#37325;&#32622;&#23884;&#20837;&#23618;&#65292;&#27169;&#22411;&#21487;&#20197;&#26356;&#24555;&#22320;&#36866;&#24212;&#26032;&#30340;&#35821;&#35328;&#65292;&#24182;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01163</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#36951;&#24536;&#22312;&#39044;&#35757;&#32451;&#20013;&#25552;&#39640;&#35821;&#35328;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Language Plasticity via Pretraining with Active Forgetting. (arXiv:2307.01163v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20027;&#21160;&#36951;&#24536;&#26426;&#21046;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#22609;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23450;&#26399;&#37325;&#32622;&#23884;&#20837;&#23618;&#65292;&#27169;&#22411;&#21487;&#20197;&#26356;&#24555;&#22320;&#36866;&#24212;&#26032;&#30340;&#35821;&#35328;&#65292;&#24182;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20027;&#35201;&#27169;&#22411;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#23558;PLMs&#24212;&#29992;&#20110;&#26032;&#35821;&#35328;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#36825;&#26159;&#20351;&#23427;&#20204;&#30340;&#33021;&#21147;&#26222;&#36941;&#21487;&#35775;&#38382;&#30340;&#22721;&#22418;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20026;&#26032;&#35821;&#35328;&#23398;&#20064;&#26032;&#30340;&#23884;&#20837;&#23618;&#21487;&#20197;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#20294;&#36825;&#26679;&#20570;&#26082;&#28010;&#36153;&#25968;&#25454;&#21448;&#28010;&#36153;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#24314;&#35758;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#20027;&#21160;&#36951;&#24536;&#26426;&#21046;&#65292;&#20316;&#20026;&#24555;&#36895;&#36866;&#24212;&#26032;&#35821;&#35328;&#30340;PLMs&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#27599;K&#27425;&#26356;&#26032;&#26102;&#37325;&#32622;&#23884;&#20837;&#23618;&#65292;&#25105;&#20204;&#40723;&#21169;PLM&#22312;&#26377;&#38480;&#27425;&#26356;&#26032;&#20869;&#25552;&#39640;&#23398;&#20064;&#26032;&#23884;&#20837;&#30340;&#33021;&#21147;&#65292;&#31867;&#20284;&#20110;&#20803;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#20351;&#29992;RoBERTa&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#36951;&#24536;&#26426;&#21046;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#35821;&#35328;&#36866;&#24212;&#36807;&#31243;&#20013;&#26174;&#31034;&#20986;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#19988;&#22312;&#20302;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (PLMs) are today the primary model for natural language processing. Despite their impressive downstream performance, it can be difficult to apply PLMs to new languages, a barrier to making their capabilities universally accessible. While prior work has shown it possible to address this issue by learning a new embedding layer for the new language, doing so is both data and compute inefficient. We propose to use an active forgetting mechanism during pretraining, as a simple way of creating PLMs that can quickly adapt to new languages. Concretely, by resetting the embedding layer every K updates during pretraining, we encourage the PLM to improve its ability of learning new embeddings within a limited number of updates, similar to a meta-learning effect. Experiments with RoBERTa show that models pretrained with our forgetting mechanism not only demonstrate faster convergence during language adaptation but also outperform standard ones in a low-data regime, parti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;&#21327;&#21516;&#25512;&#29702;&#65288;CCR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#21453;&#20107;&#23454;&#25512;&#29702;&#21644;&#36923;&#36753;&#25512;&#29702;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#29983;&#25104;&#22256;&#38590;&#30340;&#21453;&#20107;&#23454;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;CCR&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#31034;&#20102;&#22914;&#20309;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#12289;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#22686;&#24378;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.00165</link><description>&lt;p&gt;
&#21453;&#20107;&#23454;&#21327;&#21516;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Collaborative Reasoning. (arXiv:2307.00165v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;&#21327;&#21516;&#25512;&#29702;&#65288;CCR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#21453;&#20107;&#23454;&#25512;&#29702;&#21644;&#36923;&#36753;&#25512;&#29702;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#29983;&#25104;&#22256;&#38590;&#30340;&#21453;&#20107;&#23454;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;CCR&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#31034;&#20102;&#22914;&#20309;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#12289;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#22686;&#24378;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#21644;&#36923;&#36753;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#20004;&#31181;&#37325;&#35201;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#26234;&#33021;&#32972;&#26223;&#19979;&#65292;&#23427;&#20204;&#30340;&#20851;&#31995;&#36824;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20849;&#21516;&#24314;&#27169;&#36825;&#20004;&#31181;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#25972;&#21512;&#21453;&#20107;&#23454;&#25512;&#29702;&#21644;&#65288;&#31070;&#32463;&#65289;&#36923;&#36753;&#25512;&#29702;&#20004;&#31181;&#37325;&#35201;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;&#21327;&#21516;&#25512;&#29702;&#65288;CCR&#65289;&#65292;&#23427;&#36890;&#36807;&#36827;&#34892;&#21453;&#20107;&#23454;&#36923;&#36753;&#25512;&#29702;&#26469;&#25913;&#36827;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20197;&#25512;&#33616;&#31995;&#32479;&#20026;&#20363;&#65292;&#23637;&#31034;&#20102;CCR&#22914;&#20309;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#12289;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#22686;&#24378;&#36879;&#26126;&#24230;&#12290;&#20174;&#25216;&#26415;&#19978;&#35762;&#65292;&#25105;&#20204;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#29983;&#25104;&#8220;&#22256;&#38590;&#8221;&#30340;&#21453;&#20107;&#23454;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#36825;&#19982;&#21407;&#22987;&#30340;&#35757;&#32451;&#26679;&#26412;&#19968;&#36215;&#21487;&#20197;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal reasoning and logical reasoning are two important types of reasoning abilities for human intelligence. However, their relationship has not been extensively explored under machine intelligence context. In this paper, we explore how the two reasoning abilities can be jointly modeled to enhance both accuracy and explainability of machine learning models. More specifically, by integrating two important types of reasoning ability -- counterfactual reasoning and (neural) logical reasoning -- we propose Counterfactual Collaborative Reasoning (CCR), which conducts counterfactual logic reasoning to improve the performance. In particular, we use recommender system as an example to show how CCR alleviate data scarcity, improve accuracy and enhance transparency. Technically, we leverage counterfactual reasoning to generate "difficult" counterfactual training examples for data augmentation, which -together with the original training examples -- can enhance the model performance. Since the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2306.17256</link><description>&lt;p&gt;
&#20197;&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#26681;&#25454;&#29992;&#25143;&#36807;&#21435;&#30340;&#34892;&#20026;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#19982;&#20854;&#20852;&#36259;&#30456;&#31526;&#30340;&#20449;&#24687;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#21382;&#21490;&#20132;&#20114;&#35760;&#24405;&#19981;&#21487;&#29992;&#26102;&#65292;&#24320;&#21457;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#23601;&#26159;&#25152;&#35859;&#30340;&#31995;&#32479;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#12290;&#27492;&#38382;&#39064;&#22312;&#21019;&#19994;&#20225;&#19994;&#25110;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#20013;&#23588;&#20026;&#31361;&#20986;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#29992;&#25143;&#25110;&#29289;&#21697;&#30340;&#20919;&#21551;&#21160;&#22330;&#26223;&#65292;&#20854;&#20013;&#31995;&#32479;&#20173;&#28982;&#36890;&#36807;&#22312;&#21516;&#19968;&#39046;&#22495;&#20013;&#30340;&#21382;&#21490;&#29992;&#25143;&#21644;&#29289;&#21697;&#20132;&#20114;&#36827;&#34892;&#35757;&#32451;&#26469;&#20026;&#26032;&#29992;&#25143;&#25110;&#29289;&#21697;&#25552;&#20379;&#25512;&#33616;&#65292;&#32780;&#26080;&#27861;&#35299;&#20915;&#25105;&#20204;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#36164;&#26009;&#21644;&#29289;&#21697;&#23646;&#24615;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems play a crucial role in helping users discover information that aligns with their interests based on their past behaviors. However, developing personalized recommendation systems becomes challenging when historical records of user-item interactions are unavailable, leading to what is known as the system cold-start recommendation problem. This issue is particularly prominent in start-up businesses or platforms with insufficient user engagement history. Previous studies focus on user or item cold-start scenarios, where systems could make recommendations for new users or items but are still trained with historical user-item interactions in the same domain, which cannot solve our problem. To bridge the gap, our research introduces an innovative and effective approach, capitalizing on the capabilities of pre-trained language models. We transform the recommendation process into sentiment analysis of natural languages containing information of user profiles and item attribu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mapKurator&#30340;&#31995;&#32479;&#65292;&#33021;&#23436;&#25972;&#22320;&#20174;&#21382;&#21490;&#22320;&#22270;&#20013;&#25552;&#21462;&#21644;&#38142;&#25509;&#25991;&#26412;&#20449;&#24687;&#12290;&#35813;&#31995;&#32479;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#20301;&#32622;&#30456;&#20851;&#35789;&#35821;&#30340;&#24573;&#30053;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#32771;&#34385;&#26356;&#24191;&#30340;&#20027;&#39064;&#33539;&#22260;&#65292;&#33021;&#22815;&#35782;&#21035;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.17059</link><description>&lt;p&gt;
The mapKurator&#31995;&#32479;&#65306;&#20174;&#21382;&#21490;&#22320;&#22270;&#20013;&#25552;&#21462;&#21644;&#38142;&#25509;&#25991;&#26412;&#30340;&#23436;&#25972;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
The mapKurator System: A Complete Pipeline for Extracting and Linking Text from Historical Maps. (arXiv:2306.17059v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17059
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mapKurator&#30340;&#31995;&#32479;&#65292;&#33021;&#23436;&#25972;&#22320;&#20174;&#21382;&#21490;&#22320;&#22270;&#20013;&#25552;&#21462;&#21644;&#38142;&#25509;&#25991;&#26412;&#20449;&#24687;&#12290;&#35813;&#31995;&#32479;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#20301;&#32622;&#30456;&#20851;&#35789;&#35821;&#30340;&#24573;&#30053;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#32771;&#34385;&#26356;&#24191;&#30340;&#20027;&#39064;&#33539;&#22260;&#65292;&#33021;&#22815;&#35782;&#21035;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#20855;&#26377;&#31354;&#38388;&#28966;&#28857;&#21644;&#26377;&#20215;&#20540;&#30340;&#22320;&#26041;&#29305;&#24449;&#12290;&#20363;&#22914;&#65292;&#25151;&#22320;&#20135;&#25110;&#26053;&#34892;&#21338;&#23458;&#20013;&#30340;&#21015;&#34920;&#25551;&#36848;&#21253;&#21547;&#26377;&#20851;&#29305;&#23450;&#22320;&#21306;&#31038;&#21306;&#30340;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#25551;&#36848;&#20154;&#31867;&#22914;&#20309;&#24863;&#30693;&#20182;&#20204;&#30340;&#29615;&#22659;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#30340;&#31532;&#19968;&#27493;&#26159;&#35782;&#21035;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#65288;&#20363;&#22914;&#65292;&#22478;&#24066;&#65289;&#12290;&#20256;&#32479;&#26041;&#27861;&#29992;&#20110;&#35782;&#21035;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#20381;&#36182;&#20110;&#20174;&#25991;&#26723;&#20013;&#26816;&#27979;&#21644;&#28040;&#27495;&#21270;&#22320;&#21517;&#12290;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#21253;&#21547;&#20301;&#32622;&#30701;&#35821;&#21644;&#20020;&#26102;&#35268;&#21017;&#30340;&#35789;&#27719;&#38598;&#65292;&#36825;&#20123;&#35268;&#21017;&#24573;&#30053;&#20102;&#19982;&#20301;&#32622;&#30456;&#20851;&#30340;&#37325;&#35201;&#35789;&#35821;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#36890;&#24120;&#32771;&#34385;&#20960;&#20010;&#24191;&#24230;&#30340;&#20027;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#21487;&#20197;&#26159;&#19968;&#20010;&#22269;&#23478;&#12289;&#19968;&#20010;&#22478;&#24066;&#65292;&#29978;&#33267;&#26159;&#19968;&#20010;&#31038;&#21306;&#65292;&#36825;&#20123;&#33539;&#22260;&#27604;&#36825;&#20123;&#26041;&#27861;&#32771;&#34385;&#30340;&#20027;&#39064;&#25968;&#35201;&#22823;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Documents hold spatial focus and valuable locality characteristics. For example, descriptions of listings in real estate or travel blogs contain information about specific local neighborhoods. This information is valuable to characterize how humans perceive their environment. However, the first step to making use of this information is to identify the spatial focus (e.g., a city) of a document. Traditional approaches for identifying the spatial focus of a document rely on detecting and disambiguating toponyms from the document. This approach requires a vocabulary set of location phrases and ad-hoc rules, which ignore important words related to location. Recent topic modeling approaches using large language models often consider a few topics, each with broad coverage. In contrast, the spatial focus of a document can be a country, a city, or even a neighborhood, which together, is much larger than the number of topics considered in these approaches. Additionally, topic modeling methods a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DMNER&#30340;&#26032;&#22411;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#27979;&#23454;&#20307;&#36793;&#30028;&#21644;&#21305;&#37197;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#26469;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;&#22312;&#26377;&#30417;&#30563;NER&#12289;&#36828;&#31243;&#30417;&#30563;NER&#21644;&#22810;&#25968;&#25454;&#38598;&#21512;&#24182;&#35757;&#32451;NER&#31561;&#22330;&#26223;&#20013;&#65292;DMNER&#37117;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15736</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#27979;&#21644;&#21305;&#37197;&#36827;&#34892;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Biomedical Entity Recognition by Detection and Matching. (arXiv:2306.15736v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DMNER&#30340;&#26032;&#22411;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#27979;&#23454;&#20307;&#36793;&#30028;&#21644;&#21305;&#37197;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#26469;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;&#22312;&#26377;&#30417;&#30563;NER&#12289;&#36828;&#31243;&#30417;&#30563;NER&#21644;&#22810;&#25968;&#25454;&#38598;&#21512;&#24182;&#35757;&#32451;NER&#31561;&#22330;&#26223;&#20013;&#65292;DMNER&#37117;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;BNER&#65289;&#26159;&#35768;&#22810;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;&#19982;&#19968;&#33324;&#30340;NER&#19981;&#21516;&#65292;BNER&#38656;&#35201;&#20840;&#38754;&#25484;&#25569;&#39046;&#22495;&#30693;&#35782;&#65292;&#32780;&#19988;&#22312;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#34701;&#20837;&#22806;&#37096;&#30693;&#35782;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;BNER&#26694;&#26550;&#65292;&#31216;&#20026;DMNER&#12290;&#36890;&#36807;&#21033;&#29992;&#24050;&#26377;&#30340;&#23454;&#20307;&#34920;&#31034;&#27169;&#22411;SAPBERT&#65292;&#25105;&#20204;&#23558;BNER&#20316;&#20026;&#19968;&#20010;&#20004;&#27493;&#39588;&#30340;&#36807;&#31243;&#26469;&#22788;&#29702;&#65306;&#23454;&#20307;&#36793;&#30028;&#26816;&#27979;&#21644;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#21305;&#37197;&#12290;DMNER&#22312;&#22810;&#31181;NER&#22330;&#26223;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#65306;1&#65289;&#22312;&#26377;&#30417;&#30563;NER&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;DMNER&#26377;&#25928;&#32416;&#27491;&#20102;&#22522;&#32447;NER&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;2&#65289;&#22312;&#36828;&#31243;&#30417;&#30563;NER&#20013;&#65292;&#23558;MRC&#21644;AutoNER&#20316;&#20026;&#36328;&#24230;&#36793;&#30028;&#26816;&#27979;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;DMNER&#33021;&#22815;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;3&#65289;&#23545;&#20110;&#36890;&#36807;&#21512;&#24182;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;NER&#35757;&#32451;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19982;DS-NER&#31867;&#20284;&#30340;&#26694;&#26550;&#65292;&#20294;&#36824;&#39069;&#22806;&#21033;&#29992;ChatGPT&#26469;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#30701;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical named entity recognition (BNER) serves as the foundation for numerous biomedical text mining tasks. Unlike general NER, BNER require a comprehensive grasp of the domain, and incorporating external knowledge beyond training data poses a significant challenge. In this study, we propose a novel BNER framework called DMNER. By leveraging existing entity representation models SAPBERT, we tackle BNER as a two-step process: entity boundary detection and biomedical entity matching. DMNER exhibits applicability across multiple NER scenarios: 1) In supervised NER, we observe that DMNER effectively rectifies the output of baseline NER models, thereby further enhancing performance. 2) In distantly supervised NER, combining MRC and AutoNER as span boundary detectors enables DMNER to achieve satisfactory results. 3) For training NER by merging multiple datasets, we adopt a framework similar to DS-NER but additionally leverage ChatGPT to obtain high-quality phrases in the training. Through
&lt;/p&gt;</description></item><item><title>BEER^2&#26159;&#19968;&#31181;&#29992;&#20110;Retriever&#21644;Reader&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#23398;&#20064;&#65292;&#20849;&#21516;&#36827;&#27493;&#65292;&#23454;&#29616;&#31471;&#21040;&#31471;EL&#12290;</title><link>http://arxiv.org/abs/2306.12245</link><description>&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#30340;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#33539;&#24335;&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bidirectional End-to-End Learning of Retriever-Reader Paradigm for Entity Linking. (arXiv:2306.12245v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12245
&lt;/p&gt;
&lt;p&gt;
BEER^2&#26159;&#19968;&#31181;&#29992;&#20110;Retriever&#21644;Reader&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#23398;&#20064;&#65292;&#20849;&#21516;&#36827;&#27493;&#65292;&#23454;&#29616;&#31471;&#21040;&#31471;EL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#26159;&#20449;&#24687;&#25552;&#21462;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#23427;&#30340;&#19968;&#33324;&#24418;&#24335;&#65288;&#21363;&#31471;&#21040;&#31471;EL&#65289;&#26088;&#22312;&#39318;&#20808;&#22312;&#32473;&#23450;&#36755;&#20837;&#25991;&#26723;&#20013;&#25214;&#21040;&#25552;&#21450;&#65292;&#24182;&#23558;&#25552;&#21450;&#38142;&#25509;&#21040;&#29305;&#23450;&#30693;&#35782;&#24211;&#20013;&#30340;&#30456;&#24212;&#23454;&#20307;&#12290;&#26368;&#36817;&#65292;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#33539;&#24335;&#20419;&#36827;&#20102;&#31471;&#21040;&#31471;EL&#30340;&#36827;&#23637;&#65292;&#21463;&#30410;&#20110;&#23494;&#38598;&#30340;&#23454;&#20307;&#26816;&#32034;&#21644;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20165;&#20197;&#27969;&#27700;&#32447;&#26041;&#24335;&#21333;&#29420;&#35757;&#32451;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#65292;&#24573;&#30053;&#20102;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#20043;&#38388;&#20132;&#20114;&#24102;&#26469;&#30340;&#30410;&#22788;&#12290;&#20026;&#20102;&#20351;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#33539;&#24335;&#26356;&#23436;&#32654;&#22320;&#25191;&#34892;&#31471;&#21040;&#31471;EL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BEER$^2$&#65292;&#19968;&#31181;&#29992;&#20110;Retriever and Reader&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#12290;&#36890;&#36807;&#25105;&#20204;&#35774;&#35745;&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;BEER$^2$&#25351;&#23548;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#20114;&#30456;&#23398;&#20064;&#65292;&#20849;&#21516;&#36827;&#27493;&#65292;&#24182;&#26368;&#32456;&#23454;&#29616;&#31471;&#21040;&#31471;EL&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity Linking (EL) is a fundamental task for Information Extraction and Knowledge Graphs. The general form of EL (i.e., end-to-end EL) aims to first find mentions in the given input document and then link the mentions to corresponding entities in a specific knowledge base. Recently, the paradigm of retriever-reader promotes the progress of end-to-end EL, benefiting from the advantages of dense entity retrieval and machine reading comprehension. However, the existing study only trains the retriever and the reader separately in a pipeline manner, which ignores the benefit that the interaction between the retriever and the reader can bring to the task. To advance the retriever-reader paradigm to perform more perfectly on end-to-end EL, we propose BEER$^2$, a Bidirectional End-to-End training framework for Retriever and Reader. Through our designed bidirectional end-to-end training, BEER$^2$ guides the retriever and the reader to learn from each other, make progress together, and ultimate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21830;&#19994;&#29615;&#22659;&#30340;&#12289;&#33021;&#22815;&#24179;&#34913;&#23545;&#35805;&#27969;&#30021;&#24615;&#21644;&#36235;&#21521;&#20110;&#29702;&#35299;&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#25968;&#25454;&#38598;&#28151;&#21512;&#12289;&#36127;&#35282;&#33394;&#20449;&#24687;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#21450;&#35774;&#35745;&#20010;&#24615;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102; $\textit{WHAT}$&#12289;$\textit{WHEN}$&#21644;$\textit{HOW}$ &#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#23545;&#35805;&#31995;&#32479;&#21709;&#24212;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03361</link><description>&lt;p&gt;
&#35774;&#35745;&#29992;&#25143;&#35282;&#33394;&#24863;&#30693;&#30340;&#23545;&#35805;&#20195;&#29702;&#36827;&#34892;&#26377;&#36259;&#30340;&#23545;&#35805;&#65306;$\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$ to Ground
&lt;/p&gt;
&lt;p&gt;
$\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$ to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue. (arXiv:2306.03361v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21830;&#19994;&#29615;&#22659;&#30340;&#12289;&#33021;&#22815;&#24179;&#34913;&#23545;&#35805;&#27969;&#30021;&#24615;&#21644;&#36235;&#21521;&#20110;&#29702;&#35299;&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#25968;&#25454;&#38598;&#28151;&#21512;&#12289;&#36127;&#35282;&#33394;&#20449;&#24687;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#21450;&#35774;&#35745;&#20010;&#24615;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102; $\textit{WHAT}$&#12289;$\textit{WHEN}$&#21644;$\textit{HOW}$ &#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#23545;&#35805;&#31995;&#32479;&#21709;&#24212;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#31435;&#20010;&#24615;&#21270;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#20197;&#35299;&#20915;&#21830;&#19994;&#35774;&#32622;&#20013;&#28041;&#21450;&#20010;&#24615;&#21270;&#23545;&#35805;&#21709;&#24212;&#19982;&#38750;&#27491;&#24335;&#21709;&#24212;&#20132;&#26367;&#30340;$\textit{WWH}$&#65288;$\textit{WHAT}$&#12289;$\textit{WHEN}$&#21644;$\textit{HOW}$&#65289;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#21152;&#26435;&#25968;&#25454;&#38598;&#28151;&#21512;&#12289;&#36127;&#35282;&#33394;&#20449;&#24687;&#22686;&#24378;&#26041;&#27861;&#20197;&#21450;&#35774;&#35745;&#20010;&#24615;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#20197;&#24212;&#23545;&#20010;&#24615;&#21270;&#12289;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;$\textit{WWH}$&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#26377;&#25928;&#22320;&#24179;&#34913;&#20102;&#23545;&#35805;&#27969;&#30021;&#24615;&#21644;&#36235;&#21521;&#20110;&#29702;&#35299;&#23545;&#35805;&#31995;&#32479;&#65292;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#21709;&#24212;&#31867;&#22411;&#26631;&#31614;&#26469;&#25552;&#39640;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#32452;&#21512;&#23548;&#33268;&#20102;&#26356;&#21152;&#27969;&#30021;&#30340;&#23545;&#35805;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#20027;&#35266;&#20154;&#31867;&#35780;&#20272;&#21644;&#23458;&#35266;&#35780;&#20272;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a method for building a personalized open-domain dialogue system to address the $\textit{WWH}$ ($\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed approach involves weighted dataset blending, negative persona information augmentation methods, and the design of personalized conversation datasets to address the challenges of $\textit{WWH}$ in personalized, open-domain dialogue systems. Our work effectively balances dialogue fluency and tendency to ground, while also introducing a response-type label to improve the controllability and explainability of the grounded responses. The combination of these methods leads to more fluent conversations, as evidenced by subjective human evaluations as well as objective evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37322;&#20041;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#23545;&#27604;&#22411;Prompt&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#30784;Prompt&#30340;&#23569;&#26679;&#26412;&#37322;&#20041;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#25968;&#25454;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2305.18169</link><description>&lt;p&gt;
LM-CPPF: &#22522;&#20110;&#37322;&#20041;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#23545;&#27604;&#22411;Prompt&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning. (arXiv:2305.18169v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37322;&#20041;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#23545;&#27604;&#22411;Prompt&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#30784;Prompt&#30340;&#23569;&#26679;&#26412;&#37322;&#20041;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#23569;&#37327;&#25968;&#25454;&#38598;&#19978;&#30340;&#24494;&#35843;&#20173;&#28982;&#23384;&#22312;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#36866;&#24212;&#24615;&#26041;&#27861;&#12290;&#23545;&#22522;&#30784;Prompt&#36827;&#34892;&#24494;&#35843;&#26159;&#19968;&#31181;&#36739;&#20026;&#26222;&#36941;&#30340;&#26041;&#24335;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22823;&#22411;&#27169;&#22411;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#23558;&#23545;&#27604;&#23398;&#20064;&#28155;&#21152;&#21040;&#23545;Prompt&#30340;&#24494;&#35843;&#20013;&#26159;&#26377;&#25928;&#30340;&#65292;&#22240;&#20026;&#23427;&#24110;&#21161;&#27169;&#22411;&#29983;&#25104;&#26356;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#20998;&#31867;&#20043;&#38388;&#30340;&#23884;&#20837;&#65292;&#32780;&#19988;&#23427;&#21516;&#26102;&#36824;&#33021;&#20174;&#27491;&#36127;&#31034;&#20363;&#20013;&#23398;&#20064;&#65292;&#26356;&#21152;&#33410;&#30465;&#26679;&#26412;&#12290;&#23545;&#27604;&#23398;&#20064;&#26368;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#26159;&#25968;&#25454;&#22686;&#24378;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23545;&#20110;NLP&#26469;&#35828;&#65292;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LM-CPPF&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#23569;&#26679;&#26412;&#37322;&#20041;&#22411;&#23545;&#27604;Prompt&#24494;&#35843;&#65292;&#23427;&#21033;&#29992;&#22522;&#30784;Prompt&#30340;&#23569;&#26679;&#26412;&#37322;&#20041;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been significant progress in developing pre-trained language models for NLP. However, these models often struggle when fine-tuned on small datasets. To address this issue, researchers have proposed various adaptation approaches. Prompt-based tuning is arguably the most common way, especially for larger models. Previous research shows that adding contrastive learning to prompt-based fine-tuning is effective as it helps the model generate embeddings that are more distinguishable between classes, and it can also be more sample-efficient as the model learns from positive and negative examples simultaneously. One of the most important components of contrastive learning is data augmentation, but unlike computer vision, effective data augmentation for NLP is still challenging. This paper proposes LM-CPPF, Contrastive Paraphrasing-guided Prompt-based Fine-tuning of Language Models, which leverages prompt-based few-shot paraphrasing using generative language models, e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29992;&#22522;&#20110;&#34920;&#38754;&#32423;&#21035;&#30340;&#26816;&#32034;&#26426;&#21046;&#21462;&#20195;&#35821;&#20041;&#26816;&#32034;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.16243</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#38754;&#30340;&#26816;&#32034;&#38477;&#20302;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;
&lt;/p&gt;
&lt;p&gt;
Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models. (arXiv:2305.16243v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29992;&#22522;&#20110;&#34920;&#38754;&#32423;&#21035;&#30340;&#26816;&#32034;&#26426;&#21046;&#21462;&#20195;&#35821;&#20041;&#26816;&#32034;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#36890;&#36807;&#26816;&#32034;&#26426;&#21046;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#25968;&#37327;&#36739;&#20302;&#12290;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#36890;&#24120;&#20381;&#38752;&#22522;&#20110;&#26597;&#35810;&#22359;&#21644;&#28508;&#22312;&#37051;&#23621;&#20043;&#38388;&#30340;&#23494;&#38598;&#34920;&#31034;&#30456;&#20284;&#24615;&#30340;&#35821;&#20041;&#26816;&#32034;&#26426;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;Retro&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#21040;&#20854;&#24615;&#33021;&#25552;&#21319;&#26356;&#22909;&#22320;&#35299;&#37322;&#20026;&#22522;&#20110;&#34920;&#38754;&#32423;&#21035;&#30340;&#30456;&#20284;&#24615;&#65292;&#20363;&#22914;&#26631;&#35760;&#37325;&#21472;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#29992;BM25&#26367;&#25442;Retro&#20013;&#30340;&#35821;&#20041;&#26816;&#32034;&#65292;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#22256;&#24785;&#24230;&#38477;&#20302;&#12290;&#30001;&#20110;&#23436;&#25972;&#30340;BM25&#26816;&#32034;&#21487;&#33021;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#65292;&#22240;&#27492;&#25105;&#20204;&#36824;&#23558;&#20854;&#24212;&#29992;&#20110;&#37325;&#26032;&#25490;&#21517;&#22330;&#26223;&#20013;&#65292;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#33719;&#24471;&#37096;&#20998;&#22256;&#24785;&#24230;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting language models with a retrieval mechanism has been shown to significantly improve their performance while keeping the number of parameters low. Retrieval-augmented models commonly rely on a semantic retrieval mechanism based on the similarity between dense representations of the query chunk and potential neighbors. In this paper, we study the state-of-the-art Retro model and observe that its performance gain is better explained by surface-level similarities, such as token overlap. Inspired by this, we replace the semantic retrieval in Retro with a surface-level method based on BM25, obtaining a significant reduction in perplexity. As full BM25 retrieval can be computationally costly for large datasets, we also apply it in a re-ranking scenario, gaining part of the perplexity reduction with minimal computational overhead.
&lt;/p&gt;</description></item><item><title>Flan-MoE&#26159;&#19968;&#31181;&#25351;&#20196;&#35843;&#20248;&#30340;&#31232;&#30095;Mixture of Experts&#65288;MoE&#65289;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#23494;&#38598;&#27169;&#22411;&#65292;&#22312;&#25351;&#20196;&#24494;&#35843;&#21644;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#21518;&#22343;&#34920;&#29616;&#26356;&#22909;&#12290;&#26368;&#22823;&#27169;&#22411;Flan-MoE-32B&#30340;&#24615;&#33021;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;Flan-PaLM-62B&#65292;&#21516;&#26102;&#21482;&#21033;&#29992;&#20102;1/3&#30340;FLOPs&#12290;</title><link>http://arxiv.org/abs/2305.14705</link><description>&lt;p&gt;
Flan-MoE: &#36890;&#36807;&#31232;&#30095;Mixture of Experts&#25193;&#23637;&#25351;&#20196;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts. (arXiv:2305.14705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14705
&lt;/p&gt;
&lt;p&gt;
Flan-MoE&#26159;&#19968;&#31181;&#25351;&#20196;&#35843;&#20248;&#30340;&#31232;&#30095;Mixture of Experts&#65288;MoE&#65289;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#23494;&#38598;&#27169;&#22411;&#65292;&#22312;&#25351;&#20196;&#24494;&#35843;&#21644;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#21518;&#22343;&#34920;&#29616;&#26356;&#22909;&#12290;&#26368;&#22823;&#27169;&#22411;Flan-MoE-32B&#30340;&#24615;&#33021;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;Flan-PaLM-62B&#65292;&#21516;&#26102;&#21482;&#21033;&#29992;&#20102;1/3&#30340;FLOPs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#21644;&#24212;&#29992;&#38656;&#27714;&#23548;&#33268;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#26041;&#27861;&#30340;&#38656;&#27714;&#22686;&#21152;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;Instruction-Finetuned Sparse Mixture-of-Expert (MoE)&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;Flan-MoE&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#38024;&#23545;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#38598;&#36827;&#34892;MoE&#27169;&#22411;&#30340;&#24494;&#35843;&#20250;&#23548;&#33268;&#24615;&#33021;&#19981;&#22914;&#30456;&#21516;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#23494;&#38598;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;Flan-MoE&#22312;&#22810;&#20010;&#23454;&#39564;&#35774;&#32622;&#19979;&#37117;&#20248;&#20110;&#23494;&#38598;&#27169;&#22411;&#65306;&#20165;&#25351;&#20196;&#24494;&#35843;&#21644;&#25351;&#20196;&#24494;&#35843;&#21518;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#12290;&#36825;&#34920;&#26126;&#65292;&#25351;&#20196;&#24494;&#35843;&#26159;MoE&#27169;&#22411;&#30340;&#24517;&#35201;&#38454;&#27573;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26368;&#22823;&#27169;&#22411;Flan-MoE-32B&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;Flan-PaLM-62B&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21482;&#21033;&#29992;&#20102;1/3&#30340;FLOPs&#12290;Flan-MoE&#30340;&#25104;&#21151;&#40723;&#33310;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22823;&#35268;&#27169;&#12289;&#39640;&#24615;&#33021;&#35821;&#35328;&#27169;&#22411;&#30340;&#35774;&#35745;&#65292;&#23588;&#20854;&#26159;&#22312;&#20219;&#21153;&#26080;&#20851;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The explosive growth of language models and their applications have led to an increased demand for efficient and scalable methods. In this paper, we introduce Flan-MoE, a set of Instruction-Finetuned Sparse Mixture-of-Expert (MoE) models. We show that naively finetuning MoE models on a task-specific dataset (in other words, no instruction-finetuning) often yield worse performance compared to dense models of the same computational complexity. However, our Flan-MoE outperforms dense models under multiple experiment settings: instruction-finetuning only and instruction-finetuning followed by task-specific finetuning. This shows that instruction-finetuning is an essential stage for MoE models. Specifically, our largest model, Flan-MoE-32B, surpasses the performance of Flan-PaLM-62B on four benchmarks, while utilizing only one-third of the FLOPs. The success of Flan-MoE encourages rethinking the design of large-scale, high-performance language models, under the setting of task-agnostic lear
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#30701;&#35821;&#39044;&#27979;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#20559;&#32622;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#20559;&#32622;&#25439;&#22833;&#20197;&#24110;&#21161;&#35757;&#32451;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#65292;&#22312;&#22810;&#31181;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;WER&#38477;&#20302;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#27169;&#22411;&#30456;&#23545;WER&#25552;&#39640;&#20102;12.1&#65285;&#65292;&#19978;&#19979;&#25991;&#30701;&#35821;&#30340;WER&#30456;&#23545;&#38477;&#20302;&#20102;40.5&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.12493</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#30340;&#30701;&#35821;&#39044;&#27979;&#32593;&#32476;&#22312;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network. (arXiv:2305.12493v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#30701;&#35821;&#39044;&#27979;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#20559;&#32622;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#20559;&#32622;&#25439;&#22833;&#20197;&#24110;&#21161;&#35757;&#32451;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#65292;&#22312;&#22810;&#31181;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;WER&#38477;&#20302;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#27169;&#22411;&#30456;&#23545;WER&#25552;&#39640;&#20102;12.1&#65285;&#65292;&#19978;&#19979;&#25991;&#30701;&#35821;&#30340;WER&#30456;&#23545;&#38477;&#20302;&#20102;40.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#20449;&#24687;&#22312;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23558;&#20854;&#34701;&#20837;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#28145;&#24230;&#20559;&#32622;&#26041;&#27861;&#32570;&#20047;&#20559;&#32622;&#20219;&#21153;&#30340;&#26174;&#24335;&#30417;&#30563;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#30701;&#35821;&#39044;&#27979;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#20559;&#32622;&#26041;&#27861;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#19978;&#19979;&#25991;&#23884;&#20837;&#39044;&#27979;&#21457;&#38899;&#20013;&#30340;&#19978;&#19979;&#25991;&#30701;&#35821;&#65292;&#24182;&#35745;&#31639;&#20559;&#32622;&#25439;&#22833;&#20197;&#24110;&#21161;&#35757;&#32451;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#31181;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;(WER)&#38477;&#20302;&#12290;&#23545;LibriSpeech&#35821;&#26009;&#24211;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22522;&#32447;&#27169;&#22411;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#23545;WER&#25552;&#39640;&#20102;12.1&#65285;&#65292;&#19978;&#19979;&#25991;&#30701;&#35821;&#30340;WER&#30456;&#23545;&#38477;&#20302;&#20102;40.5&#65285;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24212;&#29992;&#19978;&#19979;&#25991;&#30701;&#35821;&#36807;&#28388;&#31574;&#30053;&#65292;&#25105;&#20204;&#36824;&#26377;&#25928;&#28040;&#38500;&#20102;&#20351;&#29992;&#26356;&#22823;&#30340;&#20559;&#32622;&#21015;&#34920;&#26102;&#30340;WER&#38477;&#32423;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual information plays a crucial role in speech recognition technologies and incorporating it into the end-to-end speech recognition models has drawn immense interest recently. However, previous deep bias methods lacked explicit supervision for bias tasks. In this study, we introduce a contextual phrase prediction network for an attention-based deep bias method. This network predicts context phrases in utterances using contextual embeddings and calculates bias loss to assist in the training of the contextualized model. Our method achieved a significant word error rate (WER) reduction across various end-to-end speech recognition models. Experiments on the LibriSpeech corpus show that our proposed model obtains a 12.1% relative WER improvement over the baseline model, and the WER of the context phrases decreases relatively by 40.5%. Moreover, by applying a context phrase filtering strategy, we also effectively eliminate the WER degradation when using a larger biasing list.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#23567;&#20026;&#20013;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#22806;&#37096;&#26816;&#32034;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#38382;&#31572;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#36866;&#24403;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#27604;&#21333;&#32431;&#20381;&#36182;&#21442;&#25968;&#25968;&#37327;&#26356;&#37325;&#35201;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;46.4%&#30340;&#27491;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.11991</link><description>&lt;p&gt;
&#22312;&#38646;-shot&#23553;&#38381;&#29983;&#25104;&#24335;&#38382;&#31572;&#20013;&#35780;&#20272;&#22823;&#23567;&#20026;&#20013;&#22411;-&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluation of medium-large Language Models at zero-shot closed book generative question answering. (arXiv:2305.11991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#23567;&#20026;&#20013;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#22806;&#37096;&#26816;&#32034;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#38382;&#31572;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#36866;&#24403;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#27604;&#21333;&#32431;&#20381;&#36182;&#21442;&#25968;&#25968;&#37327;&#26356;&#37325;&#35201;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;46.4%&#30340;&#27491;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#65292;&#20294;&#8220;&#22823;&#8221;&#36825;&#20010;&#23450;&#20041;&#32570;&#20047;&#28165;&#26224;&#24230;&#12290;&#26412;&#25991;&#20851;&#27880;&#20013;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#65292;&#36825;&#34987;&#23450;&#20041;&#20026;&#20855;&#26377;&#33267;&#23569;60&#20159;&#21442;&#25968;&#20294;&#23569;&#20110;1000&#20159;&#30340;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;MLMs&#22312;&#38646;-shot&#29983;&#25104;&#24335;&#38382;&#31572;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#36825;&#35201;&#27714;&#27169;&#22411;&#25552;&#20379;&#35814;&#32454;&#30340;&#31572;&#26696;&#32780;&#26080;&#38656;&#22806;&#37096;&#25991;&#26723;&#26816;&#32034;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#32473;&#20986;&#20102;&#20154;&#31867;&#35780;&#20272;&#30340;&#32467;&#26524;&#65292;&#32467;&#26524;&#26174;&#31034;&#23558;&#19981;&#21516;MLMs&#30340;&#26368;&#20339;&#31572;&#26696;&#32452;&#21512;&#21487;&#20197;&#23454;&#29616;82.7%&#30340;&#25972;&#20307;&#27491;&#30830;&#29575;&#65292;&#20248;&#20110;ChatGPT&#30340;60.9%&#12290;&#34920;&#29616;&#26368;&#22909;&#30340;MLM&#23454;&#29616;&#20102;46.4%&#65292;&#20854;&#20855;&#26377;70&#20159;&#21442;&#25968;&#65292;&#24378;&#35843;&#20102;&#20351;&#29992;&#36866;&#24403;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#36182;&#20110;&#21442;&#25968;&#25968;&#37327;&#12290;&#26356;&#32454;&#31890;&#24230;&#30340;&#21453;&#39304;&#24212;&#35813;&#34987;&#29992;&#20110;&#36827;&#19968;&#27493;&#25552;&#39640;&#31572;&#26696;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have garnered significant attention, but the definition of "large" lacks clarity. This paper focuses on medium-sized lan-guage models (MLMs), defined as having at least six billion parameters but less than 100 billion. The study evaluates MLMs regarding zero-shot genera-tive question answering, which requires models to provide elaborate answers without external document retrieval. The paper introduces an own test da-taset and presents results from human evaluation. Results show that combin-ing the best answers from different MLMs yielded an overall correct answer rate of 82.7% which is better than the 60.9% of ChatGPT. The best MLM achieved 46.4% and has 7B parameters, which highlights the importance of using appropriate training data for fine-tuning rather than solely relying on the number of parameters. More fine-grained feedback should be used to further improve the quality of answers.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#23450;&#20041;&#12289;&#21457;&#23637;&#21644;&#25361;&#25112;&#65292;&#35752;&#35770;&#20102;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#26500;&#24314;&#26356;&#22909;&#24615;&#33021;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.07611</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sentiment Analysis: A Survey. (arXiv:2305.07611v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#23450;&#20041;&#12289;&#21457;&#23637;&#21644;&#25361;&#25112;&#65292;&#35752;&#35770;&#20102;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#26500;&#24314;&#26356;&#22909;&#24615;&#33021;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36825;&#39033;&#25216;&#26415;&#24050;&#32463;&#36798;&#21040;&#20102;&#26032;&#30340;&#39640;&#24230;&#12290;&#23427;&#22312;&#24212;&#29992;&#21644;&#30740;&#31350;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#27492;&#25104;&#20026;&#20102;&#19968;&#20010;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#23450;&#20041;&#12289;&#32972;&#26223;&#21644;&#21457;&#23637;&#27010;&#36848;&#12290;&#23427;&#36824;&#28085;&#30422;&#20102;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#20808;&#36827;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#35813;&#25216;&#26415;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#21069;&#26223;&#12290;&#26368;&#21518;&#65292;&#23427;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#38656;&#35201;&#25351;&#20986;&#30340;&#26159;&#65292;&#26412;&#32508;&#36848;&#20026;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#26500;&#24314;&#26356;&#22909;&#24615;&#33021;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#25552;&#20379;&#20102;&#24314;&#35774;&#24615;&#30340;&#24314;&#35758;&#65292;&#26377;&#21161;&#20110;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal sentiment analysis has become an important research area in the field of artificial intelligence. With the latest advances in deep learning, this technology has reached new heights. It has great potential for both application and research, making it a popular research topic. This review provides an overview of the definition, background, and development of multimodal sentiment analysis. It also covers recent datasets and advanced models, emphasizing the challenges and future prospects of this technology. Finally, it looks ahead to future research directions. It should be noted that this review provides constructive suggestions for promising research directions and building better performing multimodal sentiment analysis models, which can help researchers in this field.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2305.02531</link><description>&lt;p&gt;
&#35821;&#35328;&#12289;&#26102;&#38388;&#20559;&#22909;&#21644;&#28040;&#36153;&#34892;&#20026;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models. (arXiv:2305.02531v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#23545;&#25105;&#20204;&#23545;&#26102;&#38388;&#21644;&#22870;&#21169;&#30340;&#24863;&#30693;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#20197;&#19981;&#21516;&#30340;&#35821;&#35328;&#35810;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#23427;&#20204;&#26159;&#21542;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#36873;&#25321;&#26159;&#21542;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;GPT-3.5&#65288;&#20197;&#19979;&#31616;&#31216;GPT&#65289;&#22312;&#22810;&#31181;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#21709;&#24212;&#65292;&#25506;&#32034;&#20102;&#36739;&#23567;&#12289;&#36739;&#26089;&#30340;&#22870;&#21169;&#21644;&#36739;&#22823;&#12289;&#36739;&#26202;&#30340;&#22870;&#21169;&#20043;&#38388;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#20197;&#35821;&#20041;&#21547;&#20041;&#36739;&#24369;&#30340;&#26410;&#26469;&#26102;&#24577;&#21442;&#32771;&#65288;FTR&#65289;&#65292;&#22914;&#24503;&#35821;&#21644;&#27721;&#35821;&#65292;&#20026;&#25552;&#31034;&#35821;&#26102;&#65292;GPT&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#30456;&#27604;&#33521;&#35821;&#21644;&#27861;&#35821;&#31561;&#20855;&#26377;&#24378;&#22823;FTR&#30340;&#35821;&#35328;&#12290;&#36825;&#20123;&#21457;&#29616;&#19982;&#29616;&#26377;&#25991;&#29486;&#19968;&#33268;&#65292;&#24182;&#34920;&#26126;&#20102;GPT&#30340;&#36873;&#25321;&#19982;&#36825;&#20123;&#35821;&#35328;&#30340;&#20351;&#29992;&#32773;&#30340;&#20559;&#22909;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36739;&#26089;&#25110;&#36739;&#26202;&#22870;&#21169;&#30340;&#20559;&#22909;&#24182;&#27809;&#26377;&#38543;&#30528;&#22870;&#21169;&#24046;&#24322;&#31995;&#32479;&#22320;&#25913;&#21464;&#65292;&#36825;&#34920;&#26126;&#20102;&#19968;&#31181;&#35789;&#20856;&#24207;&#20248;&#20808;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language has a strong influence on our perceptions of time and rewards. This raises the question of whether large language models, when asked in different languages, show different preferences for rewards over time and if their choices are similar to those of humans. In this study, we analyze the responses of GPT-3.5 (hereafter referred to as GPT) to prompts in multiple languages, exploring preferences between smaller, sooner rewards and larger, later rewards. Our results show that GPT displays greater patience when prompted in languages with weak future tense references (FTR), such as German and Mandarin, compared to languages with strong FTR, like English and French. These findings are consistent with existing literature and suggest a correlation between GPT's choices and the preferences of speakers of these languages. However, further analysis reveals that the preference for earlier or later rewards does not systematically change with reward gaps, indicating a lexicographic preferen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Distilling Step-by-Step&#26426;&#21046;&#65292;&#36890;&#36807;&#25552;&#21462;LLM&#22522;&#30784;&#20449;&#24687;&#20026;&#23567;&#22411;&#27169;&#22411;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#32988;&#36807;&#26356;&#22823;&#30340;LLM&#27169;&#22411;&#65292;&#24182;&#38656;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.02301</link><description>&lt;p&gt;
Distilling Step-by-Step&#65281;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26356;&#23567;&#30340;&#27169;&#22411;&#23610;&#23544;&#32988;&#36807;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. (arXiv:2305.02301v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Distilling Step-by-Step&#26426;&#21046;&#65292;&#36890;&#36807;&#25552;&#21462;LLM&#22522;&#30784;&#20449;&#24687;&#20026;&#23567;&#22411;&#27169;&#22411;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#32988;&#36807;&#26356;&#22823;&#30340;LLM&#27169;&#22411;&#65292;&#24182;&#38656;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38754;&#20020;&#20869;&#23384;&#25928;&#29575;&#20302;&#21644;&#35745;&#31639;&#23494;&#38598;&#24230;&#39640;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#24494;&#35843;&#25110;&#31934;&#28860;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#26631;&#31614;&#26469;&#35757;&#32451;&#36739;&#23567;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#35201;&#24819;&#36798;&#21040;LLM&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Distilling Step-by-Step&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292; (a)&#35757;&#32451;&#36739;&#23567;&#30340;&#27169;&#22411;&#27604;LLM&#34920;&#29616;&#26356;&#22909;&#65292;(b)&#24182;&#36890;&#36807;&#21033;&#29992;&#24494;&#35843;&#25110;&#31934;&#28860;&#25152;&#38656;&#30340;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#35757;&#32451;&#26694;&#26550;&#20013;&#25552;&#21462;LLM&#22522;&#30784;&#65292;&#24182;&#20316;&#20026;&#39069;&#22806;&#30340;&#30417;&#30563;&#26469;&#35757;&#32451;&#23567;&#22411;&#27169;&#22411;&#12290;&#22312;&#22235;&#20010;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#21457;&#29616;&#65306;&#31532;&#19968;&#65292;&#19982;&#24494;&#35843;&#21644;&#31934;&#28860;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26426;&#21046;&#20351;&#29992;&#36739;&#23569;&#30340;&#26631;&#35760;/&#26410;&#26631;&#35760;&#35757;&#32451;&#31034;&#20363;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#31532;&#20108;&#65292;&#19982;LLM&#30456;&#27604;&#65292;&#21363;&#20351;&#20351;&#29992;&#26356;&#23567;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20063;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for small models within a multi-task training framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to LLMs, we achieve better performance using substantially smaller m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#27604;&#36739;&#20102;&#20174;&#21477;&#23376;&#32423;&#21035;&#23884;&#20837;&#20013;&#20135;&#29983;&#25991;&#26723;&#32423;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;LASER&#12289;LaBSE&#21644;Sentence BERT&#12290;&#25105;&#20204;&#30528;&#37325;&#27604;&#36739;&#20102;&#36755;&#20837;&#20196;&#29260;&#25968;&#25130;&#26029;&#12289;&#21477;&#23376;&#24179;&#22343;&#20197;&#21450;&#19968;&#20123;&#31616;&#21333;&#30340;&#31383;&#21475;&#26041;&#27861;&#65292;&#23545;&#19977;&#20010;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#20219;&#21153;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2304.14796</link><description>&lt;p&gt;
&#26368;&#22909;&#30340;&#22810;&#35821;&#35328;&#25991;&#26723;&#23884;&#20837;&#26159;&#21542;&#20165;&#22522;&#20110;&#21477;&#23376;&#23884;&#20837;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are the Best Multilingual Document Embeddings simply Based on Sentence Embeddings?. (arXiv:2304.14796v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#27604;&#36739;&#20102;&#20174;&#21477;&#23376;&#32423;&#21035;&#23884;&#20837;&#20013;&#20135;&#29983;&#25991;&#26723;&#32423;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;LASER&#12289;LaBSE&#21644;Sentence BERT&#12290;&#25105;&#20204;&#30528;&#37325;&#27604;&#36739;&#20102;&#36755;&#20837;&#20196;&#29260;&#25968;&#25130;&#26029;&#12289;&#21477;&#23376;&#24179;&#22343;&#20197;&#21450;&#19968;&#20123;&#31616;&#21333;&#30340;&#31383;&#21475;&#26041;&#27861;&#65292;&#23545;&#19977;&#20010;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#20219;&#21153;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#25991;&#26412;&#25968;&#25454;&#30340;&#23494;&#38598;&#21521;&#37327;&#34920;&#24449;&#33267;&#20851;&#37325;&#35201;&#12290;&#20174;&#21407;&#22987;&#25991;&#26412;&#20272;&#35745;&#30340;&#35789;&#23884;&#20837;&#21644;&#21477;&#23376;&#23884;&#20837;&#26159;&#22312;&#22810;&#31181;&#38656;&#35201;&#35821;&#20041;&#29702;&#35299;&#30340;&#20219;&#21153;&#20013;&#23454;&#29616;&#26368;&#26032;&#25104;&#26524;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#38656;&#27714;&#21644;&#32570;&#20047;&#36866;&#24403;&#30340;&#25968;&#25454;&#65292;&#33719;&#21462;&#25991;&#26723;&#32423;&#21035;&#30340;&#23884;&#20837;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30456;&#21453;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#36864;&#32780;&#20351;&#29992;&#22522;&#20110;&#21477;&#23376;&#34920;&#31034;&#30340;&#25991;&#26723;&#23884;&#20837;&#35745;&#31639;&#12290;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#29992;&#20110;&#23436;&#20840;&#32534;&#30721;&#25991;&#26723;&#30340;&#20307;&#31995;&#32467;&#26500;&#21644;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20165;&#38480;&#20110;&#33521;&#35821;&#21644;&#20854;&#20182;&#20960;&#31181;&#39640;&#36164;&#28304;&#35821;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;LASER&#12289;LaBSE&#21644;Sentence BERT&#65292;&#31995;&#32479;&#27604;&#36739;&#20174;&#21477;&#23376;&#20013;&#20135;&#29983;&#25991;&#26723;&#32423;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#27604;&#36739;&#36755;&#20837;&#20196;&#29260;&#25968;&#25130;&#26029;&#12289;&#21477;&#23376;&#24179;&#22343;&#20197;&#21450;&#19968;&#20123;&#31616;&#21333;&#30340;&#31383;&#21475;&#26041;&#27861;&#65292;&#22312;&#19977;&#20010;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense vector representations for textual data are crucial in modern NLP. Word embeddings and sentence embeddings estimated from raw texts are key in achieving state-of-the-art results in various tasks requiring semantic understanding. However, obtaining embeddings at the document level is challenging due to computational requirements and lack of appropriate data. Instead, most approaches fall back on computing document embeddings based on sentence representations. Although there exist architectures and models to encode documents fully, they are in general limited to English and few other high-resourced languages. In this work, we provide a systematic comparison of methods to produce document-level representations from sentences based on LASER, LaBSE, and Sentence BERT pre-trained multilingual models. We compare input token number truncation, sentence averaging as well as some simple windowing and in some cases new augmented and learnable approaches, on 3 multiand cross-lingual tasks 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#25324;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;Bengali&#25991;&#26412;&#30340;&#26032;&#25968;&#25454;&#38598;- BenCoref&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;Bengali&#22810;&#20010;&#39046;&#22495;&#20013;&#20849;&#25351;&#28040;&#35299;&#29616;&#35937;&#30340;&#24046;&#24322;&#65292;&#24182;&#20419;&#36827;Bengali&#30340;&#36164;&#28304;&#24320;&#21457;&#12290;&#22810;&#20010;&#27169;&#22411;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#24615;&#33021;&#20063;&#24471;&#21040;&#20102;&#25253;&#21578;&#12290;&#22312;&#36328;&#35821;&#35328;&#27979;&#35797;&#20013;&#65292;&#20174;&#33521;&#35821;&#21040;Bengali&#30340;&#20132;&#21449;&#35821;&#35328;&#24615;&#33021;&#36739;&#24046;&#65292;&#26174;&#31034;&#20986;&#38656;&#35201;&#35821;&#35328;&#29305;&#23450;&#30340;&#20849;&#25351;&#28040;&#35299;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2304.03682</link><description>&lt;p&gt;
BenCoref:&#19968;&#31181;&#21517;&#35789;&#30701;&#35821;&#21644;&#20195;&#35789;&#25351;&#20195;&#27880;&#37322;&#30340;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BenCoref: A Multi-Domain Dataset of Nominal Phrases and Pronominal Reference Annotations. (arXiv:2304.03682v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#25324;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;Bengali&#25991;&#26412;&#30340;&#26032;&#25968;&#25454;&#38598;- BenCoref&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;Bengali&#22810;&#20010;&#39046;&#22495;&#20013;&#20849;&#25351;&#28040;&#35299;&#29616;&#35937;&#30340;&#24046;&#24322;&#65292;&#24182;&#20419;&#36827;Bengali&#30340;&#36164;&#28304;&#24320;&#21457;&#12290;&#22810;&#20010;&#27169;&#22411;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#24615;&#33021;&#20063;&#24471;&#21040;&#20102;&#25253;&#21578;&#12290;&#22312;&#36328;&#35821;&#35328;&#27979;&#35797;&#20013;&#65292;&#20174;&#33521;&#35821;&#21040;Bengali&#30340;&#20132;&#21449;&#35821;&#35328;&#24615;&#33021;&#36739;&#24046;&#65292;&#26174;&#31034;&#20986;&#38656;&#35201;&#35821;&#35328;&#29305;&#23450;&#30340;&#20849;&#25351;&#28040;&#35299;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#25351;&#28040;&#35299;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;Bengali &#30340;&#20849;&#25351;&#28040;&#35299;&#30740;&#31350;&#20027;&#35201;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;BenCoref&#65292;&#21253;&#25324;&#26469;&#33258;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;Bengali&#25991;&#26412;&#30340;&#20849;&#25351;&#27880;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;5200&#20010;&#25552;&#21450;&#27880;&#37322;&#65292;&#24418;&#25104;48,569&#20010;&#26631;&#35760;&#20013;&#30340;502&#20010;&#25552;&#21450;&#31751;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#21019;&#24314;&#27492;&#25968;&#25454;&#38598;&#30340;&#36807;&#31243;&#65292;&#24182;&#25253;&#21578;&#20102;&#20351;&#29992;BenCoref&#35757;&#32451;&#30340;&#22810;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39044;&#35745;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#25581;&#31034;Bengali&#22810;&#20010;&#39046;&#22495;&#20013;&#20849;&#25351;&#29616;&#35937;&#30340;&#24046;&#24322;&#65292;&#24182;&#40723;&#21169;&#24320;&#21457;&#20854;&#20182;Bengali&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20174;&#33521;&#35821;&#21040;Bengali&#30340;&#20132;&#21449;&#35821;&#35328;&#24615;&#33021;&#24456;&#24046;&#65292;&#36825;&#31361;&#26174;&#20986;&#38656;&#35201;&#35821;&#35328;&#29305;&#23450;&#30340;&#20849;&#25351;&#28040;&#35299;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coreference Resolution is a well studied problem in NLP. While widely studied for English and other resource-rich languages, research on coreference resolution in Bengali largely remains unexplored due to the absence of relevant datasets. Bengali, being a low-resource language, exhibits greater morphological richness compared to English. In this article, we introduce a new dataset, BenCoref, comprising coreference annotations for Bengali texts gathered from four distinct domains. This relatively small dataset contains 5200 mention annotations forming 502 mention clusters within 48,569 tokens. We describe the process of creating this dataset and report performance of multiple models trained using BenCoref. We anticipate that our work sheds some light on the variations in coreference phenomena across multiple domains in Bengali and encourages the development of additional resources for Bengali. Furthermore, we found poor crosslingual performance at zero-shot setting from English, highlig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39046;&#22495;&#8212;&#8212;&#26426;&#22120;&#24515;&#29702;&#23398;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#32771;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#35813;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#24182;&#23545;&#24515;&#29702;&#23454;&#39564;&#20013;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2303.13988</link><description>&lt;p&gt;
&#26426;&#22120;&#24515;&#29702;&#23398;&#65306;&#21033;&#29992;&#24515;&#29702;&#23398;&#26041;&#27861;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#21644;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39046;&#22495;&#8212;&#8212;&#26426;&#22120;&#24515;&#29702;&#23398;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#32771;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#35813;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#24182;&#23545;&#24515;&#29702;&#23454;&#39564;&#20013;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#23558;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#32039;&#23494;&#32467;&#21512;&#30340;&#20808;&#38155;&#12290;&#30001;&#20110;&#24555;&#36895;&#25216;&#26415;&#36827;&#27493;&#21644;&#20854;&#26497;&#39640;&#30340;&#36890;&#29992;&#24615;&#65292;&#29616;&#20170;LLM&#24050;&#32463;&#25317;&#26377;&#25968;&#30334;&#19975;&#29992;&#25143;&#65292;&#24182;&#27491;&#22788;&#20110;&#25104;&#20026;&#20027;&#35201;&#20449;&#24687;&#26816;&#32034;&#12289;&#20869;&#23481;&#29983;&#25104;&#12289;&#38382;&#39064;&#35299;&#20915;&#31561;&#25216;&#26415;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#23545;&#20854;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#21644;&#23457;&#26597;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#30001;&#20110;&#24403;&#21069;LLM&#20013;&#20986;&#29616;&#24840;&#21152;&#22797;&#26434;&#21644;&#26032;&#39062;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#21487;&#23558;&#20854;&#35270;&#20026;&#21442;&#19982;&#20154;&#31867;&#24515;&#29702;&#23454;&#39564;&#30340;&#23545;&#35937;&#65292;&#20197;&#20415;&#26356;&#20026;&#20840;&#38754;&#22320;&#35780;&#20272;&#20854;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;"&#26426;&#22120;&#24515;&#29702;&#23398;"&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#21508;&#31867;&#24515;&#29702;&#23398;&#20998;&#25903;&#22914;&#20309;&#20026;LLM&#30340;&#34892;&#20026;&#27979;&#35797;&#25552;&#20379;&#26377;&#29992;&#21442;&#32771;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#29305;&#21035;&#26159;&#19987;&#27880;&#20110;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#30340;&#21046;&#23450;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25551;&#36848;&#20102;&#34892;&#20026;&#27979;&#35797;&#32467;&#26524;&#22914;&#20309;&#20026;&#26410;&#26469;&#30340;LLM&#21457;&#23637;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called "machine psychology". The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behaviora
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25191;&#34892;&#32467;&#26524;&#26469;&#39564;&#35777;&#29983;&#25104;&#30340;&#31243;&#24207;&#30340;&#31616;&#21333;&#26041;&#27861;LEVER&#65292;&#36890;&#36807;&#35757;&#32451;&#39564;&#35777;&#22120;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#12289;&#31243;&#24207;&#26412;&#36523;&#21644;&#25191;&#34892;&#32467;&#26524;&#26469;&#30830;&#23450;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2302.08468</link><description>&lt;p&gt;
LEVER: &#20351;&#29992;&#25191;&#34892;&#36827;&#34892;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#23398;&#20064;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
LEVER: Learning to Verify Language-to-Code Generation with Execution. (arXiv:2302.08468v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08468
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25191;&#34892;&#32467;&#26524;&#26469;&#39564;&#35777;&#29983;&#25104;&#30340;&#31243;&#24207;&#30340;&#31616;&#21333;&#26041;&#27861;LEVER&#65292;&#36890;&#36807;&#35757;&#32451;&#39564;&#35777;&#22120;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#12289;&#31243;&#24207;&#26412;&#36523;&#21644;&#25191;&#34892;&#32467;&#26524;&#26469;&#30830;&#23450;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22312;&#20195;&#30721;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;code LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#24050;&#32463;&#22312;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#27492;&#39046;&#22495;&#30340;&#26368;&#26032;&#26041;&#27861;&#23558;LLM&#35299;&#30721;&#19982;&#20351;&#29992;&#27979;&#35797;&#29992;&#20363;&#25110;&#22522;&#20110;&#25191;&#34892;&#32467;&#26524;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#26679;&#26412;&#20462;&#21098;&#21644;&#37325;&#26032;&#25490;&#24207;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#21040;&#20195;&#30721;&#24212;&#29992;&#26469;&#35828;&#65292;&#33719;&#21462;&#27979;&#35797;&#29992;&#20363;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#32780;&#21551;&#21457;&#24335;&#26041;&#27861;&#19981;&#33021;&#24456;&#22909;&#22320;&#25429;&#25417;&#25191;&#34892;&#32467;&#26524;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#27604;&#22914;&#25968;&#25454;&#31867;&#22411;&#21644;&#20540;&#33539;&#22260;&#65292;&#36825;&#24448;&#24448;&#34920;&#26126;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEVER&#65292;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#20351;&#29992;&#25191;&#34892;&#32467;&#26524;&#26469;&#39564;&#35777;&#29983;&#25104;&#30340;&#31243;&#24207;&#65292;&#20174;&#32780;&#25913;&#36827;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35757;&#32451;&#39564;&#35777;&#22120;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#12289;&#31243;&#24207;&#26412;&#36523;&#21644;&#25191;&#34892;&#32467;&#26524;&#26469;&#30830;&#23450;&#20174;LLM&#20013;&#25277;&#26679;&#30340;&#31243;&#24207;&#26159;&#21542;&#27491;&#30830;&#12290;&#36890;&#36807;&#23558;&#39564;&#35777;&#20998;&#25968;&#19982;LLM&#29983;&#25104;&#20998;&#25968;&#30456;&#32467;&#21512;&#65292;&#23545;&#25277;&#26679;&#30340;&#31243;&#24207;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.04391</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#26032;&#26631;&#31614;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23384;&#22312;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#24320;&#21457;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;90&#20998;&#20197;&#19978;&#30340;&#25104;&#32489;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25214;&#20986;&#22122;&#22768;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#27169;&#22411;&#39044;&#27979;&#20316;&#20026;&#20154;&#31867;&#26631;&#35760;&#30340;&#21442;&#32771;&#26469;&#37325;&#26032;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24819;&#27861;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24207;&#21015;&#26631;&#35760;&#12289;&#29289;&#20307;&#26816;&#27979;&#12289;&#24207;&#21015;&#29983;&#25104;&#12289;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In industry deep learning application, our manually labeled data has a certain number of noisy data. To solve this problem and achieve more than 90 score in dev dataset, we present a simple method to find the noisy data and re-label the noisy data by human, given the model predictions as references in human labeling. In this paper, we illustrate our idea for a broad set of deep learning tasks, includes classification, sequence tagging, object detection, sequence generation, click-through rate prediction. The experimental results and human evaluation results verify our idea.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#24615;&#21035;&#20013;&#24615;&#21270;&#32763;&#35793;&#65288;GNT&#65289;&#20316;&#20026;&#19968;&#31181;&#24615;&#21035;&#21253;&#23481;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#33521;&#35821;&#21040;&#24847;&#22823;&#21033;&#35821;&#30340;&#32763;&#35793;&#26159;&#19968;&#20010;&#31361;&#20986;&#30340;&#24615;&#21035;&#30456;&#20851;&#30340;&#35821;&#35328;&#32763;&#35793;&#38382;&#39064;&#12290;&#30740;&#31350;&#22238;&#39038;&#20102;&#19968;&#20123;&#30456;&#20851;&#30340;&#24615;&#21035;&#21253;&#23481;&#24615;&#35821;&#35328;&#25351;&#21335;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;GNT&#30340;&#24773;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;MT&#20013;&#25191;&#34892;GNT&#30340;&#25216;&#26415;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.10075</link><description>&lt;p&gt;
&#20855;&#26377;&#21253;&#23481;&#24615;&#30340;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#21035;&#20013;&#24615;&#21270;&#65306;&#20174;&#29702;&#35770;&#22522;&#30784;&#21040;&#24320;&#25918;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Gender Neutralization for an Inclusive Machine Translation: from Theoretical Foundations to Open Challenges. (arXiv:2301.10075v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10075
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#24615;&#21035;&#20013;&#24615;&#21270;&#32763;&#35793;&#65288;GNT&#65289;&#20316;&#20026;&#19968;&#31181;&#24615;&#21035;&#21253;&#23481;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#33521;&#35821;&#21040;&#24847;&#22823;&#21033;&#35821;&#30340;&#32763;&#35793;&#26159;&#19968;&#20010;&#31361;&#20986;&#30340;&#24615;&#21035;&#30456;&#20851;&#30340;&#35821;&#35328;&#32763;&#35793;&#38382;&#39064;&#12290;&#30740;&#31350;&#22238;&#39038;&#20102;&#19968;&#20123;&#30456;&#20851;&#30340;&#24615;&#21035;&#21253;&#23481;&#24615;&#35821;&#35328;&#25351;&#21335;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;GNT&#30340;&#24773;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;MT&#20013;&#25191;&#34892;GNT&#30340;&#25216;&#26415;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#25216;&#26415;&#20013;&#30340;&#24615;&#21035;&#21253;&#23481;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#24615;&#21035;&#20013;&#24615;&#21270;&#32763;&#35793;&#65288;GNT&#65289;&#20316;&#20026;&#19968;&#31181;&#24615;&#21035;&#21253;&#23481;&#24615;&#21644;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#27169;&#22411;&#25152;&#35201;&#23454;&#29616;&#30340;&#30446;&#26631;&#65292;&#36825;&#20123;&#27169;&#22411;&#34987;&#21457;&#29616;&#20855;&#26377;&#24310;&#32493;&#24615;&#21035;&#20559;&#35265;&#21644;&#27495;&#35270;&#30340;&#20542;&#21521;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#33521;&#35793;&#24847;&#36825;&#23545;&#35821;&#35328;&#65292;&#23427;&#20195;&#34920;&#20102;&#31361;&#20986;&#30340;&#19982;&#24615;&#21035;&#26377;&#20851;&#30340;&#35821;&#35328;&#36716;&#31227;&#38382;&#39064;&#12290;&#20026;&#20102;&#23450;&#20041;GNT&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#19968;&#20123;&#30456;&#20851;&#30340;&#26426;&#26500;&#24615;&#21035;&#21253;&#23481;&#24615;&#35821;&#35328;&#25351;&#21335;&#65292;&#35752;&#35770;&#20102;&#20351;&#29992;GNT&#30340;&#24773;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;MT&#20013;&#25191;&#34892;GNT&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#40723;&#21169;&#26397;&#30528;&#26356;&#22823;&#30340;&#21253;&#23481;&#24615;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gender inclusivity in language technologies has become a prominent research topic. In this study, we explore gender-neutral translation (GNT) as a form of gender inclusivity and a goal to be achieved by machine translation (MT) models, which have been found to perpetuate gender bias and discrimination. Specifically, we focus on translation from English into Italian, a language pair representative of salient gender-related linguistic transfer problems. To define GNT, we review a selection of relevant institutional guidelines for gender-inclusive language, discuss its scenarios of use, and examine the technical challenges of performing GNT in MT, concluding with a discussion of potential solutions to encourage advancements toward greater inclusivity in MT.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#20851;&#31995;&#23545;&#40784;&#12290;&#36890;&#36807;&#40723;&#21169;&#35821;&#35328;&#27880;&#24847;&#21147;&#19982;&#35270;&#35273;&#27880;&#24847;&#21147;&#30340;&#19968;&#33268;&#24615;&#26469;&#23454;&#29616;&#20851;&#31995;&#32423;&#21035;&#30340;&#23545;&#40784;&#65292;&#20174;&#32780;&#25552;&#39640;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#27010;&#25324;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10549</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#20851;&#31995;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment. (arXiv:2212.10549v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#20851;&#31995;&#23545;&#40784;&#12290;&#36890;&#36807;&#40723;&#21169;&#35821;&#35328;&#27880;&#24847;&#21147;&#19982;&#35270;&#35273;&#27880;&#24847;&#21147;&#30340;&#19968;&#33268;&#24615;&#26469;&#23454;&#29616;&#20851;&#31995;&#32423;&#21035;&#30340;&#23545;&#40784;&#65292;&#20174;&#32780;&#25552;&#39640;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#27010;&#25324;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#32852;&#21512;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#19981;&#26029;&#25193;&#22823;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#35832;&#22914;Winoground&#31561;&#32452;&#21512;&#27010;&#25324;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#21069;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#20851;&#31995;&#32423;&#21035;&#30340;&#23545;&#40784;&#65306;&#21363;&#33021;&#22815;&#23558;&#25991;&#26412;&#20013;&#30340;&#23450;&#21521;&#35821;&#20041;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#8220;&#33609;&#22378;&#20013;&#30340;&#26479;&#23376;&#8221;&#65289;&#19982;&#22270;&#20687;&#20013;&#30340;&#31354;&#38388;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#26479;&#23376;&#30456;&#23545;&#20110;&#33609;&#22378;&#30340;&#20301;&#32622;&#65289;&#36827;&#34892;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#40723;&#21169;&#20174;&#8220;&#26479;&#23376;&#8221;&#21040;&#8220;&#33609;&#22378;&#8221;&#65288;&#25429;&#25417;&#35821;&#20041;&#20851;&#31995;&#8220;&#22312;&#8221;&#65289;&#30340;&#23450;&#21521;&#35821;&#35328;&#27880;&#24847;&#21147;&#19982;&#20174;&#26479;&#23376;&#21040;&#33609;&#22378;&#30340;&#23450;&#21521;&#35270;&#35273;&#27880;&#24847;&#21147;&#30456;&#21305;&#37197;&#26469;&#23454;&#29616;&#20851;&#31995;&#23545;&#40784;&#12290;&#36890;&#36807;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#36719;&#24615;&#22320;&#35782;&#21035;&#26631;&#35760;&#21450;&#20854;&#23545;&#24212;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#36719;&#24615;&#20851;&#31995;&#23545;&#40784;&#30340;&#27010;&#24565;&#31561;&#21516;&#20110;&#22312;&#30001;&#36328;&#27169;&#24577;&#25552;&#20379;&#30340;&#8220;&#22522;&#24213;&#21464;&#25442;&#8221;&#19979;&#23454;&#26045;&#35270;&#35273;&#21644;&#35821;&#35328;&#27880;&#24847;&#21147;&#30697;&#38453;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress towards scaling up multimodal vision-language models, these models are still known to struggle on compositional generalization benchmarks such as Winoground. We find that a critical component lacking from current vision-language models is relation-level alignment: the ability to match directional semantic relations in text (e.g., "mug in grass") with spatial relationships in the image (e.g., the position of the mug relative to the grass). To tackle this problem, we show that relation alignment can be enforced by encouraging the directed language attention from 'mug' to 'grass' (capturing the semantic relation 'in') to match the directed visual attention from the mug to the grass. Tokens and their corresponding objects are softly identified using the cross-modal attention. We prove that this notion of soft relation alignment is equivalent to enforcing congruence between vision and language attention matrices under a 'change of basis' provided by the cross-modal a
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36855;&#20320;&#27169;&#22411;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#27973;&#23618;&#36855;&#20320;&#27169;&#22411;&#20197;&#21450;&#39640;&#25928;&#35757;&#32451;&#26032;&#30340;&#35821;&#35328;&#29305;&#23450;&#23884;&#20837;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#25193;&#23637;&#21040;&#26032;&#35821;&#35328;&#19978;&#30340;&#24555;&#36895;&#36328;&#35821;&#35328;&#20256;&#36755;&#12290;</title><link>http://arxiv.org/abs/2212.10503</link><description>&lt;p&gt;
&#36855;&#20320;&#27169;&#22411;&#36866;&#24212;&#65306;&#36890;&#36807;&#23545;&#40784;&#30340;&#27973;&#23618;&#35757;&#32451;&#39640;&#25928;&#22320;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#25193;&#23637;&#21040;&#26032;&#35821;&#35328;&#19978;
&lt;/p&gt;
&lt;p&gt;
Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training. (arXiv:2212.10503v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10503
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36855;&#20320;&#27169;&#22411;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#27973;&#23618;&#36855;&#20320;&#27169;&#22411;&#20197;&#21450;&#39640;&#25928;&#35757;&#32451;&#26032;&#30340;&#35821;&#35328;&#29305;&#23450;&#23884;&#20837;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#25193;&#23637;&#21040;&#26032;&#35821;&#35328;&#19978;&#30340;&#24555;&#36895;&#36328;&#35821;&#35328;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#26032;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#20445;&#25345;transformer&#20027;&#20307;&#37096;&#20998;&#20923;&#32467;&#65292;&#21487;&#20197;&#23558;&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#25193;&#23637;&#21040;&#26032;&#35821;&#35328;&#12290;&#23613;&#31649;&#21482;&#23398;&#20064;&#20102;&#19968;&#23567;&#37096;&#20998;&#21442;&#25968;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#24182;&#19981;&#39640;&#65292;&#22240;&#20026;&#35757;&#32451;&#26032;&#30340;&#23884;&#20837;&#21521;&#37327;&#38656;&#35201;&#23545;&#25972;&#20010;&#27169;&#22411;&#36827;&#34892;&#23436;&#25972;&#30340;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36855;&#20320;&#27169;&#22411;&#36866;&#24212;&#65292;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#20174;&#22823;&#22411;&#27169;&#22411;&#30340;&#19968;&#23567;&#37096;&#20998;&#21442;&#25968;&#20013;&#26500;&#24314;&#19968;&#20010;&#27973;&#23618;&#36855;&#20320;&#27169;&#22411;&#12290;&#28982;&#21518;&#21487;&#20197;&#22312;&#36855;&#20320;&#27169;&#22411;&#19978;&#39640;&#25928;&#22320;&#35757;&#32451;&#26032;&#30340;&#35821;&#35328;&#29305;&#23450;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#23558;&#20854;&#25554;&#20837;&#21040;&#23545;&#40784;&#30340;&#22823;&#22411;&#27169;&#22411;&#20013;&#36827;&#34892;&#24555;&#36895;&#30340;&#36328;&#35821;&#35328;&#20256;&#36755;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#23398;&#20064;&#36855;&#20320;&#27169;&#22411;&#30340;&#26041;&#27861;&#65306;MiniJoint&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#20855;&#26377;&#20013;&#38388;&#23618;&#27425;&#19978;&#36741;&#21161;MLM&#22836;&#30340;&#21333;&#20010;transformer&#21516;&#26102;&#39044;&#35757;&#32451;&#20027;&#27169;&#22411;&#21644;&#36855;&#20320;&#27169;&#22411;&#12290;MiniPost&#21017;&#20174;&#24120;&#35268;&#39044;&#35757;&#32451;&#27169;&#22411;&#24320;&#22987;&#65292;&#36890;&#36807;&#25552;&#21462;&#21644;&#20923;&#32467;&#20960;&#23618;&#26469;&#26500;&#24314;&#36855;&#20320;&#27169;&#22411;&#65292;&#24182;&#23398;&#20064;&#35821;&#35328;&#29305;&#23450;&#30340;&#23884;&#20837;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work shows that it is possible to expand pretrained Masked Language Models (MLMs) to new languages by learning a new set of embeddings, while keeping the transformer body frozen. Despite learning a small subset of parameters, this approach is not compute-efficient, as training the new embeddings requires a full forward and backward pass over the entire model. We propose mini-model adaptation, a compute-efficient alternative that builds a shallow mini-model from a fraction of a large model's parameters. New language-specific embeddings can then be efficiently trained over the mini-model and plugged into the aligned large model for rapid cross-lingual transfer. We explore two approaches to learn mini-models: MiniJoint, which jointly pretrains the primary model and the mini-model using a single transformer with a secondary MLM head at a middle layer; and MiniPost, where we start from a regular pretrained model, build a mini-model by extracting and freezing a few layers, and learn a 
&lt;/p&gt;</description></item><item><title>RPN&#26159;&#19968;&#31181;&#22522;&#20110;&#35789;&#21521;&#37327;&#32423;&#21035;&#30340;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#20462;&#25913;&#21407;&#22987;&#25991;&#26412;&#30340;&#35789;&#23884;&#20837;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.05961</link><description>&lt;p&gt;
RPN: &#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20013;&#22522;&#20110;&#35789;&#21521;&#37327;&#30340;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
RPN: A Word Vector Level Data Augmentation Algorithm in Deep Learning for Language Understanding. (arXiv:2212.05961v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05961
&lt;/p&gt;
&lt;p&gt;
RPN&#26159;&#19968;&#31181;&#22522;&#20110;&#35789;&#21521;&#37327;&#32423;&#21035;&#30340;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#20462;&#25913;&#21407;&#22987;&#25991;&#26412;&#30340;&#35789;&#23884;&#20837;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#33021;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#21040;&#33258;&#28982;&#35821;&#35328;&#30340;&#22797;&#26434;&#21464;&#21270;&#65292;&#24182;&#19988;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#24212;&#29992;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#8212;&#8212;&#38543;&#26426;&#20301;&#32622;&#22122;&#22768;&#65288;RPN&#65289;&#31639;&#27861;&#65292;&#23427;&#22312;&#35789;&#21521;&#37327;&#32423;&#21035;&#19978;&#36827;&#34892;&#25805;&#20316;&#12290;RPN&#36890;&#36807;&#26681;&#25454;&#36873;&#23450;&#35789;&#21521;&#37327;&#30340;&#29616;&#26377;&#20540;&#24341;&#20837;&#22122;&#22768;&#20462;&#25913;&#21407;&#22987;&#25991;&#26412;&#30340;&#35789;&#23884;&#20837;&#65292;&#20801;&#35768;&#26356;&#32454;&#31890;&#24230;&#30340;&#20462;&#25913;&#24182;&#26356;&#22909;&#22320;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#21464;&#21270;&#12290;&#19982;&#20256;&#32479;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#19981;&#21516;&#65292;RPN&#19981;&#38656;&#35201;&#35745;&#31639;&#22270;&#20013;&#30340;&#26799;&#24230;&#26469;&#36827;&#34892;&#34394;&#25311;&#26679;&#26412;&#26356;&#26032;&#65292;&#20351;&#20854;&#26356;&#23481;&#26131;&#24212;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#31561;&#65292;RPN&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is a widely used technique in machine learning to improve model performance. However, existing data augmentation techniques in natural language understanding (NLU) may not fully capture the complexity of natural language variations, and they can be challenging to apply to large datasets. This paper proposes the Random Position Noise (RPN) algorithm, a novel data augmentation technique that operates at the word vector level. RPN modifies the word embeddings of the original text by introducing noise based on the existing values of selected word vectors, allowing for more fine-grained modifications and better capturing natural language variations. Unlike traditional data augmentation methods, RPN does not require gradients in the computational graph during virtual sample updates, making it simpler to apply to large datasets. Experimental results demonstrate that RPN consistently outperforms existing data augmentation techniques across various NLU tasks, including sentime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;OPUS-MT&#29983;&#24577;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#24320;&#25918;&#24335;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;&#24037;&#20855;&#30340;&#24320;&#21457;&#65292;&#20197;&#21450;&#23427;&#20204;&#19982;&#26368;&#32456;&#29992;&#25143;&#24212;&#29992;&#31243;&#24207;&#12289;&#24320;&#21457;&#24179;&#21488;&#21644;&#19987;&#19994;&#24037;&#20316;&#27969;&#31243;&#30340;&#25972;&#21512;&#12290;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#21644;&#32763;&#35793;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2212.01936</link><description>&lt;p&gt;
&#29992;OPUS-MT&#23454;&#29616;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
Democratizing Neural Machine Translation with OPUS-MT. (arXiv:2212.01936v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;OPUS-MT&#29983;&#24577;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#24320;&#25918;&#24335;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;&#24037;&#20855;&#30340;&#24320;&#21457;&#65292;&#20197;&#21450;&#23427;&#20204;&#19982;&#26368;&#32456;&#29992;&#25143;&#24212;&#29992;&#31243;&#24207;&#12289;&#24320;&#21457;&#24179;&#21488;&#21644;&#19987;&#19994;&#24037;&#20316;&#27969;&#31243;&#30340;&#25972;&#21512;&#12290;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#21644;&#32763;&#35793;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;OPUS&#29983;&#24577;&#31995;&#32479;&#65292;&#37325;&#28857;&#20171;&#32461;&#24320;&#25918;&#24335;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;&#24037;&#20855;&#30340;&#24320;&#21457;&#65292;&#20197;&#21450;&#23427;&#20204;&#19982;&#26368;&#32456;&#29992;&#25143;&#24212;&#29992;&#31243;&#24207;&#12289;&#24320;&#21457;&#24179;&#21488;&#21644;&#19987;&#19994;&#24037;&#20316;&#27969;&#31243;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#27491;&#22312;&#36827;&#34892;&#30340;&#22686;&#21152;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#21644;&#32763;&#35793;&#36136;&#37327;&#30340;&#20219;&#21153;&#65292;&#36824;&#25551;&#36848;&#20102;&#27491;&#22312;&#36827;&#34892;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;&#27169;&#22359;&#21270;&#32763;&#35793;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#38754;&#21521;&#24120;&#35268;&#26700;&#38754;&#21644;&#23567;&#22411;&#35774;&#22791;&#23454;&#29616;&#23454;&#26102;&#32763;&#35793;&#30340;&#36895;&#24230;&#20248;&#21270;&#32039;&#20945;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the OPUS ecosystem with a focus on the development of open machine translation models and tools, and their integration into end-user applications, development platforms and professional workflows. We discuss our on-going mission of increasing language coverage and translation quality, and also describe on-going work on the development of modular translation models and speed-optimized compact solutions for real-time translation on regular desktops and small devices.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;&#24773;&#24863;&#20998;&#26512;&#21644;&#24847;&#35265;&#25366;&#25496;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#24773;&#24863;&#26497;&#24615;&#20998;&#31867;&#25361;&#25112;&#30340;&#24191;&#27867;&#25216;&#26415;&#65292;&#24182;&#36827;&#34892;&#20102;&#21477;&#23376;&#32423;&#20998;&#31867;&#21644;&#35780;&#35770;&#32423;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2211.15536</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;&#24773;&#24863;&#20998;&#26512;&#21644;&#24847;&#35265;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis and opinion mining on E-commerce site. (arXiv:2211.15536v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;&#24773;&#24863;&#20998;&#26512;&#21644;&#24847;&#35265;&#25366;&#25496;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#24773;&#24863;&#26497;&#24615;&#20998;&#31867;&#25361;&#25112;&#30340;&#24191;&#27867;&#25216;&#26415;&#65292;&#24182;&#36827;&#34892;&#20102;&#21477;&#23376;&#32423;&#20998;&#31867;&#21644;&#35780;&#35770;&#32423;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#25110;&#24847;&#35265;&#25366;&#25496;&#26377;&#21161;&#20110;&#35828;&#26126;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#30701;&#35821;&#12290;&#24773;&#24863;&#20998;&#26512;&#26159;&#36817;&#24180;&#26469;&#26368;&#37325;&#35201;&#30340;&#20027;&#39064;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35299;&#20915;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24773;&#24863;&#26497;&#24615;&#20998;&#31867;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#27867;&#30340;&#25216;&#26415;&#26469;&#23545;&#24773;&#24863;&#23545;&#31435;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#36807;&#31243;&#35299;&#37322;&#12290;&#36890;&#36807;&#20998;&#26512;&#30340;&#32467;&#26524;&#65292;&#36827;&#34892;&#20102;&#21477;&#23376;&#32423;&#20998;&#31867;&#21644;&#35780;&#35770;&#32423;&#20998;&#31867;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#24773;&#24863;&#20998;&#26512;&#30740;&#31350;&#30340;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis or opinion mining help to illustrate the phrase NLP (Natural Language Processing). Sentiment analysis has been the most significant topic in recent years. The goal of this study is to solve the sentiment polarity classification challenges in sentiment analysis. A broad technique for categorizing sentiment opposition is presented, along with comprehensive process explanations. With the results of the analysis, both sentence-level classification and review-level categorization are conducted. Finally, we discuss our plans for future sentiment analysis research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23646;&#24615;&#36776;&#21035;&#28508;&#31354;&#38388;&#36827;&#34892;&#35821;&#35328;&#21435;&#27602;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#25237;&#24433;&#21040;&#19968;&#20010;&#33021;&#22815;&#36890;&#36807;&#23646;&#24615;&#23558;&#25991;&#26412;&#36827;&#34892;&#33391;&#22909;&#20998;&#31163;&#30340;&#28508;&#31354;&#38388;&#19978;&#65292;&#26368;&#23567;&#21270;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#65292;&#23454;&#29616;&#20102;&#23545;&#26377;&#27602;&#25991;&#26412;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2210.10329</link><description>&lt;p&gt;
&#24102;&#23646;&#24615;&#36776;&#21035;&#28508;&#31354;&#38388;&#30340;&#35821;&#35328;&#21435;&#27602;&#21270;
&lt;/p&gt;
&lt;p&gt;
Language Detoxification with Attribute-Discriminative Latent Space. (arXiv:2210.10329v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23646;&#24615;&#36776;&#21035;&#28508;&#31354;&#38388;&#36827;&#34892;&#35821;&#35328;&#21435;&#27602;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#25237;&#24433;&#21040;&#19968;&#20010;&#33021;&#22815;&#36890;&#36807;&#23646;&#24615;&#23558;&#25991;&#26412;&#36827;&#34892;&#33391;&#22909;&#20998;&#31163;&#30340;&#28508;&#31354;&#38388;&#19978;&#65292;&#26368;&#23567;&#21270;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#65292;&#23454;&#29616;&#20102;&#23545;&#26377;&#27602;&#25991;&#26412;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20063;&#21487;&#33021;&#29983;&#25104;&#21253;&#21547;&#20398;&#36785;&#12289;&#23041;&#32961;&#21644;&#20149;&#28174;&#31561;&#26377;&#27602;&#25991;&#26412;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20123;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#26088;&#22312;&#20351;&#29992;&#39069;&#22806;&#30340;&#35821;&#35328;&#27169;&#22411;&#25110;&#25200;&#21160;&#26469;&#21435;&#27602;&#21270;&#26377;&#27602;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#36807;&#22810;&#30340;&#20869;&#23384;&#12289;&#35745;&#31639;&#21644;&#26102;&#38388;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25104;&#20026;&#20102;&#20005;&#37325;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928; yet &#39640;&#25928;&#30340;&#35821;&#35328;&#21435;&#27602;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#24102;&#23646;&#24615;&#36776;&#21035;&#30340;&#28508;&#31354;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25237;&#24433;&#22359;&#21644;&#23646;&#24615;&#36776;&#21035;&#22120;&#65292;&#23558;&#21407;&#22987;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#25237;&#24433;&#21040;&#19968;&#20010;&#33021;&#22815;&#36890;&#36807;&#23646;&#24615;&#23558;&#25991;&#26412;&#36827;&#34892;&#33391;&#22909;&#20998;&#31163;&#30340;&#36776;&#21035;&#24615;&#28508;&#31354;&#38388;&#19978;&#12290;&#36825;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#22312;&#26368;&#23567;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#19979;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#20026;&#38750;&#26377;&#27602;&#30340;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Language Models (LMs) have achieved impressive results on natural language understanding tasks, but they can also generate toxic text such as insults, threats, and profanity, limiting their real-world applications. To overcome this issue, a few text generation approaches aim to detoxify toxic texts using additional LMs or perturbations. However, previous methods require excessive memory, computations, and time which are serious bottlenecks in their real-world application. To address such limitations, we propose an effective yet efficient method for language detoxification using an attribute-discriminative latent space. Specifically, we project the latent space of an original Transformer LM onto a discriminative latent space that well-separates texts by their attributes using a projection block and an attribute discriminator. This allows the LM to control the text generation to be non-toxic with minimal memory and computation overhead. We validate our model, Attribute-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;ExSSNeT&#65292;&#36890;&#36807;&#29420;&#21344;&#36229;&#25513;&#30721;&#23376;&#32593;&#32476;&#35757;&#32451;&#21644;KNN-based&#30693;&#35782;&#20256;&#36882;&#65292;&#35299;&#20915;&#20102;&#22266;&#23450;&#26435;&#37325;&#38480;&#21046;&#21644;&#30693;&#35782;&#31215;&#32047;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.10209</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30340;&#29420;&#21344;&#36229;&#25513;&#30721;&#23376;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Exclusive Supermask Subnetwork Training for Continual Learning. (arXiv:2210.10209v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;ExSSNeT&#65292;&#36890;&#36807;&#29420;&#21344;&#36229;&#25513;&#30721;&#23376;&#32593;&#32476;&#35757;&#32451;&#21644;KNN-based&#30693;&#35782;&#20256;&#36882;&#65292;&#35299;&#20915;&#20102;&#22266;&#23450;&#26435;&#37325;&#38480;&#21046;&#21644;&#30693;&#35782;&#31215;&#32047;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#20851;&#27880;&#22312;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#21516;&#26102;&#38543;&#30528;&#26102;&#38388;&#32047;&#31215;&#30693;&#35782;&#12290;&#26368;&#36817;&#65292;Wortsman&#31561;&#20154;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;SupSup&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19968;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#22266;&#23450;&#22522;&#30784;&#32593;&#32476;&#65292;&#24182;&#20026;&#27599;&#20010;&#26032;&#20219;&#21153;&#25214;&#21040;&#19968;&#20010;&#36229;&#25513;&#30721;&#65292;&#20197;&#36873;&#25321;&#24615;&#22320;&#20445;&#30041;&#25110;&#31227;&#38500;&#27599;&#20010;&#26435;&#37325;&#20197;&#20135;&#29983;&#19968;&#20010;&#23376;&#32593;&#32476;&#12290;&#20182;&#20204;&#36890;&#36807;&#19981;&#26356;&#26032;&#32593;&#32476;&#26435;&#37325;&#26469;&#36991;&#20813;&#36951;&#24536;&#12290;&#34429;&#28982;&#27809;&#26377;&#36951;&#24536;&#65292;&#20294;SupSup&#30340;&#24615;&#33021;&#19981;&#20339;&#65292;&#22240;&#20026;&#22266;&#23450;&#26435;&#37325;&#38480;&#21046;&#20102;&#20854;&#34920;&#24449;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#27169;&#22411;&#20869;&#37096;&#27809;&#26377;&#30693;&#35782;&#30340;&#31215;&#32047;&#25110;&#20256;&#36882;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ExSSNeT&#65288;&#29420;&#21344;&#36229;&#25513;&#30721;&#23376;&#32593;&#32476;&#35757;&#32451;&#65289;&#65292;&#23427;&#36827;&#34892;&#20102;&#29420;&#26377;&#19988;&#19981;&#37325;&#21472;&#30340;&#23376;&#32593;&#32476;&#26435;&#37325;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#21518;&#32493;&#20219;&#21153;&#23545;&#20849;&#20139;&#26435;&#37325;&#30340;&#20914;&#31361;&#26356;&#26032;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#20173;&#28982;&#38450;&#27490;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;KNN&#30340;&#30693;&#35782;&#20256;&#36882;&#65288;KKT&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual Learning (CL) methods focus on accumulating knowledge over time while avoiding catastrophic forgetting. Recently, Wortsman et al. (2020) proposed a CL method, SupSup, which uses a randomly initialized, fixed base network (model) and finds a supermask for each new task that selectively keeps or removes each weight to produce a subnetwork. They prevent forgetting as the network weights are not being updated. Although there is no forgetting, the performance of SupSup is sub-optimal because fixed weights restrict its representational power. Furthermore, there is no accumulation or transfer of knowledge inside the model when new tasks are learned. Hence, we propose ExSSNeT (Exclusive Supermask SubNEtwork Training), that performs exclusive and non-overlapping subnetwork weight training. This avoids conflicting updates to the shared weights by subsequent tasks to improve performance while still preventing forgetting. Furthermore, we propose a novel KNN-based Knowledge Transfer (KKT)
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;IsoVec&#65292;&#19968;&#31181;&#25511;&#21046;&#35789;&#21521;&#37327;&#31354;&#38388;&#30456;&#23545;&#21516;&#26500;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Skip-gram&#25439;&#22833;&#20989;&#25968;&#20013;&#21152;&#20837;&#20840;&#23616;&#21516;&#26500;&#24230;&#24230;&#37327;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#21518;&#35789;&#21521;&#37327;&#31354;&#38388;&#30340;&#21516;&#26500;&#24615;&#65292;&#36827;&#32780;&#25913;&#21892;&#20102;&#36328;&#35821;&#35328;&#26144;&#23556;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.05098</link><description>&lt;p&gt;
IsoVec: &#25511;&#21046;&#35789;&#21521;&#37327;&#31354;&#38388;&#30340;&#30456;&#23545;&#21516;&#26500;&#24615;
&lt;/p&gt;
&lt;p&gt;
IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces. (arXiv:2210.05098v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05098
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;IsoVec&#65292;&#19968;&#31181;&#25511;&#21046;&#35789;&#21521;&#37327;&#31354;&#38388;&#30456;&#23545;&#21516;&#26500;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Skip-gram&#25439;&#22833;&#20989;&#25968;&#20013;&#21152;&#20837;&#20840;&#23616;&#21516;&#26500;&#24230;&#24230;&#37327;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#21518;&#35789;&#21521;&#37327;&#31354;&#38388;&#30340;&#21516;&#26500;&#24615;&#65292;&#36827;&#32780;&#25913;&#21892;&#20102;&#36328;&#35821;&#35328;&#26144;&#23556;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21333;&#35821;&#35328;&#35789;&#21521;&#37327;&#31354;&#38388;&#25552;&#21462;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#35789;&#20856;&#30340;&#33021;&#21147;&#21462;&#20915;&#20110;&#31354;&#38388;&#30340;&#20960;&#20309;&#30456;&#20284;&#24615;&#8212;&#8212;&#23427;&#20204;&#30340;&#8220;&#21516;&#26500;&#24230;&#8221;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#26144;&#23556;&#20986;&#29616;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#65306;&#21363;&#35789;&#21521;&#37327;&#35757;&#32451;&#23548;&#33268;&#24213;&#23618;&#31354;&#38388;&#19981;&#21516;&#26500;&#12290;&#25105;&#20204;&#23558;&#21516;&#26500;&#24230;&#30340;&#20840;&#23616;&#24230;&#37327;&#30452;&#25509;&#21512;&#24182;&#21040;Skip-gram&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#25104;&#21151;&#22320;&#22686;&#21152;&#20102;&#35757;&#32451;&#21518;&#35789;&#21521;&#37327;&#31354;&#38388;&#30340;&#30456;&#23545;&#21516;&#26500;&#24230;&#65292;&#24182;&#25552;&#39640;&#20102;&#23427;&#20204;&#26144;&#23556;&#21040;&#20849;&#20139;&#30340;&#36328;&#35821;&#35328;&#31354;&#38388;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#26159;&#22312;&#19968;&#33324;&#25968;&#25454;&#26465;&#20214;&#19979;&#65292;&#39046;&#22495;&#19981;&#21305;&#37197;&#21644;&#35757;&#32451;&#31639;&#27861;&#19981;&#30456;&#20284;&#24773;&#20917;&#19979;&#25913;&#21892;&#20102;&#21452;&#35821;&#35789;&#27719;&#34920;&#24402;&#32435;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;https://github.com/kellymarchisio/isovec &#19978;&#21457;&#24067;&#20102;IsoVec&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to extract high-quality translation dictionaries from monolingual word embedding spaces depends critically on the geometric similarity of the spaces -- their degree of "isomorphism." We address the root-cause of faulty cross-lingual mapping: that word embedding training resulted in the underlying spaces being non-isomorphic. We incorporate global measures of isomorphism directly into the Skip-gram loss function, successfully increasing the relative isomorphism of trained word embedding spaces and improving their ability to be mapped to a shared cross-lingual space. The result is improved bilingual lexicon induction in general data conditions, under domain mismatch, and with training algorithm dissimilarities. We release IsoVec at https://github.com/kellymarchisio/isovec.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#32452;&#21512;&#24615;&#23450;&#20041;&#20026;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#23545;&#31216;&#24615;&#32422;&#26463;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#25968;&#25454;&#36716;&#25442;&#24182;&#24212;&#29992;&#20110;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#32452;&#21512;&#24402;&#32435;&#20559;&#32622;&#12290;</title><link>http://arxiv.org/abs/2201.12926</link><description>&lt;p&gt;
&#12298;&#32452;&#21512;&#24615;&#20316;&#20026;&#35789;&#27719;&#23545;&#31216;&#24615;&#12299;
&lt;/p&gt;
&lt;p&gt;
Compositionality as Lexical Symmetry. (arXiv:2201.12926v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#32452;&#21512;&#24615;&#23450;&#20041;&#20026;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#23545;&#31216;&#24615;&#32422;&#26463;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#25968;&#25454;&#36716;&#25442;&#24182;&#24212;&#29992;&#20110;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#32452;&#21512;&#24402;&#32435;&#20559;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#20041;&#35299;&#26512;&#12289;&#25351;&#20196;&#36981;&#24490;&#21644;&#38382;&#39064;&#22238;&#31572;&#31561;&#20219;&#21153;&#20013;&#65292;&#26631;&#20934;&#30340;&#28145;&#24230;&#32593;&#32476;&#22312;&#20174;&#23567;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#32452;&#21512;&#27867;&#21270;&#26102;&#20250;&#22833;&#36133;&#12290;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#24378;&#21046;&#23454;&#26045;&#21477;&#23376;&#35299;&#37322;&#30340;&#32452;&#21512;&#36807;&#31243;&#30340;&#27169;&#22411;&#26550;&#26500;&#26469;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39046;&#22495;&#36890;&#29992;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#32452;&#21512;&#24615;&#24418;&#24335;&#65292;&#23558;&#20854;&#20316;&#20026;&#25968;&#25454;&#20998;&#24067;&#30340;&#23545;&#31216;&#24615;&#32422;&#26463;&#32780;&#19981;&#26159;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#26080;&#35770;&#20309;&#26102;&#19968;&#20010;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#32452;&#21512;&#27169;&#22411;&#26469;&#35299;&#20915;&#65292;&#37117;&#23384;&#22312;&#19968;&#20010;&#30456;&#24212;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#8212;&#8212;&#23558;&#31034;&#20363;&#36716;&#25442;&#20026;&#20854;&#20182;&#21512;&#36866;&#31034;&#20363;&#30340;&#36807;&#31243;&#8212;&#8212;&#21487;&#20197;&#20026;&#35299;&#20915;&#30456;&#21516;&#20219;&#21153;&#30340;&#20219;&#20309;&#35757;&#32451;&#27169;&#22411;&#36171;&#20104;&#32452;&#21512;&#24402;&#32435;&#20559;&#32622;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#33258;&#21160;&#21457;&#29616;&#36825;&#20123;&#36716;&#25442;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26222;&#36890;&#31070;&#32463;&#24207;&#21015;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#30340;&#36807;&#31243;&#65292;&#31216;&#20026;LEXSYM&#12290;&#19982;&#29616;&#26377;&#30340;&#32452;&#21512;&#25968;&#25454;&#22686;&#24378;&#36807;&#31243;&#19981;&#21516;&#65292;LEXSYM&#21487;&#20197;&#34987;&#24555;&#36895;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
In tasks like semantic parsing, instruction following, and question answering, standard deep networks fail to generalize compositionally from small datasets. Many existing approaches overcome this limitation with model architectures that enforce a compositional process of sentence interpretation. In this paper, we present a domain-general and model-agnostic formulation of compositionality as a constraint on symmetries of data distributions rather than models. Informally, we prove that whenever a task can be solved by a compositional model, there is a corresponding data augmentation scheme -- a procedure for transforming examples into other well formed examples -- that imparts compositional inductive bias on any model trained to solve the same task. We describe a procedure called LEXSYM that discovers these transformations automatically, then applies them to training data for ordinary neural sequence models. Unlike existing compositional data augmentation procedures, LEXSYM can be deplo
&lt;/p&gt;</description></item></channel></rss>