<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#23454;&#29616;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#25913;&#21892;T5&#22312;&#38271;&#24207;&#21015;&#22788;&#29702;&#20013;&#30340;&#27880;&#24847;&#21147;&#20998;&#24067;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#26816;&#32034;&#21644;&#22810;&#25991;&#26723;&#38382;&#31572;&#31561;&#20219;&#21153;&#20013;&#30340;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00684</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#23545;&#40784;&#21644;&#28789;&#27963;&#30340;&#20301;&#32622;&#23884;&#20837;&#25552;&#39640;&#20102;Transformer&#38271;&#24230;&#22806;&#25512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation. (arXiv:2311.00684v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#23454;&#29616;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#25913;&#21892;T5&#22312;&#38271;&#24207;&#21015;&#22788;&#29702;&#20013;&#30340;&#27880;&#24847;&#21147;&#20998;&#24067;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#26816;&#32034;&#21644;&#22810;&#25991;&#26723;&#38382;&#31572;&#31561;&#20219;&#21153;&#20013;&#30340;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24819;&#30340;&#38271;&#24230;&#21487;&#22806;&#25512;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#27604;&#35757;&#32451;&#38271;&#24230;&#26356;&#38271;&#30340;&#24207;&#21015;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#38271;&#24207;&#21015;&#24494;&#35843;&#12290;&#36825;&#31181;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;&#33021;&#21147;&#39640;&#24230;&#20381;&#36182;&#20110;&#28789;&#27963;&#30340;&#20301;&#32622;&#23884;&#20837;&#35774;&#35745;&#12290;&#22312;&#35843;&#26597;&#29616;&#26377;&#22823;&#22411;&#39044;&#35757;&#32451;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;T5&#31995;&#21015;&#20540;&#24471;&#26356;&#20180;&#32454;&#30740;&#31350;&#65292;&#22240;&#20026;&#23427;&#30340;&#20301;&#32622;&#23884;&#20837;&#25429;&#25417;&#21040;&#20102;&#20016;&#23500;&#32780;&#28789;&#27963;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;T5&#23384;&#22312;&#30528;&#20998;&#25955;&#30340;&#27880;&#24847;&#21147;&#38382;&#39064;&#65306;&#36755;&#20837;&#24207;&#21015;&#36234;&#38271;&#65292;&#27880;&#24847;&#21147;&#20998;&#24067;&#23601;&#36234;&#24179;&#22374;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#23454;&#29616;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25552;&#39640;&#20102;T5&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#26816;&#32034;&#21644;&#22810;&#25991;&#26723;&#38382;&#31572;&#26041;&#38754;&#30340;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;&#33021;&#21147;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#65292;&#36825;&#34920;&#26126;&#28789;&#27963;&#30340;&#20301;&#32622;&#23884;&#20837;&#35774;&#35745;&#21644;&#27880;&#24847;&#21147;&#23545;&#40784;&#23545;&#20110;Transformer&#38271;&#24230;&#22806;&#25512;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
An ideal length-extrapolatable Transformer language model can handle sequences longer than the training length without any long sequence fine-tuning. Such long-context utilization capability highly relies on a flexible positional embedding design. Upon investigating the flexibility of existing large pre-trained Transformer language models, we find that the T5 family deserves a closer look, as its positional embeddings capture rich and flexible attention patterns. However, T5 suffers from the dispersed attention issue: the longer the input sequence, the flatter the attention distribution. To alleviate the issue, we propose two attention alignment strategies via temperature scaling. Our findings improve the long-context utilization capability of T5 on language modeling, retrieval, and multi-document question answering without any fine-tuning, suggesting that a flexible positional embedding design and attention alignment go a long way toward Transformer length extrapolation.\footnote{\url
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;Nko&#35821;&#65288;&#19968;&#31181;&#22312;&#22810;&#20010;&#35199;&#38750;&#22269;&#23478;&#20351;&#29992;&#30340;&#35821;&#35328;&#65289;&#24320;&#21457;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#22522;&#20934;&#32467;&#26524;&#65292;&#21253;&#25324;&#26032;&#39062;&#30340;&#21327;&#20316;&#24179;&#34892;&#25991;&#26412;&#25972;&#29702;&#36719;&#20214;&#12289;&#25193;&#23637;&#30340;&#35821;&#26009;&#24211;&#21644;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15612</link><description>&lt;p&gt;
Nko&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#65306;&#24037;&#20855;&#12289;&#35821;&#26009;&#24211;&#21644;&#22522;&#20934;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Machine Translation for Nko: Tools, Corpora and Baseline Results. (arXiv:2310.15612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15612
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;Nko&#35821;&#65288;&#19968;&#31181;&#22312;&#22810;&#20010;&#35199;&#38750;&#22269;&#23478;&#20351;&#29992;&#30340;&#35821;&#35328;&#65289;&#24320;&#21457;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#22522;&#20934;&#32467;&#26524;&#65292;&#21253;&#25324;&#26032;&#39062;&#30340;&#21327;&#20316;&#24179;&#34892;&#25991;&#26412;&#25972;&#29702;&#36719;&#20214;&#12289;&#25193;&#23637;&#30340;&#35821;&#26009;&#24211;&#21644;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#23612;&#31185;&#35821;&#65288;&#19968;&#31181;&#22312;&#22810;&#20010;&#35199;&#38750;&#22269;&#23478;&#20351;&#29992;&#30340;&#35821;&#35328;&#65289;&#27809;&#26377;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#20294;&#23427;&#22312;&#25991;&#21270;&#21644;&#25945;&#32946;&#20215;&#20540;&#19978;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#22522;&#20934;&#32467;&#26524;&#65292;&#26088;&#22312;&#24320;&#21457;&#21487;&#29992;&#30340;&#23612;&#31185;&#35821;&#21644;&#20854;&#20182;&#24403;&#21069;&#27809;&#26377;&#36275;&#22815;&#22823;&#30340;&#24179;&#34892;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#35821;&#35328;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;&#20855;&#20307;&#21253;&#25324;&#65306;(1) Friallel&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#20316;&#24179;&#34892;&#25991;&#26412;&#25972;&#29702;&#36719;&#20214;&#65292;&#36890;&#36807;&#22522;&#20110;&#21103;&#26412;&#32534;&#36753;&#30340;&#24037;&#20316;&#27969;&#31243;&#23454;&#29616;&#36136;&#37327;&#25511;&#21046;&#12290;(2) &#25193;&#23637;&#20102;FLoRes-200&#21644;NLLB-Seed&#35821;&#26009;&#24211;&#65292;&#20174;&#20854;&#20182;&#35821;&#35328;&#20013;&#19982;&#23612;&#31185;&#35821;&#24179;&#34892;&#32763;&#35793;&#20102;2,009&#21644;6,193&#20010;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#12290;(3) nicolingua-0005&#65306;&#21253;&#21547;130,850&#20010;&#24179;&#34892;&#29255;&#27573;&#30340;&#19977;&#35821;&#21644;&#21452;&#35821;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;&#36229;&#36807;3&#30334;&#19975;&#23612;&#31185;&#35821;&#21333;&#35821;&#35328;&#35821;&#26009;&#24211;&#12290;(4) &#22522;&#32447;&#21452;&#35821;&#21644;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#32467;&#26524;&#19982;b...
&lt;/p&gt;
&lt;p&gt;
Currently, there is no usable machine translation system for Nko, a language spoken by tens of millions of people across multiple West African countries, which holds significant cultural and educational value. To address this issue, we present a set of tools, resources, and baseline results aimed towards the development of usable machine translation systems for Nko and other languages that do not currently have sufficiently large parallel text corpora available. (1) Friallel: A novel collaborative parallel text curation software that incorporates quality control through copyedit-based workflows. (2) Expansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193 high-quality Nko translations in parallel with 204 and 40 other languages. (3) nicolingua-0005: A collection of trilingual and bilingual corpora with 130,850 parallel segments and monolingual corpora containing over 3 million Nko words. (4) Baseline bilingual and multilingual neural machine translation results with the b
&lt;/p&gt;</description></item><item><title>MedEval&#26159;&#19968;&#20010;&#22810;&#23618;&#27425;&#12289;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#30340;&#21307;&#23398;&#22522;&#20934;&#65292;&#29992;&#20110;&#20419;&#36827;&#21307;&#30103;&#20445;&#20581;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#23427;&#21253;&#21547;&#20102;&#26469;&#33258;&#22810;&#20010;&#21307;&#30103;&#31995;&#32479;&#30340;&#25968;&#25454;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#20154;&#20307;&#21306;&#22495;&#21644;&#26816;&#26597;&#27169;&#24335;&#12290;&#25105;&#20204;&#38024;&#23545;10&#20010;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#21508;&#26377;&#24046;&#24322;&#65292;&#20174;&#20013;&#25105;&#20204;&#27880;&#24847;&#21040;&#20102;&#25351;&#23548;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.14088</link><description>&lt;p&gt;
MedEval: &#19968;&#20010;&#22810;&#23618;&#27425;&#12289;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation. (arXiv:2310.14088v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14088
&lt;/p&gt;
&lt;p&gt;
MedEval&#26159;&#19968;&#20010;&#22810;&#23618;&#27425;&#12289;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#30340;&#21307;&#23398;&#22522;&#20934;&#65292;&#29992;&#20110;&#20419;&#36827;&#21307;&#30103;&#20445;&#20581;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#23427;&#21253;&#21547;&#20102;&#26469;&#33258;&#22810;&#20010;&#21307;&#30103;&#31995;&#32479;&#30340;&#25968;&#25454;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#20154;&#20307;&#21306;&#22495;&#21644;&#26816;&#26597;&#27169;&#24335;&#12290;&#25105;&#20204;&#38024;&#23545;10&#20010;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#21508;&#26377;&#24046;&#24322;&#65292;&#20174;&#20013;&#25105;&#20204;&#27880;&#24847;&#21040;&#20102;&#25351;&#23548;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#38656;&#35201;&#19987;&#23478;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#21307;&#30103;&#20445;&#20581;&#30340;&#31579;&#36873;&#25968;&#25454;&#38598;&#24448;&#24448;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MedEval&#65292;&#19968;&#20010;&#22810;&#23618;&#27425;&#12289;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#30340;&#21307;&#23398;&#22522;&#20934;&#65292;&#20197;&#20419;&#36827;&#21307;&#30103;&#20445;&#20581;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;MedEval&#26159;&#20840;&#38754;&#30340;&#65292;&#21253;&#21547;&#26469;&#33258;&#20960;&#20010;&#21307;&#30103;&#31995;&#32479;&#30340;&#25968;&#25454;&#65292;&#28085;&#30422;&#20102;8&#31181;&#26816;&#26597;&#27169;&#24335;&#19979;&#30340;35&#20010;&#20154;&#20307;&#21306;&#22495;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;22,779&#20010;&#21477;&#23376;&#21644;21,228&#20221;&#25253;&#21578;&#65292;&#24182;&#22312;&#22810;&#20010;&#23618;&#27425;&#19978;&#25552;&#20379;&#20102;&#19987;&#23478;&#27880;&#37322;&#65292;&#20026;&#25968;&#25454;&#25552;&#20379;&#20102;&#32454;&#33268;&#30340;&#28508;&#22312;&#29992;&#27861;&#65292;&#24182;&#25903;&#25345;&#24191;&#27867;&#30340;&#20219;&#21153;&#33539;&#22260;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#38646;-shot&#21644;&#24494;&#35843;&#35774;&#32622;&#19979;&#23545;10&#20010;&#36890;&#29992;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#20174;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#39046;&#22495;&#36866;&#24212;&#22522;&#32447;&#21040;&#36890;&#29992;&#30340;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#25581;&#31034;&#20102;&#20004;&#31181;&#31867;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#19981;&#21516;&#26377;&#25928;&#24615;&#65292;&#20174;&#20013;&#25105;&#20204;&#27880;&#24847;&#21040;&#20102;&#25351;&#23548;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Curated datasets for healthcare are often limited due to the need of human annotations from experts. In this paper, we present MedEval, a multi-level, multi-task, and multi-domain medical benchmark to facilitate the development of language models for healthcare. MedEval is comprehensive and consists of data from several healthcare systems and spans 35 human body regions from 8 examination modalities. With 22,779 collected sentences and 21,228 reports, we provide expert annotations at multiple levels, offering a granular potential usage of the data and supporting a wide range of tasks. Moreover, we systematically evaluated 10 generic and domain-specific language models under zero-shot and finetuning settings, from domain-adapted baselines in healthcare to general-purposed state-of-the-art large language models (e.g., ChatGPT). Our evaluations reveal varying effectiveness of the two categories of language models across different tasks, from which we notice the importance of instruction t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25512;&#26029;&#24615;&#25490;&#38500;&#25552;&#31034;&#65288;IEP&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#25490;&#38500;&#21644;&#25512;&#29702;&#30340;&#21407;&#21017;&#65292;&#24341;&#23548;LLM&#36827;&#34892;&#38750;&#32447;&#24615;&#24605;&#32771;&#12290;IEP&#36890;&#36807;&#35268;&#21010;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#21487;&#20197;&#27169;&#25311;&#22797;&#26434;&#30340;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2310.12342</link><description>&lt;p&gt;
&#36890;&#36807;&#25512;&#29702;&#19982;&#35268;&#21010;&#28040;&#38500;&#25512;&#29702;&#65306;&#19968;&#31181;&#24341;&#23548;LLMs&#38750;&#32447;&#24615;&#24605;&#32500;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs' Non-linear Thinking. (arXiv:2310.12342v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25512;&#26029;&#24615;&#25490;&#38500;&#25552;&#31034;&#65288;IEP&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#25490;&#38500;&#21644;&#25512;&#29702;&#30340;&#21407;&#21017;&#65292;&#24341;&#23548;LLM&#36827;&#34892;&#38750;&#32447;&#24615;&#24605;&#32771;&#12290;IEP&#36890;&#36807;&#35268;&#21010;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#21487;&#20197;&#27169;&#25311;&#22797;&#26434;&#30340;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Thought Chain&#65288;CoT&#65289;&#25552;&#31034;&#21450;&#20854;&#21464;&#20307;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#32447;&#24615;&#35748;&#30693;&#21644;&#36923;&#36753;&#65292;&#25506;&#32034;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35013;&#22791;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#24605;&#32500;&#22797;&#26434;&#19988;&#28151;&#21512;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#24605;&#32500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#24335;&#65292;&#31216;&#20026;&#25512;&#26029;&#24615;&#25490;&#38500;&#25552;&#31034;&#65288;IEP&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#25490;&#38500;&#21644;&#25512;&#29702;&#30340;&#21407;&#21017;&#65292;&#20197;&#24341;&#23548;LLM&#36827;&#34892;&#38750;&#32447;&#24615;&#24605;&#32771;&#12290;IEP&#25351;&#23548;LLM&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#25512;&#26029;&#27599;&#20010;&#21487;&#33021;&#35299;&#19982;&#19978;&#19979;&#25991;&#12289;&#24120;&#35782;&#25110;&#20107;&#23454;&#30340;&#25512;&#29702;&#20851;&#31995;&#65292;&#20174;&#32780;&#36890;&#36807;&#22238;&#28335;&#25512;&#29702;&#33719;&#24471;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#12290;&#30456;&#27604;&#20854;&#20182;&#22522;&#20110;CoT&#30340;&#26041;&#27861;&#65292;IEP&#30340;&#21069;&#21521;&#35268;&#21010;&#21644;&#21518;&#21521;&#25490;&#38500;&#36807;&#31243;&#26356;&#22909;&#22320;&#27169;&#25311;&#20102;&#22797;&#26434;&#30340;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#21518;&#32773;&#20165;&#21453;&#26144;&#32447;&#24615;&#35748;&#30693;&#36807;&#31243;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#39564;&#35777;&#20102;IEP&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought(CoT) prompting and its variants explore equipping large language models (LLMs) with high-level reasoning abilities by emulating human-like linear cognition and logic. However, the human mind is complicated and mixed with both linear and nonlinear thinking. In this work, we propose \textbf{I}nferential \textbf{E}xclusion \textbf{P}rompting (IEP), a novel prompting that combines the principles of elimination and inference in order to guide LLMs to think non-linearly. IEP guides LLMs to plan and then utilize Natural Language Inference (NLI) to deduce each possible solution's entailment relation with context, commonsense, or facts, therefore yielding a broader perspective by thinking back for inferring. This forward planning and backward eliminating process allows IEP to better simulate the complex human thinking processes compared to other CoT-based methods, which only reflect linear cognitive processes. We conducted a series of empirical studies and have corroborated tha
&lt;/p&gt;</description></item><item><title>VIBE&#26159;&#19968;&#31181;&#35299;&#20915;Twitter&#20998;&#31867;&#20013;&#35821;&#35328;&#29305;&#24449;&#28436;&#21464;&#38382;&#39064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24314;&#27169;&#28508;&#22312;&#20027;&#39064;&#28436;&#21464;&#20197;&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#65292;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;Twitter&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10191</link><description>&lt;p&gt;
VIBE&#65306;Twitter&#20998;&#31867;&#30340;&#20027;&#39064;&#39537;&#21160;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
VIBE: Topic-Driven Temporal Adaptation for Twitter Classification. (arXiv:2310.10191v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10191
&lt;/p&gt;
&lt;p&gt;
VIBE&#26159;&#19968;&#31181;&#35299;&#20915;Twitter&#20998;&#31867;&#20013;&#35821;&#35328;&#29305;&#24449;&#28436;&#21464;&#38382;&#39064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24314;&#27169;&#28508;&#22312;&#20027;&#39064;&#28436;&#21464;&#20197;&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#65292;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;Twitter&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#29305;&#24449;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#31038;&#20132;&#23186;&#20307;&#20013;&#19981;&#26029;&#21464;&#21270;&#65292;&#23548;&#33268;&#25991;&#26412;&#20998;&#31867;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26102;&#38388;&#33258;&#36866;&#24212;&#65292;&#21363;&#22312;&#36807;&#21435;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#26410;&#26469;&#36827;&#34892;&#27979;&#35797;&#12290;&#20808;&#21069;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#25110;&#30693;&#35782;&#26356;&#26032;&#19978;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#23427;&#20204;&#22312;&#22122;&#22768;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#27169;&#28508;&#22312;&#20027;&#39064;&#28436;&#21464;&#26469;&#21453;&#26144;&#29305;&#24449;&#21464;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;VIBE&#65306;Evolutions&#30340;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#20004;&#20010;&#20449;&#24687;&#29942;&#39048;(Bottleneck)&#27491;&#21017;&#21270;&#22120;&#26469;&#21306;&#20998;&#36807;&#21435;&#21644;&#26410;&#26469;&#30340;&#20027;&#39064;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#21306;&#20998;&#30340;&#20027;&#39064;&#36890;&#36807;&#26102;&#38388;&#25139;&#21644;&#31867;&#21035;&#26631;&#31614;&#39044;&#27979;&#36827;&#34892;&#22810;&#20219;&#21153;&#35757;&#32451;&#65292;&#20316;&#20026;&#33258;&#36866;&#24212;&#29305;&#24449;&#12290;&#22312;&#33258;&#36866;&#24212;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;VIBE&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#26102;&#38388;&#20043;&#21518;&#21019;&#24314;&#30340;&#22312;&#32447;&#27969;&#31243;&#20013;&#26816;&#32034;&#21040;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#22312;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;&#22823;&#35268;&#27169;Twitter&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language features are evolving in real-world social media, resulting in the deteriorating performance of text classification in dynamics. To address this challenge, we study temporal adaptation, where models trained on past data are tested in the future. Most prior work focused on continued pretraining or knowledge updating, which may compromise their performance on noisy social media data. To tackle this issue, we reflect feature change via modeling latent topic evolution and propose a novel model, VIBE: Variational Information Bottleneck for Evolutions. Concretely, we first employ two Information Bottleneck (IB) regularizers to distinguish past and future topics. Then, the distinguished topics work as adaptive features via multi-task training with timestamp and class label prediction. In adaptive learning, VIBE utilizes retrieved unlabeled data from online streams created posterior to training data time. Substantial Twitter experiments on three classification tasks show that our mode
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#23545;&#35805;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#21508;&#21521;&#21516;&#24615;&#21644;&#36817;&#31471;&#25628;&#32034;&#65288;IPS&#65289;&#29983;&#25104;&#20449;&#24687;&#38598;&#20013;&#30340;&#35821;&#20041;&#22238;&#24212;&#65292;&#24182;&#22312;&#23545;&#35805;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.08130</link><description>&lt;p&gt;
&#36890;&#36807;&#21508;&#21521;&#21516;&#24615;&#21644;&#36817;&#31471;&#25628;&#32034;&#23454;&#29616;&#32454;&#31890;&#24230;&#23545;&#35805;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Conversational Decoding via Isotropic and Proximal Search. (arXiv:2310.08130v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#23545;&#35805;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#21508;&#21521;&#21516;&#24615;&#21644;&#36817;&#31471;&#25628;&#32034;&#65288;IPS&#65289;&#29983;&#25104;&#20449;&#24687;&#38598;&#20013;&#30340;&#35821;&#20041;&#22238;&#24212;&#65292;&#24182;&#22312;&#23545;&#35805;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#37319;&#29992;&#36890;&#29992;&#30340;&#25991;&#26412;&#35299;&#30721;&#26041;&#27861;&#26469;&#36827;&#34892;&#23545;&#35805;&#22238;&#24212;&#29983;&#25104;&#12290;&#34429;&#28982;&#37319;&#29992;&#20102;&#23545;&#35805;&#29305;&#23450;&#30340;&#32534;&#30721;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#30340;&#22238;&#24212;&#36136;&#37327;&#65292;&#20294;&#23545;&#35805;&#35299;&#30721;&#26041;&#27861;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#21463;&#21040;wu2023learning&#30340;&#21551;&#21457;&#65292;&#35748;&#20026;&#22909;&#30340;&#23545;&#35805;&#29305;&#24449;&#31354;&#38388;&#24212;&#36981;&#24490;&#23616;&#37096;&#24615;&#21644;&#21508;&#21521;&#21516;&#24615;&#35268;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#23545;&#35805;&#35299;&#30721;&#26041;&#27861;&#65292;&#31216;&#20026;&#21508;&#21521;&#21516;&#24615;&#21644;&#36817;&#31471;&#25628;&#32034;&#65288;IPS&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#29983;&#25104;&#20449;&#24687;&#38598;&#20013;&#30340;&#35821;&#20041;&#22238;&#24212;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#37327;&#21644;&#21306;&#20998;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545;&#35805;&#39046;&#22495;&#20013;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
General-purpose text decoding approaches are usually adopted for dialogue response generation. Although the quality of the generated responses can be improved with dialogue-specific encoding methods, conversational decoding methods are still under-explored. Inspired by \citet{wu2023learning} that a good dialogue feature space should follow the rules of locality and isotropy, we present a fine-grained conversational decoding method, termed \textit{isotropic and proximal search (IPS)}. Our method is designed to generate the semantic-concentrated response, while still maintaining informativeness and discrimination against the context. Experiments show that our approach outperforms existing decoding strategies in the dialogue field across both automatic and human evaluation metrics. More in-depth analyses further confirm the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20026;&#33258;&#30417;&#30563;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#21644;&#36866;&#37197;&#22120;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#24207;&#21015;&#29983;&#25104;&#21644;&#36328;&#35821;&#35328;ASR&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#34920;&#29616;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#25552;&#31034;&#26041;&#27861;&#20248;&#20110;&#36866;&#37197;&#22120;&#35843;&#20248;&#12290;</title><link>http://arxiv.org/abs/2310.02971</link><description>&lt;p&gt;
&#20026;&#33258;&#30417;&#30563;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#21644;&#36866;&#37197;&#22120;&#35843;&#20248;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model. (arXiv:2310.02971v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02971
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20026;&#33258;&#30417;&#30563;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#21644;&#36866;&#37197;&#22120;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#24207;&#21015;&#29983;&#25104;&#21644;&#36328;&#35821;&#35328;ASR&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#34920;&#29616;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#25552;&#31034;&#26041;&#27861;&#20248;&#20110;&#36866;&#37197;&#22120;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#21644;&#36866;&#37197;&#22120;&#35843;&#20248;&#24050;&#32463;&#25104;&#20026;&#32454;&#35843;&#65288;FT&#65289;&#26041;&#27861;&#30340;&#26377;&#25928;&#26367;&#20195;&#21697;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20851;&#20110;&#35821;&#38899;&#25552;&#31034;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#24182;&#22312;&#26356;&#22797;&#26434;&#30340;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#19978;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#36866;&#37197;&#22120;&#35843;&#20248;&#20027;&#35201;&#24212;&#29992;&#20110;&#20165;&#32534;&#30721;&#22120;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;Wav2Seq&#36825;&#20010;&#33258;&#30417;&#30563;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#19978;&#36827;&#34892;&#25552;&#31034;&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#22312;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;&#23427;&#22312;ASR&#30340;&#35789;&#38169;&#35823;&#29575;&#19978;&#23454;&#29616;&#20102;53&#65285;&#30340;&#30456;&#23545;&#25913;&#36827;&#65292;&#22312;&#27133;&#22635;&#20805;&#30340;F1&#20998;&#25968;&#19978;&#23454;&#29616;&#20102;27&#65285;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#25552;&#31034;&#26041;&#27861;&#19982;FT&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;Wav2Seq&#19978;&#36890;&#36807;&#25552;&#31034;&#21644;&#36866;&#37197;&#22120;&#35843;&#20248;&#23454;&#29616;&#30340;&#36328;&#35821;&#35328;ASR&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#24403;&#28041;&#21450;&#26377;&#38480;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#26102;&#65292;&#25552;&#31034;&#21644;&#36866;&#37197;&#22120;&#35843;&#20248;&#22987;&#32456;&#20248;&#20110;&#20256;&#32479;&#30340;FT&#26041;&#27861;&#22312;7&#31181;&#35821;&#35328;&#20013;&#30340;&#34920;&#29616;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#65292;&#25552;&#31034;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#36866;&#37197;&#22120;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting and adapter tuning have emerged as efficient alternatives to fine-tuning (FT) methods. However, existing studies on speech prompting focused on classification tasks and failed on more complex sequence generation tasks. Besides, adapter tuning is primarily applied with a focus on encoder-only self-supervised models. Our experiments show that prompting on Wav2Seq, a self-supervised encoder-decoder model, surpasses previous works in sequence generation tasks. It achieves a remarkable 53% relative improvement in word error rate for ASR and a 27% in F1 score for slot filling. Additionally, prompting competes with the FT method in the low-resource scenario. Moreover, we show the transferability of prompting and adapter tuning on Wav2Seq in cross-lingual ASR. When limited trainable parameters are involved, prompting and adapter tuning consistently outperform conventional FT across 7 languages. Notably, in the low-resource scenario, prompting consistently outperforms adapter tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21518;&#32467;&#26500;&#20027;&#20041;&#31038;&#20250;&#25919;&#27835;&#29702;&#35770;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#30340;&#8220;&#23545;&#40784;&#8221;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26126;&#30830;&#25805;&#20316;&#21270;&#36825;&#31181;&#25277;&#35937;&#27010;&#24565;&#65292;&#20419;&#36827;&#36879;&#26126;&#21644;&#25209;&#21028;&#24615;&#35780;&#20272;&#30340;&#25991;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.02457</link><description>&lt;p&gt;
&#31354;&#31526;&#21495;&#38382;&#39064;&#65306;&#26397;&#21521;&#26356;&#28165;&#26224;&#30340;&#33539;&#24335;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#8220;&#23545;&#40784;&#8221;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising "Alignment" in Large Language Models. (arXiv:2310.02457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21518;&#32467;&#26500;&#20027;&#20041;&#31038;&#20250;&#25919;&#27835;&#29702;&#35770;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#30340;&#8220;&#23545;&#40784;&#8221;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26126;&#30830;&#25805;&#20316;&#21270;&#36825;&#31181;&#25277;&#35937;&#27010;&#24565;&#65292;&#20419;&#36827;&#36879;&#26126;&#21644;&#25209;&#21028;&#24615;&#35780;&#20272;&#30340;&#25991;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21518;&#32467;&#26500;&#20027;&#20041;&#31038;&#20250;&#25919;&#27835;&#29702;&#35770;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#30340;&#8220;&#23545;&#40784;&#8221;&#27010;&#24565;&#65292;&#24182;&#29305;&#21035;&#30740;&#31350;&#20102;&#20854;&#19982;&#31354;&#31526;&#21495;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#20026;&#20102;&#22312;&#32463;&#39564;&#25968;&#25454;&#38598;&#20013;&#26126;&#30830;&#25805;&#20316;&#21270;&#25277;&#35937;&#23545;&#40784;&#27010;&#24565;&#30340;&#20849;&#20139;&#35789;&#27719;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#30028;&#23450;&#20102;&#65306;1&#65289;&#21738;&#20123;&#27169;&#22411;&#34892;&#20026;&#32500;&#24230;&#34987;&#35748;&#20026;&#37325;&#35201;&#65292;&#28982;&#21518;2&#65289;&#22914;&#20309;&#36171;&#20104;&#36825;&#20123;&#32500;&#24230;&#30340;&#21547;&#20041;&#21644;&#23450;&#20041;&#65292;&#24182;&#30001;&#35841;&#30830;&#23450;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#32463;&#39564;&#25991;&#29486;&#36827;&#34892;&#20102;&#23450;&#20301;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#20915;&#23450;&#35201;&#36981;&#24490;&#21738;&#31181;&#33539;&#24335;&#26041;&#38754;&#30340;&#25351;&#23548;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#26088;&#22312;&#22521;&#20859;&#36879;&#26126;&#21644;&#25209;&#21028;&#24615;&#35780;&#20272;&#30340;&#25991;&#21270;&#65292;&#24110;&#21161;&#31038;&#21306;&#22312;&#22788;&#29702;&#23558;LLMs&#19982;&#20154;&#31867;&#32676;&#20307;&#23545;&#40784;&#30340;&#22797;&#26434;&#24615;&#26102;&#36827;&#34892;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the concept of "alignment" in large language models (LLMs) through the lens of post-structuralist socio-political theory, specifically examining its parallels to empty signifiers. To establish a shared vocabulary around how abstract concepts of alignment are operationalised in empirical datasets, we propose a framework that demarcates: 1) which dimensions of model behaviour are considered important, then 2) how meanings and definitions are ascribed to these dimensions, and by whom. We situate existing empirical literature and provide guidance on deciding which paradigm to follow. Through this framework, we aim to foster a culture of transparency and critical evaluation, aiding the community in navigating the complexities of aligning LLMs with human populations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.15452</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#32534;&#31243;&#24605;&#32500;&#23545;&#25512;&#29702;&#36215;&#20316;&#29992;?
&lt;/p&gt;
&lt;p&gt;
When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#22312;&#20307;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#20687;&#32534;&#31243;&#24605;&#32500;&#25552;&#31034;&#36825;&#26679;&#30340;&#26041;&#27861;&#23545;&#20110;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26469;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20195;&#30721;&#25968;&#25454;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#20855;&#20307;&#24433;&#21709;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#32467;&#26500;&#21644;&#36923;&#36753;&#23646;&#24615;&#65292;&#20197;&#34913;&#37327;&#20195;&#30721;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#26469;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#38590;&#24230;&#21644;&#22280;&#22797;&#26434;&#24230;&#26469;&#35745;&#31639;&#36923;&#36753;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;LLM&#23398;&#20064;&#25110;&#29702;&#35299;&#12290;&#26368;&#20339;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#36890;&#36807;&#32534;&#31243;&#36741;&#21161;&#25552;&#31034;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21512;&#25104;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20854;&#20027;&#35201;&#21407;&#22240;&#26159;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#25552;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#19968;&#26086;&#32771;&#34385;&#21040;&#21508;&#31181;&#20998;&#24067;&#24046;&#24322;&#65292;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#26174;&#33879;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2308.00755</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#25918;&#22823;&#24726;&#35770;
&lt;/p&gt;
&lt;p&gt;
The Bias Amplification Paradox in Text-to-Image Generation. (arXiv:2308.00755v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20854;&#20027;&#35201;&#21407;&#22240;&#26159;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#25552;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#19968;&#26086;&#32771;&#34385;&#21040;&#21508;&#31181;&#20998;&#24067;&#24046;&#24322;&#65292;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#35265;&#25918;&#22823;&#26159;&#19968;&#31181;&#27169;&#22411;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#24179;&#34913;&#30340;&#29616;&#35937;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#26469;&#27604;&#36739;&#35757;&#32451;&#25968;&#25454;&#19982;&#29983;&#25104;&#22270;&#20687;&#20013;&#30340;&#24615;&#21035;&#27604;&#20363;&#65292;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#39046;&#22495;&#20013;&#30340;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#20284;&#20046;&#25918;&#22823;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#24615;&#21035;-&#32844;&#19994;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#25918;&#22823;&#24456;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#25552;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20363;&#22914;&#65292;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#26631;&#39064;&#36890;&#24120;&#21253;&#21547;&#26126;&#30830;&#30340;&#24615;&#21035;&#20449;&#24687;&#65292;&#32780;&#25105;&#20204;&#20351;&#29992;&#30340;&#25552;&#31034;&#21017;&#19981;&#21253;&#21547;&#65292;&#36825;&#23548;&#33268;&#20102;&#20998;&#24067;&#30340;&#20559;&#31227;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20559;&#35265;&#24230;&#37327;&#12290;&#19968;&#26086;&#25105;&#20204;&#32771;&#34385;&#21040;&#35757;&#32451;&#21644;&#29983;&#25104;&#26102;&#20351;&#29992;&#30340;&#25991;&#26412;&#20043;&#38388;&#30340;&#21508;&#31181;&#20998;&#24067;&#24046;&#24322;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25918;&#22823;&#29616;&#35937;&#22823;&#22823;&#20943;&#23569;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#35828;&#26126;&#20102;&#27604;&#36739;&#27169;&#22411;&#21644;&#23427;&#20204;&#25152;&#35757;&#32451;&#30340;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#24378;&#35843;&#20102;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias amplification is a phenomenon in which models increase imbalances present in the training data. In this paper, we study bias amplification in the text-to-image domain using Stable Diffusion by comparing gender ratios in training vs. generated images. We find that the model appears to amplify gender-occupation biases found in the training data (LAION). However, we discover that amplification can largely be attributed to discrepancies between training captions and model prompts. For example, an inherent difference is that captions from the training data often contain explicit gender information while the prompts we use do not, which leads to a distribution shift and consequently impacts bias measures. Once we account for various distributional differences between texts used for training and generation, we observe that amplification decreases considerably. Our findings illustrate the challenges of comparing biases in models and the data they are trained on, and highlight confounding 
&lt;/p&gt;</description></item><item><title>CPET&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21387;&#32553;LLM&#30340;&#26377;&#25928;&#21442;&#25968;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#30693;&#35782;&#32487;&#25215;&#21644;&#24674;&#22797;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#22312;&#21442;&#25968;&#26377;&#25928;&#35843;&#25972;&#20013;&#21387;&#32553;LLM&#30340;&#25512;&#29702;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07705</link><description>&lt;p&gt;
CPET: &#39640;&#25928;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models. (arXiv:2307.07705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07705
&lt;/p&gt;
&lt;p&gt;
CPET&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21387;&#32553;LLM&#30340;&#26377;&#25928;&#21442;&#25968;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#30693;&#35782;&#32487;&#25215;&#21644;&#24674;&#22797;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#22312;&#21442;&#25968;&#26377;&#25928;&#35843;&#25972;&#20013;&#21387;&#32553;LLM&#30340;&#25512;&#29702;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21442;&#25968;&#26377;&#25928;&#35843;&#25972;&#65288;PET&#65289;&#22240;&#20026;&#22312;&#35843;&#25972;&#30456;&#23545;&#36739;&#23569;&#30340;&#21442;&#25968;&#65288;PET&#27169;&#22359;&#65289;&#30340;&#21516;&#26102;&#20173;&#33021;&#28608;&#27963;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36275;&#22815;&#30693;&#35782;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#32780;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#24403;PET&#29992;&#20110;&#20026;&#22810;&#20010;&#20219;&#21153;&#25552;&#20379;&#26381;&#21153;&#26102;&#65292;&#21487;&#20197;&#22312;&#20923;&#32467;&#30340;LLM&#19978;&#26500;&#24314;&#19981;&#21516;&#30340;&#20219;&#21153;&#29305;&#23450;PET&#27169;&#22359;&#65292;&#36991;&#20813;&#20887;&#20313;LLM&#37096;&#32626;&#12290;&#34429;&#28982;PET&#26174;&#33879;&#38477;&#20302;&#20102;&#35843;&#20248;&#21644;&#37096;&#32626;LLM&#30340;&#25104;&#26412;&#65292;&#20294;&#20854;&#25512;&#29702;&#20173;&#28982;&#21463;&#21040;LLM&#35745;&#31639;&#29942;&#39048;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21387;&#32553;LLM&#30340;&#26377;&#25928;PET&#26694;&#26550;&#65292;&#31216;&#20026;&#8220;CPET&#8221;&#12290;&#22312;CPET&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20027;&#27969;LLM&#21387;&#32553;&#25216;&#26415;&#23545;PET&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#28982;&#21518;&#24341;&#20837;&#20102;&#30693;&#35782;&#32487;&#25215;&#21644;&#24674;&#22797;&#31574;&#30053;&#26469;&#24674;&#22797;&#30001;&#36825;&#20123;&#21387;&#32553;&#25216;&#26415;&#24341;&#36215;&#30340;&#30693;&#35782;&#20002;&#22833;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30001;&#20110;CPET&#30340;&#24674;&#22797;&#31574;&#30053;&#65292;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient tuning (PET) has been widely explored in recent years because it tunes much fewer parameters (PET modules) than full-parameter fine-tuning (FT) while still stimulating sufficient knowledge from large language models (LLMs) for downstream tasks. Moreover, when PET is employed to serve multiple tasks, different task-specific PET modules can be built on a frozen LLM, avoiding redundant LLM deployments. Although PET significantly reduces the cost of tuning and deploying LLMs, its inference still suffers from the computational bottleneck of LLMs. To address the above issue, we propose an effective PET framework based on compressed LLMs, named "CPET". In CPET, we evaluate the impact of mainstream LLM compression techniques on PET performance and then introduce knowledge inheritance and recovery strategies to restore the knowledge loss caused by these compression techniques. Our experimental results demonstrate that, owing to the restoring strategies of CPET, collaborating
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;$FPDM$&#65292;&#20351;&#29992;&#25991;&#26723;&#20803;&#25968;&#25454;&#21644;&#39046;&#22495;&#29305;&#23450;&#20998;&#31867;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65292;&#23545;&#39046;&#22495;&#29305;&#23450;&#35821;&#26009;&#24211;&#36827;&#34892;transformer&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#12290;$FPDM$&#36890;&#36807;&#21477;&#23376;&#32423;&#21035;&#30340;&#36755;&#20837;&#39044;&#35757;&#32451;&#24320;&#25918;&#39046;&#22495;&#30340;&#32534;&#30721;&#22120;&#65292;&#22312;&#24494;&#35843;&#26102;&#20351;&#29992;&#35789;&#27719;&#32423;&#21035;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.06190</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26723;&#32423;&#20803;&#25968;&#25454;&#30340;&#39046;&#22495;&#29305;&#23450;&#24555;&#36895;&#39044;&#35757;&#32451;&#25216;&#26415;$FPDM$
&lt;/p&gt;
&lt;p&gt;
$FPDM$: Domain-Specific Fast Pre-training Technique using Document-Level Metadata. (arXiv:2306.06190v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$FPDM$&#65292;&#20351;&#29992;&#25991;&#26723;&#20803;&#25968;&#25454;&#21644;&#39046;&#22495;&#29305;&#23450;&#20998;&#31867;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65292;&#23545;&#39046;&#22495;&#29305;&#23450;&#35821;&#26009;&#24211;&#36827;&#34892;transformer&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#12290;$FPDM$&#36890;&#36807;&#21477;&#23376;&#32423;&#21035;&#30340;&#36755;&#20837;&#39044;&#35757;&#32451;&#24320;&#25918;&#39046;&#22495;&#30340;&#32534;&#30721;&#22120;&#65292;&#22312;&#24494;&#35843;&#26102;&#20351;&#29992;&#35789;&#27719;&#32423;&#21035;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#24050;&#26174;&#31034;&#20986;&#22312;&#24320;&#25918;&#39046;&#22495;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;transformers&#38656;&#35201;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$FPDM$&#65288;Fast Pre-training Technique using Document Level Metadata&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#12289;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26723;&#20803;&#25968;&#25454;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#20998;&#31867;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65292;&#23545;&#39046;&#22495;&#29305;&#23450;&#35821;&#26009;&#24211;&#36827;&#34892;transformer&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#12290;&#26368;&#20027;&#35201;&#30340;&#21019;&#26032;&#22312;&#20110;&#65292;&#22312;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#21477;&#23376;&#32423;&#21035;&#30340;&#23884;&#20837;&#20316;&#20026;&#36755;&#20837;&#65292;&#25345;&#32493;&#23545;&#24320;&#25918;&#39046;&#22495;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65288;&#20197;&#36866;&#24212;&#38271;&#25991;&#26723;&#65289;&#65292;&#20294;&#22312;&#23545;&#35813;&#32534;&#30721;&#22120;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#21017;&#20351;&#29992;&#35789;&#27719;&#32423;&#21035;&#23884;&#20837;&#20316;&#20026;&#36755;&#20837;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;$FPDM$&#22312;&#23458;&#25143;&#25903;&#25345;&#12289;&#31185;&#23398;&#21644;&#27861;&#24459;&#31561;&#39046;&#22495;&#30340;&#23383;&#31526;&#32423;F1&#20998;&#25968;&#21644;&#20854;&#20182;&#33258;&#21160;&#21270;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#22522;&#20110;transformer&#30340;&#22522;&#20934;&#65292;&#19988;&#22312;&#19979;&#28216;&#20219;&#21153;&#24494;&#35843;&#21518;&#24615;&#33021;&#19979;&#38477;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training Transformers has shown promising results on open-domain and domain-specific downstream tasks. However, state-of-the-art Transformers require an unreasonably large amount of pre-training data and compute. In this paper, we propose $FPDM$ (Fast Pre-training Technique using Document Level Metadata), a novel, compute-efficient framework that utilizes Document metadata and Domain-Specific Taxonomy as supervision signals to pre-train transformer encoder on a domain-specific corpus. The main innovation is that during domain-specific pretraining, an open-domain encoder is continually pre-trained using sentence-level embeddings as inputs (to accommodate long documents), however, fine-tuning is done with token-level embeddings as inputs to this encoder. We show that $FPDM$ outperforms several transformer-based baselines in terms of character-level F1 scores and other automated metrics in the Customer Support, Scientific, and Legal Domains, and shows a negligible drop in performance 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MiniSUPERB&#30340;&#36731;&#37327;&#32423;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35780;&#20272;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#19978;&#27604;SUPERB&#26356;&#20302;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#30740;&#31350;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#35780;&#20272;SSL&#35821;&#38899;&#27169;&#22411;&#30340;&#24615;&#33021;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.19011</link><description>&lt;p&gt;
MiniSUPERB:&#36731;&#37327;&#32423;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models. (arXiv:2305.19011v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19011
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MiniSUPERB&#30340;&#36731;&#37327;&#32423;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35780;&#20272;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#19978;&#27604;SUPERB&#26356;&#20302;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#30740;&#31350;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#35780;&#20272;SSL&#35821;&#38899;&#27169;&#22411;&#30340;&#24615;&#33021;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SUPERB&#34987;&#25552;&#20986;&#29992;&#20110;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#35821;&#38899;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#22810;&#26679;&#21270;&#20219;&#21153;&#65292;&#23427;&#23548;&#33268;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MiniSUPERB&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#20197;&#26126;&#26174;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#26377;&#25928;&#22320;&#35780;&#20272;SSL&#35821;&#38899;&#27169;&#22411;&#24182;&#19988;&#32467;&#26524;&#21487;&#19982;SUPERB&#30456;&#27604;&#12290;&#25105;&#20204;&#31934;&#36873;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#37319;&#26679;&#25968;&#25454;&#38598;&#65292;&#24182;&#31163;&#32447;&#25552;&#21462;&#27169;&#22411;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;SUPERB Paper&#21644;SUPERB Challenge&#20998;&#21035;&#36798;&#21040;0.954&#21644;0.982&#30340;&#26031;&#30382;&#23572;&#26364;&#31561;&#32423;&#30456;&#20851;&#31995;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20056;-&#32047;&#31215;&#25805;&#20316;&#65288;MACs&#65289;&#26041;&#38754;&#20943;&#23569;&#20102;97&#65285;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#35780;&#20272;SSL&#35821;&#38899;&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#21040;&#20854;&#24615;&#33021;&#26377;&#26174;&#33879;&#21464;&#21270;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#30740;&#31350;&#21516;&#26102;&#32771;&#34385;&#27169;&#22411;&#26412;&#36523;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#22312;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
SUPERB was proposed to evaluate the generalizability of self-supervised learning (SSL) speech models across various tasks. However, it incurs high computational costs due to the large datasets and diverse tasks. In this paper, we introduce MiniSUPERB, a lightweight benchmark that efficiently evaluates SSL speech models with comparable results to SUPERB but lower computational costs significantly. We carefully select representative tasks, sample datasets, and extract model representations offline. Our approach achieves a Spearman's rank correlation of 0.954 and 0.982 with SUPERB Paper and SUPERB Challenge, respectively. Additionally, we reduce the computational cost by 97% in terms of Multiply-ACcumulate operations (MACs). Furthermore, we evaluate SSL speech models in few-shot scenarios and observe significant variations in their performance. To our knowledge, this is the first study to examine both the computational cost of the model itself and the cost of evaluating it on a benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22522;&#30784;&#30340;&#26080;&#21442;&#32771;&#35780;&#20272;&#22120;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#20004;&#20010;&#23545;&#25239;&#20803;&#35780;&#20272;&#23545;&#35805;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#36825;&#20123;&#35780;&#20272;&#22120;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14658</link><description>&lt;p&gt;
&#26080;&#27861;&#35780;&#20272;&#30340;&#29983;&#25104;&#21709;&#24212;&#36136;&#37327;&#30340;&#35780;&#20272;: Evaluate What You Can't Evaluate
&lt;/p&gt;
&lt;p&gt;
Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality. (arXiv:2305.14658v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22522;&#30784;&#30340;&#26080;&#21442;&#32771;&#35780;&#20272;&#22120;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#20004;&#20010;&#23545;&#25239;&#20803;&#35780;&#20272;&#23545;&#35805;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#36825;&#20123;&#35780;&#20272;&#22120;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#24050;&#32463;&#23637;&#29616;&#20986;&#24778;&#20154;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#34429;&#28982;&#20197;LLMs&#20026;&#22522;&#30784;&#30340;&#26080;&#21442;&#32771;&#35780;&#20272;&#22120;&#27604;&#20256;&#32479;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#35780;&#20272;&#22120;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#20154;&#31867;&#35821;&#20041;&#23545;&#40784;&#24230;&#65292;&#20294;&#26159;&#22312;&#20351;&#29992;&#20197;LLMs&#20026;&#22522;&#30784;&#30340;&#26080;&#21442;&#32771;&#35780;&#20272;&#22120;&#26102;&#20173;&#28982;&#23384;&#22312;&#24456;&#22810;&#25361;&#25112;&#12290;&#26080;&#21442;&#32771;&#35780;&#20272;&#22120;&#26356;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#35821;&#20041;&#21709;&#24212;&#30340;&#24320;&#25918;&#24335;&#20363;&#23376;&#12290;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;&#20363;&#23376;&#37117;&#26159;&#24320;&#25918;&#24335;&#30340;&#65292;&#23545;&#20110;&#20855;&#26377;&#21807;&#19968;&#27491;&#30830;&#35821;&#20041;&#21709;&#24212;&#30340;&#38381;&#21512;&#24335;&#20363;&#23376;&#65292;&#22914;&#26524;&#32473;&#20986;&#19982;&#20107;&#23454;&#21644;&#21442;&#32771;&#30340;&#35821;&#20041;&#19981;&#19968;&#33268;&#30340;&#21709;&#24212;&#65292;&#26080;&#21442;&#32771;&#35780;&#20272;&#22120;&#20173;&#28982;&#20250;&#35748;&#20026;&#20854;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#20197;LLMs&#20026;&#22522;&#30784;&#30340;&#35780;&#20272;&#22120;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#23545;&#25239;&#20803;&#35780;&#20272;&#23545;&#35805;&#29983;&#25104;&#25968;&#25454;&#38598;KdConv-ADV&#21644;DSTC7-ADV&#22522;&#20110;KdConv&#21644;DSTC7-AVSD&#12290;&#19982;&#20197;&#21069;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#30456;&#27604;&#65292;KdConv-ADV&#21644;DSTC7-ADV&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs (large language models) such as ChatGPT have shown remarkable language understanding and generation capabilities. Although reference-free evaluators based on LLMs show better human alignment than traditional reference-based evaluators, there are many challenges in using reference-free evaluators based on LLMs. Reference-free evaluators are more suitable for open-ended examples with different semantics responses. But not all examples are open-ended. For closed-ended examples with unique correct semantic response, reference-free evaluators will still consider it high quality when giving a response that is inconsistent with the facts and the semantic of reference. In order to comprehensively evaluate the reliability of evaluators based on LLMs, we construct two adversarial meta-evaluation dialogue generation datasets KdConv-ADV and DSTC7-ADV based on KdConv and DSTC7-AVSD, respectively. Compared to previous meta-evaluation benchmarks, KdConv-ADV and DSTC7-ADV are much more challengin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35299;&#20915;&#27169;&#31946;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#37327;&#27979;&#37327;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#37325;&#22797;&#24615;&#65292;&#25214;&#20986;&#20102;&#22312;&#21547;&#31946;&#38382;&#39064;&#38598;&#20013;&#22238;&#31572;&#39640;&#31934;&#24230;&#23376;&#38598;&#38382;&#39064;&#30340;&#26368;&#21487;&#38752;&#26041;&#27861;&#12290;&#36825;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#27495;&#20041;&#65292;&#24182;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14613</link><description>&lt;p&gt;
&#23545;&#27169;&#31946;&#38382;&#39064;&#30340;&#26377;&#36873;&#25321;&#24615;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Selectively Answering Ambiguous Questions. (arXiv:2305.14613v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35299;&#20915;&#27169;&#31946;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#37327;&#27979;&#37327;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#37325;&#22797;&#24615;&#65292;&#25214;&#20986;&#20102;&#22312;&#21547;&#31946;&#38382;&#39064;&#38598;&#20013;&#22238;&#31572;&#39640;&#31934;&#24230;&#23376;&#38598;&#38382;&#39064;&#30340;&#26368;&#21487;&#38752;&#26041;&#27861;&#12290;&#36825;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#27495;&#20041;&#65292;&#24182;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#22312;&#19981;&#30693;&#36947;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#25918;&#24323;&#22238;&#31572;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38382;&#38382;&#32773;&#24847;&#22270;&#25110;&#19978;&#19979;&#25991;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#38382;&#39064;&#30340;&#31572;&#26696;&#20063;&#21487;&#33021;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#20174;&#36825;&#20010;&#35282;&#24230;&#35843;&#26597;&#20102;&#38382;&#39064;&#22238;&#31572;&#65292;&#19987;&#27880;&#20110;&#22312;&#20247;&#22810;&#26412;&#36136;&#19978;&#21547;&#31946;&#30340;&#38382;&#39064;&#38598;&#20013;&#22238;&#31572;&#39640;&#31934;&#24230;&#23376;&#38598;&#30340;&#38382;&#39064;&#12290;&#22312;&#27492;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#23450;&#37327;&#27979;&#37327;&#19968;&#32452;&#37319;&#26679;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#37325;&#22797;&#24615;&#26159;&#26368;&#21487;&#38752;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#32780;&#38750;&#20808;&#21069;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#30340;&#27010;&#29575;&#25110;&#33258;&#25105;&#39564;&#35777;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#19981;&#21516;&#30340;&#27169;&#22411;&#35268;&#27169;&#65292;&#20197;&#21450;&#24102;&#25110;&#19981;&#24102;&#25351;&#23548;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#27495;&#20041;&#65292;&#24182;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trustworthy language models should abstain from answering questions when they do not know the answer. However, the answer to a question can be unknown for a variety of reasons. Prior research has focused on the case in which the question is clear and the answer is unambiguous but possibly unknown. However, the answer to a question can also be unclear due to uncertainty of the questioner's intent or context. We investigate question answering from this perspective, focusing on answering a subset of questions with a high degree of accuracy, from a set of questions in which many are inherently ambiguous. In this setting, we find that the most reliable approach to calibration involves quantifying repetition within a set of sampled model outputs, rather than the model's likelihood or self-verification as used in prior work. % We find this to be the case across different types of uncertainty, varying model scales and both with or without instruction tuning. Our results suggest that sampling-b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#25552;&#31034;&#20301;&#32622;&#23545;&#20110;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#20855;&#26377;&#23454;&#36136;&#24615;&#24433;&#21709;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#20301;&#32622;&#36890;&#24120;&#26159;&#27425;&#20248;&#30340;&#65292;&#25552;&#31034;&#20301;&#32622;&#20248;&#21270;&#24212;&#25104;&#20026;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.14493</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;NLU&#20219;&#21153;&#20013;&#25552;&#31034;&#20301;&#32622;&#30830;&#23454;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Prompt position really matters in few-shot and zero-shot NLU tasks. (arXiv:2305.14493v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14493
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#25552;&#31034;&#20301;&#32622;&#23545;&#20110;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#20855;&#26377;&#23454;&#36136;&#24615;&#24433;&#21709;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#20301;&#32622;&#36890;&#24120;&#26159;&#27425;&#20248;&#30340;&#65292;&#25552;&#31034;&#20301;&#32622;&#20248;&#21270;&#24212;&#25104;&#20026;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21560;&#24341;&#20102;&#20247;&#22810;&#30740;&#31350;&#32773;&#30340;&#20851;&#27880;&#12290;&#20294;&#26159;&#65292;&#26377;&#25928;&#25552;&#31034;&#27169;&#26495;&#30340;&#24320;&#21457;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#31034;&#35789;&#27719;&#36873;&#25321;&#25110;&#20445;&#30041;&#25552;&#31034;&#20301;&#32622;&#30340;&#23884;&#20837;&#21021;&#22987;&#21270;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#25552;&#31034;&#20301;&#32622;&#36873;&#39033;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#37327;&#21270;&#20102;&#25552;&#31034;&#20301;&#32622;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#23454;&#36136;&#24615;&#24433;&#21709;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#20301;&#32622;&#23545;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#36890;&#24120;&#26159;&#27425;&#20248;&#30340;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#25552;&#31034;&#20301;&#32622;&#20248;&#21270;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#19982;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#37325;&#24515;&#24182;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based models have made remarkable advancements in the fields of zero-shot and few-shot learning, attracting a lot of attention from researchers. Developing an effective prompt template plays a critical role. However, prior studies have mainly focused on prompt vocabulary selection or embedding initialization with the reserved prompt position fixed. In this empirical study, we conduct the most comprehensive analysis to date of prompt position option for natural language understanding tasks. Our findings quantify the substantial impact prompt position has on model performance. We observe that the prompt position used in prior studies is often sub-optimal for both zero-shot and few-shot settings. These findings suggest prompt position optimisation as an interesting research direction alongside the existing focus on prompt engineering.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;-shot&#20849;&#25351;&#35299;&#26512;&#25216;&#26415;&#22312;&#22797;&#26434;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#24212;&#29992;&#65292;&#24182;&#35777;&#26126;&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#40065;&#26834;&#24615;&#38646; shot &#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14489</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#40065;&#26834;&#30340;&#38646;-shot&#20849;&#25351;&#35299;&#26512;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Robust Zero-shot Coreference Resolvers?. (arXiv:2305.14489v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;-shot&#20849;&#25351;&#35299;&#26512;&#25216;&#26415;&#22312;&#22797;&#26434;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#24212;&#29992;&#65292;&#24182;&#35777;&#26126;&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#40065;&#26834;&#24615;&#38646; shot &#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#20849;&#25351;&#28040;&#35299;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20381;&#38752;&#20351;&#29992;&#30446;&#26631;&#39046;&#22495;&#30340;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#25345;&#32493;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LMs) &#22312;&#24191;&#27867;&#30340; NLP &#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#38646;&#21644;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#65292;&#21253;&#25324;&#20195;&#35789;&#28040;&#35299;&#12290;&#34429;&#28982;&#36825;&#34920;&#26126;&#20102;&#20849;&#25351;&#33021;&#21147;&#30340;&#35777;&#25454;&#65292;&#20294;&#20197;&#24448;&#30340;&#30740;&#31350;&#22823;&#37117;&#20351;&#29992;&#31616;&#21333;&#30340;&#21477;&#23376;&#32423;&#21035;&#25968;&#25454;&#38598; (&#22914; Winograd Schema &#25361;&#25112;&#36187;) &#30740;&#31350;&#36825;&#31181;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#26356;&#21152;&#22256;&#38590;&#30340;&#12289;&#35821;&#35328;&#19978;&#22797;&#26434;&#30340;&#20849;&#25351;&#22522;&#20934;&#27979;&#35797; (&#22914; CoNLL-2012) &#19978;&#30340;&#21487;&#34892;&#24615;&#26469;&#35780;&#20272;&#38646;-shot&#23398;&#20064;&#36827;&#34892;&#20849;&#25351;&#28040;&#35299;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#38646;-shot&#25552;&#31034;&#20248;&#20110;&#24403;&#21069;&#30340;&#26080;&#30417;&#30563;&#20849;&#25351;&#31995;&#32479;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972; LMs &#22312;&#24191;&#27867;&#30340;&#39046;&#22495;&#12289;&#35821;&#35328;&#21644;&#26102;&#38388;&#27573;&#19978;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#38646; shot &#26222;&#36866;&#24615;&#65292;&#20197;&#21450;&#23545;&#19978;&#19979;&#25991;&#21644;&#25351;&#31034;&#35789;&#30340;&#24378;&#28872;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in domain adaptation for coreference resolution relies on continued training using annotated data from target domains. At the same time, pre-trained large language models (LMs) have exhibited strong zero- and few-shot learning abilities across a wide range of NLP tasks including pronoun resolution. While this demonstrates evidence of coreference ability, previous work has mostly studied this ability using simple sentence-level datasets such as the Winograd Schema Challenge. In this work, we assess the feasibility of zero-shot learning for coreference resolution by evaluating instruction-tuned language models on more difficult, linguistically-complex coreference benchmarks (e.g., CoNLL-2012). We demonstrate that zero-shot prompting outperforms current unsupervised coreference systems. Further investigations reveal the robust zero-shot generalization ability of instruction-tuned LMs across a wide range of domains, languages, and time periods, as well as a strong reliance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.13062</link><description>&lt;p&gt;
GPT4Table&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#21527;&#65311;&#19968;&#39033;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study. (arXiv:2305.13062v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23569;&#26679;&#26412;&#25512;&#29702;&#22120;&#26469;&#35299;&#20915;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#36234;&#26469;&#36234;&#20855;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#20363;&#22914;&#34920;&#26684;&#65289;&#30340;&#29702;&#35299;&#31243;&#24230;&#36824;&#26377;&#24456;&#22810;&#38656;&#35201;&#23398;&#20064;&#30340;&#22320;&#26041;&#12290;&#23613;&#31649;&#21487;&#20197;&#20351;&#29992;&#34920;&#26684;&#24207;&#21015;&#21270;&#20316;&#20026;LLMs&#30340;&#36755;&#20837;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#23545;LLMs&#26159;&#21542;&#30495;&#27491;&#33021;&#22815;&#29702;&#35299;&#36825;&#31867;&#25968;&#25454;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;LLMs&#30340;&#32467;&#26500;&#29702;&#35299;&#33021;&#21147;&#65288;SUC&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21019;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#19971;&#20010;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#26377;&#20854;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#21333;&#20803;&#26684;&#26597;&#25214;&#12289;&#34892;&#26816;&#32034;&#21644;&#22823;&#23567;&#26816;&#27979;&#12290;&#25105;&#20204;&#23545;GPT-3.5&#21644;GPT-4&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#24615;&#33021;&#22240;&#22810;&#31181;&#36755;&#20837;&#36873;&#25321;&#32780;&#24322;&#65292;&#21253;&#25324;&#34920;&#26684;&#36755;&#20837;&#26684;&#24335;&#12289;&#20869;&#23481;&#39034;&#24207;&#12289;&#35282;&#33394;&#25552;&#31034;&#21644;&#20998;&#21306;&#26631;&#35760;&#31561;&#12290;&#26681;&#25454;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#25152;&#24471;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. While it is true that tables can be used as inputs to LLMs with serialization, there lack of comprehensive studies examining whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, \eg, cell lookup, row retrieval, and size detection. We run a series of evaluations on GPT-3.5 and GPT-4. We discover that the performance varied depending on a number of input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we then propose \textit{self-augmentation} for effect
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;n-best&#37325;&#25490;&#24207;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#25552;&#20379;&#20266;&#26631;&#31614;&#65292;&#35757;&#32451;&#20986;&#21442;&#25968;&#26356;&#23569;&#20294;&#31934;&#24230;&#30456;&#24403;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12057</link><description>&lt;p&gt;
&#22522;&#20110;n-best&#37325;&#25490;&#24207;&#30340;&#31934;&#20934;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Accurate Knowledge Distillation with n-best Reranking. (arXiv:2305.12057v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12057
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;n-best&#37325;&#25490;&#24207;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#25552;&#20379;&#20266;&#26631;&#31614;&#65292;&#35757;&#32451;&#20986;&#21442;&#25968;&#26356;&#23569;&#20294;&#31934;&#24230;&#30456;&#24403;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;n-best&#37325;&#25490;&#24207;&#30340;&#24207;&#21015;&#32423;&#21035;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#25945;&#24072;&#27169;&#22411;&#30340;top-1&#20551;&#35774;&#20197;&#21450;top n-best&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#21253;&#25324;&#20844;&#24320;&#21487;&#29992;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20869;&#30340;&#22810;&#31181;&#27169;&#22411;&#65292;&#20026;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;WMT21&#24503;&#33521;&#32763;&#35793;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;&#20855;&#26377;&#20004;&#20010;&#25968;&#37327;&#32423;&#36739;&#23569;&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#19982;Tran&#31561;&#20154;&#65288;2021&#24180;&#65289;&#30340;&#21253;&#21547;47&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#32763;&#35793;&#27169;&#22411;&#30456;&#24403;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose extending the Sequence-level Knowledge Distillation (Kim and Rush, 2016) with n-best reranking to consider not only the top-1 hypotheses but also the top n-best hypotheses of teacher models. Our approach leverages a diverse set of models, including publicly-available large pretrained models, to provide more accurate pseudo-labels for training student models. We validate our proposal on the WMT21 German-English translation task and demonstrate that our student model achieves comparable accuracy to a large translation model with 4.7 billion parameters from (Tran et al., 2021) while having two orders of magnitude fewer parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Summarize and Score&#65288;SASC&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#33719;&#21462;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#35299;&#37322;&#21487;&#38752;&#31243;&#24230;&#30340;&#20998;&#25968;&#12290;&#30740;&#31350;&#32773;&#20204;&#24050;&#32463;&#22312;&#21512;&#25104;&#27169;&#22359;&#21644;BERT&#27169;&#22411;&#20013;&#20351;&#29992;SASC&#65292;&#35753;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#27169;&#22359;&#30340;&#36873;&#25321;&#24615;&#65292;&#36825;&#23545;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.09863</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Explaining black box text modules in natural language with language models. (arXiv:2305.09863v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Summarize and Score&#65288;SASC&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#33719;&#21462;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#35299;&#37322;&#21487;&#38752;&#31243;&#24230;&#30340;&#20998;&#25968;&#12290;&#30740;&#31350;&#32773;&#20204;&#24050;&#32463;&#22312;&#21512;&#25104;&#27169;&#22359;&#21644;BERT&#27169;&#22411;&#20013;&#20351;&#29992;SASC&#65292;&#35753;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#27169;&#22359;&#30340;&#36873;&#25321;&#24615;&#65292;&#36825;&#23545;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#19981;&#36879;&#26126;&#24615;&#24050;&#32463;&#24341;&#36215;&#20102;&#23545;&#21487;&#35299;&#37322;&#24615;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#35810;&#38382;&#26159;&#21542;&#21487;&#20197;&#33258;&#21160;&#33719;&#21462;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#19968;&#20010;&#8220;&#25991;&#26412;&#27169;&#22359;&#8221;&#26159;&#23558;&#25991;&#26412;&#26144;&#23556;&#21040;&#26631;&#37327;&#36830;&#32493;&#20540;&#30340;&#20219;&#20309;&#20989;&#25968;&#65292;&#20363;&#22914;LLM&#20869;&#30340;&#23376;&#27169;&#22359;&#25110;&#22823;&#33041;&#21306;&#22495;&#30340;&#25311;&#21512;&#27169;&#22411;&#12290;&#8220;&#40657;&#30418;&#8221;&#34920;&#31034;&#25105;&#20204;&#21482;&#33021;&#35775;&#38382;&#27169;&#22359;&#30340;&#36755;&#20837;/&#36755;&#20986;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Summarize and Score&#65288;SASC&#65289;&#26041;&#27861;&#65292;&#23427;&#25509;&#21463;&#25991;&#26412;&#27169;&#22359;&#24182;&#36820;&#22238;&#27169;&#22359;&#36873;&#25321;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#35299;&#37322;&#21487;&#38752;&#31243;&#24230;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19978;&#19979;&#25991;&#20013;&#30740;&#31350;SASC&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#27169;&#22359;&#19978;&#35780;&#20272;SASC&#65292;&#24182;&#21457;&#29616;&#23427;&#32463;&#24120;&#24674;&#22797;&#22522;&#26412;&#30495;&#30456;&#35828;&#26126;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;SASC&#26469;&#35299;&#37322;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#20013;&#30340;&#27169;&#22359;&#65292;&#20351;&#24471;&#26816;&#26597;BERT&#30340;&#27169;&#22359;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their rapid proliferation and increasing opaqueness have created a growing need for interpretability. Here, we ask whether we can automatically obtain natural language explanations for black box text modules. A "text module" is any function that maps text to a scalar continuous value, such as a submodule within an LLM or a fitted model of a brain region. "Black box" indicates that we only have access to the module's inputs/outputs.  We introduce Summarize and Score (SASC), a method that takes in a text module and returns a natural language explanation of the module's selectivity along with a score for how reliable the explanation is. We study SASC in 3 contexts. First, we evaluate SASC on synthetic modules and find that it often recovers ground truth explanations. Second, we use SASC to explain modules found within a pre-trained BERT model, enabling inspection of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#27604;&#36739;&#30693;&#35782;&#25552;&#28860;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21517;&#31216;&#20026;NeuroComparatives&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#35789;&#27719;&#32422;&#26463;&#35299;&#30721;&#36827;&#34892;&#27604;&#36739;&#30693;&#35782;&#25552;&#28860;&#65292;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#30693;&#35782;&#36827;&#34892;&#20005;&#26684;&#36807;&#28388;&#12290;</title><link>http://arxiv.org/abs/2305.04978</link><description>&lt;p&gt;
NeuroComparatives&#65306;&#27604;&#36739;&#30693;&#35782;&#30340;&#31070;&#32463;&#31526;&#21495;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge. (arXiv:2305.04978v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#27604;&#36739;&#30693;&#35782;&#25552;&#28860;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21517;&#31216;&#20026;NeuroComparatives&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#35789;&#27719;&#32422;&#26463;&#35299;&#30721;&#36827;&#34892;&#27604;&#36739;&#30693;&#35782;&#25552;&#28860;&#65292;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#30693;&#35782;&#36827;&#34892;&#20005;&#26684;&#36807;&#28388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#36739;&#30693;&#35782;&#26159;&#25105;&#20204;&#19990;&#30028;&#30693;&#35782;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#22312;&#20197;&#21069;&#30340;&#25991;&#29486;&#20013;&#30740;&#31350;&#19981;&#36275;&#12290;&#26412;&#25991;&#30740;&#31350;&#27604;&#36739;&#30693;&#35782;&#33719;&#21462;&#20219;&#21153;&#65292;&#21463;&#21040;&#20687;GPT-3&#36825;&#26679;&#26497;&#31471;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#26174;&#30528;&#25552;&#39640;&#30340;&#25512;&#21160;&#65292;&#25512;&#21160;&#20102;&#23558;&#20182;&#20204;&#30340;&#30693;&#35782;&#25910;&#38598;&#21040;&#30693;&#35782;&#24211;&#20013;&#30340;&#21162;&#21147;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#25512;&#29702;API&#35775;&#38382;&#21463;&#21040;&#38480;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#30693;&#35782;&#33719;&#21462;&#30340;&#33539;&#22260;&#21644;&#22810;&#26679;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30475;&#20284;&#19981;&#21487;&#34892;&#30340;&#38382;&#39064;&#65306;&#26356;&#26131;&#20110;&#35775;&#38382;&#12289;&#35268;&#27169;&#26356;&#23567;&#12289;&#24615;&#33021;&#26356;&#24369;&#30340;&#27169;&#22411;&#65288;&#22914;GPT-2&#65289;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#33719;&#21462;&#27604;&#36739;&#30693;&#35782;&#65292;&#20174;&#32780;&#36798;&#21040;&#19982;&#22823;&#35268;&#27169;&#27169;&#22411;&#30456;&#24403;&#30340;&#36136;&#37327;&#65311;&#25105;&#20204;&#24341;&#20837;&#20102;NeuroComparatives&#65292;&#19968;&#31181;&#20351;&#29992;&#35789;&#27719;&#32422;&#26463;&#35299;&#30721;&#30340;&#27604;&#36739;&#30693;&#35782;&#25552;&#28860;&#26032;&#26694;&#26550;&#65292;&#20854;&#21518;&#32039;&#23494;&#36807;&#28388;&#29983;&#25104;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparative knowledge (e.g., steel is stronger and heavier than styrofoam) is an essential component of our world knowledge, yet understudied in prior literature. In this paper, we study the task of comparative knowledge acquisition, motivated by the dramatic improvements in the capabilities of extreme-scale language models like GPT-3, which have fueled efforts towards harvesting their knowledge into knowledge bases. However, access to inference API for such models is limited, thereby restricting the scope and the diversity of the knowledge acquisition. We thus ask a seemingly implausible question: whether more accessible, yet considerably smaller and weaker models such as GPT-2, can be utilized to acquire comparative knowledge, such that the resulting quality is on par with their large-scale counterparts?  We introduce NeuroComparatives, a novel framework for comparative knowledge distillation using lexically-constrained decoding, followed by stringent filtering of generated knowledge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;Bird&#65292;&#21487;&#20197;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#25991;&#26412;&#21040;SQL&#30340;&#20219;&#21153;&#65292;&#31361;&#20986;&#20102;&#25968;&#25454;&#24211;&#20540;&#29702;&#35299;&#21644;SQL&#25928;&#29575;&#31561;&#39046;&#22495;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.03111</link><description>&lt;p&gt;
LLM&#33021;&#21542;&#20316;&#20026;&#25968;&#25454;&#24211;&#25509;&#21475;&#65311;&#22823;&#22411;&#25968;&#25454;&#24211;&#22522;&#30784;&#25991;&#26412;&#21040;SQL&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs. (arXiv:2305.03111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;Bird&#65292;&#21487;&#20197;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#25991;&#26412;&#21040;SQL&#30340;&#20219;&#21153;&#65292;&#31361;&#20986;&#20102;&#25968;&#25454;&#24211;&#20540;&#29702;&#35299;&#21644;SQL&#25928;&#29575;&#31561;&#39046;&#22495;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#26088;&#22312;&#23558;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36716;&#25442;&#20026;&#21487;&#25191;&#34892;&#30340;SQL&#21629;&#20196;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#22522;&#20934;&#27979;&#35797;Bird&#65292;&#23427;&#21253;&#21547;&#26088;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#22522;&#30784;&#30340;12,751&#23545;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#21644;95&#20010;&#25968;&#25454;&#24211;&#65292;&#24635;&#22823;&#23567;&#20026;33.4GB&#65292;&#28085;&#30422;37&#20010;&#19987;&#19994;&#39046;&#22495;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;Bird&#24378;&#35843;&#25968;&#25454;&#24211;&#20540;&#30340;&#29702;&#35299;&#65292;&#31361;&#20986;&#20102;&#33039;&#25968;&#25454;&#24211;&#20869;&#23481;&#12289;NL&#38382;&#39064;&#21644;&#25968;&#25454;&#24211;&#20869;&#23481;&#20043;&#38388;&#30340;&#22806;&#37096;&#30693;&#35782;&#20197;&#21450;SQL&#25928;&#29575;&#31561;&#26032;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#24517;&#39035;&#20855;&#22791;&#25968;&#25454;&#24211;&#20540;&#29702;&#35299;&#21644;&#35821;&#20041;&#35299;&#26512;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL parsing, which aims at converting natural language instructions into executable SQLs, has gained increasing attention in recent years. In particular, Codex and ChatGPT have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database contents leaving the gap between academic study and real-world applications. To mitigate this gap, we present Bird, a big benchmark for large-scale database grounded in text-to-SQL tasks, containing 12,751 pairs of text-to-SQL data and 95 databases with a total size of 33.4 GB, spanning 37 professional domains. Our emphasis on database values highlights the new challenges of dirty database contents, external knowledge between NL questions and database contents, and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;ChatGPT&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24378;&#35843;&#20102;&#20351;&#29992;&#36825;&#20010;&#24378;&#22823;&#24037;&#20855;&#26102;&#30340;&#36947;&#24503;&#32771;&#34385;&#65292;&#20026;&#20154;&#24037;&#26234;&#33021;&#21644;NLP&#39046;&#22495;&#30340;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2304.02017</link><description>&lt;p&gt;
&#35299;&#38145;ChatGPT&#30340;&#28508;&#21147;&#65306;&#23545;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24212;&#29992;&#12289;&#20248;&#28857;&#12289;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#20840;&#38754;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing. (arXiv:2304.02017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;ChatGPT&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24378;&#35843;&#20102;&#20351;&#29992;&#36825;&#20010;&#24378;&#22823;&#24037;&#20855;&#26102;&#30340;&#36947;&#24503;&#32771;&#34385;&#65292;&#20026;&#20154;&#24037;&#26234;&#33021;&#21644;NLP&#39046;&#22495;&#30340;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;&#20869;&#23481;&#29983;&#25104;&#12289;&#35821;&#35328;&#32763;&#35793;&#12289;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#21307;&#30103;&#35786;&#26029;&#27835;&#30103;&#12290;&#23427;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#20934;&#30830;&#24615;&#20351;&#20854;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;ChatGPT&#20063;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#20854;&#20542;&#21521;&#20110;&#20135;&#29983;&#26377;&#20559;&#35265;&#30340;&#21709;&#24212;&#20197;&#21450;&#23384;&#22312;&#28508;&#22312;&#30340;&#26377;&#23475;&#35821;&#35328;&#27169;&#24335;&#12290;&#26412;&#25991;&#20840;&#38754;&#27010;&#36848;&#20102;ChatGPT&#21450;&#20854;&#24212;&#29992;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#20351;&#29992;&#36825;&#20010;&#24378;&#22823;&#24037;&#20855;&#26102;&#36947;&#24503;&#32771;&#34385;&#30340;&#37325;&#35201;&#24615;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#30340;&#35265;&#35299;&#65292;&#20026;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#21450;&#20854;&#23545;&#35270;&#35273;&#21644;NLP&#39046;&#22495;&#30340;&#24433;&#21709;&#30340;&#25345;&#32493;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a powerful tool in the field of artificial intelligence that has been widely used in various applications. ChatGPT has been applied successfully in chatbots, content generation, language translation, personalized recommendations, and medical diagnosis and treatment. Its versatility and accuracy make it a powerful tool for natural language processing (NLP). However, there are also limitations to ChatGPT, such as its tendency to produce biased responses and its potential to perpetuate harmful language patterns. This article provides a comprehensive overview of ChatGPT, its applications, advantages, and limitations. Additionally, the paper emphasizes the importance of ethical considerations when using this robust tool in real-world scenarios. Finally, This paper contributes to ongoing discussions surrounding artificial intelligence and its impact on vision and NLP domains by providing insights into prompt engineering techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.10405</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#36890;&#24120;&#20316;&#20026;&#38745;&#24577;&#24037;&#20214;&#37096;&#32626;&#65292;&#20462;&#25913;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;E-FB15k237&#12289;A-FB15k237&#12289;E-WN18RR &#21644; A-WN18RR&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#30693;&#35782;&#32534;&#36753;&#22522;&#32447;&#65292;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#27169;&#22411;&#22788;&#29702;&#35813;&#20219;&#21153;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22522;&#32447;&#8212;&#8212;KGEditor&#65292;&#23427;&#21033;&#29992;&#36229;&#32593;&#32476;&#30340;&#38468;&#21152;&#21442;&#25968;&#23618;&#26469;&#32534;&#36753;/&#28155;&#21152;&#20107;&#23454;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#26102;&#65292;KGEditor &#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20851;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#12290;&#20316;&#32773;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.11483</link><description>&lt;p&gt;
Deanthropomorphising NLP&#65306;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#24847;&#35782;&#21040;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deanthropomorphising NLP: Can a Language Model Be Conscious?. (arXiv:2211.11483v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20851;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#12290;&#20316;&#32773;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23545;&#26368;&#36817;&#26377;&#20851;&#20351;&#29992;Transformer&#27169;&#22411;&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#36827;&#34892;&#35752;&#35770;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#24182;&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#32508;&#21512;&#20449;&#24687;&#29702;&#35770;&#23545;Transformer&#26550;&#26500;&#36827;&#34892;&#20998;&#26512;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#26159;NLP&#25253;&#36947;&#20013;&#20351;&#29992;&#25311;&#20154;&#21270;&#35821;&#35328;&#30340;&#26356;&#24191;&#27867;&#20542;&#21521;&#30340;&#19968;&#37096;&#20998;&#12290;&#26080;&#35770;&#36825;&#20123;&#35828;&#27861;&#30340;&#30495;&#23454;&#24615;&#22914;&#20309;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#22312;&#26159;&#35780;&#20272;&#35821;&#35328;&#24314;&#27169;&#36827;&#23637;&#24182;&#32771;&#34385;&#35813;&#20219;&#21153;&#30340;&#20262;&#29702;&#24433;&#21709;&#30340;&#36866;&#24403;&#26102;&#26426;&#12290;&#20026;&#20102;&#20351;&#26412;&#25991;&#26377;&#21161;&#20110;NLP&#31038;&#21306;&#20197;&#22806;&#30340;&#35835;&#32773;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;NLP&#22522;&#30784;&#30693;&#35782;&#30340;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work is intended as a voice in the discussion over the recent claims that LaMDA, a pretrained language model based on the Transformer model architecture, is sentient. This claim, if confirmed, would have serious ramifications in the Natural Language Processing (NLP) community due to wide-spread use of similar models. However, here we take the position that such a language model cannot be sentient, or conscious, and that LaMDA in particular exhibits no advances over other similar models that would qualify it. We justify this by analysing the Transformer architecture through Integrated Information Theory. We see the claims of consciousness as part of a wider tendency to use anthropomorphic language in NLP reporting. Regardless of the veracity of the claims, we consider this an opportune moment to take stock of progress in language modelling and consider the ethical implications of the task. In order to make this work helpful for readers outside the NLP community, we also present the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;Gogioso&#21644;Pinzani&#22312;QPL 2021&#20013;&#25552;&#20986;&#30340;&#26463;&#29702;&#35770;&#27169;&#22411;&#65292;&#20026;&#35821;&#20041;&#27495;&#20041;&#30340;&#20004;&#20010;&#29305;&#24449;&#65288;&#19981;&#21516;&#21487;&#33021;&#35299;&#37322;&#30340;&#32852;&#21512;&#21487;&#20449;&#24230;&#21644;&#26576;&#20123;&#35789;&#22312;&#36807;&#31243;&#20013;&#25198;&#28436;&#26356;&#37325;&#35201;&#35282;&#33394;&#30340;&#22240;&#26524;&#32467;&#26500;&#65289;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#23545;&#24515;&#29702;&#35821;&#35328;&#23398;&#25991;&#29486;&#20013;&#30340;&#27495;&#20041;&#30701;&#35821;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;&#20154;&#31867;&#23545;&#20110;&#36825;&#20123;&#27495;&#20041;&#30340;&#21028;&#26029;&#36827;&#34892;&#20102;&#23454;&#35777;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.06807</link><description>&lt;p&gt;
&#35821;&#20041;&#27495;&#20041;&#30340;&#22240;&#26524;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Causal Structure of Semantic Ambiguities. (arXiv:2206.06807v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Gogioso&#21644;Pinzani&#22312;QPL 2021&#20013;&#25552;&#20986;&#30340;&#26463;&#29702;&#35770;&#27169;&#22411;&#65292;&#20026;&#35821;&#20041;&#27495;&#20041;&#30340;&#20004;&#20010;&#29305;&#24449;&#65288;&#19981;&#21516;&#21487;&#33021;&#35299;&#37322;&#30340;&#32852;&#21512;&#21487;&#20449;&#24230;&#21644;&#26576;&#20123;&#35789;&#22312;&#36807;&#31243;&#20013;&#25198;&#28436;&#26356;&#37325;&#35201;&#35282;&#33394;&#30340;&#22240;&#26524;&#32467;&#26500;&#65289;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#23545;&#24515;&#29702;&#35821;&#35328;&#23398;&#25991;&#29486;&#20013;&#30340;&#27495;&#20041;&#30701;&#35821;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;&#20154;&#31867;&#23545;&#20110;&#36825;&#20123;&#27495;&#20041;&#30340;&#21028;&#26029;&#36827;&#34892;&#20102;&#23454;&#35777;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27495;&#20041;&#26159;&#33258;&#28982;&#35821;&#35328;&#29616;&#35937;&#65292;&#22312;&#19981;&#21516;&#30340;&#35821;&#27861;&#12289;&#35821;&#20041;&#21644;&#35821;&#29992;&#23618;&#38754;&#19978;&#21457;&#29983;&#12290;&#23427;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65307;&#20363;&#22914;&#65292;&#22312;&#24515;&#29702;&#35821;&#35328;&#23398;&#39046;&#22495;&#65292;&#25105;&#20204;&#26377;&#22810;&#31181;&#31454;&#20105;&#24615;&#30340;&#30740;&#31350;&#20154;&#31867;&#28040;&#27495;&#36807;&#31243;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#30740;&#31350;&#26159;&#32463;&#39564;&#24615;&#30340;&#65292;&#22522;&#20110;&#30524;&#21160;&#36319;&#36394;&#31561;&#27979;&#37327;&#26041;&#27861;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#20026;&#35821;&#20041;&#27495;&#20041;&#24418;&#24335;&#21270;&#36825;&#20123;&#36827;&#31243;&#65292;&#20854;&#20013;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#29305;&#24449;&#65306;(1)&#19981;&#21516;&#21487;&#33021;&#35299;&#37322;&#20043;&#38388;&#30340;&#32852;&#21512;&#21487;&#20449;&#24230;&#65292;(2)&#26681;&#25454;&#26576;&#20123;&#35789;&#22312;&#36807;&#31243;&#20013;&#25198;&#28436;&#26356;&#37325;&#35201;&#35282;&#33394;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;Gogioso&#21644;Pinzani&#22312;QPL 2021&#20013;&#25552;&#20986;&#30340;&#26032;&#22411;&#26463;&#29702;&#35770;&#30830;&#23450;&#22240;&#26524;&#24615;&#27169;&#22411;&#24182;&#23545;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#25512;&#29702;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#29702;&#35770;&#24212;&#29992;&#20110;&#20174;&#24515;&#29702;&#35821;&#35328;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30340;&#27495;&#20041;&#30701;&#35821;&#25968;&#25454;&#38598;&#21644;&#25105;&#20204;&#20351;&#29992;Amazon&#30340;&#26426;&#26800;&#22303;&#32819;&#20854;&#24341;&#25806;&#25910;&#38598;&#30340;&#20154;&#31867;&#21487;&#20449;&#24230;&#21028;&#26029;&#20013;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;&#20854;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12289;&#27495;&#20041;&#27700;&#24179;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ambiguity is a natural language phenomenon occurring at different levels of syntax, semantics, and pragmatics. It is widely studied; in Psycholinguistics, for instance, we have a variety of competing studies for the human disambiguation processes. These studies are empirical and based on eyetracking measurements. Here we take first steps towards formalizing these processes for semantic ambiguities where we identified the presence of two features: (1) joint plausibility degrees of different possible interpretations, (2) causal structures according to which certain words play a more substantial role in the processes. The novel sheaf-theoretic model of definite causality developed by Gogioso and Pinzani in QPL 2021 offers tools to model and reason about these features. We applied this theory to a dataset of ambiguous phrases extracted from Psycholinguistics literature and their human plausibility judgements collected by us using the Amazon Mechanical Turk engine. We measured the causal fr
&lt;/p&gt;</description></item></channel></rss>