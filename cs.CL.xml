<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>HiPrompt&#26159;&#19968;&#20010;&#30417;&#30563;&#25928;&#29575;&#39640;&#30340;&#30693;&#35782;&#34701;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#27425;&#23548;&#21521;&#25552;&#31034;&#21644;&#23569;&#26679;&#26412;&#25512;&#29702;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#34701;&#21512;&#21644;&#31070;&#32463;&#23884;&#20837;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2304.05973</link><description>&lt;p&gt;
HiPrompt: &#23618;&#27425;&#23548;&#21521;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
HiPrompt: Few-Shot Biomedical Knowledge Fusion via Hierarchy-Oriented Prompting. (arXiv:2304.05973v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05973
&lt;/p&gt;
&lt;p&gt;
HiPrompt&#26159;&#19968;&#20010;&#30417;&#30563;&#25928;&#29575;&#39640;&#30340;&#30693;&#35782;&#34701;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#27425;&#23548;&#21521;&#25552;&#31034;&#21644;&#23569;&#26679;&#26412;&#25512;&#29702;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#34701;&#21512;&#21644;&#31070;&#32463;&#23884;&#20837;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32508;&#21512;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#24211;&#21487;&#20197;&#22686;&#24378;&#21307;&#23398;&#20915;&#31574;&#36807;&#31243;&#65292;&#38656;&#35201;&#36890;&#36807;&#32479;&#19968;&#30340;&#32034;&#24341;&#31995;&#32479;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#32034;&#24341;&#31995;&#32479;&#36890;&#24120;&#20197;&#23618;&#27425;&#32467;&#26500;&#32452;&#32455;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#65292;&#20197;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#23545;&#40784;&#23454;&#20307;&#12290;&#20026;&#20102;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#34701;&#21512; (BKF) &#20219;&#21153;&#20013;&#30417;&#30563;&#19981;&#36275;&#30340;&#25361;&#25112;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#35789;&#27719;&#21644;&#32467;&#26500;&#21305;&#37197;&#31639;&#27861;&#65292;&#26080;&#27861;&#25429;&#25417;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#21644;&#26415;&#35821;&#25152;&#20256;&#36798;&#30340;&#20016;&#23500;&#35821;&#20041;&#12290;&#26368;&#36817;&#65292;&#31070;&#32463;&#23884;&#20837;&#27169;&#22411;&#22312;&#35821;&#20041;&#20016;&#23500;&#30340;&#20219;&#21153;&#20013;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#20805;&#36275;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20805;&#20998;&#35757;&#32451;&#12290;&#20026;&#20102;&#24357;&#34917;&#31232;&#32570;&#26631;&#35760; BKF &#21644;&#31070;&#32463;&#23884;&#20837;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; HiPrompt&#65292;&#19968;&#20010;&#30417;&#30563;&#25928;&#29575;&#39640;&#30340;&#30693;&#35782;&#34701;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#24341;&#21457;&#22823;&#35268;&#27169;&#35821;&#20041;&#25512;&#29702;&#30340;&#23569;&#26679;&#26412;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical decision-making processes can be enhanced by comprehensive biomedical knowledge bases, which require fusing knowledge graphs constructed from different sources via a uniform index system. The index system often organizes biomedical terms in a hierarchy to provide the aligned entities with fine-grained granularity. To address the challenge of scarce supervision in the biomedical knowledge fusion (BKF) task, researchers have proposed various unsupervised methods. However, these methods heavily rely on ad-hoc lexical and structural matching algorithms, which fail to capture the rich semantics conveyed by biomedical entities and terms. Recently, neural embedding models have proved effective in semantic-rich tasks, but they rely on sufficient labeled data to be adequately trained. To bridge the gap between the scarce-labeled BKF and neural embedding models, we propose HiPrompt, a supervision-efficient knowledge fusion framework that elicits the few-shot reasoning ability of large la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#25552;&#31034;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#26500;&#24314;&#38598;&#25104;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#21333;&#25552;&#31034;&#36755;&#20986;&#31354;&#38388;&#38598;&#25104;&#21644;&#34955;&#35013;&#25552;&#31034;&#31354;&#38388;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2304.05970</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;&#25552;&#31034;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Boosted Prompt Ensembles for Large Language Models. (arXiv:2304.05970v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#25552;&#31034;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#26500;&#24314;&#38598;&#25104;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#21333;&#25552;&#31034;&#36755;&#20986;&#31354;&#38388;&#38598;&#25104;&#21644;&#34955;&#35013;&#25552;&#31034;&#31354;&#38388;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#21644;&#33258;&#19968;&#33268;&#24615;&#31561;&#26041;&#27861;&#24050;&#32463;&#25512;&#21160;&#20102;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#30340;&#21069;&#27839;&#65292;&#32780;&#19988;&#27809;&#26377;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#24314;&#35758;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19968;&#31181;&#25552;&#31034;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#26469;&#26500;&#24314;&#19968;&#32452;&#23569;&#37327;&#30340;&#25552;&#31034;&#65292;&#36825;&#20123;&#25552;&#31034;&#20849;&#21516;&#26500;&#25104;&#20102;&#19968;&#20010;&#8220;&#22686;&#24378;&#30340;&#25552;&#31034;&#38598;&#25104;&#8221;&#12290;&#27599;&#20010;&#25552;&#31034;&#30340;&#23569;&#25968;&#26679;&#20363;&#26159;&#36890;&#36807;&#28176;&#36827;&#24335;&#26041;&#24335;&#36873;&#25321;&#30340;&#65292;&#20197;&#20415;&#22312;&#19978;&#19968;&#20010;&#27493;&#39588;&#30340;&#38598;&#25104;&#32467;&#26524;&#19981;&#30830;&#23450;&#26102;&#25104;&#20026;&#8220;&#22256;&#38590;&#8221;&#26679;&#20363;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;GSM8k&#21644;AQuA&#25968;&#25454;&#38598;&#31561;&#26041;&#38754;&#20248;&#20110;&#21333;&#25552;&#31034;&#36755;&#20986;&#31354;&#38388;&#38598;&#25104;&#21644;&#34955;&#35013;&#25552;&#31034;&#31354;&#38388;&#38598;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#27979;&#35797;&#26102;&#38388;&#29256;&#26412;&#30340;&#22686;&#24378;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#32423;&#21035;&#30340;&#21487;&#29992;&#27880;&#37322;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods such as chain-of-thought prompting and self-consistency have pushed the frontier of language model reasoning performance with no additional training. To further improve performance, we propose a prompt ensembling method for large language models, which uses a small dataset to construct a set of few shot prompts that together comprise a ``boosted prompt ensemble''. The few shot examples for each prompt are chosen in a stepwise fashion to be ``hard'' examples on which the previous step's ensemble is uncertain. We show that this outperforms single-prompt output-space ensembles and bagged prompt-space ensembles on the GSM8k and AQuA datasets, among others. We propose both train-time and test-time versions of boosted prompting that use different levels of available annotation and conduct a detailed empirical study of our algorithm.
&lt;/p&gt;</description></item><item><title>ASL Citizen&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#29420;&#31435;&#25163;&#35821;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#25163;&#35821;&#23383;&#20856;&#26816;&#32034;&#65292;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#22312;&#24230;&#37327;&#26631;&#20934;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#20363;&#22914;&#22312;&#35757;&#32451;&#25110;&#39564;&#35777;&#20013;&#26410;&#20986;&#29616;&#30340;&#29992;&#25143;&#30340;&#35270;&#39057;&#19978;&#65292;&#23454;&#29616;&#20102;62&#65285;&#30340;&#20934;&#30830;&#24615;&#21644;90&#65285;&#30340;&#21069;10&#39033;&#26816;&#32034;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.05934</link><description>&lt;p&gt;
ASL Citizen: &#19968;&#20010;&#25512;&#36827;&#29420;&#31435;&#25163;&#35821;&#35782;&#21035;&#30340;&#31038;&#21306;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ASL Citizen: A Community-Sourced Dataset for Advancing Isolated Sign Language Recognition. (arXiv:2304.05934v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05934
&lt;/p&gt;
&lt;p&gt;
ASL Citizen&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#29420;&#31435;&#25163;&#35821;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#25163;&#35821;&#23383;&#20856;&#26816;&#32034;&#65292;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#22312;&#24230;&#37327;&#26631;&#20934;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#20363;&#22914;&#22312;&#35757;&#32451;&#25110;&#39564;&#35777;&#20013;&#26410;&#20986;&#29616;&#30340;&#29992;&#25143;&#30340;&#35270;&#39057;&#19978;&#65292;&#23454;&#29616;&#20102;62&#65285;&#30340;&#20934;&#30830;&#24615;&#21644;90&#65285;&#30340;&#21069;10&#39033;&#26816;&#32034;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#34987;&#20840;&#29699;&#32422;7000&#19975;&#32843;&#20581;&#20154;&#22763;&#29992;&#20316;&#20027;&#35201;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20132;&#27969;&#25216;&#26415;&#36816;&#20316;&#22312;&#21475;&#22836;&#21644;&#20070;&#38754;&#35821;&#35328;&#20013;&#65292;&#23548;&#33268;&#33719;&#21462;&#20449;&#24687;&#23384;&#22312;&#19981;&#20844;&#24179;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;ASL Citizen&#65292;&#23427;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#29420;&#31435;&#25163;&#35821;&#35782;&#21035; (ISLR) &#25968;&#25454;&#38598;&#65292;&#32463;&#36807;&#21516;&#24847;&#25910;&#38598;&#65292;&#21253;&#25324;52&#20010;&#25163;&#35821;&#32773;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#25293;&#25668;&#30340;2,731&#20010;&#19981;&#21516;&#25163;&#21183;&#30340;83,912&#20010;&#35270;&#39057;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#36825;&#20010;&#25968;&#25454;&#38598;&#29992;&#20110;&#32654;&#22269;&#25163;&#35821; (ASL) &#30340;&#25163;&#35821;&#23383;&#20856;&#26816;&#32034;&#65292;&#29992;&#25143;&#36890;&#36807;&#33258;&#24049;&#30340;&#32593;&#32476;&#25668;&#20687;&#22836;&#28436;&#31034;&#25163;&#35821;&#65292;&#20174;&#23383;&#20856;&#20013;&#26816;&#32034;&#30456;&#21305;&#37197;&#30340;&#25163;&#35821;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#23545;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#19982;&#23383;&#20856;&#26816;&#32034;&#30456;&#20851;&#30340;&#24230;&#37327;&#26631;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20363;&#22914;&#22312;&#35757;&#32451;&#25110;&#39564;&#35777;&#20013;&#26410;&#20986;&#29616;&#30340;&#29992;&#25143;&#30340;&#35270;&#39057;&#19978;&#65292;&#23454;&#29616;&#20102;62&#65285;&#30340;&#20934;&#30830;&#24615;&#21644;90&#65285;&#30340;&#21069;10&#39033;&#26816;&#32034;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sign languages are used as a primary language by approximately 70 million D/deaf people world-wide. However, most communication technologies operate in spoken and written languages, creating inequities in access. To help tackle this problem, we release ASL Citizen, the largest Isolated Sign Language Recognition (ISLR) dataset to date, collected with consent and containing 83,912 videos for 2,731 distinct signs filmed by 52 signers in a variety of environments. We propose that this dataset be used for sign language dictionary retrieval for American Sign Language (ASL), where a user demonstrates a sign to their own webcam with the aim of retrieving matching signs from a dictionary. We show that training supervised machine learning classifiers with our dataset greatly advances the state-of-the-art on metrics relevant for dictionary retrieval, achieving, for instance, 62% accuracy and a recall-at-10 of 90%, evaluated entirely on videos of users who are not present in the training or valida
&lt;/p&gt;</description></item><item><title>ReDWINE&#26159;&#19968;&#20010;&#20855;&#26377;&#25991;&#26412;&#20998;&#26512;&#33021;&#21147;&#30340;&#20020;&#24202;&#25968;&#25454;&#38598;&#65292;&#23558;UPMC&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#25910;&#38598;&#30340;&#24247;&#22797;&#30456;&#20851;EHR&#25968;&#25454;&#36716;&#25442;&#20026;OHDSI&#26684;&#24335;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20123;&#20016;&#23500;&#30340;&#25968;&#25454;&#20803;&#32032;&#65292;&#21253;&#25324;&#35786;&#26029;&#12289;&#31243;&#24207;&#12289;&#33647;&#29289;&#12289;&#23454;&#39564;&#23460;&#26816;&#27979;&#32467;&#26524;&#21644;&#29983;&#21629;&#20307;&#24449;&#65292;&#21487;&#29992;&#20110;&#22238;&#31572;&#26377;&#20851;&#24247;&#22797;&#21644;&#29289;&#29702;&#21307;&#23398;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.05929</link><description>&lt;p&gt;
ReDWINE&#65306;&#20855;&#26377;&#25991;&#26412;&#20998;&#26512;&#33021;&#21147;&#30340;&#20020;&#24202;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#24247;&#22797;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ReDWINE: A Clinical Datamart with Text Analytical Capabilities to Facilitate Rehabilitation Research. (arXiv:2304.05929v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05929
&lt;/p&gt;
&lt;p&gt;
ReDWINE&#26159;&#19968;&#20010;&#20855;&#26377;&#25991;&#26412;&#20998;&#26512;&#33021;&#21147;&#30340;&#20020;&#24202;&#25968;&#25454;&#38598;&#65292;&#23558;UPMC&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#25910;&#38598;&#30340;&#24247;&#22797;&#30456;&#20851;EHR&#25968;&#25454;&#36716;&#25442;&#20026;OHDSI&#26684;&#24335;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20123;&#20016;&#23500;&#30340;&#25968;&#25454;&#20803;&#32032;&#65292;&#21253;&#25324;&#35786;&#26029;&#12289;&#31243;&#24207;&#12289;&#33647;&#29289;&#12289;&#23454;&#39564;&#23460;&#26816;&#27979;&#32467;&#26524;&#21644;&#29983;&#21629;&#20307;&#24449;&#65292;&#21487;&#29992;&#20110;&#22238;&#31572;&#26377;&#20851;&#24247;&#22797;&#21644;&#29289;&#29702;&#21307;&#23398;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24247;&#22797;&#30740;&#31350;&#19987;&#27880;&#20110;&#30830;&#23450;&#27835;&#30103;&#24178;&#39044;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23548;&#33268;&#24674;&#22797;&#21644;&#24247;&#22797;&#30340;&#26426;&#21046;&#65292;&#20197;&#21450;&#26368;&#32456;&#26368;&#22823;&#21270;&#24739;&#32773;&#36523;&#20307;&#12289;&#24515;&#29702;&#21644;&#31038;&#20132;&#21151;&#33021;&#30340;&#26368;&#20339;&#24178;&#39044;&#31574;&#30053;&#12290;&#20256;&#32479;&#30340;&#38543;&#26426;&#20020;&#24202;&#35797;&#39564;&#30740;&#31350;&#21644;&#30830;&#23450;&#26032;&#24178;&#39044;&#25514;&#26045;&#38754;&#20020;&#30528;&#24456;&#22810;&#25361;&#25112;&#65292;&#22914;&#39640;&#26114;&#30340;&#36153;&#29992;&#21644;&#26102;&#38388;&#25215;&#35834;&#12290;&#35266;&#23519;&#24615;&#30740;&#31350;&#21033;&#29992;&#29616;&#26377;&#30340;&#20020;&#24202;&#25968;&#25454;&#35266;&#23519;&#24178;&#39044;&#25928;&#26524;&#22312;RCTs&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20960;&#20010;&#20248;&#28857;&#12290;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#25104;&#20026;&#36827;&#34892;&#35266;&#23519;&#24615;&#30740;&#31350;&#30340;&#26085;&#30410;&#37325;&#35201;&#30340;&#36164;&#28304;&#12290;&#20026;&#25903;&#25345;&#36825;&#20123;&#30740;&#31350;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20020;&#24202;&#30740;&#31350;&#25968;&#25454;&#38598;ReDWINE&#65288;&#21487;&#36890;&#36807;&#20449;&#24687;&#23398;&#22522;&#30784;&#35774;&#26045;&#36827;&#34892;&#24247;&#22797;&#30740;&#31350;&#30340;&#24247;&#22797;&#25968;&#25454;&#38598;&#65289;&#65292;&#23558;UPMC&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#25910;&#38598;&#30340;&#24247;&#22797;&#30456;&#20851;EHR&#25968;&#25454;&#36716;&#25442;&#20026;&#25903;&#25345;&#35266;&#23519;&#24615;&#20581;&#24247;&#25968;&#25454;&#31185;&#23398;&#21644;&#20449;&#24687;&#23398;&#65288;OHDSI&#65289;&#26684;&#24335;&#12290;&#25105;&#20204;&#36824;&#22312;ReDWINE&#20013;&#23454;&#29616;&#20102;&#19968;&#20010;&#25991;&#26412;&#20998;&#26512;&#31995;&#32479;&#65292;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25628;&#32034;&#12289;&#27880;&#37322;&#21644;&#25552;&#21462;&#20449;&#24687;&#20197;&#25903;&#25345;&#30740;&#31350;&#38382;&#39064;&#12290;ReDWINE&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#25968;&#25454;&#20803;&#32032;&#65292;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12289;&#35786;&#26029;&#12289;&#31243;&#24207;&#12289;&#33647;&#29289;&#12289;&#23454;&#39564;&#23460;&#26816;&#27979;&#32467;&#26524;&#21644;&#29983;&#21629;&#20307;&#24449;&#12290;&#36825;&#20123;&#25968;&#25454;&#20803;&#32032;&#21487;&#29992;&#20110;&#22238;&#31572;&#26377;&#20851;&#24247;&#22797;&#21644;&#29289;&#29702;&#21307;&#23398;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rehabilitation research focuses on determining the components of a treatment intervention, the mechanism of how these components lead to recovery and rehabilitation, and ultimately the optimal intervention strategies to maximize patients' physical, psychologic, and social functioning. Traditional randomized clinical trials that study and establish new interventions face several challenges, such as high cost and time commitment. Observational studies that use existing clinical data to observe the effect of an intervention have shown several advantages over RCTs. Electronic Health Records (EHRs) have become an increasingly important resource for conducting observational studies. To support these studies, we developed a clinical research datamart, called ReDWINE (Rehabilitation Datamart With Informatics iNfrastructure for rEsearch), that transforms the rehabilitation-related EHR data collected from the UPMC health care system to the Observational Health Data Sciences and Informatics (OHDS
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21516;&#20041;&#35789;&#21477;&#23376;&#24314;&#31435;&#21516;&#24418;&#24322;&#20041;&#35789;&#35789;&#32423;&#28040;&#27495;&#34920;&#31034;&#65288;HDR&#65289;&#20197;&#25913;&#36827;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.05860</link><description>&lt;p&gt;
&#23398;&#20064;&#21516;&#24418;&#24322;&#20041;&#35789;&#28040;&#27495;&#34920;&#31034;&#20197;&#25913;&#36827;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Learning Homographic Disambiguation Representation for Neural Machine Translation. (arXiv:2304.05860v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21516;&#20041;&#35789;&#21477;&#23376;&#24314;&#31435;&#21516;&#24418;&#24322;&#20041;&#35789;&#35789;&#32423;&#28040;&#27495;&#34920;&#31034;&#65288;HDR&#65289;&#20197;&#25913;&#36827;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#24418;&#24322;&#20041;&#35789;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#19968;&#30452;&#26159;&#38590;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#21516;&#24418;&#24322;&#20041;&#35789;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#8220;HDR-encoder&#8221;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#23398;&#20064;&#36890;&#29992;&#21477;&#23376;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;WordNet&#20013;&#30340;&#21516;&#20041;&#35789;&#21477;&#23376;&#24314;&#31435;&#21516;&#24418;&#24322;&#20041;&#35789;&#35789;&#32423;&#28040;&#27495;&#34920;&#31034;&#65288;HDR&#65289;&#65292;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;HDR-encoder&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#39044;&#35757;&#32451;&#30340;HDR-encoder&#19982;&#22522;&#20110;Transformer&#30340;NMT&#22312;&#19981;&#21516;&#26041;&#26696;&#20013;&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;&#22235;&#20010;&#32763;&#35793;&#26041;&#21521;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#26412;&#26041;&#27861;&#22312;&#22686;&#24378;NMT&#31995;&#32479;&#22788;&#29702;&#21516;&#24418;&#24322;&#20041;&#35789;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Homographs, words with the same spelling but different meanings, remain challenging in Neural Machine Translation (NMT). While recent works leverage various word embedding approaches to differentiate word sense in NMT, they do not focus on the pivotal components in resolving ambiguities of homographs in NMT: the hidden states of an encoder. In this paper, we propose a novel approach to tackle homographic issues of NMT in the latent space. We first train an encoder (aka "HDR-encoder") to learn universal sentence representations in a natural language inference (NLI) task. We further fine-tune the encoder using homograph-based synset sentences from WordNet, enabling it to learn word-level homographic disambiguation representations (HDR). The pre-trained HDR-encoder is subsequently integrated with a transformer-based NMT in various schemes to improve translation accuracy. Experiments on four translation directions demonstrate the effectiveness of the proposed method in enhancing the perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;FewDR&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#23569;&#37327;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#26102;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#20351;&#24471;DR&#27169;&#22411;&#33021;&#22815;&#22312;&#22522;&#31867;&#21644;&#26032;&#39062;&#31867;&#19978;&#36827;&#34892;&#36830;&#32493;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#23569;&#26679;&#26412;&#23494;&#38598;&#26816;&#32034;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.05845</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#23494;&#38598;&#26816;&#32034;&#30340;&#23569;&#26679;&#26412;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Rethinking Dense Retrieval's Few-Shot Ability. (arXiv:2304.05845v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;FewDR&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#23569;&#37327;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#26102;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#20351;&#24471;DR&#27169;&#22411;&#33021;&#22815;&#22312;&#22522;&#31867;&#21644;&#26032;&#39062;&#31867;&#19978;&#36827;&#34892;&#36830;&#32493;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#23569;&#26679;&#26412;&#23494;&#38598;&#26816;&#32034;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#23569;&#37327;&#26679;&#26412;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#26032;&#30340;&#25628;&#32034;&#22330;&#26223;&#12290;&#23613;&#31649;&#23427;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#23545;&#20110;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#21644;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#21327;&#35758;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#20174;&#30417;&#30563;&#25968;&#25454;&#38598;&#20013;&#38543;&#26426;&#37319;&#26679;&#26469;&#21019;&#24314;&#8220;&#23569;&#37327;&#25968;&#25454;&#8221;&#35774;&#32622;&#65292;&#24182;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#37319;&#29992;&#19981;&#19968;&#33268;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36825;&#20250;&#22312;&#20934;&#30830;&#27604;&#36739;&#26368;&#36817;&#30340;&#36827;&#23637;&#26041;&#38754;&#24102;&#26469;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;FewDR&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;FewDR&#37319;&#29992;&#25353;&#31867;&#21035;&#25277;&#26679;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#8220;&#23569;&#26679;&#26412;&#8221;&#35774;&#32622;&#65292;&#23450;&#20041;&#20102;&#31934;&#32454;&#30340;&#31867;&#21035;&#65292;&#20943;&#23569;&#20102;&#22810;&#27425;&#25277;&#26679;&#30340;&#21464;&#24322;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#34987;&#20998;&#20026;&#22522;&#31867;&#21644;&#26032;&#39062;&#31867;&#65292;&#20801;&#35768;DR&#27169;&#22411;&#22312;&#22522;&#31867;&#30340;&#20016;&#23500;&#25968;&#25454;&#21644;&#26032;&#39062;&#31867;&#30340;&#23569;&#37327;&#26679;&#26412;&#19978;&#36827;&#34892;&#36830;&#32493;&#35757;&#32451;&#12290;&#36825;&#20010;&#22522;&#20934;&#28040;&#38500;&#20102;&#26032;&#39062;&#31867;&#27844;&#28431;&#30340;&#39118;&#38505;&#65292;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot dense retrieval (DR) aims to effectively generalize to novel search scenarios by learning a few samples. Despite its importance, there is little study on specialized datasets and standardized evaluation protocols. As a result, current methods often resort to random sampling from supervised datasets to create "few-data" setups and employ inconsistent training strategies during evaluations, which poses a challenge in accurately comparing recent progress. In this paper, we propose a customized FewDR dataset and a unified evaluation benchmark. Specifically, FewDR employs class-wise sampling to establish a standardized "few-shot" setting with finely-defined classes, reducing variability in multiple sampling rounds. Moreover, the dataset is disjointed into base and novel classes, allowing DR models to be continuously trained on ample data from base classes and a few samples in novel classes. This benchmark eliminates the risk of novel class leakage, providing a reliable estimation o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35199;&#26031;&#25289;&#22827;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#65292;&#21457;&#29616;&#25463;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#21644;&#26031;&#27931;&#20240;&#20811;&#35821;&#22343;&#23384;&#22312;&#30456;&#20284;&#31243;&#24230;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#36825;&#19968;&#30740;&#31350;&#22635;&#34917;&#20102;&#30740;&#31350;&#38750;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#24615;&#21035;&#20559;&#35265;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2304.05783</link><description>&lt;p&gt;
&#25463;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#21644;&#26031;&#27931;&#20240;&#20811;&#35821;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#37327;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Measuring Gender Bias in West Slavic Language Models. (arXiv:2304.05783v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35199;&#26031;&#25289;&#22827;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#65292;&#21457;&#29616;&#25463;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#21644;&#26031;&#27931;&#20240;&#20811;&#35821;&#22343;&#23384;&#22312;&#30456;&#20284;&#31243;&#24230;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#36825;&#19968;&#30740;&#31350;&#22635;&#34917;&#20102;&#30740;&#31350;&#38750;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#24615;&#21035;&#20559;&#35265;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#20250;&#23558;&#22522;&#30784;&#25968;&#25454;&#38598;&#20013;&#30340;&#20559;&#35265;&#24310;&#32493;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22823;&#22810;&#22522;&#20110;&#33521;&#35821;&#30340;&#21333;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#38024;&#23545;&#25193;&#23637;&#21040;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#35199;&#26031;&#25289;&#22827;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#26495;&#30340;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#25463;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#21644;&#26031;&#27931;&#20240;&#20811;&#35821;&#65289;&#65292;&#20197;&#27979;&#37327;&#38024;&#23545;&#30007;&#24615;&#12289;&#22899;&#24615;&#21644;&#38750;&#20108;&#36827;&#21046;&#20027;&#20307;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#20351;&#29992;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;&#36825;&#20123;&#21477;&#23376;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#26159;&#21542;&#36866;&#21512;&#20110;&#34987;&#36974;&#30422;&#30340;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#37327;&#21270;&#29983;&#25104;&#21333;&#35789;&#30340;&#26377;&#27602;&#24615;&#21644;&#24615;&#21035;&#29305;&#24449;&#26469;&#27979;&#37327;&#35199;&#26031;&#25289;&#22827;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#35821;&#21477;&#20250;&#22240;&#20027;&#20307;&#30340;&#24615;&#21035;&#32780;&#20135;&#29983;&#20260;&#23475;&#24615;&#30340;&#23436;&#25104;&#24230;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25463;&#20811;&#35821;&#12289;&#26031;&#27931;&#20240;&#20811;&#35821;&#21644;&#27874;&#20848;&#35821;&#22343;&#26174;&#31034;&#20986;&#30456;&#20284;&#31243;&#24230;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#23545;&#20110;&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#30340;&#26085;&#30410;&#22686;&#38271;&#30340;&#30740;&#31350;&#20307;&#31995;&#20570;&#20986;&#36129;&#29486;&#65292;&#24182;&#20026;&#35780;&#20272;&#21644;&#20943;&#23569;&#35199;&#26031;&#25289;&#22827;&#35821;&#35328;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have been known to perpetuate biases from the underlying datasets to downstream tasks. However, these findings are predominantly based on monolingual language models for English, whereas there are few investigative studies of biases encoded in language models for languages beyond English. In this paper, we fill this gap by analysing gender bias in West Slavic language models. We introduce the first template-based dataset in Czech, Polish, and Slovak for measuring gender bias towards male, female and non-binary subjects. We complete the sentences using both mono- and multilingual language models and assess their suitability for the masked language modelling objective. Next, we measure gender bias encoded in West Slavic language models by quantifying the toxicity and genderness of the generated words. We find that these language models produce hurtful completions that depend on the subject's gender. Perhaps surprisingly, Czech, Slovak, and Polish language mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#37327;&#35268;&#33539;&#21644;&#25551;&#36848;&#24615;&#32844;&#19994;&#20998;&#24067;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#23448;&#26041;&#30340;&#20154;&#21475;&#26222;&#26597;&#25968;&#25454;&#26469;&#25506;&#27979;&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.05764</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#21475;&#26222;&#26597;&#25968;&#25454;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35268;&#33539;&#21644;&#25551;&#36848;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Measuring Normative and Descriptive Biases in Language Models Using Census Data. (arXiv:2304.05764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#37327;&#35268;&#33539;&#21644;&#25551;&#36848;&#24615;&#32844;&#19994;&#20998;&#24067;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#23448;&#26041;&#30340;&#20154;&#21475;&#26222;&#26597;&#25968;&#25454;&#26469;&#25506;&#27979;&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#32844;&#19994;&#22312;&#24615;&#21035;&#26041;&#38754;&#30340;&#20998;&#24067;&#22914;&#20309;&#21453;&#26144;&#22312;&#27169;&#22411;&#20013;&#12290;&#36825;&#26679;&#30340;&#20998;&#24067;&#24182;&#19981;&#24635;&#26159;&#31526;&#21512;&#35268;&#33539;&#29702;&#24819;&#65292;&#20063;&#19981;&#19968;&#23450;&#21453;&#26144;&#29616;&#23454;&#20013;&#30340;&#25551;&#36848;&#35780;&#20272;&#12290;&#20026;&#20102;&#34913;&#37327;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#35268;&#33539;&#21644;&#25551;&#36848;&#24615;&#32844;&#19994;&#20998;&#24067;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#27861;&#22269;&#12289;&#25386;&#23041;&#12289;&#33521;&#22269;&#21644;&#32654;&#22269;&#22269;&#23478;&#32479;&#35745;&#26426;&#26500;&#25552;&#20379;&#30340;&#26377;&#20851;&#24615;&#21035;-&#32844;&#19994;&#20998;&#24067;&#30340;&#23448;&#26041;&#20154;&#21475;&#26222;&#26597;&#20449;&#24687;&#12290;&#25105;&#20204;&#25163;&#21160;&#29983;&#25104;&#32467;&#21512;&#24615;&#21035;&#20195;&#35789;&#21644;&#21517;&#35789;&#20197;&#21450;&#32844;&#19994;&#30340;&#22522;&#20110;&#27169;&#26495;&#30340;&#21477;&#23376;&#65292;&#24182;&#38543;&#21518;&#25506;&#27979;&#33521;&#35821;&#12289;&#27861;&#35821;&#21644;&#25386;&#23041;&#35821;&#30340;&#21313;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24341;&#20837;&#30340;&#35780;&#20998;&#31995;&#32479;&#26159;&#29420;&#31435;&#20110;&#35821;&#35328;&#30340;&#65292;&#21487;&#20197;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;&#27169;&#26495;&#30340;&#21477;&#23376;&#12289;&#32844;&#19994;&#21644;&#35821;&#35328;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate in this paper how distributions of occupations with respect to gender is reflected in pre-trained language models. Such distributions are not always aligned to normative ideals, nor do they necessarily reflect a descriptive assessment of reality. In this paper, we introduce an approach for measuring to what degree pre-trained language models are aligned to normative and descriptive occupational distributions. To this end, we use official demographic information about gender--occupation distributions provided by the national statistics agencies of France, Norway, United Kingdom, and the United States. We manually generate template-based sentences combining gendered pronouns and nouns with occupations, and subsequently probe a selection of ten language models covering the English, French, and Norwegian languages. The scoring system we introduce in this work is language independent, and can be used on any combination of template-based sentences, occupations, and languages. 
&lt;/p&gt;</description></item><item><title>&#20840;&#23616;&#25552;&#31034;&#21333;&#20803;(GPC)&#26159;&#19968;&#31181;&#29992;&#20110;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#31227;&#26893;&#25511;&#21046;&#27169;&#22359;&#12290;&#23427;&#21487;&#20197;&#22312;&#25152;&#26377;&#32534;&#30721;&#22120;&#23618;&#20013;&#36873;&#25321;&#24615;&#22320;&#20445;&#30041;&#25552;&#31034;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102; 5.8% &#30340; SuperGLUE &#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.05642</link><description>&lt;p&gt;
&#20840;&#23616;&#25552;&#31034;&#21333;&#20803;&#65306;&#19968;&#31181;&#26377;&#25928;&#30340;&#31227;&#26893;&#25511;&#21046;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Global Prompt Cell: A Portable Control Module for Effective Prompt. (arXiv:2304.05642v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05642
&lt;/p&gt;
&lt;p&gt;
&#20840;&#23616;&#25552;&#31034;&#21333;&#20803;(GPC)&#26159;&#19968;&#31181;&#29992;&#20110;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#31227;&#26893;&#25511;&#21046;&#27169;&#22359;&#12290;&#23427;&#21487;&#20197;&#22312;&#25152;&#26377;&#32534;&#30721;&#22120;&#23618;&#20013;&#36873;&#25321;&#24615;&#22320;&#20445;&#30041;&#25552;&#31034;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102; 5.8% &#30340; SuperGLUE &#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#23616;&#25552;&#31034;&#21333;&#20803;(Global Prompt Cell, GPC)&#26159;&#19968;&#31181;&#29992;&#20110;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#31227;&#26893;&#25511;&#21046;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20923;&#32467;&#21442;&#25968;&#24182;&#22312;&#31532;&#19968;&#23618;&#30340;&#36755;&#20837;&#20013;&#25554;&#20837;&#21487;&#35757;&#32451;&#30340;&#23884;&#20837;&#21521;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20449;&#24687;&#21033;&#29992;&#38382;&#39064;&#65292;GPC&#21487;&#20197;&#22312;&#25152;&#26377;&#32534;&#30721;&#22120;&#23618;&#20013;&#36873;&#25321;&#24615;&#22320;&#20445;&#30041;&#25552;&#31034;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26222;&#36890;&#25552;&#31034;&#35843;&#25972;&#30456;&#27604;&#65292;GPC &#22312; SuperGLUE &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102; 5.8% &#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a novel approach to tuning pre-trained models, prompt tuning involves freezing the parameters in downstream tasks while inserting trainable embeddings into inputs in the first layer.However,previous methods have mainly focused on the initialization of prompt embeddings. The question of how to train and utilize prompt embeddings in a reasonable way has become aa limiting factor in the effectiveness of prompt tuning. To address this issue, we introduce the Global Prompt Cell (GPC), a portable control module for prompt tuning that selectively preserves prompt information across all encoder layers. Our experimental results demonstrate a 5.8% improvement on SuperGLUE datasets compared to vanilla prompt tuning.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;22&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;ChatGPT&#22312;&#21508;&#31181;&#35821;&#35328;&#21644;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#19968;&#33268;&#24615;&#21644;&#21151;&#25928;&#65292;&#36825;&#20026;&#22810;&#35821;&#35328;&#23398;&#20064;&#21644;LLMs&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.05613</link><description>&lt;p&gt;
ChatGPT&#36229;&#36234;&#33521;&#35821;&#65306;&#26397;&#30528;&#23545;&#22810;&#35821;&#35328;&#23398;&#20064;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning. (arXiv:2304.05613v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05613
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;22&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;ChatGPT&#22312;&#21508;&#31181;&#35821;&#35328;&#21644;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#19968;&#33268;&#24615;&#21644;&#21151;&#25928;&#65292;&#36825;&#20026;&#22810;&#35821;&#35328;&#23398;&#20064;&#21644;LLMs&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#26368;&#37325;&#35201;&#30340;&#31361;&#30772;&#20043;&#19968;&#65292;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;ChatGPT&#26159;&#26368;&#36817;&#24320;&#21457;&#30340;&#26368;&#20196;&#20154;&#20852;&#22859;&#30340;LLM&#31995;&#32479;&#20043;&#19968;&#65292;&#23637;&#31034;&#20102;&#23545;&#35821;&#35328;&#29983;&#25104;&#30340;&#20986;&#33394;&#25216;&#33021;&#65292;&#24182;&#21463;&#21040;&#20102;&#20844;&#20247;&#30340;&#39640;&#24230;&#20851;&#27880;&#12290;&#22312;&#21457;&#29616;ChatGPT&#22312;&#33521;&#35821;&#20013;&#30340;&#21508;&#31181;&#20196;&#20154;&#20852;&#22859;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#21644;&#29983;&#25104;&#22810;&#31181;&#35821;&#35328;&#30340;&#25991;&#26412;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#22810;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#12290;&#32771;&#34385;&#21040;ChatGPT&#22312;&#19981;&#21516;&#38382;&#39064;&#21644;&#39046;&#22495;&#30340;&#33521;&#35821;&#20013;&#30340;&#24191;&#27867;&#37319;&#29992;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;ChatGPT&#26159;&#21542;&#20063;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#20854;&#20182;&#35821;&#35328;&#65292;&#36824;&#26159;&#38656;&#35201;&#24320;&#21457;&#26356;&#22810;&#30340;&#35821;&#35328;&#29305;&#23450;&#25216;&#26415;&#65311;&#36825;&#20010;&#38382;&#39064;&#30340;&#31572;&#26696;&#38656;&#35201;&#23545;ChatGPT&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;&#21363;&#36229;&#20986;&#20102;&#25253;&#36947;&#30340;&#36726;&#20107;&#65289;&#65292;&#36825;&#22312;&#24403;&#21069;&#30340;&#30740;&#31350;&#20013;&#20173;&#28982;&#32570;&#20047;&#25110;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;ChatGPT&#30340;&#20840;&#38754;&#19968;&#32452;22&#31181;&#35821;&#35328;&#65292;&#20174;&#19981;&#21516;&#30340;&#23478;&#26063;&#20013;&#24320;&#21457;&#19968;&#20010;&#22810;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#21508;&#31181;&#35821;&#35328;&#21644;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#19968;&#33268;&#24615;&#21644;&#21151;&#25928;&#65292;&#36825;&#20026;&#38382;&#39064;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#27491;&#38754;&#31572;&#26696;&#12290;&#25105;&#20204;&#39044;&#35745;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23558;&#20419;&#36827;&#26410;&#26469;&#30340;&#22810;&#35821;&#35328;&#23398;&#20064;&#21644;LLMs&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last few years, large language models (LLMs) have emerged as the most important breakthroughs in natural language processing (NLP) that fundamentally transform research and developments in the field. ChatGPT represents one of the most exciting LLM systems developed recently to showcase impressive skills for language generation and highly attract public attention. Among various exciting applications discovered for ChatGPT in English, the model can process and generate texts for multiple languages due to its multilingual training data. Given the broad adoption of ChatGPT for English in different problems and areas, a natural question is whether ChatGPT can also be applied effectively for other languages or it is necessary to develop more language-specific technologies. The answer to this question requires a thorough evaluation of ChatGPT over multiple tasks with diverse languages and large datasets (i.e., beyond reported anecdotes), which is still missing or limited in current r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#20256;&#32479;&#30340;&#35821;&#20041;&#29305;&#24449;&#35268;&#33539;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#20043;&#33021;&#22815;&#25429;&#25417;&#36229;&#36807;&#20154;&#31867;&#35268;&#33539;&#33539;&#30068;&#30340;&#20449;&#24687;&#65292;&#23545;&#20110;&#25105;&#20204;&#29702;&#35299;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#27010;&#24565;&#34920;&#31034;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.05591</link><description>&lt;p&gt;
FLAN-T5&#20013;&#35821;&#20041;&#29305;&#24449;&#39564;&#35777;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Semantic Feature Verification in FLAN-T5. (arXiv:2304.05591v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#20256;&#32479;&#30340;&#35821;&#20041;&#29305;&#24449;&#35268;&#33539;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#20043;&#33021;&#22815;&#25429;&#25417;&#36229;&#36807;&#20154;&#31867;&#35268;&#33539;&#33539;&#30068;&#30340;&#20449;&#24687;&#65292;&#23545;&#20110;&#25105;&#20204;&#29702;&#35299;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#27010;&#24565;&#34920;&#31034;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#35821;&#20041;&#29305;&#24449;&#35268;&#33539;&#26041;&#38754;&#30340;&#28508;&#21147;&#8212;&#8212;&#36825;&#26159;&#35780;&#20215;&#35748;&#30693;&#31185;&#23398;&#20013;&#27010;&#24565;&#32467;&#26500;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#25105;&#20204;&#22522;&#20110;&#29616;&#26377;&#30340;&#20154;&#24037;&#29983;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#26500;&#24314;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#26426;&#22120;&#39564;&#35777;&#30340;&#35268;&#33539;&#25429;&#25417;&#20102;&#27010;&#24565;&#32467;&#26500;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#36229;&#36234;&#20102;&#20165;&#32771;&#34385;&#20154;&#31867;&#35268;&#33539;&#25152;&#33021;&#28085;&#30422;&#30340;&#33539;&#22260;&#65292;&#24182;&#19988;&#26356;&#22909;&#22320;&#35299;&#37322;&#20102;&#22312;&#36828;&#36317;&#31163;&#30456;&#20851;&#30340;&#39033;&#30446;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#20154;&#31867;&#21028;&#26029;&#12290;&#35813;&#32467;&#26524;&#34920;&#26126;LLM&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#20256;&#32479;&#30340;&#35821;&#20041;&#29305;&#24449;&#35268;&#33539;&#39564;&#35777;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#25105;&#20204;&#29702;&#35299;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#27010;&#24565;&#34920;&#31034;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study evaluates the potential of a large language model for aiding in generation of semantic feature norms - a critical tool for evaluating conceptual structure in cognitive science. Building from an existing human-generated dataset, we show that machine-verified norms capture aspects of conceptual structure beyond what is expressed in human norms alone, and better explain human judgments of semantic similarity amongst items that are distally related. The results suggest that LLMs can greatly enhance traditional methods of semantic feature norm verification, with implications for our understanding of conceptual representation in humans and machines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20248;&#20110;&#38543;&#26426;&#25277;&#26679;&#26041;&#27861;&#21644;&#20854;&#20182;AL&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.05578</link><description>&lt;p&gt;
&#20449;&#24687;&#37327;&#26159;&#21542;&#37325;&#35201;&#65311;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Does Informativeness Matter? Active Learning for Educational Dialogue Act Classification. (arXiv:2304.05578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20248;&#20110;&#38543;&#26426;&#25277;&#26679;&#26041;&#27861;&#21644;&#20854;&#20182;AL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#35805;&#34892;&#20026;&#65288;DA&#65289;&#65292;&#21487;&#20197;&#35299;&#37322;&#19987;&#23478;&#23548;&#24072;&#22312;&#36741;&#23548;&#36807;&#31243;&#20013;&#20570;&#20102;&#20160;&#20040;&#20197;&#21450;&#23398;&#29983;&#30693;&#36947;&#20160;&#20040;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23454;&#35777;&#30740;&#31350;&#22810;&#37319;&#29992;&#38543;&#26426;&#25277;&#26679;&#27861;&#26469;&#33719;&#24471;&#25163;&#21160;&#27880;&#37322;DA&#30340;&#21477;&#23376;&#26679;&#26412;&#65292;&#28982;&#21518;&#29992;&#20110;&#22521;&#35757;DA&#20998;&#31867;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#30340;&#20449;&#24687;&#26679;&#26412;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20182;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#38543;&#26426;&#25277;&#26679;&#26041;&#27861;&#21644;&#20854;&#20182;&#26368;&#26032;&#30340;AL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue Acts (DAs) can be used to explain what expert tutors do and what students know during the tutoring process. Most empirical studies adopt the random sampling method to obtain sentence samples for manual annotation of DAs, which are then used to train DA classifiers. However, these studies have paid little attention to sample informativeness, which can reflect the information quantity of the selected samples and inform the extent to which a classifier can learn patterns. Notably, the informativeness level may vary among the samples and the classifier might only need a small amount of low informative samples to learn the patterns. Random sampling may overlook sample informativeness, which consumes human labelling costs and contributes less to training the classifiers. As an alternative, researchers suggest employing statistical sampling methods of Active Learning (AL) to identify the informative samples for training the classifiers. However, the use of AL methods in educational D
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;GPT(-3.5&#21644;-4)&#29983;&#25104;&#30340;&#26085;&#35821;&#25991;&#20307;&#29305;&#24449;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#23398;&#26415;&#35770;&#25991;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#25991;&#20307;&#29305;&#24449;&#19978;&#26377;&#26174;&#33879;&#21306;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.05534</link><description>&lt;p&gt;
&#36890;&#36807;&#26085;&#35821;&#25991;&#20307;&#20998;&#26512;&#21306;&#20998;ChatGPT(-3.5,-4)&#30340;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Distinguishing ChatGPT(-3.5, -4)-generated and human-written papers through Japanese stylometric analysis. (arXiv:2304.05534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;GPT(-3.5&#21644;-4)&#29983;&#25104;&#30340;&#26085;&#35821;&#25991;&#20307;&#29305;&#24449;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#23398;&#26415;&#35770;&#25991;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#25991;&#20307;&#29305;&#24449;&#19978;&#26377;&#26174;&#33879;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#21253;&#25324;OpenAI&#30340;GPT-3.5&#21644;GPT-4&#65292;&#24341;&#36215;&#20102;&#20840;&#29699;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GPT(-3.5&#21644;-4)&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#26085;&#35821;&#25991;&#20307;&#29305;&#24449;&#12290;&#36890;&#36807;&#22810;&#32500;&#23610;&#24230;&#20998;&#26512;&#65292;&#23558;216&#20010;&#25991;&#26412;&#65288;36&#20301;&#21333;&#19968;&#20316;&#32773;&#30340;72&#31687;&#23398;&#26415;&#35770;&#25991;&#12289;72&#31687;GPT-3.5&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;72&#31687;GPT-4&#29983;&#25104;&#30340;&#25991;&#26412;&#65289;&#26681;&#25454;&#35789;&#24615;&#30340;&#20108;&#20803;&#32452;&#65292;&#35789;&#23614;&#30340;&#20108;&#20803;&#32452;&#65292;&#36887;&#21495;&#30340;&#20301;&#32622;&#21644;&#21151;&#33021;&#35789;&#30340;&#27604;&#20363;&#20998;&#25104;&#19977;&#31867;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;GPT(-3.5&#65292;-4)&#21644;&#20154;&#31867;&#20043;&#38388;&#22312;&#25991;&#20307;&#29305;&#24449;&#19978;&#26377;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-generative artificial intelligence (AI), including ChatGPT, equipped with GPT-3.5 and GPT-4, from OpenAI, has attracted considerable attention worldwide. In this study, first, we compared Japanese stylometric features generated by GPT (-3.5 and -4) and those written by humans. In this work, we performed multi-dimensional scaling (MDS) to confirm the classification of 216 texts into three classes (72 academic papers written by 36 single authors, 72 texts generated by GPT-3.5, and 72 texts generated by GPT-4 on the basis of the titles of the aforementioned papers) focusing on the following stylometric features: (1) bigrams of parts-of-speech, (2) bigram of postpositional particle words, (3) positioning of commas, and (4) rate of function words. MDS revealed distinct distributions at each stylometric feature of GPT (-3.5 and -4) and human. Although GPT-4 is more powerful than GPT-3.5 because it has more parameters, both GPT (-3.5 and -4) distributions are likely to overlap. These res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#22238;&#31572;&#22522;&#20110;&#24050;&#26377;&#22240;&#26524;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#21457;&#29616;&#26032;&#30693;&#35782;&#25110;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#30340;&#39640;&#31934;&#24230;&#35201;&#27714;&#19981;&#36275;&#12290;&#26410;&#26469;&#21487;&#20197;&#36890;&#36807;&#21551;&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30340;&#22240;&#26524;&#27169;&#22359;&#20197;&#21450;&#28145;&#24230;&#22240;&#26524;&#24863;&#30693;&#30340;LLMs&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.05524</link><description>&lt;p&gt;
&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;: &#21487;&#34892;&#24615;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Understanding Causality with Large Language Models: Feasibility and Opportunities. (arXiv:2304.05524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#22238;&#31572;&#22522;&#20110;&#24050;&#26377;&#22240;&#26524;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#21457;&#29616;&#26032;&#30693;&#35782;&#25110;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#30340;&#39640;&#31934;&#24230;&#35201;&#27714;&#19981;&#36275;&#12290;&#26410;&#26469;&#21487;&#20197;&#36890;&#36807;&#21551;&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30340;&#22240;&#26524;&#27169;&#22359;&#20197;&#21450;&#28145;&#24230;&#22240;&#26524;&#24863;&#30693;&#30340;LLMs&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22238;&#31572;&#19977;&#31181;&#31867;&#22411;&#22240;&#26524;&#38382;&#39064;&#26102;&#30340;&#20248;&#32570;&#28857;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24403;&#21069;&#30340;LLMs&#21487;&#20197;&#20687;&#39046;&#22495;&#19987;&#23478;&#19968;&#26679;&#22238;&#31572;&#22522;&#20110;&#24050;&#26377;&#22240;&#26524;&#30693;&#35782;&#30340;&#22240;&#26524;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#36824;&#19981;&#33021;&#20026;&#21457;&#29616;&#26032;&#30693;&#35782;&#25110;&#39640;&#31934;&#24230;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#21487;&#33021;&#30340;&#26041;&#21521;&#21644;&#26426;&#36935;&#65292;&#20363;&#22914;&#21551;&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30340;&#22240;&#26524;&#27169;&#22359;&#20197;&#21450;&#28145;&#24230;&#22240;&#26524;&#24863;&#30693;&#30340;LLMs&#12290;&#36825;&#20123;&#19981;&#20165;&#21487;&#20197;&#20351;LLMs&#22238;&#31572;&#26356;&#22810;&#31867;&#22411;&#30340;&#22240;&#26524;&#38382;&#39064;&#20197;&#23454;&#29616;&#26356;&#22823;&#30340;&#24433;&#21709;&#21147;&#65292;&#36824;&#21487;&#20197;&#20351;LLMs&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26356;&#21152;&#20540;&#24471;&#20449;&#20219;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We assess the ability of large language models (LLMs) to answer causal questions by analyzing their strengths and weaknesses against three types of causal question. We believe that current LLMs can answer causal questions with existing causal knowledge as combined domain experts. However, they are not yet able to provide satisfactory answers for discovering new knowledge or for high-stakes decision-making tasks with high precision. We discuss possible future directions and opportunities, such as enabling explicit and implicit causal modules as well as deep causal-aware LLMs. These will not only enable LLMs to answer many different types of causal questions for greater impact but also enable LLMs to be more trustworthy and efficient in general.
&lt;/p&gt;</description></item><item><title>MoMo&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#30340;&#20849;&#20139;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#22788;&#29702;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#22791;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21333;&#19968;&#30340;&#21464;&#21387;&#22120;&#21644;&#38454;&#27573;&#24615;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#20445;&#30041;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#19982;&#24378;&#27169;&#22411;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.05523</link><description>&lt;p&gt;
MoMo: &#19968;&#20010;&#20849;&#20139;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22810;&#27169;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MoMo: A shared encoder Model for text, image and multi-Modal representations. (arXiv:2304.05523v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05523
&lt;/p&gt;
&lt;p&gt;
MoMo&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#30340;&#20849;&#20139;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#22788;&#29702;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#22791;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21333;&#19968;&#30340;&#21464;&#21387;&#22120;&#21644;&#38454;&#27573;&#24615;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#20445;&#30041;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#19982;&#24378;&#27169;&#22411;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#30340;&#20849;&#20139;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#23427;&#22312;&#20960;&#20010;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20855;&#26377;&#25968;&#25454;&#12289;&#20869;&#23384;&#21644;&#36816;&#34892;&#26102;&#25928;&#29575;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#20316;&#21697;&#30456;&#27604;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#21464;&#21387;&#22120;&#65292;&#25152;&#26377;&#32534;&#30721;&#22120;&#23618;&#22788;&#29702;&#25991;&#26412;&#21644;&#22270;&#20687;&#27169;&#24577;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20854;&#20013;&#27169;&#22411;&#39318;&#20808;&#22312;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#21333;&#27169;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#26368;&#21518;&#22312;&#25991;&#26412;&#21644;&#25991;&#26412;-&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#12290;&#31532;&#19977;&#65292;&#20026;&#20102;&#22312;&#20004;&#31181;&#27169;&#24335;&#19979;&#20445;&#30041;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35757;&#32451;&#31649;&#36947;&#65292;&#23427;&#22312;&#27599;&#20010;&#35757;&#32451;&#26356;&#26032;&#27493;&#39588;&#26102;&#21516;&#26102;&#20174;&#19981;&#21516;&#27169;&#24577;&#30340;&#26799;&#24230;&#26356;&#26032;&#20013;&#23398;&#20064;&#12290;&#19979;&#28216;&#30340;&#32431;&#25991;&#26412;&#12289;&#32431;&#22270;&#20687;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20960;&#20010;&#24378;&#27169;&#22411;&#31454;&#20105;&#65292;&#21516;&#26102;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#36739;&#23569;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#20363;&#22914;&#65292;MoMo&#22312;&#19982;FLAVA&#30340;&#31454;&#20105;&#20013;&#34920;&#29616;&#24471;&#24456;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a self-supervised shared encoder model that achieves strong results on several visual, language and multimodal benchmarks while being data, memory and run-time efficient. We make three key contributions. First, in contrast to most existing works, we use a single transformer with all the encoder layers processing both the text and the image modalities. Second, we propose a stage-wise training strategy where the model is first trained on images, then jointly with unimodal text and image datasets and finally jointly with text and text-image datasets. Third, to preserve information across both the modalities, we propose a training pipeline that learns simultaneously from gradient updates of different modalities at each training update step. The results on downstream text-only, image-only and multimodal tasks show that our model is competitive with several strong models while using fewer parameters and lesser pre-training data. For example, MoMo performs competitively with FLAVA 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;&#35834;&#36125;&#23572;&#25991;&#23398;&#22870;&#24471;&#20027;&#22885;&#23572;&#32597;&#183;&#24085;&#24917;&#20811;&#30340;&#33879;&#20316;&#20026;&#20363;&#23376;&#65292;&#36890;&#36807;&#35745;&#31639;&#25991;&#26412;&#20013;&#23383;&#27597;&#21644;&#21333;&#35789;&#30340;&#25968;&#37327;&#65292;&#20351;&#29992;&#20998;&#24418;&#20960;&#20309;&#26041;&#27861;&#35745;&#31639;&#20102;&#20182;&#30340;&#25991;&#26412;&#30340;&#20998;&#24418;&#32500;&#24230;&#65292;&#24182;&#19982;&#24212;&#29992;&#20110;&#23383;&#27597;&#21644;&#21333;&#35789;&#30340;Zipf&#23450;&#24459;&#36827;&#34892;&#27604;&#36739;&#12290;&#21457;&#29616;&#23567;&#35828;&#12298;&#25105;&#30340;&#21517;&#23383;&#21483;&#32418;&#12299;&#30340;Zipf&#32500;&#24230;&#19982;&#20182;&#30340;&#20854;&#20182;&#23567;&#35828;&#26377;&#24456;&#22823;&#30340;&#19981;&#21516;&#65292;&#20294;&#35821;&#35328;&#23398;&#19978;&#24182;&#27809;&#26377;&#26681;&#26412;&#30340;&#21306;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.05512</link><description>&lt;p&gt;
&#22885;&#23572;&#32597;&#183;&#24085;&#24917;&#20811;&#35834;&#36125;&#23572;&#25991;&#23398;&#22870;&#20316;&#21697;&#30340;&#25968;&#23398;&#21644;&#35821;&#35328;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Mathematical and Linguistic Characterization of Orhan Pamuk's Nobel Works. (arXiv:2304.05512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;&#35834;&#36125;&#23572;&#25991;&#23398;&#22870;&#24471;&#20027;&#22885;&#23572;&#32597;&#183;&#24085;&#24917;&#20811;&#30340;&#33879;&#20316;&#20026;&#20363;&#23376;&#65292;&#36890;&#36807;&#35745;&#31639;&#25991;&#26412;&#20013;&#23383;&#27597;&#21644;&#21333;&#35789;&#30340;&#25968;&#37327;&#65292;&#20351;&#29992;&#20998;&#24418;&#20960;&#20309;&#26041;&#27861;&#35745;&#31639;&#20102;&#20182;&#30340;&#25991;&#26412;&#30340;&#20998;&#24418;&#32500;&#24230;&#65292;&#24182;&#19982;&#24212;&#29992;&#20110;&#23383;&#27597;&#21644;&#21333;&#35789;&#30340;Zipf&#23450;&#24459;&#36827;&#34892;&#27604;&#36739;&#12290;&#21457;&#29616;&#23567;&#35828;&#12298;&#25105;&#30340;&#21517;&#23383;&#21483;&#32418;&#12299;&#30340;Zipf&#32500;&#24230;&#19982;&#20182;&#30340;&#20854;&#20182;&#23567;&#35828;&#26377;&#24456;&#22823;&#30340;&#19981;&#21516;&#65292;&#20294;&#35821;&#35328;&#23398;&#19978;&#24182;&#27809;&#26377;&#26681;&#26412;&#30340;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36873;&#25321;&#20102;&#35834;&#36125;&#23572;&#25991;&#23398;&#22870;&#24471;&#20027;&#22885;&#23572;&#32597;&#183;&#24085;&#24917;&#20811;&#30340;&#33879;&#20316;&#20316;&#20026;&#22303;&#32819;&#20854;&#25991;&#23398;&#30340;&#20363;&#23376;&#12290;&#36890;&#36807;&#35745;&#31639;&#20182;&#30340;&#25991;&#26412;&#20013;&#23383;&#27597;&#21644;&#21333;&#35789;&#30340;&#25968;&#37327;&#65292;&#25105;&#20204;&#21457;&#29616;&#21487;&#20197;&#36827;&#34892;&#32479;&#35745;&#23398;&#30740;&#31350;&#12290;&#24050;&#30693;&#25991;&#26412;&#32467;&#26500;&#20013;&#23384;&#22312;&#20960;&#20309;&#39034;&#24207;&#12290;&#36825;&#37324;&#20171;&#32461;&#20102;&#22522;&#20110;&#20998;&#24418;&#20960;&#20309;&#22522;&#26412;&#20551;&#35774;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#24085;&#22982;&#20811;&#25991;&#26412;&#30340;&#20998;&#24418;&#32500;&#24230;&#12290;&#32467;&#26524;&#19982;&#24212;&#29992;&#20110;&#23383;&#27597;&#21644;&#21333;&#35789;&#30340;Zipf&#23450;&#24459;&#30340;&#24212;&#29992;&#36827;&#34892;&#27604;&#36739;&#65292;&#24341;&#20837;&#20102;&#20004;&#20010;&#27010;&#24565;&#65292;&#21363;Zipf&#32500;&#24230;&#21644;Zipf&#39034;&#24207;&#12290;&#21457;&#29616;&#23567;&#35828;&#12298;&#25105;&#30340;&#21517;&#23383;&#21483;&#32418;&#12299;&#30340;Zipf&#32500;&#24230;&#19982;&#20182;&#30340;&#20854;&#20182;&#23567;&#35828;&#26377;&#24456;&#22823;&#30340;&#19981;&#21516;&#12290;&#28982;&#32780;&#65292;&#20174;&#35821;&#35328;&#23398;&#35282;&#24230;&#35266;&#23519;&#65292;&#20182;&#30340;&#35821;&#26009;&#24211;&#20043;&#38388;&#24182;&#27809;&#26377;&#26681;&#26412;&#30340;&#21306;&#21035;&#12290;&#32467;&#26524;&#20197;&#20998;&#24418;&#32500;&#24230;&#21644;&#22303;&#32819;&#20854;&#35821;&#35328;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, Nobel Laureate Orhan Pamuk's works are chosen as examples of Turkish literature. By counting the number of letters and words in his texts, we find it possible to study his works statistically. It has been known that there is a geometrical order in text structures. Here the method based on the basic assumption of fractal geometry is introduced for calculating the fractal dimensions of Pamuk's texts. The results are compared with the applications of Zipf's law, which is successfully applied for letters and words, where two concepts, namely Zipf's dimension and Zipf's order, are introduced. The Zipf dimension of the novel My Name is Red is found to be much different than his other novels. However, it is linguistically observed that there is no fundamental difference between his corpora. The results are interpreted in terms of fractal dimensions and the Turkish language.
&lt;/p&gt;</description></item><item><title>chatIPCC&#26159;&#19968;&#20010;&#22522;&#20110;&#27668;&#20505;&#31185;&#23398;&#30340;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#36890;&#36807;&#25972;&#21512;IPCC AR6&#20013;&#30340;&#20449;&#24687;&#26469;&#22686;&#24378;GPT-4&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#21487;&#38752;&#12289;&#31185;&#23398;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.05510</link><description>&lt;p&gt;
chatIPCC: &#22522;&#20110;&#27668;&#20505;&#31185;&#23398;&#30340;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
chatIPCC: Grounding Conversational AI in Climate Science. (arXiv:2304.05510v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05510
&lt;/p&gt;
&lt;p&gt;
chatIPCC&#26159;&#19968;&#20010;&#22522;&#20110;&#27668;&#20505;&#31185;&#23398;&#30340;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#36890;&#36807;&#25972;&#21512;IPCC AR6&#20013;&#30340;&#20449;&#24687;&#26469;&#22686;&#24378;GPT-4&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#21487;&#38752;&#12289;&#31185;&#23398;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36817;&#24180;&#26469;&#22312;&#38382;&#31572;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#24187;&#35273;&#21644;&#35757;&#32451;&#21518;&#20449;&#24687;&#36807;&#26102;&#12290;&#22312;&#20851;&#38190;&#39046;&#22495;&#65292;&#22914;&#27668;&#20505;&#21464;&#21270;&#65292;&#24555;&#36895;&#20174;&#21487;&#38752;&#26469;&#28304;&#33719;&#21462;&#20934;&#30830;&#21644;&#26368;&#26032;&#20449;&#24687;&#26159;&#33267;&#20851;&#37325;&#35201;&#19988;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#22806;&#37096;&#31185;&#23398;&#20934;&#30830;&#19988;&#21487;&#38752;&#30340;&#26469;&#28304;&#65288;&#38271;&#26399;&#35760;&#24518;&#65289;&#65292;&#20197;&#25345;&#32493;&#26356;&#26032;&#20854;&#30693;&#35782;&#24182;&#38450;&#27490;&#19981;&#20934;&#30830;&#12289;&#19981;&#27491;&#30830;&#25110;&#36807;&#26102;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25972;&#21512;&#32852;&#21512;&#25919;&#24220;&#38388;&#27668;&#20505;&#21464;&#21270;&#19987;&#38376;&#22996;&#21592;&#20250;&#31532;&#20845;&#27425;&#35780;&#20272;&#25253;&#21578;&#65288;IPCC AR6&#65289;&#20013;&#30340;&#20449;&#24687;&#26469;&#22686;&#24378;GPT-4&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#21407;&#22411;&#65292;&#21487;&#20197;&#22312;www.chatclima.com&#19978;&#25552;&#38382;&#19982;&#27668;&#20505;&#31185;&#23398;&#30456;&#20851;&#30340;&#38382;&#39064;&#24182;&#33719;&#24471;&#22522;&#20110;IPCC AR6&#25253;&#21578;&#30340;&#31185;&#23398;&#20934;&#30830;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made significant progress in recent years, achieving remarkable results in question-answering tasks (QA). However, they still face two major challenges: hallucination and outdated information after the training phase. These challenges take center stage in critical domains like climate change, where obtaining accurate and up-to-date information from reliable sources in a limited time is essential and difficult. To overcome these barriers, one potential solution is to provide LLMs with access to external, scientifically accurate, and robust sources (long-term memory) to continuously update their knowledge and prevent the propagation of inaccurate, incorrect, or outdated information. In this study, we enhanced GPT-4 by integrating the information from the Sixth Assessment Report of the Intergovernmental (IPCC AR6), the most comprehensive, up-to-date, and reliable source in this domain. We present our conversational AI prototype, available at www.chatclima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#31185;&#35774;&#32622;&#30340;&#33258;&#36866; &#24212;&#35821;&#35328;&#23398;&#20064;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#35813;&#26426;&#22120;&#20154;&#37319;&#29992;&#35789;&#27719;&#38480;&#21046;&#35299;&#30721;&#24341;&#23548;&#31995;&#32479;&#29983;&#25104;&#31526;&#21512;&#23398;&#29983;&#35838;&#31243;&#35774;&#32622;&#30340;&#35821;&#35328;&#65292;&#32463;&#36807;&#22312;&#20013;&#23398;&#29983;&#23398;&#20064;&#33521;&#35821;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#21644;&#20132;&#27969;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2304.05489</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#31185;&#35774;&#32622;&#30340;&#33258;&#36866;&#24212;&#35821;&#35328;&#23398;&#20064;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
User Adaptive Language Learning Chatbots with a Curriculum. (arXiv:2304.05489v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#31185;&#35774;&#32622;&#30340;&#33258;&#36866; &#24212;&#35821;&#35328;&#23398;&#20064;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#35813;&#26426;&#22120;&#20154;&#37319;&#29992;&#35789;&#27719;&#38480;&#21046;&#35299;&#30721;&#24341;&#23548;&#31995;&#32479;&#29983;&#25104;&#31526;&#21512;&#23398;&#29983;&#35838;&#31243;&#35774;&#32622;&#30340;&#35821;&#35328;&#65292;&#32463;&#36807;&#22312;&#20013;&#23398;&#29983;&#23398;&#20064;&#33521;&#35821;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#21644;&#20132;&#27969;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#23545;&#35805;&#31995;&#32479;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35821;&#35328;&#23398;&#20064;&#21644;&#32451;&#20064;&#12290;&#35768;&#22810;&#24403;&#21069;&#30340;&#25945;&#32946;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#38386;&#32842;&#65292;&#29983;&#25104;&#20869;&#23481;&#21644;&#35789;&#27719;&#19981;&#21463;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#22312;&#23398;&#26657;&#29615;&#22659;&#20013;&#65292;&#22914;&#26524;&#23545;&#35805;&#31526;&#21512;&#23398;&#29983;&#30340;&#35838;&#31243;&#35774;&#32622;&#24182;&#20391;&#37325;&#20110;&#35838;&#26412;&#35789;&#27719;&#65292;&#21017;&#36890;&#36807;&#23545;&#35805;&#23454;&#36341;&#26356;&#20026;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35843;&#25972;&#20102;&#19968;&#31181;&#20855;&#26377;&#35789;&#27719;&#38480;&#21046;&#35299;&#30721;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#20419;&#20351;&#23545;&#35805;&#31995;&#32479;&#22312;&#29983;&#25104;&#35821;&#35328;&#26102;&#21253;&#21547;&#19982;&#35838;&#31243;&#30456;&#20851;&#30340;&#35789;&#27719;&#21644;&#30701;&#35821;&#12290;&#25105;&#20204;&#37319;&#29992;&#29983;&#25104;&#23545;&#35805;&#31995;&#32479;BlenderBot3&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#27169;&#22411;&#65292;&#24182;&#23545;&#23398;&#20064;&#33521;&#35821;&#20316;&#20026;&#31532;&#20108;&#35821;&#35328;&#30340;&#20013;&#23398;&#29983;&#36827;&#34892;&#35838;&#31243;&#35774;&#32622;&#30340;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#35780;&#20272;&#12290;&#38480;&#21046;&#30340;&#21333;&#35789;&#21644;&#30701;&#35821;&#26469;&#33258;&#20182;&#20204;&#30340;&#35838;&#26412;&#65292;&#24182;&#30001;&#20182;&#20204;&#30340;&#33521;&#35821;&#32769;&#24072;&#24314;&#35758;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#35838;&#31243;&#20449;&#24687;&#30340;&#23545;&#35805;&#31995;&#32479;&#21487;&#20197;&#19982;&#23398;&#29983;&#20135;&#29983;&#26356;&#26377;&#25928;&#21644;&#21560;&#24341;&#20154;&#30340;&#23545;&#35805;&#65292;&#20934;&#30830;&#20351;&#29992;&#19982;&#35838;&#31243;&#23545;&#40784;&#30340;&#35789;&#27719;&#21487;&#20197;&#22686;&#24378;&#23398;&#20064;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Along with the development of systems for natural language understanding and generation, dialog systems have been widely adopted for language learning and practicing. Many current educational dialog systems perform chitchat, where the generated content and vocabulary are not constrained. However, for learners in a school setting, practice through dialog is more effective if it aligns with students' curriculum and focuses on textbook vocabulary. Therefore, we adapt lexically constrained decoding to a dialog system, which urges the dialog system to include curriculum-aligned words and phrases in its generated utterances. We adopt a generative dialog system, BlenderBot3, as our backbone model and evaluate our curriculum-based dialog system with middle school students learning English as their second language. The constrained words and phrases are derived from their textbooks, suggested by their English teachers. The evaluation result demonstrates that the dialog system with curriculum inf
&lt;/p&gt;</description></item><item><title>&#22622;&#23572;&#32500;&#20122;&#35821;&#26159;&#19968;&#31181;&#20302;&#36164;&#28304;&#12289;&#39640;&#23624;&#25240;&#35821;&#35328;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36807;&#21435;&#19977;&#21313;&#24180;&#20013;&#65292;&#26377;&#35768;&#22810;&#20513;&#35758;&#24320;&#23637;&#20102;&#22622;&#23572;&#32500;&#20122;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36164;&#28304;&#21644;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2304.05468</link><description>&lt;p&gt;
&#22622;&#23572;&#32500;&#20122;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36164;&#28304;&#21644;&#26041;&#27861;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Resources and Methods for Natural Language Processing of Serbian Language. (arXiv:2304.05468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05468
&lt;/p&gt;
&lt;p&gt;
&#22622;&#23572;&#32500;&#20122;&#35821;&#26159;&#19968;&#31181;&#20302;&#36164;&#28304;&#12289;&#39640;&#23624;&#25240;&#35821;&#35328;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36807;&#21435;&#19977;&#21313;&#24180;&#20013;&#65292;&#26377;&#35768;&#22810;&#20513;&#35758;&#24320;&#23637;&#20102;&#22622;&#23572;&#32500;&#20122;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36164;&#28304;&#21644;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22622;&#23572;&#32500;&#20122;&#35821;&#26159;&#19968;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#65292;&#25317;&#26377;&#36229;&#36807;1200&#19975;&#30340;&#35828;&#35805;&#32773;&#65292;&#24182;&#34987;&#36229;&#36807;1500&#19975;&#20154;&#20102;&#35299;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#23427;&#21487;&#34987;&#35270;&#20316;&#19968;&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#24182;&#19988;&#26159;&#19968;&#31181;&#39640;&#23624;&#25240;&#35821;&#35328;&#12290;&#35768;&#22810;&#21333;&#35789;&#30340;&#23624;&#25240;&#24418;&#24335;&#21644;&#35821;&#35328;&#36164;&#28304;&#30340;&#31232;&#32570;&#24615;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20351;&#24471;&#22622;&#23572;&#32500;&#20122;&#35821;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#36807;&#21435;&#30340;&#19977;&#21313;&#24180;&#20013;&#65292;&#26377;&#35768;&#22810;&#20513;&#35758;&#24320;&#23637;&#20102;&#22622;&#23572;&#32500;&#20122;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36164;&#28304;&#21644;&#26041;&#27861;&#30340;&#24320;&#21457;&#65292;&#20174;&#20070;&#31821;&#21644;&#20114;&#32852;&#32593;&#30340;&#20813;&#36153;&#25991;&#26412;&#35821;&#26009;&#24211;&#12289;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#21040;&#21508;&#31181;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#30340;&#26041;&#27861;&#21644;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#36825;&#20123;&#20513;&#35758;&#12289;&#36164;&#28304;&#12289;&#26041;&#27861;&#21450;&#20854;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Serbian language is a Slavic language spoken by over 12 million speakers and well understood by over 15 million people. In the area of natural language processing, it can be considered a low-resourced language. Also, Serbian is considered a high-inflectional language. The combination of many word inflections and low availability of language resources makes natural language processing of Serbian challenging. Nevertheless, over the past three decades, there have been a number of initiatives to develop resources and methods for natural language processing of Serbian, ranging from developing a corpus of free text from books and the internet, annotated corpora for classification and named entity recognition tasks to various methods and models performing these tasks. In this paper, we review the initiatives, resources, methods, and their availability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;ChatGPT&#30340;&#38646;&#26679;&#26412;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;&#65292;&#35774;&#35745;&#20102;&#19977;&#31181;&#25552;&#31034;&#25216;&#26415;&#26469;&#25286;&#20998;&#20219;&#21153;&#24182;&#35780;&#20272;ChatGPT&#65292;&#23454;&#39564;&#34920;&#26126;ChatGPT&#30340;&#34920;&#29616;&#19982;&#30417;&#30563;&#26041;&#27861;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#65292;&#20294;&#23427;&#21487;&#20197;&#26356;&#27491;&#30830;&#22320;&#25512;&#26029;&#20986;&#26356;&#22810;&#30340;&#23567;&#20851;&#31995;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.05454</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT&#30340;&#38646;&#26679;&#26412;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Temporal Relation Extraction with ChatGPT. (arXiv:2304.05454v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;ChatGPT&#30340;&#38646;&#26679;&#26412;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;&#65292;&#35774;&#35745;&#20102;&#19977;&#31181;&#25552;&#31034;&#25216;&#26415;&#26469;&#25286;&#20998;&#20219;&#21153;&#24182;&#35780;&#20272;ChatGPT&#65292;&#23454;&#39564;&#34920;&#26126;ChatGPT&#30340;&#34920;&#29616;&#19982;&#30417;&#30563;&#26041;&#27861;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#65292;&#20294;&#23427;&#21487;&#20197;&#26356;&#27491;&#30830;&#22320;&#25512;&#26029;&#20986;&#26356;&#22810;&#30340;&#23567;&#20851;&#31995;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;&#30340;&#30446;&#26631;&#26159;&#25512;&#26029;&#25991;&#26723;&#20013;&#20004;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#12290; &#30417;&#30563;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;ChatGPT&#22312;&#38646;&#26679;&#26412;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#25216;&#26415;&#26469;&#25286;&#20998;&#20219;&#21153;&#24182;&#35780;&#20272;ChatGPT&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ChatGPT&#30340;&#34920;&#29616;&#19982;&#30417;&#30563;&#26041;&#27861;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#65292;&#32780;&#19988;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25552;&#31034;&#30340;&#35774;&#35745;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#19982;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;ChatGPT&#21487;&#20197;&#26356;&#27491;&#30830;&#22320;&#25512;&#26029;&#20986;&#26356;&#22810;&#30340;&#23567;&#20851;&#31995;&#31867;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;ChatGPT&#22312;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;&#29616;&#26377;&#32570;&#38519;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#22312;&#26102;&#38388;&#25512;&#26029;&#36807;&#31243;&#20013;&#26080;&#27861;&#20445;&#25345;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#22312;&#20027;&#21160;&#38271;&#20381;&#36182;&#26102;&#38388;&#25512;&#26029;&#20013;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of temporal relation extraction is to infer the temporal relation between two events in the document. Supervised models are dominant in this task. In this work, we investigate ChatGPT's ability on zero-shot temporal relation extraction. We designed three different prompt techniques to break down the task and evaluate ChatGPT. Our experiments show that ChatGPT's performance has a large gap with that of supervised methods and can heavily rely on the design of prompts. We further demonstrate that ChatGPT can infer more small relation classes correctly than supervised methods. The current shortcomings of ChatGPT on temporal relation extraction are also discussed in this paper. We found that ChatGPT cannot keep consistency during temporal inference and it fails in actively long-dependency temporal inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;OpenAI GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#19982;&#22825;&#25991;&#23398;&#25991;&#29486;&#36827;&#34892;&#20132;&#20114;&#30340;&#28508;&#21147;&#12290;&#35813;&#27169;&#22411;&#21487;&#29992;&#20110;&#25552;&#20379;&#22810;&#25991;&#29486;&#19978;&#19979;&#25991;&#19979;&#30340;&#35814;&#32454;&#31572;&#26696;&#65292;&#20026;&#22825;&#25991;&#23398;&#30028;&#25506;&#32034;&#24320;&#36767;&#20102;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2304.05406</link><description>&lt;p&gt;
&#26143;&#38469;&#38386;&#32842;&#65306;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#19982;&#22825;&#25991;&#23398;&#25991;&#29486;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature. (arXiv:2304.05406v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;OpenAI GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#19982;&#22825;&#25991;&#23398;&#25991;&#29486;&#36827;&#34892;&#20132;&#20114;&#30340;&#28508;&#21147;&#12290;&#35813;&#27169;&#22411;&#21487;&#29992;&#20110;&#25552;&#20379;&#22810;&#25991;&#29486;&#19978;&#19979;&#25991;&#19979;&#30340;&#35814;&#32454;&#31572;&#26696;&#65292;&#20026;&#22825;&#25991;&#23398;&#30028;&#25506;&#32034;&#24320;&#36767;&#20102;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;OpenAI GPT-4&#22823;&#35821;&#35328;&#27169;&#22411;&#19982;&#22825;&#25991;&#23398;&#35770;&#25991;&#36827;&#34892;&#26377;&#24847;&#20041;&#20114;&#21160;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#33976;&#39311;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#21407;&#22987;&#36755;&#20837;&#35770;&#25991;&#30340;50\&#65285;&#65292;&#21516;&#26102;&#20445;&#25345;&#27573;&#33853;&#32467;&#26500;&#21644;&#25972;&#20307;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#25991;&#26723;&#20869;&#23481;&#36827;&#34892;&#27169;&#22411;&#30340;&#21709;&#24212;&#65288;&#21313;&#20010;&#33976;&#39311;&#36807;&#30340;&#25991;&#29486;&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#22312;&#22810;&#25991;&#26723;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#20102;&#22312;&#30456;&#20851;&#30740;&#31350;&#21457;&#29616;&#26694;&#26550;&#20013;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#35814;&#32454;&#35299;&#31572;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22825;&#25991;&#23398;&#30028;&#30340;&#28508;&#21147;&#65292;&#20026;&#36827;&#19968;&#27493;&#25506;&#32034;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#65292;&#23588;&#20854;&#26159;&#21033;&#29992;&#27169;&#22411;&#36827;&#34892;&#20551;&#35774;&#29983;&#25104;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate the potential of the state-of-the-art OpenAI GPT-4 large language model to engage in meaningful interactions with Astronomy papers using in-context prompting. To optimize for efficiency, we employ a distillation technique that effectively reduces the size of the original input paper by 50\%, while maintaining the paragraph structure and overall semantic integrity. We then explore the model's responses using a multi-document context (ten distilled documents). Our findings indicate that GPT-4 excels in the multi-document domain, providing detailed answers contextualized within the framework of related research findings. Our results showcase the potential of large language models for the astronomical community, offering a promising avenue for further exploration, particularly the possibility of utilizing the models for hypothesis generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COTI&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#29702;&#35770;&#25351;&#23548;&#30340;&#25439;&#22833;&#30446;&#26631;&#21644;&#20840;&#38754;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#25991;&#26412;&#21453;&#36716;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2304.05265</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#21487;&#25511;&#25991;&#26412;&#21453;&#36716;
&lt;/p&gt;
&lt;p&gt;
Controllable Textual Inversion for Personalized Text-to-Image Generation. (arXiv:2304.05265v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COTI&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#29702;&#35770;&#25351;&#23548;&#30340;&#25439;&#22833;&#30446;&#26631;&#21644;&#20840;&#38754;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#25991;&#26412;&#21453;&#36716;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#22312;&#20197;&#25991;&#26412;&#20026;&#24341;&#23548;&#30340;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#24403;&#24341;&#23548;&#20449;&#24687;&#21253;&#21547;&#29992;&#25143;&#23450;&#20041;&#30340;&#12289;&#26410;&#35265;&#36807;&#30340;&#25110;&#38271;&#23614;&#27010;&#24565;&#26631;&#35760;&#26102;&#65292;&#25991;&#26412;&#21453;&#36716;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#29983;&#25104;&#25216;&#26415;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#23637;&#31034;&#20102;&#25991;&#26412;&#21453;&#36716;&#30340;&#37096;&#32626;&#20173;&#20805;&#28385;&#20102;&#8220;&#40657;&#39764;&#27861;&#8221;&#65292;&#20363;&#22914;&#39069;&#22806;&#25968;&#25454;&#38598;&#30340;&#20005;&#33499;&#35201;&#27714;&#65292;&#22312;&#24490;&#29615;&#20013;&#38656;&#35201;&#33392;&#33510;&#30340;&#20154;&#21147;&#25104;&#26412;&#21644;&#32570;&#20047;&#40065;&#26834;&#24615;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#25511;&#25991;&#26412;&#21453;&#36716;&#30340;&#22823;&#22823;&#22686;&#24378;&#29256;&#21453;&#36716;&#65292;&#35299;&#20915;&#20102;&#25152;&#26377;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#21453;&#36807;&#26469;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;COTI&#30340;&#26680;&#24515;&#26159;&#22522;&#20110;&#29702;&#35770;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#20855;&#26377;&#20840;&#38754;&#21644;&#26032;&#39062;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#30001;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#25152;&#25552;&#21462;&#12290;&#24191;&#27867;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;COTI&#30340;&#24615;&#33021;&#27604;&#20043;&#21069;&#25216;&#26415;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent large-scale generative modeling has attained unprecedented performance especially in producing high-fidelity images driven by text prompts. Text inversion (TI), alongside the text-to-image model backbones, is proposed as an effective technique in personalizing the generation when the prompts contain user-defined, unseen or long-tail concept tokens. Despite that, we find and show that the deployment of TI remains full of "dark-magics" -- to name a few, the harsh requirement of additional datasets, arduous human efforts in the loop and lack of robustness. In this work, we propose a much-enhanced version of TI, dubbed Controllable Textual Inversion (COTI), in resolving all the aforementioned problems and in turn delivering a robust, data-efficient and easy-to-use framework. The core to COTI is a theoretically-guided loss objective instantiated with a comprehensive and novel weighted scoring mechanism, encapsulated by an active-learning paradigm. The extensive results show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#21033;&#29992;GPT-3&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25552;&#21462;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#23398;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.02213</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38053;&#21273;&#65306;&#29992;GPT&#35299;&#23494;&#26448;&#26009;&#31185;&#23398;&#30340;&#31192;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#21033;&#29992;GPT-3&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25552;&#21462;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#23398;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#20197;&#35299;&#20915;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#20449;&#24687;&#25552;&#21462;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#38041;&#38043;&#30719;&#22826;&#38451;&#33021;&#30005;&#27744;FAIR&#25968;&#25454;&#38598;&#23545;GPT-3&#36827;&#34892;&#24494;&#35843;&#65292;&#33719;&#24471;&#20102;91.8 F1&#24471;&#20998;&#65292;&#24182;&#26356;&#26032;&#20102;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36804;&#20170;&#20026;&#27490;&#25152;&#26377;&#30456;&#20851;&#31185;&#23398;&#35770;&#25991;&#12290;&#25152;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#24050;&#34987;&#26684;&#24335;&#21270;&#21644;&#26631;&#20934;&#21270;&#65292;&#20351;&#24471;&#23427;&#21487;&#20197;&#30452;&#25509;&#20316;&#20026;&#21518;&#32493;&#25968;&#25454;&#20998;&#26512;&#30340;&#36755;&#20837;&#12290;&#36825;&#20010;&#29305;&#24615;&#23558;&#20351;&#26448;&#26009;&#31185;&#23398;&#23478;&#36890;&#36807;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#35780;&#35770;&#25991;&#31456;&#26469;&#24320;&#21457;&#20854;&#33258;&#24049;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#24182;&#33719;&#24471;&#20102;&#19982;DFT&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#36825;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20687;&#26448;&#26009;&#23398;&#23478;&#19968;&#26679;&#35780;&#21028;&#26448;&#26009;&#21644;&#35774;&#35745;&#26032;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a new NLP task called structured information inference (SIS) to address the complexities of information extraction at the device level in materials science. We accomplished this task by finetuning GPT-3 on a exsiting perovskite solar cell FAIR dataset with 91.8 F1-score and we updated the dataset with all related scientific papers up to now. The produced dataset is formatted and normalized, enabling its direct utilization as input in subsequent data analysis. This feature will enable materials scientists to develop their own models by selecting high-quality review papers within their domain. Furthermore, we designed experiments to predict PCE and reverse-predict parameters and obtained comparable performance with DFT, which demonstrates the potential of large language models to judge materials and design new materials like a materials scientist.
&lt;/p&gt;</description></item><item><title>HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01890</link><description>&lt;p&gt;
&#31038;&#20250;&#25991;&#21270;&#30693;&#35782;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#23545;&#36873;&#39033;&#30340;&#36873;&#25321;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01890
&lt;/p&gt;
&lt;p&gt;
HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;HATELEXICON&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#30340;&#34065;&#31216;&#21644;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#30340;&#35789;&#27719;&#34920;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#22914;&#20309;&#29992;&#20110;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#65292;&#34920;&#26126;&#21457;&#23637;&#29992;&#20110;&#20998;&#31867;&#26497;&#31471;&#35328;&#35770;&#30340;&#27169;&#22411;&#65292;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#20005;&#37325;&#20381;&#36182;&#30446;&#26631;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;HATELEXICON&#26469;&#36741;&#21161;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35757;&#32451;&#36873;&#39033;&#30340;&#26041;&#27861;&#65292;&#36873;&#39033;&#36873;&#25321;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;HASOC&#25968;&#25454;&#23545;&#24503;&#35821;&#21644;&#21360;&#22320;&#35821;&#36827;&#34892;&#20102;&#20960;&#20010;&#31034;&#33539;&#23398;&#20064;&#65292;&#24182;&#23558;Multilingual HateCheck&#65288;MHC&#65289;&#20316;&#20026;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26681;&#25454;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#36873;&#25321;&#26679;&#26412;&#65292;&#30456;&#23545;&#20110;&#38543;&#26426;&#37319;&#26679;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;MHC&#19978;&#34920;&#29616;&#12290;&#22240;&#27492;&#65292;&#24403;&#20165;&#26377;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#26102;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#26469;&#36873;&#25321;&#21253;&#21547;&#26356;&#22810;&#31038;&#20250;&#25991;&#21270;&#20449;&#24687;&#30340;&#26679;&#26412;&#33021;&#22815;&#26356;&#22909;&#22320;&#25552;&#39640;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot perf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#8212;&#8212;GPT-4&#22312;&#21307;&#23398;&#33021;&#21147;&#32771;&#35797;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#21161;&#20110;&#21307;&#23398;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.13375</link><description>&lt;p&gt;
GPT-4&#22312;&#21307;&#23398;&#25361;&#25112;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Capabilities of GPT-4 on Medical Challenge Problems. (arXiv:2303.13375v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#8212;&#8212;GPT-4&#22312;&#21307;&#23398;&#33021;&#21147;&#32771;&#35797;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#21161;&#20110;&#21307;&#23398;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#21253;&#25324;&#21307;&#23398;&#12290;&#25105;&#20204;&#23545;&#19968;&#39033;&#26368;&#20808;&#36827;&#30340;LLM&#8212;&#8212;GPT-4&#22312;&#21307;&#23398;&#33021;&#21147;&#32771;&#35797;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;GPT-4&#26159;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#27809;&#26377;&#32463;&#36807;&#38024;&#23545;&#21307;&#23398;&#38382;&#39064;&#30340;&#35757;&#32451;&#25110;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#32654;&#22269;&#20020;&#24202;&#33021;&#21147;&#35780;&#20272;&#21644;&#25480;&#26435;&#32771;&#26680;&#35745;&#21010;&#65288;USMLE&#65289;&#30340;&#20004;&#32452;&#23448;&#26041;&#32451;&#20064;&#26448;&#26009;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22312;MultiMedQA&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#38500;&#20102;&#27979;&#37327;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36824;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#27979;&#35797;&#38382;&#39064;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25506;&#32034;&#35757;&#32451;&#26399;&#38388;&#20869;&#23481;&#35760;&#24518;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#30740;&#31350;&#27010;&#29575;&#26657;&#20934;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;CommonsenseQA&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#22871;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#19978;&#65292;GPT-4&#30340;&#34920;&#29616;&#21450;&#20854;&#23545;&#24120;&#35782;&#30693;&#35782;&#30340;&#22788;&#29702;&#21644;&#25972;&#21512;&#36807;&#31243;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#25105;&#20204;&#20063;&#21457;&#29616;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11436</link><description>&lt;p&gt;
&#24515;&#28789;&#19982;&#26426;&#22120;: &#35299;&#24320;GPT-4&#30340;&#35748;&#30693;&#24515;&#29702;&#23398;&#20043;&#35868;
&lt;/p&gt;
&lt;p&gt;
Mind meets machine: Unravelling GPT-4's cognitive psychology. (arXiv:2303.11436v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;CommonsenseQA&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#22871;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#19978;&#65292;GPT-4&#30340;&#34920;&#29616;&#21450;&#20854;&#23545;&#24120;&#35782;&#30693;&#35782;&#30340;&#22788;&#29702;&#21644;&#25972;&#21512;&#36807;&#31243;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#25105;&#20204;&#20063;&#21457;&#29616;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#26412;&#25104;&#20998;&#65292;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#29615;&#22659;&#35266;&#23519;&#25512;&#26029;&#32467;&#35770;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#25104;&#20026;&#36234;&#26469;&#36234;&#33021;&#22815;&#25191;&#34892;&#20154;&#31867;&#32423;&#20219;&#21153;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#12290;&#26368;&#36817;&#24320;&#21457;&#30340;GPT-4&#21450;&#20854;&#22312;&#21307;&#23398;&#32771;&#35797;&#12289;&#24459;&#24072;&#32771;&#35797;&#31561;&#20154;&#31867;&#38590;&#20197;&#23436;&#25104;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#30340;&#25104;&#21151;&#65292;&#22686;&#21152;&#20102;LLMs&#25104;&#20026;&#23436;&#32654;&#26234;&#33021;&#24037;&#20855;&#30340;&#20449;&#24515;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;GPT-4&#35770;&#25991;&#21521;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#26576;&#20123;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#20294;&#23545;GPT-4&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#29305;&#21035;&#26159;&#29616;&#26377;&#30340;&#24050;&#32463;&#30830;&#31435;&#22909;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#36824;&#26159;&#32570;&#22833;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20851;&#27880;GPT-4&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;CommonsenseQA&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#22871;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#35780;&#20272;&#21450;&#20854;&#35748;&#30693;&#24515;&#29702;&#23398;&#24037;&#20855;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#33021;&#22815;&#29702;&#35299;GPT-4&#22914;&#20309;&#22312;&#20854;&#35821;&#35328;&#29983;&#25104;&#36807;&#31243;&#20013;&#22788;&#29702;&#21644;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#65292;&#20197;&#21450;&#20854;&#22312;&#36825;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense reasoning is a basic ingredient of intelligence in humans, empowering the ability to deduce conclusions based on the observations of surroundings. Large language models (LLMs) are emerging as potent tools increasingly capable of performing human-level tasks. The recent development in the form of GPT-4 and its demonstrated success in tasks complex to humans such as medical exam, bar exam and others has led to an increased confidence in the LLMs to become perfect instruments of intelligence. Though, the GPT-4 paper has shown performance on some common sense reasoning tasks, a comprehensive assessment of GPT-4 on common sense reasoning tasks, particularly on the existing well-established datasets is missing. In this study, we focus on the evaluation of GPT-4's performance on a set of common sense reasoning questions from the widely used CommonsenseQA dataset along with tools from cognitive psychology. In doing so, we understand how GPT-4 processes and integrates common sense k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#30340;&#21407;&#22411;&#35821;&#20041;&#35299;&#32806;&#26041;&#27861;(PSDC)&#29992;&#20110;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#36890;&#36807;&#20004;&#31181;&#23631;&#34109;&#31574;&#30053;&#35299;&#32806;&#31867;&#29305;&#23450;&#30340;&#21407;&#22411;&#21644;&#35821;&#22659;&#35821;&#20041;&#21407;&#22411;&#65292;&#38450;&#27490;&#35821;&#20041;&#22349;&#22604;&#65292;&#23454;&#39564;&#35777;&#26126;PSDC&#21487;&#20197;&#25345;&#32493;&#20248;&#20110;&#20197;&#21069;&#30340;SOTA&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.13610</link><description>&lt;p&gt;
&#19968;&#31181;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#30340;&#21407;&#22411;&#35821;&#20041;&#35299;&#32806;&#26041;&#27861;&#29992;&#20110;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
A Prototypical Semantic Decoupling Method via Joint Contrastive Learning for Few-Shot Name Entity Recognition. (arXiv:2302.13610v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#30340;&#21407;&#22411;&#35821;&#20041;&#35299;&#32806;&#26041;&#27861;(PSDC)&#29992;&#20110;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#36890;&#36807;&#20004;&#31181;&#23631;&#34109;&#31574;&#30053;&#35299;&#32806;&#31867;&#29305;&#23450;&#30340;&#21407;&#22411;&#21644;&#35821;&#22659;&#35821;&#20041;&#21407;&#22411;&#65292;&#38450;&#27490;&#35821;&#20041;&#22349;&#22604;&#65292;&#23454;&#39564;&#35777;&#26126;PSDC&#21487;&#20197;&#25345;&#32493;&#20248;&#20110;&#20197;&#21069;&#30340;SOTA&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26088;&#22312;&#22522;&#20110;&#26497;&#23569;&#37327;&#26631;&#27880;&#23454;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#35782;&#21035;&#20986;&#21629;&#21517;&#23454;&#20307;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#20542;&#21521;&#20110;&#35760;&#24518;&#23454;&#20307;&#25552;&#21450;&#65292;&#20294;&#36825;&#23481;&#26131;&#34987;&#30456;&#20284;&#30340;&#21407;&#22411;&#25152;&#28151;&#28102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#30340;&#21407;&#22411;&#35821;&#20041;&#35299;&#32806;&#26041;&#27861;(PSDC)&#29992;&#20110;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#23631;&#34109;&#31574;&#30053;&#26469;&#35299;&#32806;&#31867;&#29305;&#23450;&#30340;&#21407;&#22411;&#21644;&#35821;&#22659;&#35821;&#20041;&#21407;&#22411;&#65292;&#20197;&#24341;&#23548;&#27169;&#22411;&#19987;&#27880;&#20110;&#25512;&#26029;&#30340;&#20004;&#31181;&#19981;&#21516;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#25972;&#21512;&#20004;&#31181;&#35299;&#32806;&#20449;&#24687;&#65292;&#24182;&#38450;&#27490;&#35821;&#20041;&#22349;&#22604;&#12290;&#22312;&#20004;&#20010;&#23569;&#26679;&#26412;NER&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PSDC&#22312;&#25972;&#20307;&#24615;&#33021;&#26041;&#38754;&#19968;&#33268;&#20248;&#20110;&#20197;&#21069;&#30340;SOTA&#26041;&#27861;&#12290;&#24191;&#27867;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;PSDC&#30340;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot named entity recognition (NER) aims at identifying named entities based on only few labeled instances. Most existing prototype-based sequence labeling models tend to memorize entity mentions which would be easily confused by close prototypes. In this paper, we proposed a Prototypical Semantic Decoupling method via joint Contrastive learning (PSDC) for few-shot NER. Specifically, we decouple class-specific prototypes and contextual semantic prototypes by two masking strategies to lead the model to focus on two different semantic information for inference. Besides, we further introduce joint contrastive learning objectives to better integrate two kinds of decoupling information and prevent semantic collapse. Experimental results on two few-shot NER benchmarks demonstrate that PSDC consistently outperforms the previous SOTA methods in terms of overall performance. Extensive analysis further validates the effectiveness and generalization of PSDC.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#31181;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;EVJVQA&#65292;&#21253;&#25324;&#36234;&#21335;&#35821;&#65292;&#33521;&#35821;&#21644;&#26085;&#35821;&#30340;33,000+&#38382;&#31572;&#23545;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#22810;&#35821;&#35328;VQA&#31995;&#32479;&#25110;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.11752</link><description>&lt;p&gt;
VLSP2022-EVJVQA&#25361;&#25112;&#65306;&#22810;&#35821;&#31181;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
VLSP2022-EVJVQA Challenge: Multilingual Visual Question Answering. (arXiv:2302.11752v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11752
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#31181;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;EVJVQA&#65292;&#21253;&#25324;&#36234;&#21335;&#35821;&#65292;&#33521;&#35821;&#21644;&#26085;&#35821;&#30340;33,000+&#38382;&#31572;&#23545;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#22810;&#35821;&#35328;VQA&#31995;&#32479;&#25110;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21560;&#24341;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#37325;&#35270;&#12290; &#33521;&#35821;&#26159;&#19968;&#20010;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#65292;&#22312;&#35270;&#35273;&#38382;&#31572;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26041;&#38754;&#26377;&#30528;&#21508;&#31181;&#21457;&#23637;&#12290; &#20854;&#20182;&#35821;&#35328;&#30340;&#35270;&#35273;&#38382;&#31572;&#20063;&#23558;&#20250;&#26377;&#36164;&#28304;&#21644;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290; &#27492;&#22806;&#65292;&#36824;&#27809;&#26377;&#38024;&#23545;&#29305;&#23450;&#22269;&#23478;&#30340;&#35270;&#35273;&#20869;&#23481;&#21644;&#25991;&#21270;&#29305;&#28857;&#25552;&#20379;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;EVJVQA&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22312;&#36234;&#21335;&#25293;&#25668;&#30340;&#32422;5,000&#24352;&#22270;&#29255;&#19978;&#30340;&#19977;&#31181;&#35821;&#35328;&#65288;&#36234;&#21335;&#35821;&#65292;&#33521;&#35821;&#21644;&#26085;&#35821;&#65289;&#30340;33,000&#22810;&#23545;&#38382;&#31572;&#23545;&#65292;&#20197;&#35780;&#20272;&#22810;&#35821;&#35328;VQA&#31995;&#32479;&#25110;&#27169;&#22411;&#12290; EVJVQA&#20316;&#20026;&#25361;&#25112;&#22810;&#35821;&#31181;&#35270;&#35273;&#38382;&#31572;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#22312;&#31532;9&#23626;&#36234;&#21335;&#35821;&#35328;&#21644;&#35821;&#38899;&#22788;&#29702;&#30740;&#35752;&#20250;&#65288;VLSP2022&#65289;&#19978;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering (VQA) is a challenging task of natural language processing (NLP) and computer vision (CV), attracting significant attention from researchers. English is a resource-rich language that has witnessed various developments in datasets and models for visual question answering. Visual question answering in other languages also would be developed for resources and models. In addition, there is no multilingual dataset targeting the visual content of a particular country with its own objects and cultural characteristics. To address the weakness, we provide the research community with a benchmark dataset named EVJVQA, including 33,000+ pairs of question-answer over three languages: Vietnamese, English, and Japanese, on approximately 5,000 images taken from Vietnam for evaluating multilingual VQA systems or models. EVJVQA is used as a benchmark dataset for the challenge of multilingual visual question answering at the 9th Workshop on Vietnamese Language and Speech Process
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#36136;&#37327;&#39184;&#21381;&#35780;&#35770;&#29983;&#25104;&#34394;&#20551;&#35780;&#35770;&#24182;&#24494;&#35843;GPT&#36755;&#20986;&#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39044;&#27979;&#34394;&#20551;&#35780;&#35770;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#39044;&#27979;&#38750;&#31934;&#33521;&#35780;&#35770;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20010;&#32500;&#24230;&#19978;&#23545;&#36825;&#20123;&#35780;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#27492;&#31867;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#26159;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#38754;&#20020;&#30340;&#25345;&#32493;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.07731</link><description>&lt;p&gt;
AI&#23545;&#25239;AI&#65306;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#25171;&#20987;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#39184;&#21381;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Combat AI With AI: Counteract Machine-Generated Fake Restaurant Reviews on Social Media. (arXiv:2302.07731v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#36136;&#37327;&#39184;&#21381;&#35780;&#35770;&#29983;&#25104;&#34394;&#20551;&#35780;&#35770;&#24182;&#24494;&#35843;GPT&#36755;&#20986;&#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39044;&#27979;&#34394;&#20551;&#35780;&#35770;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#39044;&#27979;&#38750;&#31934;&#33521;&#35780;&#35770;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20010;&#32500;&#24230;&#19978;&#23545;&#36825;&#20123;&#35780;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#27492;&#31867;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#26159;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#38754;&#20020;&#30340;&#25345;&#32493;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;GPT&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#21046;&#36896;&#20986;&#38590;&#20197;&#21306;&#20998;&#30340;&#34394;&#20551;&#39038;&#23458;&#35780;&#35770;&#65292;&#20174;&#32780;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26816;&#27979;&#36825;&#20123;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#36896;&#25104;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;Yelp&#39564;&#35777;&#30340;&#39640;&#36136;&#37327;&#30340;&#31934;&#33521;&#39184;&#21381;&#35780;&#35770;&#26469;&#29983;&#25104;OpenAI GPT&#35780;&#35770;&#29983;&#25104;&#22120;&#30340;&#34394;&#20551;&#35780;&#35770;&#65292;&#24182;&#26368;&#32456;&#24494;&#35843;GPT&#36755;&#20986;&#26816;&#27979;&#22120;&#26469;&#39044;&#27979;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#34394;&#20551;&#35780;&#35770;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#27169;&#22411;&#24212;&#29992;&#20110;&#39044;&#27979;&#38750;&#31934;&#33521;&#35780;&#35770;&#65292;&#24182;&#22312;&#20960;&#20010;&#32500;&#24230;&#65288;&#22914;&#35780;&#35770;&#12289;&#29992;&#25143;&#21644;&#39184;&#21381;&#29305;&#24449;&#20197;&#21450;&#20889;&#20316;&#39118;&#26684;&#65289;&#19978;&#35782;&#21035;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#27491;&#22312;&#19981;&#26029;&#38754;&#20020;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#30340;&#25361;&#25112;&#65292;&#23613;&#31649;&#20182;&#20204;&#21487;&#33021;&#23454;&#26045;&#26816;&#27979;&#31995;&#32479;&#20197;&#36807;&#28388;&#20986;&#21487;&#30097;&#30340;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative models such as GPT may be used to fabricate indistinguishable fake customer reviews at a much lower cost, thus posing challenges for social media platforms to detect these machine-generated fake reviews. We propose to leverage the high-quality elite restaurant reviews verified by Yelp to generate fake reviews from the OpenAI GPT review creator and ultimately fine-tune a GPT output detector to predict fake reviews that significantly outperform existing solutions. We further apply the model to predict non-elite reviews and identify the patterns across several dimensions, such as review, user and restaurant characteristics, and writing style. We show that social media platforms are continuously challenged by machine-generated fake reviews, although they may implement detection systems to filter out suspicious reviews.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31995;&#21015;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#35821;&#26009;&#24211;&#26469;&#36880;&#27493;&#36866;&#24212;&#39046;&#22495;&#20197;&#25552;&#39640;LM&#22312;&#39046;&#22495;&#20869;&#30340;&#32456;&#31471;&#20219;&#21153;&#34920;&#29616;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#36719;&#25513;&#34109;&#26426;&#21046;&#26469;&#30452;&#25509;&#25511;&#21046;LM&#30340;&#26356;&#26032;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20195;&#29702;&#26469;&#20445;&#30041;LM&#20013;&#30340;&#25972;&#20307;&#30693;&#35782;&#65292;&#21516;&#26102;&#23545;&#27604;&#24050;&#23398;&#20064;&#39046;&#22495;&#30693;&#35782;&#21644;&#24403;&#21069;&#20840;&#32593;&#32476;&#30340;&#30693;&#35782;&#26469;&#23454;&#29616;&#30693;&#35782;&#25972;&#21512;&#12290;</title><link>http://arxiv.org/abs/2302.03241</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Continual Pre-training of Language Models. (arXiv:2302.03241v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31995;&#21015;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#35821;&#26009;&#24211;&#26469;&#36880;&#27493;&#36866;&#24212;&#39046;&#22495;&#20197;&#25552;&#39640;LM&#22312;&#39046;&#22495;&#20869;&#30340;&#32456;&#31471;&#20219;&#21153;&#34920;&#29616;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#36719;&#25513;&#34109;&#26426;&#21046;&#26469;&#30452;&#25509;&#25511;&#21046;LM&#30340;&#26356;&#26032;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20195;&#29702;&#26469;&#20445;&#30041;LM&#20013;&#30340;&#25972;&#20307;&#30693;&#35782;&#65292;&#21516;&#26102;&#23545;&#27604;&#24050;&#23398;&#20064;&#39046;&#22495;&#30693;&#35782;&#21644;&#24403;&#21069;&#20840;&#32593;&#32476;&#30340;&#30693;&#35782;&#26469;&#23454;&#29616;&#30693;&#35782;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24555;&#36895;&#21457;&#23637;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#30740;&#31350;LMs&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#29305;&#21035;&#26159;&#25345;&#32493;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;&#25110;&#25345;&#32493;DAP&#35757;&#32451;&#65289;&#12290;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#39046;&#22495;&#35821;&#26009;&#24211;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;LMs&#20197;&#20351;&#20854;&#36866;&#24212;&#20110;&#39046;&#22495;&#65292;&#21487;&#20197;&#25552;&#39640;&#39046;&#22495;&#20869;&#30340;&#26368;&#32456;&#20219;&#21153;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31995;&#21015;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#35821;&#26009;&#24211;&#26469;&#25345;&#32493;DAP&#35757;&#32451;LMs&#65292;&#20197;&#20351;&#20854;&#36866;&#24212;&#20110;&#36825;&#20123;&#39046;&#22495;&#65292;&#20174;&#32780;&#25552;&#39640;&#23427;&#20204;&#30340;&#32456;&#31471;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#19968;&#31181;&#36719;&#25513;&#34109;&#26426;&#21046;&#65292;&#21487;&#30452;&#25509;&#25511;&#21046;LMs&#30340;&#26356;&#26032;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20195;&#29702;&#26469;&#20445;&#30041;&#21407;&#22987;LMs&#20013;&#30340;&#26222;&#36890;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#23427;&#23545;&#20808;&#21069;&#23398;&#20064;&#30340;&#39046;&#22495;&#30693;&#35782;&#65288;&#21253;&#25324;&#39044;&#20808;&#35757;&#32451;&#30340;LMs&#20013;&#30340;&#26222;&#36890;&#30693;&#35782;&#65289;&#21644;&#26469;&#33258;&#24403;&#21069;&#20840;&#32593;&#32476;&#30340;&#30693;&#35782;&#36827;&#34892;&#23545;&#27604;&#65292;&#20197;&#23454;&#29616;&#30693;&#35782;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have been instrumental for the rapid advance of natural language processing. This paper studies continual pre-training of LMs, in particular, continual domain-adaptive pre-training (or continual DAP-training). Existing research has shown that further pre-training an LM using a domain corpus to adapt the LM to the domain can improve the end-task performance in the domain. This paper proposes a novel method to continually DAP-train an LM with a sequence of unlabeled domain corpora to adapt the LM to these domains to improve their end-task performances. The key novelty of our method is a soft-masking mechanism that directly controls the update to the LM. A novel proxy is also proposed to preserve the general knowledge in the original LM. Additionally, it contrasts the representations of the previously learned domain knowledge (including the general knowledge in the pre-trained LM) and the knowledge from the current full network to achieve knowledge integration. The m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25277;&#21462;&#24335;&#38382;&#39064;&#22238;&#31572;&#30340;&#21160;&#37327;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#22635;&#31354;&#24335;&#21644;&#33258;&#28982;&#26597;&#35810;-&#25991;&#31456;&#26679;&#26412;&#23545;&#30340;&#31572;&#26696;&#27010;&#29575;&#65292;&#33021;&#26356;&#22909;&#22320;&#23558;&#22312;&#22635;&#31354;&#24335;&#26679;&#26412;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#22238;&#31572;&#33258;&#28982;&#38382;&#39064;&#19978;&#12290;</title><link>http://arxiv.org/abs/2212.05762</link><description>&lt;p&gt;
&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#30340;&#21160;&#37327;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Momentum Contrastive Pre-training for Question Answering. (arXiv:2212.05762v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05762
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25277;&#21462;&#24335;&#38382;&#39064;&#22238;&#31572;&#30340;&#21160;&#37327;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#22635;&#31354;&#24335;&#21644;&#33258;&#28982;&#26597;&#35810;-&#25991;&#31456;&#26679;&#26412;&#23545;&#30340;&#31572;&#26696;&#27010;&#29575;&#65292;&#33021;&#26356;&#22909;&#22320;&#23558;&#22312;&#22635;&#31354;&#24335;&#26679;&#26412;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#22238;&#31572;&#33258;&#28982;&#38382;&#39064;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25277;&#21462;&#24335;&#38382;&#31572;&#65288;QA&#65289;&#39044;&#35757;&#32451;&#26041;&#27861;&#29983;&#25104;&#31867;&#20284;&#20110;&#22635;&#31354;&#39064;&#30340;&#26597;&#35810;&#65292;&#20854;&#35821;&#27861;&#32467;&#26500;&#19982;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#19981;&#21516;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#31616;&#21333;&#30340;&#20851;&#38190;&#35789;&#21305;&#37197;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#37327;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;MCROSS&#65288;Momentum Contrastive pRe-training fOr queStion anSwering&#65289;&#29992;&#20110;&#25277;&#21462;&#24335;&#30340;&#38382;&#39064;&#22238;&#31572;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MCROSS&#24341;&#20837;&#20102;&#21160;&#37327;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#21305;&#37197;&#22635;&#31354;&#24335;&#21644;&#33258;&#28982;&#26597;&#35810;-&#25991;&#31456;&#26679;&#26412;&#23545;&#20043;&#38388;&#30340;&#31572;&#26696;&#27010;&#29575;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#23558;&#22312;&#22635;&#31354;&#24335;&#26679;&#26412;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#22238;&#31572;&#33258;&#28982;&#38382;&#39064;&#19978;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;QA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#30417;&#30563;&#21644;&#38646;-shot&#22330;&#26223;&#19979;&#22343;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing pre-training methods for extractive Question Answering (QA) generate cloze-like queries different from natural questions in syntax structure, which could overfit pre-trained models to simple keyword matching. In order to address this problem, we propose a novel Momentum Contrastive pRe-training fOr queStion anSwering (MCROSS) method for extractive QA. Specifically, MCROSS introduces a momentum contrastive learning framework to align the answer probability between cloze-like and natural query-passage sample pairs. Hence, the pre-trained models can better transfer the knowledge learned in cloze-like samples to answering natural questions. Experimental results on three benchmarking QA datasets show that our method achieves noticeable improvement compared with all baselines in both supervised and zero-shot scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#20154;&#24615;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#38750;&#35821;&#35328;&#22270;&#28789;&#27979;&#35797;&#65292;&#21457;&#29616;&#24403;&#21069;AI&#39550;&#39542;&#21592;&#19981;&#33021;&#21019;&#36896;&#31867;&#20284;&#20154;&#31867;&#30340;&#39550;&#20056;&#20307;&#39564;&#65292;&#38656;&#35201;&#22312;&#24773;&#24863;&#36807;&#28193;&#27169;&#22411;&#20013;&#36827;&#34892;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2212.02908</link><description>&lt;p&gt;
&#36808;&#21521;&#20154;&#31867;&#20860;&#23481;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65306;&#24773;&#24863;&#36807;&#28193;&#27169;&#22411;&#20013;&#30340;&#38750;&#35821;&#35328;&#22270;&#28789;&#27979;&#35797;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards human-compatible autonomous car: A study of non-verbal Turing test in automated driving with affective transition modelling. (arXiv:2212.02908v5 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#20154;&#24615;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#38750;&#35821;&#35328;&#22270;&#28789;&#27979;&#35797;&#65292;&#21457;&#29616;&#24403;&#21069;AI&#39550;&#39542;&#21592;&#19981;&#33021;&#21019;&#36896;&#31867;&#20284;&#20154;&#31867;&#30340;&#39550;&#20056;&#20307;&#39564;&#65292;&#38656;&#35201;&#22312;&#24773;&#24863;&#36807;&#28193;&#27169;&#22411;&#20013;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#31867;&#36208;&#21521;&#26080;&#38656;&#21452;&#25163;&#30340;&#29983;&#27963;&#26041;&#24335;&#26102;&#65292;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#29616;&#26377;&#25991;&#29486;&#24378;&#35843;&#65292;&#22914;&#26524;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#33021;&#22815;&#20197;&#31867;&#20284;&#20154;&#31867;&#30340;&#26041;&#24335;&#39550;&#39542;&#65292;&#20154;&#20204;&#20250;&#26356;&#23481;&#26131;&#25509;&#21463;&#23427;&#12290;&#28982;&#32780;&#65292;&#20165;&#26377;&#23569;&#37327;&#30740;&#31350;&#20174;&#20056;&#23458;&#35282;&#24230;&#30340;&#33258;&#28982;&#20307;&#39564;&#26469;&#26816;&#39564;&#30446;&#21069;&#30340;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#30340;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#39033;&#30495;&#23454;&#36947;&#36335;&#29615;&#22659;&#19979;&#30340;&#35797;&#39564;&#65292;&#27979;&#35797;&#20102;69&#20301;&#21442;&#19982;&#32773;&#30340;&#21453;&#39304;&#65292;&#23581;&#35797;&#20102;&#35299;AI&#39550;&#39542;&#20154;&#21592;&#33021;&#21542;&#20026;&#20056;&#23458;&#21019;&#36896;&#31867;&#20284;&#20154;&#31867;&#30340;&#39550;&#20056;&#20307;&#39564;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#39550;&#20056;&#20307;&#39564;&#30340;&#38750;&#35821;&#35328;&#22270;&#28789;&#27979;&#35797;&#65292;&#35201;&#27714;&#21442;&#19982;&#32773;&#20316;&#20026;&#20056;&#23458;&#20056;&#22352;AI&#39550;&#39542;&#20154;&#21592;&#25110;&#20154;&#31867;&#39550;&#39542;&#20154;&#21592;&#39550;&#39542;&#30340;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65292;&#24182;&#21028;&#26029;&#21496;&#26426;&#26159;&#20154;&#31867;&#36824;&#26159;AI&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20805;&#24403;AI&#39550;&#39542;&#21592;&#30340;&#27773;&#36710;&#26410;&#33021;&#36890;&#36807;&#25105;&#20204;&#30340;&#27979;&#35797;&#65292;&#22240;&#20026;&#20056;&#23458;&#33021;&#22815;&#36229;&#36807;&#38543;&#26426;&#29468;&#27979;&#22320;&#35782;&#21035;&#20986;AI&#39550;&#39542;&#21592;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20154;&#31867;&#39550;&#39542;&#21592;&#39550;&#39542;&#36710;&#36742;&#26102;&#65292;&#20056;&#23458;&#30340;&#21028;&#26029;&#32467;&#26524;&#32422;&#22312;&#38543;&#26426;&#29468;&#27979;&#27700;&#24179;&#38468;&#36817;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#20154;&#31867;&#20056;&#23458;&#22312;&#39550;&#39542;&#36807;&#31243;&#20013;&#32473;&#20104;&#20102;&#21738;&#20123;&#20154;&#24615;&#21270;&#29305;&#24449;&#30340;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous cars are indispensable when humans go further down the hands-free route. Although existing literature highlights that the acceptance of the autonomous car will increase if it drives in a human-like manner, sparse research offers the naturalistic experience from a passenger's seat perspective to examine the human likeness of current autonomous cars. The present study tested whether the AI driver could create a human-like ride experience for passengers based on 69 participants' feedback in a real-road scenario. We designed a ride experience-based version of the non-verbal Turing test for automated driving. Participants rode in autonomous cars (driven by either human or AI drivers) as a passenger and judged whether the driver was human or AI. The AI driver failed to pass our test because passengers detected the AI driver above chance. In contrast, when the human driver drove the car, the passengers' judgement was around chance. We further investigated how human passengers ascri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#35821;&#20041;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#35780;&#20272;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;&#27604;&#36739;&#19981;&#21516;PLM&#30340;&#21487;&#38752;&#24615;&#65292;&#20445;&#35777;&#20854;&#22312;&#30456;&#21516;&#25110;&#30456;&#20284;&#30340;&#25552;&#31034;&#19979;&#29983;&#25104;&#30340;&#36755;&#20986;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2211.05853</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#19968;&#33268;&#24615;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring Reliability of Large Language Models through Semantic Consistency. (arXiv:2211.05853v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#35821;&#20041;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#35780;&#20272;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;&#27604;&#36739;&#19981;&#21516;PLM&#30340;&#21487;&#38752;&#24615;&#65292;&#20445;&#35777;&#20854;&#22312;&#30456;&#21516;&#25110;&#30456;&#20284;&#30340;&#25552;&#31034;&#19979;&#29983;&#25104;&#30340;&#36755;&#20986;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26497;&#39640;&#30340;&#27969;&#30021;&#24615;&#21644;&#24615;&#33021;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#34920;&#29616;&#33391;&#22909;&#30340;PLMs&#38750;&#24120;&#25935;&#24863;&#65292;&#23545;&#20110;&#36755;&#20837;&#30340;&#25552;&#31034;&#38750;&#24120;&#25935;&#24863;&#12290;&#21363;&#20351;&#25552;&#31034;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#21516;&#30340;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#33021;&#32473;&#20986;&#38750;&#24120;&#19981;&#21516;&#30340;&#31572;&#26696;&#12290;&#24403;&#32771;&#34385;PLMs&#30340;&#23433;&#20840;&#21644;&#21487;&#20449;&#36182;&#37096;&#32626;&#26102;&#65292;&#25105;&#20204;&#24076;&#26395;&#23427;&#20204;&#22312;&#24847;&#24605;&#30456;&#21516;&#25110;&#34920;&#36798;&#30456;&#21516;&#24847;&#22270;&#30340;&#25552;&#31034;&#19979;&#30340;&#36755;&#20986;&#26159;&#19968;&#33268;&#30340;&#12290;&#34429;&#28982;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#26368;&#20808;&#36827;&#30340;PLMs&#22914;&#20309;&#35299;&#20915;&#36825;&#20010;&#38656;&#27714;&#65292;&#20294;&#23427;&#20204;&#20165;&#38480;&#20110;&#35780;&#20272;&#21333;&#20010;&#25110;&#22810;&#20010;&#21333;&#35789;&#31572;&#26696;&#30340;&#35789;&#27719;&#31561;&#20215;&#24615;&#65292;&#32780;&#19981;&#28041;&#21450;&#29983;&#25104;&#25991;&#26412;&#24207;&#21015;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#29702;&#35299;&#22312;&#29983;&#25104;&#25991;&#26412;&#35774;&#32622;&#19979;PLMs&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35821;&#20041;&#19968;&#33268;&#24615;&#24230;&#37327;&#65292;&#20801;&#35768;&#27604;&#36739;&#24320;&#25918;&#24335;&#25991;&#26412;&#36755;&#20986;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#20960;&#20010;&#29256;&#26412;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#35780;&#20272;&#22810;&#20010;PLM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large pretrained language models (PLMs) demonstrate incredible fluency and performance on many natural language tasks, recent work has shown that well-performing PLMs are very sensitive to what prompts are feed into them. Even when prompts are semantically identical, language models may give very different answers. When considering safe and trustworthy deployments of PLMs we would like their outputs to be consistent under prompts that mean the same thing or convey the same intent. While some work has looked into how state-of-the-art PLMs address this need, they have been limited to only evaluating lexical equality of single- or multi-word answers and do not address consistency of generative text sequences. In order to understand consistency of PLMs under text generation settings, we develop a measure of semantic consistency that allows the comparison of open-ended text outputs. We implement several versions of this consistency metric to evaluate the performance of a number of PLM
&lt;/p&gt;</description></item><item><title>&#38754;&#20855;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#24615;&#21035;&#21270;&#24515;&#29702;&#20581;&#24247;&#27495;&#35270;&#65292;&#25429;&#25417;&#21040;&#20102;&#20851;&#20110;&#24615;&#21035;&#22312;&#24515;&#29702;&#20581;&#24247;&#26041;&#38754;&#30340;&#31038;&#20250;&#27495;&#35270;&#65292;&#38656;&#35201;&#32771;&#34385;&#24615;&#21035;&#22240;&#32032;&#26469;&#36991;&#20813;&#27495;&#35270;&#12290;</title><link>http://arxiv.org/abs/2210.15144</link><description>&lt;p&gt;
&#38754;&#20855;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#21270;&#24515;&#29702;&#20581;&#24247;&#27495;&#35270;
&lt;/p&gt;
&lt;p&gt;
Gendered Mental Health Stigma in Masked Language Models. (arXiv:2210.15144v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15144
&lt;/p&gt;
&lt;p&gt;
&#38754;&#20855;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#24615;&#21035;&#21270;&#24515;&#29702;&#20581;&#24247;&#27495;&#35270;&#65292;&#25429;&#25417;&#21040;&#20102;&#20851;&#20110;&#24615;&#21035;&#22312;&#24515;&#29702;&#20581;&#24247;&#26041;&#38754;&#30340;&#31038;&#20250;&#27495;&#35270;&#65292;&#38656;&#35201;&#32771;&#34385;&#24615;&#21035;&#22240;&#32032;&#26469;&#36991;&#20813;&#27495;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#20581;&#24247;&#27495;&#35270;&#38459;&#30861;&#20102;&#35768;&#22810;&#20154;&#33719;&#24471;&#36866;&#24403;&#30340;&#27835;&#30103;&#65292;&#31038;&#20250;&#24515;&#29702;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;&#24515;&#29702;&#20581;&#24247;&#20542;&#21521;&#20110;&#34987;&#30007;&#24615;&#24573;&#35270;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38754;&#20855;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#21270;&#24515;&#29702;&#20581;&#24247;&#27495;&#35270;&#12290;&#25105;&#20204;&#36816;&#29992;&#20020;&#24202;&#24515;&#29702;&#23398;&#25991;&#29486;&#26469;&#24314;&#31435;&#26694;&#26550;&#26469;&#23454;&#29616;&#24515;&#29702;&#20581;&#24247;&#27495;&#35270;&#30340;&#25805;&#20316;&#24615;&#65292;&#20351;&#29992;&#24515;&#29702;&#23398;&#30740;&#31350;&#20013;&#30340;&#25552;&#31034;&#26469;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#24615;&#21035;&#21270;&#35789;&#27719;&#30340;&#20542;&#21521;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#38754;&#20855;&#35821;&#35328;&#27169;&#22411;&#25429;&#25417;&#21040;&#20102;&#20851;&#20110;&#24615;&#21035;&#22312;&#24515;&#29702;&#20581;&#24247;&#26041;&#38754;&#30340;&#31038;&#20250;&#27495;&#35270;&#65306;&#22312;&#26377;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#21477;&#23376;&#20013;&#65292;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#39044;&#27979;&#22899;&#24615;&#20027;&#35821;&#32780;&#38750;&#30007;&#24615;&#65288;32&#65285;&#27604;19&#65285;&#65289;&#65292;&#36825;&#31181;&#24046;&#24322;&#22312;&#34920;&#26126;&#23547;&#27714;&#27835;&#30103;&#34892;&#20026;&#30340;&#21477;&#23376;&#20013;&#26356;&#20026;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;&#27169;&#22411;&#22312;&#30007;&#22899;&#30340;&#27495;&#35270;&#32500;&#24230;&#19978;&#34920;&#29616;&#19981;&#21516;&#65292;&#23558;&#22914;&#24868;&#24594;&#12289;&#36131;&#24618;&#21644;&#24604;&#24751;&#31561;&#21051;&#26495;&#21360;&#35937;&#19982;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#22899;&#24615;&#32852;&#31995;&#24471;&#26356;&#32039;&#23494;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#38656;&#35201;&#32771;&#34385;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#21035;&#21270;&#24515;&#29702;&#20581;&#24247;&#27495;&#35270;&#65292;&#24182;&#25958;&#20419;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#27880;&#24847;&#20182;&#20204;&#30340;&#27169;&#22411;&#21487;&#33021;&#23545;&#32500;&#25345;&#26377;&#23475;&#21051;&#26495;&#21360;&#35937;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mental health stigma prevents many individuals from receiving the appropriate care, and social psychology studies have shown that mental health tends to be overlooked in men. In this work, we investigate gendered mental health stigma in masked language models. In doing so, we operationalize mental health stigma by developing a framework grounded in psychology research: we use clinical psychology literature to curate prompts, then evaluate the models' propensity to generate gendered words. We find that masked language models capture societal stigma about gender in mental health: models are consistently more likely to predict female subjects than male in sentences about having a mental health condition (32% vs. 19%), and this disparity is exacerbated for sentences that indicate treatment-seeking behavior. Furthermore, we find that different models capture dimensions of stigma differently for men and women, associating stereotypes like anger, blame, and pity more with women with mental he
&lt;/p&gt;</description></item><item><title>LittleBird&#26159;&#19968;&#20010;&#22522;&#20110;BigBird&#30340;&#38382;&#31572;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#28789;&#27963;&#21644;&#39640;&#25928;&#30340;&#20301;&#32622;&#34920;&#31034;&#26041;&#27861;ALiBi&#21644;&#26367;&#25442;&#20840;&#23616;&#20449;&#24687;&#34920;&#31034;&#26041;&#27861;&#26469;&#25552;&#39640;&#36895;&#24230;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#23427;&#21487;&#20197;&#22312;&#30701;&#36755;&#20837;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#23545;&#38271;&#36755;&#20837;&#36827;&#34892;&#24037;&#20316;&#24182;&#19988;&#22312;&#20302;&#36164;&#28304;&#30340;&#35821;&#35328;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.11870</link><description>&lt;p&gt;
LittleBird&#65306;&#29992;&#20110;&#38382;&#31572;&#30340;&#26356;&#24555;&#26356;&#38271;Transformer
&lt;/p&gt;
&lt;p&gt;
LittleBird: Efficient Faster &amp; Longer Transformer for Question Answering. (arXiv:2210.11870v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11870
&lt;/p&gt;
&lt;p&gt;
LittleBird&#26159;&#19968;&#20010;&#22522;&#20110;BigBird&#30340;&#38382;&#31572;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#28789;&#27963;&#21644;&#39640;&#25928;&#30340;&#20301;&#32622;&#34920;&#31034;&#26041;&#27861;ALiBi&#21644;&#26367;&#25442;&#20840;&#23616;&#20449;&#24687;&#34920;&#31034;&#26041;&#27861;&#26469;&#25552;&#39640;&#36895;&#24230;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#23427;&#21487;&#20197;&#22312;&#30701;&#36755;&#20837;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#23545;&#38271;&#36755;&#20837;&#36827;&#34892;&#24037;&#20316;&#24182;&#19988;&#22312;&#20302;&#36164;&#28304;&#30340;&#35821;&#35328;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BERT&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#25104;&#26524;&#12290;&#20294;&#30001;&#20110;&#20854;&#27880;&#24847;&#26426;&#21046;&#65292;&#23427;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;Longformer&#12289;ETC&#21644;BigBird&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#24182;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20108;&#27425;&#20381;&#36182;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#24182;&#19981;&#36275;&#22815;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;LittleBird&#65292;&#23427;&#26159;&#22522;&#20110;BigBird&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#36895;&#24230;&#21644;&#26356;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26356;&#28789;&#27963;&#12289;&#26356;&#26377;&#25928;&#30340;&#20301;&#32622;&#34920;&#31034;&#26041;&#27861;&#65292;&#31216;&#20026;Attention with Linear Biases (ALiBi)&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#22312;BigBird&#20013;&#29992;&#20110;&#34920;&#31034;&#20840;&#23616;&#20449;&#24687;&#30340;&#26041;&#27861;&#21487;&#20197;&#26367;&#25442;&#20026;&#25171;&#21253;&#21644;&#35299;&#21253;&#27880;&#24847;&#21147;&#65292;&#36825;&#26356;&#26377;&#25928;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#29992;&#20110;&#30701;&#36755;&#20837;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#23545;&#38271;&#36755;&#20837;&#36827;&#34892;&#24037;&#20316;&#65292;&#24182;&#19988;&#21487;&#20197;&#37325;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#39640;&#25928;&#22320;&#35757;&#32451;&#30701;&#36755;&#20837;&#12290;&#36825;&#23545;&#20110;&#33719;&#21462;&#22823;&#37327;&#38271;&#25991;&#26412;&#25968;&#25454;&#22256;&#38590;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#32780;&#35328;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has a limitation dealing with long inputs due to its attention mechanism. Longformer, ETC and BigBird addressed this issue and effectively solved the quadratic dependency problem. However we find that these models are not sufficient, and propose LittleBird, a novel model based on BigBird with improved speed and memory footprint while maintaining accuracy. In particular, we devise a more flexible and efficient position representation method based on Attention with Linear Biases (ALiBi). We also show that replacing the method of global information represented in the BigBird with pack and unpack attention is more effective. The proposed model can work on long inputs even after being pre-trained on short inputs, and can be trained efficiently reusing existing pre-trained language model for short inputs. This is a significant benefit for low-resource languages where large amounts of long text data are difficult to obtain.
&lt;/p&gt;</description></item><item><title>&#20998;&#35299;&#25552;&#31034;&#26159;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#26469;&#22996;&#25176;&#32473;&#35774;&#35745;&#19987;&#38376;&#30340;&#25552;&#31034;&#24211;&#65292;&#20248;&#21270;&#27599;&#20010;&#25552;&#31034;&#30340;&#29305;&#23450;&#23376;&#20219;&#21153;&#65292;&#24182;&#21487;&#20197;&#36827;&#19968;&#27493;&#20998;&#35299;&#12289;&#26367;&#25442;&#25110;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#34920;&#26126;&#20855;&#26377;&#28508;&#21147;&#25193;&#23637;&#21040;&#26356;&#22823;&#21644;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2210.02406</link><description>&lt;p&gt;
&#20998;&#35299;&#25552;&#31034;&#65306;&#19968;&#31181;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decomposed Prompting: A Modular Approach for Solving Complex Tasks. (arXiv:2210.02406v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02406
&lt;/p&gt;
&lt;p&gt;
&#20998;&#35299;&#25552;&#31034;&#26159;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#26469;&#22996;&#25176;&#32473;&#35774;&#35745;&#19987;&#38376;&#30340;&#25552;&#31034;&#24211;&#65292;&#20248;&#21270;&#27599;&#20010;&#25552;&#31034;&#30340;&#29305;&#23450;&#23376;&#20219;&#21153;&#65292;&#24182;&#21487;&#20197;&#36827;&#19968;&#27493;&#20998;&#35299;&#12289;&#26367;&#25442;&#25110;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#34920;&#26126;&#20855;&#26377;&#28508;&#21147;&#25193;&#23637;&#21040;&#26356;&#22823;&#21644;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#25552;&#31034;&#26159;&#19968;&#31181;&#20196;&#20154;&#24778;&#35766;&#30340;&#24378;&#22823;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#25110;&#32773;&#24403;&#20219;&#21153;&#30340;&#21508;&#20010;&#25512;&#29702;&#27493;&#39588;&#26412;&#36523;&#38590;&#20197;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#23884;&#20837;&#21040;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#20013;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#20998;&#35299;&#25552;&#31034;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#22797;&#26434;&#20219;&#21153;&#65288;&#36890;&#36807;&#25552;&#31034;&#65289;&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#20123;&#23376;&#20219;&#21153;&#21487;&#20197;&#22996;&#25176;&#32473;&#19987;&#38376;&#20026;&#36825;&#20123;&#23376;&#20219;&#21153;&#35774;&#35745;&#30340;&#25552;&#31034;&#22522;&#30784;&#30340;LLM&#24211;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#32467;&#26500;&#20801;&#35768;&#20248;&#21270;&#27599;&#20010;&#25552;&#31034;&#30340;&#29305;&#23450;&#23376;&#20219;&#21153;&#65292;&#24517;&#35201;&#26102;&#36827;&#19968;&#27493;&#20998;&#35299;&#65292;&#29978;&#33267;&#21487;&#20197;&#36731;&#26494;&#26367;&#25442;&#26356;&#26377;&#25928;&#30340;&#25552;&#31034;&#12289;&#35757;&#32451;&#27169;&#22411;&#25110;&#31526;&#21495;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#35299;&#25552;&#31034;&#30340;&#28789;&#27963;&#24615;&#21644;&#27169;&#22359;&#21270;&#24615;&#33021;&#22815;&#20248;&#20110;&#20351;&#29992;GPT3&#30340;&#20808;&#21069;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#24037;&#20316;&#12290;&#23545;&#20110;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#23545;LLM&#22256;&#38590;&#30340;&#23376;&#20219;&#21153;&#36827;&#19968;&#27493;&#20998;&#35299;&#20026;&#26356;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#65292;&#20174;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#35774;&#35745;&#30340;&#31526;&#21495;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#22312;&#25193;&#23637;&#21040;&#26356;&#22823;&#12289;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#26102;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even sim
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;&#31105;&#27490;&#38382;&#39064;&#31038;&#21306;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#36801;&#31227;&#21040;&#30417;&#31649;&#26631;&#20934;&#36739;&#20302;&#30340;&#36793;&#32536;&#24179;&#21488;&#65292;&#32780;&#21442;&#19982;&#36825;&#20123;&#24179;&#21488;&#21487;&#33021;&#20250;&#23548;&#33268;&#22312;&#20027;&#27969;&#24179;&#21488;&#19978;&#30340;&#21453;&#31038;&#20250;&#34892;&#20026;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2209.09803</link><description>&lt;p&gt;
&#36793;&#32536;&#24179;&#21488;&#19978;&#30340;&#21453;&#31038;&#20250;&#34892;&#20026;&#22806;&#28322;&#65306;&#31038;&#21306;&#31105;&#27490;&#30340;&#24847;&#22806;&#21518;&#26524;
&lt;/p&gt;
&lt;p&gt;
Spillover of Antisocial Behavior from Fringe Platforms: The Unintended Consequences of Community Banning. (arXiv:2209.09803v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09803
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#31105;&#27490;&#38382;&#39064;&#31038;&#21306;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#36801;&#31227;&#21040;&#30417;&#31649;&#26631;&#20934;&#36739;&#20302;&#30340;&#36793;&#32536;&#24179;&#21488;&#65292;&#32780;&#21442;&#19982;&#36825;&#20123;&#24179;&#21488;&#21487;&#33021;&#20250;&#23548;&#33268;&#22312;&#20027;&#27969;&#24179;&#21488;&#19978;&#30340;&#21453;&#31038;&#20250;&#34892;&#20026;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24179;&#21488;&#38754;&#20020;&#30528;&#20445;&#25345;&#31038;&#21306;&#25991;&#26126;&#21644;&#23562;&#37325;&#30340;&#21387;&#21147;&#12290;&#22240;&#27492;&#65292;&#20687;Reddit&#21644;Facebook&#36825;&#26679;&#30340;&#20027;&#27969;&#24179;&#21488;&#31105;&#27490;&#38382;&#39064;&#31038;&#21306;&#36890;&#24120;&#20250;&#21463;&#21040;&#28909;&#28872;&#30340;&#20844;&#20247;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25919;&#31574;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#36801;&#31227;&#21040;&#30417;&#31649;&#26631;&#20934;&#36739;&#20302;&#19988;&#20801;&#35768;&#20687;&#21943;&#23376;&#21644;&#39578;&#25200;&#36825;&#26679;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#26367;&#20195;&#36793;&#32536;&#24179;&#21488;&#12290;&#30001;&#20110;&#36825;&#20123;&#31038;&#21306;&#30340;&#29992;&#25143;&#36890;&#24120;&#22312;&#20027;&#27969;&#21644;&#36793;&#32536;&#24179;&#21488;&#19978;&#20445;&#25345;&#20849;&#21516;&#27963;&#21160;&#65292;&#25152;&#20197;&#21453;&#31038;&#20250;&#34892;&#20026;&#21487;&#33021;&#20250;&#27874;&#21450;&#21040;&#20027;&#27969;&#24179;&#21488;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#20174;r/The_Donald&#12289;r/GenderCritical&#21644;r/Incels&#36801;&#31227;&#21040;&#36793;&#32536;&#24179;&#21488;&#30340;&#32422;70,000&#21517;&#29992;&#25143;&#26469;&#30740;&#31350;&#36825;&#31181;&#21487;&#33021;&#30340;&#22806;&#28322;&#25928;&#24212;&#12290;&#20351;&#29992;&#24046;&#24322;&#20998;&#31163;&#35774;&#35745;&#65292;&#25105;&#20204;&#23558;&#20849;&#21516;&#27963;&#21160;&#29992;&#25143;&#19982;&#21305;&#37197;&#30340;&#23545;&#29031;&#32452;&#23545;&#27604;&#65292;&#20272;&#35745;&#36793;&#32536;&#24179;&#21488;&#21442;&#19982;&#23545;Reddit&#29992;&#25143;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#22240;&#26524;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#30417;&#31649;&#26631;&#20934;&#36739;&#20302;&#30340;&#36793;&#32536;&#24179;&#21488;&#19978;&#21442;&#19982;&#21487;&#33021;&#23548;&#33268;Reddit&#29992;&#25143;&#30340;&#21453;&#31038;&#20250;&#34892;&#20026;&#22686;&#21152;&#12290;&#36825;&#31361;&#26174;&#20102;&#31038;&#21306;&#31105;&#27490;&#30340;&#24847;&#22806;&#21518;&#26524;&#65292;&#24182;&#24314;&#35758;&#38656;&#35201;&#26356;&#21152;&#32771;&#34385;&#21608;&#20840;&#30340;&#30417;&#31649;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online platforms face pressure to keep their communities civil and respectful. Thus, the bannings of problematic online communities from mainstream platforms like Reddit and Facebook are often met with enthusiastic public reactions. However, this policy can lead users to migrate to alternative fringe platforms with lower moderation standards and where antisocial behaviors like trolling and harassment are widely accepted. As users of these communities often remain co-active across mainstream and fringe platforms, antisocial behaviors may spill over onto the mainstream platform. We study this possible spillover by analyzing around 70,000 users from three banned communities that migrated to fringe platforms: r/The_Donald, r/GenderCritical, and r/Incels. Using a difference-in-differences design, we contrast co-active users with matched counterparts to estimate the causal effect of fringe platform participation on users' antisocial behavior on Reddit. Our results show that participating in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Medical X-VL&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#21333;&#27169;&#22411;&#21644;&#34701;&#21512;&#32534;&#30721;&#22120;&#65292;&#20197;&#23454;&#29616;&#25918;&#23556;&#23398;&#20013;&#30340;&#38646;&#26679;&#26412;&#30417;&#30563;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.05140</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#35757;&#32451;&#65292;&#21033;&#29992;&#26410;&#32463;&#25972;&#29702;&#30340;&#22270;&#20687;&#21644;&#25253;&#21578;&#23454;&#29616;&#25918;&#23556;&#23398;&#38646;&#26679;&#26412;&#30417;&#30563;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Multi-modal Training from Uncurated Image and Reports Enables Zero-shot Oversight Artificial Intelligence in Radiology. (arXiv:2208.05140v4 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Medical X-VL&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#21333;&#27169;&#22411;&#21644;&#34701;&#21512;&#32534;&#30721;&#22120;&#65292;&#20197;&#23454;&#29616;&#25918;&#23556;&#23398;&#20013;&#30340;&#38646;&#26679;&#26412;&#30417;&#30563;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#31649;&#22411;AI&#26159;&#25918;&#23556;&#23398;&#20013;&#30340;&#26032;&#20852;&#27010;&#24565;&#65292;&#20854;&#20013;AI&#36890;&#36807;&#19981;&#26029;&#25903;&#25345;&#25918;&#23556;&#23398;&#23478;&#30340;&#20915;&#31574;&#65292;&#24418;&#25104;&#19982;&#25918;&#23556;&#23398;&#23478;&#30340;&#20849;&#29983;&#20851;&#31995;&#12290;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#25581;&#31034;&#20102;&#30417;&#31649;&#24615;AI&#30340;&#38271;&#26399;&#38382;&#39064;&#65292;&#21363;&#23427;&#20204;&#29702;&#35299;&#35270;&#35273;&#21644;&#25991;&#26412;&#27010;&#24565;&#21450;&#20854;&#35821;&#20041;&#23545;&#24212;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#23558;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#25104;&#21151;&#26696;&#20363;&#36824;&#24456;&#26377;&#38480;&#65292;&#22240;&#20026;&#30446;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#23398;&#20064;&#31574;&#30053;&#38656;&#35201;&#22270;&#20687;&#21644;&#25991;&#26412;&#23545;&#30340;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#35821;&#26009;&#24211;&#65292;&#36825;&#22312;&#21307;&#23398;&#39046;&#22495;&#36890;&#24120;&#38590;&#20197;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#21307;&#23398;&#20132;&#21449;&#20851;&#27880;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;Medical X-VL&#65289;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#36866;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#21307;&#23398;X-VL&#27169;&#22411;&#22522;&#20110;&#20197;&#19979;&#32452;&#20214;&#65306;&#21307;&#23398;&#39046;&#22495;&#30340;&#33258;&#30417;&#30563;&#21333;&#27169;&#22411;&#21644;&#34701;&#21512;&#32534;&#30721;&#22120;&#65292;&#20197;&#26500;&#24314;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Oversight AI is an emerging concept in radiology where the AI forms a symbiosis with radiologists by continuously supporting radiologists in their decision-making. Recent advances in vision-language models sheds a light on the long-standing problems of the oversight AI by the understanding both visual and textual concepts and their semantic correspondences. However, there have been limited successes in the application of vision-language models in the medical domain, as the current vision-language models and learning strategies for photographic images and captions call for the web-scale data corpus of image and text pairs which was not often feasible in the medical domain. To address this, here we present a model dubbed Medical Cross-attention Vision-Language model (Medical X-VL), leveraging the key components to be tailored for the medical domain. Our medical X-VL model is based on the following components: self-supervised uni-modal models in medical domain and fusion encoder to bridge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36807;&#21435;&#21313;&#24180;&#31070;&#32463;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#65288;DTG&#65289;&#39046;&#22495;&#30340;&#21019;&#26032;&#65292;&#23558;&#20854;&#19982;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#65288;NLG&#65289;&#21306;&#20998;&#24320;&#26469;&#65292;&#24182;&#24378;&#35843;&#20102;&#20851;&#27880;&#31995;&#32479;&#35821;&#35328;&#33021;&#21147;&#21644;&#20844;&#24179;&#12289;&#20844;&#27491;&#30340;DTG&#30740;&#31350;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2207.12571</link><description>&lt;p&gt;
&#31070;&#32463;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#30340;&#21019;&#26032;&#65306;&#32508;&#36848;&#65288;arXiv&#65306;2207.12571v2 [cs.CL] &#26356;&#26032;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Innovations in Neural Data-to-text Generation: A Survey. (arXiv:2207.12571v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36807;&#21435;&#21313;&#24180;&#31070;&#32463;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#65288;DTG&#65289;&#39046;&#22495;&#30340;&#21019;&#26032;&#65292;&#23558;&#20854;&#19982;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#65288;NLG&#65289;&#21306;&#20998;&#24320;&#26469;&#65292;&#24182;&#24378;&#35843;&#20102;&#20851;&#27880;&#31995;&#32479;&#35821;&#35328;&#33021;&#21147;&#21644;&#20844;&#24179;&#12289;&#20844;&#27491;&#30340;DTG&#30740;&#31350;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#38388;&#65292;&#31070;&#32463;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#20852;&#36215;&#22312;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#65288;DTG&#65289;&#39046;&#22495;&#21516;&#26679;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#21019;&#26032;&#12290;&#26412;&#32508;&#36848;&#36890;&#36807;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#65292;&#23545;&#31070;&#32463;DTG&#33539;&#24335;&#30340;&#26041;&#27861;&#12289;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#21327;&#35758;&#36827;&#34892;&#20102;&#32508;&#21512;&#24615;&#30340;&#27010;&#36848;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#39046;&#22495;&#20013;&#21306;&#20998;&#20986;DTG&#65292;&#21253;&#25324;&#23545;&#25991;&#29486;&#30340;&#26368;&#26032;&#32508;&#21512;&#21644;&#25216;&#26415;&#37319;&#29992;&#38454;&#27573;&#30340;&#37325;&#28857;&#20171;&#32461;&#65292;&#24378;&#35843;&#20102;&#19981;&#20165;&#20851;&#27880;&#35774;&#35745;&#20855;&#26377;&#35821;&#35328;&#33021;&#21147;&#30340;&#31995;&#32479;&#65292;&#32780;&#19988;&#36824;&#35201;&#20851;&#27880;&#20307;&#29616;&#20844;&#24179;&#21644;&#20844; accountability &#30340;&#31995;&#32479;&#30340;DTG&#30740;&#31350;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The neural boom that has sparked natural language processing (NLP) research through the last decade has similarly led to significant innovations in data-to-text generation (DTG). This survey offers a consolidated view into the neural DTG paradigm with a structured examination of the approaches, benchmark datasets, and evaluation protocols. This survey draws boundaries separating DTG from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella. With this holistic view, we highlight promising avenues for DTG research that not only focus on the design of linguistically capable systems but also systems that exhibit fairness and accountability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;10&#31181;&#21360;&#24230;&#23612;&#35199;&#20122;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31532;&#19968;&#20010;&#24179;&#34892;&#36164;&#28304;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#35789;&#20856;&#12290;&#35813;&#36164;&#28304;&#23558;&#26377;&#21161;&#20110;&#28608;&#21457;&#26377;&#20851;&#21360;&#23612;&#35821;&#21644;&#20854;&#20182;&#20195;&#34920;&#24615;&#19981;&#36275;&#35821;&#35328;&#30340;NLP&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2205.15960</link><description>&lt;p&gt;
NusaX: 10&#31181;&#21360;&#24230;&#23612;&#35199;&#20122;&#24403;&#22320;&#35821;&#30340;&#22810;&#35821;&#35328;&#24773;&#24863;&#24179;&#34892;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages. (arXiv:2205.15960v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;10&#31181;&#21360;&#24230;&#23612;&#35199;&#20122;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31532;&#19968;&#20010;&#24179;&#34892;&#36164;&#28304;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#35789;&#20856;&#12290;&#35813;&#36164;&#28304;&#23558;&#26377;&#21161;&#20110;&#28608;&#21457;&#26377;&#20851;&#21360;&#23612;&#35821;&#21644;&#20854;&#20182;&#20195;&#34920;&#24615;&#19981;&#36275;&#35821;&#35328;&#30340;NLP&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#25628;&#32034;&#24341;&#25806;&#31561;&#25216;&#26415;&#26041;&#38754;&#23545;&#31038;&#20250;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#23613;&#31649;NLP&#25216;&#26415;&#33719;&#24471;&#25104;&#21151;&#65292;&#20294;&#20165;&#36866;&#29992;&#20110;&#39640;&#36164;&#28304;&#35821;&#35328;&#65292;&#22914;&#33521;&#35821;&#21644;&#27721;&#35821;&#65292;&#32780;&#23545;&#20110;&#35768;&#22810;&#35821;&#35328;&#20173;&#28982;&#26080;&#27861;&#33719;&#24471;&#25968;&#25454;&#36164;&#28304;&#21644;&#22522;&#20934;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#20026;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#35328;&#24320;&#21457;&#36164;&#28304;&#65292;&#23613;&#31649;&#23427;&#26159;&#35821;&#35328;&#22810;&#26679;&#24615;&#31532;&#20108;&#65292;&#20294;&#22823;&#22810;&#25968;&#21360;&#23612;&#35821;&#35328;&#37117;&#34987;&#24402;&#31867;&#20026;&#28626;&#21361;&#35821;&#35328;&#65292;&#20854;&#20013;&#19968;&#20123;&#29978;&#33267;&#28781;&#32477;&#20102;&#12290;&#25105;&#20204;&#20026;10&#31181;&#20302;&#36164;&#28304;&#21360;&#23612;&#35821;&#35328;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#24179;&#34892;&#36164;&#28304;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#22810;&#20219;&#21153;&#22522;&#20934;&#21644;&#35789;&#20856;&#65292;&#20197;&#21450;&#24179;&#34892;&#21360;&#23612;&#35821;-&#33521;&#35821;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#24182;&#25551;&#36848;&#20102;&#21019;&#24314;&#36825;&#31181;&#36164;&#28304;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#22815;&#28608;&#21457;&#26377;&#20851;&#21360;&#23612;&#35821;&#21644;&#20854;&#20182;&#20195;&#34920;&#24615;&#19981;&#36275;&#35821;&#35328;&#30340;NLP&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing (NLP) has a significant impact on society via technologies such as machine translation and search engines. Despite its success, NLP technology is only widely available for high-resource languages such as English and Chinese, while it remains inaccessible to many languages due to the unavailability of data resources and benchmarks. In this work, we focus on developing resources for languages in Indonesia. Despite being the second most linguistically diverse country, most languages in Indonesia are categorized as endangered and some are even extinct. We develop the first-ever parallel resource for 10 low-resource languages in Indonesia. Our resource includes datasets, a multi-task benchmark, and lexicons, as well as a parallel Indonesian-English dataset. We provide extensive analyses and describe the challenges when creating such resources. We hope that our work can spark NLP research on Indonesian and other underrepresented languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26631;&#31614;&#35268;&#33539;&#21270;&#25216;&#26415;&#65292;&#21457;&#29616;&#26631;&#31614;&#35268;&#33539;&#21270;&#36890;&#24120;&#20250;&#25552;&#39640;&#24494;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#24403;&#20351;&#29992;&#36739;&#23567;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#25945;&#24072;&#32593;&#32476;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#25928;&#26524;&#19982;&#30693;&#35782;&#33976;&#39311;&#30456;&#24403;&#65292;&#20294;&#21462;&#20915;&#20110;&#25945;&#24072;&#32593;&#32476;&#30340;&#22823;&#23567;&#21644;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2205.12428</link><description>&lt;p&gt;
&#25105;&#20204;&#38656;&#35201;&#26631;&#31614;&#35268;&#33539;&#21270;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do we need Label Regularization to Fine-tune Pre-trained Language Models?. (arXiv:2205.12428v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26631;&#31614;&#35268;&#33539;&#21270;&#25216;&#26415;&#65292;&#21457;&#29616;&#26631;&#31614;&#35268;&#33539;&#21270;&#36890;&#24120;&#20250;&#25552;&#39640;&#24494;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#24403;&#20351;&#29992;&#36739;&#23567;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#25945;&#24072;&#32593;&#32476;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#25928;&#26524;&#19982;&#30693;&#35782;&#33976;&#39311;&#30456;&#24403;&#65292;&#20294;&#21462;&#20915;&#20110;&#25945;&#24072;&#32593;&#32476;&#30340;&#22823;&#23567;&#21644;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#31070;&#32463;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#23427;&#20005;&#37325;&#20381;&#36182;&#20110;&#25945;&#24072;&#32593;&#32476;&#30340;&#39044;&#27979;&#26469;&#25351;&#23548;&#23398;&#29983;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#22686;&#22823;&#65292;&#30693;&#35782;&#33976;&#39311;&#32463;&#24120;&#34987;&#37319;&#29992;&#22312;&#35768;&#22810;&#28041;&#21450;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#21508;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#26631;&#31614;&#35268;&#33539;&#21270;&#36890;&#24120;&#20250;&#25552;&#39640;&#24494;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#20351;&#29992;&#36739;&#23567;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#35813;&#26041;&#27861;&#30340;&#22909;&#22788;&#21462;&#20915;&#20110;&#25945;&#24072;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#25945;&#24072;&#32593;&#32476;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#32852;&#21512;&#24494;&#35843;&#8221;&#65292;&#23427;&#20351;&#29992;&#23398;&#29983;&#27169;&#22411;&#30340;&#38598;&#21512;&#26469;&#20195;&#26367;&#25945;&#24072;&#32593;&#32476;&#65292;&#21462;&#24471;&#20102;&#19982;&#30693;&#35782;&#33976;&#39311;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation (KD) is a prominent neural model compression technique that heavily relies on teacher network predictions to guide the training of a student model. Considering the ever-growing size of pre-trained language models (PLMs), KD is often adopted in many NLP tasks involving PLMs. However, it is evident that in KD, deploying the teacher network during training adds to the memory and computational requirements of training. In the computer vision literature, the necessity of the teacher network is put under scrutiny by showing that KD is a label regularization technique that can be replaced with lighter teacher-free variants such as the label-smoothing technique. However, to the best of our knowledge, this issue is not investigated in NLP. Therefore, this work concerns studying different label regularization techniques and whether we actually need them to improve the fine-tuning of smaller PLM networks on downstream tasks. In this regard, we did a comprehensive set of exp
&lt;/p&gt;</description></item><item><title>TemporalWiki&#26159;&#19968;&#20010;&#29992;&#26469;&#35757;&#32451;&#21644;&#35780;&#20272;&#19981;&#26029;&#26356;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#32456;&#36523;&#22522;&#20934;&#65292;&#36890;&#36807;&#21033;&#29992;&#33521;&#35821;&#32500;&#22522;&#30334;&#31185;&#21644;&#33521;&#35821;&#32500;&#22522;&#25968;&#25454;&#20043;&#38388;&#30340;&#36830;&#32493;&#24555;&#29031;&#24046;&#24322;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20351;&#30740;&#31350;&#32773;&#21487;&#20197;&#21608;&#26399;&#24615;&#22320;&#36319;&#36394;LM&#30340;&#20445;&#30041;&#21069;&#19968;&#30693;&#35782;&#21644;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#19978;&#33719;&#21462;&#26356;&#26032;/&#26032;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2204.14211</link><description>&lt;p&gt;
TemporalWiki: &#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#19981;&#26029;&#26356;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#32456;&#36523;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models. (arXiv:2204.14211v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.14211
&lt;/p&gt;
&lt;p&gt;
TemporalWiki&#26159;&#19968;&#20010;&#29992;&#26469;&#35757;&#32451;&#21644;&#35780;&#20272;&#19981;&#26029;&#26356;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#32456;&#36523;&#22522;&#20934;&#65292;&#36890;&#36807;&#21033;&#29992;&#33521;&#35821;&#32500;&#22522;&#30334;&#31185;&#21644;&#33521;&#35821;&#32500;&#22522;&#25968;&#25454;&#20043;&#38388;&#30340;&#36830;&#32493;&#24555;&#29031;&#24046;&#24322;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20351;&#30740;&#31350;&#32773;&#21487;&#20197;&#21608;&#26399;&#24615;&#22320;&#36319;&#36394;LM&#30340;&#20445;&#30041;&#21069;&#19968;&#30693;&#35782;&#21644;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#19978;&#33719;&#21462;&#26356;&#26032;/&#26032;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#19990;&#30028;&#30340;&#21464;&#21270;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21464;&#24471;&#36807;&#26102;&#65292;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#25191;&#34892;&#38656;&#35201;&#26368;&#26032;&#20107;&#23454;&#20449;&#24687;&#30340;&#20219;&#21153;&#65292;&#36825;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#23384;&#22312;&#25110;&#23384;&#22312;&#19981;&#21516;&#65292;&#36825;&#31181;&#29616;&#35937;&#21483;&#20570;&#26102;&#38388;&#38169;&#20301;&#12290;&#36825;&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#30740;&#31350;&#30028;&#36824;&#32570;&#20047;&#19968;&#20010;&#19968;&#33268;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;LMs&#23545;&#20110;&#32463;&#24120;&#26356;&#26032;&#30340;&#30693;&#35782;&#24211;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#65289;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TemporalWiki&#65292;&#19968;&#20010;&#32456;&#36523;&#22522;&#20934;&#65292;&#29992;&#20110;&#19981;&#26029;&#26356;&#26032;&#30340;LMs&#65292;&#21033;&#29992;&#33521;&#35821;&#32500;&#22522;&#30334;&#31185;&#21644;&#33521;&#35821;&#32500;&#22522;&#25968;&#25454;&#20043;&#38388;&#30340;&#36830;&#32493;&#24555;&#29031;&#24046;&#24322;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#35813;&#22522;&#20934;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#21608;&#26399;&#24615;&#22320;&#36319;&#36394;LM&#30340;&#20445;&#30041;&#21069;&#19968;&#30693;&#35782;&#21644;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#19978;&#33719;&#21462;&#26356;&#26032;/&#26032;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#36890;&#36807;&#32487;&#32493;&#23398;&#20064;&#26041;&#27861;&#23545;&#24046;&#24322;&#25968;&#25454;&#36827;&#34892;LM&#30340;&#35757;&#32451;&#65292;&#19982;&#22312;&#25972;&#20010;&#24555;&#29031;&#19978;&#20351;&#29992;12&#20493;&#26356;&#23569;&#30340;&#35745;&#31639;&#23454;&#29616;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) become outdated as the world changes; they often fail to perform tasks requiring recent factual information which was absent or different during training, a phenomenon called temporal misalignment. This is especially a challenging problem because the research community still lacks a coherent dataset for assessing the adaptability of LMs to frequently-updated knowledge corpus such as Wikipedia. To this end, we introduce TemporalWiki, a lifelong benchmark for ever-evolving LMs that utilizes the difference between consecutive snapshots of English Wikipedia and English Wikidata for training and evaluation, respectively. The benchmark hence allows researchers to periodically track an LM's ability to retain previous knowledge and acquire updated/new knowledge at each point in time. We also find that training an LM on the diff data through continual learning methods achieves similar or better perplexity than on the entire snapshot in our benchmark with 12 times less comp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; BASIC &#30340;&#32508;&#21512;&#32553;&#25918;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;85.7%&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#31283;&#20581;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#22823;&#23567;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#25209;&#37327;&#22823;&#23567;&#19977;&#20010;&#32500;&#24230;&#19978;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#20102;&#25918;&#22823;&#12290;</title><link>http://arxiv.org/abs/2111.10050</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#32508;&#21512;&#32553;&#25918;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Combined Scaling for Zero-shot Transfer Learning. (arXiv:2111.10050v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10050
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; BASIC &#30340;&#32508;&#21512;&#32553;&#25918;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;85.7%&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#31283;&#20581;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#22823;&#23567;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#25209;&#37327;&#22823;&#23567;&#19977;&#20010;&#32500;&#24230;&#19978;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#20102;&#25918;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#32553;&#25918;&#26041;&#27861;&#65288;&#31216;&#20026; BASIC&#65289;&#65292;&#22312;&#19981;&#23398;&#20064;&#20219;&#20309;&#26631;&#35760;&#30340; ImageNet &#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#22312; ImageNet ILSVRC-2012 &#39564;&#35777;&#38598;&#19978;&#23454;&#29616;&#20102;85.7%&#30340; top-1 &#20934;&#30830;&#29575;&#12290;&#35813;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;&#26368;&#20339;&#21457;&#24067;&#30340;&#31867;&#20284;&#27169;&#22411;&#65288;CLIP &#21644; ALIGN&#65289;9.3%&#12290;&#25105;&#20204;&#30340; BASIC &#27169;&#22411;&#36824;&#22312;&#31283;&#20581;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#20363;&#22914;&#65292;&#22312; 5 &#20010;&#20855;&#26377;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#30340;&#27979;&#35797;&#38598;&#65288;&#20363;&#22914; ImageNet-{A,R,V2, Sketch} &#21644; ObjectNet&#65289;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;84.3% &#30340; top-1 &#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#21482;&#26377;&#19968;&#20010;&#23567;&#23567;&#30340;&#36300;&#33853;&#65292;&#19982;&#20854;&#21407;&#22987;&#30340; ImageNet &#20934;&#30830;&#29575;&#30456;&#27604;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#32500;&#24230;&#19978;&#25918;&#22823;&#20102; CLIP &#21644; ALIGN &#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65306;&#25968;&#25454;&#22823;&#23567;&#65292;&#27169;&#22411;&#22823;&#23567;&#21644;&#25209;&#37327;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25317;&#26377; 66 &#20159;&#20010;&#24102;&#26377;&#22122;&#22768;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#27604; ALIGN &#22823; 4 &#20493;&#65292;&#27604; CLIP &#22823; 16 &#20493;&#12290;&#25105;&#20204;&#26368;&#22823;&#30340;&#27169;&#22411;&#26377; 30 &#20159;&#20010;&#26435;&#37325;&#65292;&#21442;&#25968;&#27604; ALIGN &#21644; CLIP &#22810;&#20986; 3.75 &#20493;&#65292;FLOPs &#22810;&#20986; 8 &#20493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#20026; 65536&#65292;&#27604; CLIP &#22810; 2 &#20493;&#65292;&#27604; ALIGN &#22810; 4 &#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a combined scaling method - named BASIC - that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best published similar models - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our model achieves 84.3% top-1 average accuracy, only a small drop from its original ImageNet accuracy. To achieve these results, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions: data size, model size, and batch size. Our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and 4x more than
&lt;/p&gt;</description></item></channel></rss>