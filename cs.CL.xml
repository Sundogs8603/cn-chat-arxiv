<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#26469;&#35299;&#20915;&#39564;&#35777;&#35821;&#27861;&#27491;&#30830;&#21477;&#23376;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.08036</link><description>&lt;p&gt;
&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#23454;&#29616;&#23545;&#35821;&#27861;&#27491;&#30830;&#21477;&#23376;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
A Neural-Symbolic Approach Towards Identifying Grammatically Correct Sentences. (arXiv:2307.08036v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08036
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#26469;&#35299;&#20915;&#39564;&#35777;&#35821;&#27861;&#27491;&#30830;&#21477;&#23376;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36523;&#36793;&#30340;&#25991;&#26412;&#20869;&#23481;&#27599;&#22825;&#37117;&#22312;&#22686;&#38271;&#12290;&#22312;&#32593;&#19978;&#25253;&#32440;&#12289;&#21338;&#23458;&#25110;&#31038;&#20132;&#23186;&#20307;&#19978;&#65292;&#25105;&#20204;&#27491;&#22312;&#25776;&#20889;&#22823;&#37327;&#30340;&#25991;&#31456;&#12290;&#31867;&#20284;&#22320;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22914;&#35821;&#35328;&#27169;&#22411;&#25110;&#20256;&#32479;&#30340;&#32463;&#20856;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#27491;&#22312;&#21033;&#29992;&#20197;&#19978;&#25152;&#26377;&#20869;&#23481;&#65292;&#25552;&#39640;&#20182;&#20204;&#23398;&#20064;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#31867;&#20284;&#20154;&#31867;&#30340;&#20934;&#30830;&#24615;&#26469;&#24212;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25361;&#25112;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#33719;&#21462;&#26469;&#33258;&#26377;&#25928;&#26469;&#28304;&#30340;&#20889;&#20316;&#25991;&#26412;&#23545;&#20110;&#24212;&#23545;&#25991;&#26412;&#25688;&#35201;&#12289;&#38382;&#31572;&#12289;&#26426;&#22120;&#32763;&#35793;&#29978;&#33267;&#20195;&#35789;&#28040;&#35299;&#31561;&#25361;&#25112;&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#35201;&#36827;&#34892;&#33391;&#22909;&#30340;&#25688;&#35201;&#65292;&#38656;&#35201;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#21477;&#23376;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#36830;&#25509;&#36215;&#26469;&#24418;&#25104;&#25688;&#35201;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25105;&#20204;&#27809;&#26377;&#33719;&#21462;&#21040;&#20889;&#24471;&#22909;&#30340;&#33521;&#25991;&#21477;&#23376;&#29978;&#33267;&#38750;&#26377;&#25928;&#30340;&#21477;&#23376;&#20250;&#24590;&#20040;&#26679;&#21602;&#65311;&#23613;&#31649;&#33719;&#24471;&#20889;&#24471;&#22909;&#30340;&#21477;&#23376;&#30340;&#37325;&#35201;&#24615;&#34987;&#24191;&#27867;&#35748;&#21487;&#65292;&#20294;&#25214;&#20986;&#39564;&#35777;&#23427;&#20204;&#30340;&#26041;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual content around us is growing on a daily basis. Numerous articles are being written as we speak on online newspapers, blogs, or social media. Similarly, recent advances in the AI field, like language models or traditional classic AI approaches, are utilizing all the above to improve their learned representation to tackle NLP challenges with human-like accuracy. It is commonly accepted that it is crucial to have access to well-written text from valid sources to tackle challenges like text summarization, question-answering, machine translation, or even pronoun resolution. For instance, to summarize well, one needs to select the most important sentences in order to concatenate them to form the summary. However, what happens if we do not have access to well-formed English sentences or even non-valid sentences? Despite the importance of having access to well-written sentences, figuring out ways to validate them is still an open area of research. To address this problem, we present a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31215;&#26497;&#24773;&#32490;&#24341;&#21457;&#26469;&#20419;&#36827;&#22810;&#36718;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#24341;&#21457;&#24378;&#24230;&#24182;&#20445;&#25345;&#20250;&#35805;&#30446;&#26631;&#19968;&#33268;&#24615;&#65292;&#23454;&#29616;&#20102;&#31215;&#26497;&#24773;&#32490;&#24341;&#21457;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07994</link><description>&lt;p&gt;
&#21033;&#29992;&#31215;&#26497;&#24773;&#32490;&#24341;&#21457;&#26469;&#20419;&#36827;&#22810;&#36718;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#65306;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation: A Reinforcement Learning Approach. (arXiv:2307.07994v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31215;&#26497;&#24773;&#32490;&#24341;&#21457;&#26469;&#20419;&#36827;&#22810;&#36718;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#24341;&#21457;&#24378;&#24230;&#24182;&#20445;&#25345;&#20250;&#35805;&#30446;&#26631;&#19968;&#33268;&#24615;&#65292;&#23454;&#29616;&#20102;&#31215;&#26497;&#24773;&#32490;&#24341;&#21457;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#26088;&#22312;&#25552;&#20379;&#24773;&#24863;&#25903;&#25345;&#20197;&#25913;&#21892;&#20154;&#20204;&#30340;&#24515;&#29702;&#29366;&#24577;&#12290;&#29616;&#26377;&#30740;&#31350;&#20165;&#23616;&#38480;&#20110;&#37197;&#21512;&#22238;&#24212;&#21644;&#22238;&#31574;&#30053;&#65288;&#22914;&#38382;&#39064;&#65289;&#65292;&#24573;&#30053;&#20102;&#23545;&#24773;&#24863;&#25903;&#25345;&#30340;&#24433;&#21709;&#65292;&#24182;&#32570;&#20047;&#26126;&#30830;&#30340;&#30446;&#26631;&#26469;&#24341;&#23548;&#31215;&#26497;&#24773;&#24863;&#36716;&#21464;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#23558;&#22810;&#36718;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#24418;&#24335;&#21270;&#20026;&#31215;&#26497;&#24773;&#24863;&#24341;&#21457;&#30340;&#36807;&#31243;&#12290;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#38656;&#35201;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#32454;&#35843;&#24773;&#24863;&#24341;&#21457;&#30340;&#24378;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#35805;&#30340;&#19968;&#33268;&#24615;&#31561;&#20250;&#35805;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#19987;&#23478;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;Supporter&#65292;&#24182;&#35774;&#35745;&#20102;&#24773;&#24863;&#25903;&#25345;&#21644;&#23545;&#35805;&#19968;&#33268;&#24615;&#22870;&#21169;&#26469;&#25351;&#23548;&#22238;&#24212;&#31574;&#30053;&#30340;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Supporter&#22312;&#23454;&#29616;&#31215;&#26497;&#24773;&#24863;&#24341;&#21457;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20102;&#21253;&#25324;&#19968;&#33268;&#24615;&#22312;&#20869;&#30340;&#20250;&#35805;&#30446;&#26631;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotional support conversation (ESC) aims to provide emotional support (ES) to improve one's mental state. Existing works stay at fitting grounded responses and responding strategies (e.g., question), which ignore the effect on ES and lack explicit goals to guide emotional positive transition. To this end, we introduce a new paradigm to formalize multi-turn ESC as a process of positive emotion elicitation. Addressing this task requires finely adjusting the elicitation intensity in ES as the conversation progresses while maintaining conversational goals like coherence. In this paper, we propose Supporter, a mixture-of-expert-based reinforcement learning model, and well design ES and dialogue coherence rewards to guide policy's learning for responding. Experiments verify the superiority of Supporter in achieving positive emotion elicitation during responding while maintaining conversational goals including coherence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20248;&#21270;Transformer&#25512;&#29702;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#30693;&#35782;&#33976;&#39311;&#12289;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#36731;&#37327;&#32423;&#32593;&#32476;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2307.07982</link><description>&lt;p&gt;
&#20248;&#21270;Transformer&#25512;&#29702;&#25216;&#26415;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Techniques for Optimizing Transformer Inference. (arXiv:2307.07982v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20248;&#21270;Transformer&#25512;&#29702;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#30693;&#35782;&#33976;&#39311;&#12289;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#36731;&#37327;&#32423;&#32593;&#32476;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Transformer&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#24212;&#29992;&#33539;&#22260;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#12290;&#21253;&#25324;BERT&#12289;GPT&#21644;ViT&#22312;&#20869;&#30340;Transformer&#32593;&#32476;&#23478;&#26063;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#39046;&#22495;&#23637;&#29616;&#20102;&#25928;&#26524;&#12290;ChatGPT&#31561;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#20063;&#24433;&#21709;&#20102;&#26222;&#36890;&#20154;&#30340;&#29983;&#27963;&#12290;&#28982;&#32780;&#65292;&#36861;&#27714;&#39640;&#39044;&#27979;&#24615;&#33021;&#23548;&#33268;&#20102;Transformer&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#28040;&#32791;&#30340;&#25351;&#25968;&#22686;&#38271;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#22312;&#25277;&#35937;&#32423;&#21035;&#30340;Transformer&#25512;&#29702;&#20248;&#21270;&#25216;&#26415;&#12290;&#26412;&#25991;&#23545;Transformer&#32593;&#32476;&#25512;&#29702;&#38454;&#27573;&#30340;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#22312;&#31639;&#27861;&#23618;&#38754;&#19978;&#35843;&#26597;&#20102;&#30693;&#35782;&#33976;&#39311;&#12289;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#36731;&#37327;&#32423;&#32593;&#32476;&#35774;&#35745;&#31561;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen a phenomenal rise in performance and applications of transformer neural networks. The family of transformer networks, including Bidirectional Encoder Representations from Transformer (BERT), Generative Pretrained Transformer (GPT) and Vision Transformer (ViT), have shown their effectiveness across Natural Language Processing (NLP) and Computer Vision (CV) domains. Transformer-based networks such as ChatGPT have impacted the lives of common men. However, the quest for high predictive performance has led to an exponential increase in transformers' memory and compute footprint. Researchers have proposed techniques to optimize transformer inference at all levels of abstraction. This paper presents a comprehensive survey of techniques for optimizing the inference phase of transformer networks. We survey techniques such as knowledge distillation, pruning, quantization, neural architecture search and lightweight network design at the algorithmic level. We further review
&lt;/p&gt;</description></item><item><title>MinT&#36890;&#36807;&#22810;&#35270;&#35282;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#19981;&#21516;&#26631;&#27880;&#39118;&#26684;&#30340;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#20013;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07951</link><description>&lt;p&gt;
MinT: &#36890;&#36807;&#22810;&#35270;&#35282;&#24494;&#35843;&#25552;&#21319;&#25968;&#23398;&#25512;&#29702;&#30340;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
MinT: Boosting Generalization in Mathematical Reasoning via Multi-View Fine-Tuning. (arXiv:2307.07951v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07951
&lt;/p&gt;
&lt;p&gt;
MinT&#36890;&#36807;&#22810;&#35270;&#35282;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#19981;&#21516;&#26631;&#27880;&#39118;&#26684;&#30340;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#20013;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30456;&#23545;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#65292;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#36827;&#34892;&#25512;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#35768;&#22810;&#24403;&#21069;&#26041;&#27861;&#19987;&#27880;&#20110;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#19987;&#38376;&#21270;LM&#65292;&#24182;&#19988;&#36807;&#24230;&#20381;&#36182;&#20110;&#24378;&#22823;&#20294;&#20302;&#25928;&#30340;&#22823;&#22411;LM&#65288;LLM&#65289;&#25152;&#25552;&#20379;&#30340;&#30693;&#35782;&#33976;&#39311;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#36991;&#20813;&#36807;&#24230;&#20381;&#36182;LLM&#25945;&#24072;&#30340;&#26032;&#24605;&#36335;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#20855;&#26377;&#19981;&#21516;&#26631;&#27880;&#39118;&#26684;&#30340;&#29616;&#26377;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#30340;&#22810;&#35270;&#35282;&#24494;&#35843;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#19981;&#21516;&#30340;&#26631;&#27880;&#26684;&#24335;&#35270;&#20026;&#19981;&#21516;&#30340;&#8220;&#35270;&#22270;&#8221;&#65292;&#24182;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#21033;&#29992;&#23427;&#20204;&#12290;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#25351;&#20196;&#38468;&#21152;&#21040;&#36755;&#20837;&#38382;&#39064;&#19978;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#20197;&#28789;&#27963;&#30340;&#26041;&#24335;&#29983;&#25104;&#19981;&#21516;&#26684;&#24335;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#20351;LLaMA-7B&#27169;&#22411;&#33021;&#22815;&#36229;&#36234;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#20808;&#21069;&#26041;&#27861;&#20197;&#21450;&#31934;&#24515;&#24314;&#31435;&#30340;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#27169;&#22411;&#21462;&#24471;&#20102;&#27963;&#36291;
&lt;/p&gt;
&lt;p&gt;
Reasoning in mathematical domains remains a significant challenge for relatively small language models (LMs). Many current methods focus on specializing LMs in mathematical reasoning and rely heavily on knowledge distillation from powerful but inefficient large LMs (LLMs). In this work, we explore a new direction that avoids over-reliance on LLM teachers, introducing a multi-view fine-tuning method that efficiently exploits existing mathematical problem datasets with diverse annotation styles. Our approach uniquely considers the various annotation formats as different "views" and leverages them in training the model. By postpending distinct instructions to input questions, models can learn to generate solutions in diverse formats in a flexible manner. Experimental results show that our strategy enables a LLaMA-7B model to outperform prior approaches that utilize knowledge distillation, as well as carefully established baselines. Additionally, the proposed method grants the models promi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#21360;&#24230;&#35821;&#35328;&#30340;ASR&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21516;&#28304;&#35821;&#35328;&#26469;&#20811;&#26381;&#25968;&#25454;&#30340;&#20302;&#36164;&#28304;&#24615;&#36136;&#65292;&#35299;&#20915;&#20102;&#22810;&#31181;&#26041;&#35328;&#21644;&#30456;&#20284;&#35821;&#35328;&#23384;&#22312;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#23545;&#22768;&#23398;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2307.07948</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#21360;&#24230;&#35821;&#35328;&#30340;ASR&#27169;&#22411;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Model Adaptation for ASR in low-resource Indian Languages. (arXiv:2307.07948v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#21360;&#24230;&#35821;&#35328;&#30340;ASR&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21516;&#28304;&#35821;&#35328;&#26469;&#20811;&#26381;&#25968;&#25454;&#30340;&#20302;&#36164;&#28304;&#24615;&#36136;&#65292;&#35299;&#20915;&#20102;&#22810;&#31181;&#26041;&#35328;&#21644;&#30456;&#20284;&#35821;&#35328;&#23384;&#22312;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#23545;&#22768;&#23398;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20026;&#22522;&#30784;&#30340;&#22768;&#23398;&#27169;&#22411;&#65288;&#22914;wav2vec2&#65289;&#21644;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#35757;&#32451;&#65288;&#22914;Whisper&#65289;&#20351;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20173;&#23384;&#22312;&#24040;&#22823;&#25361;&#25112;&#65292;&#36825;&#20123;&#35821;&#35328;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#21487;&#29992;&#24615;&#37117;&#24456;&#26377;&#38480;&#12290;&#21360;&#24230;&#35821;&#35328;&#20013;&#23384;&#22312;&#22810;&#31181;&#26041;&#35328;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#21360;&#24230;&#35821;&#35328;&#21487;&#20197;&#20998;&#20026;&#30456;&#21516;&#30340;&#35821;&#31995;&#65292;&#20849;&#20139;&#30456;&#21516;&#30340;&#20070;&#20889;&#21644;&#35821;&#27861;&#32467;&#26500;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#24212;&#29992;&#35768;&#22810;&#36866;&#24212;&#21644;&#24494;&#35843;&#25216;&#26415;&#26469;&#20811;&#26381;&#25968;&#25454;&#30340;&#20302;&#36164;&#28304;&#24615;&#36136;&#65292;&#36890;&#36807;&#21033;&#29992;&#36164;&#28304;&#20016;&#23500;&#30340;&#30456;&#20284;&#35821;&#35328;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#37325;&#35201;&#30340;&#26159;&#20102;&#35299;&#21508;&#31181;&#27169;&#24577;&#65288;&#22914;&#22768;&#23398;&#21644;&#25991;&#26412;&#65289;&#22312;&#26500;&#24314;&#21487;&#38752;&#30340;ASR&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#21487;&#33021;&#30340;&#24773;&#20917;&#26159;&#65292;&#22312;&#26576;&#31181;&#35821;&#35328;&#20013;&#65292;&#20016;&#23500;&#30340;&#22768;&#23398;&#25968;&#25454;&#38477;&#20302;&#20102;&#23545;&#22823;&#22411;&#32431;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) performance has improved drastically in recent years, mainly enabled by self-supervised learning (SSL) based acoustic models such as wav2vec2 and large-scale multi-lingual training like Whisper. A huge challenge still exists for low-resource languages where the availability of both audio and text is limited. This is further complicated by the presence of multiple dialects like in Indian languages. However, many Indian languages can be grouped into the same families and share the same script and grammatical structure. This is where a lot of adaptation and fine-tuning techniques can be applied to overcome the low-resource nature of the data by utilising well-resourced similar languages.  In such scenarios, it is important to understand the extent to which each modality, like acoustics and text, is important in building a reliable ASR. It could be the case that an abundance of acoustic data in a language reduces the need for large text-only corpora. Or, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#20196;&#29260;&#32423;&#21035;&#21644;&#36328;&#24230;&#32423;&#21035;&#30417;&#30563;&#30340;CDAP&#32593;&#32476;&#29992;&#20110;&#23569;&#26679;&#26412;&#24207;&#21015;&#26631;&#27880;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#31890;&#24230;&#19978;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#21644;&#19968;&#33268;&#25439;&#22833;&#65292;&#23454;&#29616;&#20102;&#20004;&#20010;&#32593;&#32476;&#30340;&#21327;&#21516;&#23398;&#20064;&#65292;&#22312;&#25512;&#29702;&#38454;&#27573;&#20351;&#29992;&#20102;&#19968;&#33268;&#36138;&#23146;&#25512;&#29702;&#31639;&#27861;&#26469;&#36873;&#25321;&#38750;&#37325;&#21472;&#36328;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.07946</link><description>&lt;p&gt;
&#32852;&#21512;&#20196;&#29260;&#32423;&#21035;&#21644;&#36328;&#24230;&#32423;&#21035;&#30340;&#30417;&#30563;&#29992;&#20110;&#23569;&#26679;&#26412;&#24207;&#21015;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Unifying Token and Span Level Supervisions for Few-Shot Sequence Labeling. (arXiv:2307.07946v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#20196;&#29260;&#32423;&#21035;&#21644;&#36328;&#24230;&#32423;&#21035;&#30417;&#30563;&#30340;CDAP&#32593;&#32476;&#29992;&#20110;&#23569;&#26679;&#26412;&#24207;&#21015;&#26631;&#27880;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#31890;&#24230;&#19978;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#21644;&#19968;&#33268;&#25439;&#22833;&#65292;&#23454;&#29616;&#20102;&#20004;&#20010;&#32593;&#32476;&#30340;&#21327;&#21516;&#23398;&#20064;&#65292;&#22312;&#25512;&#29702;&#38454;&#27573;&#20351;&#29992;&#20102;&#19968;&#33268;&#36138;&#23146;&#25512;&#29702;&#31639;&#27861;&#26469;&#36873;&#25321;&#38750;&#37325;&#21472;&#36328;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#24207;&#21015;&#26631;&#27880;&#26088;&#22312;&#20165;&#20381;&#25454;&#23569;&#37327;&#26631;&#27880;&#26679;&#26412;&#26469;&#35782;&#21035;&#26032;&#30340;&#31867;&#21035;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#24230;&#37327;&#23398;&#20064;&#30340;&#20196;&#29260;&#32423;&#21035;&#25110;&#36328;&#24230;&#32423;&#21035;&#26631;&#27880;&#27169;&#22411;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#22312;&#21333;&#19968;&#31890;&#24230;&#19978;&#35757;&#32451;&#65288;&#21363;&#20196;&#29260;&#32423;&#21035;&#25110;&#36328;&#24230;&#32423;&#21035;&#65289;&#65292;&#24182;&#19988;&#20855;&#26377;&#30456;&#24212;&#31890;&#24230;&#30340;&#19968;&#20123;&#24369;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#32479;&#19968;&#20102;&#20196;&#29260;&#32423;&#21035;&#21644;&#36328;&#24230;&#32423;&#21035;&#30340;&#30417;&#30563;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#21452;&#33258;&#36866;&#24212;&#21407;&#22411;&#32593;&#32476;&#65288;CDAP&#65289;&#29992;&#20110;&#23569;&#26679;&#26412;&#24207;&#21015;&#26631;&#27880;&#12290;CDAP&#21253;&#21547;&#20196;&#29260;&#32423;&#21035;&#21644;&#36328;&#24230;&#32423;&#21035;&#30340;&#32593;&#32476;&#65292;&#22312;&#19981;&#21516;&#31890;&#24230;&#19978;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#12290;&#20026;&#20102;&#20351;&#20004;&#20010;&#32593;&#32476;&#30340;&#36755;&#20986;&#20445;&#25345;&#19968;&#33268;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#25439;&#22833;&#65292;&#20351;&#23427;&#20204;&#21487;&#20197;&#20114;&#30456;&#23398;&#20064;&#12290;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#36138;&#23146;&#25512;&#29702;&#31639;&#27861;&#65292;&#39318;&#20808;&#35843;&#25972;&#39044;&#27979;&#27010;&#29575;&#65292;&#28982;&#21518;&#36138;&#23146;&#22320;&#36873;&#25321;&#20855;&#26377;&#26368;&#22823;&#27010;&#29575;&#30340;&#38750;&#37325;&#21472;&#36328;&#24230;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Few-shot sequence labeling aims to identify novel classes based on only a few labeled samples. Existing methods solve the data scarcity problem mainly by designing token-level or span-level labeling models based on metric learning. However, these methods are only trained at a single granularity (i.e., either token level or span level) and have some weaknesses of the corresponding granularity. In this paper, we first unify token and span level supervisions and propose a Consistent Dual Adaptive Prototypical (CDAP) network for few-shot sequence labeling. CDAP contains the token-level and span-level networks, jointly trained at different granularities. To align the outputs of two networks, we further propose a consistent loss to enable them to learn from each other. During the inference phase, we propose a consistent greedy inference algorithm that first adjusts the predicted probability and then greedily selects non-overlapping spans with maximum probability. Extensive experiments show t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32534;&#31243;&#38382;&#39064;&#20013;&#21435;&#38500;&#37325;&#22797;&#31243;&#24207;&#24182;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#20197;&#40723;&#21169;&#23398;&#20064;&#32773;&#21442;&#32771;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#20174;&#32780;&#23398;&#20064;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.07940</link><description>&lt;p&gt;
&#20026;&#25512;&#33616;&#21442;&#32771;&#35299;&#20915;&#26041;&#26696;&#32780;&#21435;&#37325;&#21644;&#25490;&#21517;&#35299;&#20915;&#26041;&#26696;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Deduplicating and Ranking Solution Programs for Suggesting Reference Solutions. (arXiv:2307.07940v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32534;&#31243;&#38382;&#39064;&#20013;&#21435;&#38500;&#37325;&#22797;&#31243;&#24207;&#24182;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#20197;&#40723;&#21169;&#23398;&#20064;&#32773;&#21442;&#32771;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#20174;&#32780;&#23398;&#20064;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32534;&#31243;&#25945;&#32946;&#20013;&#65292;&#21442;&#32771;&#20854;&#20182;&#29992;&#25143;&#32534;&#20889;&#30340;&#35299;&#20915;&#26041;&#26696;&#31243;&#24207;&#23545;&#23398;&#20064;&#32773;&#24456;&#26377;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22312;&#32447;&#35780;&#27979;&#31995;&#32479;&#21482;&#26159;&#21015;&#20986;&#29992;&#25143;&#25552;&#20132;&#30340;&#25152;&#26377;&#35299;&#20915;&#26041;&#26696;&#31243;&#24207;&#20379;&#21442;&#32771;&#65292;&#24182;&#26681;&#25454;&#25552;&#20132;&#26085;&#26399;&#12289;&#25191;&#34892;&#26102;&#38388;&#25110;&#29992;&#25143;&#35780;&#20998;&#36827;&#34892;&#25490;&#24207;&#65292;&#24573;&#35270;&#20102;&#31243;&#24207;&#33021;&#22815;&#25104;&#20026;&#21442;&#32771;&#30340;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#22826;&#22810;&#37325;&#22797;&#21644;&#36817;&#20284;&#37325;&#22797;&#30340;&#31243;&#24207;&#65292;&#29992;&#25143;&#24456;&#38590;&#21442;&#32771;&#22810;&#31181;&#35299;&#20915;&#26041;&#27861;&#12290;&#20026;&#20102;&#28608;&#21169;&#23398;&#20064;&#32773;&#21442;&#32771;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#27861;&#20197;&#23398;&#20064;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27599;&#20010;&#32534;&#31243;&#38382;&#39064;&#20013;&#21435;&#37325;&#21644;&#25490;&#21517;&#24120;&#35265;&#35299;&#20915;&#26041;&#26696;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#26356;&#22810;&#37325;&#22797;&#30340;&#31243;&#24207;&#37319;&#29992;&#26356;&#24120;&#35265;&#30340;&#26041;&#27861;&#24182;&#21487;&#20316;&#20026;&#21442;&#32771;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#21024;&#38500;&#20102;&#36817;&#20284;&#37325;&#22797;&#30340;&#35299;&#20915;&#26041;&#26696;&#31243;&#24207;&#65292;&#24182;&#26681;&#25454;&#37325;&#22797;&#35745;&#25968;&#23545;&#21807;&#19968;&#30340;&#31243;&#24207;&#36827;&#34892;&#25490;&#24207;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#21435;&#37325;&#21644;&#25490;&#21517;&#35299;&#20915;&#26041;&#26696;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Referring to the solution programs written by the other users is helpful for learners in programming education. However, current online judge systems just list all solution programs submitted by users for references, and the programs are sorted based on the submission date and time, execution time, or user rating, ignoring to what extent the program can be a reference. In addition, users struggle to refer to a variety of solution approaches since there are too many duplicated and near-duplicated programs. To motivate the learners to refer to various solutions to learn the better solution approaches, in this paper, we propose an approach to deduplicate and rank common solution programs in each programming problem. Based on the hypothesis that the more duplicated programs adopt a more common approach and can be a reference, we remove the near-duplicated solution programs and rank the unique programs based on the duplicate count. The experiments on the solution programs submitted to a rea
&lt;/p&gt;</description></item><item><title>GeoGPT&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#20027;GPT&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#19982;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#38477;&#20302;&#38750;&#19987;&#19994;&#29992;&#25143;&#35299;&#20915;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30340;&#38376;&#27099;&#12290;</title><link>http://arxiv.org/abs/2307.07930</link><description>&lt;p&gt;
GeoGPT:&#36890;&#36807;&#33258;&#20027;GPT&#29702;&#35299;&#21644;&#22788;&#29702;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
GeoGPT: Understanding and Processing Geospatial Tasks through An Autonomous GPT. (arXiv:2307.07930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07930
&lt;/p&gt;
&lt;p&gt;
GeoGPT&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#20027;GPT&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#19982;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#38477;&#20302;&#38750;&#19987;&#19994;&#29992;&#25143;&#35299;&#20915;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30340;&#38376;&#27099;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GIS&#20915;&#31574;&#32773;&#38656;&#35201;&#32467;&#21512;&#19968;&#31995;&#21015;&#30340;&#31354;&#38388;&#31639;&#27861;&#21644;&#25805;&#20316;&#26469;&#35299;&#20915;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#22312;&#35821;&#20041;&#29702;&#35299;&#21644;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20123;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#19982;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#38477;&#20302;&#38750;&#19987;&#19994;&#29992;&#25143;&#35299;&#20915;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30340;&#38376;&#27099;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-makers in GIS need to combine a series of spatial algorithms and operations to solve geospatial tasks. For example, in the task of facility siting, the Buffer tool is usually first used to locate areas close or away from some specific entities; then, the Intersect or Erase tool is used to select candidate areas satisfied multiple requirements. Though professionals can easily understand and solve these geospatial tasks by sequentially utilizing relevant tools, it is difficult for non-professionals to handle these problems. Recently, Generative Pre-trained Transformer (e.g., ChatGPT) presents strong performance in semantic understanding and reasoning. Especially, AutoGPT can further extend the capabilities of large language models (LLMs) by automatically reasoning and calling externally defined tools. Inspired by these studies, we attempt to lower the threshold of non-professional users to solve geospatial tasks by integrating the semantic understanding ability inherent in LLMs 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36719;&#20214;&#24320;&#21457;&#33539;&#24335;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25972;&#20010;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#65292;&#28040;&#38500;&#20102;&#27599;&#20010;&#38454;&#27573;&#38656;&#35201;&#19987;&#38376;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#35813;&#33539;&#24335;&#20351;&#29992;ChatDev&#20316;&#20026;&#19968;&#20010;&#34394;&#25311;&#32842;&#22825;&#39537;&#21160;&#30340;&#36719;&#20214;&#24320;&#21457;&#20844;&#21496;&#65292;&#36890;&#36807;&#35774;&#35745;&#12289;&#32534;&#30721;&#12289;&#27979;&#35797;&#21644;&#25991;&#26723;&#21270;&#22235;&#20010;&#38454;&#27573;&#30340;&#20195;&#29702;&#20154;&#22242;&#38431;&#20419;&#36827;&#21327;&#20316;&#12290;</title><link>http://arxiv.org/abs/2307.07924</link><description>&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#20132;&#27969;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Communicative Agents for Software Development. (arXiv:2307.07924v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36719;&#20214;&#24320;&#21457;&#33539;&#24335;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25972;&#20010;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#65292;&#28040;&#38500;&#20102;&#27599;&#20010;&#38454;&#27573;&#38656;&#35201;&#19987;&#38376;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#35813;&#33539;&#24335;&#20351;&#29992;ChatDev&#20316;&#20026;&#19968;&#20010;&#34394;&#25311;&#32842;&#22825;&#39537;&#21160;&#30340;&#36719;&#20214;&#24320;&#21457;&#20844;&#21496;&#65292;&#36890;&#36807;&#35774;&#35745;&#12289;&#32534;&#30721;&#12289;&#27979;&#35797;&#21644;&#25991;&#26723;&#21270;&#22235;&#20010;&#38454;&#27573;&#30340;&#20195;&#29702;&#20154;&#22242;&#38431;&#20419;&#36827;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24037;&#31243;&#26159;&#19968;&#20010;&#20197;&#24494;&#22937;&#30340;&#30452;&#35273;&#21644;&#21672;&#35810;&#20026;&#29305;&#24449;&#30340;&#39046;&#22495;&#65292;&#20915;&#31574;&#36807;&#31243;&#22797;&#26434;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#24320;&#22987;&#36890;&#36807;&#22312;&#36719;&#20214;&#24320;&#21457;&#30340;&#21508;&#20010;&#38454;&#27573;&#23454;&#26045;&#31934;&#24515;&#35774;&#35745;&#26469;&#38761;&#26032;&#36719;&#20214;&#24037;&#31243;&#23454;&#36341;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#65292;&#22312;&#25972;&#20010;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#31616;&#21270;&#21644;&#32479;&#19968;&#20851;&#38190;&#27969;&#31243;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#22312;&#27599;&#20010;&#38454;&#27573;&#38656;&#35201;&#19987;&#38376;&#30340;&#27169;&#22411;&#30340;&#38656;&#35201;&#12290;&#36825;&#20010;&#33539;&#24335;&#30340;&#26680;&#24515;&#26159;ChatDev&#65292;&#19968;&#20010;&#34394;&#25311;&#30340;&#32842;&#22825;&#39537;&#21160;&#36719;&#20214;&#24320;&#21457;&#20844;&#21496;&#65292;&#23427;&#27169;&#20223;&#20102;&#24050;&#32463;&#24314;&#31435;&#30340;&#28689;&#24067;&#27169;&#22411;&#65292;&#23558;&#24320;&#21457;&#36807;&#31243;&#32454;&#20998;&#20026;&#22235;&#20010;&#19981;&#21516;&#30340;&#26102;&#38388;&#38454;&#27573;&#65306;&#35774;&#35745;&#12289;&#32534;&#30721;&#12289;&#27979;&#35797;&#21644;&#25991;&#26723;&#21270;&#12290;&#27599;&#20010;&#38454;&#27573;&#37117;&#28041;&#21450;&#19968;&#20010;&#22242;&#38431;&#30340;&#20195;&#29702;&#20154;&#65292;&#22914;&#31243;&#24207;&#21592;&#12289;&#20195;&#30721;&#23457;&#26597;&#20154;&#21592;&#21644;&#27979;&#35797;&#24037;&#31243;&#24072;&#65292;&#20419;&#36827;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software engineering is a domain characterized by intricate decision-making processes, often relying on nuanced intuition and consultation. Recent advancements in deep learning have started to revolutionize software engineering practices through elaborate designs implemented at various stages of software development. In this paper, we present an innovative paradigm that leverages large language models (LLMs) throughout the entire software development process, streamlining and unifying key processes through natural language communication, thereby eliminating the need for specialized models at each phase. At the core of this paradigm lies ChatDev, a virtual chat-powered software development company that mirrors the established waterfall model, meticulously dividing the development process into four distinct chronological stages: designing, coding, testing, and documenting. Each stage engages a team of agents, such as programmers, code reviewers, and test engineers, fostering collaborativ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#27604;&#36739;&#21028;&#23450;&#26469;&#30830;&#23450;&#20505;&#36873;&#22238;&#24212;&#30340;&#20248;&#21155;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#32477;&#23545;&#35780;&#20998;&#65292;&#27604;&#36739;&#35780;&#20272;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#24471;&#36739;&#23567;&#30340;&#24320;&#28304;LLMs&#36798;&#21040;&#20102;&#19982;&#26356;&#22823;&#30340;&#20844;&#20849;&#35775;&#38382;API&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07889</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#21028;&#23450;&#36827;&#34892;&#38646;&#26679;&#26412;NLG&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Zero-shot NLG evaluation through Pairware Comparisons with LLMs. (arXiv:2307.07889v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#27604;&#36739;&#21028;&#23450;&#26469;&#30830;&#23450;&#20505;&#36873;&#22238;&#24212;&#30340;&#20248;&#21155;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#32477;&#23545;&#35780;&#20998;&#65292;&#27604;&#36739;&#35780;&#20272;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#24471;&#36739;&#23567;&#30340;&#24320;&#28304;LLMs&#36798;&#21040;&#20102;&#19982;&#26356;&#22823;&#30340;&#20844;&#20849;&#35775;&#38382;API&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#36755;&#20986;&#26159;&#33267;&#20851;&#37325;&#35201;&#20294;&#36153;&#26102;&#36153;&#21147;&#30340;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#33258;&#21160;NLG&#35780;&#20272;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26159;&#29305;&#23450;&#20219;&#21153;&#29305;&#23450;&#39046;&#22495;&#30340;&#65292;&#38656;&#35201;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#21644;&#23646;&#24615;&#36827;&#34892;&#24037;&#31243;&#35774;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#38646;&#26679;&#26412;NLG&#35780;&#20272;&#30340;&#31283;&#20581;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#37197;&#23545;&#27604;&#36739;&#21028;&#23450;&#30340;&#26041;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#21160;&#26426;&#26159;&#65292;&#21363;&#20351;&#20316;&#20026;&#20154;&#31867;&#65292;&#30830;&#23450;&#20004;&#20010;&#36873;&#39033;&#20013;&#21738;&#20010;&#26356;&#22909;&#35201;&#27604;&#29420;&#31435;&#23458;&#35266;&#35780;&#20998;&#27599;&#20010;&#36873;&#39033;&#26356;&#23481;&#26131;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#24182;&#21033;&#29992;LLMs&#26032;&#20852;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25506;&#27979;FlanT5&#65292;&#30830;&#23450;&#20004;&#20010;&#20505;&#36873;&#22238;&#24212;&#20013;&#21738;&#19968;&#20010;&#26356;&#22909;&#65292;&#32780;&#19981;&#26159;&#25351;&#23450;&#32477;&#23545;&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27604;&#36739;&#35780;&#20272;&#26159;&#27604;&#32477;&#23545;&#35780;&#20998;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#36739;&#23567;&#30340;&#24320;&#28304;LLMs&#33021;&#22815;&#36798;&#21040;&#19982;&#26356;&#22823;&#30340;&#20844;&#20849;&#35775;&#38382;API&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating Natural Language Generation (NLG) outputs is crucial but laborious and expensive. While various automatic NLG assessment methods have been proposed, they often are quite task-specific and have to be engineered with a particular domain and attribute in mind. In this work, we propose a robust zero-shot approach to NLG evaluation using pairwise comparative judgment with open-source Large Language Models (LLMs). The motivation for this approach is that even as humans, it is easier to determine which of two options are better, than it is to independently objectively score each option. We use this insight and leverage the emergent abilities of LLMs, where we probe FlanT5 to determine which of two candidate responses is better, rather than assigning absolute scores. Our results demonstrate that comparative assessment is a more effective approach than absolute scoring, enabling smaller open-source LLMs to achieve comparable performance to larger public access APIs. We evaluate syste
&lt;/p&gt;</description></item><item><title>&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#24494;&#35843;&#22312;&#22810;&#35821;&#31181;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20173;&#28982;&#26377;&#38480;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;ProFiT&#27969;&#31243;&#23545;&#27492;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#19981;&#21516;&#30340;&#23569;&#26679;&#26412;&#21644;&#20840;&#25968;&#25454;&#35774;&#32622;&#19979;&#65292;&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#24494;&#35843;&#20855;&#26377;&#19981;&#21516;&#30340;&#24615;&#33021;&#21464;&#21270;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.07880</link><description>&lt;p&gt;
&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#24494;&#35843;&#24635;&#26159;&#27604;&#21407;&#22987;&#24494;&#35843;&#26356;&#22909;&#21527;&#65311;&#26469;&#33258;&#36328;&#35821;&#35328;&#29702;&#35299;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is Prompt-Based Finetuning Always Better than Vanilla Finetuning? Insights from Cross-Lingual Language Understanding. (arXiv:2307.07880v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07880
&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#24494;&#35843;&#22312;&#22810;&#35821;&#31181;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20173;&#28982;&#26377;&#38480;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;ProFiT&#27969;&#31243;&#23545;&#27492;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#19981;&#21516;&#30340;&#23569;&#26679;&#26412;&#21644;&#20840;&#25968;&#25454;&#35774;&#32622;&#19979;&#65292;&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#24494;&#35843;&#20855;&#26377;&#19981;&#21516;&#30340;&#24615;&#33021;&#21464;&#21270;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;MPLM&#65289;&#36890;&#36807;&#22312;&#28304;&#35821;&#35328;&#65288;&#20363;&#22914;&#33521;&#35821;&#65289;&#19978;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#27880;&#25968;&#25454;&#19978;&#23545;MPLM&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#21508;&#31181;&#30446;&#26631;&#35821;&#35328;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24050;&#32463;&#22312;&#38646;&#36716;&#21270;&#36328;&#35821;&#35328;&#20256;&#36882;&#30340;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#36229;&#36807;&#20102;&#24120;&#35268;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#35821;&#31181;&#20219;&#21153;&#20013;&#65292;&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#23398;&#20064;&#30340;&#25506;&#32034;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ProFiT&#27969;&#31243;&#65292;&#20197;&#30740;&#31350;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#30340;&#36328;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#22810;&#26679;&#36328;&#35821;&#35328;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65288;&#24773;&#24863;&#20998;&#31867;&#12289;&#37322;&#20041;&#35782;&#21035;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65289;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#23569;&#26679;&#26412;&#21644;&#20840;&#25968;&#25454;&#35774;&#32622;&#19979;&#32463;&#39564;&#24615;&#22320;&#20998;&#26512;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#24615;&#33021;&#30340;&#21464;&#21270;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#24494;&#35843;&#22312;&#36328;&#35821;&#35328;&#20256;&#36882;&#20013;&#30340;&#24615;&#33021;&#21464;&#21270;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual pretrained language models (MPLMs) have demonstrated substantial performance improvements in zero-shot cross-lingual transfer across various natural language understanding tasks by finetuning MPLMs on task-specific labelled data of a source language (e.g. English) and evaluating on a wide range of target languages. Recent studies show that prompt-based finetuning surpasses regular finetuning in few-shot scenarios. However, the exploration of prompt-based learning in multilingual tasks remains limited. In this study, we propose the ProFiT pipeline to investigate the cross-lingual capabilities of Prompt-based Finetuning. We conduct comprehensive experiments on diverse cross-lingual language understanding tasks (sentiment classification, paraphrase identification, and natural language inference) and empirically analyze the variation trends of prompt-based finetuning performance in cross-lingual transfer across different few-shot and full-data settings. Our results reveal the 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#21472;&#21152;&#12290;&#36890;&#36807;&#35282;&#24230;&#21487;&#25511;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35282;&#24230;&#19979;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;</title><link>http://arxiv.org/abs/2307.07870</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25991;&#21270;&#35282;&#24230;&#30340;&#21472;&#21152;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07870
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#21472;&#21152;&#12290;&#36890;&#36807;&#35282;&#24230;&#21487;&#25511;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35282;&#24230;&#19979;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24120;&#24120;&#34987;&#38169;&#35823;&#22320;&#35748;&#20026;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#12290;&#25105;&#20204;&#35748;&#20026;LLMs&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#21472;&#21152;&#12290;LLMs&#34920;&#29616;&#20986;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#22522;&#20110;&#20135;&#29983;&#30340;&#35282;&#24230;&#32780;&#25913;&#21464;&#65288;&#19982;&#20154;&#31867;&#30456;&#21453;&#65292;&#20154;&#31867;&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#36890;&#24120;&#20855;&#26377;&#26356;&#19968;&#33268;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#35282;&#24230;&#21487;&#25511;&#24615;&#8221;&#30340;&#27010;&#24565;&#65292;&#25351;&#30340;&#26159;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24515;&#29702;&#23398;&#38382;&#21367;&#65288;PVQ&#12289;VSM&#12289;IPIP&#65289;&#26469;&#30740;&#31350;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#22914;&#20309;&#22522;&#20110;&#19981;&#21516;&#35282;&#24230;&#32780;&#25913;&#21464;&#12290;&#36890;&#36807;&#23450;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#25552;&#31034;&#20013;&#65288;&#38544;&#24335;&#25110;&#26174;&#24335;&#65289;&#26263;&#31034;&#20102;&#26576;&#20123;&#20215;&#20540;&#35266;&#26102;&#65292;LLMs&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#26263;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrat
&lt;/p&gt;</description></item><item><title>CIDER&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#30701;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#25512;&#26029;&#20986;&#24773;&#24863;&#35789;&#30340;&#20542;&#21521;&#26469;&#35780;&#20998;&#20010;&#21035;&#25991;&#26412;&#65292;&#30456;&#27604;&#36890;&#29992;&#26041;&#27861;&#22312;&#22825;&#27668;&#25512;&#25991;&#38598;&#21512;&#19978;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2307.07864</link><description>&lt;p&gt;
CIDER: &#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#30701;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
CIDER: Context sensitive sentiment analysis for short-form text. (arXiv:2307.07864v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07864
&lt;/p&gt;
&lt;p&gt;
CIDER&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#30701;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#25512;&#26029;&#20986;&#24773;&#24863;&#35789;&#30340;&#20542;&#21521;&#26469;&#35780;&#20998;&#20010;&#21035;&#25991;&#26412;&#65292;&#30456;&#27604;&#36890;&#29992;&#26041;&#27861;&#22312;&#22825;&#27668;&#25512;&#25991;&#38598;&#21512;&#19978;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#23545;&#22823;&#37327;&#20851;&#20110;&#29305;&#23450;&#20027;&#39064;&#12289;&#20027;&#39064;&#25110;&#20107;&#20214;&#30340;&#30701;&#25991;&#26412;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#22914;&#25512;&#25991;&#12289;Reddit&#24086;&#23376;&#25110;&#25253;&#32440;&#22836;&#26465;&#12290;&#36890;&#24120;&#20351;&#29992;&#36890;&#29992;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#24179;&#22343;&#24847;&#20041;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20250;&#24573;&#30053;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#21457;&#29983;&#30340;&#24847;&#20041;&#21464;&#21270;&#65292;&#20363;&#22914;&#65292;&#8220;active&#8221;&#19968;&#35789;&#22312;&#8220;active lifestyle&#8221;&#21644;&#8220;active volcano&#8221;&#20013;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#24847;&#22270;&#21644;&#20542;&#21521;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;CIDER&#65288;&#19978;&#19979;&#25991;&#24863;&#30693;&#35789;&#20856;&#21644;&#24773;&#24863;&#25512;&#29702;&#22120;&#65289;&#65292;&#23427;&#36827;&#34892;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#20854;&#20013;&#20174;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#25512;&#26029;&#20986;&#24773;&#24863;&#35789;&#30340;&#20542;&#21521;&#65292;&#28982;&#21518;&#20877;&#29992;&#20110;&#35780;&#20998;&#20010;&#21035;&#25991;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;CIDER&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#22823;&#37327;&#20851;&#20110;&#22825;&#27668;&#30340;&#25512;&#25991;&#38598;&#21512;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36890;&#29992;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#24050;&#23558;CIDER&#30340;&#23454;&#29616;&#20197;python&#20195;&#30721;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers commonly perform sentiment analysis on large collections of short texts like tweets, Reddit posts or newspaper headlines that are all focused on a specific topic, theme or event. Usually, general purpose sentiment analysis methods are used which perform well on average but miss the variation in meaning that happens across different contexts, for example, the word "active" has a very different intention and valence in the phrase "active lifestyle" versus "active volcano". This work presents a new approach, CIDER (Context Informed Dictionary and sEntiment Reasoner), which performs context sensitive sentiment analysis, where the valence of sentiment laden terms is inferred from the whole corpus before being used to score the individual texts. In this paper we detail the CIDER algorithm and demonstrate that it outperforms state-of-the-art generalist sentiment analysis on a large collection of tweets about the weather. We have made our implementation of CIDER available as a pyth
&lt;/p&gt;</description></item><item><title>AspectCSE&#26159;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#22909;&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;3.97%&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#30340;&#23884;&#20837;&#27169;&#22411;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.07851</link><description>&lt;p&gt;
AspectCSE: &#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity using Contrastive Learning and Structured Knowledge. (arXiv:2307.07851v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07851
&lt;/p&gt;
&lt;p&gt;
AspectCSE&#26159;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#22909;&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;3.97%&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#30340;&#23884;&#20837;&#27169;&#22411;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#20102;&#23545;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#31895;&#30053;&#36817;&#20284;&#65292;&#20294;&#24573;&#30053;&#20102;&#20351;&#25991;&#26412;&#30456;&#20284;&#30340;&#29305;&#23450;&#26041;&#38754;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#26041;&#38754;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#20102;&#22522;&#20110;&#39044;&#23450;&#20041;&#26041;&#38754;&#30340;&#25991;&#26412;&#30456;&#20284;&#24615;&#12290;&#22240;&#27492;&#65292;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#39044;&#27979;&#26356;&#21152;&#38024;&#23545;&#29305;&#23450;&#35201;&#27714;&#65292;&#24182;&#19988;&#26356;&#23481;&#26131;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AspectCSE&#65292;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#26368;&#22909;&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;AspectCSE&#22312;&#22810;&#20010;&#26041;&#38754;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24179;&#22343;&#25913;&#21892;3.97%&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;Wikidata&#30693;&#35782;&#22270;&#23646;&#24615;&#26469;&#35757;&#32451;&#22810;&#26041;&#38754;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#20854;&#20013;&#22312;&#30456;&#20284;&#24615;&#39044;&#27979;&#36807;&#31243;&#20013;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#26041;&#38754;&#23884;&#20837;&#22312;&#29305;&#23450;&#26041;&#38754;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#19978;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23884;&#20837;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25913;&#36827;&#23884;&#20837;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generic sentence embeddings provide a coarse-grained approximation of semantic textual similarity but ignore specific aspects that make texts similar. Conversely, aspect-based sentence embeddings provide similarities between texts based on certain predefined aspects. Thus, similarity predictions of texts are more targeted to specific requirements and more easily explainable. In this paper, we present AspectCSE, an approach for aspect-based contrastive learning of sentence embeddings. Results indicate that AspectCSE achieves an average improvement of 3.97% on information retrieval tasks across multiple aspects compared to the previous best results. We also propose using Wikidata knowledge graph properties to train models of multi-aspect sentence embeddings in which multiple specific aspects are simultaneously considered during similarity predictions. We demonstrate that multi-aspect embeddings outperform single-aspect embeddings on aspect-specific information retrieval tasks. Finally, w
&lt;/p&gt;</description></item><item><title>Transformers&#26550;&#26500;&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#20855;&#26377;&#36890;&#29992;&#30340;&#39044;&#27979;&#24615;&#36136;&#65292;&#24182;&#19988;&#22312;&#38750;&#28176;&#36817;&#25968;&#25454;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.07843</link><description>&lt;p&gt;
Transformers&#26159;&#36890;&#29992;&#30340;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers are Universal Predictors. (arXiv:2307.07843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07843
&lt;/p&gt;
&lt;p&gt;
Transformers&#26550;&#26500;&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#20855;&#26377;&#36890;&#29992;&#30340;&#39044;&#27979;&#24615;&#36136;&#65292;&#24182;&#19988;&#22312;&#38750;&#28176;&#36817;&#25968;&#25454;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25214;&#21040;&#20102;Transformer&#26550;&#26500;&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#22312;&#20449;&#24687;&#29702;&#35770;&#24847;&#20041;&#19978;&#20855;&#26377;&#36890;&#29992;&#30340;&#39044;&#27979;&#24615;&#36136;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#22312;&#38750;&#28176;&#36817;&#25968;&#25454;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#65292;&#20197;&#20102;&#35299;Transformer&#26550;&#26500;&#30340;&#21508;&#20010;&#32452;&#20214;&#22312;&#25968;&#25454;&#39640;&#25928;&#35757;&#32451;&#30340;&#32972;&#26223;&#19979;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We find limits to the Transformer architecture for language modeling and show it has a universal prediction property in an information-theoretic sense. We further analyze performance in non-asymptotic data regimes to understand the role of various components of the Transformer architecture, especially in the context of data-efficient training. We validate our theoretical analysis with experiments on both synthetic and real datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#36890;&#36947;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24847;&#35265;&#25366;&#25496;&#65292;&#24182;&#29992;&#20110;&#25512;&#33616;&#20135;&#21697;&#12290;&#36890;&#36807;&#24212;&#29992;SMOTE&#31639;&#27861;&#22686;&#21152;&#35780;&#35770;&#25968;&#37327;&#24182;&#23545;&#25968;&#25454;&#36827;&#34892;&#24179;&#34913;&#65292;&#20351;&#29992;&#24352;&#37327;&#20998;&#35299;&#31639;&#27861;&#20026;&#32858;&#31867;&#20998;&#37197;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07798</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#36890;&#36947;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25512;&#33616;&#31995;&#32479;&#30340;&#24847;&#35265;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Opinion mining using Double Channel CNN for Recommender System. (arXiv:2307.07798v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#36890;&#36947;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24847;&#35265;&#25366;&#25496;&#65292;&#24182;&#29992;&#20110;&#25512;&#33616;&#20135;&#21697;&#12290;&#36890;&#36807;&#24212;&#29992;SMOTE&#31639;&#27861;&#22686;&#21152;&#35780;&#35770;&#25968;&#37327;&#24182;&#23545;&#25968;&#25454;&#36827;&#34892;&#24179;&#34913;&#65292;&#20351;&#29992;&#24352;&#37327;&#20998;&#35299;&#31639;&#27861;&#20026;&#32858;&#31867;&#20998;&#37197;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#21644;&#31038;&#20132;&#23186;&#20307;&#30340;&#21457;&#23637;&#65292;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#36825;&#20123;&#25968;&#25454;&#20013;&#21253;&#25324;&#29992;&#25143;&#23545;&#22312;&#32447;&#21830;&#24215;&#21644;&#31038;&#20132;&#23186;&#20307;&#19978;&#20135;&#21697;&#30340;&#24847;&#35265;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#24847;&#35265;&#36827;&#34892;&#25506;&#32034;&#21644;&#20998;&#31867;&#65292;&#21487;&#20197;&#33719;&#21462;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;&#29992;&#25143;&#28385;&#24847;&#24230;&#12289;&#29992;&#25143;&#23545;&#29305;&#23450;&#20107;&#20214;&#30340;&#21453;&#39304;&#12289;&#39044;&#27979;&#29305;&#23450;&#20135;&#21697;&#30340;&#38144;&#21806;&#24773;&#20917;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#24182;&#29992;&#20110;&#25512;&#33616;&#20135;&#21697;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#20116;&#23618;&#30340;&#21452;&#36890;&#36947;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#24847;&#35265;&#25366;&#25496;&#65292;&#35813;&#27169;&#22411;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#20102;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;SMOTE&#31639;&#27861;&#26469;&#22686;&#21152;&#21021;&#22987;&#25968;&#25454;&#38598;&#30340;&#35780;&#35770;&#25968;&#37327;&#65292;&#24182;&#23545;&#25968;&#25454;&#36827;&#34892;&#24179;&#34913;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#36825;&#20123;&#35780;&#35770;&#36827;&#34892;&#20102;&#32858;&#31867;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#24352;&#37327;&#20998;&#35299;&#31639;&#27861;&#20026;&#27599;&#20010;&#32858;&#31867;&#20998;&#37197;&#20102;&#19968;&#20010;&#26435;&#37325;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#24050;&#32463;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Much unstructured data has been produced with the growth of the Internet and social media. A significant volume of textual data includes users' opinions about products in online stores and social media. By exploring and categorizing them, helpful information can be acquired, including customer satisfaction, user feedback about a particular event, predicting the sale of a specific product, and other similar cases. In this paper, we present an approach for sentiment analysis with a deep learning model and use it to recommend products. A two-channel convolutional neural network model has been used for opinion mining, which has five layers and extracts essential features from the data. We increased the number of comments by applying the SMOTE algorithm to the initial dataset and balanced the data. Then we proceed to cluster the aspects. We also assign a weight to each cluster using tensor decomposition algorithms that improve the recommender system's performance. Our proposed method has re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;CNN-LSTM&#27169;&#22411;&#23545;&#27874;&#26031;&#25512;&#29305;&#30340;&#25919;&#27835;&#24773;&#24863;&#36827;&#34892;&#20998;&#26512;&#65292;&#20351;&#29992;ParsBERT&#36827;&#34892;&#35789;&#27719;&#34920;&#31034;&#65292;&#24182;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#20854;&#20013;CNN-LSTM&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;89%&#21644;71%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.07740</link><description>&lt;p&gt;
&#20351;&#29992;CNN-LSTM&#27169;&#22411;&#23545;&#27874;&#26031;&#25512;&#29305;&#30340;&#25919;&#27835;&#24773;&#24863;&#36827;&#34892;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Political Sentiment Analysis of Persian Tweets Using CNN-LSTM Model. (arXiv:2307.07740v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;CNN-LSTM&#27169;&#22411;&#23545;&#27874;&#26031;&#25512;&#29305;&#30340;&#25919;&#27835;&#24773;&#24863;&#36827;&#34892;&#20998;&#26512;&#65292;&#20351;&#29992;ParsBERT&#36827;&#34892;&#35789;&#27719;&#34920;&#31034;&#65292;&#24182;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#20854;&#20013;CNN-LSTM&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;89%&#21644;71%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#26159;&#35782;&#21035;&#21644;&#20998;&#31867;&#20154;&#20204;&#23545;&#21508;&#31181;&#35805;&#39064;&#30340;&#24773;&#24863;&#25110;&#35266;&#28857;&#30340;&#36807;&#31243;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;Twitter&#24773;&#24863;&#30340;&#20998;&#26512;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#35805;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#27874;&#26031;&#25919;&#27835;&#25512;&#29305;&#30340;&#24773;&#24863;&#12290;&#25105;&#20204;&#20351;&#29992;&#35789;&#34955;&#27169;&#22411;&#21644;ParsBERT&#36827;&#34892;&#35789;&#27719;&#34920;&#31034;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#39640;&#26031;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#26799;&#24230;&#25552;&#21319;&#12289;&#36923;&#36753;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#20197;&#21450;CNN&#21644;LSTM&#30340;&#32452;&#21512;&#26469;&#20998;&#31867;&#25512;&#29305;&#30340;&#26497;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;ParsBERT&#23884;&#20837;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27604;&#26426;&#22120;&#23398;&#20064;&#34920;&#29616;&#26356;&#22909;&#12290;CNN-LSTM&#27169;&#22411;&#22312;&#31532;&#19968;&#20010;&#26377;&#19977;&#31181;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#20026;89&#65285;&#65292;&#22312;&#31532;&#20108;&#20010;&#26377;&#19971;&#31181;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#20026;71&#65285;&#12290;&#30001;&#20110;&#27874;&#26031;&#35821;&#30340;&#22797;&#26434;&#24615;&#65292;&#36798;&#21040;&#36825;&#19968;&#25928;&#29575;&#27700;&#24179;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis is the process of identifying and categorizing people's emotions or opinions regarding various topics. The analysis of Twitter sentiment has become an increasingly popular topic in recent years. In this paper, we present several machine learning and a deep learning model to analysis sentiment of Persian political tweets. Our analysis was conducted using Bag of Words and ParsBERT for word representation. We applied Gaussian Naive Bayes, Gradient Boosting, Logistic Regression, Decision Trees, Random Forests, as well as a combination of CNN and LSTM to classify the polarities of tweets. The results of this study indicate that deep learning with ParsBERT embedding performs better than machine learning. The CNN-LSTM model had the highest classification accuracy with 89 percent on the first dataset with three classes and 71 percent on the second dataset with seven classes. Due to the complexity of Persian, it was a difficult task to achieve this level of efficiency.
&lt;/p&gt;</description></item><item><title>CPET&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21387;&#32553;LLM&#30340;&#26377;&#25928;&#21442;&#25968;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#30693;&#35782;&#32487;&#25215;&#21644;&#24674;&#22797;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#22312;&#21442;&#25968;&#26377;&#25928;&#35843;&#25972;&#20013;&#21387;&#32553;LLM&#30340;&#25512;&#29702;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07705</link><description>&lt;p&gt;
CPET: &#39640;&#25928;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models. (arXiv:2307.07705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07705
&lt;/p&gt;
&lt;p&gt;
CPET&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21387;&#32553;LLM&#30340;&#26377;&#25928;&#21442;&#25968;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#30693;&#35782;&#32487;&#25215;&#21644;&#24674;&#22797;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#22312;&#21442;&#25968;&#26377;&#25928;&#35843;&#25972;&#20013;&#21387;&#32553;LLM&#30340;&#25512;&#29702;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21442;&#25968;&#26377;&#25928;&#35843;&#25972;&#65288;PET&#65289;&#22240;&#20026;&#22312;&#35843;&#25972;&#30456;&#23545;&#36739;&#23569;&#30340;&#21442;&#25968;&#65288;PET&#27169;&#22359;&#65289;&#30340;&#21516;&#26102;&#20173;&#33021;&#28608;&#27963;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36275;&#22815;&#30693;&#35782;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#32780;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#24403;PET&#29992;&#20110;&#20026;&#22810;&#20010;&#20219;&#21153;&#25552;&#20379;&#26381;&#21153;&#26102;&#65292;&#21487;&#20197;&#22312;&#20923;&#32467;&#30340;LLM&#19978;&#26500;&#24314;&#19981;&#21516;&#30340;&#20219;&#21153;&#29305;&#23450;PET&#27169;&#22359;&#65292;&#36991;&#20813;&#20887;&#20313;LLM&#37096;&#32626;&#12290;&#34429;&#28982;PET&#26174;&#33879;&#38477;&#20302;&#20102;&#35843;&#20248;&#21644;&#37096;&#32626;LLM&#30340;&#25104;&#26412;&#65292;&#20294;&#20854;&#25512;&#29702;&#20173;&#28982;&#21463;&#21040;LLM&#35745;&#31639;&#29942;&#39048;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21387;&#32553;LLM&#30340;&#26377;&#25928;PET&#26694;&#26550;&#65292;&#31216;&#20026;&#8220;CPET&#8221;&#12290;&#22312;CPET&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20027;&#27969;LLM&#21387;&#32553;&#25216;&#26415;&#23545;PET&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#28982;&#21518;&#24341;&#20837;&#20102;&#30693;&#35782;&#32487;&#25215;&#21644;&#24674;&#22797;&#31574;&#30053;&#26469;&#24674;&#22797;&#30001;&#36825;&#20123;&#21387;&#32553;&#25216;&#26415;&#24341;&#36215;&#30340;&#30693;&#35782;&#20002;&#22833;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30001;&#20110;CPET&#30340;&#24674;&#22797;&#31574;&#30053;&#65292;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient tuning (PET) has been widely explored in recent years because it tunes much fewer parameters (PET modules) than full-parameter fine-tuning (FT) while still stimulating sufficient knowledge from large language models (LLMs) for downstream tasks. Moreover, when PET is employed to serve multiple tasks, different task-specific PET modules can be built on a frozen LLM, avoiding redundant LLM deployments. Although PET significantly reduces the cost of tuning and deploying LLMs, its inference still suffers from the computational bottleneck of LLMs. To address the above issue, we propose an effective PET framework based on compressed LLMs, named "CPET". In CPET, we evaluate the impact of mainstream LLM compression techniques on PET performance and then introduce knowledge inheritance and recovery strategies to restore the knowledge loss caused by these compression techniques. Our experimental results demonstrate that, owing to the restoring strategies of CPET, collaborating
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#31572;&#26696;&#38598;&#32534;&#31243;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#31572;&#26696;&#38598;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07699</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31572;&#26696;&#38598;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models to Generate Answer Set Programs. (arXiv:2307.07699v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#31572;&#26696;&#38598;&#32534;&#31243;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#31572;&#26696;&#38598;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3&#21644;GPT-4&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#31034;&#20986;&#35299;&#20915;&#26576;&#20123;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#37319;&#29992;&#20102;&#21508;&#31181;&#25552;&#31034;&#25216;&#26415;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#26377;&#38480;&#19988;&#30456;&#23545;&#27973;&#26174;&#12290;&#30456;&#21453;&#65292;&#24418;&#24335;&#36923;&#36753;&#25797;&#38271;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#65292;&#20294;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#24418;&#24335;&#36923;&#36753;&#26159;&#19968;&#20010;&#38750;&#19987;&#23478;&#38590;&#20197;&#24212;&#23545;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#31572;&#26696;&#38598;&#32534;&#31243;&#30340;&#20248;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;LLM&#23558;&#36923;&#36753;&#35868;&#39064;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#31572;&#26696;&#38598;&#31243;&#24207;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;LLM&#30340;&#25552;&#31034;&#65292;&#20197;&#36880;&#27493;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#31572;&#26696;&#38598;&#31243;&#24207;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20165;&#20165;&#36890;&#36807;&#20960;&#20010;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#20363;&#65292;LLMs&#23601;&#33021;&#29983;&#25104;&#30456;&#24403;&#22797;&#26434;&#30340;&#31572;&#26696;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated exceptional performance in various natural language processing tasks and have shown the ability to solve certain reasoning problems. However, their reasoning capabilities are limited and relatively shallow, despite the application of various prompting techniques. In contrast, formal logic is adept at handling complex reasoning, but translating natural language descriptions into formal logic is a challenging task that non-experts struggle with. This paper proposes a neuro-symbolic method that combines the strengths of large language models and answer set programming. Specifically, we employ an LLM to transform natural language descriptions of logic puzzles into answer set programs. We carefully design prompts for an LLM to convert natural language descriptions into answer set programs in a step by step manner. Surprisingly, with just a few in-context learning examples, LLMs can generate reasonably complex answer se
&lt;/p&gt;</description></item><item><title>Think-on-Graph&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.07697</link><description>&lt;p&gt;
Think-on-Graph: &#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph. (arXiv:2307.07697v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07697
&lt;/p&gt;
&lt;p&gt;
Think-on-Graph&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#38656;&#35201;&#30693;&#35782;&#36861;&#28335;&#24615;&#12289;&#21450;&#26102;&#24615;&#21644;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#22330;&#26223;&#20013;&#65292;&#23427;&#20204;&#32463;&#24120;&#22312;&#22797;&#26434;&#25512;&#29702;&#21644;&#34920;&#29616;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Think-on-Graph&#65288;ToG&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;LLMs&#28145;&#24230;&#21644;&#36127;&#36131;&#20219;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;ToG&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#19982;&#32473;&#23450;&#38382;&#39064;&#30456;&#20851;&#30340;&#23454;&#20307;&#65292;&#24182;&#23545;&#22806;&#37096;&#30693;&#35782;&#25968;&#25454;&#24211;&#36827;&#34892;&#25506;&#32034;&#21644;&#25512;&#29702;&#65292;&#20197;&#26816;&#32034;&#30456;&#20851;&#19977;&#20803;&#32452;&#12290;&#36825;&#20010;&#36845;&#20195;&#36807;&#31243;&#29983;&#25104;&#21253;&#21547;&#39034;&#24207;&#36830;&#25509;&#30340;&#19977;&#20803;&#32452;&#30340;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#65292;&#30452;&#21040;&#25910;&#38598;&#21040;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#22238;&#31572;&#38382;&#39064;&#25110;&#36798;&#21040;&#26368;&#22823;&#28145;&#24230;&#20026;&#27490;&#12290;&#36890;&#36807;&#22312;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ToG&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;LLMs&#30340;&#21069;&#36848;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant strides in various tasks, yet they often struggle with complex reasoning and exhibit poor performance in scenarios where knowledge traceability, timeliness, and accuracy are crucial. To address these limitations, we present Think-on-Graph (ToG), a novel framework that leverages knowledge graphs to enhance LLMs' ability for deep and responsible reasoning. By employing ToG, we can identify entities relevant to a given question and conduct exploration and reasoning to retrieve related triples from an external knowledge database. This iterative procedure generates multiple reasoning pathways consisting of sequentially connected triplets until sufficient information is gathered to answer the question or the maximum depth is reached. Through experiments on complex multi-hop reasoning question-answering tasks, we demonstrate that ToG outperforms existing methods, effectively addressing the aforementioned limitations of LLMs without incurring 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36923;&#36753;&#32534;&#31243;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#36716;&#25442;&#20026;&#36923;&#36753;&#24418;&#24335;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#24378;&#22823;&#19988;&#36890;&#29992;&#30340;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#31034;&#20363;&#21644;&#21487;&#37325;&#29992;&#30340;&#30693;&#35782;&#27169;&#22359;&#65292;&#21363;&#21487;&#22312;&#22810;&#20010;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07696</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36923;&#36753;&#32534;&#31243;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#25991;&#26412;&#30340;&#24378;&#22823;&#21644;&#36890;&#29992;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text. (arXiv:2307.07696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07696
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36923;&#36753;&#32534;&#31243;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#36716;&#25442;&#20026;&#36923;&#36753;&#24418;&#24335;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#24378;&#22823;&#19988;&#36890;&#29992;&#30340;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#31034;&#20363;&#21644;&#21487;&#37325;&#29992;&#30340;&#30693;&#35782;&#27169;&#22359;&#65292;&#21363;&#21487;&#22312;&#22810;&#20010;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#40065;&#26834;&#24615;&#21644;&#36890;&#29992;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#36824;&#19981;&#33021;&#19982;&#38024;&#23545;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38382;&#39064;&#35757;&#32451;&#30340;&#26368;&#20339;&#27169;&#22411;&#30456;&#31454;&#20105;&#12290;&#26412;&#30740;&#31350;&#35266;&#23519;&#21040;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#23569;&#31034;&#20363;&#35821;&#20041;&#35299;&#26512;&#22120;&#12290;&#23427;&#21487;&#20197;&#23558;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#36716;&#25442;&#20026;&#36923;&#36753;&#24418;&#24335;&#65292;&#20316;&#20026;&#31572;&#26696;&#38598;&#31243;&#24207;&#30340;&#36755;&#20837;&#65292;&#35813;&#31243;&#24207;&#26159;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#30340;&#22768;&#26126;&#24615;&#30693;&#35782;&#34920;&#31034;&#24418;&#24335;&#12290;&#36825;&#31181;&#32452;&#21512;&#32467;&#26524;&#24418;&#25104;&#20102;&#19968;&#20010;&#24378;&#22823;&#32780;&#36890;&#29992;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;&#38382;&#31572;&#20219;&#21153;&#65292;&#26080;&#38656;&#20026;&#27599;&#20010;&#26032;&#20219;&#21153;&#37325;&#26032;&#35757;&#32451;&#12290;&#23427;&#21482;&#38656;&#35201;&#23569;&#37327;&#31034;&#20363;&#26469;&#25351;&#23548;LLM&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#65292;&#20197;&#21450;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#20219;&#21153;&#30340;&#21487;&#37325;&#29992;ASP&#30693;&#35782;&#27169;&#22359;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#20960;&#20010;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;bAbI&#12289;StepGame&#12289;CLUTRR&#21644;gSCAN&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25104;&#21151;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study, we observe that a large language model can serve as a highly effective few-shot semantic parser. It can convert natural language sentences into a logical form that serves as input for answer set programs, a logic-based declarative knowledge representation formalism. The combination results in a robust and general system that can handle multiple question-answering tasks without requiring retraining for each new task. It only needs a few examples to guide the LLM's adaptation to a specific task, along with reusable ASP knowledge modules that can be applied to multiple tasks. We demonstrate that this method achieves state-of-the-art performance on several NLP benchmarks, including bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot pla
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19977;&#31181;&#21306;&#20998;&#30495;&#23454;&#22768;&#38899;&#21644;&#35797;&#22270;&#20882;&#20805;&#29305;&#23450;&#20154;&#29289;&#22768;&#38899;&#30340;&#20811;&#38534;&#22768;&#38899;&#30340;&#25216;&#26415;&#65292;&#20998;&#21035;&#37319;&#29992;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21333;&#20010;&#21644;&#22810;&#20010;&#22768;&#38899;&#19978;&#35757;&#32451;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#23398;&#20064;&#29305;&#24449;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#24182;&#19988;&#23545;&#23545;&#25239;&#24615;&#28165;&#27927;&#20855;&#26377;&#30456;&#24403;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07683</link><description>&lt;p&gt;
&#21333;&#22768;&#36947;&#21644;&#22810;&#22768;&#36947;&#20811;&#38534;&#22768;&#38899;&#26816;&#27979;&#65306;&#20174;&#24863;&#30693;&#21040;&#23398;&#20064;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Single and Multi-Speaker Cloned Voice Detection: From Perceptual to Learned Features. (arXiv:2307.07683v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19977;&#31181;&#21306;&#20998;&#30495;&#23454;&#22768;&#38899;&#21644;&#35797;&#22270;&#20882;&#20805;&#29305;&#23450;&#20154;&#29289;&#22768;&#38899;&#30340;&#20811;&#38534;&#22768;&#38899;&#30340;&#25216;&#26415;&#65292;&#20998;&#21035;&#37319;&#29992;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21333;&#20010;&#21644;&#22810;&#20010;&#22768;&#38899;&#19978;&#35757;&#32451;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#23398;&#20064;&#29305;&#24449;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#24182;&#19988;&#23545;&#23545;&#25239;&#24615;&#28165;&#27927;&#20855;&#26377;&#30456;&#24403;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21512;&#25104;&#35821;&#38899;&#20811;&#38534;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24341;&#21457;&#20102;&#19968;&#31995;&#21015;&#28508;&#22312;&#30340;&#21361;&#23475;&#12290;&#20174;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#30340;&#37329;&#34701;&#27450;&#35784;&#21040;&#34394;&#20551;&#20449;&#24687;&#20256;&#25773;&#27963;&#21160;&#65292;&#38656;&#35201;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#21306;&#20998;&#30495;&#23454;&#21644;&#21512;&#25104;&#22768;&#38899;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19977;&#31181;&#21306;&#20998;&#30495;&#23454;&#22768;&#38899;&#21644;&#35797;&#22270;&#20882;&#20805;&#29305;&#23450;&#20154;&#29289;&#22768;&#38899;&#30340;&#20811;&#38534;&#22768;&#38899;&#30340;&#25216;&#26415;&#12290;&#36825;&#19977;&#31181;&#26041;&#27861;&#22312;&#29305;&#24449;&#25552;&#21462;&#38454;&#27573;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#20302;&#32500;&#24863;&#30693;&#29305;&#24449;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#20294;&#20934;&#30830;&#24615;&#36739;&#20302;&#65292;&#32780;&#36890;&#29992;&#30340;&#39057;&#35889;&#29305;&#24449;&#21644;&#31471;&#21040;&#31471;&#23398;&#20064;&#29305;&#24449;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#20294;&#21487;&#35299;&#37322;&#24615;&#36739;&#20302;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#21333;&#20010;&#35828;&#35805;&#20154;&#30340;&#22768;&#38899;&#19978;&#35757;&#32451;&#21644;&#22312;&#22810;&#20010;&#22768;&#38899;&#19978;&#35757;&#32451;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#23398;&#20064;&#29305;&#24449;&#22987;&#32456;&#20445;&#25345;&#30528;$0\%$&#33267;$4\%$&#20043;&#38388;&#30340;&#31561;&#38169;&#35823;&#29575;&#65292;&#24182;&#19988;&#23545;&#23545;&#25239;&#24615;&#28165;&#27927;&#20855;&#26377;&#30456;&#24403;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic-voice cloning technologies have seen significant advances in recent years, giving rise to a range of potential harms. From small- and large-scale financial fraud to disinformation campaigns, the need for reliable methods to differentiate real and synthesized voices is imperative. We describe three techniques for differentiating a real from a cloned voice designed to impersonate a specific person. These three approaches differ in their feature extraction stage with low-dimensional perceptual features offering high interpretability but lower accuracy, to generic spectral features, and end-to-end learned features offering less interpretability but higher accuracy. We show the efficacy of these approaches when trained on a single speaker's voice and when trained on multiple voices. The learned features consistently yield an equal error rate between $0\%$ and $4\%$, and are reasonably robust to adversarial laundering.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;2.1M&#33521;&#35821;Yelp&#35780;&#35770;&#30340;&#39184;&#21381;&#36827;&#34892;&#35821;&#35328;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#31227;&#27665;&#32654;&#39135;&#26356;&#23481;&#26131;&#34987;&#26500;&#26550;&#20026;&#23458;&#35266;&#21644;&#20182;&#32773;&#21270;&#65292;&#32780;&#38750;&#35199;&#26041;&#31227;&#27665;&#32654;&#39135;&#21463;&#27426;&#36814;&#31243;&#24230;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2307.07645</link><description>&lt;p&gt;
&#32654;&#22269;&#39184;&#21381;&#35780;&#35770;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31227;&#27665;&#32654;&#39135;&#20182;&#32773;&#21270;&#21644;&#20302;&#22768;&#26395;&#26500;&#26550;
&lt;/p&gt;
&lt;p&gt;
Othering and low prestige framing of immigrant cuisines in US restaurant reviews and large language models. (arXiv:2307.07645v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07645
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;2.1M&#33521;&#35821;Yelp&#35780;&#35770;&#30340;&#39184;&#21381;&#36827;&#34892;&#35821;&#35328;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#31227;&#27665;&#32654;&#39135;&#26356;&#23481;&#26131;&#34987;&#26500;&#26550;&#20026;&#23458;&#35266;&#21644;&#20182;&#32773;&#21270;&#65292;&#32780;&#38750;&#35199;&#26041;&#31227;&#27665;&#32654;&#39135;&#21463;&#27426;&#36814;&#31243;&#24230;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#29702;&#35299;&#23545;&#39135;&#29289;&#30340;&#38544;&#21547;&#24577;&#24230;&#26377;&#21161;&#20110;&#20943;&#36731;&#22240;&#39135;&#29289;&#20316;&#20026;&#25991;&#21270;&#21644;&#31181;&#26063;&#36523;&#20221;&#30340;&#26631;&#24535;&#32780;&#23548;&#33268;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#23545;&#39135;&#29289;&#30340;&#21051;&#26495;&#21360;&#35937;&#26159;&#19968;&#31181;&#24494;&#20405;&#30053;&#65292;&#23427;&#23545;&#26377;&#23475;&#30340;&#20844;&#20849;&#35805;&#35821;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#36825;&#21487;&#33021;&#21453;&#36807;&#26469;&#21152;&#28145;&#23545;&#27665;&#26063;&#32676;&#20307;&#30340;&#20559;&#35265;&#65292;&#24182;&#23545;&#39184;&#39302;&#30340;&#32463;&#27982;&#32467;&#26524;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#36890;&#36807;&#20180;&#32454;&#30340;&#35821;&#35328;&#20998;&#26512;&#65292;&#25105;&#20204;&#22312;&#19968;&#39033;&#22823;&#35268;&#27169;&#30740;&#31350;&#20013;&#35780;&#20272;&#20102;&#23545;&#31227;&#27665;&#32654;&#39135;&#24577;&#24230;&#30340;&#31038;&#20250;&#29702;&#35770;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#20102;2.1M&#33521;&#35821;Yelp&#35780;&#35770;&#30340;&#39184;&#21381;&#22312;14&#20010;&#32654;&#22269;&#24030;&#30340;&#26694;&#26550;&#24046;&#24322;&#12290;&#22312;&#25511;&#21046;&#20102;&#39184;&#21381;&#20215;&#26684;&#21644;&#37051;&#37324;&#31181;&#26063;&#22810;&#26679;&#24615;&#31561;&#22240;&#32032;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#31227;&#27665;&#32654;&#39135;&#26356;&#26377;&#21487;&#33021;&#20197;&#23458;&#35266;&#21644;&#20182;&#32773;&#21270;&#30340;&#24418;&#24335;&#36827;&#34892;&#26500;&#26550;&#65292;&#22914;&#30495;&#23454;&#24615;&#65288;&#20363;&#22914;&#65292;&#30495;&#23454;&#65292;&#20256;&#32479;&#65289;&#65292;&#24322;&#22269;&#24773;&#35843;&#65288;&#20363;&#22914;&#65292;&#24322;&#22269;&#65292;&#19981;&#21516;&#65289;&#21644;&#20856;&#22411;&#24615;&#65288;&#20363;&#22914;&#65292;&#20856;&#22411;&#65292;&#36890;&#24120;&#65289;&#12290;&#20294;&#38750;&#35199;&#26041;&#31227;&#27665;&#32654;&#39135;&#65288;&#20363;&#22914;&#65292;&#21360;&#24230;&#65292;&#22696;&#35199;&#21733;&#65289;&#26356;&#21463;&#27426;&#36814;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying and understanding implicit attitudes toward food can help efforts to mitigate social prejudice due to food's pervasive role as a marker of cultural and ethnic identity. Stereotypes about food are a form of microaggression that contribute to harmful public discourse that may in turn perpetuate prejudice toward ethnic groups and negatively impact economic outcomes for restaurants. Through careful linguistic analyses, we evaluate social theories about attitudes toward immigrant cuisine in a large-scale study of framing differences in 2.1M English language Yelp reviews of restaurants in 14 US states. Controlling for factors such as restaurant price and neighborhood racial diversity, we find that immigrant cuisines are more likely to be framed in objectifying and othering terms of authenticity (e.g., authentic, traditional), exoticism (e.g., exotic, different), and prototypicality (e.g., typical, usual), but that non-Western immigrant cuisines (e.g., Indian, Mexican) receive mor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#23398;&#29983;&#22312;&#32447;&#35838;&#31243;&#35752;&#35770;&#35770;&#22363;&#38382;&#39064;&#30340;&#35268;&#27169;&#25193;&#23637;&#38590;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#65292;&#33258;&#21160;&#30830;&#23450;&#35770;&#22363;&#24086;&#23376;&#30340;&#32039;&#24613;&#31243;&#24230;&#65292;&#24182;&#25552;&#20379;&#32473;&#25945;&#24072;&#27880;&#24847;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#39044;&#27979;7&#20998;&#21046;&#32039;&#24613;&#31243;&#24230;&#32423;&#21035;&#65292;&#23454;&#29616;&#20102;&#26356;&#32454;&#31890;&#24230;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07614</link><description>&lt;p&gt;
&#23454;&#29616;&#23545;&#35752;&#35770;&#35770;&#22363;&#24086;&#23376;&#32039;&#24613;&#24615;&#30340;&#36890;&#29992;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Generalizable Detection of Urgency of Discussion Forum Posts. (arXiv:2307.07614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#23398;&#29983;&#22312;&#32447;&#35838;&#31243;&#35752;&#35770;&#35770;&#22363;&#38382;&#39064;&#30340;&#35268;&#27169;&#25193;&#23637;&#38590;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#65292;&#33258;&#21160;&#30830;&#23450;&#35770;&#22363;&#24086;&#23376;&#30340;&#32039;&#24613;&#31243;&#24230;&#65292;&#24182;&#25552;&#20379;&#32473;&#25945;&#24072;&#27880;&#24847;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#39044;&#27979;7&#20998;&#21046;&#32039;&#24613;&#31243;&#24230;&#32423;&#21035;&#65292;&#23454;&#29616;&#20102;&#26356;&#32454;&#31890;&#24230;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#19982;&#22312;&#32447;&#35838;&#31243;&#65288;&#22914;MOOC&#65289;&#30340;&#23398;&#29983;&#20250;&#20351;&#29992;&#35838;&#31243;&#30340;&#35752;&#35770;&#35770;&#22363;&#65292;&#22312;&#36935;&#21040;&#38382;&#39064;&#26102;&#21521;&#25945;&#24072;&#25552;&#38382;&#25110;&#23547;&#27714;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#32771;&#34385;&#27599;&#26465;&#28040;&#24687;&#30340;&#26102;&#38388;&#65292;&#38405;&#35835;&#21644;&#22238;&#22797;&#23398;&#29983;&#30340;&#38382;&#39064;&#38590;&#20197;&#25193;&#23637;&#12290;&#32467;&#26524;&#65292;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#21487;&#33021;&#24471;&#19981;&#21040;&#35299;&#20915;&#65292;&#23398;&#29983;&#21487;&#33021;&#22833;&#21435;&#32487;&#32493;&#23398;&#20064;&#30340;&#21160;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#33021;&#22815;&#33258;&#21160;&#30830;&#23450;&#27599;&#20010;&#35770;&#22363;&#24086;&#23376;&#32039;&#24613;&#24615;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#20415;&#21487;&#20197;&#23558;&#36825;&#20123;&#24086;&#23376;&#24102;&#32473;&#25945;&#24072;&#30340;&#27880;&#24847;&#12290;&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#19981;&#20165;&#20108;&#36827;&#21046;&#20915;&#31574;&#20999;&#21106;&#28857;&#65292;&#32780;&#19988;&#36824;&#39044;&#27979;&#20102;&#24086;&#23376;&#22312;7&#20998;&#21046;&#32039;&#24613;&#31243;&#24230;&#19978;&#30340;&#32423;&#21035;&#65292;&#20174;&#32780;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#23486;&#22805;&#27861;&#23612;&#20122;&#22823;&#23398;&#30340;MOOCs&#30340;3503&#26465;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#20132;&#21449;&#39564;&#35777;&#20102;&#22810;&#20010;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#30830;&#23450;&#25105;&#20204;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;29,604&#26465;&#24086;&#23376;&#30340;&#20808;&#21069;&#21457;&#34920;&#30340;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Students who take an online course, such as a MOOC, use the course's discussion forum to ask questions or reach out to instructors when encountering an issue. However, reading and responding to students' questions is difficult to scale because of the time needed to consider each message. As a result, critical issues may be left unresolved, and students may lose the motivation to continue in the course. To help address this problem, we build predictive models that automatically determine the urgency of each forum post, so that these posts can be brought to instructors' attention. This paper goes beyond previous work by predicting not just a binary decision cut-off but a post's level of urgency on a 7-point scale. First, we train and cross-validate several models on an original data set of 3,503 posts from MOOCs at University of Pennsylvania. Second, to determine the generalizability of our models, we test their performance on a separate, previously published data set of 29,604 posts fro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26597;&#35810;&#37325;&#28857;&#25688;&#35201; (QFS) &#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; QontSum &#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#24110;&#21161;&#27169;&#22411;&#38598;&#20013;&#27880;&#24847;&#21147;&#20110;&#36755;&#20837;&#25991;&#26723;&#20013;&#26368;&#30456;&#20851;&#30340;&#21306;&#22495;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#25110;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07586</link><description>&lt;p&gt;
QontSum: &#23545;&#27604;&#31361;&#20986;&#26597;&#35810;&#37325;&#28857;&#30340;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
QontSum: On Contrasting Salient Content for Query-focused Summarization. (arXiv:2307.07586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26597;&#35810;&#37325;&#28857;&#25688;&#35201; (QFS) &#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; QontSum &#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#24110;&#21161;&#27169;&#22411;&#38598;&#20013;&#27880;&#24847;&#21147;&#20110;&#36755;&#20837;&#25991;&#26723;&#20013;&#26368;&#30456;&#20851;&#30340;&#21306;&#22495;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#25110;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#37325;&#28857;&#25688;&#35201; (QFS) &#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#29983;&#25104;&#20197;&#35299;&#20915;&#29305;&#23450;&#26597;&#35810;&#20026;&#30446;&#30340;&#30340;&#25688;&#35201;&#12290;&#26356;&#24191;&#27867;&#30340;&#29983;&#25104;&#24335;&#20449;&#24687;&#26816;&#32034; (Gen-IR) &#39046;&#22495;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#24335;&#26041;&#27861;&#25913;&#21464;&#20174;&#24222;&#22823;&#30340;&#25991;&#26723;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#26041;&#24335;&#65292;&#21253;&#25324;&#29983;&#25104;&#24335;&#25991;&#26723;&#26816;&#32034; (GDR) &#21644;&#22522;&#30784;&#31572;&#26696;&#26816;&#32034; (GAR)&#12290;&#26412;&#25991;&#24378;&#35843;&#20102; QFS &#22312;&#22522;&#30784;&#31572;&#26696;&#29983;&#25104; (GAR) &#20013;&#30340;&#20316;&#29992;&#65292;&#36825;&#26159; Gen-IR &#30340;&#19968;&#20010;&#20851;&#38190;&#23376;&#39046;&#22495;&#65292;&#23427;&#20135;&#29983;&#19982;&#26597;&#35810;&#30452;&#25509;&#23545;&#24212;&#12289;&#20197;&#30456;&#20851;&#25991;&#26723;&#20026;&#22522;&#30784;&#30340;&#21487;&#35835;&#24615;&#31572;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; QontSum&#65292;&#19968;&#31181;&#26032;&#39062;&#30340; QFS &#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#24110;&#21161;&#27169;&#22411;&#38598;&#20013;&#27880;&#24847;&#21147;&#20110;&#36755;&#20837;&#25991;&#26723;&#20013;&#26368;&#30456;&#20851;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#22312;&#19968;&#20123; QFS &#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#35201;&#20040;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#35201;&#20040;&#22312;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query-focused summarization (QFS) is a challenging task in natural language processing that generates summaries to address specific queries. The broader field of Generative Information Retrieval (Gen-IR) aims to revolutionize information extraction from vast document corpora through generative approaches, encompassing Generative Document Retrieval (GDR) and Grounded Answer Retrieval (GAR). This paper highlights the role of QFS in Grounded Answer Generation (GAR), a key subdomain of Gen-IR that produces human-readable answers in direct correspondence with queries, grounded in relevant documents. In this study, we propose QontSum, a novel approach for QFS that leverages contrastive learning to help the model attend to the most relevant regions of the input document. We evaluate our approach on a couple of benchmark datasets for QFS and demonstrate that it either outperforms existing state-of-the-art or exhibits a comparable performance with considerably reduced computational cost through
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512; Twitter&#65292;&#28145;&#20837;&#20102;&#35299;&#38271;&#26399; COVID &#24739;&#32773;&#30340;&#24773;&#32490;&#21644;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#65292;&#21457;&#29616;&#36127;&#38754;&#24773;&#32490;&#20027;&#23548;&#65292;&#24182;&#19982;&#20851;&#38190;&#26102;&#26399;&#30456;&#20851;&#12290;&#36825;&#23545;&#21046;&#23450;&#24212;&#23545;&#38271;&#26399; COVID &#24739;&#32773;&#24515;&#29702;&#20581;&#24247;&#25361;&#25112;&#30340;&#25919;&#31574;&#21644;&#25514;&#26045;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.07558</link><description>&lt;p&gt;
&#36890;&#36807; Twitter &#20998;&#26512;&#25506;&#32034;&#38271;&#26399; COVID &#24739;&#32773;&#30340;&#24773;&#32490;&#21644;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;
&lt;/p&gt;
&lt;p&gt;
Exploring the Emotional and Mental Well-Being of Individuals with Long COVID Through Twitter Analysis. (arXiv:2307.07558v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07558
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512; Twitter&#65292;&#28145;&#20837;&#20102;&#35299;&#38271;&#26399; COVID &#24739;&#32773;&#30340;&#24773;&#32490;&#21644;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#65292;&#21457;&#29616;&#36127;&#38754;&#24773;&#32490;&#20027;&#23548;&#65292;&#24182;&#19982;&#20851;&#38190;&#26102;&#26399;&#30456;&#20851;&#12290;&#36825;&#23545;&#21046;&#23450;&#24212;&#23545;&#38271;&#26399; COVID &#24739;&#32773;&#24515;&#29702;&#20581;&#24247;&#25361;&#25112;&#30340;&#25919;&#31574;&#21644;&#25514;&#26045;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19 &#30123;&#24773;&#23548;&#33268;&#20102;&#38271;&#26399; COVID&#65292;&#36825;&#26159;&#22312;&#24863;&#26579;&#21518;&#25345;&#32493;&#23384;&#22312;&#30340;&#19968;&#31995;&#21015;&#30151;&#29366;&#12290;&#38271;&#26399; COVID &#24739;&#32773;&#21487;&#33021;&#36824;&#20250;&#38754;&#20020;&#24515;&#29702;&#20581;&#24247;&#25361;&#25112;&#65292;&#22240;&#27492;&#20102;&#35299;&#20010;&#20307;&#30340;&#24773;&#32490;&#21644;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#28145;&#20837;&#20102;&#35299;&#38271;&#26399; COVID &#24739;&#32773;&#30340;&#24773;&#32490;&#21644;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#65292;&#30830;&#23450;&#20182;&#20204;&#26368;&#20851;&#27880;&#30340;&#20027;&#39064;&#65292;&#24182;&#25506;&#32034;&#20182;&#20204;&#30340;&#24773;&#32490;&#19982;&#31038;&#20132;&#23186;&#20307;&#27963;&#21160;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#32852;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26681;&#25454;&#20869;&#23481;&#23558;&#25512;&#25991;&#20998;&#31867;&#20026;&#22235;&#20010;&#31867;&#21035;&#65292;&#26816;&#27979;&#20854;&#20013;&#30340;&#20845;&#31181;&#22522;&#26412;&#24773;&#32490;&#65292;&#24182;&#25552;&#21462;&#24120;&#35265;&#20027;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25972;&#20010;&#30740;&#31350;&#26399;&#38388;&#65292;&#36127;&#38754;&#24773;&#32490;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#24182;&#22312;&#20851;&#38190;&#26102;&#26399;&#65288;&#22914;&#26032;&#20896;&#21464;&#31181;&#29190;&#21457;&#65289;&#36798;&#21040;&#20004;&#20010;&#39640;&#23792;&#12290;&#26412;&#30740;&#31350;&#30340;&#21457;&#29616;&#23545;&#21046;&#23450;&#25919;&#31574;&#21644;&#25514;&#26045;&#20197;&#24212;&#23545;&#38271;&#26399; COVID &#24739;&#32773;&#30340;&#24515;&#29702;&#20581;&#24247;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has led to the emergence of Long COVID, a cluster of symptoms that persist after infection. Long COVID patients may also experience mental health challenges, making it essential to understand individuals' emotional and mental well-being. This study aims to gain a deeper understanding of Long COVID individuals' emotional and mental well-being, identify the topics that most concern them, and explore potential correlations between their emotions and social media activity. Specifically, we classify tweets into four categories based on the content, detect the presence of six basic emotions, and extract prevalent topics. Our analyses reveal that negative emotions dominated throughout the study period, with two peaks during critical periods, such as the outbreak of new COVID variants. The findings of this study have implications for policy and measures for addressing the mental health challenges of individuals with Long COVID and provide a foundation for future work.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#35780;&#20272;&#21592;&#21644;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#35780;&#20272;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07544</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#65306;&#36890;&#36807;&#25166;&#26681;&#30693;&#35782;&#25552;&#39640;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Dialogue System for Assessing Activities of Daily Living: Improving Consistency with Grounded Knowledge. (arXiv:2307.07544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07544
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#35780;&#20272;&#21592;&#21644;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#35780;&#20272;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#33258;&#25105;&#29031;&#39038;&#33021;&#21147;&#20307;&#29616;&#22312;&#8220;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#65288;ADL&#65289;&#8221;&#20013;&#65292;ADL&#20316;&#20026;&#21151;&#33021;&#33021;&#21147;&#65288;&#36816;&#20316;&#33021;&#21147;&#65289;&#30340;&#34913;&#37327;&#12290;&#21151;&#33021;&#33021;&#21147;&#19981;&#36275;&#21487;&#33021;&#23548;&#33268;&#38656;&#35201;&#20010;&#20154;&#25252;&#29702;&#21644;&#21327;&#21161;&#30340;&#24694;&#21155;&#29983;&#27963;&#26465;&#20214;&#12290;&#20026;&#20102;&#20934;&#30830;&#35782;&#21035;&#38656;&#35201;&#25903;&#25345;&#30340;&#20154;&#65292;&#21327;&#21161;&#35745;&#21010;&#20250;&#25345;&#32493;&#35780;&#20272;&#21442;&#19982;&#32773;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21151;&#33021;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#27700;&#24179;&#30340;&#19987;&#23478;&#35780;&#20272;&#21592;&#26102;&#65292;&#35780;&#20272;&#36807;&#31243;&#21487;&#33021;&#36935;&#21040;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#23588;&#20854;&#26159;&#21021;&#23398;&#32773;&#35780;&#20272;&#21592;&#21487;&#33021;&#32570;&#20047;&#19982;&#21442;&#19982;&#32773;&#36827;&#34892;&#23454;&#38469;&#20114;&#21160;&#25152;&#38656;&#30340;&#24517;&#35201;&#20934;&#22791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#23545;&#35805;&#31995;&#32479;&#65292;&#20197;&#33258;&#28982;&#19988;&#21487;&#37325;&#29616;&#30340;&#26041;&#24335;&#27169;&#25311;&#35780;&#20272;&#21592;&#21644;&#20855;&#26377;&#19981;&#21516;&#21151;&#33021;&#33021;&#21147;&#30340;&#20010;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#23545;&#35805;&#31995;&#32479;&#30001;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In healthcare, the ability to care for oneself is reflected in the "Activities of Daily Living (ADL)," which serve as a measure of functional ability (functioning). A lack of functioning may lead to poor living conditions requiring personal care and assistance. To accurately identify those in need of support, assistance programs continuously evaluate participants' functioning across various domains. However, the assessment process may encounter consistency issues when multiple assessors with varying levels of expertise are involved. Novice assessors, in particular, may lack the necessary preparation for real-world interactions with participants. To address this issue, we developed a dialogue system that simulates interactions between assessors and individuals of varying functioning in a natural and reproducible way. The dialogue system consists of two major modules, one for natural language understanding (NLU) and one for natural language generation (NLG), respectively. In order to gen
&lt;/p&gt;</description></item><item><title>PapagAI&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25945;&#23398;&#29702;&#35770;&#24182;&#23454;&#29616;&#20026;&#28151;&#21512;AI&#31995;&#32479;&#30340;&#24320;&#28304;&#33258;&#21160;&#21453;&#39304;&#24037;&#20855;&#65292;&#26088;&#22312;&#25552;&#39640;&#23398;&#29983;&#23398;&#20064;&#25104;&#26524;&#24182;&#34917;&#20805;&#35762;&#24072;&#30340;&#25945;&#23398;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2307.07523</link><description>&lt;p&gt;
PapagAI&#65306;&#21453;&#24605;&#24615;&#25991;&#31456;&#30340;&#33258;&#21160;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
PapagAI:Automated Feedback for Reflective Essays. (arXiv:2307.07523v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07523
&lt;/p&gt;
&lt;p&gt;
PapagAI&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25945;&#23398;&#29702;&#35770;&#24182;&#23454;&#29616;&#20026;&#28151;&#21512;AI&#31995;&#32479;&#30340;&#24320;&#28304;&#33258;&#21160;&#21453;&#39304;&#24037;&#20855;&#65292;&#26088;&#22312;&#25552;&#39640;&#23398;&#29983;&#23398;&#20064;&#25104;&#26524;&#24182;&#34917;&#20805;&#35762;&#24072;&#30340;&#25945;&#23398;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#31561;&#25945;&#32946;&#26399;&#38388;&#65292;&#25776;&#20889;&#21453;&#24605;&#24615;&#23454;&#36341;&#26159;&#39044;&#22791;&#25945;&#24072;&#36827;&#34892;&#30340;&#24120;&#35268;&#32451;&#20064;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#20182;&#20204;&#30340;&#35762;&#24072;&#38656;&#35201;&#25552;&#20379;&#20010;&#21035;&#21453;&#39304;&#65292;&#36825;&#21487;&#33021;&#26159;&#19968;&#39033;&#24120;&#35268;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#25945;&#23398;&#29702;&#35770;&#24182;&#23454;&#29616;&#20026;&#28151;&#21512;AI&#31995;&#32479;&#30340;&#24320;&#28304;&#33258;&#21160;&#21453;&#39304;&#24037;&#20855;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#32452;&#20214;&#65292;&#24182;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23454;&#29616;&#23398;&#29983;&#30340;&#26356;&#22909;&#23398;&#20064;&#32467;&#26524;&#65292;&#24182;&#34917;&#20805;&#35762;&#24072;&#30340;&#25945;&#23398;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Written reflective practice is a regular exercise pre-service teachers perform during their higher education. Usually, their lecturers are expected to provide individual feedback, which can be a challenging task to perform on a regular basis. In this paper, we present the first open-source automated feedback tool based on didactic theory and implemented as a hybrid AI system. We describe the components and discuss the advantages and disadvantages of our system compared to the state-of-art generative large language models. The main objective of our work is to enable better learning outcomes for students and to complement the teaching activities of lecturers.
&lt;/p&gt;</description></item><item><title>&#29992;&#20154;&#24037;&#26234;&#33021;&#32763;&#35793;&#24037;&#20855;&#21487;&#20197;&#35299;&#20915;&#30740;&#31350;&#26089;&#26399;&#31185;&#23398;&#25991;&#29486;&#30340;&#25289;&#19969;&#25991;&#32763;&#35793;&#38590;&#39064;&#65292;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#21457;&#29616;ChatGPT&#31639;&#27861;&#22312;&#34920;&#29616;&#19978;&#26356;&#20986;&#33394;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#32422;&#32752;&#183;&#20271;&#21162;&#21033;&#32473;&#27431;&#25289;&#30340;&#19968;&#23553;&#20449;&#30340;&#32763;&#35793;&#12290;</title><link>http://arxiv.org/abs/2307.07520</link><description>&lt;p&gt;
&#29992;&#20154;&#24037;&#26234;&#33021;&#32763;&#35793;&#25289;&#19969;&#25991;
&lt;/p&gt;
&lt;p&gt;
Translating Latin with Artificial Intelligence. (arXiv:2307.07520v1 [math.HO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07520
&lt;/p&gt;
&lt;p&gt;
&#29992;&#20154;&#24037;&#26234;&#33021;&#32763;&#35793;&#24037;&#20855;&#21487;&#20197;&#35299;&#20915;&#30740;&#31350;&#26089;&#26399;&#31185;&#23398;&#25991;&#29486;&#30340;&#25289;&#19969;&#25991;&#32763;&#35793;&#38590;&#39064;&#65292;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#21457;&#29616;ChatGPT&#31639;&#27861;&#22312;&#34920;&#29616;&#19978;&#26356;&#20986;&#33394;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#32422;&#32752;&#183;&#20271;&#21162;&#21033;&#32473;&#27431;&#25289;&#30340;&#19968;&#23553;&#20449;&#30340;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#26089;&#26399;&#31185;&#23398;&#25991;&#29486;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;&#25289;&#19969;&#25991;&#20316;&#21697;&#30340;&#29616;&#20195;&#35821;&#35328;&#32763;&#35793;&#30340;&#21487;&#33719;&#24471;&#24615;&#12290;&#36825;&#23588;&#20854;&#36866;&#29992;&#20110;&#27431;&#25289;&#30340;&#20316;&#21697;&#65292;&#20182;&#25776;&#20889;&#20102;&#32422;850&#20221;&#25163;&#31295;&#65292;&#20889;&#20102;&#19968;&#21315;&#23553;&#20449;&#24182;&#25910;&#21040;&#20102;&#36817;&#20004;&#21315;&#23553;&#22238;&#20449;&#12290;&#36825;&#20123;&#25163;&#31295;&#12289;&#20070;&#31821;&#21644;&#20449;&#20214;&#30340;&#32763;&#35793;&#24050;&#32463;&#22312;&#36807;&#21435;&#20004;&#20010;&#19990;&#32426;&#20013;&#21457;&#24067;&#22312;&#21508;&#31181;&#26469;&#28304;&#20013;&#65292;&#20294;&#36824;&#26377;&#35768;&#22810;&#23578;&#26410;&#20986;&#29256;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#29616;&#22914;&#20170;&#21487;&#20197;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#32763;&#35793;&#25216;&#26415;&#26469;&#35299;&#20915;&#32763;&#35793;&#22914;&#27492;&#22823;&#37327;&#25991;&#26412;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#19968;&#24037;&#20855;&#65292;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#20197;&#27604;&#36739;&#20004;&#31181;&#27969;&#34892;&#30340;&#20154;&#24037;&#26234;&#33021;&#32763;&#35793;&#31639;&#27861;&#65292;&#21363;&#35895;&#27468;&#32763;&#35793;&#21644;ChatGPT&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#21457;&#29616;ChatGPT&#22312;&#36825;&#20123;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#22240;&#27492;&#35813;&#32763;&#35793;&#25903;&#25345;&#24037;&#20855;&#38543;&#21518;&#34987;&#29992;&#20110;&#23558;&#32422;&#32752;&#183;&#20271;&#21162;&#21033;&#22312;1739&#24180;&#20889;&#32473;&#27431;&#25289;&#30340;&#19968;&#23553;&#20449;&#30340;&#25688;&#24405;&#36827;&#34892;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
The major hindrance in the study of earlier scientific literature is the availability of Latin translations into modern languages. This is particular true for the works of Euler who authored about 850 manuscripts and wrote a thousand letters and received back almost two thousand more. The translation of many of these manuscripts, books and letters have been published in various sources over the last two centuries, but many more have not yet appeared. Fortunately, nowadays, the artificial intelligence AI translation can be used to circumvent the challenges of translating such substantial number of texts. To validate this tool, benchmark tests have been performed to compare the performance of two popular AI translating algorithms, namely Google Translate and ChatGPT. Since it was found that ChatGPT performed better on these tests, this translating support was then used on an excerpt of a 1739 letter from Johann Bernoulli to Euler, where he notifies that he was sending to Euler the first 
&lt;/p&gt;</description></item><item><title>CephGPT-4&#26159;&#19968;&#20010;&#20855;&#26377;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#22810;&#27169;&#24577;&#39045;&#39052;&#27979;&#37327;&#19982;&#35786;&#26029;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#33258;&#21160;&#20998;&#26512;&#39045;&#39052;&#26631;&#24535;&#28857;&#21644;&#29983;&#25104;&#35786;&#26029;&#25253;&#21578;&#65292;&#23454;&#29616;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20026;&#27491;&#30072;&#27979;&#37327;&#21644;&#35786;&#26029;&#24212;&#29992;&#24102;&#26469;&#38761;&#21629;&#24615;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07518</link><description>&lt;p&gt;
CephGPT-4:&#19968;&#31181;&#20855;&#26377;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#22810;&#27169;&#24577;&#39045;&#39052;&#27979;&#37327;&#19982;&#35786;&#26029;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
CephGPT-4: An Interactive Multimodal Cephalometric Measurement and Diagnostic System with Visual Large Language Model. (arXiv:2307.07518v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07518
&lt;/p&gt;
&lt;p&gt;
CephGPT-4&#26159;&#19968;&#20010;&#20855;&#26377;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#22810;&#27169;&#24577;&#39045;&#39052;&#27979;&#37327;&#19982;&#35786;&#26029;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#33258;&#21160;&#20998;&#26512;&#39045;&#39052;&#26631;&#24535;&#28857;&#21644;&#29983;&#25104;&#35786;&#26029;&#25253;&#21578;&#65292;&#23454;&#29616;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20026;&#27491;&#30072;&#27979;&#37327;&#21644;&#35786;&#26029;&#24212;&#29992;&#24102;&#26469;&#38761;&#21629;&#24615;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#20110;&#22810;&#27169;&#24577;&#39045;&#39052;&#21307;&#23398;&#25968;&#25454;&#30340;&#35786;&#26029;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#32034;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#39045;&#39052;&#20998;&#26512;&#21644;&#35786;&#26029;&#23545;&#35805;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#26500;&#24314;&#20102;&#22810;&#27169;&#24577;&#27491;&#30072;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#39045;&#39052;&#24433;&#20687;&#21644;&#21307;&#24739;&#23545;&#35805;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;U-net&#33258;&#21160;&#20998;&#26512;&#39045;&#39052;&#26631;&#24535;&#28857;&#21644;&#29983;&#25104;&#35786;&#26029;&#25253;&#21578;&#12290;&#28982;&#21518;&#65292;&#23558;&#39045;&#39052;&#25968;&#25454;&#38598;&#21644;&#29983;&#25104;&#30340;&#35786;&#26029;&#25253;&#21578;&#20998;&#21035;&#22312;Minigpt-4&#21644;VisualGLM&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CephGPT-4&#27169;&#22411;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#39072;&#35206;&#27491;&#30072;&#27979;&#37327;&#21644;&#35786;&#26029;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#21019;&#26032;&#22312;&#27491;&#30072;&#23398;&#39046;&#22495;&#20855;&#26377;&#38761;&#21629;&#24615;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale multimodal language models (LMMs) have achieved remarkable success in general domains. However, the exploration of diagnostic language models based on multimodal cephalometric medical data remains limited. In this paper, we propose a novel multimodal cephalometric analysis and diagnostic dialogue model. Firstly, a multimodal orthodontic medical dataset is constructed, comprising cephalometric images and doctor-patient dialogue data, with automatic analysis of cephalometric landmarks using U-net and generation of diagnostic reports. Then, the cephalometric dataset and generated diagnostic reports are separately fine-tuned on Minigpt-4 and VisualGLM. Results demonstrate that the CephGPT-4 model exhibits excellent performance and has the potential to revolutionize orthodontic measurement and diagnostic applications. These innovations hold revolutionary application potential in the field of orthodontics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#65292;&#36890;&#36807;&#35270;&#39057;&#30340;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#27450;&#39575;&#26816;&#27979;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.07516</link><description>&lt;p&gt;
&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#27169;&#24577;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Voting-based Multimodal Automatic Deception Detection. (arXiv:2307.07516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#65292;&#36890;&#36807;&#35270;&#39057;&#30340;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#27450;&#39575;&#26816;&#27979;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#19968;&#30452;&#26159;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#26816;&#27979;&#27450;&#39575;&#32473;&#36825;&#19968;&#26087;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#20809;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35270;&#39057;&#20013;&#20351;&#29992;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#21035;&#26159;&#23494;&#27463;&#26681;&#22823;&#23398;&#30340;&#30495;&#23454;&#35797;&#39564;&#25968;&#25454;&#38598;&#21644;&#36808;&#38463;&#23494;&#22823;&#23398;&#30340;&#27450;&#39575;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#35270;&#39057;&#26679;&#26412;&#34987;&#20998;&#25104;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#25163;&#31295;&#30340;&#24103;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22810;&#27169;&#24577;&#25237;&#31080;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#19977;&#20010;&#27169;&#22411;&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#26159;&#29992;&#20110;&#20174;&#22270;&#20687;&#20013;&#26816;&#27979;&#27450;&#39575;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#31532;&#20108;&#20010;&#27169;&#22411;&#26159;&#29992;&#20110;&#20174;&#38899;&#39057;&#20013;&#26816;&#27979;&#27450;&#39575;&#30340;Mel&#39057;&#35889;&#22270;&#19978;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#31532;&#19977;&#20010;&#27169;&#22411;&#26159;&#29992;&#20110;&#20174;&#25163;&#31295;&#20013;&#26816;&#27979;&#27450;&#39575;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#19978;&#30340;Word2Vec&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#19978;&#21462;&#24471;&#30340;&#26368;&#20339;&#32467;&#26524;&#20998;&#21035;&#20026;97&#65285;&#12289;96&#65285;&#12289;9
&lt;/p&gt;
&lt;p&gt;
Automatic Deception Detection has been a hot research topic for a long time, using machine learning and deep learning to automatically detect deception, brings new light to this old field. In this paper, we proposed a voting-based method for automatic deception detection from videos using audio, visual and lexical features. Experiments were done on two datasets, the Real-life trial dataset by Michigan University and the Miami University deception detection dataset. Video samples were split into frames of images, audio, and manuscripts. Our Voting-based Multimodal proposed solution consists of three models. The first model is CNN for detecting deception from images, the second model is Support Vector Machine (SVM) on Mel spectrograms for detecting deception from audio and the third model is Word2Vec on Support Vector Machine (SVM) for detecting deception from manuscripts. Our proposed solution outperforms state of the art. Best results achieved on images, audio and text were 97%, 96%, 9
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#25918;&#23556;&#23398;&#25253;&#21578;&#21644;&#22270;&#20687;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#30340;&#27515;&#20129;&#29575;&#65292;&#24182;&#22312;MIMIC-IV&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;0.7829&#30340;&#24179;&#22343;C-index&#12290;</title><link>http://arxiv.org/abs/2307.07513</link><description>&lt;p&gt;
&#20351;&#29992;&#25918;&#23556;&#23398;&#25253;&#21578;&#21644;&#22270;&#20687;&#25913;&#21892;ICU&#27515;&#20129;&#29575;&#39044;&#27979;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An empirical study of using radiology reports and images to improve ICU mortality prediction. (arXiv:2307.07513v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#25918;&#23556;&#23398;&#25253;&#21578;&#21644;&#22270;&#20687;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#30340;&#27515;&#20129;&#29575;&#65292;&#24182;&#22312;MIMIC-IV&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;0.7829&#30340;&#24179;&#22343;C-index&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#39044;&#27979;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#30340;&#35780;&#20998;&#31995;&#32479;&#22312;ICU&#31649;&#29702;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#33021;&#39044;&#27979;&#37325;&#35201;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#26159;&#27515;&#20129;&#29575;&#12290;&#35768;&#22810;&#35780;&#20998;&#31995;&#32479;&#24050;&#32463;&#22312;ICU&#20013;&#24320;&#21457;&#21644;&#20351;&#29992;&#12290;&#36825;&#20123;&#35780;&#20998;&#31995;&#32479;&#20027;&#35201;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#30340;&#32467;&#26500;&#21270;&#20020;&#24202;&#25968;&#25454;&#65292;&#20294;&#21487;&#33021;&#20250;&#20007;&#22833;&#21465;&#36848;&#21644;&#22270;&#20687;&#20013;&#30340;&#37325;&#35201;&#20020;&#24202;&#20449;&#24687;&#12290;&#26041;&#27861;&#65306;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#26469;&#39044;&#27979;ICU&#27515;&#20129;&#29575;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#22235;&#32452;&#29305;&#24449;&#65306;&#65288;1&#65289;&#31616;&#21270;&#24613;&#24615;&#29983;&#29702;&#23398;&#35780;&#20998;&#65288;SAPS&#65289;II&#30340;&#29983;&#29702;&#27979;&#37327;&#12289;&#65288;2&#65289;&#25918;&#23556;&#19987;&#23478;&#39044;&#23450;&#20041;&#30340;&#24120;&#35265;&#33016;&#37096;&#30142;&#30149;&#12289;&#65288;3&#65289;&#22522;&#20110;BERT&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#65288;4&#65289;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;Medical Information Mart for Intensive Care IV&#65288;MIMIC-IV&#65289;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;0.7829&#30340;&#24179;&#22343;C-index&#65288;95%&#30340;&#32622;&#20449;&#21306;&#38388;&#65289;
&lt;/p&gt;
&lt;p&gt;
Background: The predictive Intensive Care Unit (ICU) scoring system plays an important role in ICU management because it predicts important outcomes, especially mortality. Many scoring systems have been developed and used in the ICU. These scoring systems are primarily based on the structured clinical data in the electronic health record (EHR), which may suffer the loss of important clinical information in the narratives and images. Methods: In this work, we build a deep learning based survival prediction model with multi-modality data to predict ICU mortality. Four sets of features are investigated: (1) physiological measurements of Simplified Acute Physiology Score (SAPS) II, (2) common thorax diseases pre-defined by radiologists, (3) BERT-based text representations, and (4) chest X-ray image features. We use the Medical Information Mart for Intensive Care IV (MIMIC-IV) dataset to evaluate the proposed model. Results: Our model achieves the average C-index of 0.7829 (95% confidence i
&lt;/p&gt;</description></item><item><title>RoPDA&#26159;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;NER&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#36830;&#32493;&#25552;&#31034;&#36827;&#34892;&#23454;&#20307;&#21644;&#19978;&#19979;&#25991;&#22686;&#24378;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#36807;&#28388;&#21644;&#28151;&#21512;&#25216;&#26415;&#20197;&#20248;&#21270;&#22686;&#24378;&#26679;&#26412;&#30340;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.07417</link><description>&lt;p&gt;
RoPDA&#65306;&#29992;&#20110;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#40065;&#26834;&#22522;&#20110;&#25552;&#31034;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
RoPDA: Robust Prompt-based Data Augmentation for Low-Resource Named Entity Recognition. (arXiv:2307.07417v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07417
&lt;/p&gt;
&lt;p&gt;
RoPDA&#26159;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;NER&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#36830;&#32493;&#25552;&#31034;&#36827;&#34892;&#23454;&#20307;&#21644;&#19978;&#19979;&#25991;&#22686;&#24378;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#36807;&#28388;&#21644;&#28151;&#21512;&#25216;&#26415;&#20197;&#20248;&#21270;&#22686;&#24378;&#26679;&#26412;&#30340;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#22312;&#20302;&#36164;&#28304;NER&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#23384;&#22312;&#30772;&#22351;&#21477;&#27861;&#32467;&#26500;&#12289;&#26631;&#35760;-&#26631;&#31614;&#19981;&#21305;&#37197;&#21644;&#23545;&#22806;&#37096;&#30693;&#35782;&#25110;&#25163;&#21160;&#24037;&#20316;&#30340;&#38656;&#27714;&#30340;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RoPDA: &#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;NER&#30340;&#40065;&#26834;&#22522;&#20110;&#25552;&#31034;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21644;&#36830;&#32493;&#25552;&#31034;&#65292;RoPDA&#36890;&#36807;&#20116;&#20010;&#22522;&#26412;&#30340;&#22686;&#24378;&#25805;&#20316;&#36827;&#34892;&#23454;&#20307;&#22686;&#24378;&#21644;&#19978;&#19979;&#25991;&#22686;&#24378;&#65292;&#29983;&#25104;&#26631;&#31614;&#32763;&#36716;&#21644;&#20445;&#30041;&#26631;&#31614;&#30340;&#26679;&#26412;&#12290;&#20026;&#20102;&#20248;&#21270;&#22686;&#24378;&#26679;&#26412;&#30340;&#21033;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25216;&#26415;&#65306;&#33258;&#19968;&#33268;&#24615;&#36807;&#28388;&#21644;&#28151;&#21512;&#12290;&#21069;&#32773;&#26377;&#25928;&#22320;&#28040;&#38500;&#20302;&#36136;&#37327;&#26679;&#26412;&#65292;&#21518;&#32773;&#38450;&#27490;&#30452;&#25509;&#21033;&#29992;&#26631;&#31614;&#32763;&#36716;&#26679;&#26412;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
Data augmentation has been widely used in low-resource NER tasks to tackle the problem of data sparsity. However, previous data augmentation methods have the disadvantages of disrupted syntactic structures, token-label mismatch, and requirement for external knowledge or manual effort. To address these issues, we propose \textbf{Ro}bust \textbf{P}rompt-based \textbf{D}ata \textbf{A}ugmentation (RoPDA) for low-resource NER. Based on pre-trained language models (PLMs) with continuous prompt, RoPDA performs entity augmentation and context augmentation through five fundamental augmentation operations to generate label-flipping and label-preserving examples. To optimize the utilization of the augmented samples, we present two techniques: Self-Consistency Filtering and mixup. The former effectively eliminates low-quality samples, while the latter prevents performance degradation arising from the direct utilization of label-flipping samples. Extensive experiments on three benchmarks from diffe
&lt;/p&gt;</description></item><item><title>Parmesan&#26159;&#19968;&#20010;&#21407;&#22411;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#19978;&#19979;&#25991;&#20013;&#25628;&#32034;&#21644;&#23450;&#20041;&#25968;&#23398;&#27010;&#24565;&#65292;&#29305;&#21035;&#20851;&#27880;&#33539;&#30068;&#35770;&#39046;&#22495;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32452;&#20214;&#36827;&#34892;&#27010;&#24565;&#25552;&#21462;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#23450;&#20041;&#25552;&#21462;&#21644;&#23454;&#20307;&#38142;&#25509;&#12290;&#36890;&#36807;&#35813;&#31995;&#32479;&#30340;&#24320;&#21457;&#65292;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#33539;&#30068;&#35770;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#25968;&#23398;&#35821;&#26009;&#24211;&#20197;&#25903;&#25345;&#31995;&#32479;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.06699</link><description>&lt;p&gt;
Parmesan&#65306;&#25945;&#32946;&#20013;&#30340;&#25968;&#23398;&#27010;&#24565;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Parmesan: mathematical concept extraction for education. (arXiv:2307.06699v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06699
&lt;/p&gt;
&lt;p&gt;
Parmesan&#26159;&#19968;&#20010;&#21407;&#22411;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#19978;&#19979;&#25991;&#20013;&#25628;&#32034;&#21644;&#23450;&#20041;&#25968;&#23398;&#27010;&#24565;&#65292;&#29305;&#21035;&#20851;&#27880;&#33539;&#30068;&#35770;&#39046;&#22495;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32452;&#20214;&#36827;&#34892;&#27010;&#24565;&#25552;&#21462;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#23450;&#20041;&#25552;&#21462;&#21644;&#23454;&#20307;&#38142;&#25509;&#12290;&#36890;&#36807;&#35813;&#31995;&#32479;&#30340;&#24320;&#21457;&#65292;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#33539;&#30068;&#35770;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#25968;&#23398;&#35821;&#26009;&#24211;&#20197;&#25903;&#25345;&#31995;&#32479;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#26159;&#19968;&#20010;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#39046;&#22495;&#65292;&#20855;&#26377;&#33258;&#24049;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#21364;&#26377;&#38480;&#12290;&#28982;&#32780;&#65292;&#25968;&#23398;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#20013;&#32463;&#24120;&#20381;&#36182;&#20110;&#23545;&#25968;&#23398;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#24110;&#21161;&#26469;&#33258;&#20854;&#20182;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21407;&#22411;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#19978;&#19979;&#25991;&#20013;&#25628;&#32034;&#21644;&#23450;&#20041;&#25968;&#23398;&#27010;&#24565;&#65292;&#37325;&#28857;&#20851;&#27880;&#33539;&#30068;&#35770;&#39046;&#22495;&#12290;&#36825;&#20010;&#31995;&#32479;&#21517;&#20026;Parmesan&#65292;&#20381;&#36182;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32452;&#20214;&#65292;&#21253;&#25324;&#27010;&#24565;&#25552;&#21462;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#23450;&#20041;&#25552;&#21462;&#21644;&#23454;&#20307;&#38142;&#25509;&#12290;&#22312;&#24320;&#21457;&#36825;&#20010;&#31995;&#32479;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#25216;&#26415;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#33539;&#30068;&#35770;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25216;&#26415;&#65292;&#36825;&#31181;&#25216;&#26415;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#25105;&#20204;&#39044;&#35745;&#31995;&#32479;&#23558;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#19981;&#26029;&#28436;&#21464;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#28165;&#29702;&#36807;&#30340;&#25968;&#23398;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#25903;&#25345;&#21407;&#22411;&#31995;&#32479;&#65292;&#36825;&#20123;&#35821;&#26009;&#24211;&#22522;&#20110;&#26399;&#21002;&#25991;&#31456;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematics is a highly specialized domain with its own unique set of challenges that has seen limited study in natural language processing. However, mathematics is used in a wide variety of fields and multidisciplinary research in many different domains often relies on an understanding of mathematical concepts. To aid researchers coming from other fields, we develop a prototype system for searching for and defining mathematical concepts in context, focusing on the field of category theory. This system, Parmesan, depends on natural language processing components including concept extraction, relation extraction, definition extraction, and entity linking. In developing this system, we show that existing techniques cannot be applied directly to the category theory domain, and suggest hybrid techniques that do perform well, though we expect the system to evolve over time. We also provide two cleaned mathematical corpora that power the prototype system, which are based on journal articles 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GLORY&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20840;&#23616;&#22270;&#19982;&#26412;&#22320;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26500;&#24314;&#20840;&#23616;&#24863;&#30693;&#21382;&#21490;&#26032;&#38395;&#32534;&#30721;&#22120;&#26469;&#34701;&#21512;&#21382;&#21490;&#26032;&#38395;&#34920;&#31034;&#65292;&#24182;&#32771;&#34385;&#20102;&#29992;&#25143;&#38544;&#34255;&#30340;&#21160;&#26426;&#21644;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.06576</link><description>&lt;p&gt;
&#36229;&#36234;&#26412;&#22320;&#33539;&#22260;&#65306;&#20840;&#29699;&#22270;&#22686;&#24378;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Going Beyond Local: Global Graph-Enhanced Personalized News Recommendations. (arXiv:2307.06576v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GLORY&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20840;&#23616;&#22270;&#19982;&#26412;&#22320;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26500;&#24314;&#20840;&#23616;&#24863;&#30693;&#21382;&#21490;&#26032;&#38395;&#32534;&#30721;&#22120;&#26469;&#34701;&#21512;&#21382;&#21490;&#26032;&#38395;&#34920;&#31034;&#65292;&#24182;&#32771;&#34385;&#20102;&#29992;&#25143;&#38544;&#34255;&#30340;&#21160;&#26426;&#21644;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#22320;&#21521;&#29992;&#25143;&#25512;&#33616;&#20505;&#36873;&#26032;&#38395;&#25991;&#31456;&#19968;&#30452;&#26159;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#30340;&#26680;&#24515;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#36817;&#26399;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20174;&#20016;&#23500;&#30340;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#29992;&#20174;&#26412;&#22320;&#21382;&#21490;&#26032;&#38395;&#27966;&#29983;&#30340;&#22522;&#20110;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#20840;&#23616;&#35270;&#35282;&#65292;&#26410;&#33021;&#32771;&#34385;&#29992;&#25143;&#38544;&#34255;&#30340;&#21160;&#26426;&#21644;&#34892;&#20026;&#65292;&#36229;&#36234;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411; GLORY&#65288;Global-LOcal news Recommendation sYstem&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#20174;&#20854;&#20182;&#29992;&#25143;&#23398;&#21040;&#30340;&#20840;&#23616;&#34920;&#31034;&#21644;&#26412;&#22320;&#34920;&#31034;&#65292;&#26469;&#22686;&#24378;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20840;&#23616;&#24863;&#30693;&#21382;&#21490;&#26032;&#38395;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#20840;&#23616;&#26032;&#38395;&#22270;&#65292;&#24182;&#20351;&#29992;&#38376;&#25511;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20016;&#23500;&#26032;&#38395;&#34920;&#31034;&#65292;&#20174;&#32780;&#36890;&#36807;&#21382;&#21490;&#26032;&#38395;&#32858;&#21512;&#22120;&#34701;&#21512;&#21382;&#21490;&#26032;&#38395;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precisely recommending candidate news articles to users has always been a core challenge for personalized news recommendation systems. Most recent works primarily focus on using advanced natural language processing techniques to extract semantic information from rich textual data, employing content-based methods derived from local historical news. However, this approach lacks a global perspective, failing to account for users' hidden motivations and behaviors beyond semantic information. To address this challenge, we propose a novel model called GLORY (Global-LOcal news Recommendation sYstem), which combines global representations learned from other users with local representations to enhance personalized recommendation systems. We accomplish this by constructing a Global-aware Historical News Encoder, which includes a global news graph and employs gated graph neural networks to enrich news representations, thereby fusing historical news representations by a historical news aggregator.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#32534;&#30721;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#26368;&#22823;&#30340;&#32534;&#30721;&#25361;&#25112;&#30446;&#24405;&#65292;&#37325;&#28857;&#20851;&#27880;Python&#32534;&#31243;&#35821;&#35328;&#21644;&#25968;&#25454;&#32467;&#26500;&#31639;&#27861;&#20004;&#20010;&#22522;&#30784;&#20027;&#39064;&#12290;&#24635;&#32467;&#27979;&#35797;&#20013;ChatGPT&#30340;&#20195;&#30721;&#35299;&#20915;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12289;&#20195;&#30721;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2307.05360</link><description>&lt;p&gt;
&#25581;&#24320;&#24040;&#20154;&#30340;&#30495;&#38754;&#30446;&#65306;&#23545;ChatGPT&#22312;&#32534;&#30721;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency in coding algorithms and data structures. (arXiv:2307.05360v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#32534;&#30721;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#26368;&#22823;&#30340;&#32534;&#30721;&#25361;&#25112;&#30446;&#24405;&#65292;&#37325;&#28857;&#20851;&#27880;Python&#32534;&#31243;&#35821;&#35328;&#21644;&#25968;&#25454;&#32467;&#26500;&#31639;&#27861;&#20004;&#20010;&#22522;&#30784;&#20027;&#39064;&#12290;&#24635;&#32467;&#27979;&#35797;&#20013;ChatGPT&#30340;&#20195;&#30721;&#35299;&#20915;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12289;&#20195;&#30721;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#28145;&#21051;&#22320;&#37325;&#22609;&#20102;&#20154;&#24037;&#26234;&#33021;(AI)&#25216;&#26415;&#39046;&#22495;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ChatGPT&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#26377;&#30528;&#29420;&#29305;&#20043;&#22788;&#65292;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#22810;&#36718;&#23545;&#35805;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#23637;&#31034;&#20986;&#23545;&#32534;&#30721;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#32534;&#30721;&#25361;&#25112;&#30446;&#24405;&#23545;ChatGPT&#30340;&#32534;&#30721;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;Python&#32534;&#31243;&#35821;&#35328;&#65292;&#20197;&#21450;&#38598;&#20013;&#22312;&#25968;&#25454;&#32467;&#26500;&#21644;&#31639;&#27861;&#19978;&#30340;&#38382;&#39064;&#65292;&#36825;&#20004;&#20010;&#20027;&#39064;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#35780;&#20272;ChatGPT&#35299;&#20915;&#25152;&#25552;&#20132;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#35780;&#20272;&#20854;&#20195;&#30721;&#36136;&#37327;&#20197;&#21450;&#20195;&#30721;&#24341;&#21457;&#30340;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#24615;&#36136;&#12290;&#24403;ChatGPT&#30340;&#20195;&#30721;&#25104;&#21151;&#25191;&#34892;&#20294;&#26410;&#33021;&#35299;&#20915;&#25163;&#22836;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#20250;&#30740;&#31350;&#36890;&#36807;&#30340;&#27979;&#35797;&#26696;&#20363;&#20013;&#30340;&#27169;&#24335;&#65292;&#20197;&#20102;&#35299;ChatGPT&#20195;&#30721;&#20013;&#30340;&#38169;&#35823;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformative influence of Large Language Models (LLMs) is profoundly reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT distinguishes itself within these models, demonstrating remarkable performance in multi-turn conversations and exhibiting code proficiency across an array of languages. In this paper, we carry out a comprehensive evaluation of ChatGPT's coding capabilities based on what is to date the largest catalog of coding challenges. Our focus is on the python programming language and problems centered on data structures and algorithms, two topics at the very foundations of Computer Science. We evaluate ChatGPT for its ability to generate correct solutions to the problems fed to it, its code quality, and nature of run-time errors thrown by its code. Where ChatGPT code successfully executes, but fails to solve the problem at hand, we look into patterns in the test cases passed in order to gain some insights into how wrong ChatGPT code is in these 
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#30001;OpenAI&#21019;&#24314;&#30340;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#38761;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#20844;&#20247;&#22823;&#35268;&#27169;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#20114;&#21160;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20063;&#24341;&#21457;&#20102;&#31867;&#20284;&#25216;&#26415;&#30340;&#30740;&#31350;&#20852;&#36259;&#21644;&#24212;&#29992;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.04251</link><description>&lt;p&gt;
ChatGPT&#22312;&#29983;&#25104;&#24335;AI&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#24212;&#29992;&#65306;&#19968;&#20221;&#31616;&#27905;&#30340;&#35843;&#26597;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey. (arXiv:2307.04251v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04251
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#21019;&#24314;&#30340;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#38761;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#20844;&#20247;&#22823;&#35268;&#27169;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#20114;&#21160;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20063;&#24341;&#21457;&#20102;&#31867;&#20284;&#25216;&#26415;&#30340;&#30740;&#31350;&#20852;&#36259;&#21644;&#24212;&#29992;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#21019;&#24314;&#30340;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#32463;&#36807;&#31934;&#24515;&#35757;&#32451;&#24182;&#20351;&#29992;&#20102;&#22823;&#37327;&#25968;&#25454;&#12290;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#38761;&#65292;&#24182;&#25512;&#21160;&#20102;LLM&#33021;&#21147;&#30340;&#36793;&#30028;&#12290;ChatGPT&#22312;&#22823;&#35268;&#27169;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#26222;&#36941;&#20844;&#20247;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#30340;&#20114;&#21160;&#65292;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#23427;&#36824;&#24341;&#21457;&#20102;&#24320;&#21457;&#31867;&#20284;&#25216;&#26415;&#21644;&#30740;&#31350;&#20854;&#24212;&#29992;&#21644;&#24433;&#21709;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;ChatGPT&#21450;&#20854;&#28436;&#21270;&#30340;&#24403;&#21069;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#31616;&#26126;&#35843;&#26597;&#12290;&#25105;&#20204;&#21516;&#26102;&#32771;&#34385;&#20102;ChatGPT&#30340;&#29627;&#29827;&#30418;&#21644;&#40657;&#30418;&#35270;&#35282;&#65292;&#21253;&#25324;&#25216;&#26415;&#30340;&#32452;&#25104;&#37096;&#20998;&#21644;&#22522;&#26412;&#35201;&#32032;&#65292;&#20197;&#21450;&#20854;&#24212;&#29992;&#12289;&#24433;&#21709;&#21644;&#24433;&#21709;&#12290;&#29627;&#29827;&#30418;&#26041;&#27861;&#30528;&#37325;&#20110;&#29702;&#35299;&#25216;&#26415;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#32780;&#40657;&#30418;&#26041;&#27861;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#22797;&#26434;&#31995;&#32479;&#65292;&#22240;&#27492;&#30740;&#31350;&#20854;&#36755;&#20837;&#65292;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a large language model (LLM) created by OpenAI that has been carefully trained on a large amount of data. It has revolutionized the field of natural language processing (NLP) and has pushed the boundaries of LLM capabilities. ChatGPT has played a pivotal role in enabling widespread public interaction with generative artificial intelligence (GAI) on a large scale. It has also sparked research interest in developing similar technologies and investigating their applications and implications. In this paper, our primary goal is to provide a concise survey on the current lines of research on ChatGPT and its evolution. We considered both the glass box and black box views of ChatGPT, encompassing the components and foundational elements of the technology, as well as its applications, impacts, and implications. The glass box approach focuses on understanding the inner workings of the technology, and the black box approach embraces it as a complex system, and thus examines its inputs,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;Speech-LLaMA&#65292;&#23558;&#22768;&#23398;&#20449;&#24687;&#26377;&#25928;&#22320;&#34701;&#20837;&#22522;&#20110;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#20351;&#29992;&#36830;&#25509;&#20027;&#20041;&#26102;&#24207;&#20998;&#31867;&#21644;&#31616;&#21333;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#23558;&#21387;&#32553;&#30340;&#22768;&#23398;&#29305;&#24449;&#26144;&#23556;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36830;&#32493;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#30340;&#23454;&#36136;&#24615;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.03917</link><description>&lt;p&gt;
&#20851;&#20110;&#20165;&#35299;&#30721;&#22120;&#26550;&#26500;&#22312;&#35821;&#38899;&#21040;&#25991;&#26412;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On decoder-only architecture for speech-to-text and large language model integration. (arXiv:2307.03917v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03917
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;Speech-LLaMA&#65292;&#23558;&#22768;&#23398;&#20449;&#24687;&#26377;&#25928;&#22320;&#34701;&#20837;&#22522;&#20110;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#20351;&#29992;&#36830;&#25509;&#20027;&#20041;&#26102;&#24207;&#20998;&#31867;&#21644;&#31616;&#21333;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#23558;&#21387;&#32553;&#30340;&#22768;&#23398;&#29305;&#24449;&#26144;&#23556;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36830;&#32493;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#30340;&#23454;&#36136;&#24615;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#26426;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#35821;&#38899;&#20449;&#21495;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#21516;&#26102;&#65292;&#20851;&#20110;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#30340;&#8220;&#20165;&#35299;&#30721;&#22120;&#8221;&#26550;&#26500;&#20063;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Speech-LLaMA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23558;&#22768;&#23398;&#20449;&#24687;&#34701;&#20837;&#22522;&#20110;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36830;&#25509;&#20027;&#20041;&#26102;&#24207;&#20998;&#31867;&#21644;&#31616;&#21333;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#23558;&#21387;&#32553;&#30340;&#22768;&#23398;&#29305;&#24449;&#26144;&#23556;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36830;&#32493;&#35821;&#20041;&#31354;&#38388;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#20165;&#35299;&#30721;&#22120;&#26550;&#26500;&#22312;&#35821;&#38899;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#35821;&#38899;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#36739;&#23567;&#35268;&#27169;&#12289;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;Speech-LLaMA&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#22810;&#35821;&#35328;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#19982;&#24378;&#22522;&#20934;&#30456;&#27604;&#26377;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The "decoder-only" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baseli
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;BERT&#27169;&#22411;&#20197;&#39044;&#27979;&#32473;&#23450;&#25991;&#26412;&#30340;&#34920;&#24773;&#31526;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#31038;&#20132;&#23186;&#20307;&#33829;&#38144;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.02054</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#27169;&#22411;&#39044;&#27979;&#34920;&#24773;&#31526;&#21495;
&lt;/p&gt;
&lt;p&gt;
Emoji Prediction using Transformer Models. (arXiv:2307.02054v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02054
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;BERT&#27169;&#22411;&#20197;&#39044;&#27979;&#32473;&#23450;&#25991;&#26412;&#30340;&#34920;&#24773;&#31526;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#31038;&#20132;&#23186;&#20307;&#33829;&#38144;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31038;&#20132;&#23186;&#20307;&#20013;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#30340;&#39057;&#29575;&#22823;&#24133;&#22686;&#21152;&#65292;&#20351;&#24471;&#23427;&#20204;&#25104;&#20026;&#20102;&#29702;&#35299;&#22312;&#32447;&#27807;&#36890;&#30340;&#37325;&#35201;&#20803;&#32032;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#21547;&#31946;&#30340;&#29305;&#24615;&#65292;&#39044;&#27979;&#32473;&#23450;&#25991;&#26412;&#20013;&#34920;&#24773;&#31526;&#21495;&#30340;&#21547;&#20041;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#26469;&#20351;&#29992;BERT&#36827;&#34892;&#34920;&#24773;&#31526;&#21495;&#39044;&#27979;&#65292;BERT&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;&#25991;&#26412;&#21644;&#34920;&#24773;&#31526;&#21495;&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#39044;&#27979;&#32473;&#23450;&#25991;&#26412;&#30340;&#26368;&#21512;&#36866;&#30340;&#34920;&#24773;&#31526;&#21495;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#34920;&#24773;&#31526;&#21495;&#26041;&#38754;&#30340;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;75&#65285;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#35813;&#30740;&#31350;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#31038;&#20132;&#23186;&#20307;&#33829;&#38144;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the use of emojis in social media has increased dramatically, making them an important element in understanding online communication. However, predicting the meaning of emojis in a given text is a challenging task due to their ambiguous nature. In this study, we propose a transformer-based approach for emoji prediction using BERT, a widely-used pre-trained language model. We fine-tuned BERT on a large corpus of text containing both text and emojis to predict the most appropriate emoji for a given text. Our experimental results demonstrate that our approach outperforms several state-of-the-art models in predicting emojis with an accuracy of over 75 percent. This work has potential applications in natural language processing, sentiment analysis, and social media marketing.
&lt;/p&gt;</description></item><item><title>InstructEval&#24320;&#21457;&#20102;&#19968;&#20010;&#35780;&#20272;&#22871;&#20214;&#65292;&#29992;&#20110;&#23545;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;&#31574;&#21010;&#30340;&#25163;&#21160;&#32534;&#20889;&#30340;&#25351;&#20196;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00259</link><description>&lt;p&gt;
InstructEval: &#31995;&#32479;&#35780;&#20272;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InstructEval: Systematic Evaluation of Instruction Selection Methods. (arXiv:2307.00259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00259
&lt;/p&gt;
&lt;p&gt;
InstructEval&#24320;&#21457;&#20102;&#19968;&#20010;&#35780;&#20272;&#22871;&#20214;&#65292;&#29992;&#20110;&#23545;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;&#31574;&#21010;&#30340;&#25163;&#21160;&#32534;&#20889;&#30340;&#25351;&#20196;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064; (ICL) &#36890;&#36807;&#20351;&#29992;&#25351;&#20196;&#21644;&#19968;&#23567;&#32452;&#27880;&#37322;&#31034;&#20363;&#26469;&#25552;&#31034;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26469;&#25191;&#34892;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#25552;&#31034;&#20013;&#20351;&#29992;&#30340;&#36755;&#20837;&#30340;&#32454;&#33410;&#23545; ICL &#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#65292;&#36825;&#28608;&#21169;&#20102;&#25351;&#20196;&#36873;&#25321;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#25351;&#20196;&#36873;&#25321;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#65292;&#29616;&#26377;&#30340;&#20998;&#26512;&#20165;&#38480;&#20110;&#27169;&#22411;&#21644;&#20219;&#21153;&#30340;&#27973;&#23618;&#23376;&#38598;&#65292;&#36825;&#38480;&#21046;&#20102;&#27934;&#23519;&#21147;&#30340;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010; ICL &#35780;&#20272;&#22871;&#20214;&#65292;&#20197;&#23545;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#35813;&#22871;&#20214;&#21253;&#25324;&#26469;&#33258;4&#20010;&#19981;&#21516;&#27169;&#22411;&#23478;&#26063;&#30340;13&#20010;&#24320;&#28304;LLM&#65292;&#28085;&#30422;9&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20195;&#34920;&#20102;3&#20010;&#20998;&#31867;&#20013;&#21508;&#31181;&#31867;&#22411;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#20102;7&#31181;&#21463;&#27426;&#36814;&#30340;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;&#30456;&#23545;&#20110;ICL&#30456;&#20851;&#30340;&#20116;&#39033;&#26399;&#26395;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#31574;&#21010;&#30340;&#25163;&#21160;&#32534;&#20889;&#30340;&#25351;&#20196;&#21487;&#20197;&#26174;&#33879;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) performs tasks by prompting a large language model (LLM) using an instruction and a small set of annotated examples called demonstrations. Recent work has shown that the precise details of the inputs used in the prompt significantly impacts ICL, which has incentivized instruction selection algorithms. The effect of instruction-choice however is severely underexplored, with existing analyses being restricted to shallow subsets of models and tasks, which limits the generalizability of their insights. We develop an ICL evaluation suite to conduct a thorough assessment of these techniques. The suite includes 13 open-sourced LLMs of varying scales from 4 distinct model families and covers 9 different tasks, representing a range of task types across 3 categories. In this work, we evaluate the relative performance of 7 popular instruction selection methods using our benchmark over five desiderata relevant to ICL. We discover that using curated manually-written instru
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#27169;&#25311;&#20154;&#31867;&#31867;&#20154;&#27010;&#24565;&#23398;&#20064;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#35758;&#20998;&#24067;&#24182;&#25311;&#21512;&#20808;&#39564;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#32773;&#65292;&#24182;&#22312;&#29983;&#25104;&#24615;&#21644;&#36923;&#36753;&#24615;&#27010;&#24565;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.02797</link><description>&lt;p&gt;
&#29992;&#36125;&#21494;&#26031;&#25512;&#29702;&#27169;&#25311;&#20154;&#31867;&#31867;&#20154;&#27010;&#24565;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Modeling Human-like Concept Learning with Bayesian Inference over Natural Language. (arXiv:2306.02797v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#27169;&#25311;&#20154;&#31867;&#31867;&#20154;&#27010;&#24565;&#23398;&#20064;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#35758;&#20998;&#24067;&#24182;&#25311;&#21512;&#20808;&#39564;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#32773;&#65292;&#24182;&#22312;&#29983;&#25104;&#24615;&#21644;&#36923;&#36753;&#24615;&#27010;&#24565;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#27169;&#25311;&#23545;&#25277;&#35937;&#31526;&#21495;&#27010;&#24565;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#39640;&#25928;&#25512;&#29702;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#35758;&#20998;&#24067;&#12290;&#25105;&#20204;&#26681;&#25454;&#20154;&#31867;&#25968;&#25454;&#25311;&#21512;&#20808;&#39564;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#32773;&#65292;&#24182;&#22312;&#29983;&#25104;&#24615;&#21644;&#36923;&#36753;&#24615;&#27010;&#24565;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We model learning of abstract symbolic concepts by performing Bayesian inference over utterances in natural language. For efficient inference, we use a large language model as a proposal distribution. We fit a prior to human data to better model human learners, and evaluate on both generative and logical concepts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20132;&#36890;&#39046;&#22495;&#24773;&#22659;&#25512;&#29702;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#22659;&#20915;&#31574;&#12289;&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#21644;&#35299;&#20915;&#20154;&#31867;&#39550;&#39542;&#32771;&#35797;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#22235;&#31181;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#65292;&#20855;&#26377;&#28508;&#21147;&#22312;&#19981;&#21516;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.02520</link><description>&lt;p&gt;
&#20132;&#36890;&#29702;&#35299;&#30340;&#24773;&#22659;&#25512;&#29702;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Situational Reasoning for Traffic Understanding. (arXiv:2306.02520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20132;&#36890;&#39046;&#22495;&#24773;&#22659;&#25512;&#29702;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#22659;&#20915;&#31574;&#12289;&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#21644;&#35299;&#20915;&#20154;&#31867;&#39550;&#39542;&#32771;&#35797;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#22235;&#31181;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#65292;&#20855;&#26377;&#28508;&#21147;&#22312;&#19981;&#21516;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20132;&#36890;&#30417;&#25511;(ITMo)&#25216;&#26415;&#26377;&#28508;&#21147;&#25913;&#21892;&#36947;&#36335;&#23433;&#20840;/&#23433;&#20840;&#24615;&#65292;&#23454;&#29616;&#26234;&#33021;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#12290;&#20102;&#35299;&#20132;&#36890;&#24773;&#20917;&#38656;&#35201;&#23558;&#24863;&#30693;&#20449;&#24687;&#19982;&#39046;&#22495;&#29305;&#23450;&#21644;&#22240;&#26524;&#24120;&#35782;&#30693;&#35782;&#22797;&#26434;&#34701;&#21512;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#20026;&#20132;&#36890;&#30417;&#25511;&#25552;&#20379;&#20102;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#20294;&#27169;&#22411;&#33021;&#21542;&#26377;&#25928;&#22320;&#23545;&#40784;&#36825;&#20123;&#20449;&#24687;&#26469;&#28304;&#24182;&#22312;&#26032;&#22330;&#26223;&#20013;&#25512;&#29702;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#35780;&#20272;&#24046;&#36317;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#29992;&#20110;&#20132;&#36890;&#39046;&#22495;&#24773;&#22659;&#25512;&#29702;&#30340;&#26032;&#22411;&#25991;&#26412;&#20219;&#21153;&#65306;i) BDD-QA&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;(LMs)&#25191;&#34892;&#24773;&#22659;&#20915;&#31574;&#30340;&#33021;&#21147;&#65292;ii) TV-QA&#65292;&#35780;&#20272;LMs&#25512;&#29702;&#22797;&#26434;&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;iii) HDT-QA&#65292;&#35780;&#20272;&#27169;&#22411;&#35299;&#20915;&#20154;&#31867;&#39550;&#39542;&#32771;&#35797;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20043;&#21069;&#24037;&#20316;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#36890;&#29992;&#24615;&#30340;&#22235;&#31181;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent Traffic Monitoring (ITMo) technologies hold the potential for improving road safety/security and for enabling smart city infrastructure. Understanding traffic situations requires a complex fusion of perceptual information with domain-specific and causal commonsense knowledge. Whereas prior work has provided benchmarks and methods for traffic monitoring, it remains unclear whether models can effectively align these information sources and reason in novel scenarios. To address this assessment gap, we devise three novel text-based tasks for situational reasoning in the traffic domain: i) BDD-QA, which evaluates the ability of Language Models (LMs) to perform situational decision-making, ii) TV-QA, which assesses LMs' abilities to reason about complex event causality, and iii) HDT-QA, which evaluates the ability of models to solve human driving exams. We adopt four knowledge-enhanced methods that have shown generalization capability across language reasoning tasks in prior work
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#20559;&#32622;&#26041;&#27861;&#65292;&#22522;&#20110;Context-Aware Transformer Transducer (CATT) &#26469;&#36827;&#34892;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312; WER &#21644; CER &#19978;&#21487;&#20197;&#20998;&#21035;&#20943;&#23569;6.7%&#21644;20.7%&#65292;&#20943;&#23569;&#20102;96.7%&#21644;84.9% &#30340;&#30456;&#23545; WER &#21644; CER &#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2306.00804</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#20559;&#32622;&#30340;&#22522;&#20110;Transducer&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Adaptive Contextual Biasing for Transducer Based Streaming Speech Recognition. (arXiv:2306.00804v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00804
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#20559;&#32622;&#26041;&#27861;&#65292;&#22522;&#20110;Context-Aware Transformer Transducer (CATT) &#26469;&#36827;&#34892;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312; WER &#21644; CER &#19978;&#21487;&#20197;&#20998;&#21035;&#20943;&#23569;6.7%&#21644;20.7%&#65292;&#20943;&#23569;&#20102;96.7%&#21644;84.9% &#30340;&#30456;&#23545; WER &#21644; CER &#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21152;&#20837;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#28145;&#24230;&#20559;&#32622;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#20010;&#24615;&#21270;&#35805;&#35821;&#35782;&#21035;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#30340;&#35821;&#38899;&#21161;&#25163;&#20013;&#65292;&#24635;&#26159;&#23558;&#39640;&#39044;&#27979;&#20998;&#25968;&#30340;&#20010;&#24615;&#21270;&#35789;&#35821;&#36827;&#34892;&#20559;&#32622;&#22788;&#29702;&#20250;&#26174;&#33879;&#38477;&#20302;&#23545;&#24120;&#35265;&#35789;&#35821;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;Transformer Transducer&#65288;CATT&#65289;&#30340;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#20559;&#32622;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20559;&#32622;&#32534;&#30721;&#22120;&#21644;&#39044;&#27979;&#22120;&#23884;&#20837;&#26469;&#25191;&#34892;&#19978;&#19979;&#25991;&#30701;&#35821;&#20986;&#29616;&#30340;&#27969;&#24335;&#39044;&#27979;&#12290;&#36825;&#31181;&#39044;&#27979;&#28982;&#21518;&#34987;&#29992;&#26469;&#21160;&#24577;&#22320;&#20999;&#25442;&#20559;&#32622;&#21015;&#34920;&#30340;&#24320;&#20851;&#65292;&#20351;&#27169;&#22411;&#36866;&#24212;&#20010;&#24615;&#21270;&#21644;&#24120;&#35265;&#24773;&#26223;&#12290;&#22312;Librispeech&#21644;&#20869;&#37096;&#35821;&#38899;&#21161;&#25163;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;WER&#21644;CER&#19978;&#20998;&#21035;&#21487;&#20197;&#36798;&#21040;6.7%&#21644;20.7%&#30340;&#30456;&#23545;&#38477;&#20302;&#65292;&#23558;&#30456;&#23545;WER&#21644;CER&#30340;&#22686;&#21152;&#20943;&#23569;&#21040;96.7%&#21644;84.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
By incorporating additional contextual information, deep biasing methods have emerged as a promising solution for speech recognition of personalized words. However, for real-world voice assistants, always biasing on such personalized words with high prediction scores can significantly degrade the performance of recognizing common words. To address this issue, we propose an adaptive contextual biasing method based on Context-Aware Transformer Transducer (CATT) that utilizes the biased encoder and predictor embeddings to perform streaming prediction of contextual phrase occurrences. Such prediction is then used to dynamically switch the bias list on and off, enabling the model to adapt to both personalized and common scenarios. Experiments on Librispeech and internal voice assistant datasets show that our approach can achieve up to 6.7% and 20.7% relative reduction in WER and CER compared to the baseline respectively, mitigating up to 96.7% and 84.9% of the relative WER and CER increase 
&lt;/p&gt;</description></item><item><title>W-procer&#26159;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18624</link><description>&lt;p&gt;
W-procer: &#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18624
&lt;/p&gt;
&lt;p&gt;
W-procer&#26159;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#19968;&#31181;&#21463;&#27426;&#36814;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20256;&#32479;&#37197;&#32622;&#21147;&#27714;&#20943;&#23569;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#22686;&#21152;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#23384;&#22312;&#22823;&#37327;&#34987;&#27880;&#37322;&#20026;&#8220;O&#8221;&#65288;&#21363;&#8220;OUTSIDE&#8221;&#65289;&#30340;&#23454;&#20307;&#65292;&#24182;&#19988;&#23427;&#20204;&#19981;&#24076;&#26395;&#34987;&#25512;&#31163;&#21040;&#24403;&#21069;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26631;&#35760;&#20026;&#8220;O&#8221;&#20197;&#22806;&#30340;&#20854;&#20182;&#23454;&#20307;&#65292;&#36825;&#31181;&#35774;&#23450;&#25928;&#26524;&#19981;&#20339;&#65292;&#21487;&#33021;&#20250;&#24471;&#20986;&#21547;&#26377;&#22122;&#22768;&#21407;&#22411;&#26631;&#31614;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#8220;O&#8221;&#26631;&#31614;&#23454;&#20307;&#19982;&#26377;&#26631;&#31614;&#23454;&#20307;&#30456;&#20851;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;W-PROCER&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#22260;&#32469;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#23637;&#24320;&#12290;&#36825;&#20123;&#32452;&#20214;&#22312;&#21327;&#21161;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;W-PROCER&#24212;&#29992;&#20110;&#19968;&#20010;&#20844;&#20849;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has become a popular solution for few-shot Name Entity Recognization (NER). The conventional configuration strives to reduce the distance between tokens with the same labels and increase the distance between tokens with different labels. The effect of this setup may, however, in the medical domain, there are a lot of entities annotated as OUTSIDE (O), and they are undesirably pushed apart to other entities that are not labeled as OUTSIDE (O) by the current contrastive learning method end up with a noisy prototype for the semantic representation of the label, though there are many OUTSIDE (O) labeled entities are relevant to the labeled entities. To address this challenge, we propose a novel method named Weighted Prototypical Contrastive Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our approach primarily revolves around constructing the prototype-based contractive loss and weighting network. These components play a crucial role in assisting t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20013;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22823;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#24182;&#22312;&#23545;&#40784;&#22522;&#20934;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#23637;&#31034;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16960</link><description>&lt;p&gt;
&#22312;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20013;&#35757;&#32451;&#31038;&#20250;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Socially Aligned Language Models in Simulated Human Society. (arXiv:2305.16960v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20013;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22823;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#24182;&#22312;&#23545;&#40784;&#22522;&#20934;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#23637;&#31034;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#20013;&#30340;&#31038;&#20250;&#23545;&#40784;&#26088;&#22312;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#25353;&#29031;&#26082;&#23450;&#30340;&#31038;&#20250;&#20215;&#20540;&#34892;&#20107;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#19981;&#21516;&#65292;&#20154;&#20204;&#36890;&#36807;&#31038;&#20132;&#20114;&#21160;&#24471;&#20986;&#23545;&#20215;&#20540;&#21028;&#26029;&#30340;&#20849;&#35782;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21017;&#22312;&#23396;&#31435;&#22320;&#22797;&#21046;&#20854;&#35757;&#32451;&#35821;&#26009;&#24211;&#26102;&#34987;&#35757;&#32451;&#20986;&#26469;&#65292;&#23548;&#33268;&#22312;&#38476;&#29983;&#22330;&#26223;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#20801;&#35768;LMs&#20174;&#27169;&#25311;&#30340;&#31038;&#20132;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22823;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#22312;&#23545;&#40784;&#22522;&#20934;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#23637;&#31034;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;LMs&#35757;&#32451;&#20013;&#30340;&#33539;&#24335;&#36716;&#21464;&#20351;&#25105;&#20204;&#31163;&#24320;&#21457;&#33021;&#22815;&#24378;&#26377;&#21147;&#19988;&#20934;&#30830;&#21453;&#26144;&#31038;&#20250;&#35268;&#33539;&#21644;&#20215;&#20540;&#30340;AI&#31995;&#32479;&#26356;&#36817;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social alignment in AI systems aims to ensure that these models behave according to established societal values. However, unlike humans, who derive consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks. This work presents a novel training paradigm that permits LMs to learn from simulated social interactions. In comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations. This paradigm shift in the training of LMs brings us a step closer to developing AI systems that can robustly and accurately reflect societal norms and values.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#23384;&#22312;&#20294;&#36890;&#24120;&#19981;&#25104;&#31435;&#30340;&#20266;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#37051;&#22495;&#20998;&#26512;&#26694;&#26550;&#20197;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#20266;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#27491;&#21017;&#21270;&#26041;&#27861;NFL&#65288;&#19981;&#35201;&#24536;&#35760;&#20320;&#30340;&#35821;&#35328;&#65289;&#36991;&#20813;&#20102;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.13654</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#20943;&#23569;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#20266;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding and Mitigating Spurious Correlations in Text Classification. (arXiv:2305.13654v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#23384;&#22312;&#20294;&#36890;&#24120;&#19981;&#25104;&#31435;&#30340;&#20266;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#37051;&#22495;&#20998;&#26512;&#26694;&#26550;&#20197;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#20266;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#27491;&#21017;&#21270;&#26041;&#27861;NFL&#65288;&#19981;&#35201;&#24536;&#35760;&#20320;&#30340;&#35821;&#35328;&#65289;&#36991;&#20813;&#20102;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#23384;&#22312;&#20294;&#36890;&#24120;&#19981;&#25104;&#31435;&#30340;&#20266;&#30456;&#20851;&#24615;&#12290;&#20363;&#22914;&#24773;&#24863;&#20998;&#31867;&#22120;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#23398;&#20064;&#21040;&#20196;&#20154;&#24841;&#24742;&#30340;&#30005;&#24433;&#35780;&#35770;&#24635;&#26159;&#19982;&#8220;Spielberg&#8221;&#36825;&#20010;&#35789;&#30456;&#20851;&#32852;&#12290;&#20381;&#36182;&#20110;&#20266;&#30456;&#20851;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#27867;&#21270;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#22240;&#27492;&#24212;&#35813;&#36991;&#20813;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37051;&#22495;&#20998;&#26512;&#26694;&#26550;&#26469;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#20266;&#30456;&#20851;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#27491;&#21017;&#21270;&#26041;&#27861;NFL&#65288;&#19981;&#35201;&#24536;&#35760;&#20320;&#30340;&#35821;&#35328;&#65289;&#65292;&#20197;&#36991;&#20813;&#36825;&#31181;&#24773;&#20917;&#12290;&#22312;&#20004;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NFL&#30456;&#23545;&#20110;&#26631;&#20934;&#30340;&#24494;&#35843;&#31639;&#27861;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#32780;&#27809;&#26377;&#29306;&#29298;&#22312;&#25968;&#25454;&#20869;&#37096;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that deep learning models are prone to exploit spurious correlations that are present in the training set, yet may not hold true in general. A sentiment classifier may erroneously learn that the token spielberg is always tied to positive movie reviews. Relying on spurious correlations may lead to significant degradation in generalizability and should be avoided. In this paper, we propose a neighborhood analysis framework to explain how exactly language models exploit spurious correlations. Driven by the analysis, we propose a family of regularization methods, NFL (do Not Forget your Language) to prevent the situation. Experiments on two text classification tasks show that NFL brings a significant improvement over standard fine-tuning in terms of robustness without sacrificing in-distribution accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992;MildTriple Loss&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#24182;&#27169;&#25311;&#36328;&#27169;&#24577;&#20154;&#31867;&#21160;&#20316;&#24207;&#21015;&#19982;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.04195</link><description>&lt;p&gt;
MildTriple Loss&#27169;&#22411;&#19979;&#30340;&#36816;&#21160;&#21644;&#25991;&#26412;&#36328;&#27169;&#24577;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Retrieval for Motion and Text via MildTriple Loss. (arXiv:2305.04195v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992;MildTriple Loss&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#24182;&#27169;&#25311;&#36328;&#27169;&#24577;&#20154;&#31867;&#21160;&#20316;&#24207;&#21015;&#19982;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#26816;&#32034;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#30740;&#31350;&#35838;&#39064;&#65292;&#38543;&#30528;&#22270;&#20687;&#25991;&#26412;&#21644;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;&#23613;&#31649;&#22312;&#34394;&#25311;&#29616;&#23454;&#31561;&#24191;&#27867;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#20294;&#20154;&#31867;&#21160;&#20316;&#24207;&#21015;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#23578;&#26410;&#24341;&#36215;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#36825;&#20010;&#20219;&#21153;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#23545;&#20004;&#31181;&#35821;&#35328;&#30340;&#20849;&#21516;&#24314;&#27169;&#65292;&#35201;&#27714;&#20174;&#25991;&#26412;&#20013;&#29702;&#35299;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20449;&#24687;&#65292;&#24182;&#20174;&#19977;&#32500;&#20154;&#20307;&#36816;&#21160;&#24207;&#21015;&#20013;&#23398;&#20064;&#34892;&#20026;&#29305;&#24449;&#12290;&#20197;&#24448;&#30340;&#36816;&#21160;&#25968;&#25454;&#24314;&#27169;&#20027;&#35201;&#20381;&#36182;&#20110;&#33258;&#22238;&#24402;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36825;&#21487;&#33021;&#20250;&#36951;&#24536;&#20197;&#21069;&#30340;&#20449;&#24687;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#36816;&#21160;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#20174;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#20013;&#23398;&#20064;&#34920;&#31034;&#24182;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
Cross-modal retrieval has become a prominent research topic in computer vision and natural language processing with advances made in image-text and video-text retrieval technologies. However, cross-modal retrieval between human motion sequences and text has not garnered sufficient attention despite the extensive application value it holds, such as aiding virtual reality applications in better understanding users' actions and language. This task presents several challenges, including joint modeling of the two modalities, demanding the understanding of person-centered information from text, and learning behavior features from 3D human motion sequences. Previous work on motion data modeling mainly relied on autoregressive feature extractors that may forget previous information, while we propose an innovative model that includes simple yet powerful transformer-based motion and text encoders, which can learn representations from the two different modalities and capture long-term dependencie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24544;&#23454;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#27604;&#25945;&#24072;&#27169;&#22411;&#22823;&#25968;&#20493;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#19968;&#20010;&#23567;&#30340;&#12289;&#33258;&#25105;&#19968;&#33268;&#30340;&#24605;&#36335;&#20018;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35777;&#26126;&#20915;&#31574;&#24182;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.01879</link><description>&lt;p&gt;
SCOTT: &#33258;&#25105;&#19968;&#33268;&#24615;&#24605;&#36335;&#20018;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
SCOTT: Self-Consistent Chain-of-Thought Distillation. (arXiv:2305.01879v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24544;&#23454;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#27604;&#25945;&#24072;&#27169;&#22411;&#22823;&#25968;&#20493;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#19968;&#20010;&#23567;&#30340;&#12289;&#33258;&#25105;&#19968;&#33268;&#30340;&#24605;&#36335;&#20018;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35777;&#26126;&#20915;&#31574;&#24182;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20986;&#19968;&#23450;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#36890;&#36807;&#19968;&#31995;&#21015;&#36830;&#32493;&#30340;&#24605;&#32771;&#36807;&#31243;&#33719;&#24471;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#30340;&#31361;&#20986;&#33021;&#21147;&#12290;&#34429;&#28982;&#24605;&#36335;&#20018;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20165;&#22312;&#36275;&#22815;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25165;&#33021;&#35266;&#23519;&#21040;&#36825;&#31181;&#25910;&#30410;&#12290;&#26356;&#20196;&#20154;&#25285;&#24551;&#30340;&#26159;&#65292;&#29983;&#25104;&#30340;&#29702;&#30001;&#24456;&#23569;&#20445;&#35777;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#20445;&#25345;&#19968;&#33268;&#25110;&#32773;&#24544;&#23454;&#22320;&#35777;&#26126;&#20915;&#31574;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24544;&#23454;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#27604;&#25945;&#24072;&#27169;&#22411;&#22823;&#25968;&#20493;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#19968;&#20010;&#23567;&#30340;&#12289;&#33258;&#25105;&#19968;&#33268;&#30340;&#24605;&#36335;&#20018;&#27169;&#22411;&#12290;&#20026;&#20102;&#24418;&#25104;&#26356;&#22909;&#30340;&#30417;&#30563;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#35299;&#30721;&#24341;&#23548;&#25945;&#24072;&#27169;&#22411;&#20135;&#29983;&#25903;&#25345;&#27491;&#30830;&#31572;&#26696;&#30340;&#29702;&#30001;&#65292;&#36825;&#40723;&#21169;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#30340;token&#21482;&#22312;&#32771;&#34385;&#21040;&#31572;&#26696;&#26102;&#25165;&#26356;&#21152;&#21487;&#20449;&#12290;&#20026;&#20102;&#20445;&#35777;&#24544;&#23454;&#30340;&#33976;&#39311;&#65292;&#25105;&#20204;&#20351;&#29992;&#25945;&#24072;&#29983;&#25104;&#30340;&#29702;&#30001;&#26469;&#23398;&#20064;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21453;&#20107;&#23454;&#25512;&#29702;&#30446;&#26631;&#65292;&#21363;&#26681;&#25454;&#20855;&#26377;&#33258;&#25105;&#19968;&#33268;&#24615;&#19988;&#24544;&#23454;&#20110;&#25945;&#24072;&#39044;&#27979;&#30340;&#24605;&#36335;&#20018;&#29702;&#30001;&#39044;&#27979;&#20915;&#31574;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#25277;&#35937;&#25688;&#35201;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23398;&#29983;&#27169;&#22411;&#20013;&#30340;&#33258;&#25105;&#19968;&#33268;&#24615;&#26377;&#21161;&#20110;&#35777;&#26126;&#20915;&#31574;&#24182;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (CoT) prompting. While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM's predictions or faithfully justify the decisions. In this work, we propose a faithful knowledge distillation method to learn a small, self-consistent CoT model from a teacher model that is orders of magnitude larger. To form better supervision, we elicit rationales supporting the gold answers from a large LM (teacher) by contrastive decoding, which encourages the teacher to generate tokens that become more plausible only when the answer is considered. To ensure faithful distillation, we use the teacher-generated rationales to learn a student LM with a counterfactual reasoning objective, which pre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"gisting"&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23558;&#25552;&#31034;&#21387;&#32553;&#20026;&#26356;&#23567;&#30340;"&#35201;&#28857;"&#26631;&#35760;&#38598;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;26&#20493;&#30340;&#25552;&#31034;&#21387;&#32553;&#65292;&#20943;&#23569;40&#65285;&#30340;FLOPs&#12289;4.2&#65285;&#30340;&#22681;&#26102;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#33410;&#30465;&#23384;&#20648;&#31354;&#38388;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#36755;&#20986;&#36136;&#37327;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2304.08467</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#35201;&#28857;&#26631;&#35760;&#21387;&#32553;&#25552;&#31034;&#35821;
&lt;/p&gt;
&lt;p&gt;
Learning to Compress Prompts with Gist Tokens. (arXiv:2304.08467v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08467
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"gisting"&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23558;&#25552;&#31034;&#21387;&#32553;&#20026;&#26356;&#23567;&#30340;"&#35201;&#28857;"&#26631;&#35760;&#38598;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;26&#20493;&#30340;&#25552;&#31034;&#21387;&#32553;&#65292;&#20943;&#23569;40&#65285;&#30340;FLOPs&#12289;4.2&#65285;&#30340;&#22681;&#26102;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#33410;&#30465;&#23384;&#20648;&#31354;&#38388;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#36755;&#20986;&#36136;&#37327;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#26159;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#33021;&#21147;&#30340;&#20027;&#35201;&#26041;&#24335;&#65292;&#20294;&#26159;&#25552;&#31034;&#21344;&#25454;&#20102;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#23453;&#36149;&#30340;&#31354;&#38388;&#65292;&#37325;&#22797;&#32534;&#30721;&#30456;&#21516;&#30340;&#25552;&#31034;&#22312;&#35745;&#31639;&#19978;&#26159;&#20302;&#25928;&#30340;&#12290;&#24494;&#35843;&#21644;&#33976;&#39311;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#19987;&#38376;&#21270;&#65292;&#20294;&#38656;&#35201;&#20026;&#27599;&#20010;&#20219;&#21153;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#20102;&#23436;&#20840;&#36991;&#20813;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;gist&#65292;&#23427;&#35757;&#32451;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#23558;&#25552;&#31034;&#21387;&#32553;&#25104;&#26356;&#23567;&#30340;&#8220;&#35201;&#28857;&#8221;&#26631;&#35760;&#38598;&#65292;&#21487;&#20197;&#29992;&#20110;&#35745;&#31639;&#25928;&#29575;&#30340;&#32531;&#23384;&#21644;&#37325;&#29992;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#20462;&#25913;Transformer&#30340;&#27880;&#24847;&#21147;&#25513;&#30721;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#39069;&#22806;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#23545;gist&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#25552;&#31034;&#30340;&#39640;&#36798;26&#20493;&#30340;&#21387;&#32553;&#65292;&#20174;&#32780;&#20943;&#23569;&#39640;&#36798;40&#65285;&#30340;FLOPs&#12289;4.2&#65285;&#30340;&#22681;&#26102;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#33410;&#30465;&#23384;&#20648;&#31354;&#38388;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#36755;&#20986;&#36136;&#37327;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting is the primary way to utilize the multitask capabilities of language models (LMs), but prompts occupy valuable space in the input context window, and repeatedly encoding the same prompt is computationally inefficient. Finetuning and distillation methods allow for specialization of LMs without prompting, but require retraining the model for each task. To avoid this trade-off entirely, we present gisting, which trains an LM to compress prompts into smaller sets of "gist" tokens which can be cached and reused for compute efficiency. Gist models can be trained with no additional cost over standard instruction finetuning by simply modifying Transformer attention masks to encourage prompt compression. On decoder (LLaMA-7B) and encoder-decoder (FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting in up to 40% FLOPs reductions, 4.2% wall time speedups, and storage savings, all with minimal loss in output quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#33258;&#22238;&#24402;&#26426;&#22120;&#32763;&#35793;&#20013;&#36890;&#36807;&#27169;&#31946;&#23545;&#40784;&#35299;&#20915;&#22810;&#27169;&#24577;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#22270;&#19982;&#21442;&#32771;&#20043;&#38388;&#30340;&#27169;&#31946;&#23545;&#40784;&#24471;&#20998;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#32763;&#35793;&#24615;&#33021;&#65292;&#24182;&#22686;&#21152;&#20102;&#39044;&#27979;&#32622;&#20449;&#24230;&#65292;&#21019;&#36896;&#20102;&#38750;&#33258;&#22238;&#24402;&#26426;&#22120;&#32763;&#35793;&#30340;&#26032;&#30340;state of the art&#12290;</title><link>http://arxiv.org/abs/2303.06662</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#26426;&#22120;&#32763;&#35793;&#20013;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#27169;&#31946;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Fuzzy Alignments in Directed Acyclic Graph for Non-Autoregressive Machine Translation. (arXiv:2303.06662v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#33258;&#22238;&#24402;&#26426;&#22120;&#32763;&#35793;&#20013;&#36890;&#36807;&#27169;&#31946;&#23545;&#40784;&#35299;&#20915;&#22810;&#27169;&#24577;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#22270;&#19982;&#21442;&#32771;&#20043;&#38388;&#30340;&#27169;&#31946;&#23545;&#40784;&#24471;&#20998;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#32763;&#35793;&#24615;&#33021;&#65292;&#24182;&#22686;&#21152;&#20102;&#39044;&#27979;&#32622;&#20449;&#24230;&#65292;&#21019;&#36896;&#20102;&#38750;&#33258;&#22238;&#24402;&#26426;&#22120;&#32763;&#35793;&#30340;&#26032;&#30340;state of the art&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;NAT&#65289;&#36890;&#36807;&#24341;&#20837;&#39030;&#28857;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#19982;&#21442;&#32771;&#26631;&#35760;&#21644;&#39030;&#28857;&#20043;&#38388;&#38656;&#35201;&#20005;&#26684;&#23545;&#40784;&#65292;&#20005;&#37325;&#21066;&#24369;&#20102;&#22788;&#29702;&#22810;&#31181;&#32763;&#35793;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#35748;&#20026;&#22270;&#20013;&#30340;&#25152;&#26377;&#36335;&#24452;&#37117;&#19982;&#21442;&#32771;&#21477;&#23376;&#27169;&#31946;&#23545;&#40784;&#65292;&#19981;&#38656;&#35201;&#31934;&#30830;&#23545;&#40784;&#65292;&#32780;&#26159;&#35757;&#32451;&#27169;&#22411;&#20197;&#26368;&#22823;&#21270;&#22270;&#19982;&#21442;&#32771;&#20043;&#38388;&#30340;&#27169;&#31946;&#23545;&#40784;&#24471;&#20998;&#65292;&#32771;&#34385;&#21040;&#25152;&#26377;&#27169;&#24577;&#20013;&#25429;&#25417;&#21040;&#30340;&#32763;&#35793;&#12290;&#22312;&#20027;&#35201;&#30340;WMT&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#32763;&#35793;&#24615;&#33021;&#65292;&#24182;&#22686;&#21152;&#20102;&#39044;&#27979;&#32622;&#20449;&#24230;&#65292;&#21019;&#36896;&#20102;NA&#30340;&#26032;&#30340;state of the art&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive translation (NAT) reduces the decoding latency but suffers from performance degradation due to the multi-modality problem. Recently, the structure of directed acyclic graph has achieved great success in NAT, which tackles the multi-modality problem by introducing dependency between vertices. However, training it with negative log-likelihood loss implicitly requires a strict alignment between reference tokens and vertices, weakening its ability to handle multiple translation modalities. In this paper, we hold the view that all paths in the graph are fuzzily aligned with the reference sentence. We do not require the exact alignment but train the model to maximize a fuzzy alignment score between the graph and reference, which takes captured translations in all modalities into account. Extensive experiments on major WMT benchmarks show that our method substantially improves translation performance and increases prediction confidence, setting a new state of the art for NA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#26080;&#26799;&#24230;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#65292;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20165;&#38656;&#20960;&#20998;&#38047;&#23601;&#33021;&#23558;&#21407;&#22987;FLOP&#35745;&#25968;&#30340;&#26368;&#39640;40%&#20943;&#23569;&#32780;&#20934;&#30830;&#24230;&#20165;&#19979;&#38477;&#19981;&#36229;&#36807;4%&#12290;</title><link>http://arxiv.org/abs/2303.04185</link><description>&lt;p&gt;
&#26080;&#26799;&#24230;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#19982;&#26080;&#26631;&#31614;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Gradient-Free Structured Pruning with Unlabeled Data. (arXiv:2303.04185v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#26080;&#26799;&#24230;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#65292;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20165;&#38656;&#20960;&#20998;&#38047;&#23601;&#33021;&#23558;&#21407;&#22987;FLOP&#35745;&#25968;&#30340;&#26368;&#39640;40%&#20943;&#23569;&#32780;&#20934;&#30830;&#24230;&#20165;&#19979;&#38477;&#19981;&#36229;&#36807;4%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#35299;&#20915;&#22256;&#38590;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#36825;&#31181;&#25104;&#21151;&#20276;&#38543;&#30528;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#25512;&#29702;&#24310;&#36831;&#12290;&#38543;&#30528;&#24320;&#21457;&#20154;&#21592;&#21644;&#31532;&#19977;&#26041;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#23450;&#21046;&#65292;&#25552;&#20379;&#39640;&#25928;&#30340;&#25512;&#29702;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22823;&#12290;&#35768;&#22810;&#21162;&#21147;&#23581;&#35797;&#36890;&#36807;&#21098;&#26525;&#21644;&#33976;&#39311;&#31561;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#26469;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#35201;&#20040;&#38656;&#35201;&#26377;&#26631;&#31614;&#30340;&#25968;&#25454;&#65292;&#35201;&#20040;&#22240;&#20026;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#21387;&#32553;&#27169;&#22411;&#20197;&#24674;&#22797;&#20934;&#30830;&#24615;&#32780;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#26080;&#26799;&#24230;&#32467;&#26500;&#21270;&#21098;&#26525;&#26694;&#26550;&#12290;&#20351;&#29992;BERT$_{BASE}$&#21644;DistilBERT&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26435;&#37325;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#22312;&#21333;&#20010;GPU&#19978;&#20165;&#38656;&#20960;&#20998;&#38047;&#65292;&#21363;&#21487;&#23558;&#21407;&#22987;FLOP&#35745;&#25968;&#30340;&#26368;&#39640;40%&#20943;&#23569;&#65292;&#20934;&#30830;&#24230;&#19979;&#38477;&#19981;&#36229;&#36807;4%&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved great success in solving difficult tasks across many domains, but such success comes with a high computation cost, and inference latency. As developers and third parties customize these models, the need to provide efficient inference has increased. Many efforts have attempted to reduce inference cost through model compression techniques such as pruning and distillation. However, these techniques either require labeled data, or are time-consuming as they require the compressed model to be retrained to regain accuracy. In this paper, we propose a gradient-free structured pruning framework that uses only unlabeled data. An evaluation on the GLUE and SQuAD benchmarks using BERT$_{BASE}$ and DistilBERT illustrates the effectiveness of the proposed approach. By only using the weights of the pre-trained model and unlabeled data, in a matter of a few minutes on a single GPU, up to 40% of the original FLOP count can be reduced with less than a 4% accur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36328;&#26102;&#26399;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;SpanKL&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#22810;&#26631;&#31614;&#39044;&#27979;&#26469;&#23454;&#29616;&#35760;&#24518;&#20445;&#30041;&#21644;&#20914;&#31361;&#38450;&#27490;&#65292;&#35813;&#27169;&#22411;&#22312;&#36328;&#26102;&#26399;NER&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26174;&#31034;&#20986;&#39640;&#23454;&#38469;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2302.12200</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36328;&#26102;&#26399;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Neural Span-Based Continual Named Entity Recognition Model. (arXiv:2302.12200v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36328;&#26102;&#26399;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;SpanKL&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#22810;&#26631;&#31614;&#39044;&#27979;&#26469;&#23454;&#29616;&#35760;&#24518;&#20445;&#30041;&#21644;&#20914;&#31361;&#38450;&#27490;&#65292;&#35813;&#27169;&#22411;&#22312;&#36328;&#26102;&#26399;NER&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26174;&#31034;&#20986;&#39640;&#23454;&#38469;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#20307;&#31867;&#22411;&#19981;&#26029;&#22686;&#21152;&#30340;&#39046;&#22495;&#65288;&#20363;&#22914;&#20010;&#20154;&#21161;&#25163;&#65289;&#20013;&#65292;&#20855;&#22791;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65288;NER&#65289;&#20855;&#26377;&#29616;&#23454;&#20215;&#20540;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;NER&#30340;&#23398;&#20064;&#33539;&#24335;&#36880;&#28176;&#21457;&#23637;&#20986;&#20102;&#26032;&#30340;&#27169;&#24335;&#65292;&#22914;&#22522;&#20110;&#36328;&#24230;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36328;&#26102;&#26399;&#23398;&#20064;&#22312;&#36825;&#26041;&#38754;&#30340;&#28508;&#21147;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SpanKL&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#36328;&#24230;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26469;&#20445;&#30041;&#35760;&#24518;&#65292;&#24182;&#37319;&#29992;&#22810;&#26631;&#31614;&#39044;&#27979;&#26469;&#38450;&#27490;&#36328;&#26102;&#26399;NER&#20013;&#30340;&#20914;&#31361;&#12290;&#19982;&#20043;&#21069;&#30340;&#24207;&#21015;&#26631;&#35760;&#26041;&#27861;&#19981;&#21516;&#65292;&#22312;SpanKL&#20013;&#65292;&#36328;&#24230;&#21644;&#23454;&#20307;&#32423;&#21035;&#30340;&#29420;&#31435;&#24314;&#27169;&#20197;&#21450;&#35774;&#35745;&#30340;&#19968;&#33268;&#20248;&#21270;&#20419;&#36827;&#20102;&#27599;&#20010;&#22686;&#37327;&#27493;&#39588;&#30340;&#23398;&#20064;&#65292;&#24182;&#20943;&#36731;&#20102;&#36951;&#24536;&#12290;&#22312;&#20174;OntoNotes&#21644;Few-NERD&#34893;&#29983;&#30340;&#21512;&#25104;CL&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SpanKL&#22312;&#35768;&#22810;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#24182;&#19988;&#20174;CL&#21040;&#19978;&#38480;&#30340;&#24046;&#36317;&#26368;&#23567;&#65292;&#26174;&#31034;&#20102;&#20854;&#39640;&#23454;&#38469;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) models capable of Continual Learning (CL) are realistically valuable in areas where entity types continuously increase (e.g., personal assistants). Meanwhile the learning paradigm of NER advances to new patterns such as the span-based methods. However, its potential to CL has not been fully explored. In this paper, we propose SpanKL, a simple yet effective Span-based model with Knowledge distillation (KD) to preserve memories and multi-Label prediction to prevent conflicts in CL-NER. Unlike prior sequence labeling approaches, the inherently independent modeling in span and entity level with the designed coherent optimization on SpanKL promotes its learning at each incremental step and mitigates the forgetting. Experiments on synthetic CL datasets derived from OntoNotes and Few-NERD show that SpanKL significantly outperforms previous SoTA in many aspects, and obtains the smallest gap from CL to the upper bound revealing its high practiced value. The code i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;Reddit&#24086;&#23376;&#25968;&#25454;&#38598;&#65292;&#25506;&#35752;&#20102;&#23545;&#20351;&#29992;&#29289;&#36136;&#30340;&#20154;&#65288;PWUS&#65289;&#30340;&#27745;&#21517;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#29289;&#36136;&#20351;&#29992;&#32463;&#39564;&#30340;&#24037;&#20154;&#26356;&#26377;&#21487;&#33021;&#35780;&#20026;&#20855;&#26377;&#27745;&#21517;&#21270;&#65292;&#20026;&#27492;&#24314;&#31435;&#20102;&#19968;&#20010;&#20197;&#20146;&#36523;&#29289;&#36136;&#20351;&#29992;&#32463;&#39564;&#30340;&#24037;&#20154;&#20026;&#20013;&#24515;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2302.02064</link><description>&lt;p&gt;
&#37325;&#22312;&#20146;&#36523;&#32463;&#21382;&#65306;&#33258;&#21160;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#23545;&#20351;&#29992;&#29289;&#36136;&#30340;&#20154;&#30340;&#27745;&#21517;&#21270;
&lt;/p&gt;
&lt;p&gt;
Lived Experience Matters: Automatic Detection of Stigma on Social Media Toward People Who Use Substances. (arXiv:2302.02064v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;Reddit&#24086;&#23376;&#25968;&#25454;&#38598;&#65292;&#25506;&#35752;&#20102;&#23545;&#20351;&#29992;&#29289;&#36136;&#30340;&#20154;&#65288;PWUS&#65289;&#30340;&#27745;&#21517;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#29289;&#36136;&#20351;&#29992;&#32463;&#39564;&#30340;&#24037;&#20154;&#26356;&#26377;&#21487;&#33021;&#35780;&#20026;&#20855;&#26377;&#27745;&#21517;&#21270;&#65292;&#20026;&#27492;&#24314;&#31435;&#20102;&#19968;&#20010;&#20197;&#20146;&#36523;&#29289;&#36136;&#20351;&#29992;&#32463;&#39564;&#30340;&#24037;&#20154;&#20026;&#20013;&#24515;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20351;&#29992;&#29289;&#36136;&#30340;&#20154;&#65288;PWUS&#65289;&#30340;&#27745;&#21517;&#21270;&#26159;&#23547;&#27714;&#27835;&#30103;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#20182;&#20204;&#36973;&#21463;&#26356;&#39640;&#27700;&#24179;&#30340;&#27745;&#21517;&#21270;&#65292;&#25509;&#21463;&#27835;&#30103;&#30340;&#20154;&#26356;&#26377;&#21487;&#33021;&#20013;&#36884;&#36864;&#20986;&#12290;&#23613;&#31649;&#19982;&#24974;&#24680;&#35328;&#35770;&#21644;&#26377;&#27602;&#24615;&#26377;&#20851;&#30340;&#27010;&#24565;&#24050;&#32463;&#25104;&#20026;&#33258;&#21160;&#20869;&#23481;&#30417;&#31649;&#30740;&#31350;&#30340;&#37325;&#28857;&#65292;&#21253;&#25324;&#37027;&#20123;&#38024;&#23545;&#24369;&#21183;&#32676;&#20307;&#30340;&#20869;&#23481;&#65292;&#20294;&#27745;&#21517;&#21270;&#65292;&#29305;&#21035;&#26159;&#23545;&#20351;&#29992;&#29289;&#36136;&#30340;&#20154;&#30340;&#27745;&#21517;&#21270;&#23578;&#26410;&#24471;&#21040;&#20851;&#27880;&#12290;&#26412;&#25991;&#21033;&#29992;&#22823;&#32422;5000&#20010;&#20844;&#24320;&#30340;Reddit&#24086;&#23376;&#25968;&#25454;&#38598;&#26469;&#25506;&#35752;&#23545;PWUS&#30340;&#27745;&#21517;&#21270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#20247;&#21253;&#27880;&#37322;&#20219;&#21153;&#65292;&#35201;&#27714;&#24037;&#20154;&#27880;&#37322;&#27599;&#20010;&#24086;&#23376;&#26159;&#21542;&#23384;&#22312;&#23545;PWUS&#30340;&#27745;&#21517;&#21270;&#65292;&#24182;&#22238;&#31572;&#19982;&#20182;&#20204;&#23545;&#29289;&#36136;&#20351;&#29992;&#32463;&#39564;&#30456;&#20851;&#30340;&#19968;&#31995;&#21015;&#38382;&#39064;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#29289;&#36136;&#25110;&#35748;&#35782;&#26377;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#30340;&#24037;&#20154;&#26356;&#26377;&#21487;&#33021;&#23558;&#24086;&#23376;&#35780;&#20026;&#20855;&#26377;&#27745;&#21517;&#21270;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26377;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#20146;&#36523;&#29289;&#36136;&#20351;&#29992;&#32463;&#39564;&#30340;&#24037;&#20154;&#20026;&#20013;&#24515;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stigma toward people who use substances (PWUS) is a leading barrier to seeking treatment.Further, those in treatment are more likely to drop out if they experience higher levels of stigmatization. While related concepts of hate speech and toxicity, including those targeted toward vulnerable populations, have been the focus of automatic content moderation research, stigma and, in particular, people who use substances have not. This paper explores stigma toward PWUS using a data set of roughly 5,000 public Reddit posts. We performed a crowd-sourced annotation task where workers are asked to annotate each post for the presence of stigma toward PWUS and answer a series of questions related to their experiences with substance use. Results show that workers who use substances or know someone with a substance use disorder are more likely to rate a post as stigmatizing. Building on this, we use a supervised machine learning framework that centers workers with lived substance use experience to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#26126;&#30830;&#30340;&#32467;&#26500;&#25512;&#29702;&#21644;&#35821;&#35328;&#39044;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#20197;&#36171;&#20104;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.08913</link><description>&lt;p&gt;
&#32479;&#19968;&#32467;&#26500;&#25512;&#29702;&#21644;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20197;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning. (arXiv:2301.08913v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#26126;&#30830;&#30340;&#32467;&#26500;&#25512;&#29702;&#21644;&#35821;&#35328;&#39044;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#20197;&#36171;&#20104;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#37197;&#22791;&#22522;&#30784;&#25512;&#29702;&#25216;&#33021;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#19979;&#28216;&#22797;&#26434;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26174;&#33879;&#30340;&#32467;&#26500;&#25512;&#29702;&#25216;&#33021;&#24456;&#23569;&#34987;&#30740;&#31350;&#65292;&#36825;&#28041;&#21450;&#23545;&#25991;&#26412;&#20013;&#30340;&#38544;&#21547;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#26126;&#30830;&#30340;&#36923;&#36753;&#25512;&#29702;&#20197;&#25512;&#23548;&#20986;&#32467;&#35770;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#26126;&#30830;&#30340;&#32467;&#26500;&#25512;&#29702;&#21644;&#35821;&#35328;&#39044;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#36171;&#20104;PLMs&#32467;&#26500;&#25512;&#29702;&#33021;&#21147;&#12290;&#23427;&#39318;&#20808;&#35782;&#21035;&#19978;&#19979;&#25991;&#20013;&#30340;&#20960;&#20010;&#22522;&#26412;&#32467;&#26500;&#65292;&#26500;&#24314;&#32467;&#26500;&#21270;&#26597;&#35810;&#65292;&#24182;&#27839;&#30528;&#26597;&#35810;&#36880;&#27493;&#25512;&#29702;&#20197;&#30830;&#23450;&#31572;&#26696;&#23454;&#20307;&#12290;&#36890;&#36807;&#20351;&#29992;PLMs&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#26469;&#21021;&#22987;&#21270;&#32467;&#26500;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#22312;&#36825;&#20010;&#35821;&#20041;&#34920;&#31034;&#31354;&#38388;&#19978;&#36827;&#34892;&#36880;&#27493;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#35821;&#20041;&#21644;&#32467;&#26500;&#25512;&#29702;&#30340;&#34701;&#21512;&#12290;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent pre-trained language models (PLMs) equipped with foundation reasoning skills have shown remarkable performance on downstream complex tasks. However, the significant structure reasoning skill has been rarely studied, which involves modeling implicit structure information within the text and performing explicit logical reasoning over them to deduce the conclusion. This paper proposes a unified learning framework that combines explicit structure reasoning and language pre-training to endow PLMs with the structure reasoning skill. It first identifies several elementary structures within contexts to construct structured queries and performs step-by-step reasoning along the queries to identify the answer entity. The fusion of textual semantics and structure reasoning is achieved by using contextual representations learned by PLMs to initialize the representation space of structures, and performing stepwise reasoning on this semantic representation space. Experimental results on four d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SuS-X&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#21517;&#31216;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36801;&#31227;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.16198</link><description>&lt;p&gt;
SuS-X&#65306;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#21517;&#31216;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36801;&#31227;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SuS-X: Training-Free Name-Only Transfer of Vision-Language Models. (arXiv:2211.16198v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SuS-X&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#21517;&#31216;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36801;&#31227;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#24050;&#25104;&#20026;&#35757;&#32451;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;CLIP&#22312;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#21644;&#26816;&#32034;&#26041;&#38754;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#35201;&#21457;&#25381;&#20854;&#20840;&#37096;&#28508;&#21147;&#65292;&#24494;&#35843;&#20173;&#28982;&#26159;&#24517;&#35201;&#30340;&#12290;&#24494;&#35843;&#25972;&#20010;CLIP&#27169;&#22411;&#20250;&#28040;&#32791;&#36164;&#28304;&#19988;&#19981;&#31283;&#23450;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#34429;&#28982;&#26088;&#22312;&#36991;&#20813;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#20173;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#20998;&#24067;&#20013;&#30340;&#22270;&#20687;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#21478;&#19968;&#31181;&#26041;&#27861;&#8212;&#8212;&#26080;&#38656;&#35757;&#32451;&#30340;&#8220;&#20165;&#22522;&#20110;&#21517;&#31216;&#36801;&#31227;&#8221;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;SuS-X&#65292;&#30001;&#20004;&#20010;&#20851;&#38190;&#26500;&#24314;&#22359;&#8212;&#8212;SuS&#21644;TIP-X&#32452;&#25104;&#65292;&#26082;&#19981;&#38656;&#35201;&#23494;&#38598;&#30340;&#24494;&#35843;&#65292;&#20063;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;SuS-X&#22312;19&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet effective way to train large-scale vision-language models. CLIP demonstrates impressive zero-shot classification and retrieval on diverse downstream tasks. However, to leverage its full potential, fine-tuning still appears to be necessary. Fine-tuning the entire CLIP model can be resource-intensive and unstable. Moreover, recent methods that aim to circumvent this need for fine-tuning still require access to images from the target distribution. In this paper, we pursue a different approach and explore the regime of training-free "name-only transfer" in which the only knowledge we possess about the downstream task comprises the names of downstream target categories. We propose a novel method, SuS-X, consisting of two key building blocks -- SuS and TIP-X, that requires neither intensive fine-tuning nor costly labelled data. SuS-X achieves state-of-the-art zero-shot classification results on 19 benchmark datasets. 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26469;&#25506;&#35752;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#35270;&#35282;&#65292;&#29305;&#21035;&#20851;&#27880;&#26500;&#24565;&#25928;&#24230;&#21644;&#27979;&#37327;&#24037;&#20855;&#30340;&#20449;&#24230;&#65292;&#22312;&#34913;&#37327;&#27169;&#22411;&#20559;&#35265;&#30340;&#24773;&#22659;&#20013;&#22914;&#20309;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.13709</link><description>&lt;p&gt;
NLP&#20013;&#30340;&#19981;&#33391;&#20559;&#35265;&#65306;&#36991;&#20813;&#34913;&#37327;&#21361;&#26426;
&lt;/p&gt;
&lt;p&gt;
Undesirable biases in NLP: Averting a crisis of measurement. (arXiv:2211.13709v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13709
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26469;&#25506;&#35752;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#35270;&#35282;&#65292;&#29305;&#21035;&#20851;&#27880;&#26500;&#24565;&#25928;&#24230;&#21644;&#27979;&#37327;&#24037;&#20855;&#30340;&#20449;&#24230;&#65292;&#22312;&#34913;&#37327;&#27169;&#22411;&#20559;&#35265;&#30340;&#24773;&#22659;&#20013;&#22914;&#20309;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#26222;&#21450;&#65292;&#39044;&#27979;&#20854;&#20351;&#29992;&#21487;&#33021;&#23545;&#20154;&#20204;&#36896;&#25104;&#20260;&#23475;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#19968;&#20010;&#21463;&#21040;&#20851;&#27880;&#30340;&#38382;&#39064;&#26159;&#36825;&#19968;&#25216;&#26415;&#22312;&#34892;&#20026;&#20013;&#26174;&#31034;&#20986;&#26377;&#23475;&#20559;&#35265;&#12290;&#23613;&#31649;&#24050;&#32463;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#26469;&#35780;&#20272;&#21644;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#65292;&#20294;&#25105;&#20204;&#34913;&#37327;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#36890;&#24120;&#19981;&#28165;&#26970;&#23427;&#20204;&#21040;&#24213;&#34913;&#37327;&#20102;&#20160;&#20040;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26469;&#35752;&#35770;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#24515;&#29702;&#27979;&#37327;&#23398;&#19987;&#27880;&#20110;&#34913;&#37327;&#19981;&#30452;&#25509;&#21487;&#35266;&#23519;&#21040;&#30340;&#27010;&#24565;&#65292;&#22914;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25506;&#35752;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#20004;&#20010;&#26680;&#24515;&#27010;&#24565;&#65292;&#21363;&#26500;&#24565;&#25928;&#24230;&#21644;&#27979;&#37327;&#24037;&#20855;&#30340;&#20449;&#24230;&#65292;&#24182;&#35752;&#35770;&#23427;&#20204;&#22312;&#34913;&#37327;&#27169;&#22411;&#20559;&#35265;&#30340;&#24773;&#22659;&#20013;&#22914;&#20309;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models and Natural Language Processing (NLP) technology rapidly develops and spreads into daily life, it becomes crucial to anticipate how its use could harm people. One problem that has received a lot of attention in recent years is that this technology has displayed harmful biases in its behavior. Although a lot of effort has been invested in assessing and mitigating these biases, our methods of measuring the biases of NLP models have serious problems (e.g., it is often unclear what they actually measure). In this paper, we provide an interdisciplinary approach to discussing the issue of NLP model bias by adopting the lens of psychometrics -- a field specialized in the measurement of concepts like bias that are not directly observable. In particular, we will explore two central notions from psychometrics, the construct validity and the reliability of measurement tools, and discuss how they can be applied in the context of measuring model bias. Our goal is to provide
&lt;/p&gt;</description></item><item><title>DialoGen&#26159;&#19968;&#31181;&#23545;&#35805;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#24191;&#20041;&#19978;&#19979;&#25991;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#23545;&#35805;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2210.06282</link><description>&lt;p&gt;
DialoGen: &#23545;&#35805;&#31995;&#32479;&#30340;&#24191;&#20041;&#38271;&#31243;&#19978;&#19979;&#25991;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DialoGen: Generalized Long-Range Context Representation for Dialogue Systems. (arXiv:2210.06282v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06282
&lt;/p&gt;
&lt;p&gt;
DialoGen&#26159;&#19968;&#31181;&#23545;&#35805;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#24191;&#20041;&#19978;&#19979;&#25991;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#23545;&#35805;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#31243;&#19978;&#19979;&#25991;&#24314;&#27169;&#23545;&#20110;&#23545;&#35805;&#29702;&#35299;&#21644;&#29983;&#25104;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#35805;&#19978;&#19979;&#25991;&#34920;&#31034;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#23558;&#26368;&#21518;-k&#20010;&#20808;&#21069;&#30340;&#35805;&#35821;&#36830;&#25509;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21253;&#21547;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#30340;&#23545;&#35805;&#32780;&#35328;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#19981;&#26159;&#26368;&#29702;&#24819;&#30340;&#65292;&#22240;&#20026;&#23427;&#26080;&#27861;&#36229;&#36234;&#26368;&#21518;-k&#20010;&#35805;&#35821;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DialoGen&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#26694;&#26550;&#65292;&#20854;&#20855;&#26377;&#21487;&#20197;&#36229;&#36234;&#26368;&#21518;-k&#20010;&#35805;&#35821;&#30340;&#24191;&#20041;&#19978;&#19979;&#25991;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#35813;&#26041;&#27861;&#36866;&#24212;&#20855;&#26377;&#38271;&#31243;&#20381;&#36182;&#30340;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#35782;&#21035;&#21644;&#21033;&#29992;&#26368;&#30456;&#20851;&#30340;&#21382;&#21490;&#35805;&#35821;&#65292;&#32780;&#19981;&#26159;&#25353;&#26102;&#38388;&#39034;&#24207;&#30340;&#26368;&#21518;-k&#20010;&#35805;&#35821;&#12290;&#25105;&#20204;&#22312;&#23545;&#35805;&#29983;&#25104;&#65288;&#24320;&#25918;&#22495;&#65289;&#21644;&#29702;&#35299;&#65288;DST&#65289;&#20219;&#21153;&#19978;&#30740;&#31350;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;DialoGen&#22312;DailyDialog&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-range context modeling is crucial to both dialogue understanding and generation. The most popular method for dialogue context representation is to concatenate the last-$k$ previous utterances. However, this method may not be ideal for conversations containing long-range dependencies as it cannot look beyond last-$k$ utterances. In this work, we propose DialoGen, a novel encoder-decoder based framework for conversational response generation with a generalized context representation that can look beyond the last-$k$ utterances. Hence the method is adaptive to conversations with long-range dependencies. The main idea of our approach is to identify and utilize the most relevant historical utterances instead of the last-$k$ utterances in chronological order. We study the effectiveness of our proposed method on both dialogue generation (open-domain) and understanding (DST) tasks. DialoGen achieves comparable performance with the state-of-the-art models on DailyDialog dataset. We also ob
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#22240;&#26524;&#27169;&#22411;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25506;&#35752;&#20102;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36731;&#37327;&#32423;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#26469;&#24110;&#21161;&#26816;&#27979;&#20219;&#21153;&#30340;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#65292;&#24182;&#22312;&#24615;&#21035;&#20195;&#35789;&#28040;&#35299;&#20219;&#21153;&#20013;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#24615;&#21035;&#19982;&#26102;&#38388;&#12289;&#24615;&#21035;&#19982;&#20301;&#32622;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.00131</link><description>&lt;p&gt;
&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#65306;&#19968;&#20010;&#20197;&#22240;&#26524;&#20851;&#31995;&#20026;&#22522;&#30784;&#30340;&#24615;&#21035;&#20195;&#35789;&#28040;&#35299;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Underspecification in Language Modeling Tasks: A Causality-Informed Study of Gendered Pronoun Resolution. (arXiv:2210.00131v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#22240;&#26524;&#27169;&#22411;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25506;&#35752;&#20102;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36731;&#37327;&#32423;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#26469;&#24110;&#21161;&#26816;&#27979;&#20219;&#21153;&#30340;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#65292;&#24182;&#22312;&#24615;&#21035;&#20195;&#35789;&#28040;&#35299;&#20219;&#21153;&#20013;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#24615;&#21035;&#19982;&#26102;&#38388;&#12289;&#24615;&#21035;&#19982;&#20301;&#32622;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#24120;&#24120;&#23384;&#22312;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#30340;&#38382;&#39064;&#65306;&#23545;&#20110;&#32473;&#23450;&#30340;&#26631;&#35760;&#39044;&#27979;&#65292;&#22312;&#25512;&#26029;&#26102;&#21487;&#33021;&#26377;&#22810;&#20010;&#21333;&#35789;&#31526;&#21512;&#29992;&#25143;&#20135;&#29983;&#33258;&#28982;&#35821;&#35328;&#30340;&#24847;&#22270;&#65292;&#28982;&#32780;&#22312;&#35757;&#32451;&#26102;&#21482;&#26377;&#19968;&#20010;&#21333;&#35789;&#33021;&#22815;&#26368;&#23567;&#21270;&#20219;&#21153;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#21512;&#29702;&#30340;&#22240;&#26524;&#26426;&#21046;&#65292;&#25551;&#36848;&#20102;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#22312;&#29983;&#25104;&#34394;&#20551;&#30456;&#20851;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#20854;&#31616;&#27905;&#24615;&#65292;&#25105;&#20204;&#30340;&#22240;&#26524;&#27169;&#22411;&#30452;&#25509;&#25351;&#23548;&#20102;&#20004;&#31181;&#36731;&#37327;&#32423;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#30340;&#24320;&#21457;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#20013;&#30340;&#24615;&#21035;&#20195;&#35789;&#28040;&#35299;&#19978;&#65292;&#20197;&#24110;&#21161; 1) &#26816;&#27979;&#25512;&#26029;&#26102;&#20219;&#21153;&#30340;&#19981;&#20805;&#20998;&#35268;&#33539;&#21270;&#65292;&#21033;&#29992;&#20102; 2&#65289;&#20043;&#21069;&#26410;&#25253;&#36947;&#30340;&#24615;&#21035;&#19982;&#26102;&#38388;&#12289;&#24615;&#21035;&#19982;&#20301;&#32622;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#28085;&#30422;&#20102; A&#65289;&#19981;&#21516;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;BERT-base&#21040;GPT 3.5&#65292;B&#65289;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#20174;&#36974;&#34109;&#21644;&#33258;&#22238;&#24402;&#35821;&#35328;&#24314;&#27169;&#21040;&#36825;&#20123;&#30446;&#26631;&#30340;&#28151;&#21512;&#65292;&#20197;&#21450;C&#65289;&#19981;&#21516;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#20174;&#20165;&#39044;&#35757;&#32451;&#21040;&#22686;&#24378;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language modeling tasks are often underspecified: for a given token prediction, many words may satisfy the user's intent of producing natural language at inference time, however only one word would minimize the task's loss function at training time. We provide a simple yet plausible causal mechanism describing the role underspecification plays in the generation of spurious correlations. Despite its simplicity, our causal model directly informs the development of two lightweight black-box evaluation methods, that we apply to gendered pronoun resolution tasks on a wide range of LLMs to 1) aid in the detection of inference-time task underspecification by exploiting 2) previously unreported gender vs. time and gender vs. location spurious correlations on LLMs with a range of A) sizes: from BERT-base to GPT 3.5, B) pre-training objectives: from masked &amp; autoregressive language modeling to a mixture of these objectives, and C) training stages: from pre-training only to reinforcement l
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35268;&#27169;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#24494;&#35843;&#29983;&#25104;&#26377;&#29992;&#30340;&#20013;&#38388;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#24120;&#35782;&#38382;&#31572;&#24615;&#33021;&#65292;&#24182;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#35814;&#32454;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2209.01232</link><description>&lt;p&gt;
&#20197;&#35268;&#27169;&#29983;&#25104;&#24120;&#35782;&#38382;&#31572;&#30340;&#35814;&#32454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Elaboration-Generating Commonsense Question Answering at Scale. (arXiv:2209.01232v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35268;&#27169;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#24494;&#35843;&#29983;&#25104;&#26377;&#29992;&#30340;&#20013;&#38388;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#24120;&#35782;&#38382;&#31572;&#24615;&#33021;&#65292;&#24182;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#35814;&#32454;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38656;&#35201;&#24120;&#35782;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-3&#65289;&#34987;&#29992;&#26469;&#29983;&#25104;&#34920;&#36798;&#32972;&#26223;&#30693;&#35782;&#20197;&#25552;&#39640;&#24615;&#33021;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#25104;&#26412;&#38750;&#24120;&#39640;&#65307;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#26377;&#29992;&#30340;&#20013;&#38388;&#19978;&#19979;&#25991;&#65292;&#31216;&#20026;&#35814;&#32454;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20132;&#26367;&#26356;&#26032;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;-&#35814;&#32454;&#35299;&#37322;&#29983;&#25104;&#22120;&#21644;&#31572;&#26696;&#39044;&#27979;&#22120;-&#20801;&#35768;&#23427;&#20204;&#30456;&#20114;&#24433;&#21709;&#12290;&#20351;&#29992;&#19981;&#21040;GPT-3&#21442;&#25968;&#30340;0.5&#65285;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19982;&#31867;&#20284;&#35268;&#27169;&#30340;&#26367;&#20195;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#22312;&#22235;&#20010;&#24120;&#35782;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#19978;&#25509;&#36817;GPT-3&#12290;&#20154;&#24037;&#35780;&#20272;&#26174;&#31034;&#29983;&#25104;&#30340;&#35814;&#32454;&#35299;&#37322;&#36136;&#37327;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In question answering requiring common sense, language models (e.g., GPT-3) have been used to generate text expressing background knowledge that helps improve performance. Yet the cost of working with such models is very high; in this work, we finetune smaller language models to generate useful intermediate context, referred to here as elaborations. Our framework alternates between updating two language models -- an elaboration generator and an answer predictor -- allowing each to influence the other. Using less than 0.5% of the parameters of GPT-3, our model outperforms alternatives with similar sizes and closes the gap on GPT-3 on four commonsense question answering benchmarks. Human evaluations show that the quality of the generated elaborations is high.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35770;&#35777;&#25366;&#25496;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#31934;&#24515;&#32452;&#32455;&#30340;&#35757;&#32451;&#26679;&#26412;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#22312;&#20943;&#23567;&#35757;&#32451;&#26679;&#26412;&#22823;&#23567;&#33267;&#23569;85&#65285;&#30340;&#24773;&#20917;&#19979;&#65292;&#36798;&#21040;&#26368;&#22823;&#24615;&#33021;&#30340;95&#65285;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#20379;&#26410;&#26469;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2205.11472</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#36229;&#36807;&#35268;&#27169;&#65306;&#20851;&#20110;&#35770;&#35777;&#25366;&#25496;&#25968;&#25454;&#38598;&#30340;&#26679;&#26412;&#21644;&#20027;&#39064;&#22823;&#23567;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Diversity Over Size: On the Effect of Sample and Topic Sizes for Argument Mining Datasets. (arXiv:2205.11472v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35770;&#35777;&#25366;&#25496;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#31934;&#24515;&#32452;&#32455;&#30340;&#35757;&#32451;&#26679;&#26412;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#22312;&#20943;&#23567;&#35757;&#32451;&#26679;&#26412;&#22823;&#23567;&#33267;&#23569;85&#65285;&#30340;&#24773;&#20917;&#19979;&#65292;&#36798;&#21040;&#26368;&#22823;&#24615;&#33021;&#30340;95&#65285;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#20379;&#26410;&#26469;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#35777;&#25366;&#25496;&#30340;&#20219;&#21153;&#26159;&#20174;&#22823;&#35268;&#27169;&#25991;&#26723;&#26469;&#28304;&#20013;&#25552;&#21462;&#29305;&#23450;&#20027;&#39064;&#30340;&#35770;&#35777;&#21477;&#23376;&#65292;&#36825;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#20154;&#31867;&#26469;&#35828;&#37117;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#22823;&#35268;&#27169;&#30340;&#35770;&#35777;&#25366;&#25496;&#25968;&#25454;&#38598;&#24456;&#23569;&#65292;&#32780;&#35782;&#21035;&#35770;&#35777;&#21477;&#23376;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#12290;&#22914;&#26524;&#36824;&#28041;&#21450;&#21040;&#26816;&#27979;&#26816;&#32034;&#21040;&#30340;&#35770;&#35777;&#30340;&#31435;&#22330;&#65292;&#36825;&#20010;&#20219;&#21153;&#23601;&#26356;&#21152;&#22256;&#38590;&#20102;&#12290;&#37492;&#20110;&#21019;&#24314;&#21512;&#36866;&#35268;&#27169;&#30340;&#35770;&#35777;&#25366;&#25496;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#21644;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#24819;&#30693;&#36947;&#26159;&#21542;&#26377;&#24517;&#35201;&#20026;&#20102;&#36798;&#21040;&#21487;&#25509;&#21463;&#30340;&#24615;&#33021;&#32780;&#22686;&#21152;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#31934;&#24515;&#32452;&#32455;&#30340;&#35757;&#32451;&#26679;&#26412;&#21644;&#22312;&#30456;&#20851;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#35757;&#32451;&#26679;&#26412;&#30340;&#22823;&#23567;&#20943;&#23569;&#33267;&#23569;85&#65285;&#65292;&#21516;&#26102;&#36798;&#21040;&#26368;&#22823;&#24615;&#33021;&#30340;95&#65285;&#12290;&#36825;&#31181;&#25910;&#30410;&#22312;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#19977;&#20010;&#35770;&#35777;&#25366;&#25496;&#20219;&#21153;&#20013;&#26159;&#19968;&#33268;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#20379;&#26410;&#26469;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of Argument Mining, that is extracting argumentative sentences for a specific topic from large document sources, is an inherently difficult task for machine learning models and humans alike, as large Argument Mining datasets are rare and recognition of argumentative sentences requires expert knowledge. The task becomes even more difficult if it also involves stance detection of retrieved arguments. Given the cost and complexity of creating suitably large Argument Mining datasets, we ask whether it is necessary for acceptable performance to have datasets growing in size. Our findings show that, when using carefully composed training samples and a model pretrained on related tasks, we can reach 95% of the maximum performance while reducing the training sample size by at least 85%. This gain is consistent across three Argument Mining tasks on three different datasets. We also publish a new dataset for future benchmarking.
&lt;/p&gt;</description></item><item><title>&#23545;&#21382;&#21490;&#21477;&#27861;&#32593;&#32476;&#30340;&#20998;&#23618;&#32452;&#32455;&#20998;&#26512;&#25581;&#31034;&#20102;&#21477;&#27861;&#28436;&#21270;&#30340;&#26041;&#24335;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35828;&#35805;&#32773;&#30340;&#20132;&#27969;&#38656;&#27714;&#26159;&#21477;&#27861;&#32452;&#32455;&#30340;&#37325;&#35201;&#39537;&#21160;&#21147;&#12290;</title><link>http://arxiv.org/abs/2112.05783</link><description>&lt;p&gt;
&#21477;&#27861;&#30340;&#20998;&#23618;&#32452;&#32455;
&lt;/p&gt;
&lt;p&gt;
The Hierarchical Organization of Syntax. (arXiv:2112.05783v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.05783
&lt;/p&gt;
&lt;p&gt;
&#23545;&#21382;&#21490;&#21477;&#27861;&#32593;&#32476;&#30340;&#20998;&#23618;&#32452;&#32455;&#20998;&#26512;&#25581;&#31034;&#20102;&#21477;&#27861;&#28436;&#21270;&#30340;&#26041;&#24335;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35828;&#35805;&#32773;&#30340;&#20132;&#27969;&#38656;&#27714;&#26159;&#21477;&#27861;&#32452;&#32455;&#30340;&#37325;&#35201;&#39537;&#21160;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#32467;&#26500;&#26159;&#22797;&#26434;&#31995;&#32479;&#30340;&#38544;&#34255;&#39592;&#26550;&#65292;&#36890;&#36807;&#23545;&#20854;&#20998;&#26512;&#21487;&#20197;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20854;&#32467;&#26500;&#21644;&#28436;&#21464;&#26041;&#24335;&#12290;&#25105;&#20204;&#35748;&#20026;&#35821;&#35328;&#20063;&#26159;&#19968;&#31181;&#22797;&#26434;&#33258;&#36866;&#24212;&#31995;&#32479;&#65292;&#20855;&#26377;&#22810;&#20010;&#22797;&#26434;&#32593;&#32476;&#65292;&#25429;&#25417;&#20102;&#20854;&#32467;&#26500;&#21644;&#21151;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20915;&#23450;&#20998;&#26512;&#21382;&#21490;&#21477;&#27861;&#32593;&#32476;&#30340;&#20998;&#23618;&#32452;&#32455;&#65292;&#20197;&#20102;&#35299;&#21477;&#27861;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;11&#19990;&#32426;&#21040;17&#19990;&#32426;&#30340;&#24503;&#35821;&#25991;&#26412;&#35821;&#26009;&#24211;&#21019;&#24314;&#20102;&#36825;&#20123;&#32593;&#32476;&#65292;&#37325;&#28857;&#20851;&#27880;&#36825;&#20123;&#32593;&#32476;&#30340;&#20998;&#23618;&#27700;&#24179;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#35828;&#35805;&#32773;&#30340;&#29305;&#23450;&#20132;&#27969;&#38656;&#27714;&#19978;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#20174;&#21382;&#26102;&#30340;&#35282;&#24230;&#23454;&#35777;&#36861;&#36394;&#21477;&#27861;&#32467;&#26500;&#30340;&#20986;&#29616;&#65292;&#20174;&#32780;&#33021;&#22815;&#23558;&#35828;&#35805;&#32773;&#30340;&#20132;&#27969;&#38656;&#27714;&#19982;&#36825;&#20123;&#32467;&#26500;&#36827;&#34892;&#26144;&#23556;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#21477;&#27861;&#32467;&#26500;&#31216;&#20026;"&#21477;&#27861;&#20132;&#38469;&#23618;&#27425;&#32467;&#26500;"&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35828;&#35805;&#32773;&#30340;&#20132;&#27969;&#38656;&#27714;&#26159;&#21477;&#27861;&#30340;&#32452;&#32455;&#21147;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Hierarchies are the hidden backbones of complex systems and their analysis allows for a deeper understanding of their structure and how they evolve. We consider languages also to be complex adaptive systems with several intricate networks that capture their structure and function. Hence, we decided to analyze the hierarchical organization of historical syntactic networks to understand how syntax evolves over time. We created these networks from a corpus of German texts from the 11th to 17th centuries, focusing on the hierarchical levels of these networks. diachronically and to map them to specific communicative needs of speakers. We developed a framework to empirically track the emergence of syntactic structures diachronically, enabling us to map the communicative needs of speakers with these structures. We named these syntactic structures "syntactic communicative hierarchies." We showed that the communicative needs of speakers are the organizational force of syntax. Thus, we argue tha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#33258;&#21160;&#23398;&#26415;&#35770;&#25991;&#23457;&#31295;&#65288;ASPR&#65289;&#30340;&#27010;&#24565;&#21644;&#27969;&#31243;&#65292;&#32508;&#36848;&#20102;&#23454;&#29616;&#20840;&#38754;&#35745;&#31639;&#26426;&#21270;&#23457;&#31295;&#27969;&#31243;&#30340;&#30456;&#20851;&#25991;&#29486;&#21644;&#25216;&#26415;&#65292;&#21516;&#26102;&#25351;&#20986;&#23454;&#29616;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22914;&#25991;&#26723;&#35299;&#26512;&#21644;&#34920;&#36798;&#19981;&#23436;&#32654;&#12289;&#25968;&#25454;&#19981;&#36275;&#12289;&#20154;&#26426;&#20132;&#20114;&#32570;&#38519;&#21644;&#21457;&#29616;&#20302;&#36136;&#37327;&#25991;&#31456;&#30340;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2111.07533</link><description>&lt;p&gt;
&#33258;&#21160;&#23398;&#26415;&#35770;&#25991;&#23457;&#31295;&#65306;&#27010;&#24565;&#12289;&#25216;&#26415;&#19982;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated scholarly paper review: Concepts, technologies, and challenges. (arXiv:2111.07533v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.07533
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#33258;&#21160;&#23398;&#26415;&#35770;&#25991;&#23457;&#31295;&#65288;ASPR&#65289;&#30340;&#27010;&#24565;&#21644;&#27969;&#31243;&#65292;&#32508;&#36848;&#20102;&#23454;&#29616;&#20840;&#38754;&#35745;&#31639;&#26426;&#21270;&#23457;&#31295;&#27969;&#31243;&#30340;&#30456;&#20851;&#25991;&#29486;&#21644;&#25216;&#26415;&#65292;&#21516;&#26102;&#25351;&#20986;&#23454;&#29616;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22914;&#25991;&#26723;&#35299;&#26512;&#21644;&#34920;&#36798;&#19981;&#23436;&#32654;&#12289;&#25968;&#25454;&#19981;&#36275;&#12289;&#20154;&#26426;&#20132;&#20114;&#32570;&#38519;&#21644;&#21457;&#29616;&#20302;&#36136;&#37327;&#25991;&#31456;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#34892;&#35780;&#23457;&#26159;&#30740;&#31350;&#35780;&#20215;&#30340;&#24191;&#27867;&#25509;&#21463;&#26426;&#21046;&#65292;&#22312;&#23398;&#26415;&#20986;&#29256;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#25928;&#29575;&#20302;&#19979;&#21644;&#21487;&#37325;&#22797;&#24615;&#24046;&#65292;&#36825;&#19968;&#26426;&#21046;&#38271;&#26399;&#20197;&#26469;&#22791;&#21463;&#25209;&#35780;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#36741;&#21161;&#21516;&#34892;&#35780;&#23457;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#28041;&#21450;&#20154;&#21592;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#38480;&#21046;&#20173;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#23398;&#26415;&#35770;&#25991;&#23457;&#31295;&#65288;ASPR&#65289;&#30340;&#27010;&#24565;&#21644;&#27969;&#31243;&#65292;&#24182;&#32508;&#36848;&#20102;&#23454;&#29616;&#20840;&#38754;&#35745;&#31639;&#26426;&#21270;&#23457;&#31295;&#27969;&#31243;&#30340;&#30456;&#20851;&#25991;&#29486;&#21644;&#25216;&#26415;&#12290;&#22312;&#23457;&#26597;&#21644;&#35752;&#35770;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;ASPR &#30340;&#27599;&#20010;&#38454;&#27573;&#24050;&#32463;&#26377;&#30456;&#24212;&#30340;&#30740;&#31350;&#21644;&#21021;&#27493;&#23454;&#26045;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;ASPR&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#20027;&#35201;&#22256;&#38590;&#22312;&#20110;&#25991;&#26723;&#35299;&#26512;&#21644;&#34920;&#36798;&#19981;&#23436;&#32654;&#12289;&#25968;&#25454;&#19981;&#36275;&#12289;&#20154;&#26426;&#20132;&#20114;&#32570;&#38519;&#21644;&#21457;&#29616;&#20302;&#36136;&#37327;&#25991;&#31456;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peer review is a widely accepted mechanism for research evaluation, playing a pivotal role in academic publishing. However, criticisms have long been leveled on this mechanism, mostly because of its poor efficiency and low reproducibility. Recent years have seen the application of artificial intelligence (AI) in assisting the peer review process. Nonetheless, with the involvement of humans, such limitations remain inevitable. In this paper, we propose the concept and pipeline of automated scholarly paper review (ASPR) and review the relevant literature and technologies of achieving a full-scale computerized review process. On the basis of the review and discussion, we conclude that there is already corresponding research and preliminary implementation at each stage of ASPR. We further look into the challenges in ASPR with the existing technologies. The major difficulties lie in imperfect document parsing and representation, inadequate data, defective human-computer interaction, and fla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#35843;&#26597;&#20102;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24635;&#32467;&#20102;&#23427;&#20204;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#12290;</title><link>http://arxiv.org/abs/2110.05006</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65306;&#31995;&#32479;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models in Biomedical Domain: A Systematic Survey. (arXiv:2110.05006v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.05006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#35843;&#26597;&#20102;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24635;&#32467;&#20102;&#23427;&#20204;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper systematically surveys pre-trained language models in the biomedical domain, summarizes their recent progress and applications, and proposes a taxonomy.
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#25104;&#20026;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#36825;&#20063;&#26377;&#30410;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65306;&#26469;&#33258;&#20449;&#24687;&#23398;&#12289;&#21307;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#65288;CS&#65289;&#31038;&#21306;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;PLMs&#65292;&#20363;&#22914;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#12289;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12289;&#34507;&#30333;&#36136;&#21644;DNA&#24207;&#21015;&#65292;&#29992;&#20110;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29983;&#29289;&#21307;&#23398;PLMs&#30340;&#36328;&#23398;&#31185;&#29305;&#24615;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#31038;&#21306;&#20043;&#38388;&#30340;&#20256;&#25773;&#65307;&#19968;&#20123;&#29616;&#26377;&#30340;&#24037;&#20316;&#30456;&#20114;&#23396;&#31435;&#65292;&#32570;&#20047;&#20840;&#38754;&#30340;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;&#26399;&#26395;&#19968;&#39033;&#35843;&#26597;&#65292;&#19981;&#20165;&#31995;&#32479;&#22320;&#23457;&#26597;&#29983;&#29289;&#21307;&#23398;PLMs&#21450;&#20854;&#24212;&#29992;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#32780;&#19988;&#26631;&#20934;&#21270;&#26415;&#35821;&#21644;&#22522;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#20197;&#21450;&#23427;&#20204;&#22312;&#29983;&#29289;&#21307;&#23398;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21160;&#26426;&#24182;&#25552;&#20986;&#20102;&#29616;&#26377;&#29983;&#29289;&#21307;&#23398;PLMs&#30340;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) have been the de facto paradigm for most natural language processing (NLP) tasks. This also benefits biomedical domain: researchers from informatics, medicine, and computer science (CS) communities propose various PLMs trained on biomedical datasets, e.g., biomedical text, electronic health records, protein, and DNA sequences for various biomedical tasks. However, the cross-discipline characteristics of biomedical PLMs hinder their spreading among communities; some existing works are isolated from each other without comprehensive comparison and discussions. It expects a survey that not only systematically reviews recent advances of biomedical PLMs and their applications but also standardizes terminology and benchmarks. In this paper, we summarize the recent progress of pre-trained language models in the biomedical domain and their applications in biomedical downstream tasks. Particularly, we discuss the motivations and propose a taxonomy of existing b
&lt;/p&gt;</description></item></channel></rss>