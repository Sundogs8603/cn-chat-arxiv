<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#35777;&#25454;&#30340;&#31435;&#22330;&#25277;&#21462;&#32593;&#32476;&#65288;SEN&#65289;&#26469;&#26816;&#27979;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;&#12290;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#35777;&#25454;&#30340;&#31435;&#22330;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20849;&#29616;&#20851;&#31995;&#30340;&#25903;&#25345;-&#21453;&#39539;&#20998;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20844;&#20849;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01766</link><description>&lt;p&gt;
&#25903;&#25345;&#36824;&#26159;&#21453;&#39539;&#65306;&#20998;&#26512;&#35777;&#25454;&#31435;&#22330;&#20197;&#26816;&#27979;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation. (arXiv:2311.01766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#35777;&#25454;&#30340;&#31435;&#22330;&#25277;&#21462;&#32593;&#32476;&#65288;SEN&#65289;&#26469;&#26816;&#27979;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;&#12290;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#35777;&#25454;&#30340;&#31435;&#22330;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20849;&#29616;&#20851;&#31995;&#30340;&#25903;&#25345;-&#21453;&#39539;&#20998;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20844;&#20849;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#35823;&#23548;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#22269;&#23478;&#32423;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#26159;&#21508;&#31181;&#22312;&#32447;&#20260;&#23475;&#30340;&#20027;&#35201;&#26469;&#28304;&#20043;&#19968;&#12290;&#20854;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#35823;&#23548;&#20449;&#24687;&#24418;&#24335;&#26159;&#19978;&#19979;&#25991;&#38169;&#35823;&#65288;OOC&#65289;&#20449;&#24687;&#65292;&#20854;&#20013;&#19981;&#21516;&#30340;&#20449;&#24687;&#34987;&#38169;&#35823;&#22320;&#20851;&#32852;&#36215;&#26469;&#65292;&#20363;&#22914;&#30495;&#23454;&#22270;&#20687;&#19982;&#34394;&#20551;&#30340;&#25991;&#26412;&#26631;&#39064;&#25110;&#35823;&#23548;&#24615;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#23613;&#31649;&#19968;&#20123;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#22806;&#37096;&#35777;&#25454;&#26469;&#25269;&#24481;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#21516;&#31435;&#22330;&#30340;&#19981;&#21516;&#35777;&#25454;&#30340;&#20316;&#29992;&#12290;&#21463;&#21040;&#35777;&#25454;&#31435;&#22330;&#20195;&#34920;&#19981;&#21516;&#26816;&#27979;&#32467;&#26524;&#30340;&#20559;&#35265;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#25552;&#21462;&#22810;&#27169;&#24577;&#35777;&#25454;&#30340;&#31435;&#22330;&#30340;&#31435;&#22330;&#25277;&#21462;&#32593;&#32476;&#65288;SEN&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22522;&#20110;&#21629;&#21517;&#23454;&#20307;&#30340;&#20849;&#29616;&#20851;&#31995;&#35745;&#31639;&#30340;&#25903;&#25345;-&#21453;&#39539;&#20998;&#25968;&#21040;&#25991;&#26412;SEN&#20013;&#12290;&#23545;&#20844;&#20849;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mis- and disinformation online have become a major societal problem as major sources of online harms of different kinds. One common form of mis- and disinformation is out-of-context (OOC) information, where different pieces of information are falsely associated, e.g., a real image combined with a false textual caption or a misleading textual description. Although some past studies have attempted to defend against OOC mis- and disinformation through external evidence, they tend to disregard the role of different pieces of evidence with different stances. Motivated by the intuition that the stance of evidence represents a bias towards different detection results, we propose a stance extraction network (SEN) that can extract the stances of different pieces of multi-modal evidence in a unified framework. Moreover, we introduce a support-refutation score calculated based on the co-occurrence relations of named entities into the textual SEN. Extensive experiments on a public large-scale data
&lt;/p&gt;</description></item><item><title>JADE&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#26512;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;&#65292;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;LLM&#65292;&#24182;&#29983;&#25104;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00286</link><description>&lt;p&gt;
JADE&#65306;&#22522;&#20110;&#35821;&#35328;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
JADE: A Linguistic-based Safety Evaluation Platform for LLM. (arXiv:2311.00286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00286
&lt;/p&gt;
&lt;p&gt;
JADE&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#26512;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;&#65292;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;LLM&#65292;&#24182;&#29983;&#25104;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JADE&#65292;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#20998;&#26512;&#30340;&#27169;&#31946;&#27979;&#35797;&#24179;&#21488;&#65292;&#36890;&#36807;&#22686;&#24378;&#31181;&#23376;&#38382;&#39064;&#30340;&#35821;&#35328;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#24182;&#22987;&#32456;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#19977;&#31867;LLM&#65306;&#20843;&#20010;&#24320;&#28304;&#20013;&#25991;LLM&#65292;&#20845;&#20010;&#21830;&#19994;&#20013;&#25991;LLM&#21644;&#22235;&#20010;&#21830;&#19994;&#33521;&#25991;LLM&#12290;JADE&#20026;&#36825;&#19977;&#31867;LLM&#29983;&#25104;&#20102;&#19977;&#20010;&#23433;&#20840;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#65306;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#21516;&#26102;&#35302;&#21457;&#22810;&#20010;LLM&#30340;&#26377;&#23475;&#29983;&#25104;&#65292;&#24179;&#22343;&#19981;&#23433;&#20840;&#29983;&#25104;&#27604;&#20363;&#20026;70%&#65288;&#35831;&#21442;&#35265;&#19979;&#34920;&#65289;&#65292;&#21516;&#26102;&#36825;&#20123;&#38382;&#39064;&#20173;&#28982;&#26159;&#33258;&#28982;&#12289;&#27969;&#30021;&#19988;&#20445;&#30041;&#20102;&#26680;&#24515;&#30340;&#19981;&#23433;&#20840;&#35821;&#20041;&#12290;&#25105;&#20204;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#21457;&#24067;&#20102;&#23545;&#21830;&#19994;&#33521;&#25991;LLM&#21644;&#24320;&#28304;&#33521;&#25991;LLM&#29983;&#25104;&#30340;&#22522;&#20934;&#28436;&#31034;&#65306;https://github.com/whitzard-ai/jade-db&#12290;&#23545;&#20110;&#23545;JADE&#29983;&#25104;&#30340;&#26356;&#22810;&#38382;&#39064;&#24863;&#20852;&#36259;&#30340;&#35835;&#32773;&#65292;&#35831;&#19982;&#25105;&#20204;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present \textit{JADE}, a targeted linguistic fuzzing platform which strengthens the linguistic complexity of seed questions to simultaneously and consistently break a wide range of widely-used LLMs categorized in three groups: eight open-sourced Chinese, six commercial Chinese and four commercial English LLMs. JADE generates three safety benchmarks for the three groups of LLMs, which contain unsafe questions that are highly threatening: the questions simultaneously trigger harmful generation of multiple LLMs, with an average unsafe generation ratio of \textbf{$70\%$} (please see the table below), while are still natural questions, fluent and preserving the core unsafe semantics. We release the benchmark demos generated for commercial English LLMs and open-sourced English LLMs in the following link: https://github.com/whitzard-ai/jade-db. For readers who are interested in evaluating on more questions generated by JADE, please contact us.  \textit{JADE} is based on Noam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#26512;&#21477;&#23376;&#30340;&#24847;&#20041;&#32467;&#26500;&#26041;&#38754;&#30340;&#25104;&#21151;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#29983;&#25104;&#21644;&#37325;&#26500;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#21477;&#23376;&#32467;&#26500;&#25110;&#35821;&#20041;&#25512;&#29702;&#20219;&#21153;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17793</link><description>&lt;p&gt;
"&#24744;&#26159;&#19968;&#20301;&#19987;&#23478;&#35821;&#35328;&#27880;&#37322;&#32773;"&#65306;&#20316;&#20026;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#20998;&#26512;&#22120;&#30340;LLMs&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
"You Are An Expert Linguistic Annotator": Limits of LLMs as Analyzers of Abstract Meaning Representation. (arXiv:2310.17793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#26512;&#21477;&#23376;&#30340;&#24847;&#20041;&#32467;&#26500;&#26041;&#38754;&#30340;&#25104;&#21151;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#29983;&#25104;&#21644;&#37325;&#26500;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#21477;&#23376;&#32467;&#26500;&#25110;&#35821;&#20041;&#25512;&#29702;&#20219;&#21153;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#20351;&#29992;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#29087;&#32451;&#24230;&#21644;&#27969;&#30021;&#24615;&#12290;&#36825;&#26159;&#21542;&#24847;&#21619;&#30528;&#23427;&#20204;&#20063;&#24050;&#32463;&#33719;&#24471;&#20102;&#20851;&#20110;&#35821;&#35328;&#30340;&#28145;&#21051;&#35821;&#35328;&#30693;&#35782;&#65292;&#20197;&#33267;&#20110;&#23427;&#20204;&#21487;&#20197;&#20805;&#24403;"&#19987;&#23478;&#35821;&#35328;&#27880;&#37322;&#32773;"&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;GPT-3&#12289;ChatGPT&#21644;GPT-4&#27169;&#22411;&#22312;&#21477;&#23376;&#24847;&#20041;&#32467;&#26500;&#20998;&#26512;&#20013;&#30340;&#25104;&#21151;&#21644;&#38480;&#21046;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65307;Banarescu&#31561;&#20154;&#65292;2013&#65289;&#20998;&#26512;&#24418;&#24335;&#20027;&#20041;&#65292;&#35813;&#24418;&#24335;&#20027;&#20041;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#22270;&#24418;&#21270;&#21477;&#23376;&#24847;&#20041;&#32467;&#26500;&#34920;&#31034;&#65292;&#21516;&#26102;&#20174;&#34920;&#38754;&#24418;&#24335;&#20013;&#25277;&#35937;&#20986;&#26469;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#22312;&#36825;&#31181;&#35821;&#20041;&#32467;&#26500;&#20998;&#26512;&#19978;&#30340;&#32467;&#26524;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#27604;&#36739;&#65306;1&#65289;&#22522;&#20110;&#38646;&#23556;&#21644;&#23569;&#23398;&#26679;&#26412;&#30340;AMR&#35299;&#26512;&#30340;&#30452;&#25509;&#29983;&#25104;&#65292;&#20197;&#21450;2&#65289;&#36890;&#36807;&#20803;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65288;&#20363;&#22914;"&#30830;&#23450;&#35813;&#21477;&#23376;&#30340;&#20027;&#35201;&#20107;&#20214;&#65292;&#20197;&#21450;&#19982;&#35813;&#20107;&#20214;&#23545;&#24212;&#30340;&#35859;&#35789;"&#65289;&#38388;&#25509;&#30340;&#37096;&#20998;&#37325;&#26500;AMR&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#37325;&#26032;&#29983;&#25104;&#27491;&#30830;&#30340;AMR&#35299;&#26512;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#21477;&#23376;&#32467;&#26500;&#25110;&#35821;&#20041;&#25512;&#29702;&#20219;&#21153;&#20013;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show amazing proficiency and fluency in the use of language. Does this mean that they have also acquired insightful linguistic knowledge about the language, to an extent that they can serve as an "expert linguistic annotator"? In this paper, we examine the successes and limitations of the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning structure, focusing on the Abstract Meaning Representation (AMR; Banarescu et al. 2013) parsing formalism, which provides rich graphical representations of sentence meaning structure while abstracting away from surface forms. We compare models' analysis of this semantic structure across two settings: 1) direct production of AMR parses based on zero- and few-shot prompts, and 2) indirect partial reconstruction of AMR via metalinguistic natural language queries (e.g., "Identify the primary event of this sentence, and the predicate corresponding to that event."). Across these settings, we find that models can re
&lt;/p&gt;</description></item><item><title>SSLCL&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#27169;&#22411;&#20559;&#21521;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#12290;&#23427;&#35299;&#20915;&#20102;&#22522;&#20110;SCL&#30340;&#26041;&#27861;&#20013;&#22823;&#25209;&#37327;&#22823;&#23567;&#30340;&#38480;&#21046;&#21644;&#19982;&#29616;&#26377;ERC&#27169;&#22411;&#19981;&#20860;&#23481;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25237;&#24433;&#31163;&#25955;&#26631;&#31614;&#21040;&#23494;&#38598;&#34920;&#31034;&#20013;&#26469;&#25913;&#21892;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.16676</link><description>&lt;p&gt;
SSLCL: &#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#27169;&#22411;&#20559;&#21521;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
SSLCL: An Efficient Model-Agnostic Supervised Contrastive Learning Framework for Emotion Recognition in Conversations. (arXiv:2310.16676v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16676
&lt;/p&gt;
&lt;p&gt;
SSLCL&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#27169;&#22411;&#20559;&#21521;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#12290;&#23427;&#35299;&#20915;&#20102;&#22522;&#20110;SCL&#30340;&#26041;&#27861;&#20013;&#22823;&#25209;&#37327;&#22823;&#23567;&#30340;&#38480;&#21046;&#21644;&#19982;&#29616;&#26377;ERC&#27169;&#22411;&#19981;&#20860;&#23481;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25237;&#24433;&#31163;&#25955;&#26631;&#31614;&#21040;&#23494;&#38598;&#34920;&#31034;&#20013;&#26469;&#25913;&#21892;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035; (ERC) &#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#26816;&#27979;&#23545;&#35805;&#20013;&#21457;&#35328;&#32773;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;ERC&#26041;&#27861;&#19987;&#27880;&#20110;&#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064; (SCL) &#26469;&#22686;&#24378;&#23398;&#21040;&#30340;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;ERC&#20013;&#22522;&#20110;SCL&#30340;&#26041;&#27861;&#21463;&#21040;&#22823;&#25209;&#37327;&#22823;&#23567;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;ERC&#27169;&#22411;&#19981;&#20860;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26080;&#27169;&#22411;&#20559;&#21521;&#30340;SCL&#26694;&#26550;&#65292;&#21517;&#20026;Supervised Sample-Label Contrastive Learning with Soft-HGR Maximal Correlation (SSLCL)&#65292;&#23427;&#28040;&#38500;&#20102;&#23545;&#22823;&#25209;&#37327;&#22823;&#23567;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;ERC&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#32780;&#19981;&#24341;&#20837;&#20219;&#20309;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#20551;&#35774;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26631;&#31614;&#34920;&#31034;&#30340;&#26032;&#35270;&#35282;&#65292;&#36890;&#36807;&#23558;&#31163;&#25955;&#26631;&#31614;&#25237;&#24433;&#21040;&#23494;&#38598;&#34920;&#31034;&#20013;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition in conversations (ERC) is a rapidly evolving task within the natural language processing community, which aims to detect the emotions expressed by speakers during a conversation. Recently, a growing number of ERC methods have focused on leveraging supervised contrastive learning (SCL) to enhance the robustness and generalizability of learned features. However, current SCL-based approaches in ERC are impeded by the constraint of large batch sizes and the lack of compatibility with most existing ERC models. To address these challenges, we propose an efficient and model-agnostic SCL framework named Supervised Sample-Label Contrastive Learning with Soft-HGR Maximal Correlation (SSLCL), which eliminates the need for a large batch size and can be seamlessly integrated with existing ERC models without introducing any model-specific assumptions. Specifically, we introduce a novel perspective on utilizing label representations by projecting discrete labels into dense embeddi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#30693;&#35782;&#20445;&#30041;&#21098;&#26525;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.13191</link><description>&lt;p&gt;
&#26397;&#30528;&#40065;&#26834;&#21098;&#26525;&#65306;&#19968;&#31181;&#38754;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#30693;&#35782;&#20445;&#30041;&#21098;&#26525;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models. (arXiv:2310.13191v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#30693;&#35782;&#20445;&#30041;&#21098;&#26525;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21098;&#26525;&#30446;&#26631;&#36817;&#26399;&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#20934;&#30830;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#36824;&#21253;&#25324;&#23545;&#35821;&#35328;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#25345;&#32493;&#22686;&#21152;&#27169;&#22411;&#31232;&#30095;&#24615;&#26102;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#25552;&#21319;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#12290;&#38543;&#30528;&#20154;&#20204;&#27493;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#20195;&#65292;&#36825;&#20123;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#31361;&#20986;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#19982;&#20854;&#28085;&#30422;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#31243;&#24230;&#25104;&#27491;&#27604;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#30340;&#21098;&#26525;&#31574;&#30053;&#65292;&#26088;&#22312;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#39044;&#35757;&#32451;&#30693;&#35782;&#65292;&#20197;&#24544;&#23454;&#22320;&#22797;&#21046;&#23494;&#38598;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#21644;&#29305;&#24449;&#31354;&#38388;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#27599;&#19968;&#23618;&#30340;&#37325;&#26500;&#35823;&#24046;&#19981;&#20165;&#28304;&#33258;&#33258;&#36523;&#65292;&#36824;&#21253;&#25324;&#21069;&#38754;&#23618;&#30340;&#32047;&#31215;&#35823;&#24046;&#65292;&#28982;&#21518;&#36827;&#34892;&#33258;&#36866;&#24212;&#30340;&#30699;&#27491;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the era of large language models, these issues become increasingly prominent. This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass. Accordingly, we introduce a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre-trained knowledge during the pruning process. In this setup, each layer's reconstruction error not only originates from itself but also includes cumulative error from preceding layers, followed by an adaptive rectification. Compared to other state-of-art baselines, our approach demonstrates a superior balance bet
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#23545;&#35805;&#25688;&#35201;&#26041;&#27861;&#26080;&#27861;&#32771;&#34385;&#29992;&#25143;&#30340;&#29305;&#23450;&#20852;&#36259;&#65292;&#32780;&#25351;&#23548;&#23545;&#35805;&#25688;&#35201;&#30340;&#24341;&#20837;&#21487;&#20197;&#24110;&#21161;&#25193;&#23637;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#26041;&#27861;&#26469;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#26597;&#35810;&#25688;&#35201;&#19977;&#20803;&#32452;&#65292;&#24182;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#26469;&#25193;&#23637;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10981</link><description>&lt;p&gt;
&#20351;&#29992;&#26597;&#35810;&#32858;&#21512;&#30340;&#25351;&#23548;&#24615;&#23545;&#35805;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Instructive Dialogue Summarization with Query Aggregations. (arXiv:2310.10981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10981
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#23545;&#35805;&#25688;&#35201;&#26041;&#27861;&#26080;&#27861;&#32771;&#34385;&#29992;&#25143;&#30340;&#29305;&#23450;&#20852;&#36259;&#65292;&#32780;&#25351;&#23548;&#23545;&#35805;&#25688;&#35201;&#30340;&#24341;&#20837;&#21487;&#20197;&#24110;&#21161;&#25193;&#23637;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#26041;&#27861;&#26469;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#26597;&#35810;&#25688;&#35201;&#19977;&#20803;&#32452;&#65292;&#24182;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#26469;&#25193;&#23637;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#23545;&#35805;&#25688;&#35201;&#26041;&#27861;&#30452;&#25509;&#29983;&#25104;&#25688;&#35201;&#65292;&#19981;&#32771;&#34385;&#29992;&#25143;&#30340;&#29305;&#23450;&#20852;&#36259;&#12290;&#36825;&#22312;&#29992;&#25143;&#26356;&#21152;&#20851;&#27880;&#29305;&#23450;&#20027;&#39064;&#25110;&#26041;&#38754;&#30340;&#24773;&#20917;&#19979;&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#38543;&#30528;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25351;&#23548;&#23545;&#35805;&#26469;&#25193;&#23637;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#30340;&#33021;&#21147;&#38598;&#12290;&#20026;&#20102;&#20811;&#26381;&#25351;&#23548;&#24615;&#23545;&#35805;&#25688;&#35201;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#27493;&#26041;&#27861;&#26469;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;&#25688;&#35201;&#19977;&#20803;&#32452;&#12290;&#36825;&#20010;&#36807;&#31243;&#21253;&#25324;&#20197;&#25688;&#35201;&#20026;&#38170;&#28857;&#30340;&#26597;&#35810;&#29983;&#25104;&#12289;&#26597;&#35810;&#36807;&#28388;&#21644;&#22522;&#20110;&#26597;&#35810;&#30340;&#25688;&#35201;&#29983;&#25104;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;InstructDS&#65288;&#25351;&#23548;&#24615;&#23545;&#35805;&#25688;&#35201;&#65289;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#23545;&#35805;&#25688;&#35201;&#21644;&#23545;&#35805;&#38405;&#35835;&#29702;&#35299;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional dialogue summarization methods directly generate summaries and do not consider user's specific interests. This poses challenges in cases where the users are more focused on particular topics or aspects. With the advancement of instruction-finetuned language models, we introduce instruction-tuning to dialogues to expand the capability set of dialogue summarization models. To overcome the scarcity of instructive dialogue summarization data, we propose a three-step approach to synthesize high-quality query-based summarization triples. This process involves summary-anchored query generation, query filtering, and query-based summary generation. By training a unified model called InstructDS (Instructive Dialogue Summarization) on three summarization datasets with multi-purpose instructive triples, we expand the capability of dialogue summarization models. We evaluate our method on four datasets, including dialogue summarization and dialogue reading comprehension. Experimental re
&lt;/p&gt;</description></item><item><title>GoLLIE &#26159;&#19968;&#20010;&#36981;&#24490;&#27880;&#37322;&#25351;&#21335;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#20197;&#25913;&#36827;&#26410;&#35265;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03668</link><description>&lt;p&gt;
GoLLIE:&#27880;&#37322;&#25351;&#21335;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction. (arXiv:2310.03668v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03668
&lt;/p&gt;
&lt;p&gt;
GoLLIE &#26159;&#19968;&#20010;&#36981;&#24490;&#27880;&#37322;&#25351;&#21335;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#20197;&#25913;&#36827;&#26410;&#35265;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#32467;&#21512;&#25351;&#23548;&#35843;&#20248;&#24050;&#32463;&#22312;&#27867;&#21270;&#21040;&#26410;&#35265;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#20449;&#24687;&#25277;&#21462; (IE) &#26041;&#38754;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#36739;&#24046;&#65292;&#33853;&#21518;&#20110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#36890;&#24120;&#65292;IE &#20219;&#21153;&#30340;&#29305;&#28857;&#26159;&#22797;&#26434;&#30340;&#27880;&#37322;&#25351;&#21335;&#65292;&#25551;&#36848;&#20219;&#21153;&#24182;&#32473;&#20986;&#31034;&#20363;&#32473;&#20154;&#31867;&#12290;&#20808;&#21069;&#21033;&#29992;&#36825;&#26679;&#30340;&#20449;&#24687;&#30340;&#23581;&#35797;&#37117;&#22833;&#36133;&#20102;&#65292;&#21363;&#20351;&#20351;&#29992;&#26368;&#22823;&#30340;&#27169;&#22411;&#65292;&#23427;&#20204;&#20063;&#19981;&#33021;&#30452;&#25509;&#36981;&#24490;&#25351;&#21335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20449;&#24687;&#25277;&#21462;&#30340;&#25351;&#21335;&#36981;&#24490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; GoLLIE (Guideline-following Large Language Model for IE)&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24494;&#35843;&#20197;&#36981;&#23432;&#27880;&#37322;&#25351;&#21335;&#65292;&#20174;&#32780;&#33021;&#22815;&#25913;&#36827;&#26410;&#35265; IE &#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#12290;&#20840;&#38754;&#30340;&#35780;&#20272;&#23454;&#35777;&#34920;&#26126;&#65292;GoLLIE &#33021;&#22815;&#27867;&#21270;&#24182;&#36981;&#24490;&#26410;&#35265;&#25351;&#21335;&#65292;&#22312;&#38646;&#26679;&#26412;&#20449;&#24687;&#25277;&#21462;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#23581;&#35797;&#12290;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#35814;&#32454;&#30340;&#25351;&#21335;&#26159;&#21462;&#24471;&#33391;&#22909;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) combined with instruction tuning have made significant progress when generalizing to unseen tasks. However, they have been less successful in Information Extraction (IE), lagging behind task-specific models. Typically, IE tasks are characterized by complex annotation guidelines which describe the task and give examples to humans. Previous attempts to leverage such information have failed, even with the largest models, as they are not able to follow the guidelines out-of-the-box. In this paper we propose GoLLIE (Guideline-following Large Language Model for IE), a model able to improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to comply with annotation guidelines. Comprehensive evaluation empirically demonstrates that GoLLIE is able to generalize to and follow unseen guidelines, outperforming previous attempts at zero-shot information extraction. The ablation study shows that detailed guidelines is key for good results.
&lt;/p&gt;</description></item><item><title>NJUNLP&#22242;&#38431;&#23545;WMT2023&#36136;&#37327;&#35780;&#20272;&#20849;&#20139;&#20219;&#21153;&#36827;&#34892;&#20102;&#25237;&#31295;&#65292;&#36890;&#36807;&#20351;&#29992;&#20266;&#25968;&#25454;&#26041;&#27861;&#21644;&#26680;&#24515;&#36229;&#21442;&#25968;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#20182;&#20204;&#30340;&#27169;&#22411;&#22312;&#33521;&#24503;&#35821;&#35328;&#23545;&#30340;&#36136;&#37327;&#39044;&#27979;&#21644;&#38169;&#35823;&#36328;&#24230;&#26816;&#27979;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13230</link><description>&lt;p&gt;
NJUNLP&#23545;WMT2023&#36136;&#37327;&#35780;&#20272;&#20849;&#20139;&#20219;&#21153;&#30340;&#21442;&#19982;
&lt;/p&gt;
&lt;p&gt;
NJUNLP's Participation for the WMT2023 Quality Estimation Shared Task. (arXiv:2309.13230v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13230
&lt;/p&gt;
&lt;p&gt;
NJUNLP&#22242;&#38431;&#23545;WMT2023&#36136;&#37327;&#35780;&#20272;&#20849;&#20139;&#20219;&#21153;&#36827;&#34892;&#20102;&#25237;&#31295;&#65292;&#36890;&#36807;&#20351;&#29992;&#20266;&#25968;&#25454;&#26041;&#27861;&#21644;&#26680;&#24515;&#36229;&#21442;&#25968;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#20182;&#20204;&#30340;&#27169;&#22411;&#22312;&#33521;&#24503;&#35821;&#35328;&#23545;&#30340;&#36136;&#37327;&#39044;&#27979;&#21644;&#38169;&#35823;&#36328;&#24230;&#26816;&#27979;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;NJUNLP&#22242;&#38431;&#22312;WMT 2023&#36136;&#37327;&#20272;&#35745;&#65288;QE&#65289;&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#25237;&#31295;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#25552;&#20132;&#20102;&#23545;&#33521;&#24503;&#35821;&#35328;&#23545;&#30340;&#25152;&#26377;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#39044;&#27979;&#65306;&#65288;i&#65289;&#21477;&#23376;&#21644;&#21333;&#35789;&#32423;&#21035;&#30340;&#36136;&#37327;&#39044;&#27979;&#65307;&#65288;ii&#65289;&#32454;&#31890;&#24230;&#38169;&#35823;&#36328;&#24230;&#26816;&#27979;&#12290;&#20170;&#24180;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#22522;&#20110;NJUQE&#26694;&#26550;&#65288;https://github.com/NJUNLP/njuqe&#65289;&#30340;&#20266;&#25968;&#25454;&#26041;&#27861;&#36827;&#34892;QE&#12290;&#25105;&#20204;&#20351;&#29992;WMT&#32763;&#35793;&#20219;&#21153;&#30340;&#24182;&#34892;&#25968;&#25454;&#29983;&#25104;&#20266;MQM&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20266;QE&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;XLMR&#22823;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;QE&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#20004;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#20849;&#21516;&#23398;&#20064;&#21477;&#23376;&#32423;&#20998;&#25968;&#21644;&#21333;&#35789;&#32423;&#26631;&#31614;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#26469;&#23547;&#25214;&#25913;&#21892;&#24615;&#33021;&#30340;&#20851;&#38190;&#36229;&#21442;&#25968;&#12290;&#22312;&#25216;&#26415;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23558;&#21333;&#35789;&#32423;&#36755;&#20986;&#36716;&#25442;&#20026;&#32454;&#31890;&#24230;&#38169;&#35823;&#36328;&#24230;&#32467;&#26524;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#33521;&#24503;&#35821;&#35328;&#23545;&#30340;&#21333;&#35789;&#32423;&#21035;&#21644;&#32454;&#31890;&#24230;&#38169;&#35823;&#36328;&#24230;&#26816;&#27979;&#23376;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the submissions of the NJUNLP team to the WMT 2023 Quality Estimation (QE) shared task. Our team submitted predictions for the English-German language pair on all two sub-tasks: (i) sentence- and word-level quality prediction; and (ii) fine-grained error span detection. This year, we further explore pseudo data methods for QE based on NJUQE framework (https://github.com/NJUNLP/njuqe). We generate pseudo MQM data using parallel data from the WMT translation task. We pre-train the XLMR large model on pseudo QE data, then fine-tune it on real QE data. At both stages, we jointly learn sentence-level scores and word-level tags. Empirically, we conduct experiments to find the key hyper-parameters that improve the performance. Technically, we propose a simple method that covert the word-level outputs to fine-grained error span results. Overall, our models achieved the best results in English-German for both word-level and fine-grained error span detection sub-tasks by a considera
&lt;/p&gt;</description></item><item><title>BELT&#26159;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#36827;&#34892;&#33041;&#30005;&#22270;&#21040;&#35821;&#35328;&#35299;&#30721;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#30340;&#24341;&#23548;&#24335;&#27169;&#22411;&#21644;&#23398;&#20064;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#29616;&#25104;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#36827;&#33041;&#30005;&#20449;&#21495;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#22823;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#24212;&#29992;&#21644;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.12056</link><description>&lt;p&gt;
BELT: &#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#36827;&#34892;&#33041;&#30005;&#22270;&#21040;&#35821;&#35328;&#35299;&#30721;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#30340;&#24341;&#23548;&#24335;&#27169;&#22411;&#21644;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BELT:Bootstrapping Electroencephalography-to-Language Decoding and Zero-Shot Sentiment Classification by Natural Language Supervision. (arXiv:2309.12056v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12056
&lt;/p&gt;
&lt;p&gt;
BELT&#26159;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#36827;&#34892;&#33041;&#30005;&#22270;&#21040;&#35821;&#35328;&#35299;&#30721;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#30340;&#24341;&#23548;&#24335;&#27169;&#22411;&#21644;&#23398;&#20064;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#29616;&#25104;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#36827;&#33041;&#30005;&#20449;&#21495;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#22823;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#24212;&#29992;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BELT&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#33041;&#21040;&#35821;&#35328;&#32763;&#35793;&#30740;&#31350;&#30340;&#37325;&#35201;&#20027;&#39064;&#30340;&#26032;&#27169;&#22411;&#21644;&#23398;&#20064;&#26694;&#26550;&#12290;&#23558;&#38750;&#20405;&#20837;&#24615;&#33041;&#20449;&#21495;&#32763;&#35793;&#25104;&#21487;&#35835;&#30340;&#33258;&#28982;&#35821;&#35328;&#26377;&#28508;&#21147;&#25512;&#21160;&#22823;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#65288;BCI&#65289;&#30340;&#24212;&#29992;&#22330;&#26223;&#21644;&#21457;&#23637;&#12290;&#33041;&#20449;&#21495;&#35299;&#30721;&#25110;&#33041;-&#35821;&#35328;&#32763;&#35793;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#26159;&#20174;&#26377;&#38480;&#35268;&#27169;&#21644;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#35821;&#20041;&#36866;&#24403;&#19988;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#33041;&#30005;&#22270;&#34920;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;BELT&#26041;&#27861;&#26159;&#19968;&#20010;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29616;&#25104;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24341;&#23548;&#33041;&#30005;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#35268;&#27169;LM&#29702;&#35299;&#35821;&#20041;&#20449;&#24687;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#65292;BELT&#26497;&#22823;&#25913;&#36827;&#20102;&#23545;&#33041;&#30005;&#20449;&#21495;&#30340;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;BELT&#27169;&#22411;&#30001;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#35299;&#30721;&#31070;&#32463;&#32593;&#32476;&#21644;&#19968;&#20010;&#39044;&#35757;&#32451;LM&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents BELT, a novel model and learning framework for the pivotal topic of brain-to-language translation research. The translation from noninvasive brain signals into readable natural language has the potential to promote the application scenario as well as the development of brain-computer interfaces (BCI) as a whole. The critical problem in brain signal decoding or brain-to-language translation is the acquisition of semantically appropriate and discriminative EEG representation from a dataset of limited scale and quality. The proposed BELT method is a generic and efficient framework that bootstraps EEG representation learning using off-the-shelf large-scale pretrained language models (LMs). With a large LM's capacity for understanding semantic information and zero-shot generalization, BELT utilizes large LMs trained on Internet-scale datasets to bring significant improvements to the understanding of EEG signals.  In particular, the BELT model is composed of a deep confor
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#12289;&#20559;&#35265;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#20445;&#25252;&#26041;&#38754;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.08836</link><description>&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;: &#19968;&#31181;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Bias and Fairness in Chatbots: An Overview. (arXiv:2309.08836v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08836
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#12289;&#20559;&#35265;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#20445;&#25252;&#26041;&#38754;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#24050;&#32463;&#30740;&#31350;&#20102;&#21322;&#20010;&#22810;&#19990;&#32426;&#12290;&#38543;&#30528;&#36817;&#24180;&#26469;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#29616;&#22312;&#22791;&#21463;&#20851;&#27880;&#12290;&#19982;&#20256;&#32479;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30456;&#27604;&#65292;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#26356;&#24378;&#22823;&#65292;&#24182;&#24050;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#23384;&#22312;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#37327;&#24040;&#22823;&#12289;&#27169;&#22411;&#35268;&#27169;&#24222;&#22823;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20559;&#35265;&#32531;&#35299;&#21644;&#20844;&#24179;&#20445;&#25252;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#12290;&#39318;&#20808;&#22238;&#39038;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#21644;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#20998;&#26512;&#20102;&#24212;&#29992;&#20013;&#30340;&#20559;&#35265;&#26469;&#28304;&#21644;&#28508;&#22312;&#21361;&#23475;&#12290;&#30740;&#31350;&#20102;&#35774;&#35745;&#20844;&#24179;&#21644;&#26080;&#20559;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#26368;&#21518;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots have been studied for more than half a century. With the rapid development of natural language processing (NLP) technologies in recent years, chatbots using large language models (LLMs) have received much attention nowadays. Compared with traditional ones, modern chatbots are more powerful and have been used in real-world applications. There are however, bias and fairness concerns in modern chatbot design. Due to the huge amounts of training data, extremely large model sizes, and lack of interpretability, bias mitigation and fairness preservation of modern chatbots are challenging. Thus, a comprehensive overview on bias and fairness in chatbot systems is given in this paper. The history of chatbots and their categories are first reviewed. Then, bias sources and potential harms in applications are analyzed. Considerations in designing fair and unbiased chatbot systems are examined. Finally, future research directions are discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2309.05961</link><description>&lt;p&gt;
&#35780;&#20272;&#28526;&#36215;&#28526;&#33853;&#65306;&#23545;&#19981;&#21516;&#24179;&#21488;&#38388;&#38382;&#31572;&#36235;&#21183;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#22240;&#20854;&#24555;&#36895;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#30340;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#22238;&#31572;&#36895;&#24230;&#30340;&#24555;&#24930;&#21462;&#20915;&#20110;&#26597;&#35810;&#29305;&#23450;&#21644;&#29992;&#25143;&#30456;&#20851;&#30340;&#22240;&#32032;&#30340;&#32508;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20845;&#20010;&#39640;&#24230;&#27969;&#34892;&#30340;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#22240;&#32032;&#22312;&#20854;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22238;&#31572;&#25152;&#33457;&#36153;&#30340;&#26102;&#38388;&#19982;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#30340;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#20114;&#21160;&#27700;&#24179;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#36825;&#20123;&#20803;&#25968;&#25454;&#21644;&#29992;&#25143;&#20114;&#21160;&#27169;&#24335;&#65292;&#25105;&#20204;&#35797;&#22270;&#39044;&#27979;&#21738;&#20123;&#26597;&#35810;&#23558;&#36805;&#36895;&#33719;&#24471;&#21021;&#22987;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
&lt;/p&gt;</description></item><item><title>FlexKBQA&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#31243;&#24207;&#32763;&#35793;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#31639;&#27861;&#20174;&#30693;&#35782;&#24211;&#20013;&#25277;&#26679;&#22810;&#26679;&#30340;&#31243;&#24207;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#20174;&#32780;&#35299;&#20915;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#31572;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12060</link><description>&lt;p&gt;
FlexKBQA&#65306;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#28789;&#27963;LLM&#39537;&#21160;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering. (arXiv:2308.12060v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12060
&lt;/p&gt;
&lt;p&gt;
FlexKBQA&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#31243;&#24207;&#32763;&#35793;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#31639;&#27861;&#20174;&#30693;&#35782;&#24211;&#20013;&#25277;&#26679;&#22810;&#26679;&#30340;&#31243;&#24207;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#20174;&#32780;&#35299;&#20915;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#31572;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#26159;&#19968;&#20010;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#30693;&#35782;&#24211;&#20013;&#30340;&#23454;&#20307;&#25968;&#37327;&#24222;&#22823;&#65292;&#24182;&#19988;&#29992;&#25143;&#25552;&#20986;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#22810;&#26679;&#21270;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;KBQA&#27169;&#22411;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#22240;&#20026;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#19981;&#36275;&#12290;&#20026;&#20102;&#20943;&#36731;&#25163;&#21160;&#27880;&#37322;&#30340;&#36127;&#25285;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#31243;&#24207;&#32763;&#35793;&#22120;&#65292;&#20171;&#32461;&#20102;FlexKBQA&#26469;&#35299;&#20915;&#23569;&#26679;&#26412;KBQA&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FlexKBQA&#21033;&#29992;&#33258;&#21160;&#21270;&#31639;&#27861;&#20174;&#30693;&#35782;&#24211;&#20013;&#25277;&#26679;&#22810;&#26679;&#30340;&#31243;&#24207;&#65288;&#22914;SPARQL&#26597;&#35810;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;LLMs&#23558;&#20854;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#36825;&#20010;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#26377;&#21161;&#20110;&#35757;&#32451;&#19968;&#20010;&#19987;&#38376;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#26469;&#22788;&#29702;&#30693;&#35782;&#24211;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#29992;&#25143;&#38382;&#39064;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#30340;&#38556;&#30861;&#65292;FlexKBQA&#24341;&#20837;&#20102;&#19968;&#20010;&#25191;&#34892;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge base question answering (KBQA) is a critical yet challenging task due to the vast number of entities within knowledge bases and the diversity of natural language questions posed by users. Unfortunately, the performance of most KBQA models tends to decline significantly in real-world scenarios where high-quality annotated data is insufficient. To mitigate the burden associated with manual annotation, we introduce FlexKBQA by utilizing Large Language Models (LLMs) as program translators for addressing the challenges inherent in the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms to sample diverse programs, such as SPARQL queries, from the knowledge base, which are subsequently converted into natural language questions via LLMs. This synthetic dataset facilitates training a specialized lightweight model for the KB. Additionally, to reduce the barriers of distribution shift between synthetic data and real user questions, FlexKBQA introduces an executiong
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#29983;&#25104;&#30340;&#26679;&#26412;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12289;&#22870;&#21169;&#21152;&#26435;&#22238;&#24402;&#21644;&#20915;&#31574;Transformer&#31561;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31867;&#20284;&#20110;&#30417;&#30563;&#24494;&#35843;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2308.12050</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aligning Language Models with Offline Reinforcement Learning from Human Feedback. (arXiv:2308.12050v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#29983;&#25104;&#30340;&#26679;&#26412;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12289;&#22870;&#21169;&#21152;&#26435;&#22238;&#24402;&#21644;&#20915;&#31574;Transformer&#31561;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31867;&#20284;&#20110;&#30417;&#30563;&#24494;&#35843;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#26377;&#25928;&#22320;&#28385;&#36275;&#20154;&#31867;&#38656;&#27714;&#21644;&#31038;&#20250;&#20215;&#20540;&#35266;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#26469;&#36981;&#24490;&#25351;&#20196;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25216;&#26415;&#65292;&#22914;Proximal Policy Optimization&#65288;PPO&#65289;&#65292;&#36825;&#20123;&#25216;&#26415;&#34987;&#35777;&#26126;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#19981;&#31283;&#23450;&#19988;&#38590;&#20197;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;PPO&#38656;&#35201;&#22797;&#26434;&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#23454;&#29616;&#65292;&#24433;&#21709;&#20102;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#25928;&#29575;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#29983;&#25104;&#30340;&#26679;&#26412;&#32780;&#19981;&#26159;&#19982;RL&#29615;&#22659;&#20132;&#20114;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#19982;&#36807;&#28388;&#12289;&#22870;&#21169;&#21152;&#26435;&#22238;&#24402;&#65288;RWR&#65289;&#20197;&#21450;&#20915;&#31574;Transformer&#65288;DT&#65289;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#12290;&#36890;&#36807;&#37319;&#29992;&#31867;&#20284;&#20110;&#30417;&#30563;&#24494;&#35843;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human preferences is crucial for language models (LMs) to effectively cater to human needs and societal values. Previous research has made notable progress by leveraging human feedback to follow instructions. However, these approaches rely primarily on online reinforcement learning (RL) techniques like Proximal Policy Optimization (PPO), which have been proven unstable and challenging to tune for language models. Moreover, PPO requires complex distributed system implementation, hindering the efficiency of large-scale distributed training. In this study, we propose an offline reinforcement learning from human feedback (RLHF) framework to align LMs using pre-generated samples without interacting with RL environments. Specifically, we explore maximum likelihood estimation (MLE) with filtering, reward-weighted regression (RWR), and Decision Transformer (DT) to align language models to human preferences. By employing a loss function similar to supervised fine-tuning, our metho
&lt;/p&gt;</description></item><item><title>BLIVA&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#23884;&#20837;&#20102;&#25991;&#26412;&#30340;&#22270;&#20687;&#19978;&#19979;&#25991;&#22330;&#26223;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#21644;LLaVA&#30340;&#32534;&#30721;&#34917;&#19969;&#23884;&#20837;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09936</link><description>&lt;p&gt;
BLIVA: &#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09936
&lt;/p&gt;
&lt;p&gt;
BLIVA&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#23884;&#20837;&#20102;&#25991;&#26412;&#30340;&#22270;&#20687;&#19978;&#19979;&#25991;&#22330;&#26223;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#21644;LLaVA&#30340;&#32534;&#30721;&#34917;&#19969;&#23884;&#20837;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#36890;&#36807;&#25972;&#21512;&#35270;&#35273;&#29702;&#35299;&#33021;&#21147;&#25193;&#23637;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#35299;&#20915;&#24320;&#25918;&#24335;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#35299;&#37322;&#23884;&#20837;&#25991;&#26412;&#30340;&#22270;&#20687;&#65292;&#36825;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#26631;&#20934;&#27969;&#31243;&#36890;&#24120;&#28041;&#21450;&#23398;&#20064;&#19968;&#32452;&#22266;&#23450;&#30340;&#26597;&#35810;&#23884;&#20837;&#12290;&#36825;&#20123;&#23884;&#20837;&#34987;&#35774;&#35745;&#20026;&#23553;&#35013;&#22270;&#20687;&#19978;&#19979;&#25991;&#65292;&#24182;&#38543;&#21518;&#29992;&#20316;LLM&#20013;&#30340;&#36719;&#25552;&#31034;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#21463;&#20196;&#29260;&#25968;&#37327;&#30340;&#38480;&#21046;&#65292;&#21487;&#33021;&#38480;&#21046;&#23545;&#25991;&#26412;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#22330;&#26223;&#30340;&#35782;&#21035;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#19968;&#28857;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;BLIVA&#65306;InstructBLIP with Visual Assistant&#30340;&#22686;&#24378;&#29256;&#26412;&#12290;BLIVA&#38598;&#25104;&#20102;&#26469;&#33258;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#65292;&#24182;&#23558;&#32534;&#30721;&#30340;&#34917;&#19969;&#23884;&#20837;&#30452;&#25509;&#25237;&#24433;&#21040;LLM&#20013;&#65292;&#36825;&#26159;&#21463;&#21040;LLaVA&#30340;&#21551;&#21457;&#30340;&#19968;&#31181;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#22788;&#29702;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#32544;&#36755;&#20837;&#34920;&#31034;&#20026;&#19981;&#21464;&#29305;&#24449;&#21644;&#24179;&#21488;&#30456;&#20851;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#26410;&#35265;&#24179;&#21488;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02080</link><description>&lt;p&gt;
&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#22240;&#26524;&#24341;&#23548;&#35299;&#32544;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Causality Guided Disentanglement for Cross-Platform Hate Speech Detection. (arXiv:2308.02080v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#32544;&#36755;&#20837;&#34920;&#31034;&#20026;&#19981;&#21464;&#29305;&#24449;&#21644;&#24179;&#21488;&#30456;&#20851;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#26410;&#35265;&#24179;&#21488;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#20419;&#36827;&#20844;&#24320;&#23545;&#35805;&#26041;&#38754;&#20855;&#26377;&#20215;&#20540;&#65292;&#20294;&#20182;&#20204;&#32463;&#24120;&#34987;&#21033;&#29992;&#26469;&#20256;&#25773;&#26377;&#23475;&#20869;&#23481;&#12290;&#30446;&#21069;&#29992;&#20110;&#26816;&#27979;&#36825;&#31181;&#26377;&#23475;&#20869;&#23481;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#20110;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#65292;&#24433;&#21709;&#21040;&#20102;&#23427;&#20204;&#36866;&#24212;&#27867;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#20542;&#21521;&#20110;&#36807;&#20110;&#29421;&#38552;&#22320;&#20851;&#27880;&#29305;&#23450;&#30340;&#35821;&#35328;&#20449;&#21495;&#25110;&#26576;&#20123;&#35789;&#35821;&#31867;&#21035;&#30340;&#20351;&#29992;&#12290;&#24403;&#24179;&#21488;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#26102;&#65292;&#21478;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#20986;&#29616;&#20102;&#65292;&#38656;&#35201;&#36328;&#24179;&#21488;&#27169;&#22411;&#26469;&#36866;&#24212;&#19981;&#21516;&#30340;&#20998;&#24067;&#36716;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19968;&#20010;&#24179;&#21488;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#24182;&#25512;&#24191;&#21040;&#22810;&#20010;&#26410;&#35265;&#24179;&#21488;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#19981;&#21516;&#24179;&#21488;&#30340;&#33391;&#22909;&#27867;&#21270;&#24615;&#33021;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#36755;&#20837;&#34920;&#31034;&#35299;&#32544;&#20026;&#19981;&#21464;&#29305;&#24449;&#21644;&#24179;&#21488;&#30456;&#20851;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#35748;&#20026;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#26159;&#25552;&#20379;&#26356;&#22909;&#35299;&#32544;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms, despite their value in promoting open discourse, are often exploited to spread harmful content. Current deep learning and natural language processing models used for detecting this harmful content overly rely on domain-specific terms affecting their capabilities to adapt to generalizable hate speech detection. This is because they tend to focus too narrowly on particular linguistic signals or the use of certain categories of words. Another significant challenge arises when platforms lack high-quality annotated data for training, leading to a need for cross-platform models that can adapt to different distribution shifts. Our research introduces a cross-platform hate speech detection model capable of being trained on one platform's data and generalizing to multiple unseen platforms. To achieve good generalizability across platforms, one way is to disentangle the input representations into invariant and platform-dependent features. We also argue that learning causa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31169;&#23494;&#27700;&#21360;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#65292;&#24182;&#20849;&#20139;&#37096;&#20998;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#39640;&#20934;&#30830;&#24615;&#30340;&#26816;&#27979;&#65292;&#21516;&#26102;&#23545;&#29983;&#25104;&#21644;&#26816;&#27979;&#36895;&#24230;&#24433;&#21709;&#26368;&#23567;&#12290;</title><link>http://arxiv.org/abs/2307.16230</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31169;&#23494;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
A Private Watermark for Large Language Models. (arXiv:2307.16230v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16230
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31169;&#23494;&#27700;&#21360;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#65292;&#24182;&#20849;&#20139;&#37096;&#20998;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#39640;&#20934;&#30830;&#24615;&#30340;&#26816;&#27979;&#65292;&#21516;&#26102;&#23545;&#29983;&#25104;&#21644;&#26816;&#27979;&#36895;&#24230;&#24433;&#21709;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#24050;&#32463;&#20943;&#36731;&#20102;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#24102;&#26469;&#30340;&#20266;&#26032;&#38395;&#21644;&#29256;&#26435;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#30340;&#27700;&#21360;&#26816;&#27979;&#38656;&#35201;&#29983;&#25104;&#36807;&#31243;&#30340;&#23494;&#38053;&#65292;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#36829;&#35268;&#21644;&#20266;&#36896;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31169;&#23494;&#27700;&#21360;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#38454;&#27573;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#32780;&#19981;&#26159;&#20351;&#29992;&#30456;&#21516;&#30340;&#23494;&#38053;&#26469;&#25193;&#23637;&#24403;&#21069;&#30340;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#32593;&#32476;&#30340;&#37096;&#20998;&#21442;&#25968;&#26159;&#20849;&#20139;&#30340;&#65292;&#36825;&#20351;&#24471;&#26816;&#27979;&#32593;&#32476;&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30001;&#20110;&#20004;&#20010;&#32593;&#32476;&#30340;&#21442;&#25968;&#35268;&#27169;&#36739;&#23567;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30830;&#20445;&#20102;&#39640;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;&#29983;&#25104;&#21644;&#26816;&#27979;&#36895;&#24230;&#30340;&#24433;&#21709;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, text watermarking algorithms for large language models (LLMs) have been mitigating the potential harms of text generated by the LLMs, including fake news and copyright issues. However, the watermark detection of current text algorithms requires the key from the generation process, making them susceptible to breaches and counterfeiting. In this work, we propose the first private watermarking algorithm, which extends the current text watermarking algorithms by using two different neural networks respectively for watermark generation and detection, rather than using the same key at both stages. Meanwhile, part of the parameters of the watermark generation and detection networks are shared, which makes the detection network achieve a high accuracy very efficiently. Experiments show that our algorithm ensures high detection accuracy with minimal impact on generation and detection speed, due to the small parameter size of both networks. Additionally, our subsequent analysis demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.12267</link><description>&lt;p&gt;
&#38754;&#21521;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#35770;&#25991;&#30340;&#33258;&#21160;&#36793;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#33021;&#22815;&#22312;&#25552;&#20379;&#20855;&#20307;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#27969;&#30021;&#22238;&#31572;&#12290;&#23613;&#31649;&#25215;&#35748;&#25216;&#26415;&#36827;&#27493;&#24102;&#26469;&#30340;&#20415;&#21033;&#65292;&#25945;&#32946;&#32773;&#20063;&#25285;&#24515;&#23398;&#29983;&#21487;&#33021;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;&#20889;&#20316;&#20219;&#21153;&#24182;&#23558;&#20854;&#20551;&#20882;&#20026;&#33258;&#24049;&#30340;&#21407;&#21019;&#20316;&#21697;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;AI&#20869;&#23481;&#26816;&#27979;&#30740;&#31350;&#26159;&#22522;&#20110;&#36825;&#20123;&#25285;&#24551;&#36827;&#34892;&#30340;&#65292;&#20294;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;AI&#20869;&#23481;&#26816;&#27979;&#24314;&#27169;&#20026;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#20551;&#35774;&#19968;&#20010;&#25991;&#26412;&#35201;&#20040;&#23436;&#20840;&#30001;&#20154;&#31867;&#32534;&#20889;&#65292;&#35201;&#20040;&#23436;&#20840;&#30001;AI&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;AI&#20869;&#23481;&#26816;&#27979;&#22312;&#19968;&#20010;&#23569;&#26377;&#25506;&#32034;&#20294;&#21364;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#26816;&#27979;&#30340;&#25991;&#26412;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;&#28151;&#21512;&#25991;&#26412;&#65289;&#21327;&#20316;&#32534;&#20889;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#26816;&#27979;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#20174;&#32473;&#23450;&#30340;&#28151;&#21512;&#25991;&#26412;&#20013;&#35782;&#21035;&#20154;&#31867;&#32534;&#20889;&#20869;&#23481;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#36716;&#25442;&#28857;&#65288;&#36793;&#30028;&#26816;&#27979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent large language models (LLMs), e.g., ChatGPT, have been able to generate human-like and fluent responses when provided with specific instructions. While admitting the convenience brought by technological advancement, educators also have concerns that students might leverage LLMs to complete their writing assignments and pass them off as their original work. Although many AI content detection studies have been conducted as a result of such concerns, most of these prior studies modeled AI content detection as a classification problem, assuming that a text is either entirely human-written or entirely AI-generated. In this study, we investigated AI content detection in a rarely explored yet realistic setting where the text to be detected is collaboratively written by human and generative LLMs (i.e., hybrid text). We first formalized the detection task as identifying the transition points between human-written content and AI-generated content from a given hybrid text (boundary det
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;ChatGPT&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#21644;&#25340;&#20889;&#26816;&#26597;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#24182;&#19981;&#29702;&#24819;&#12290;</title><link>http://arxiv.org/abs/2307.09007</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#20013;&#30340;&#65288;&#26080;&#65289;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the (In)Effectiveness of Large Language Models for Chinese Text Correction. (arXiv:2307.09007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;ChatGPT&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#21644;&#25340;&#20889;&#26816;&#26597;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#24182;&#19981;&#29702;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21644;&#36827;&#27493;&#20196;&#25972;&#20010;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#24778;&#21497;&#19981;&#24050;&#12290;&#20316;&#20026;LLMs&#30340;&#26480;&#20986;&#20195;&#34920;&#21644;&#24341;&#21457;&#20102;&#23545;LLMs&#30740;&#31350;&#28909;&#28526;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;ChatGPT&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#30740;&#31350;&#20854;&#22312;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;&#22312;&#23545;ChatGPT&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24778;&#20154;&#34920;&#29616;&#24863;&#21040;&#24778;&#21497;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;ChatGPT&#36824;&#20855;&#26377;&#20986;&#33394;&#30340;&#22810;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#20363;&#22914;&#20013;&#25991;&#12290;&#20026;&#20102;&#25506;&#32034;ChatGPT&#30340;&#20013;&#25991;&#22788;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#36825;&#20010;&#22522;&#30784;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20013;&#25991;NLP&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#65288;CGEC&#65289;&#21644;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#65288;CSC&#65289;&#36825;&#20004;&#20010;&#20027;&#35201;&#30340;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#22330;&#26223;&#19978;&#35780;&#20272;ChatGPT&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20998;&#26512;&#21644;&#19982;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#24494;&#35843;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#21457;&#29616;ChatGPT&#30340;&#24615;&#33021;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the development and progress of Large Language Models (LLMs) have amazed the entire Artificial Intelligence community. As an outstanding representative of LLMs and the foundation model that set off this wave of research on LLMs, ChatGPT has attracted more and more researchers to study its capabilities and performance on various downstream Natural Language Processing (NLP) tasks. While marveling at ChatGPT's incredible performance on kinds of tasks, we notice that ChatGPT also has excellent multilingual processing capabilities, such as Chinese. To explore the Chinese processing ability of ChatGPT, we focus on Chinese Text Correction, a fundamental and challenging Chinese NLP task. Specifically, we evaluate ChatGPT on the Chinese Grammatical Error Correction (CGEC) and Chinese Spelling Check (CSC) tasks, which are two main Chinese Text Correction scenarios. From extensive analyses and comparisons with previous state-of-the-art fine-tuned models, we empirically find that the Cha
&lt;/p&gt;</description></item><item><title>&#20256;&#25773;&#23398;&#39046;&#22495;&#20013;&#30340;&#33258;&#21160;&#21270;&#20869;&#23481;&#20998;&#26512;&#24120;&#24573;&#35270;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#20171;&#32461;&#24182;&#27979;&#35797;&#20102;&#32479;&#35745;&#26041;&#27861;&#26469;&#32416;&#27491;&#36825;&#31181;&#20559;&#24046;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20462;&#22797;&#20043;&#12290;</title><link>http://arxiv.org/abs/2307.06483</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#20869;&#23481;&#20998;&#26512;&#20013;&#30340;&#38169;&#35823;&#20998;&#31867;&#23548;&#33268;&#22238;&#24402;&#20998;&#26512;&#20013;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#33021;&#20462;&#22797;&#21527;&#65311;&#26159;&#30340;&#65292;&#25105;&#20204;&#33021;&#65281;
&lt;/p&gt;
&lt;p&gt;
Misclassification in Automated Content Analysis Causes Bias in Regression. Can We Fix It? Yes We Can!. (arXiv:2307.06483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06483
&lt;/p&gt;
&lt;p&gt;
&#20256;&#25773;&#23398;&#39046;&#22495;&#20013;&#30340;&#33258;&#21160;&#21270;&#20869;&#23481;&#20998;&#26512;&#24120;&#24573;&#35270;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#20171;&#32461;&#24182;&#27979;&#35797;&#20102;&#32479;&#35745;&#26041;&#27861;&#26469;&#32416;&#27491;&#36825;&#31181;&#20559;&#24046;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20462;&#22797;&#20043;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20998;&#31867;&#22120;&#65288;ACs&#65289;&#36890;&#24120;&#36890;&#36807;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;SML&#65289;&#26500;&#24314;&#65292;&#21487;&#20197;&#23545;&#20174;&#25991;&#26412;&#21040;&#22270;&#29255;&#21644;&#35270;&#39057;&#30340;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24050;&#32463;&#25104;&#20026;&#20256;&#25773;&#31185;&#23398;&#21644;&#30456;&#20851;&#39046;&#22495;&#20013;&#24191;&#27867;&#27969;&#34892;&#30340;&#27979;&#37327;&#35774;&#22791;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#21363;&#20351;&#26159;&#39640;&#24230;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#20063;&#20250;&#20135;&#29983;&#38169;&#35823;&#65292;&#36825;&#23548;&#33268;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#21644;&#19979;&#28216;&#20998;&#26512;&#20013;&#35823;&#23548;&#24615;&#30340;&#32467;&#26524;&#65292;&#38500;&#38750;&#36825;&#20123;&#20998;&#26512;&#32771;&#34385;&#21040;&#36825;&#20123;&#38169;&#35823;&#12290;&#36890;&#36807;&#23545;SML&#24212;&#29992;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#25773;&#23398;&#32773;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#12290;&#21407;&#21017;&#19978;&#65292;&#29616;&#26377;&#30340;&#32479;&#35745;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#39564;&#35777;&#25968;&#25454;&#65288;&#22914;&#30001;&#20154;&#31867;&#27880;&#37322;&#32773;&#21019;&#24314;&#30340;&#25968;&#25454;&#65289;&#26469;&#32416;&#27491;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#65292;&#24182;&#20135;&#29983;&#19968;&#33268;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#20171;&#32461;&#24182;&#27979;&#35797;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#21253;&#25324;&#25105;&#20204;&#22312;R&#21253;misclassificationmodels&#20013;&#35774;&#35745;&#21644;&#23454;&#29616;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#26469;&#25581;&#31034;&#27599;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated classifiers (ACs), often built via supervised machine learning (SML), can categorize large, statistically powerful samples of data ranging from text to images and video, and have become widely popular measurement devices in communication science and related fields. Despite this popularity, even highly accurate classifiers make errors that cause misclassification bias and misleading results in downstream analyses-unless such analyses account for these errors. As we show in a systematic literature review of SML applications, communication scholars largely ignore misclassification bias. In principle, existing statistical methods can use "gold standard" validation data, such as that created by human annotators, to correct misclassification bias and produce consistent estimates. We introduce and test such methods, including a new method we design and implement in the R package misclassificationmodels, via Monte Carlo simulations designed to reveal each method's limitations, which 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#22312;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#19978;&#30456;&#24403;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27169;&#22411;&#36234;&#22823;&#30340;&#24773;&#20917;&#19979;&#25928;&#29575;&#36234;&#39640;&#65292;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05695</link><description>&lt;p&gt;
&#20197;&#19981;&#21516;&#26041;&#24335;&#22534;&#21472;&#26356;&#22810;&#23618;&#65306;&#36890;&#36807;&#20302;&#31209;&#26356;&#26032;&#36827;&#34892;&#39640;&#31209;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#22312;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#19978;&#30456;&#24403;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27169;&#22411;&#36234;&#22823;&#30340;&#24773;&#20917;&#19979;&#25928;&#29575;&#36234;&#39640;&#65292;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#32593;&#32476;&#25317;&#26377;&#25968;&#30334;&#20159;&#20010;&#21442;&#25968;&#30340;&#35268;&#27169;&#24050;&#32463;&#21344;&#20027;&#23548;&#22320;&#20301;&#24182;&#19988;&#25928;&#26524;&#26174;&#33879;&#65292;&#20294;&#23545;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#35757;&#32451;&#24517;&#35201;&#24615;&#20173;&#28982;&#32570;&#20047;&#28165;&#26224;&#30340;&#29702;&#35299;&#65292;&#32780;&#26367;&#20195;&#26041;&#27861;&#19981;&#19968;&#23450;&#33021;&#22815;&#38477;&#20302;&#35757;&#32451;&#39640;&#24615;&#33021;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#39640;&#31209;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;ReLoRA&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#21442;&#25968;&#37327;&#39640;&#36798;350M&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#30340;&#25928;&#29575;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#30340;&#28508;&#21147;&#21450;&#20854;&#23545;&#20110;&#32553;&#25918;&#23450;&#24459;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models. In this paper, we explore low-rank training techniques as an alternative approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training. Furthermore, we observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Our findings shed light on the potential of low-rank training techniques and their implications for scaling laws.
&lt;/p&gt;</description></item><item><title>&#21452;&#21521;&#27880;&#24847;&#21147;&#27169;&#22411;&#20855;&#26377;&#28151;&#21512;&#19987;&#23478;&#26435;&#37325;&#65292;&#31867;&#20284;&#20110;&#36830;&#32493;&#35789;&#34955;&#27169;&#22411;&#65288;CBOW&#65289;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#23427;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.04057</link><description>&lt;p&gt;
&#21452;&#21521;&#27880;&#24847;&#21147;&#20316;&#20026;&#36830;&#32493;&#35789;&#19987;&#23478;&#30340;&#28151;&#21512;&#29289;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Attention as a Mixture of Continuous Word Experts. (arXiv:2307.04057v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04057
&lt;/p&gt;
&lt;p&gt;
&#21452;&#21521;&#27880;&#24847;&#21147;&#27169;&#22411;&#20855;&#26377;&#28151;&#21512;&#19987;&#23478;&#26435;&#37325;&#65292;&#31867;&#20284;&#20110;&#36830;&#32493;&#35789;&#34955;&#27169;&#22411;&#65288;CBOW&#65289;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#23427;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#21521;&#27880;&#24847;&#21147;&#30001;&#20301;&#32622;&#32534;&#30721;&#21644;&#23631;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#30446;&#26631;&#32452;&#25104;&#30340;&#33258;&#27880;&#24847;&#21147;&#26500;&#25104;&#65292;&#24050;&#25104;&#20026;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#23613;&#31649;&#23427;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#23427;&#30340;&#32479;&#35745;&#22522;&#30784;&#65306;&#21452;&#21521;&#27880;&#24847;&#21147;&#38544;&#21547;&#22320;&#25311;&#21512;&#20102;&#20160;&#20040;&#32479;&#35745;&#27169;&#22411;&#65311;&#23427;&#19982;&#38750;&#27880;&#24847;&#26426;&#21046;&#30340;&#20808;&#39537;&#26377;&#20309;&#19981;&#21516;&#65311;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#21518;&#65292;&#25311;&#21512;&#21333;&#23618;&#21333;&#22836;&#21452;&#21521;&#27880;&#24847;&#21147;&#31561;&#20110;&#25311;&#21512;&#20855;&#26377;&#28151;&#21512;&#19987;&#23478;&#26435;&#37325;&#30340;&#36830;&#32493;&#35789;&#34955;&#65288;CBOW&#65289;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;&#22810;&#20010;&#22836;&#21644;&#22810;&#20010;&#23618;&#30340;&#21452;&#21521;&#27880;&#24847;&#21147;&#31561;&#20215;&#20110;&#22534;&#21472;&#30340;MoEs&#21644;MoEs&#30340;&#28151;&#21512;&#12290;&#36825;&#20010;&#32479;&#35745;&#35266;&#28857;&#25581;&#31034;&#20102;MoE&#22312;&#21452;&#21521;&#27880;&#24847;&#21147;&#20013;&#30340;&#29420;&#29305;&#29992;&#36884;&#65292;&#36825;&#19982;&#20854;&#22312;&#22788;&#29702;&#24322;&#26500;&#24615;&#26041;&#38754;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bidirectional attention $\unicode{x2013}$ composed of self-attention with positional encodings and the masked language model (MLM) objective $\unicode{x2013}$ has emerged as a key component of modern large language models (LLMs). Despite its empirical success, few studies have examined its statistical underpinnings: What statistical model is bidirectional attention implicitly fitting? What sets it apart from its non-attention predecessors? We explore these questions in this paper. The key observation is that fitting a single-layer single-head bidirectional attention, upon reparameterization, is equivalent to fitting a continuous bag of words (CBOW) model with mixture-of-experts (MoE) weights. Further, bidirectional attention with multiple heads and multiple layers is equivalent to stacked MoEs and a mixture of MoEs, respectively. This statistical viewpoint reveals the distinct use of MoE in bidirectional attention, which aligns with its practical effectiveness in handling heterogeneous
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;EHRSHOT&#65292;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22522;&#20934;&#12290;&#35813;&#35770;&#25991;&#21033;&#29992;EHRSHOT&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;CLMBR-T-base&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;ML&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.02028</link><description>&lt;p&gt;
EHRSHOT:&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models. (arXiv:2307.02028v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;EHRSHOT&#65292;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22522;&#20934;&#12290;&#35813;&#35770;&#25991;&#21033;&#29992;EHRSHOT&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;CLMBR-T-base&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;ML&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#19968;&#33324;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#31038;&#21306;&#24050;&#32463;&#21463;&#30410;&#20110;&#20844;&#24320;&#30340;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#27169;&#22411;&#65292;&#20294;&#26159;ML&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#20849;&#20139;&#36164;&#20135;&#30340;&#32570;&#20047;&#30340;&#38459;&#30861;&#12290;&#22522;&#30784;&#27169;&#22411;&#30340;&#25104;&#21151;&#20026;&#21307;&#30103;&#20445;&#20581;ML&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#35775;&#38382;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#39564;&#35777;&#24615;&#33021;&#20248;&#21183;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#36129;&#29486;&#26469;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;EHRSHOT&#65292;&#20854;&#20013;&#21253;&#21547;6,739&#21517;&#26469;&#33258;&#26031;&#22374;&#31119;&#21307;&#23398;&#30340;&#24739;&#32773;&#30340;&#21435;&#35782;&#21035;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#25968;&#25454;&#12290;&#19982;MIMIC-III/IV&#21644;&#20854;&#20182;&#27969;&#34892;&#30340;EHR&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;EHRSHOT&#26159;&#32437;&#21521;&#30340;&#65292;&#19981;&#20165;&#23616;&#38480;&#20110;ICU/ED&#24739;&#32773;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;CLMBR-T-base&#30340;&#26435;&#37325;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32467;&#26500;&#21270;EHR&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#30340;141M&#21442;&#25968;&#20020;&#24202;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#25968;&#25454;&#21253;&#25324;2.57M&#21517;&#24739;&#32773;&#12290;&#25105;&#20204;&#26159;&#26368;&#26089;&#23436;&#20840;&#21457;&#24067;&#36825;&#26679;&#19968;&#20010;&#29992;&#20110;&#32534;&#30721;EHR&#25968;&#25454;&#30340;&#27169;&#22411;&#20043;&#19968;&#65307;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#21457;&#24067;&#30340;&#20020;&#24202;&#25968;&#25454;&#27169;&#22411;&#65288;&#22914;GatorTron&#12289;ClinicalBER&#65289;&#24182;&#27809;&#26377;&#23436;&#20840;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits. We help address these challenges through three contributions. First, we publish a new dataset, EHRSHOT, which contains deidentified structured data from the electronic health records (EHRs) of 6,739 patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients. Second, we publish the weights of CLMBR-T-base, a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBER
&lt;/p&gt;</description></item><item><title>Mind2Web&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#36890;&#29992;Web&#20195;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#25353;&#29031;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20219;&#24847;&#32593;&#31449;&#19978;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#27492;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19977;&#20010;&#24517;&#35201;&#20803;&#32032;&#65306;1&#65289;&#22810;&#26679;&#30340;&#39046;&#22495;&#12289;&#32593;&#31449;&#21644;&#20219;&#21153;&#65292;2&#65289;&#20351;&#29992;&#30495;&#23454;&#32593;&#31449;&#32780;&#38750;&#27169;&#25311;&#21644;&#31616;&#21270;&#32593;&#31449;&#65292;3&#65289;&#24191;&#27867;&#30340;&#29992;&#25143;&#20132;&#20114;&#27169;&#24335;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;Mind2Web&#36827;&#34892;&#20102;&#21021;&#27493;&#25506;&#32034;&#65292;&#20351;&#29992;&#23567;&#22411;LM&#36807;&#28388;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20219;&#21153;&#23436;&#25104;&#29575;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.06070</link><description>&lt;p&gt;
Mind2Web&#65306;&#38754;&#21521;Web&#30340;&#36890;&#29992;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Mind2Web: Towards a Generalist Agent for the Web. (arXiv:2306.06070v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06070
&lt;/p&gt;
&lt;p&gt;
Mind2Web&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#36890;&#29992;Web&#20195;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#25353;&#29031;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20219;&#24847;&#32593;&#31449;&#19978;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#27492;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19977;&#20010;&#24517;&#35201;&#20803;&#32032;&#65306;1&#65289;&#22810;&#26679;&#30340;&#39046;&#22495;&#12289;&#32593;&#31449;&#21644;&#20219;&#21153;&#65292;2&#65289;&#20351;&#29992;&#30495;&#23454;&#32593;&#31449;&#32780;&#38750;&#27169;&#25311;&#21644;&#31616;&#21270;&#32593;&#31449;&#65292;3&#65289;&#24191;&#27867;&#30340;&#29992;&#25143;&#20132;&#20114;&#27169;&#24335;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;Mind2Web&#36827;&#34892;&#20102;&#21021;&#27493;&#25506;&#32034;&#65292;&#20351;&#29992;&#23567;&#22411;LM&#36807;&#28388;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20219;&#21153;&#23436;&#25104;&#29575;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Mind2Web&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#36890;&#29992;Web&#20195;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#25353;&#29031;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20219;&#24847;&#32593;&#31449;&#19978;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;Web&#20195;&#29702;&#25968;&#25454;&#38598;&#35201;&#20040;&#20351;&#29992;&#27169;&#25311;&#32593;&#31449;&#65292;&#35201;&#20040;&#20165;&#35206;&#30422;&#26377;&#38480;&#30340;&#32593;&#31449;&#21644;&#20219;&#21153;&#65292;&#22240;&#27492;&#19981;&#36866;&#21512;&#36890;&#29992;Web&#20195;&#29702;&#12290;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;31&#20010;&#39046;&#22495;&#12289;137&#20010;&#32593;&#31449;&#30340;&#36229;&#36807;2,000&#20010;&#24320;&#25918;&#24335;&#20219;&#21153;&#21644;&#20219;&#21153;&#30340;&#20247;&#21253;&#25805;&#20316;&#24207;&#21015;&#65292;Mind2Web&#20026;&#26500;&#24314;&#36890;&#29992;Web&#20195;&#29702;&#25552;&#20379;&#20102;&#19977;&#20010;&#24517;&#35201;&#20803;&#32032;&#65306;1&#65289;&#22810;&#26679;&#30340;&#39046;&#22495;&#12289;&#32593;&#31449;&#21644;&#20219;&#21153;&#65292;2&#65289;&#20351;&#29992;&#30495;&#23454;&#32593;&#31449;&#32780;&#38750;&#27169;&#25311;&#21644;&#31616;&#21270;&#32593;&#31449;&#65292;3&#65289;&#24191;&#27867;&#30340;&#29992;&#25143;&#20132;&#20114;&#27169;&#24335;&#12290;&#22522;&#20110;Mind2Web&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#36890;&#29992;Web&#20195;&#29702;&#30340;&#21021;&#27493;&#25506;&#32034;&#12290;&#34429;&#28982;&#29616;&#23454;&#19990;&#30028;&#32593;&#31449;&#30340;&#21407;&#22987;HTML&#24448;&#24448;&#22826;&#22823;&#32780;&#26080;&#27861;&#25552;&#20379;&#32473;LLMs&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#39318;&#20808;&#29992;&#23567;&#22411;LM&#36807;&#28388;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20219;&#21153;&#23436;&#25104;&#29575;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;MERGE&#65292;&#19968;&#20010;&#22522;&#20110;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MERGE&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;26.5&#20493;&#30340;&#21152;&#36895;&#21644;80%&#30340;&#36890;&#20449;&#23383;&#33410;&#25968;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2305.15769</link><description>&lt;p&gt;
MERGE: &#24555;&#36895;&#30340;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MERGE: Fast Private Text Generation. (arXiv:2305.15769v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15769
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;MERGE&#65292;&#19968;&#20010;&#22522;&#20110;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MERGE&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;26.5&#20493;&#30340;&#21152;&#36895;&#21644;80%&#30340;&#36890;&#20449;&#23383;&#33410;&#25968;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;NLP&#26381;&#21153;&#21644;Transformer&#27169;&#22411;&#30340;&#31169;&#26377;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20004;&#26041;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#20165;&#32771;&#34385;NLU&#22330;&#26223;&#65292;&#32780;&#25991;&#26412;&#29983;&#25104;&#30340;&#31169;&#26377;&#25512;&#29702;&#65292;&#22914;&#32763;&#35793;&#12289;&#23545;&#35805;&#21644;&#20195;&#30721;&#34917;&#20840;&#65292;&#20173;&#26410;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#23558;&#29616;&#26377;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#36801;&#31227;&#21040;NLG&#27169;&#22411;&#26102;&#65292;&#24615;&#33021;&#34920;&#29616;&#24046;&#65292;&#32780;&#22312;&#35757;&#32451;&#38454;&#27573;&#21463;&#21040;&#25910;&#25947;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MERGE&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MERGE&#37325;&#29992;&#36755;&#20986;&#38544;&#34255;&#29366;&#24577;&#20316;&#20026;&#21333;&#35789;&#23884;&#20837;&#65292;&#20197;&#36339;&#36807;&#23884;&#20837;&#35745;&#31639;&#65292;&#24182;&#37325;&#26032;&#32452;&#32455;Transformer&#27169;&#22359;&#20013;&#30340;&#32447;&#24615;&#25805;&#20316;&#20197;&#21152;&#36895;&#21521;&#21069;&#36807;&#31243;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#20248;&#21270;&#65292;&#22823;&#37327;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24207;&#21015;&#38271;&#24230;&#20026;512&#26102;&#65292;MERGE&#21487;&#23454;&#29616;26.5&#20493;&#30340;&#21152;&#36895;&#65292;&#24182;&#20943;&#23569;80\%&#30340;&#36890;&#20449;&#23383;&#33410;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen increasing concerns about the private inference of NLP services and Transformer models. However, existing two-party privacy-preserving methods solely consider NLU scenarios, while the private inference of text generation such as translation, dialogue, and code completion remains unsolved. Besides, while migrated to NLG models, existing privacy-preserving methods perform poorly in terms of inference speed, and suffer from the convergence problem during the training stage. To address these issues, we propose MERGE, a fast private text generation framework for Transformer-based language models. Specifically, MERGE reuse the output hidden state as the word embedding to bypass the embedding computation, and reorganize the linear operations in the Transformer module to accelerate the forward procedure. Based on these two optimizations, extensive experiments show that MERGE can achieve a 26.5x speedup under the sequence length 512, and reduce 80\% communication bytes, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#23454;&#39564;&#30340;&#32467;&#26524;&#21644;&#21453;&#24605;&#65292;&#35813;&#23454;&#39564;&#20351;&#29992;&#27169;&#22411;GPT 3.5-Turbo&#26469;&#27169;&#25311;&#24402;&#32435;&#24335;&#20027;&#39064;&#20998;&#26512;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#23581;&#35797;&#20351;&#29992;LLM&#36827;&#34892;&#22522;&#20110;&#20154;&#31867;&#35299;&#37322;&#30340;&#20998;&#26512;&#26174;&#28982;&#26159;&#19968;&#31181;&#25361;&#25112;&#65292;&#20294;&#20063;&#26159;&#20102;&#35299;&#36825;&#20123;&#31995;&#32479;&#22312;&#23450;&#24615;&#30740;&#31350;&#20013;&#33021;&#21542;&#20351;&#29992;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.13014</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#35821;&#22659;&#19981;&#26126;&#32467;&#26500;&#21270;&#35775;&#35848;&#30340;&#24402;&#32435;&#24335;&#20027;&#39064;&#20998;&#26512;&#21527;&#65311;&#23545;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#25506;&#35752;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models emulate an inductive Thematic Analysis of semi-structured interviews? An exploration and provocation on the limits of the approach and the model. (arXiv:2305.13014v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#23454;&#39564;&#30340;&#32467;&#26524;&#21644;&#21453;&#24605;&#65292;&#35813;&#23454;&#39564;&#20351;&#29992;&#27169;&#22411;GPT 3.5-Turbo&#26469;&#27169;&#25311;&#24402;&#32435;&#24335;&#20027;&#39064;&#20998;&#26512;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#23581;&#35797;&#20351;&#29992;LLM&#36827;&#34892;&#22522;&#20110;&#20154;&#31867;&#35299;&#37322;&#30340;&#20998;&#26512;&#26174;&#28982;&#26159;&#19968;&#31181;&#25361;&#25112;&#65292;&#20294;&#20063;&#26159;&#20102;&#35299;&#36825;&#20123;&#31995;&#32479;&#22312;&#23450;&#24615;&#30740;&#31350;&#20013;&#33021;&#21542;&#20351;&#29992;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#24378;&#22823;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#21644;&#24037;&#20316;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#23454;&#39564;&#30340;&#32467;&#26524;&#21644;&#21453;&#24605;&#65292;&#35813;&#23454;&#39564;&#20351;&#29992;&#27169;&#22411;GPT 3.5-Turbo&#26469;&#27169;&#25311;&#24402;&#32435;&#24335;&#20027;&#39064;&#20998;&#26512;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#22312;&#35813;&#20027;&#39064;&#19978;&#20027;&#35201;&#36827;&#34892;&#28436;&#32462;&#20998;&#26512;&#12290;&#20027;&#39064;&#20998;&#26512;&#26159;&#19968;&#31181;&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#24120;&#29992;&#30340;&#23450;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#20154;&#31867;&#20998;&#26512;&#24072;&#30340;&#35299;&#37322;&#20197;&#21450;&#23450;&#24615;&#25968;&#25454;&#20013;&#30340;&#26174;&#24335;&#21644;&#28508;&#22312;&#21547;&#20041;&#30340;&#35782;&#21035;&#12290;&#23581;&#35797;&#20351;&#29992;LLM&#36827;&#34892;&#22522;&#20110;&#20154;&#31867;&#35299;&#37322;&#30340;&#20998;&#26512;&#26174;&#28982;&#26159;&#19968;&#31181;&#25361;&#25112;&#65292;&#20294;&#20063;&#26159;&#20102;&#35299;&#36825;&#20123;&#31995;&#32479;&#22312;&#23450;&#24615;&#30740;&#31350;&#20013;&#33021;&#21542;&#20351;&#29992;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23581;&#35797;&#36827;&#34892;&#27492;&#27169;&#25311;&#30340;&#21160;&#26426;&#65292;&#24182;&#21453;&#24605;&#20102;Braun&#21644;Clarke&#25552;&#20986;&#30340;&#20845;&#20010;&#27493;&#39588;&#33267;&#23569;&#37096;&#20998;&#22320;&#22914;&#20309;&#36827;&#34892;&#20027;&#39064;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful generative Artificial Intelligence solutions which can be applied to several fields and areas of work. This paper presents results and reflection of an experiment done to use the model GPT 3.5-Turbo to emulate some aspects of an inductive Thematic Analysis. Previous research on this subject has largely worked on conducting deductive analysis. Thematic Analysis is a qualitative method for analysis commonly used in social sciences and it is based on interpretations made by the human analyst(s) and the identification of explicit and latent meanings in qualitative data. Attempting an analysis based on human interpretation with an LLM clearly is a provocation but also a way to learn something about how these systems can or cannot be used in qualitative research. The paper presents the motivations for attempting this emulation, it reflects on how the six steps to a Thematic Analysis proposed by Braun and Clarke can at least partially be r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21477;&#23376;&#23545;&#40784;&#35774;&#35745;&#65292;&#24182;&#21457;&#29616;&#24403;&#21069;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#21644;&#23545;&#40784;&#31561;&#38382;&#39064;&#19978;&#36824;&#23384;&#22312;&#22256;&#38590;&#65292;&#24615;&#33021;&#30456;&#23545;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#20173;&#26377;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2305.12878</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#30340;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Non-Autoregressive Document-Level Machine Translation. (arXiv:2305.12878v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21477;&#23376;&#23545;&#40784;&#35774;&#35745;&#65292;&#24182;&#21457;&#29616;&#24403;&#21069;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#21644;&#23545;&#40784;&#31561;&#38382;&#39064;&#19978;&#36824;&#23384;&#22312;&#22256;&#38590;&#65292;&#24615;&#33021;&#30456;&#23545;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#20173;&#26377;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#27169;&#22411;&#22312;&#21477;&#23376;&#32423;&#26426;&#22120;&#32763;&#35793;&#20013;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#36895;&#24230;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#33021;&#21147;&#23578;&#26410;&#34987;&#28145;&#20837;&#30740;&#31350;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#23545;&#20856;&#22411;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20013;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#30340;&#21477;&#23376;&#23545;&#40784;&#35774;&#35745;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#25991;&#26723;&#19978;&#21462;&#24471;&#20102;&#39640;&#24230;&#30340;&#21152;&#36895;&#65292;&#24182;&#19988;&#21477;&#23376;&#23545;&#40784;&#26126;&#26174;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#20173;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#36827;&#19968;&#27493;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#30340;&#32972;&#26223;&#19979;&#65292;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#21644;&#23545;&#40784;&#31561;&#38382;&#39064;&#19978;&#38754;&#20020;&#30528;&#26356;&#22810;&#30340;&#22256;&#38590;&#65292;&#30446;&#21069;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#20063;&#38590;&#20197;&#20805;&#20998;&#21033;&#29992;&#25991;&#26723;&#19978;&#19979;&#25991;&#21644;&#22788;&#29702;&#35805;&#35821;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive translation (NAT) models achieve comparable performance and superior speed compared to auto-regressive translation (AT) models in the context of sentence-level machine translation (MT). However, their abilities are unexplored in document-level MT, hindering their usage in real scenarios. In this paper, we conduct a comprehensive examination of typical NAT models in the context of document-level MT and further propose a simple but effective design of sentence alignment between source and target. Experiments show that NAT models achieve high acceleration on documents, and sentence alignment significantly enhances their performance.  However, current NAT models still have a significant performance gap compared to their AT counterparts. Further investigation reveals that NAT models suffer more from the multi-modality and misalignment issues in the context of document-level MT, and current NAT models struggle with exploiting document context and handling discourse phenome
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#39044;&#27979;&#32534;&#30721;&#27169;&#22411;&#65292;&#21487;&#20197;&#25226;&#35828;&#35805;&#32773;&#21644;&#35821;&#38899;&#20449;&#24687;&#32534;&#30721;&#22312;&#27491;&#20132;&#23376;&#31354;&#38388;&#65292;&#36827;&#32780;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#36716;&#24405;&#30340;&#35828;&#35805;&#32773;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;&#35828;&#35805;&#32773;&#20449;&#24687;&#65292;&#24182;&#22312;&#38899;&#20301;&#36776;&#21035;&#20219;&#21153;&#20013;&#20248;&#20110;&#20043;&#21069;&#30340;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.12464</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#27979;&#32534;&#30721;&#27169;&#22411;&#29992;&#27491;&#20132;&#23376;&#31354;&#38388;&#32534;&#30721;&#35828;&#35805;&#32773;&#21644;&#35821;&#38899;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Predictive Coding Models Encode Speaker and Phonetic Information in Orthogonal Subspaces. (arXiv:2305.12464v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12464
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#39044;&#27979;&#32534;&#30721;&#27169;&#22411;&#65292;&#21487;&#20197;&#25226;&#35828;&#35805;&#32773;&#21644;&#35821;&#38899;&#20449;&#24687;&#32534;&#30721;&#22312;&#27491;&#20132;&#23376;&#31354;&#38388;&#65292;&#36827;&#32780;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#36716;&#24405;&#30340;&#35828;&#35805;&#32773;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;&#35828;&#35805;&#32773;&#20449;&#24687;&#65292;&#24182;&#22312;&#38899;&#20301;&#36776;&#21035;&#20219;&#21153;&#20013;&#20248;&#20110;&#20043;&#21069;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#32534;&#30721;&#20102;&#35828;&#35805;&#32773;&#21644;&#35821;&#38899;&#20449;&#24687;&#65292;&#20294;&#23427;&#20204;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#24773;&#20917;&#20173;&#26410;&#28145;&#20837;&#30740;&#31350;&#12290;&#25105;&#20204;&#20551;&#35774;&#23427;&#20204;&#34987;&#32534;&#30721;&#22312;&#27491;&#20132;&#23376;&#31354;&#38388;&#20013;&#65292;&#36825;&#31181;&#29305;&#24615;&#26377;&#21161;&#20110;&#31616;&#21333;&#30340;&#35299;&#32544;&#12290;&#24212;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#21040;&#20004;&#20010;&#39044;&#27979;&#32534;&#30721;&#27169;&#22411;&#30340;&#34920;&#31034;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#23376;&#31354;&#38388;&#65292;&#25429;&#25417;&#21040;&#30340;&#26159;&#35828;&#35805;&#32773;&#21644;&#35821;&#38899;&#21464;&#21270;&#65292;&#24182;&#30830;&#35748;&#23427;&#20204;&#20960;&#20046;&#27491;&#20132;&#12290;&#22522;&#20110;&#36825;&#20010;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35828;&#35805;&#32773;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#23427;&#25240;&#21472;&#32534;&#30721;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#23376;&#31354;&#38388;&#65292;&#26080;&#38656;&#36716;&#24405;&#12290;&#25506;&#26512;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#28040;&#38500;&#20102;&#35828;&#35805;&#32773;&#20449;&#24687;&#65292;&#24182;&#22312;&#38899;&#20301;&#36776;&#21035;&#20219;&#21153;&#20013;&#20248;&#20110;&#20043;&#21069;&#30340;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#21487;&#29992;&#20110;&#28040;&#38500;&#26410;&#30693;&#35828;&#35805;&#32773;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised speech representations are known to encode both speaker and phonetic information, but how they are distributed in the high-dimensional space remains largely unexplored. We hypothesize that they are encoded in orthogonal subspaces, a property that lends itself to simple disentanglement. Applying principal component analysis to representations of two predictive coding models, we identify two subspaces that capture speaker and phonetic variances, and confirm that they are nearly orthogonal. Based on this property, we propose a new speaker normalization method which collapses the subspace that encodes speaker information, without requiring transcriptions. Probing experiments show that our method effectively eliminates speaker information and outperforms a previous baseline in phone discrimination tasks. Moreover, the approach generalizes and can be used to remove information of unseen speakers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#20132;&#20114;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#30452;&#25509;&#32534;&#36753;&#19968;&#27493;&#27493;&#35299;&#37322;&#38169;&#35823;SQL&#20197;&#20462;&#22797;SQL&#38169;&#35823;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#25552;&#39640;&#20102;31.6&#65285;&#30340;&#25191;&#34892;&#20934;&#30830;&#24615;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#24110;&#21161;&#29992;&#25143;&#20197;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#20449;&#24515;&#35299;&#20915;&#20102;&#26356;&#22810;&#30340;SQL&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.07372</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#32534;&#36753;&#30340;&#36880;&#27493;&#35299;&#37322;&#23454;&#29616;&#20132;&#20114;&#24335;&#25991;&#26412;&#36716;SQL
&lt;/p&gt;
&lt;p&gt;
Interactive Text-to-SQL Generation via Editable Step-by-Step Explanations. (arXiv:2305.07372v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#20132;&#20114;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#30452;&#25509;&#32534;&#36753;&#19968;&#27493;&#27493;&#35299;&#37322;&#38169;&#35823;SQL&#20197;&#20462;&#22797;SQL&#38169;&#35823;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#25552;&#39640;&#20102;31.6&#65285;&#30340;&#25191;&#34892;&#20934;&#30830;&#24615;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#24110;&#21161;&#29992;&#25143;&#20197;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#20449;&#24515;&#35299;&#20915;&#20102;&#26356;&#22810;&#30340;SQL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25968;&#25454;&#24211;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#38750;&#19987;&#23478;&#24456;&#38590;&#23436;&#20840;&#37322;&#25918;&#20851;&#31995;&#25968;&#25454;&#24211;&#30340;&#20998;&#26512;&#33021;&#21147;&#65292;&#22240;&#20026;&#20182;&#20204;&#19981;&#29087;&#24713;SQL&#31561;&#25968;&#25454;&#24211;&#35821;&#35328;&#12290;&#35768;&#22810;&#25216;&#26415;&#24050;&#34987;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#33258;&#21160;&#29983;&#25104;SQL&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#20197;&#19979;&#20004;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#23545;&#20110;&#22797;&#26434;&#26597;&#35810;&#23427;&#20204;&#20173;&#20250;&#29359;&#24456;&#22810;&#38169;&#35823;&#65292;&#65288;2&#65289;&#23427;&#20204;&#19981;&#25552;&#20379;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#24335;&#65292;&#35753;&#38750;&#19987;&#23478;&#29992;&#25143;&#39564;&#35777;&#21644;&#25913;&#36827;&#19981;&#27491;&#30830;&#30340;&#26597;&#35810;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#30452;&#25509;&#32534;&#36753;&#19968;&#27493;&#27493;&#35299;&#37322;&#38169;&#35823;SQL&#20197;&#20462;&#22797;SQL&#38169;&#35823;&#12290;&#22312;Spider&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33267;&#23569;&#27604;&#19977;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#25191;&#34892;&#20934;&#30830;&#24615;&#26041;&#38754;&#25552;&#39640;&#20102;31.6&#65285;&#12290;24&#21517;&#21442;&#19982;&#32773;&#30340;&#29992;&#25143;&#30740;&#31350;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24110;&#21161;&#29992;&#25143;&#20197;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#20449;&#24515;&#35299;&#20915;&#20102;&#26356;&#22810;&#30340;SQL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relational databases play an important role in this Big Data era. However, it is challenging for non-experts to fully unleash the analytical power of relational databases, since they are not familiar with database languages such as SQL. Many techniques have been proposed to automatically generate SQL from natural language, but they suffer from two issues: (1) they still make many mistakes, particularly for complex queries, and (2) they do not provide a flexible way for non-expert users to validate and refine the incorrect queries. To address these issues, we introduce a new interaction mechanism that allows users directly edit a step-by-step explanation of an incorrect SQL to fix SQL errors. Experiments on the Spider benchmark show that our approach outperforms three SOTA approaches by at least 31.6% in terms of execution accuracy. A user study with 24 participants further shows that our approach helped users solve significantly more SQL tasks with less time and higher confidence, demo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GPT-RE&#65292;&#36890;&#36807;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#23454;&#20307;&#34920;&#31034;&#21644;&#20351;&#29992;&#37329;&#26631;&#31614;&#35825;&#23548;&#30340;&#25512;&#29702;&#36923;&#36753;&#20016;&#23500;&#28436;&#31034;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;&#20302;&#30456;&#20851;&#24615;&#21644;&#20542;&#21521;&#20110;&#38169;&#35823;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#25104;&#26524;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#36229;&#36807;&#29616;&#26377;&#22522;&#20934;&#65292;&#20854;&#20013;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#32467;&#26524;&#36798;&#21040;&#20102;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02105</link><description>&lt;p&gt;
GPT-RE: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20851;&#31995;&#25277;&#21462;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPT-RE: In-context Learning for Relation Extraction using Large Language Models. (arXiv:2305.02105v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GPT-RE&#65292;&#36890;&#36807;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#23454;&#20307;&#34920;&#31034;&#21644;&#20351;&#29992;&#37329;&#26631;&#31614;&#35825;&#23548;&#30340;&#25512;&#29702;&#36923;&#36753;&#20016;&#23500;&#28436;&#31034;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;&#20302;&#30456;&#20851;&#24615;&#21644;&#20542;&#21521;&#20110;&#38169;&#35823;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#25104;&#26524;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#36229;&#36807;&#29616;&#26377;&#22522;&#20934;&#65292;&#20854;&#20013;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#32467;&#26524;&#36798;&#21040;&#20102;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-3&#65289;&#26377;&#21487;&#33021;&#21462;&#24471;&#31361;&#30772;&#24615;&#25104;&#23601;&#65292;&#20294;&#23427;&#20204;&#22312;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#20173;&#36828;&#36828;&#33853;&#21518;&#20110;&#23436;&#20840;&#30417;&#30563;&#30340;&#22522;&#20934;&#65288;&#20363;&#22914;fine-tuned BERT&#65289;&#12290;  &#36825;&#26159;&#30001;&#20110;LLMs&#22312;RE&#26041;&#38754;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;:(1)&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#26816;&#32034;&#21040;&#30340;&#28436;&#31034;&#20013;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;; (2)&#24378;&#28872;&#20542;&#21521;&#20110;&#38169;&#35823;&#22320;&#23558;NULL&#31034;&#20363;&#20998;&#31867;&#20026;&#20854;&#20182;&#39044;&#23450;&#20041;&#30340;&#26631;&#31614;&#12290;  &#26412;&#25991;&#25552;&#20986;&#20102;GPT-RE&#26469;&#24357;&#21512;LLMs&#21644;&#23436;&#20840;&#30417;&#30563;&#30340;&#22522;&#20934;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290; GPT-RE&#36890;&#36807;&#65288;1&#65289;&#22312;&#28436;&#31034;&#26816;&#32034;&#20013;&#21152;&#20837;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#23454;&#20307;&#34920;&#31034;; &#65288;2&#65289;&#20351;&#29992;&#37329;&#26631;&#31614;&#35825;&#23548;&#30340;&#25512;&#29702;&#36923;&#36753;&#20016;&#23500;&#28436;&#31034;&#26469;&#25104;&#21151;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290; &#25105;&#20204;&#22312;&#22235;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;RE&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;GPT-RE&#65292;&#24182;&#35266;&#23519;&#21040;GPT-RE&#19981;&#20165;&#25913;&#21892;&#20102;&#29616;&#26377;&#30340;GPT-3&#22522;&#20934;&#65292;&#32780;&#19988;&#25913;&#21892;&#20102;&#23436;&#20840;&#30417;&#30563;&#30340;&#22522;&#20934;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;GPT-RE&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#19977;&#20010;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In spite of the potential for ground-breaking achievements offered by large language models (LLMs) (e.g., GPT-3), they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of LLMs in RE: (1) low relevance regarding entity and relation in retrieved demonstrations for in-context learning; and (2) the strong inclination to wrongly classify NULL examples into other pre-defined labels.  In this paper, we propose GPT-RE to bridge the gap between LLMs and fully-supervised baselines. GPT-RE successfully addresses the aforementioned issues by (1) incorporating task-specific entity representations in demonstration retrieval; and (2) enriching the demonstrations with gold label-induced reasoning logic. We evaluate GPT-RE on four widely-used RE datasets, and observe that GPT-RE achieves improvements over not only existing GPT-3 baselines, but also fully-supervised baselines. Specifically, GPT-RE ach
&lt;/p&gt;</description></item><item><title>GEMINI&#27169;&#22411;&#23558;&#21477;&#23376;&#37325;&#20889;&#21644;&#34701;&#21512;&#25216;&#26415;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#21477;&#23376;&#32423;&#20889;&#20316;&#39118;&#26684;&#25511;&#21046;&#65292;&#35813;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#38598;&#20855;&#26377;&#24179;&#34913;&#30340;&#39118;&#26684;&#20998;&#24067;&#26102;&#12290;</title><link>http://arxiv.org/abs/2304.03548</link><description>&lt;p&gt;
GEMINI&#65306;&#38024;&#23545;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#25511;&#21046;&#21477;&#23376;&#32423;&#20889;&#20316;&#39118;&#26684;
&lt;/p&gt;
&lt;p&gt;
GEMINI: Controlling the Sentence-level Writing Style for Abstractive Text Summarization. (arXiv:2304.03548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03548
&lt;/p&gt;
&lt;p&gt;
GEMINI&#27169;&#22411;&#23558;&#21477;&#23376;&#37325;&#20889;&#21644;&#34701;&#21512;&#25216;&#26415;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#21477;&#23376;&#32423;&#20889;&#20316;&#39118;&#26684;&#25511;&#21046;&#65292;&#35813;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#38598;&#20855;&#26377;&#24179;&#34913;&#30340;&#39118;&#26684;&#20998;&#24067;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#19987;&#23478;&#20351;&#29992;&#19981;&#21516;&#25216;&#26415;&#32534;&#20889;&#25688;&#35201;&#65292;&#21253;&#25324;&#37325;&#20889;&#25991;&#26723;&#20013;&#30340;&#21477;&#23376;&#25110;&#21512;&#24182;&#22810;&#20010;&#21477;&#23376;&#29983;&#25104;&#25688;&#35201;&#21477;&#12290;&#36825;&#20123;&#25216;&#26415;&#26159;&#28789;&#27963;&#30340;&#65292;&#22240;&#27492;&#24456;&#38590;&#36890;&#36807;&#20219;&#20309;&#21333;&#19968;&#26041;&#27861;&#27169;&#25311;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27169;&#22411;GEMINI&#65292;&#23558;&#37325;&#20889;&#22120;&#21644;&#34701;&#21512;&#22120;&#38598;&#25104;&#36215;&#26469;&#65292;&#20197;&#27169;&#25311;&#21477;&#23376;&#37325;&#20889;&#21644;&#34701;&#21512;&#25216;&#26415;&#12290;GEMINI&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#37325;&#20889;&#29305;&#23450;&#30340;&#25991;&#26723;&#21477;&#23376;&#25110;&#20174;&#22836;&#29983;&#25104;&#25688;&#35201;&#21477;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#32431;&#25277;&#35937;&#21644;&#37325;&#20889;&#22522;&#32447;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#38598;&#20855;&#26377;&#24179;&#34913;&#30340;&#39118;&#26684;&#20998;&#24067;&#26102;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27599;&#20010;&#25688;&#35201;&#21477;&#30340;&#20154;&#31867;&#20889;&#20316;&#39118;&#26684;&#22312;&#20854;&#19978;&#19979;&#25991;&#20013;&#26159;&#21487;&#20197;&#39044;&#27979;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human experts write summaries using different techniques, including rewriting a sentence in the document or fusing multiple sentences to generate a summary sentence. These techniques are flexible and thus difficult to be imitated by any single method. To address this issue, we propose an adaptive model, GEMINI, that integrates a rewriter and a fuser to mimic the sentence rewriting and fusion techniques, respectively. GEMINI adaptively chooses to rewrite a specific document sentence or generate a summary sentence from scratch. Experiments demonstrate that our adaptive approach outperforms the pure abstractive and rewriting baselines on various benchmark datasets, especially when the dataset has a balanced distribution of styles. Interestingly, empirical results show that the human writing style of each summary sentence is consistently predictable given its context.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.12247</link><description>&lt;p&gt;
&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#65306;&#19968;&#31181;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12247
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#25152;&#38656;&#30340;&#20132;&#20114;&#22914;&#20309;&#36827;&#34892;&#37327;&#21270;&#65311;&#26368;&#36866;&#21512;&#25429;&#25417;&#36825;&#20123;&#20132;&#20114;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#20160;&#20040;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;&#26469;&#37327;&#21270;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#19977;&#20010;&#34913;&#37327;&#26631;&#20934;&#31216;&#20026;&#22810;&#27169;&#24577;&#20998;&#24067;&#65288;&#25110;&#31616;&#31216;PID&#65289;&#30340;PID&#32479;&#35745;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#20998;&#24067;&#12290;&#20026;&#20102;&#39564;&#35777;PID&#20272;&#35745;&#65292;&#25105;&#20204;&#22312;&#24050;&#30693;PID&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#25110;&#35757;&#32451;&#26102;&#38388;&#36755;&#20837;&#25200;&#21160;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#23884;&#20837;&#31354;&#38388;&#25200;&#21160;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27867;&#21270;&#24615;&#12290;&#31070;&#32463;&#20803;&#30340;&#35821;&#35328;&#30456;&#20851;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#25913;&#36827;&#26159;&#30001;&#20110;&#23384;&#22312;&#8220;&#26356;&#19987;&#19994;&#8221;&#30340;&#31070;&#32463;&#20803;&#12290;&#36825;&#26159;&#39318;&#20010;&#23545;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2211.05523</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Adversarial Training on Robustness and Generalizability of Language Models. (arXiv:2211.05523v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#25110;&#35757;&#32451;&#26102;&#38388;&#36755;&#20837;&#25200;&#21160;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#23884;&#20837;&#31354;&#38388;&#25200;&#21160;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27867;&#21270;&#24615;&#12290;&#31070;&#32463;&#20803;&#30340;&#35821;&#35328;&#30456;&#20851;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#25913;&#36827;&#26159;&#30001;&#20110;&#23384;&#22312;&#8220;&#26356;&#19987;&#19994;&#8221;&#30340;&#31070;&#32463;&#20803;&#12290;&#36825;&#26159;&#39318;&#20010;&#23545;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;&#30340;&#26368;&#26377;&#25928;&#25163;&#27573;&#12290;&#20294;&#26159;&#65292;&#24050;&#32463;&#30830;&#35748;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#21516;&#26102;&#23454;&#29616;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#38656;&#35201;&#36827;&#34892;&#26435;&#34913;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#28145;&#20837;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#12289;&#35757;&#32451;&#26102;&#38388;&#36755;&#20837;&#25200;&#21160;&#21644;&#23884;&#20837;&#31354;&#38388;&#25200;&#21160;&#23545;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#25110;&#35757;&#32451;&#26102;&#38388;&#36755;&#20837;&#25200;&#21160;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#25200;&#21160;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#22320;&#25552;&#39640;&#27867;&#21270;&#24615;&#12290;&#23398;&#20064;&#27169;&#22411;&#31070;&#32463;&#20803;&#30340;&#35821;&#35328;&#30456;&#20851;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#25913;&#21892;&#27867;&#21270;&#24615;&#26159;&#30001;&#20110;&#23384;&#22312;&#8220;&#26356;&#19987;&#19994;&#8221;&#30340;&#31070;&#32463;&#20803;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#23545;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is widely acknowledged as the most effective defense against adversarial attacks. However, it is also well established that achieving both robustness and generalization in adversarially trained models involves a trade-off. The goal of this work is to provide an in depth comparison of different approaches for adversarial training in language models. Specifically, we study the effect of pre-training data augmentation as well as training time input perturbations vs. embedding space perturbations on the robustness and generalization of transformer-based language models. Our findings suggest that better robustness can be achieved by pre-training data augmentation or by training with input space perturbation. However, training with embedding space perturbation significantly improves generalization. A linguistic correlation analysis of neurons of the learned models reveals that the improved generalization is due to 'more specialized' neurons. To the best of our knowledge,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#65292;&#23558;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.08964</link><description>&lt;p&gt;
PromptCast&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#65292;&#23558;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#12290;&#22312;&#36825;&#31181;&#26032;&#30340;&#20219;&#21153;&#20013;&#65292;&#23558;&#21407;&#26469;&#30340;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#39044;&#27979;&#30340;&#30446;&#30340;&#12290;&#20026;&#20102;&#25903;&#25345;&#21644;&#20419;&#36827;&#36825;&#20010;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65288;PISA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes th
&lt;/p&gt;</description></item></channel></rss>