<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#30340;&#30740;&#31350;&#21457;&#29616;&#20013;&#38388;&#26816;&#26597;&#28857;&#27604;&#23436;&#20840;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#30340;&#35760;&#24518;&#21270;&#34892;&#20026;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35760;&#24518;&#21270;&#24471;&#20998;&#30340;&#20998;&#24067;&#35268;&#24459;&#12290;</title><link>http://arxiv.org/abs/2304.11158</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31361;&#29616;&#21644;&#21487;&#39044;&#30693;&#24615;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Emergent and Predictable Memorization in Large Language Models. (arXiv:2304.11158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30340;&#30740;&#31350;&#21457;&#29616;&#20013;&#38388;&#26816;&#26597;&#28857;&#27604;&#23436;&#20840;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#30340;&#35760;&#24518;&#21270;&#34892;&#20026;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35760;&#24518;&#21270;&#24471;&#20998;&#30340;&#20998;&#24067;&#35268;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#21270;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36755;&#20986;&#20854;&#35757;&#32451;&#25968;&#25454;&#23436;&#20840;&#30456;&#21516;&#24207;&#21015;&#30340;&#20542;&#21521;&#65292;&#36825;&#26159;&#23433;&#20840;&#37096;&#32626;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#38382;&#39064;&#20043;&#19968;&#12290;&#29305;&#21035;&#22320;&#65292;&#26368;&#23567;&#21270;&#27169;&#22411;&#23545;&#21253;&#21547;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#65288;PII&#65289;&#31561;&#25935;&#24863;&#25968;&#25454;&#28857;&#30340;&#35760;&#24518;&#21270;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36825;&#31181;&#19981;&#33391;&#30340;&#35760;&#24518;&#21270;&#30340;&#26222;&#21450;&#21487;&#33021;&#20250;&#32473;&#27169;&#22411;&#35757;&#32451;&#32773;&#24102;&#26469;&#38382;&#39064;&#65292;&#29978;&#33267;&#21487;&#33021;&#38656;&#35201;&#20002;&#24323;&#21542;&#21017;&#21151;&#33021;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#25512;&#26029;&#20302;&#35745;&#31639;&#21147;&#35797;&#39564;&#36816;&#34892;&#30340;&#35760;&#24518;&#21270;&#34892;&#20026;&#26469;&#39044;&#27979;&#21738;&#20123;&#24207;&#21015;&#23558;&#22312;&#22823;&#22411;&#27169;&#22411;&#30340;&#20840;&#23616;&#22521;&#35757;&#26399;&#38388;&#36827;&#34892;&#35760;&#24518;&#21270;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;Pythia&#27169;&#22411;&#22871;&#20214;&#30340;&#35760;&#24518;&#21270;&#65292;&#21457;&#29616;&#20013;&#38388;&#26816;&#26597;&#28857;&#27604;&#36739;&#23567;&#30340;&#24050;&#23436;&#20840;&#35757;&#32451;&#27169;&#22411;&#26356;&#22909;&#22320;&#39044;&#27979;&#20102;&#27169;&#22411;&#30340;&#35760;&#24518;&#21270;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26377;&#20851;&#27169;&#22411;&#21644;&#25968;&#25454;&#35760;&#24518;&#21270;&#20998;&#25968;&#20998;&#24067;&#30340;&#36827;&#19968;&#27493;&#26032;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memorization, or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim, is a key concern for safely deploying language models. In particular, it is vital to minimize a model's memorization of sensitive datapoints such as those containing personal identifiable information (PII). The prevalence of such undesirable memorization can pose issues for model trainers, and may even require discarding an otherwise functional model. We therefore seek to predict which sequences will be memorized before a large model's full train-time by extrapolating the memorization behavior of lower-compute trial runs. We measure memorization of the Pythia model suite, and find that intermediate checkpoints are better predictors of a model's memorization behavior than smaller fully-trained models. We additionally provide further novel discoveries on the distribution of memorization scores across models and data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#23558;CVE&#28431;&#27934;&#35760;&#24405;&#26144;&#23556;&#21040;MITRE CWE&#24369;&#28857;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.11130</link><description>&lt;p&gt;
&#33258;&#21160;&#23558;CVE&#28431;&#27934;&#35760;&#24405;&#26144;&#23556;&#21040;MITRE CWE&#24369;&#28857;
&lt;/p&gt;
&lt;p&gt;
Automated Mapping of CVE Vulnerability Records to MITRE CWE Weaknesses. (arXiv:2304.11130v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11130
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#23558;CVE&#28431;&#27934;&#35760;&#24405;&#26144;&#23556;&#21040;MITRE CWE&#24369;&#28857;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#21644;&#22810;&#26679;&#24615;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#28431;&#27934;&#25253;&#21578;&#21644;&#20998;&#26512;&#30340;&#22686;&#21152;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#35768;&#22810;&#38750;&#33829;&#21033;&#32452;&#32455;&#22312;&#36825;&#19968;&#39046;&#22495;&#23835;&#36215;&#65292;&#22914;MITRE&#21644;OSWAP&#65292;&#20182;&#20204;&#19968;&#30452;&#22312;&#31215;&#26497;&#36861;&#36394;&#28431;&#27934;&#65292;&#24182;&#20197;&#26631;&#20934;&#21270;&#26684;&#24335;&#21457;&#24067;&#38450;&#24481;&#24314;&#35758;&#12290;&#30001;&#20110;&#25163;&#21160;&#29983;&#20135;&#36825;&#31181;&#26684;&#24335;&#30340;&#25968;&#25454;&#38750;&#24120;&#32791;&#26102;&#65292;&#22240;&#27492;&#19968;&#20123;&#25552;&#35758;&#35797;&#22270;&#33258;&#21160;&#21270;&#35813;&#36807;&#31243;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#37319;&#29992;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#26159;&#32570;&#20047;&#20844;&#24320;&#30340;&#19987;&#19994;&#25968;&#25454;&#38598;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#23558;CVE&#35760;&#24405;&#26144;&#23556;&#21040;MITRE CWE&#24369;&#28857;&#65292;&#24182;&#21521;&#30740;&#31350;&#31038;&#21306;&#21457;&#24067;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;4,012&#26465;&#35760;&#24405;&#12290;&#22312;&#32771;&#34385;&#21040;&#20154;&#22312;&#24490;&#29615;&#26694;&#26550;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#35270;&#20026;&#25490;&#21517;&#20219;&#21153;&#65292;&#24182;&#26088;&#22312;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#21033;&#29992;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, a proliferation of cyber-security threats and diversity has been on the rise culminating in an increase in their reporting and analysis. To counter that, many non-profit organizations have emerged in this domain, such as MITRE and OSWAP, which have been actively tracking vulnerabilities, and publishing defense recommendations in standardized formats. As producing data in such formats manually is very time-consuming, there have been some proposals to automate the process. Unfortunately, a major obstacle to adopting supervised machine learning for this problem has been the lack of publicly available specialized datasets. Here, we aim to bridge this gap. In particular, we focus on mapping CVE records into MITRE CWE Weaknesses, and we release to the research community a manually annotated dataset of 4,012 records for this task. With a human-in-the-loop framework in mind, we approach the problem as a ranking task and aim to incorporate reinforced learning to make use of the
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26045;&#21152;&#28966;&#34385;&#33021;&#24433;&#21709;&#23427;&#20204;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;&#65292;&#36825;&#38656;&#35201;&#26356;&#22810;&#36947;&#24503;&#32771;&#34385;&#21644;&#30417;&#31649;&#12290;</title><link>http://arxiv.org/abs/2304.11111</link><description>&lt;p&gt;
&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28966;&#34385;&#20250;&#22686;&#21152;&#23427;&#20204;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Inducing anxiety in large language models increases exploration and bias. (arXiv:2304.11111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11111
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26045;&#21152;&#28966;&#34385;&#33021;&#24433;&#21709;&#23427;&#20204;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;&#65292;&#36825;&#38656;&#35201;&#26356;&#22810;&#36947;&#24503;&#32771;&#34385;&#21644;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#25913;&#21464;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#65292;&#24341;&#21457;&#20844;&#20247;&#30340;&#36777;&#35770;&#12290;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#19981;&#20165;&#20309;&#26102;&#33021;&#22815;&#27491;&#24120;&#24037;&#20316;&#21644;&#25104;&#21151;&#65292;&#20063;&#20026;&#20160;&#20040;&#20250;&#22833;&#36133;&#21644;&#34892;&#20026;&#22833;&#24120;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#31038;&#20250;&#24847;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#35745;&#31639;&#31934;&#31070;&#30149;&#23398;&#30340;&#35270;&#35282;&#36716;&#21521;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;Generative Pre-Trained Transformer 3.5&#65292;&#24182;&#23558;&#20854;&#32622;&#20110;&#31934;&#31070;&#30149;&#23398;&#20013;&#24120;&#35265;&#30340;&#20219;&#21153;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-3.5&#23545;&#24120;&#35265;&#30340;&#28966;&#34385;&#38382;&#21367;&#20570;&#20986;&#26377;&#21147;&#30340;&#21453;&#24212;&#65292;&#20135;&#29983;&#27604;&#20154;&#31867;&#20027;&#20307;&#26356;&#39640;&#30340;&#28966;&#34385;&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#24773;&#32490;&#24863;&#24212;&#25552;&#31034;&#21487;&#20197;&#21487;&#39044;&#27979;&#22320;&#25913;&#21464;GPT-3.5&#30340;&#21453;&#24212;&#12290;&#24773;&#24863;&#24863;&#24212;&#19981;&#20165;&#24433;&#21709;GPT-3.5&#22312;&#34913;&#37327;&#25506;&#32034;&#20915;&#31574;-making&#30340;&#35748;&#30693;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65292;&#36824;&#24433;&#21709;&#20854;&#22312;&#20043;&#21069;&#24314;&#31435;&#30340;&#34913;&#37327;&#31181;&#26063;&#20027;&#20041;&#21644;&#22833;&#33021;&#20027;&#20041;&#31561;&#20559;&#35265;&#30340;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;GPT-3.5&#22312;&#21463;&#21040;&#28966;&#34385;&#35825;&#23548;&#26102;&#21576;&#29616;&#20986;&#26126;&#26174;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;&#22686;&#21152;&#65292;&#34920;&#26126;&#20854;&#36755;&#20986;&#23481;&#26131;&#21463;&#21040;&#24773;&#24863;&#25805;&#32437;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#20351;&#29992;&#36807;&#31243;&#20013;&#38656;&#35201;&#26356;&#22810;&#30340;&#36947;&#24503;&#32771;&#34385;&#21644;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are transforming research on machine learning while galvanizing public debates. Understanding not only when these models work well and succeed but also why they fail and misbehave is of great societal relevance. We propose to turn the lens of computational psychiatry, a framework used to computationally describe and modify aberrant behavior, to the outputs produced by these models. We focus on the Generative Pre-Trained Transformer 3.5 and subject it to tasks commonly studied in psychiatry. Our results show that GPT-3.5 responds robustly to a common anxiety questionnaire, producing higher anxiety scores than human subjects. Moreover, GPT-3.5's responses can be predictably changed by using emotion-inducing prompts. Emotion-induction not only influences GPT-3.5's behavior in a cognitive task measuring exploratory decision-making but also influences its behavior in a previously-established task measuring biases such as racism and ableism. Crucially, GPT-3.5 shows a s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;ChatABL&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLM&#25972;&#21512;&#21040;&#24402;&#32435;&#23398;&#20064;ABL&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#23454;&#29616;&#20132;&#20114;&#24335;&#23398;&#20064;&#65292;&#26088;&#22312;&#20197;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#21644;&#26131;&#29702;&#35299;&#30340;&#26041;&#24335;&#32479;&#19968;&#24863;&#30693;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatABL&#21487;&#20197;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#22320;&#23398;&#20064;&#35299;&#20915;&#24402;&#32435;&#25512;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.11107</link><description>&lt;p&gt;
ChatABL&#65306;&#36890;&#36807;&#19982;ChatGPT&#30340;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#23454;&#29616;&#24402;&#32435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT. (arXiv:2304.11107v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;ChatABL&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLM&#25972;&#21512;&#21040;&#24402;&#32435;&#23398;&#20064;ABL&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#23454;&#29616;&#20132;&#20114;&#24335;&#23398;&#20064;&#65292;&#26088;&#22312;&#20197;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#21644;&#26131;&#29702;&#35299;&#30340;&#26041;&#24335;&#32479;&#19968;&#24863;&#30693;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatABL&#21487;&#20197;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#22320;&#23398;&#20064;&#35299;&#20915;&#24402;&#32435;&#25512;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#25968;&#23398;&#33021;&#21147;&#26041;&#38754;&#30340;&#37325;&#22823;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#19982;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#19968;&#33268;&#30340;&#26377;&#20215;&#20540;&#30340;&#25512;&#29702;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#20043;&#38388;&#24213;&#23618;&#20449;&#24687;&#27969;&#30340;&#19981;&#20860;&#23481;&#24615;&#65292;LLM&#30446;&#21069;&#38590;&#20197;&#22312;&#24863;&#30693;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#24314;&#31435;&#26725;&#26753;&#65292;&#20351;&#24471;&#23454;&#29616;&#33258;&#20027;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29992;&#20110;&#23558;&#24863;&#30693;&#21644;&#25512;&#29702;&#20004;&#31181;&#33021;&#21147;&#25972;&#21512;&#30340;&#24402;&#32435;&#23398;&#20064;(ABL)&#26694;&#26550;&#22312;&#19981;&#23436;&#25972;&#20107;&#23454;&#30340;&#36870;&#21521;&#35299;&#23494;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#21463;&#21040;&#36923;&#36753;&#25512;&#29702;&#35268;&#21017;&#30340;&#35821;&#20041;&#29702;&#35299;&#19981;&#36275;&#20197;&#21450;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#39046;&#22495;&#30693;&#35782;&#34920;&#31034;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;(ChatABL)&#65292;&#23558;LLM&#25972;&#21512;&#21040;ABL&#26694;&#26550;&#20013;&#65292;&#26088;&#22312;&#20197;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#21644;&#26131;&#29702;&#35299;&#30340;&#26041;&#24335;&#32479;&#19968;&#36825;&#19977;&#31181;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;LLM&#30340;&#20248;&#21183;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#36827;&#34892;&#20132;&#20114;&#24335;&#23398;&#20064;&#65292;&#23454;&#29616;&#24402;&#32435;&#25512;&#29702;&#12290;ChatABL&#26694;&#26550;&#26088;&#22312;&#20351;LLM&#22312;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#24314;&#31435;&#26725;&#26753;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#31526;&#21495;&#25512;&#29702;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;ABL&#27169;&#22411;&#30456;&#27604;&#65292;ChatABL&#21487;&#20197;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#22320;&#23398;&#20064;&#35299;&#20915;&#24402;&#32435;&#25512;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT have recently demonstrated significant potential in mathematical abilities, providing valuable reasoning paradigm consistent with human natural language. However, LLMs currently have difficulty in bridging perception, language understanding and reasoning capabilities due to incompatibility of the underlying information flow among them, making it challenging to accomplish tasks autonomously. On the other hand, abductive learning (ABL) frameworks for integrating the two abilities of perception and reasoning has seen significant success in inverse decipherment of incomplete facts, but it is limited by the lack of semantic understanding of logical reasoning rules and the dependence on complicated domain knowledge representation. This paper presents a novel method (ChatABL) for integrating LLMs into the ABL framework, aiming at unifying the three abilities in a more user-friendly and understandable manner. The proposed method uses the strengths o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19981;&#38656;&#35201;&#35757;&#32451;&#65292;&#22522;&#20110;&#31616;&#21333;&#26144;&#23556;&#30340;&#36328;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32534;&#30721;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#35821;&#20041;&#19978;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#26144;&#23556;&#21040;&#21516;&#19968;&#31354;&#38388;&#65292;&#24182;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.11095</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#35757;&#32451;&#65292;&#36328;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#26159;&#21542;&#21487;&#34892;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Cross-modal Information Retrieval Possible without Training?. (arXiv:2304.11095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19981;&#38656;&#35201;&#35757;&#32451;&#65292;&#22522;&#20110;&#31616;&#21333;&#26144;&#23556;&#30340;&#36328;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32534;&#30721;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#35821;&#20041;&#19978;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#26144;&#23556;&#21040;&#21516;&#19968;&#31354;&#38388;&#65292;&#24182;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#34920;&#31034;(&#20363;&#22914;BERT&#25991;&#26412;&#23884;&#20837;&#65292;&#22270;&#20687;&#30340;&#20498;&#25968;&#31532;&#20108;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23618;&#28608;&#27963;)&#20256;&#36882;&#20102;&#19968;&#32452;&#26377;&#30410;&#30340;&#20449;&#24687;&#26816;&#32034;&#29305;&#24449;&#12290;&#32473;&#23450;&#25968;&#25454;&#27169;&#24577;&#30340;&#23884;&#20837;&#23384;&#22312;&#33258;&#24049;&#30340;&#39640;&#32500;&#31354;&#38388;&#20013;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#26144;&#23556;&#36827;&#34892;&#35821;&#20041;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#26368;&#23567;&#20108;&#20056;&#27861;&#21644;&#22855;&#24322;&#20540;&#20998;&#35299; (SVD) &#30340;&#31616;&#21333;&#26144;&#23556;&#20316;&#20026;Procrustes&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#23454;&#29616;&#36328;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#30340;&#25163;&#27573;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#32473;&#23450;&#19968;&#20010;&#27169;&#24577;&#20013;&#30340;&#20449;&#24687;&#65292;&#20363;&#22914;&#25991;&#26412;&#65292;&#35813;&#26144;&#23556;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#22312;&#21478;&#19968;&#20010;&#27169;&#24577;&#20013;&#25214;&#21040;&#19982;&#20854;&#35821;&#20041;&#30456;&#24403;&#30340;&#25968;&#25454;&#39033;&#65292;&#20363;&#22914;&#22270;&#20687;&#12290;&#20351;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;&#26816;&#32034;&#20219;&#21153;&#20013;&#23581;&#35797;&#20102;&#19978;&#36848;&#31616;&#21333;&#30340;&#36328;&#27169;&#24577;&#26144;&#23556;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#25105;&#20204;&#30340;&#26144;&#23556;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Encoded representations from a pretrained deep learning model (e.g., BERT text embeddings, penultimate CNN layer activations of an image) convey a rich set of features beneficial for information retrieval. Embeddings for a particular modality of data occupy a high-dimensional space of its own, but it can be semantically aligned to another by a simple mapping without training a deep neural net. In this paper, we take a simple mapping computed from the least squares and singular value decomposition (SVD) for a solution to the Procrustes problem to serve a means to cross-modal information retrieval. That is, given information in one modality such as text, the mapping helps us locate a semantically equivalent data item in another modality such as image. Using off-the-shelf pretrained deep learning models, we have experimented the aforementioned simple cross-modal mappings in tasks of text-to-image and image-to-text retrieval. Despite simplicity, our mappings perform reasonably well reachin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#26412;&#22303;&#30340;&#35270;&#35282;&#25506;&#35752;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21435;&#20559;&#35265;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#21628;&#21505;&#22312;&#31639;&#27861;&#20013;&#32435;&#20837;&#26412;&#22320;&#30693;&#35782;&#21644;&#29702;&#35299;&#20197;&#30830;&#20445;&#20844;&#27491;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#31038;&#20250;&#26102;&#12290;</title><link>http://arxiv.org/abs/2304.11094</link><description>&lt;p&gt;
&#21435;&#20559;&#35265;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65306;&#19968;&#20010;&#26412;&#22303;&#30340;&#23450;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Effectiveness of Debiasing Techniques: An Indigenous Qualitative Analysis. (arXiv:2304.11094v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#26412;&#22303;&#30340;&#35270;&#35282;&#25506;&#35752;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21435;&#20559;&#35265;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#21628;&#21505;&#22312;&#31639;&#27861;&#20013;&#32435;&#20837;&#26412;&#22320;&#30693;&#35782;&#21644;&#29702;&#35299;&#20197;&#30830;&#20445;&#20844;&#27491;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#31038;&#20250;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#26412;&#22303;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21435;&#20559;&#35265;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;&#30446;&#21069;&#34913;&#37327;&#19982;&#21435;&#20559;&#35265;PLMs&#20351;&#29992;&#30340;&#25216;&#26415;&#23384;&#22312;&#32654;&#22269;&#31181;&#26063;&#20559;&#35265;&#30340;&#20542;&#21521;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#20559;&#35265;&#23646;&#24615;&#65288;&#20363;&#22914;&#8220;&#40657;&#20154;&#8221;&#19982;&#8220;&#30333;&#20154;&#8221;&#65289;&#12290;&#26377;&#20123;&#25216;&#26415;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#38598;&#21644;&#36827;&#19968;&#27493;&#30340;&#39044;&#35757;&#32451;&#12290;&#36825;&#26679;&#30340;&#25216;&#26415;&#24182;&#19981;&#33021;&#25429;&#25417;&#20854;&#20182;&#22269;&#23478;&#20013;&#34987;&#36739;&#23569;&#20195;&#34920;&#30340;&#22303;&#33879;&#20154;&#21475;&#65292;&#20363;&#22914;&#26032;&#35199;&#20848;&#30340;&#27611;&#21033;&#20154;&#12290;&#24517;&#39035;&#32435;&#20837;&#26412;&#22320;&#30340;&#30693;&#35782;&#21644;&#29702;&#35299;&#65292;&#20197;&#30830;&#20445;&#20844;&#27491;&#30340;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#31038;&#20250;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
An indigenous perspective on the effectiveness of debiasing techniques for pre-trained language models (PLMs) is presented in this paper. The current techniques used to measure and debias PLMs are skewed towards the US racial biases and rely on pre-defined bias attributes (e.g. "black" vs "white"). Some require large datasets and further pre-training. Such techniques are not designed to capture the underrepresented indigenous populations in other countries, such as M\=aori in New Zealand. Local knowledge and understanding must be incorporated to ensure unbiased algorithms, especially when addressing a resource-restricted society.
&lt;/p&gt;</description></item><item><title>&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#36890;&#36807;DPCC&#21019;&#36896;&#20986;&#33021;&#22815;&#19982;&#29992;&#25143;&#36827;&#34892;&#35270;&#21548;&#20132;&#20114;&#30340;&#28145;&#24230;&#20010;&#24615;&#21270;&#25968;&#23383;&#35282;&#33394;&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#36817;10k&#20010;&#35805;&#35821;&#21644;6&#20010;&#23567;&#26102;&#38899;&#39057;/&#35270;&#39057;&#30340;&#35282;&#33394;&#20013;&#24515;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.11093</link><description>&lt;p&gt;
Hi Sheldon! &#20174;&#30005;&#35270;&#21095;&#20013;&#21019;&#24314;&#28145;&#24230;&#20010;&#24615;&#21270;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hi Sheldon! Creating Deep Personalized Characters from TV Shows. (arXiv:2304.11093v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11093
&lt;/p&gt;
&lt;p&gt;
&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#36890;&#36807;DPCC&#21019;&#36896;&#20986;&#33021;&#22815;&#19982;&#29992;&#25143;&#36827;&#34892;&#35270;&#21548;&#20132;&#20114;&#30340;&#28145;&#24230;&#20010;&#24615;&#21270;&#25968;&#23383;&#35282;&#33394;&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#36817;10k&#20010;&#35805;&#35821;&#21644;6&#20010;&#23567;&#26102;&#38899;&#39057;/&#35270;&#39057;&#30340;&#35282;&#33394;&#20013;&#24515;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24819;&#35937;&#19968;&#19979;&#65292;&#20320;&#21487;&#20197;&#19982;&#19968;&#20010;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25968;&#23383;&#35282;&#33394;&#36827;&#34892;&#35270;&#21548;&#20132;&#20114;&#65292;&#20854;&#22806;&#35980;&#21644;&#20010;&#24615;&#19982;&#12298;&#29983;&#27963;&#22823;&#29190;&#28856;&#12299;&#20013;&#30340;Sheldon&#20960;&#20046;&#19968;&#27169;&#19968;&#26679;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#31070;&#22855;&#30340;&#35270;&#21548;&#20132;&#20114;&#22330;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"Deep Personalized Character Creation&#65288;DPCC&#65289;"&#30340;&#21019;&#26032;&#20219;&#21153;&#65306;&#20174;&#30005;&#35270;&#21095;&#31561;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#21019;&#36896;&#20986;&#20010;&#24615;&#21270;&#35282;&#33394;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#21333;&#19968;&#25110;&#22810;&#20010;&#27169;&#24335;&#30340;&#25991;&#26412;&#12289;&#38899;&#39057;&#25110;&#35270;&#39057;&#36755;&#20837;&#65292;DPCC&#26088;&#22312;&#29983;&#25104;&#19982;&#26576;&#20010;&#29305;&#23450;&#35282;&#33394;&#65288;&#22914;Sheldon&#65289;&#30340;&#20010;&#24615;&#29305;&#28857;&#38750;&#24120;&#21305;&#37197;&#19988;&#36136;&#37327;&#39640;&#30340;&#22810;&#27169;&#24577;&#65288;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#65289;&#21709;&#24212;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#21019;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25910;&#38598;&#20102;&#19968;&#20010;&#21517;&#20026;"Deep Personalized Character Dataset&#65288;DPCD&#65289;"&#30340;&#35282;&#33394;&#20013;&#24515;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;~10k&#20010;&#35805;&#35821;&#21644;~6&#20010;&#23567;&#26102;&#30340;&#38899;&#39057;/&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imagine an interesting multimodal interactive scenario that you can see, hear, and chat with an AI-generated digital character, who is capable of behaving like Sheldon from The Big Bang Theory, as a DEEP copy from appearance to personality. Towards this fantastic multimodal chatting scenario, we propose a novel task, named Deep Personalized Character Creation (DPCC): creating multimodal chat personalized characters from multimodal data such as TV shows. Specifically, given a single- or multi-modality input (text, audio, video), the goal of DPCC is to generate a multi-modality (text, audio, video) response, which should be well-matched the personality of a specific character such as Sheldon, and of high quality as well. To support this novel task, we further collect a character centric multimodal dialogue dataset, named Deep Personalized Character Dataset (DPCD), from TV shows. DPCD contains character-specific multimodal dialogue data of ~10k utterances and ~6 hours of audio/video per c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2304.11090</link><description>&lt;p&gt;
&#22312;ChatGPT&#26102;&#20195;&#36808;&#21521;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#30340;&#21442;&#32771;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Towards Responsible AI in the Era of ChatGPT: A Reference Architecture for Designing Foundation Model-based AI Systems. (arXiv:2304.11090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#12289;Bard&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25512;&#20986;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24341;&#36215;&#20102;&#24040;&#22823;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#23558;&#25104;&#20026;&#26410;&#26469;&#22823;&#22810;&#25968;AI&#31995;&#32479;&#30340;&#22522;&#30784;&#26500;&#24314;&#22359;&#30340;&#36235;&#21183;&#27491;&#22312;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#23558;&#22522;&#30784;&#27169;&#22411;&#32435;&#20837;AI&#31995;&#32479;&#24341;&#21457;&#20102;&#23545;&#36127;&#36131;&#20219;AI&#30340;&#37325;&#22823;&#20851;&#27880;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#40657;&#21283;&#23376;&#24615;&#36136;&#21644;&#24555;&#36895;&#21457;&#23637;&#30340;&#36229;&#32423;&#26234;&#33021;&#24341;&#36215;&#30340;&#12290;&#27492;&#22806;&#65292;&#22522;&#30784;&#27169;&#22411;&#30340;&#22686;&#38271;&#33021;&#21147;&#26368;&#32456;&#21487;&#33021;&#20250;&#21534;&#22124;AI&#31995;&#32479;&#30340;&#20854;&#20182;&#32452;&#20214;&#65292;&#24341;&#20837;&#26550;&#26500;&#35774;&#35745;&#20013;&#30340;&#36816;&#21160;&#36793;&#30028;&#21644;&#25509;&#21475;&#28436;&#21464;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#12290;&#29305;&#21035;&#22320;&#65292;&#26412;&#25991;&#39318;&#20808;&#21576;&#29616;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#22312;&#26550;&#26500;&#28436;&#36827;&#26041;&#38754;&#30340;&#21457;&#23637;&#65292;&#20174;"&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#36830;&#25509;&#22120;"&#21040;"&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#21333;&#29255;&#26426;&#26680;"&#12290;&#28982;&#21518;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#32771;&#26550;&#26500;&#65292;&#21253;&#25324;&#20116;&#20010;&#31867;&#21035;&#30340;&#27169;&#24335;&#65292;&#37325;&#28857;&#20851;&#27880;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#65292;&#20363;&#22914;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#21442;&#32771;&#26550;&#26500;&#20026;&#35774;&#35745;&#36127;&#36131;&#20219;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#20102;&#31995;&#32479;&#21270;&#21644;&#36879;&#26126;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The release of ChatGPT, Bard, and other large language model (LLM)-based chatbots has drawn huge attention on foundations models worldwide. There is a growing trend that foundation models will serve as the fundamental building blocks for most of the future AI systems. However, incorporating foundation models in AI systems raises significant concerns about responsible AI due to their black box nature and rapidly advancing super-intelligence. Additionally, the foundation model's growing capabilities can eventually absorb the other components of AI systems, introducing the moving boundary and interface evolution challenges in architecture design. To address these challenges, this paper proposes a pattern-oriented responsible-AI-by-design reference architecture for designing foundation model-based AI systems. Specially, the paper first presents an architecture evolution of AI systems in the era of foundation models, from "foundation-model-as-a-connector" to "foundation-model-as-a-monolithi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#26032;&#38395;&#26631;&#39064;&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#24773;&#24863;&#29305;&#24449;&#26469;&#23545;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#36827;&#34892;&#20998;&#31867;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2304.11088</link><description>&lt;p&gt;
&#20351;&#29992;&#26032;&#38395;&#26631;&#39064;&#26469;&#20998;&#26512;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Profiling the news spreading barriers using news headlines. (arXiv:2304.11088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26032;&#38395;&#26631;&#39064;&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#24773;&#24863;&#29305;&#24449;&#26469;&#23545;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#36827;&#34892;&#20998;&#31867;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#26631;&#39064;&#21487;&#20197;&#26159;&#26816;&#27979;&#26032;&#38395;&#23186;&#20307;&#20013;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#30340;&#22909;&#25968;&#25454;&#28304;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#26412;&#25991;&#21033;&#29992;&#22522;&#20110;&#25512;&#29702;&#30340;&#27169;&#22411;COMET&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#26032;&#38395;&#26631;&#39064;&#30340;&#24773;&#24863;&#29305;&#24449;&#26469;&#23545;&#38556;&#30861;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#25991;&#21270;&#12289;&#32463;&#27982;&#12289;&#25919;&#27835;&#12289;&#35821;&#35328;&#21644;&#22320;&#29702;&#31561;&#20116;&#31181;&#38556;&#30861;&#65292;&#20197;&#21450;&#21253;&#25324;&#20581;&#24247;&#12289;&#36816;&#21160;&#12289;&#31185;&#23398;&#12289;&#23089;&#20048;&#12289;&#28216;&#25103;&#12289;&#20303;&#25151;&#12289;&#31038;&#20250;&#12289;&#36141;&#29289;&#12289;&#35745;&#31639;&#26426;&#21644;&#21830;&#19994;&#31561;&#19981;&#21516;&#31867;&#22411;&#30340;&#26032;&#38395;&#26631;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#26032;&#38395;&#20986;&#29256;&#21830;&#30340;&#20803;&#25968;&#25454;&#33258;&#21160;&#25910;&#38598;&#21644;&#26631;&#35760;&#26032;&#38395;&#26631;&#39064;&#65292;&#20197;&#27492;&#26469;&#26816;&#27979;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;transformer&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#25512;&#29702;&#20026;&#22522;&#30784;&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#24773;&#24863;&#29305;&#24449;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
News headlines can be a good data source for detecting the news spreading barriers in news media, which may be useful in many real-world applications. In this paper, we utilize semantic knowledge through the inference-based model COMET and sentiments of news headlines for barrier classification. We consider five barriers including cultural, economic, political, linguistic, and geographical, and different types of news headlines including health, sports, science, recreation, games, homes, society, shopping, computers, and business. To that end, we collect and label the news headlines automatically for the barriers using the metadata of news publishers. Then, we utilize the extracted commonsense inferences and sentiments as features to detect the news spreading barriers. We compare our approach to the classical text classification methods, deep learning, and transformer-based methods. The results show that the proposed approach using inferences-based semantic knowledge and sentiment offe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;ChatGPT&#22312;&#25991;&#26412;&#26631;&#27880;&#21644;&#20998;&#31867;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#36755;&#20986;&#30340;&#19968;&#33268;&#24615;&#19981;&#36275;&#20197;&#28385;&#36275;&#31185;&#23398;&#21487;&#38752;&#24615;&#30340;&#38408;&#20540;&#65292;&#22240;&#27492;&#20351;&#29992;ChatGPT&#26102;&#38656;&#35201;&#35880;&#24910;&#12290;</title><link>http://arxiv.org/abs/2304.11085</link><description>&lt;p&gt;
&#23545;ChatGPT&#29992;&#20110;&#25991;&#26412;&#26631;&#27880;&#21644;&#20998;&#31867;&#30340;&#21487;&#38752;&#24615;&#36827;&#34892;&#27979;&#35797;&#8212;&#8212;&#19968;&#20010;&#35686;&#21578;&#24615;&#30340;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Testing the Reliability of ChatGPT for Text Annotation and Classification: A Cautionary Remark. (arXiv:2304.11085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;ChatGPT&#22312;&#25991;&#26412;&#26631;&#27880;&#21644;&#20998;&#31867;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#36755;&#20986;&#30340;&#19968;&#33268;&#24615;&#19981;&#36275;&#20197;&#28385;&#36275;&#31185;&#23398;&#21487;&#38752;&#24615;&#30340;&#38408;&#20540;&#65292;&#22240;&#27492;&#20351;&#29992;ChatGPT&#26102;&#38656;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;ChatGPT&#22312;&#21508;&#31181;&#25991;&#26412;&#26631;&#27880;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;ChatGPT&#26159;&#38750;&#30830;&#23450;&#24615;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#19982;&#20154;&#31867;&#32534;&#30721;&#22120;&#19968;&#26679;&#65292;&#30456;&#21516;&#30340;&#36755;&#20837;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#36755;&#20986;&#12290;&#37492;&#20110;&#27492;&#65292;&#27979;&#35797;ChatGPT&#30340;&#21487;&#38752;&#24615;&#20284;&#20046;&#26159;&#24688;&#24403;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#22312;&#25991;&#26412;&#26631;&#27880;&#21644;&#20998;&#31867;&#20013;&#30340;&#38646;-shot&#33021;&#21147;&#30340;&#19968;&#33268;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#19981;&#21516;&#30340;&#27169;&#22411;&#21442;&#25968;&#12289;&#25552;&#31034;&#21464;&#21270;&#21644;&#30456;&#21516;&#36755;&#20837;&#30340;&#37325;&#22797;&#12290;&#22522;&#20110;&#23545;&#23558;&#32593;&#31449;&#25991;&#26412;&#21306;&#20998;&#20026;&#26032;&#38395;&#21644;&#38750;&#26032;&#38395;&#30340;&#23454;&#38469;&#20998;&#31867;&#20219;&#21153;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#30340;&#20998;&#31867;&#36755;&#20986;&#19968;&#33268;&#24615;&#21487;&#33021;&#19981;&#36275;&#20197;&#28385;&#36275;&#31185;&#23398;&#21487;&#38752;&#24615;&#30340;&#38408;&#20540;&#12290;&#20363;&#22914;&#65292;&#25552;&#31034;&#20013;&#21363;&#20351;&#36827;&#34892;&#24494;&#23567;&#30340;&#25514;&#36766;&#25913;&#21464;&#25110;&#37325;&#22797;&#30456;&#21516;&#30340;&#36755;&#20837;&#20063;&#20250;&#23548;&#33268;&#36755;&#20986;&#19981;&#21516;&#12290;&#34429;&#28982;&#20174;&#22810;&#27425;&#37325;&#22797;&#30340;&#36755;&#20986;&#20013;&#27719;&#24635;&#21487;&#20197;&#25552;&#39640;&#21487;&#38752;&#24615;&#65292;&#20294;&#26412;&#30740;&#31350;&#24314;&#35758;&#22312;&#20351;&#29992;ChatGPT&#36827;&#34892;&#25991;&#26412;&#26631;&#27880;&#21644;&#20998;&#31867;&#26102;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated promising potential of ChatGPT for various text annotation and classification tasks. However, ChatGPT is non-deterministic which means that, as with human coders, identical input can lead to different outputs. Given this, it seems appropriate to test the reliability of ChatGPT. Therefore, this study investigates the consistency of ChatGPT's zero-shot capabilities for text annotation and classification, focusing on different model parameters, prompt variations, and repetitions of identical inputs. Based on the real-world classification task of differentiating website texts into news and not news, results show that consistency in ChatGPT's classification output can fall short of scientific thresholds for reliability. For example, even minor wording alterations in prompts or repeating the identical input can lead to varying outputs. Although pooling outputs from multiple repetitions can improve reliability, this study advises caution when using ChatGPT for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.11082</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19982;&#20154;&#20132;&#20114;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#26041;&#38754;&#26159;&#23545;&#40784;&#20854;&#34892;&#20026;&#65292;&#20351;&#20854;&#23545;&#20854;&#20154;&#31867;&#29992;&#25143;&#26377;&#29992;&#19988;&#26080;&#23475;&#12290;&#36825;&#36890;&#24120;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#23454;&#29616;&#65292;&#20197;&#22686;&#24378;&#25152;&#38656;&#30340;&#34892;&#20026;&#24182;&#25233;&#21046;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;(BEB)&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#27491;&#24335;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20960;&#20010;&#20869;&#22312;&#29305;&#24449;&#21644;&#23545;&#40784;&#30340;&#38480;&#21046;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;&#34987;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#26377;&#38480;&#27010;&#29575;&#30340;&#34892;&#20026;&#65292;&#37117;&#23384;&#22312;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#36755;&#20986;&#27492;&#34892;&#20026;&#30340;&#25552;&#31034;&#65292;&#20854;&#27010;&#29575;&#38543;&#25552;&#31034;&#30340;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#36825;&#24847;&#21619;&#30528;&#20219;&#20309;&#20943;&#24369;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#20294;&#26410;&#23558;&#20854;&#23436;&#20840;&#28040;&#38500;&#30340;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#25269;&#24481;&#38024;&#23545;&#24615;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#31034;&#20102;&#39046;&#20808;&#30340;
&lt;/p&gt;
&lt;p&gt;
An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading al
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#24076;&#20271;&#26469;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#25152;&#38656;&#30340;&#26368;&#22823;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;HeDC4&#65292;&#20197;&#21450;&#20004;&#31181;&#34920;&#29616;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65306;&#29992;&#20110;&#26631;&#20934;&#38271;&#24230;&#36755;&#20837;&#30340;HeRo&#21644;&#29992;&#20110;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;LongHeRo&#12290;&#20004;&#20010;&#27169;&#22411;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.11077</link><description>&lt;p&gt;
HeRo: RoBERTa&#21644; Longformer&#30340;&#24076;&#20271;&#26469;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HeRo: RoBERTa and Longformer Hebrew Language Models. (arXiv:2304.11077v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#24076;&#20271;&#26469;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#25152;&#38656;&#30340;&#26368;&#22823;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;HeDC4&#65292;&#20197;&#21450;&#20004;&#31181;&#34920;&#29616;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65306;&#29992;&#20110;&#26631;&#20934;&#38271;&#24230;&#36755;&#20837;&#30340;HeRo&#21644;&#29992;&#20110;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;LongHeRo&#12290;&#20004;&#20010;&#27169;&#22411;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22635;&#34917;&#20102;&#24076;&#20271;&#26469;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#29616;&#26377;&#36164;&#28304;&#30340;&#31354;&#30333;&#65292;&#25552;&#20379;&#36804;&#20170;&#26368;&#22823;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;HeDC4&#12289;&#29992;&#20110;&#26631;&#20934;&#38271;&#24230;&#36755;&#20837;&#30340;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;HeRo&#20197;&#21450;&#29992;&#20110;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#39640;&#25928;transformer LongHeRo. HeRo &#27169;&#22411;&#22312;&#24773;&#24863;&#20998;&#26512;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32780; LongHeRo &#27169;&#22411;&#22312;&#30001;&#38271;&#25991;&#26723;&#32452;&#25104;&#30340;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290; &#36825;&#20004;&#20010;&#27169;&#22411;&#22343;&#21576;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26816;&#26597;&#28857;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we fill in an existing gap in resources available to the Hebrew NLP community by providing it with the largest so far pre-train dataset HeDC4, a state-of-the-art pre-trained language model HeRo for standard length inputs and an efficient transformer LongHeRo for long input sequences. The HeRo model was evaluated on the sentiment analysis, the named entity recognition, and the question answering tasks while the LongHeRo model was evaluated on the document classification task with a dataset composed of long documents. Both HeRo and LongHeRo presented state-of-the-art performance. The dataset and model checkpoints used in this work are publicly available.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#36136;&#30097;&#31867;ChatGPT&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#33021;&#20445;&#35777;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#24456;&#22810;&#26032;&#19968;&#20195;&#25628;&#32034;&#24341;&#25806;&#30340;&#20844;&#24320;&#28436;&#31034;&#20013;&#23384;&#22312;&#20107;&#23454;&#38169;&#35823;&#65292;&#21628;&#21505;&#31185;&#30740;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25552;&#39640;&#36879;&#26126;&#24230;&#21644;&#20107;&#23454;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11076</link><description>&lt;p&gt;
&#8220;&#31867;ChatGPT&#29983;&#25104;&#27169;&#22411;&#33021;&#21542;&#20445;&#35777;&#20107;&#23454;&#20934;&#30830;&#24615;&#65311;&#26032;&#19968;&#20195;&#25628;&#32034;&#24341;&#25806;&#30340;&#22833;&#35823;&#8221;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT-like Generative Models Guarantee Factual Accuracy? On the Mistakes of New Generation Search Engines. (arXiv:2304.11076v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11076
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#36136;&#30097;&#31867;ChatGPT&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#33021;&#20445;&#35777;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#24456;&#22810;&#26032;&#19968;&#20195;&#25628;&#32034;&#24341;&#25806;&#30340;&#20844;&#24320;&#28436;&#31034;&#20013;&#23384;&#22312;&#20107;&#23454;&#38169;&#35823;&#65292;&#21628;&#21505;&#31185;&#30740;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25552;&#39640;&#36879;&#26126;&#24230;&#21644;&#20107;&#23454;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;OpenAI&#30340;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#23545;&#35805;&#22411;AI&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#25105;&#20204;&#36136;&#30097;&#36825;&#20123;&#27169;&#22411;&#33021;&#21542;&#20445;&#35777;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#26368;&#36817;&#65292;&#24494;&#36719;&#21644;&#35895;&#27468;&#31561;&#31185;&#25216;&#20844;&#21496;&#23459;&#24067;&#20102;&#26088;&#22312;&#23558;&#25628;&#32034;&#24341;&#25806;&#19982;&#23545;&#35805;&#22411;AI&#30456;&#32467;&#21512;&#30340;&#26032;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20844;&#24320;&#28436;&#31034;&#20013;&#23384;&#22312;&#35768;&#22810;&#38169;&#35823;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#19981;&#24212;&#36731;&#26131;&#30456;&#20449;AI&#27169;&#22411;&#30340;&#20107;&#23454;&#20027;&#24352;&#12290;&#25105;&#20204;&#24076;&#26395;&#21628;&#21505;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25913;&#21892;AI&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#20107;&#23454;&#27491;&#30830;&#24615;&#65292;&#32780;&#19981;&#26159;&#25209;&#35780;&#29305;&#23450;&#30340;&#27169;&#22411;&#25110;&#20844;&#21496;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large conversational AI models such as OpenAI's ChatGPT have demonstrated great potential, we question whether such models can guarantee factual accuracy. Recently, technology companies such as Microsoft and Google have announced new services which aim to combine search engines with conversational AI. However, we have found numerous mistakes in the public demonstrations that suggest we should not easily trust the factual claims of the AI models. Rather than criticizing specific models or companies, we hope to call on researchers and developers to improve AI models' transparency and factual correctness.
&lt;/p&gt;</description></item><item><title>&#26412;&#39033;&#30446;&#22312;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;ASR&#27169;&#22411;&#30340;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24605;&#36335;&#65292;&#36890;&#36807;&#25552;&#20986;&#32771;&#34385;&#35821;&#20041;&#36317;&#31163;&#30340;&#26032;&#39062;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;OpenAI&#30340;Whisper&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#24403;&#21069;&#20808;&#36827;&#25104;&#26524;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.11075</link><description>&lt;p&gt;
Spaiche: &#23558;&#26368;&#20808;&#36827;&#30340;ASR&#27169;&#22411;&#25193;&#23637;&#21040;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;
&lt;/p&gt;
&lt;p&gt;
Spaiche: Extending State-of-the-Art ASR Models to Swiss German Dialects. (arXiv:2304.11075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#30446;&#22312;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;ASR&#27169;&#22411;&#30340;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24605;&#36335;&#65292;&#36890;&#36807;&#25552;&#20986;&#32771;&#34385;&#35821;&#20041;&#36317;&#31163;&#30340;&#26032;&#39062;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;OpenAI&#30340;Whisper&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#24403;&#21069;&#20808;&#36827;&#25104;&#26524;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#31361;&#30772;&#22823;&#22823;&#22686;&#21152;&#20102;ASR&#31995;&#32479;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#30001;&#20110;&#38590;&#20197;&#33719;&#21462;&#30456;&#20851;&#25968;&#25454;&#65292;ASR&#27169;&#22411;&#20173;&#38656;&#35201;&#25913;&#36827;&#12290;&#26412;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#26368;&#36817;&#21457;&#24067;&#30340;&#29790;&#22763;&#24503;&#35821;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;ASR&#27169;&#22411;&#24615;&#33021;&#30340;&#35265;&#35299;&#65292;&#24110;&#21161;&#25512;&#36827;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;ASR&#27169;&#22411;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#32771;&#34385;&#20102;&#39044;&#27979;&#21644;&#22522;&#20934;&#26631;&#31614;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#12290;&#36890;&#36807;&#23545;&#29790;&#22763;&#24503;&#35821;&#25968;&#25454;&#38598;&#23545;OpenAI&#30340;Whisper&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102;&#24403;&#21069;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in NLP largely increased the presence of ASR systems in our daily lives. However, for many low-resource languages, ASR models still need to be improved due in part to the difficulty of acquiring pertinent data. This project aims to help advance research in ASR models for Swiss German dialects, by providing insights about the performance of state-of-the-art ASR models on recently published Swiss German speech datasets. We propose a novel loss that takes into account the semantic distance between the predicted and the ground-truth labels. We outperform current state-of-the-art results by fine-tuning OpenAI's Whisper model on Swiss-German datasets.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;OLISIA&#65292;&#19968;&#20010;&#21475;&#35821;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#32423;&#32852;&#31995;&#32479;&#65292;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;DST&#27169;&#22411;&#65292;&#37319;&#29992;&#20960;&#20010;&#36866;&#24212;&#24615;&#31574;&#30053;&#26469;&#25552;&#39640;&#31283;&#20581;&#24615;&#65292;&#24182;&#22312;DSTC11 Track3&#20013;&#21462;&#24471;&#31532;&#19968;&#21517;&#30340;&#22909;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2304.11073</link><description>&lt;p&gt;
OLISIA: &#19968;&#20010;&#29992;&#20110;&#21475;&#35821;&#21270;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#32423;&#32852;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
OLISIA: a Cascade System for Spoken Dialogue State Tracking. (arXiv:2304.11073v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11073
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;OLISIA&#65292;&#19968;&#20010;&#21475;&#35821;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#32423;&#32852;&#31995;&#32479;&#65292;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;DST&#27169;&#22411;&#65292;&#37319;&#29992;&#20960;&#20010;&#36866;&#24212;&#24615;&#31574;&#30053;&#26469;&#25552;&#39640;&#31283;&#20581;&#24615;&#65292;&#24182;&#22312;DSTC11 Track3&#20013;&#21462;&#24471;&#31532;&#19968;&#21517;&#30340;&#22909;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394; (DST) &#26159;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20851;&#20110;&#35813;&#20219;&#21153;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#20110;&#32842;&#22825;&#26102;&#30340;&#35821;&#26009;&#24211;&#65292;&#24573;&#30053;&#20102;&#21475;&#35821;&#21644;&#20070;&#38754;&#35821;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; OLISIA&#65292;&#36825;&#26159;&#19968;&#20010;&#32423;&#32852;&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035; (ASR) &#27169;&#22411;&#21644; DST &#27169;&#22411;&#12290;&#25105;&#20204;&#22312; ASR &#21644; DST &#27169;&#22359;&#20013;&#24341;&#20837;&#20102;&#20960;&#20010;&#36866;&#24212;&#24615;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#23545;&#21475;&#35821;&#23545;&#35805;&#30340;&#25972;&#21512;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#32463;&#36807;&#36825;&#20123;&#31574;&#30053;&#30340;&#35843;&#25972;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312; DSTC11 Track 3 &#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#21475;&#35821; DST &#24615;&#33021;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#32467;&#26524;&#20998;&#26512;&#65292;&#21457;&#29616;&#35268;&#33539;&#21270; ASR &#30340;&#36755;&#20986;&#21644;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#35843;&#25972; DST &#30340;&#36755;&#20837;&#65292;&#20197;&#21450;&#22686;&#21152;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#37117;&#22312;&#38477;&#20302;&#20070;&#38754;&#21644;&#21475;&#35821;&#23545;&#35805;&#20043;&#38388;&#24615;&#33021;&#24046;&#24322;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though Dialogue State Tracking (DST) is a core component of spoken dialogue systems, recent work on this task mostly deals with chat corpora, disregarding the discrepancies between spoken and written language.In this paper, we propose OLISIA, a cascade system which integrates an Automatic Speech Recognition (ASR) model and a DST model. We introduce several adaptations in the ASR and DST modules to improve integration and robustness to spoken conversations.With these adaptations, our system ranked first in DSTC11 Track 3, a benchmark to evaluate spoken DST. We conduct an in-depth analysis of the results and find that normalizing the ASR outputs and adapting the DST inputs through data augmentation, along with increasing the pre-trained models size all play an important role in reducing the performance discrepancy between written and spoken conversations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#30340;&#30740;&#31350;&#20102;&#29616;&#26377;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#20110;&#25903;&#25345;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#25152;&#25552;&#20379;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2304.11065</link><description>&lt;p&gt;
&#23545;&#35805;&#36807;&#31243;&#24314;&#27169;&#65306;&#29616;&#29366;&#12289;&#24212;&#29992;&#21644;&#23454;&#36341;&#24433;&#21709;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Conversational Process Modelling: State of the Art, Applications, and Implications in Practice. (arXiv:2304.11065v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#30340;&#30740;&#31350;&#20102;&#29616;&#26377;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#20110;&#25903;&#25345;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#25152;&#25552;&#20379;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;Chatbots&#31561;&#32842;&#22825;&#26426;&#22120;&#20154;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#23545;&#20110;BPM&#24212;&#29992;&#26469;&#35828;&#65292;&#22914;&#20309;&#24212;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#26469;&#29983;&#25104;&#21830;&#19994;&#20215;&#20540;&#36890;&#24120;&#26159;&#19981;&#26126;&#30830;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#20998;&#26512;&#29616;&#26377;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#20110;&#25903;&#25345;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#20316;&#20026;&#38754;&#21521;&#27969;&#31243;&#30340;&#33021;&#21147;&#30340;&#25903;&#25345;&#12290;&#35813;&#30740;&#31350;&#35782;&#21035;&#20102;&#27839;&#27969;&#31243;&#29983;&#21629;&#21608;&#26399;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#12290;&#24471;&#20986;&#30340;&#20998;&#31867;&#23398;&#29992;&#20316;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#30340;&#24212;&#29992;&#22330;&#26223;&#30340;&#35782;&#21035;&#65292;&#21253;&#25324;&#27969;&#31243;&#25551;&#36848;&#30340;&#37322;&#20041;&#21644;&#25913;&#36827;&#12290;&#24212;&#29992;&#22330;&#26223;&#22522;&#20110;&#39640;&#31561;&#25945;&#32946;&#39046;&#22495;&#30340;&#23454;&#38469;&#27979;&#35797;&#38598;&#23545;&#29616;&#26377;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#35780;&#20272;&#12290;&#35813;&#27979;&#35797;&#38598;&#21253;&#21547;&#27969;&#31243;&#25551;&#36848;&#21450;&#20854;&#23545;&#24212;&#30340;&#27969;&#31243;&#27169;&#22411;&#65292;&#20197;&#21450;&#27169;&#22411;&#36136;&#37327;&#30340;&#35780;&#20272;&#12290;&#22522;&#20110;&#25991;&#29486;&#21644;&#24212;&#29992;&#22330;&#26223;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#22312;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#20013;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots such as ChatGPT have caused a tremendous hype lately. For BPM applications, it is often not clear how to apply chatbots to generate business value. Hence, this work aims at the systematic analysis of existing chatbots for their support of conversational process modelling as process-oriented capability. Application scenarios are identified along the process life cycle. Then a systematic literature review on conversational process modelling is performed. The resulting taxonomy serves as input for the identification of application scenarios for conversational process modelling, including paraphrasing and improvement of process descriptions. The application scenarios are evaluated for existing chatbots based on a real-world test set from the higher education domain. It contains process descriptions as well as corresponding process models, together with an assessment of the model quality. Based on the literature and application scenario analyses, recommendations for the usage (prac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#35821;&#35328;&#25512;&#29702;&#19982;&#21160;&#20316;&#32479;&#19968;&#22312;&#21333;&#20010;&#31574;&#30053;&#20013;&#65292;&#21033;&#29992; transformer &#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20132;&#26367;&#20351;&#29992;&#21160;&#20316;&#30340;&#25991;&#26412;&#26631;&#39064;&#12290;&#22312; BabyAI &#20219;&#21153;&#20013;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#25512;&#29702;&#31574;&#30053;&#22987;&#32456;&#20248;&#20110;&#27809;&#26377;&#26631;&#39064;&#30340;&#22522;&#20934;&#32447;&#12290;</title><link>http://arxiv.org/abs/2304.11063</link><description>&lt;p&gt;
&#24910;&#24605;&#20043;&#21518;&#20877;&#34892;&#21160;&#65306;&#23558;&#35821;&#35328;&#25512;&#29702;&#19982;&#21160;&#20316;&#32479;&#19968;&#30340;&#20132;&#38169;&#31574;&#30053;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Think Before You Act: Unified Policy for Interleaving Language Reasoning with Actions. (arXiv:2304.11063v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#35821;&#35328;&#25512;&#29702;&#19982;&#21160;&#20316;&#32479;&#19968;&#22312;&#21333;&#20010;&#31574;&#30053;&#20013;&#65292;&#21033;&#29992; transformer &#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20132;&#26367;&#20351;&#29992;&#21160;&#20316;&#30340;&#25991;&#26412;&#26631;&#39064;&#12290;&#22312; BabyAI &#20219;&#21153;&#20013;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#25512;&#29702;&#31574;&#30053;&#22987;&#32456;&#20248;&#20110;&#27809;&#26377;&#26631;&#39064;&#30340;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#30340; transformer &#27169;&#22411;&#30340;&#25104;&#21151;&#35757;&#32451;&#20026;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#24102;&#26469;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26426;&#20250;&#65292;Decision Transformer &#26159;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#36808;&#20986;&#30340;&#19968;&#27493;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#31163;&#32447;&#25968;&#25454;&#19978;&#35757;&#32451;&#31867;&#20284;&#30340;&#19979;&#19968;&#27493;&#39044;&#27979;&#30446;&#26631;&#30340; transformers&#65307;&#32780;&#36825;&#20010;&#39046;&#22495;&#30340;&#21478;&#19968;&#20010;&#37325;&#35201;&#21457;&#23637;&#26159;&#36817;&#26399;&#20986;&#29616;&#20102;&#20174;&#20114;&#32852;&#32593;&#25910;&#38598;&#32780;&#26469;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#30001;&#35270;&#39057;&#25945;&#31243;&#21644;&#23383;&#24149;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#30340;&#20154;&#20204;&#35762;&#36848;&#20182;&#20204;&#27491;&#22312;&#20570;&#30340;&#20107;&#24773;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#31181;&#35821;&#35328;&#32452;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35821;&#35328;&#25512;&#29702;&#21644;&#21160;&#20316;&#32479;&#19968;&#22312;&#21333;&#20010;&#31574;&#30053;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#19968;&#20010;&#24102;&#26377;&#21333;&#35789;&#36755;&#20986;&#30340; transformer &#31574;&#30053;&#65292;&#20197;&#20415;&#23427;&#21487;&#20197;&#29983;&#25104;&#20132;&#26367;&#20351;&#29992;&#21160;&#20316;&#30340;&#25991;&#26412;&#26631;&#39064;&#12290;&#22312;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340; BabyAI &#20219;&#21153;&#20013;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#25512;&#29702;&#31574;&#30053;&#22312;&#25551;&#36848;&#19979;&#19968;&#20010;&#23376;&#30446;&#26631;&#30340;&#26631;&#39064;&#19978;&#65292;&#22987;&#32456;&#20248;&#20110;&#27809;&#26377;&#26631;&#39064;&#30340;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of transformer models trained with a language modeling objective brings a promising opportunity to the reinforcement learning framework. Decision Transformer is a step towards this direction, showing how to train transformers with a similar next-step prediction objective on offline data. Another important development in this area is the recent emergence of large-scale datasets collected from the internet, such as the ones composed of tutorial videos with captions where people talk about what they are doing. To take advantage of this language component, we propose a novel method for unifying language reasoning with actions in a single policy. Specifically, we augment a transformer policy with word outputs, so it can generate textual captions interleaved with actions. When tested on the most challenging task in BabyAI, with captions describing next subgoals, our reasoning policy consistently outperforms the caption-free baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;&#25193;&#23637;BERT&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25193;&#23637;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;200&#19975;&#20010;&#26631;&#35760;&#65292;&#26377;&#26395;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#24182;&#20026;&#20869;&#23384;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.11062</link><description>&lt;p&gt;
&#21033;&#29992;RMT&#23558;Transformer&#25193;&#23637;&#21040;100&#19975;&#20010;&#26631;&#35760;&#21450;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling Transformer to 1M tokens and beyond with RMT. (arXiv:2304.11062v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;&#25193;&#23637;BERT&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25193;&#23637;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;200&#19975;&#20010;&#26631;&#35760;&#65292;&#26377;&#26395;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#24182;&#20026;&#20869;&#23384;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;&#25193;&#23637;BERT&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;BERT&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#26377;&#25928;&#30340;&#22522;&#20110;Transformer&#27169;&#22411;&#20043;&#19968;&#12290;&#36890;&#36807;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;Transformer&#26550;&#26500;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#27169;&#22411;&#30340;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#22686;&#21152;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;200&#19975;&#20010;&#26631;&#35760;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#30340;&#20869;&#23384;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#23384;&#20648;&#21644;&#22788;&#29702;&#26412;&#22320;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#23454;&#29616;&#36755;&#20837;&#24207;&#21015;&#21508;&#37096;&#20998;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#28508;&#21147;&#26469;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#65292;&#24182;&#33021;&#22815;&#20026;&#20869;&#23384;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report presents the application of a recurrent memory to extend the context length of BERT, one of the most effective Transformer-based models in natural language processing. By leveraging the Recurrent Memory Transformer architecture, we have successfully increased the model's effective context length to an unprecedented two million tokens, while maintaining high memory retrieval accuracy. Our method allows for the storage and processing of both local and global information and enables information flow between segments of the input sequence through the use of recurrence. Our experiments demonstrate the effectiveness of our approach, which holds significant potential to enhance long-term dependency handling in natural language understanding and generation tasks as well as enable large-scale context processing for memory-intensive applications.
&lt;/p&gt;</description></item><item><title>CEIL&#26159;&#19968;&#31181;&#36845;&#20195;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#31867;&#30446;&#26631;&#26469;&#25913;&#36827;&#29305;&#24449;&#34920;&#31034;&#21644;&#25991;&#26412;&#32858;&#31867;&#25928;&#26524;&#65292;&#33021;&#22815;&#26222;&#36866;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#25991;&#26412;&#32858;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.11061</link><description>&lt;p&gt;
CEIL&#65306;&#19968;&#31181;&#36890;&#29992;&#30340;&#20998;&#31867;&#22686;&#24378;&#36845;&#20195;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#25991;&#26412;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
CEIL: A General Classification-Enhanced Iterative Learning Framework for Text Clustering. (arXiv:2304.11061v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11061
&lt;/p&gt;
&lt;p&gt;
CEIL&#26159;&#19968;&#31181;&#36845;&#20195;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#31867;&#30446;&#26631;&#26469;&#25913;&#36827;&#29305;&#24449;&#34920;&#31034;&#21644;&#25991;&#26412;&#32858;&#31867;&#25928;&#26524;&#65292;&#33021;&#22815;&#26222;&#36866;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#25991;&#26412;&#32858;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#22522;&#26412;&#30340;&#25361;&#25112;&#20043;&#19968;&#65292;&#25991;&#26412;&#32858;&#31867;&#26088;&#22312;&#23558;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#25991;&#26412;&#27573;&#20998;&#32452;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#27880;&#37322;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#28145;&#24230;&#32858;&#31867;&#22312;&#20256;&#32479;&#32858;&#31867;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#28145;&#24230;&#25991;&#26412;&#32858;&#31867;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#22312;&#19968;&#33324;&#39046;&#22495;&#39044;&#35757;&#32451;&#30340;&#34920;&#31034;&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#22312;&#29305;&#23450;&#30446;&#26631;&#39046;&#22495;&#32858;&#31867;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CEIL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#30701;&#25991;&#26412;&#32858;&#31867;&#20998;&#31867;&#22686;&#24378;&#36845;&#20195;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#20998;&#31867;&#30446;&#26631;&#24341;&#20837;&#36845;&#20195;&#22320;&#25913;&#36827;&#29305;&#24449;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#26222;&#36941;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;&#22312;&#27599;&#20010;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#26816;&#32034;&#21021;&#22987;&#25991;&#26412;&#34920;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#20998;&#31867;&#19981;&#21487;&#20998;&#21106;&#20998;&#24067;&#65288;CDC&#65289;&#27169;&#22359;&#25910;&#38598;&#32858;&#31867;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20998;&#31867;&#27169;&#22359;&#23558;&#20998;&#31867;&#30446;&#26631;&#25972;&#21512;&#21040;&#32858;&#31867;&#26694;&#26550;&#20013;&#65292;&#24182;&#20351;&#29992;&#24471;&#21040;&#30340;&#25913;&#36827;&#29305;&#24449;&#34920;&#31034;&#22312;&#19979;&#19968;&#27425;&#36845;&#20195;&#20013;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;CEIL&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20854;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text clustering, as one of the most fundamental challenges in unsupervised learning, aims at grouping semantically similar text segments without relying on human annotations. With the rapid development of deep learning, deep clustering has achieved significant advantages over traditional clustering methods. Despite the effectiveness, most existing deep text clustering methods rely heavily on representations pre-trained in general domains, which may not be the most suitable solution for clustering in specific target domains. To address this issue, we propose CEIL, a novel Classification-Enhanced Iterative Learning framework for short text clustering, which aims at generally promoting the clustering performance by introducing a classification objective to iteratively improve feature representations. In each iteration, we first adopt a language model to retrieve the initial text representations, from which the clustering results are collected using our proposed Category Disentangled Contr
&lt;/p&gt;</description></item><item><title>SkillGPT&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25216;&#33021;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;&#30340;RESTful API&#26381;&#21153;&#65292;&#36890;&#36807;&#25688;&#35201;&#21644;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#24179;&#34913;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.11060</link><description>&lt;p&gt;
SkillGPT: &#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25216;&#33021;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;&#30340;RESTful API&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
SkillGPT: a RESTful API service for skill extraction and standardization using a Large Language Model. (arXiv:2304.11060v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11060
&lt;/p&gt;
&lt;p&gt;
SkillGPT&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25216;&#33021;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;&#30340;RESTful API&#26381;&#21153;&#65292;&#36890;&#36807;&#25688;&#35201;&#21644;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#24179;&#34913;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; SkillGPT&#65292;&#19968;&#31181;&#21033;&#29992;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#36827;&#34892;&#20174;&#33258;&#30001;&#39118;&#26684;&#32844;&#20301;&#25551;&#36848;&#21644;&#29992;&#25143;&#36164;&#26009;&#20013;&#36827;&#34892;&#25216;&#33021;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270; (SES) &#30340;&#24037;&#20855;&#12290;&#19982;&#22823;&#22810;&#25968;&#31867;&#20284;&#20219;&#21153;&#30340;&#20197;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;SkillGPT &#30452;&#25509;&#20351;&#29992;&#26368;&#26032;&#30340;&#23545;&#35805; LLM &#36827;&#34892;&#26631;&#20934;&#25216;&#33021;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#25688;&#35201;&#21644;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#26469;&#24179;&#34913;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#20813;&#36153; SkillGPT &#35753;&#29992;&#25143;&#33021;&#22815;&#39640;&#25928;&#21487;&#38752;&#22320;&#36827;&#34892;&#23545;&#35805;&#22411; SES&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SkillGPT, a tool for skill extraction and standardization (SES) from free-style job descriptions and user profiles with an open-source Large Language Model (LLM) as backbone. Most previous methods for similar tasks either need supervision or rely on heavy data-preprocessing and feature engineering. Directly prompting the latest conversational LLM for standard skills, however, is slow, costly and inaccurate. In contrast, SkillGPT utilizes a LLM to perform its tasks in steps via summarization and vector similarity search, to balance speed with precision. The backbone LLM of SkillGPT is based on Llama, free for academic use and thus useful for exploratory research and prototype development. Hence, our cost-free SkillGPT gives users the convenience of conversational SES, efficiently and reliably.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NIDAL&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#19981;&#21516;&#35821;&#35328;&#20013;&#20986;&#29616;&#30340;&#26032;&#22411;&#24847;&#22270;&#31867;&#21035;&#24182;&#20943;&#23569;&#20154;&#21592;&#26631;&#27880;&#25104;&#26412;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#29575;&#21644;&#23439;F1&#20540;&#12290;</title><link>http://arxiv.org/abs/2304.11058</link><description>&lt;p&gt;
&#26032;&#22411;&#24847;&#22270;&#26816;&#27979;&#21644;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#20998;&#31867;&#65288;&#23398;&#29983;&#25688;&#35201;&#65289;
&lt;/p&gt;
&lt;p&gt;
Novel Intent Detection and Active Learning Based Classification (Student Abstract). (arXiv:2304.11058v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NIDAL&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#19981;&#21516;&#35821;&#35328;&#20013;&#20986;&#29616;&#30340;&#26032;&#22411;&#24847;&#22270;&#31867;&#21035;&#24182;&#20943;&#23569;&#20154;&#21592;&#26631;&#27880;&#25104;&#26412;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#29575;&#21644;&#23439;F1&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#20132;&#20114;&#30340;&#23545;&#35805;&#20195;&#29702;&#24773;&#22659;&#20013;&#65292;&#26032;&#22411;&#24847;&#22270;&#31867;&#21035;&#26816;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#24050;&#32463;&#36827;&#34892;&#20102;&#35768;&#22810;&#30740;&#31350;&#24037;&#20316;&#65292;&#20197;&#26816;&#27979;&#33521;&#35821;&#20026;&#20027;&#35201;&#25991;&#26412;&#21644;&#22270;&#20687;&#20013;&#30340;&#26032;&#22411;&#24847;&#22270;&#12290;&#20294;&#26159;&#65292;&#24403;&#21069;&#31995;&#32479;&#32570;&#20047;&#19968;&#31181;&#31471;&#21040;&#31471;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#22312;&#21508;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#21516;&#26102;&#26816;&#27979;&#26032;&#22411;&#24847;&#22270;&#65292;&#21516;&#26102;&#20943;&#23569;&#23545;&#20154;&#31867;&#27880;&#37322;&#30340;&#38656;&#27714;&#20197;&#22788;&#29702;&#34987;&#20998;&#31867;&#38169;&#35823;&#25110;&#31995;&#32479;&#25298;&#32477;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;NIDAL&#65288;Novel Intent Detection and Active Learning based classification&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#26032;&#22411;&#24847;&#22270;&#24182;&#20943;&#23569;&#20154;&#31867;&#27880;&#37322;&#25104;&#26412;&#12290;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21644;&#23439;F1&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#25552;&#39640;&#20102;10%&#20197;&#19978;&#65292;&#19988;&#24635;&#27880;&#37322;&#25104;&#26412;&#20165;&#20026;&#31995;&#32479;&#21487;&#29992;&#26410;&#26631;&#27880;&#25968;&#25454;&#30340;6-10&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Novel intent class detection is an important problem in real world scenario for conversational agents for continuous interaction. Several research works have been done to detect novel intents in a mono-lingual (primarily English) texts and images. But, current systems lack an end-to-end universal framework to detect novel intents across various different languages with less human annotation effort for mis-classified and system rejected samples. This paper proposes NIDAL (Novel Intent Detection and Active Learning based classification), a semi-supervised framework to detect novel intents while reducing human annotation cost. Empirical results on various benchmark datasets demonstrate that this system outperforms the baseline methods by more than 10% margin for accuracy and macro-F1. The system achieves this while maintaining overall annotation cost to be just ~6-10% of the unlabeled data available to the system.
&lt;/p&gt;</description></item><item><title>&#27604;&#36739;&#19977;&#31181;&#21322;&#30417;&#30563;ASR&#26041;&#27861;&#30340;&#20248;&#21155;&#65292;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#22312;&#21407;&#22987;WER&#26041;&#38754;&#26377;&#25152;&#25552;&#39640;&#65292;&#32780;&#19988;&#22312;&#23614;&#37096;&#21333;&#35789;WER&#12289;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#35299;&#30721;&#22120;&#35745;&#31639;&#21644;&#26629;&#26684;&#23494;&#24230;&#31561;&#26041;&#38754;&#37117;&#26377;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.11053</link><description>&lt;p&gt;
&#19968;&#31181;&#22823;&#35268;&#27169;&#35821;&#38899;&#35782;&#21035;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Comparison of Semi-Supervised Learning Techniques for Streaming ASR at Scale. (arXiv:2304.11053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11053
&lt;/p&gt;
&lt;p&gt;
&#27604;&#36739;&#19977;&#31181;&#21322;&#30417;&#30563;ASR&#26041;&#27861;&#30340;&#20248;&#21155;&#65292;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#22312;&#21407;&#22987;WER&#26041;&#38754;&#26377;&#25152;&#25552;&#39640;&#65292;&#32780;&#19988;&#22312;&#23614;&#37096;&#21333;&#35789;WER&#12289;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#35299;&#30721;&#22120;&#35745;&#31639;&#21644;&#26629;&#26684;&#23494;&#24230;&#31561;&#26041;&#38754;&#37117;&#26377;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32570;&#20047;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#37197;&#23545;&#25991;&#26412;&#21644;&#38899;&#39057;&#27880;&#20837;&#24050;&#25104;&#20026;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35757;&#32451;&#26377;&#38750;&#24120;&#22823;&#30340;&#21463;&#30417;&#30563;&#35821;&#26009;&#24211;&#24182;&#19988;&#26377;&#19968;&#20123;&#29616;&#23454;&#30340;&#35201;&#27714;&#65288;&#22914;&#27169;&#22411;&#22823;&#23567;&#21644;CPU&#39044;&#31639;&#12289;&#27969;&#23186;&#20307;&#33021;&#21147;&#20197;&#21450;&#29992;&#20110;&#37325;&#26032;&#35780;&#20998;&#21644;&#19979;&#28216;NLU&#20219;&#21153;&#30340;&#20016;&#23500;&#26629;&#26684;&#65289;&#30340;&#20135;&#21697;ASR&#31995;&#32479;&#65292;&#32570;&#20047;&#26377;&#25928;&#30340;&#37096;&#32626;&#25351;&#21335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#32852;&#21512;&#35757;&#32451;&#30340;&#25511;&#21046;&#29615;&#22659;&#20013;&#27604;&#36739;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#21253;&#25324;&#26080;&#37197;&#23545;&#25991;&#26412;&#21644;&#38899;&#39057;&#20197;&#21450;&#20960;&#31181;&#23427;&#20204;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#38500;&#20102;&#21407;&#22987;WER&#20043;&#22806;&#36824;&#25552;&#20379;&#20102;&#35768;&#22810;&#25913;&#36827;&#65292;&#21253;&#25324;&#23614;&#37096;&#21333;&#35789;WER&#30340;&#22823;&#24133;&#24230;&#22686;&#30410;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#35299;&#30721;&#22120;&#35745;&#31639;&#21644;&#26629;&#26684;&#23494;&#24230;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unpaired text and audio injection have emerged as dominant methods for improving ASR performance in the absence of a large labeled corpus. However, little guidance exists on deploying these methods to improve production ASR systems that are trained on very large supervised corpora and with realistic requirements like a constrained model size and CPU budget, streaming capability, and a rich lattice for rescoring and for downstream NLU tasks. In this work, we compare three state-of-the-art semi-supervised methods encompassing both unpaired text and audio as well as several of their combinations in a controlled setting using joint training. We find that in our setting these methods offer many improvements beyond raw WER, including substantial gains in tail-word WER, decoder computation during inference, and lattice density.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#31038;&#20132;&#21270;&#20154;&#24418;&#26234;&#33021;&#31995;&#32479;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20154;&#31867;&#22768;&#38899;&#30340;&#24773;&#24863;&#35821;&#20041;&#65292;&#23454;&#29616;&#31867;&#20154;&#23545;&#35805;&#12290;</title><link>http://arxiv.org/abs/2304.11046</link><description>&lt;p&gt;
&#24773;&#24863;&#31038;&#20132;&#21270;&#20154;&#24418;&#26234;&#33021;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Affective social anthropomorphic intelligent system. (arXiv:2304.11046v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#31038;&#20132;&#21270;&#20154;&#24418;&#26234;&#33021;&#31995;&#32479;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20154;&#31867;&#22768;&#38899;&#30340;&#24773;&#24863;&#35821;&#20041;&#65292;&#23454;&#29616;&#31867;&#20154;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30340;&#23545;&#35805;&#39118;&#26684;&#21487;&#20197;&#36890;&#36807;&#24189;&#40664;&#24863;&#12289;&#20010;&#24615;&#21644;&#35821;&#35843;&#26469;&#34913;&#37327;&#12290;&#36825;&#20123;&#29305;&#24449;&#24050;&#32463;&#25104;&#20026;&#23545;&#35805;&#26234;&#33021;&#34394;&#25311;&#21161;&#25163;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26234;&#33021;&#34394;&#25311;&#21161;&#25163;&#65288;IVAs&#65289;&#26080;&#27861;&#35299;&#37322;&#20154;&#31867;&#22768;&#38899;&#30340;&#24773;&#24863;&#35821;&#20041;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#24418;&#26234;&#33021;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#34920;&#36798;&#24773;&#24863;&#21644;&#20010;&#24615;&#26469;&#36827;&#34892;&#31867;&#20154;&#23545;&#35805;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#38899;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#29305;&#23450;&#24773;&#24863;&#30340;&#23646;&#24615;&#26144;&#23556;&#20986;&#26469;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#38899;&#39057;&#27874;&#24418;&#25968;&#25454;&#36716;&#25442;&#20026;&#39057;&#22495;&#25968;&#25454;&#65288;Mel-Spectrogram&#65289;&#65292;&#21019;&#24314;&#20102;&#31163;&#25955;&#30340;&#38899;&#39057;&#29305;&#24449;&#27169;&#24335;&#65292;&#22914;&#38899;&#31526;&#12289;&#38899;&#35843;&#12289;&#33410;&#22863;&#12289;&#26059;&#24459;&#31561;&#31561;&#12290;&#24182;&#20351;&#29992;&#19968;&#20010;&#22806;&#37096;CNN-Transformer-Encoder&#27169;&#22411;&#26469;&#39044;&#27979;&#22768;&#38899;&#20013;&#30340;&#19971;&#31181;&#19981;&#21516;&#30340;&#24773;&#24863;&#29366;&#24577;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#23558;&#35821;&#38899;&#36755;&#20837;&#21040;&#19968;&#20010;RNN&#27169;&#22411;&#65288;Deep-speech&#65289;&#20013;&#65292;&#29983;&#25104;&#23545;&#38899;&#39057;&#30340;&#25991;&#26412;&#36716;&#24405;&#12290;&#28982;&#21518;&#65292;&#36716;&#24405;&#25991;&#26412;&#23558;&#36890;&#36807;&#19968;&#20010;transformer&#35299;&#30721;&#22120;&#19982;&#30456;&#24212;&#24773;&#24863;&#30340;&#21709;&#24212;&#19968;&#36215;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human conversational styles are measured by the sense of humor, personality, and tone of voice. These characteristics have become essential for conversational intelligent virtual assistants. However, most of the state-of-the-art intelligent virtual assistants (IVAs) are failed to interpret the affective semantics of human voices. This research proposes an anthropomorphic intelligent system that can hold a proper human-like conversation with emotion and personality. A voice style transfer method is also proposed to map the attributes of a specific emotion. Initially, the frequency domain data (Mel-Spectrogram) is created by converting the temporal audio wave data, which comprises discrete patterns for audio features such as notes, pitch, rhythm, and melody. A collateral CNN-Transformer-Encoder is used to predict seven different affective states from voice. The voice is also fed parallelly to the deep-speech, an RNN model that generates the text transcription from the spectrogram. Then t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;MATLAB&#20989;&#25968;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#22768;&#38899;&#30340;&#29305;&#24449;&#26469;&#35782;&#21035;&#20154;&#31867;&#19981;&#21516;&#30340;&#24773;&#24863;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.11040</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#21475;&#35821;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Emotional Expression Detection in Spoken Language Employing Machine Learning Algorithms. (arXiv:2304.11040v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11040
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;MATLAB&#20989;&#25968;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#22768;&#38899;&#30340;&#29305;&#24449;&#26469;&#35782;&#21035;&#20154;&#31867;&#19981;&#21516;&#30340;&#24773;&#24863;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30340;&#22768;&#38899;&#20855;&#26377;&#22810;&#31181;&#29305;&#24449;&#65292;&#22914;&#38899;&#39640;&#12289;&#38899;&#33394;&#12289;&#38899;&#37327;&#21644;&#21971;&#38899;&#12290;&#36890;&#36807;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#35266;&#23519;&#21040;&#20154;&#20204;&#22312;&#35828;&#35805;&#26102;&#20351;&#29992;&#19981;&#21516;&#30340;&#21971;&#38899;&#36136;&#37327;&#26469;&#34920;&#36798;&#24773;&#24863;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20351;&#29992;&#22810;&#20010;MATLAB&#20989;&#25968;&#65292;&#22914;&#35889;&#25551;&#36848;&#31526;&#12289;&#21608;&#26399;&#24615;&#21644;&#35856;&#27874;&#65292;&#35782;&#21035;&#20154;&#31867;&#30340;&#19981;&#21516;&#24773;&#24863;&#65292;&#22914;&#24868;&#24594;&#12289;&#24754;&#20260;&#12289;&#24656;&#24807;&#12289;&#20013;&#31435;&#12289;&#21388;&#24694;&#12289;&#24778;&#21916;&#21644;&#24555;&#20048;&#12290;&#20026;&#20102;&#23436;&#25104;&#36825;&#39033;&#24037;&#20316;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20154;&#31867;&#35821;&#38899;&#30340;CREMA-D&#65288;&#20247;&#21253;&#24773;&#24863;&#22810;&#27169;&#24577;&#28436;&#21592;&#25968;&#25454;&#65289;&#21644;TESS&#65288;&#22810;&#20262;&#22810;&#24773;&#24863;&#35821;&#38899;&#38598;&#65289;&#25968;&#25454;&#38598;&#12290;&#38899;&#39057;&#25991;&#20214;&#21253;&#21547;&#20855;&#26377;&#21508;&#31181;&#29305;&#24449;&#65288;&#22914;&#22024;&#26434;&#12289;&#24555;&#36895;&#12289;&#32531;&#24930;&#65289;&#30340;&#25968;&#25454;&#65292;&#22240;&#27492;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;&#21033;&#29992;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#65288;EMD&#65289;&#36827;&#34892;&#20449;&#21495;&#20998;&#35299;&#30340;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#20960;&#20010;
&lt;/p&gt;
&lt;p&gt;
There are a variety of features of the human voice that can be classified as pitch, timbre, loudness, and vocal tone. It is observed in numerous incidents that human expresses their feelings using different vocal qualities when they are speaking. The primary objective of this research is to recognize different emotions of human beings such as anger, sadness, fear, neutrality, disgust, pleasant surprise, and happiness by using several MATLAB functions namely, spectral descriptors, periodicity, and harmonicity. To accomplish the work, we analyze the CREMA-D (Crowd-sourced Emotional Multimodal Actors Data) &amp; TESS (Toronto Emotional Speech Set) datasets of human speech. The audio file contains data that have various characteristics (e.g., noisy, speedy, slow) thereby the efficiency of the ML (Machine Learning) models increases significantly. The EMD (Empirical Mode Decomposition) is utilized for the process of signal decomposition. Then, the features are extracted through the use of severa
&lt;/p&gt;</description></item><item><title>DIN-SQL&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#24182;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#39304;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#65292;&#20351;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2304.11015</link><description>&lt;p&gt;
DIN-SQL: &#33258;&#32416;&#27491;&#30340;&#25991;&#26412;&#21040;SQL&#20998;&#35299;&#24335;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction. (arXiv:2304.11015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11015
&lt;/p&gt;
&lt;p&gt;
DIN-SQL&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#24182;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#39304;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#65292;&#20351;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22797;&#26434;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#31181;&#20998;&#35299;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23613;&#31649;SQL&#26597;&#35810;&#20855;&#26377;&#22768;&#26126;&#24335;&#32467;&#26500;&#65292;&#20294;&#21487;&#20197;&#23558;&#20854;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#24182;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#39304;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#23427;&#20204;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#31283;&#23450;&#25552;&#39640;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#22823;&#32422;&#25552;&#39640;&#20102;10&#65285;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25512;&#21521;&#26368;&#26032;&#27700;&#24179;&#65292;&#24182;&#22312;Holdout Spider&#25968;&#25454;&#38598;&#19978;&#29978;&#33267;&#36229;&#36807;&#20102;&#32463;&#36807;&#31934;&#35843;&#30340;&#22823;&#22411;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of decomposing a complex text-to-sql task into smaller sub-tasks and how such a decomposition can significantly improve the performance of Large Language Models (LLMs) in the reasoning process. There is currently a significant gap between the performance of fine-tuned models and prompting approaches using LLMs on challenging text-to-sql datasets such as Spider. We show that SQL queries, despite their declarative structure, can be broken down into sub-problems and the solutions of those sub-problems can be fed into LLMs to significantly improve their performance. Our experiments with three LLMs show that this approach consistently improves their performance by roughly 10%, pushing the accuracy of LLMs towards state-of-the-art, and even beating large fine-tuned models on the holdout Spider dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#21644;CRF&#25216;&#26415;&#30340;&#29983;&#29289;&#21307;&#23398;&#20020;&#24202;&#31508;&#35760;&#30693;&#35782;&#25552;&#21462;&#21644;&#20998;&#26512;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#21644;&#20998;&#26512;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#12290;</title><link>http://arxiv.org/abs/2304.10996</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#30340;&#20020;&#24202;&#30693;&#35782;&#25552;&#21462;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
BERT Based Clinical Knowledge Extraction for Biomedical Knowledge Graph Construction and Analysis. (arXiv:2304.10996v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#21644;CRF&#25216;&#26415;&#30340;&#29983;&#29289;&#21307;&#23398;&#20020;&#24202;&#31508;&#35760;&#30693;&#35782;&#25552;&#21462;&#21644;&#20998;&#26512;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#21644;&#20998;&#26512;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#30693;&#35782;&#38543;&#26102;&#38388;&#32780;&#28436;&#21464;&#65292;&#24448;&#24448;&#26159;&#30001;&#20110;&#26032;&#30340;&#21457;&#29616;&#25110;&#25512;&#29702;&#26041;&#27861;&#30340;&#21464;&#21270;&#23548;&#33268;&#12290;&#27492;&#22806;&#65292;&#21487;&#33021;&#20250;&#26377;&#26032;&#30340;&#20107;&#23454;&#25110;&#35777;&#25454;&#20986;&#29616;&#65292;&#23548;&#33268;&#23545;&#22797;&#26434;&#29616;&#35937;&#30340;&#26032;&#29702;&#35299;&#12290;&#36825;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#23588;&#20026;&#30495;&#23454;&#65292;&#31185;&#23398;&#23478;&#21644;&#21307;&#29983;&#19981;&#26029;&#21162;&#21147;&#23547;&#25214;&#26032;&#30340;&#35786;&#26029;&#12289;&#27835;&#30103;&#26041;&#27861;&#21644;&#26368;&#32456;&#30340;&#27835;&#24840;&#12290;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#30495;&#23454;&#30340;&#26041;&#27861;&#26469;&#32452;&#32455;&#21644;&#26816;&#32034;&#22823;&#37327;&#22686;&#38271;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background : Knowledge is evolving over time, often as a result of new discoveries or changes in the adopted methods of reasoning. Also, new facts or evidence may become available, leading to new understandings of complex phenomena. This is particularly true in the biomedical field, where scientists and physicians are constantly striving to find new methods of diagnosis, treatment and eventually cure. Knowledge Graphs (KGs) offer a real way of organizing and retrieving the massive and growing amount of biomedical knowledge.  Objective : We propose an end-to-end approach for knowledge extraction and analysis from biomedical clinical notes using the Bidirectional Encoder Representations from Transformers (BERT) model and Conditional Random Field (CRF) layer.  Methods : The approach is based on knowledge graphs, which can effectively process abstract biomedical concepts such as relationships and interactions between medical entities. Besides offering an intuitive way to visualize these co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20256;&#32479;&#26631;&#35760;&#20998;&#31867;&#21644;&#26032;&#20852;&#38382;&#31572;&#26041;&#27861;&#22312;&#25991;&#29486;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#38382;&#31572;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#20449;&#24687;&#30340;&#25991;&#26723;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.10994</link><description>&lt;p&gt;
&#25991;&#20214;&#20449;&#24687;&#25552;&#21462;&#65306;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#65292;&#38382;&#31572;&#19982;&#26631;&#35760;&#20998;&#31867;&#21738;&#20010;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Information Extraction from Documents: Question Answering vs Token Classification in real-world setups. (arXiv:2304.10994v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20256;&#32479;&#26631;&#35760;&#20998;&#31867;&#21644;&#26032;&#20852;&#38382;&#31572;&#26041;&#27861;&#22312;&#25991;&#29486;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#38382;&#31572;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#20449;&#24687;&#30340;&#25991;&#26723;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20449;&#24687;&#25552;&#21462;&#36890;&#24120;&#34987;&#24403;&#20570;&#26631;&#35760;&#20998;&#31867;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#30340;&#31361;&#30772;&#23548;&#33268;&#20102;&#19968;&#31181;&#21517;&#20026;Question Answering&#65288;&#38382;&#31572;&#65289;&#30340;&#25552;&#21462;&#25991;&#26723;&#20851;&#38190;&#20449;&#24687;&#30340;&#26032;&#23376;&#20219;&#21153;&#30340;&#20986;&#29616;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#38382;&#31572;&#26041;&#27861;&#21644;&#20256;&#32479;&#30340;&#26631;&#35760;&#20998;&#31867;&#26041;&#27861;&#22312;&#25991;&#26723;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#24182;&#35774;&#35745;&#20102;&#20116;&#20010;&#23454;&#39564;&#65292;&#27979;&#35797;&#20854;&#22312;&#21407;&#22987;&#24615;&#33021;&#12289;&#22122;&#22768;&#29615;&#22659;&#19979;&#30340;&#31283;&#20581;&#24615;&#12289;&#25552;&#21462;&#38271;&#23454;&#20307;&#30340;&#33021;&#21147;&#12289;Few-Shot Learning&#65288;&#23569;&#37327;&#25968;&#25454;&#23398;&#20064;&#65289;&#30340;&#24494;&#35843;&#36895;&#24230;&#21644;Zero-Shot Learning&#65288;&#38646;&#25968;&#25454;&#23398;&#20064;&#65289;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22788;&#29702;&#21547;&#26377;&#22797;&#26434;&#20449;&#24687;&#30340;&#25991;&#26723;&#26102;&#65292;&#38382;&#31572;&#26041;&#27861;&#27604;&#26631;&#35760;&#20998;&#31867;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research in Document Intelligence and especially in Document Key Information Extraction (DocKIE) has been mainly solved as Token Classification problem. Recent breakthroughs in both natural language processing (NLP) and computer vision helped building document-focused pre-training methods, leveraging a multimodal understanding of the document text, layout and image modalities. However, these breakthroughs also led to the emergence of a new DocKIE subtask of extractive document Question Answering (DocQA), as part of the Machine Reading Comprehension (MRC) research field. In this work, we compare the Question Answering approach with the classical token classification approach for document key information extraction. We designed experiments to benchmark five different experimental setups : raw performances, robustness to noisy environment, capacity to extract long entities, fine-tuning speed on Few-Shot Learning and finally Zero-Shot Learning. Our research showed that when dealing with cl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEIA&#30340;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#30001;&#36229;&#36807;6&#30334;&#19975;&#20010;&#33258;&#27880;&#37322;&#25991;&#26412;&#24086;&#23376;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#21033;&#29992;&#25513;&#34109;&#21333;&#35789;&#30340;&#26041;&#27861;&#22686;&#24378;&#27169;&#22411;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#24773;&#24863;&#21333;&#35789;&#30340;&#23398;&#20064;&#65292;&#24182;&#22312;&#19977;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#32422;73&#30340;&#23439;F1&#20540;&#65292;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.10973</link><description>&lt;p&gt;
LEIA&#65306;&#35821;&#35328;&#23884;&#20837;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
LEIA: Linguistic Embeddings for the Identification of Affect. (arXiv:2304.10973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10973
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEIA&#30340;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#30001;&#36229;&#36807;6&#30334;&#19975;&#20010;&#33258;&#27880;&#37322;&#25991;&#26412;&#24086;&#23376;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#21033;&#29992;&#25513;&#34109;&#21333;&#35789;&#30340;&#26041;&#27861;&#22686;&#24378;&#27169;&#22411;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#24773;&#24863;&#21333;&#35789;&#30340;&#23398;&#20064;&#65292;&#24182;&#22312;&#19977;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#32422;73&#30340;&#23439;F1&#20540;&#65292;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#20135;&#29983;&#20102;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#65292;&#20351;&#24471;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#24773;&#24863;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#30001;&#35835;&#32773;&#29983;&#25104;&#30340;&#23567;&#22411;&#32780;&#26114;&#36149;&#30340;&#25991;&#26412;&#27880;&#37322;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#35835;&#32773;&#29468;&#27979;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;&#36825;&#24433;&#21709;&#20102;&#24773;&#24863;&#35782;&#21035;&#26041;&#27861;&#30340;&#36136;&#37327;&#65292;&#22240;&#20026;&#23384;&#22312;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#38480;&#21046;&#21644;&#29992;&#20110;&#27169;&#22411;&#24320;&#21457;&#30340;&#26631;&#31614;&#29983;&#20135;&#20013;&#30340;&#22122;&#22768;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LEIA&#65292;&#36825;&#26159;&#19968;&#31181;&#25991;&#26412;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#65292;&#23427;&#22522;&#20110;&#30001;&#36229;&#36807;6&#30334;&#19975;&#20010;&#24086;&#23376;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#36825;&#20123;&#24086;&#23376;&#20855;&#26377;&#33258;&#27880;&#37322;&#30340;&#24773;&#24863;&#26631;&#31614;&#65292;&#21253;&#25324;&#24555;&#20048;&#12289;&#20146;&#24773;&#12289;&#24754;&#20260;&#12289;&#24868;&#24594;&#21644;&#24656;&#24807;&#12290;LEIA&#22522;&#20110;&#19968;&#31181;&#25513;&#34109;&#21333;&#35789;&#30340;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#24773;&#24863;&#21333;&#35789;&#30340;&#23398;&#20064;&#12290;LEIA&#22312;&#19977;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#32422;73&#30340;&#23439;F1&#20540;&#65292;&#20248;&#20110;&#20854;&#20182;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#22312;&#24378;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;LEIA&#21487;&#20197;&#27010;&#25324;&#19981;&#21516;&#30340;&#24086;&#23376;&#12289;&#29992;&#25143;&#21644;&#26102;&#38388;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The wealth of text data generated by social media has enabled new kinds of analysis of emotions with language models. These models are often trained on small and costly datasets of text annotations produced by readers who guess the emotions expressed by others in social media posts. This affects the quality of emotion identification methods due to training data size limitations and noise in the production of labels used in model development. We present LEIA, a model for emotion identification in text that has been trained on a dataset of more than 6 million posts with self-annotated emotion labels for happiness, affection, sadness, anger, and fear. LEIA is based on a word masking method that enhances the learning of emotion words during model pre-training. LEIA achieves macro-F1 values of approximately 73 on three in-domain test datasets, outperforming other supervised and unsupervised methods in a strong benchmark that shows that LEIA generalizes across posts, users, and time periods.
&lt;/p&gt;</description></item><item><title>CancerGPT &#26159;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#22312;&#29983;&#29289;&#23398;&#25512;&#26029;&#20013;&#39044;&#27979;&#32597;&#35265;&#32452;&#32455;&#20013;&#30340;&#33647;&#29289;&#23545;&#21327;&#21516;&#20316;&#29992;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#25216;&#26415;&#20934;&#30830;&#24615;&#39640;&#65292;&#21363;&#20351;&#22312;&#26679;&#26412;&#25968;&#25454;&#38750;&#24120;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20173;&#21487;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.10946</link><description>&lt;p&gt;
CancerGPT: &#22522;&#20110;LLMs&#30340;&#26497;&#23569;&#26679;&#26412;&#33647;&#29289;&#23545;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
CancerGPT: Few-shot Drug Pair Synergy Prediction using Large Pre-trained Language Models. (arXiv:2304.10946v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10946
&lt;/p&gt;
&lt;p&gt;
CancerGPT &#26159;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#22312;&#29983;&#29289;&#23398;&#25512;&#26029;&#20013;&#39044;&#27979;&#32597;&#35265;&#32452;&#32455;&#20013;&#30340;&#33647;&#29289;&#23545;&#21327;&#21516;&#20316;&#29992;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#25216;&#26415;&#20934;&#30830;&#24615;&#39640;&#65292;&#21363;&#20351;&#22312;&#26679;&#26412;&#25968;&#25454;&#38750;&#24120;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20173;&#21487;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#26174;&#30528;&#30340;&#36828;&#31243;&#30417;&#25511;&#28508;&#21147;&#65292;&#21363;&#20351;&#21482;&#26377;&#26497;&#23569;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;&#26356;&#22797;&#26434;&#30340;&#39046;&#22495;&#65292;&#22914;&#29983;&#29289;&#23398;&#39046;&#22495;&#20013;&#23545;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35780;&#20272;&#12290; LLM&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#26679;&#26412;&#22823;&#23567;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20174;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#20808;&#39564;&#30693;&#35782;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#32570;&#20047;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#29305;&#24449;&#30340;&#32597;&#35265;&#32452;&#32455;&#20013;&#33647;&#29289;&#23545;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290; &#23454;&#39564;&#28041;&#21450;&#26469;&#33258;&#19981;&#21516;&#30284;&#30151;&#31867;&#22411;&#30340;&#19971;&#31181;&#32597;&#35265;&#32452;&#32455;&#65292;&#34920;&#26126;&#22522;&#20110;LLMs&#30340;&#39044;&#27979;&#27169;&#22411;&#22312;&#38750;&#24120;&#23569;&#25110;&#38646;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21462;&#24471;&#26174;&#30528;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;CancerGPT&#65288;&#20855;&#26377;$\sim 124M$&#21442;&#25968;&#65289;&#29978;&#33267;&#21487;&#20197;&#19982;&#26356;&#22823;&#30340;&#24494;&#35843;GPT-3&#27169;&#22411;&#65288;&#20855;&#26377;$\sim 175B$&#21442;&#25968;&#65289;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;LLMs&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26696;&#20363;&#65292;&#20026;&#29983;&#29289;&#23398;&#25512;&#26029;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models (LLMs) have been shown to have significant potential in few-shot learning across various fields, even with minimal training data. However, their ability to generalize to unseen tasks in more complex fields, such as biology, has yet to be fully evaluated. LLMs can offer a promising alternative approach for biological inference, particularly in cases where structured data and sample size are limited, by extracting prior knowledge from text corpora. Our proposed few-shot learning approach uses LLMs to predict the synergy of drug pairs in rare tissues that lack structured data and features. Our experiments, which involved seven rare tissues from different cancer types, demonstrated that the LLM-based prediction model achieved significant accuracy with very few or zero samples. Our proposed model, the CancerGPT (with $\sim$ 124M parameters), was even comparable to the larger fine-tuned GPT-3 model (with $\sim$ 175B parameters). Our research is the first to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#25991;&#31456;&#26102;&#38388;&#27573;&#39044;&#27979;&#22120;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#23545;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#30340;&#32467;&#26524;&#34920;&#29616;&#20248;&#20110;&#20808;&#21069;&#23581;&#35797;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10859</link><description>&lt;p&gt;
Text2Time: &#22522;&#20110;Transformer&#30340;&#25991;&#31456;&#26102;&#38388;&#27573;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Text2Time: Transformer-based article time period predictor. (arXiv:2304.10859v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#25991;&#31456;&#26102;&#38388;&#27573;&#39044;&#27979;&#22120;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#23545;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#30340;&#32467;&#26524;&#34920;&#29616;&#20248;&#20110;&#20808;&#21069;&#23581;&#35797;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#21033;&#29992;&#25991;&#26412;&#20869;&#23481;&#39044;&#27979;&#25991;&#31456;&#21457;&#34920;&#26102;&#38388;&#27573;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;35&#19975;&#31687;&#12298;&#32445;&#32422;&#26102;&#25253;&#12299;&#21382;&#26102;&#20845;&#21313;&#24180;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26420;&#32032;&#36125;&#21494;&#26031;&#22522;&#20934;&#27169;&#22411;&#65292;&#23427;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20154;&#24847;&#26009;&#20043;&#22806;&#30340;&#19981;&#38169;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#24494;&#35843;&#20197;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#25105;&#20204;&#30340;&#39044;&#26399;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#38750;&#24120;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20934;&#30830;&#22320;&#23558;&#26032;&#38395;&#25991;&#31456;&#20998;&#31867;&#33267;&#20854;&#20986;&#29256;&#30340;&#24180;&#20195;&#12290;&#32467;&#26524;&#36229;&#36807;&#20102;&#20808;&#21069;&#23581;&#35797;&#30340;&#36825;&#31181;&#30456;&#23545;&#19981;&#21463;&#20851;&#27880;&#30340;&#25991;&#26412;&#39044;&#27979;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the problem of predicting the publication period of text document, such as a news article, using the text from that document. In order to do so, we created our own extensive labeled dataset of over 350,000 news articles published by The New York Times over six decades. We then provide an implementation of a simple Naive Bayes baseline model, which surprisingly achieves decent performance in terms of accuracy.Finally, for our approach, we use a pretrained BERT model fine-tuned for the task of text classification. This model exceeds our expectations and provides some very impressive results in terms of accurately classifying news articles into their respective publication decades. The results beat the performance of the few previously tried models for this relatively unexplored task of time prediction from text.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35268;&#21017;&#36716;&#25442;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#22823;&#35268;&#27169;&#21333;&#35821;&#25968;&#25454;&#36716;&#24405;&#20026;&#20854;&#20266;&#25163;&#35821;&#32534;&#30721;&#20197;&#25552;&#39640;&#25163;&#35821;&#32763;&#35793;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.10844</link><description>&lt;p&gt;
&#21033;&#29992;&#21333;&#35821;&#25968;&#25454;&#23454;&#29616;&#26356;&#22909;&#30340;&#25163;&#35821;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Better Sign Language Translation with Monolingual Data. (arXiv:2304.10844v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35268;&#21017;&#36716;&#25442;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#22823;&#35268;&#27169;&#21333;&#35821;&#25968;&#25454;&#36716;&#24405;&#20026;&#20854;&#20266;&#25163;&#35821;&#32534;&#30721;&#20197;&#25552;&#39640;&#25163;&#35821;&#32763;&#35793;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#32763;&#35793;&#31995;&#32479;&#36890;&#24120;&#30001;&#35270;&#39057;&#21040;&#25163;&#35821;&#32534;&#30721;&#65288;V2G&#65289;&#35782;&#21035;&#21644;&#36890;&#36807;&#20013;&#20171;&#25163;&#35821;&#32534;&#30721;&#30340;&#20114;&#35793;&#65288;G2T&#65289;&#32763;&#35793;&#32452;&#25104;&#65292;&#19988;&#35813;&#20013;&#20171;&#25163;&#35821;&#32534;&#30721;&#30340;&#20154;&#24037;&#27880;&#37322;&#36827;&#19968;&#27493;&#24694;&#21270;&#20102;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#35268;&#21017;&#36716;&#25442;&#26041;&#27861;&#65292;&#23558;&#22823;&#35268;&#27169;&#30446;&#26631;&#35821;&#21333;&#35821;&#25968;&#25454;&#33258;&#21160;&#36716;&#24405;&#20026;&#20854;&#20266;&#25163;&#35821;&#32534;&#30721;&#65292;&#20197;&#25552;&#39640;&#25163;&#35821;&#32763;&#35793;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25163;&#35821;&#32763;&#35793;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312; PHEONIX-WEATHER 2014T &#21644; ASLG-PC12 &#20004;&#20010;&#25163;&#35821;&#32763;&#35793;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#33021;&#21147;&#12290;&#24050;&#32463;&#36890;&#36807;&#20195;&#30721;&#24320;&#28304;: https://github.com/pengr/Mono\_SLT&#12290;
&lt;/p&gt;
&lt;p&gt;
Sign language translation (SLT) systems, which are often decomposed into video-to-gloss (V2G) recognition and gloss-to-text (G2T) translation through the pivot gloss, heavily relies on the availability of large-scale parallel G2T pairs. However, the manual annotation of pivot gloss, which is a sequence of transcribed written-language words in the order in which they are signed, further exacerbates the scarcity of data for SLT. To address this issue, this paper proposes a simple and efficient rule transformation method to transcribe the large-scale target monolingual data into its pseudo glosses automatically for enhancing the SLT translation. Empirical results show that the proposed approach can significantly improve the performance of SLT, especially achieving state-of-the-art results on two SLT benchmark datasets PHEONIX-WEATHER 2014T and ASLG-PC12. Our code has been released at: https://github.com/pengr/Mono\_SLT.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#27604;&#36739;&#20102;&#20845;&#31181;&#20998;&#35789;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#21487;&#36861;&#28335;&#30340;&#20998;&#35789;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20998;&#35789;&#19981;&#19968;&#23450;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2304.10813</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#35789;&#21487;&#36861;&#28335;&#24615;&#65306;&#19968;&#20010;&#27880;&#37322;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tokenization Tractability for Human and Machine Learning Model: An Annotation Study. (arXiv:2304.10813v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10813
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#27604;&#36739;&#20102;&#20845;&#31181;&#20998;&#35789;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#21487;&#36861;&#28335;&#30340;&#20998;&#35789;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20998;&#35789;&#19981;&#19968;&#23450;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#36861;&#28335;&#30340;&#20998;&#35789;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#21542;&#20063;&#26159;&#21487;&#36861;&#28335;&#30340;&#65311;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#21487;&#36861;&#28335;&#30340;&#20998;&#35789;&#65288;&#22914;&#36866;&#24403;&#24615;&#21644;&#21487;&#35835;&#24615;&#65289;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20998;&#35789;&#65288;&#22914;&#22312;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#26085;&#35821;&#24120;&#35782;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;JGLUE&#30340;JCommmonsenseQA&#65289;&#20013;&#27604;&#36739;&#20102;&#20845;&#31181;&#20998;&#35789;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#20998;&#35789;&#22120;&#23545;&#38382;&#31572;&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#25991;&#26412;&#36827;&#34892;&#20998;&#35789;&#65292;&#24182;&#27604;&#36739;&#20102;&#20154;&#31867;&#26631;&#27880;&#32773;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24615;&#33021;&#12289;&#20998;&#35789;&#30340;&#36866;&#24403;&#24615;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#21709;&#24212;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23450;&#37327;&#35843;&#26597;&#32467;&#26524;&#65292;&#26174;&#31034;&#20986;&#23545;&#20110;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#65292;&#21487;&#36861;&#28335;&#30340;&#20998;&#35789;&#19981;&#19968;&#23450;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is tractable tokenization for humans also tractable for machine learning models? This study investigates relations between tractable tokenization for humans (e.g., appropriateness and readability) and one for models of machine learning (e.g., performance on an NLP task). We compared six tokenization methods on the Japanese commonsense question-answering dataset (JCommmonsenseQA in JGLUE). We tokenized question texts of the QA dataset with different tokenizers and compared the performance of human annotators and machine-learning models. Besides,we analyze relationships among the performance, appropriateness of tokenization, and response time to questions. This paper provides a quantitative investigation result that shows the tractable tokenizations for humans and machine learning models are not necessarily the same as each other.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#20808;&#35757;&#32451;&#30340;&#19979;&#28216;&#27169;&#22411;&#20248;&#21270; tokenization &#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#35789;&#27719;&#30340;&#26041;&#24335;&#21487;&#20197;&#29983;&#25104;&#26356;&#20302;&#25439;&#22833;&#20540;&#30340; tokenization &#32467;&#26524;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#22797;&#29616; tokenization &#32467;&#26524;&#30340;&#20998;&#35789;&#22120;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640; tokenization &#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10808</link><description>&lt;p&gt;
&#38480;&#21046;&#35789;&#27719;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#35789;&#22120;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Downstream Task-Oriented Neural Tokenizer Optimization with Vocabulary Restriction as Post Processing. (arXiv:2304.10808v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#20808;&#35757;&#32451;&#30340;&#19979;&#28216;&#27169;&#22411;&#20248;&#21270; tokenization &#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#35789;&#27719;&#30340;&#26041;&#24335;&#21487;&#20197;&#29983;&#25104;&#26356;&#20302;&#25439;&#22833;&#20540;&#30340; tokenization &#32467;&#26524;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#22797;&#29616; tokenization &#32467;&#26524;&#30340;&#20998;&#35789;&#22120;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640; tokenization &#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#20808;&#35757;&#32451;&#30340;&#19979;&#28216;&#27169;&#22411;&#20248;&#21270; tokenization &#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#38480;&#21046;&#35789;&#27719;&#30340;&#26041;&#24335;&#29983;&#25104; tokenization &#32467;&#26524;&#65292;&#20351;&#24471;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#32473;&#23450;&#19979;&#28216;&#27169;&#22411;&#30340;&#25439;&#22833;&#20540;&#26356;&#20302;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#21487;&#20197;&#22797;&#29616; tokenization &#32467;&#26524;&#30340;&#20998;&#35789;&#22120;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#20998;&#35789;&#26041;&#27861;&#65292;&#32780;&#29616;&#26377;&#30340;&#24037;&#20316;&#30001;&#20110;&#20998;&#35789;&#22120;&#21644;&#19979;&#28216;&#27169;&#22411;&#30340;&#21516;&#26102;&#23398;&#20064;&#65292;&#22240;&#27492;&#19981;&#33021;&#24212;&#29992;&#20110;&#21508;&#31181;&#20998;&#35789;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; BiLSTM-based &#20998;&#35789;&#22120;&#30340;&#31034;&#20363;&#65292;&#23427;&#21487;&#20197;&#27604;&#29616;&#26377;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#38750;&#31070;&#32463;&#32593;&#32476;&#20998;&#35789;&#26041;&#27861;&#25429;&#25417;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22312;&#26085;&#35821;&#12289;&#27721;&#35821;&#21644;&#33521;&#35821;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312; tokenization &#20248;&#21270;&#26041;&#38754;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method to optimize tokenization for the performance improvement of already trained downstream models. Our method generates tokenization results attaining lower loss values of a given downstream model on the training data for restricting vocabularies and trains a tokenizer reproducing the tokenization results. Therefore, our method can be applied to variety of tokenization methods, while existing work cannot due to the simultaneous learning of the tokenizer and the downstream model. This paper proposes an example of the BiLSTM-based tokenizer with vocabulary restriction, which can capture wider contextual information for the tokenization process than non-neural-based tokenization methods used in existing work. Experimental results on text classification in Japanese, Chinese, and English text classification tasks show that the proposed method improves performance compared to the existing methods for tokenization optimization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#35805;&#31995;&#32479;&#20013;&#24433;&#21709;&#32842;&#22825;&#20307;&#39564;&#30340;&#22810;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#25552;&#31034;&#12289;&#36830;&#36143;&#24615;&#12289;&#24773;&#24863;&#12289;&#30456;&#20284;&#24615;&#21644;&#29992;&#25143;&#23545;&#35805;&#20195;&#29702;&#30340;&#22909;&#24863;&#24230;&#65292;&#21457;&#29616;&#29992;&#25143;&#30340;&#22909;&#24863;&#24230;&#21644;&#36830;&#36143;&#24615;&#12289;&#24773;&#24863;&#12289;&#30456;&#20284;&#24615;&#26159;&#32842;&#22825;&#20307;&#39564;&#30340;&#27491;&#21521;&#39044;&#27979;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#21487;&#33021;&#26356;&#21916;&#27426;&#20855;&#26377;&#22806;&#21521;&#24615;&#12289;&#24320;&#25918;&#24615;&#12289;&#36131;&#20219;&#24515;&#12289;&#23452;&#20154;&#24615;&#21644;&#38750;&#31070;&#32463;&#36136;&#29305;&#24449;&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.10785</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#35805;&#26381;&#21153;&#30340;&#32842;&#22825;&#20307;&#39564;&#39044;&#27979;&#22240;&#32032;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Which Factors Predict the Chat Experience of a Natural Language Generation Dialogue Service?. (arXiv:2304.10785v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#35805;&#31995;&#32479;&#20013;&#24433;&#21709;&#32842;&#22825;&#20307;&#39564;&#30340;&#22810;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#25552;&#31034;&#12289;&#36830;&#36143;&#24615;&#12289;&#24773;&#24863;&#12289;&#30456;&#20284;&#24615;&#21644;&#29992;&#25143;&#23545;&#35805;&#20195;&#29702;&#30340;&#22909;&#24863;&#24230;&#65292;&#21457;&#29616;&#29992;&#25143;&#30340;&#22909;&#24863;&#24230;&#21644;&#36830;&#36143;&#24615;&#12289;&#24773;&#24863;&#12289;&#30456;&#20284;&#24615;&#26159;&#32842;&#22825;&#20307;&#39564;&#30340;&#27491;&#21521;&#39044;&#27979;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#21487;&#33021;&#26356;&#21916;&#27426;&#20855;&#26377;&#22806;&#21521;&#24615;&#12289;&#24320;&#25918;&#24615;&#12289;&#36131;&#20219;&#24515;&#12289;&#23452;&#20154;&#24615;&#21644;&#38750;&#31070;&#32463;&#36136;&#29305;&#24449;&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#32842;&#22825;&#20307;&#39564;&#12290;&#25105;&#20204;&#20351;&#29992;&#37096;&#20998;&#26368;&#23567;&#20108;&#20056;&#32467;&#26500;&#26041;&#31243;&#24314;&#27169;&#26041;&#27861;&#65288;PLS-SEM&#65289;&#23545;120&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#33719;&#24471;&#20102;0.541&#30340;R&#26041;&#20540;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#22810;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#29992;&#20110;&#29983;&#25104;&#30340;&#25552;&#31034;&#12289;&#23545;&#35805;&#20013;&#30340;&#36830;&#36143;&#24615;&#12289;&#24773;&#24863;&#21644;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#29992;&#25143;&#23545;&#35805;&#20195;&#29702;&#30340;&#22909;&#24863;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#23376;&#38598;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#29992;&#25143;&#30340;&#22909;&#24863;&#24230;&#21644;&#23545;&#35805;&#20013;&#30340;&#36830;&#36143;&#24615;&#12289;&#24773;&#24863;&#21644;&#30456;&#20284;&#24615;&#26159;&#29992;&#25143;&#32842;&#22825;&#20307;&#39564;&#30340;&#27491;&#21521;&#39044;&#27979;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#29992;&#25143;&#21487;&#33021;&#26356;&#21916;&#27426;&#20855;&#26377;&#22806;&#21521;&#24615;&#12289;&#24320;&#25918;&#24615;&#12289;&#36131;&#20219;&#24515;&#12289;&#23452;&#20154;&#24615;&#21644;&#38750;&#31070;&#32463;&#36136;&#29305;&#24449;&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#33258;&#36866;&#24212;&#23545;&#35805;&#31995;&#32479;&#21487;&#20197;&#20351;&#29992;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#26469;&#25512;&#26029;&#25105;&#20204;&#27169;&#22411;&#20013;&#30340;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#23545;&#35805;&#20195;&#29702;&#30340;&#29305;&#24449;&#26469;&#39044;&#27979;&#29992;&#25143;&#30340;&#32842;&#22825;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we proposed a conceptual model to predict the chat experience in a natural language generation dialog system. We evaluated the model with 120 participants with Partial Least Squares Structural Equation Modeling (PLS-SEM) and obtained an R-square (R2) with 0.541. The model considers various factors, including the prompts used for generation; coherence, sentiment, and similarity in the conversation; and users' perceived dialog agents' favorability. We then further explore the effectiveness of the subset of our proposed model. The results showed that users' favorability and coherence, sentiment, and similarity in the dialogue are positive predictors of users' chat experience. Moreover, we found users may prefer dialog agents with characteristics of Extroversion, Openness, Conscientiousness, Agreeableness, and Non-Neuroticism. Through our research, an adaptive dialog system might use collected data to infer factors in our model, predict the chat experience for users through 
&lt;/p&gt;</description></item><item><title>Eyettention&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#22788;&#29702;&#35821;&#35328;&#24207;&#21015;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#38405;&#35835;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#38405;&#35835;&#32773;&#30340;&#25195;&#35270;&#36335;&#24452;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20855;&#26377;&#20511;&#37492;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.10784</link><description>&lt;p&gt;
Eyettention&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21452;&#24207;&#21015;&#27169;&#22411;&#20197;&#39044;&#27979;&#20154;&#31867;&#38405;&#35835;&#26102;&#30340;&#25195;&#35270;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading. (arXiv:2304.10784v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10784
&lt;/p&gt;
&lt;p&gt;
Eyettention&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#22788;&#29702;&#35821;&#35328;&#24207;&#21015;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#38405;&#35835;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#38405;&#35835;&#32773;&#30340;&#25195;&#35270;&#36335;&#24452;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20855;&#26377;&#20511;&#37492;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38405;&#35835;&#26102;&#30340;&#30524;&#21160;&#25581;&#31034;&#20102;&#38405;&#35835;&#32773;&#30340;&#35748;&#30693;&#36807;&#31243;&#21644;&#25152;&#38405;&#35835;&#25991;&#26412;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#38405;&#35835;&#20013;&#25195;&#35270;&#36335;&#24452;&#30340;&#20998;&#26512;&#24050;&#24341;&#36215;&#21508;&#20010;&#39046;&#22495;&#30340;&#20851;&#27880;&#65292;&#28085;&#30422;&#20102;&#20174;&#35748;&#30693;&#31185;&#23398;&#21040;&#35821;&#35328;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#38405;&#35835;&#26102;&#20154;&#31867;&#30340;&#25195;&#35270;&#36335;&#24452;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#23427;&#20204;&#26159;&#30001;&#21452;&#24207;&#21015;&#32452;&#25104;&#30340;&#65306;&#21333;&#35789;&#25353;&#29031;&#35821;&#35328;&#30340;&#35821;&#27861;&#35268;&#21017;&#25490;&#24207;&#65292;&#32780;&#27880;&#35270;&#21017;&#25353;&#29031;&#26102;&#38388;&#39034;&#24207;&#25490;&#24207;&#12290;&#20154;&#31867;&#24182;&#19981;&#20005;&#26684;&#25353;&#24038;&#21040;&#21491;&#30340;&#39034;&#24207;&#38405;&#35835;&#65292;&#32780;&#26159;&#36339;&#36807;&#25110;&#37325;&#22797;&#27880;&#35270;&#21333;&#35789;&#65292;&#24182;&#20498;&#36864;&#21040;&#20197;&#21069;&#30340;&#21333;&#35789;&#65292;&#35821;&#35328;&#24207;&#21015;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#23545;&#40784;&#24182;&#19981;&#23481;&#26131;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;Eyettention&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#22788;&#29702;&#35821;&#35328;&#24207;&#21015;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#21452;&#24207;&#21015;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eye movements during reading offer insights into both the reader's cognitive processes and the characteristics of the text that is being read. Hence, the analysis of scanpaths in reading have attracted increasing attention across fields, ranging from cognitive science over linguistics to computer science. In particular, eye-tracking-while-reading data has been argued to bear the potential to make machine-learning-based language models exhibit a more human-like linguistic behavior. However, one of the main challenges in modeling human scanpaths in reading is their dual-sequence nature: the words are ordered following the grammatical rules of the language, whereas the fixations are chronologically ordered. As humans do not strictly read from left-to-right, but rather skip or refixate words and regress to previous words, the alignment of the linguistic and the temporal sequence is non-trivial. In this paper, we develop Eyettention, the first dual-sequence model that simultaneously process
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#20114;&#21160;&#21453;&#39304;&#19982;&#20195;&#29702;&#20132;&#20114;&#26469;&#25552;&#39640;&#21327;&#20316;&#29615;&#22659;&#19979;&#22522;&#20110;&#23454;&#22320;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.10750</link><description>&lt;p&gt;
&#36890;&#36807;&#20114;&#21160;&#21453;&#39304;&#19982;&#20195;&#29702;&#20132;&#20114;&#26469;&#25552;&#39640;&#21327;&#20316;&#29615;&#22659;&#19979;&#22522;&#20110;&#23454;&#22320;&#29702;&#35299;&#65288;Grounded Language Understanding&#65289;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback. (arXiv:2304.10750v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10750
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#20114;&#21160;&#21453;&#39304;&#19982;&#20195;&#29702;&#20132;&#20114;&#26469;&#25552;&#39640;&#21327;&#20316;&#29615;&#22659;&#19979;&#22522;&#20110;&#23454;&#22320;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36890;&#24120;&#34987;&#35270;&#20026;&#21333;&#27493;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#20195;&#29702;&#25509;&#25910;&#19968;&#20010;&#25351;&#20196;&#65292;&#25191;&#34892;&#23427;&#65292;&#28982;&#21518;&#26681;&#25454;&#26368;&#32456;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#20132;&#20114;&#24335;&#30340;&#65292;&#25105;&#20204;&#20027;&#24352;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21327;&#20316;&#20063;&#24212;&#26159;&#20132;&#20114;&#24335;&#30340;&#65292;&#20154;&#31867;&#30417;&#30563;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#24037;&#20316;&#65292;&#24182;&#25552;&#20379;&#20195;&#29702;&#21487;&#20197;&#29702;&#35299;&#21644;&#21033;&#29992;&#30340;&#21453;&#39304;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;Help Feedback&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many approaches to Natural Language Processing (NLP) tasks often treat them as single-step problems, where an agent receives an instruction, executes it, and is evaluated based on the final outcome. However, human language is inherently interactive, as evidenced by the back-and-forth nature of human conversations. In light of this, we posit that human-AI collaboration should also be interactive, with humans monitoring the work of AI agents and providing feedback that the agent can understand and utilize. Further, the AI agent should be able to detect when it needs additional information and proactively ask for help. Enabling this scenario would lead to more natural, efficient, and engaging human-AI collaborations.  In this work, we explore these directions using the challenging task defined by the IGLU competition, an interactive grounded language understanding task in a MineCraft-like world. We explore multiple types of help players can give to the AI to guide it and analyze the impac
&lt;/p&gt;</description></item><item><title>KitchenScale&#26159;&#19968;&#20010;&#32463;&#36807;Fine-tuned&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#21487;&#26681;&#25454;&#39135;&#35889;&#19978;&#19979;&#25991;&#39044;&#27979;&#30446;&#26631;&#25104;&#20998;&#30340;&#25968;&#37327;&#21644;&#27979;&#37327;&#21333;&#20301;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#31163;&#25955;&#28508;&#22312;&#25351;&#25968;&#65288;DExp&#65289;&#26041;&#27861;&#22788;&#29702;&#39135;&#35889;&#35821;&#26009;&#24211;&#20013;&#25968;&#23383;&#23610;&#24230;&#30340;&#39640;&#26041;&#24046;&#65292;&#23581;&#35797;&#20174;&#39135;&#35889;&#25991;&#26412;&#21040;PLMs&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#22312;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#21644;&#25512;&#33616;&#31034;&#20363;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;KitchenScale&#20855;&#26377;&#27867;&#21270;&#24615;&#24182;&#21487;&#20197;&#29702;&#35299;&#21508;&#31181;&#39135;&#35889;&#35821;&#22659;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#26469;&#20026;&#29992;&#25143;&#25552;&#20379;&#25152;&#38656;&#30340;&#39135;&#21697;&#37327;&#30340;&#37197;&#26041;&#29305;&#23450;&#30340;&#27979;&#37327;&#21333;&#20301;&#12290;</title><link>http://arxiv.org/abs/2304.10739</link><description>&lt;p&gt;
KitchenScale: &#20174;&#39135;&#35889;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#39044;&#27979;&#25104;&#20998;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
KitchenScale: Learning to predict ingredient quantities from recipe contexts. (arXiv:2304.10739v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10739
&lt;/p&gt;
&lt;p&gt;
KitchenScale&#26159;&#19968;&#20010;&#32463;&#36807;Fine-tuned&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#21487;&#26681;&#25454;&#39135;&#35889;&#19978;&#19979;&#25991;&#39044;&#27979;&#30446;&#26631;&#25104;&#20998;&#30340;&#25968;&#37327;&#21644;&#27979;&#37327;&#21333;&#20301;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#31163;&#25955;&#28508;&#22312;&#25351;&#25968;&#65288;DExp&#65289;&#26041;&#27861;&#22788;&#29702;&#39135;&#35889;&#35821;&#26009;&#24211;&#20013;&#25968;&#23383;&#23610;&#24230;&#30340;&#39640;&#26041;&#24046;&#65292;&#23581;&#35797;&#20174;&#39135;&#35889;&#25991;&#26412;&#21040;PLMs&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#22312;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#21644;&#25512;&#33616;&#31034;&#20363;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;KitchenScale&#20855;&#26377;&#27867;&#21270;&#24615;&#24182;&#21487;&#20197;&#29702;&#35299;&#21508;&#31181;&#39135;&#35889;&#35821;&#22659;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#26469;&#20026;&#29992;&#25143;&#25552;&#20379;&#25152;&#38656;&#30340;&#39135;&#21697;&#37327;&#30340;&#37197;&#26041;&#29305;&#23450;&#30340;&#27979;&#37327;&#21333;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28921;&#39274;&#23454;&#36341;&#20013;&#65292;&#30830;&#23450;&#25104;&#20998;&#30340;&#36866;&#24403;&#37327;&#23545;&#20110;&#20016;&#23500;&#21475;&#24863;&#21644;&#20419;&#36827;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;KitchenScale&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;Fine-tuned&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#26681;&#25454;&#39135;&#35889;&#19978;&#19979;&#25991;&#39044;&#27979;&#30446;&#26631;&#25104;&#20998;&#30340;&#25968;&#37327;&#21644;&#27979;&#37327;&#21333;&#20301;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#35757;&#32451;&#25105;&#20204;&#30340;KitchenScale&#27169;&#22411;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#21253;&#25324;&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#25104;&#20998;&#37327;&#39044;&#27979;&#20219;&#21153;&#65292;&#36825;&#20123;&#23376;&#20219;&#21153;&#26159;&#25104;&#20998;&#27979;&#37327;&#31867;&#22411;&#20998;&#31867;&#12289;&#21333;&#20301;&#20998;&#31867;&#21644;&#25968;&#37327;&#22238;&#24402;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#20174;&#39135;&#35889;&#25991;&#26412;&#21040;PLMs&#30340;&#28921;&#39274;&#30693;&#35782;&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#31163;&#25955;&#28508;&#22312;&#25351;&#25968;&#65288;DExp&#65289;&#26041;&#27861;&#26469;&#24212;&#23545;&#39135;&#35889;&#35821;&#26009;&#24211;&#20013;&#25968;&#23383;&#23610;&#24230;&#30340;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#21644;&#25512;&#33616;&#31034;&#20363;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;KitchenScale&#29702;&#35299;&#21508;&#31181;&#39135;&#35889;&#35821;&#22659;&#20197;&#21450;&#22312;&#39044;&#27979;&#25104;&#20998;&#37327;&#26041;&#38754;&#20855;&#26377;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#25152;&#38656;&#30340;&#29992;&#20110;&#25152;&#38656;&#20154;&#25968;&#39135;&#21697;&#37327;&#30340;&#37197;&#26041;&#29305;&#23450;&#30340;&#27979;&#37327;&#21333;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining proper quantities for ingredients is an essential part of cooking practice from the perspective of enriching tastiness and promoting healthiness. We introduce KitchenScale, a fine-tuned Pre-trained Language Model (PLM) that predicts a target ingredient's quantity and measurement unit given its recipe context. To effectively train our KitchenScale model, we formulate an ingredient quantity prediction task that consists of three sub-tasks which are ingredient measurement type classification, unit classification, and quantity regression task. Furthermore, we utilized transfer learning of cooking knowledge from recipe texts to PLMs. We adopted the Discrete Latent Exponent (DExp) method to cope with high variance of numerical scales in recipe corpora. Experiments with our newly constructed dataset and recommendation examples demonstrate KitchenScale's understanding of various recipe contexts and generalizability in predicting ingredient quantities. We implemented a web applicati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#23548;&#38142;&#27491;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#30340;&#25512;&#29702;&#38142;&#35780;&#20272;&#26694;&#26550;ReCEval&#65292;&#29992;&#20197;&#35780;&#20272;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#23458;&#35266;&#12289;&#31995;&#32479;&#21644;&#20934;&#30830;&#22320;&#35780;&#20272;&#25512;&#29702;&#38142;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.10703</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#35780;&#20272;&#25512;&#29702;&#38142;&#30340;ReCEval
&lt;/p&gt;
&lt;p&gt;
ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness. (arXiv:2304.10703v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#23548;&#38142;&#27491;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#30340;&#25512;&#29702;&#38142;&#35780;&#20272;&#26694;&#26550;ReCEval&#65292;&#29992;&#20197;&#35780;&#20272;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#23458;&#35266;&#12289;&#31995;&#32479;&#21644;&#20934;&#30830;&#22320;&#35780;&#20272;&#25512;&#29702;&#38142;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#37117;&#26159;&#22522;&#30784;&#65292;&#20294;&#20160;&#20040;&#26500;&#25104;&#22909;&#30340;&#25512;&#29702;&#38142;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#23427;&#20204;&#23578;&#19981;&#28165;&#26970;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20165;&#20851;&#27880;&#25512;&#29702;&#38142;&#26159;&#21542;&#23548;&#33268;&#27491;&#30830;&#30340;&#32467;&#35770;&#65292;&#20294;&#36825;&#31181;&#20197;&#31572;&#26696;&#20026;&#23548;&#21521;&#30340;&#35266;&#28857;&#21487;&#33021;&#20250;&#23558;&#22909;&#30340;&#25512;&#29702;&#36136;&#37327;&#19982;&#20854;&#20182;&#29992;&#20110;&#39044;&#27979;&#31572;&#26696;&#30340;&#20551;&#25463;&#24452;&#28151;&#28102;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23558;&#25512;&#29702;&#38142;&#35270;&#20026;&#25512;&#23548;&#26368;&#32456;&#31572;&#26696;&#30340;&#38750;&#27491;&#24335;&#35777;&#26126;&#65292;&#36890;&#36807;&#35780;&#20272;&#25512;&#29702;&#38142;&#30340;&#20004;&#20010;&#20851;&#38190;&#29305;&#24615;&#8212;&#8212;&#65288;1&#65289;&#27491;&#30830;&#24615;&#65292;&#21363;&#27599;&#20010;&#27493;&#39588;&#22522;&#20110;&#27493;&#39588;&#65292;&#21069;&#32622;&#27493;&#39588;&#21644;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#36827;&#34892;&#26377;&#25928;&#25512;&#29702;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20449;&#24687;&#37327;&#65292;&#21363;&#27599;&#20010;&#27493;&#39588;&#25552;&#20379;&#26032;&#20449;&#24687;&#26377;&#21161;&#20110;&#25512;&#23548;&#29983;&#25104;&#30340;&#31572;&#26696;&#8212;&#8212;&#25105;&#20204;&#25552;&#20986;&#20102;ReCEval&#65288;&#25512;&#29702;&#38142;&#35780;&#20272;&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#21644;&#20449;&#24687;&#29702;&#35770;&#27979;&#37327;&#23454;&#29616;&#20102;ReCEval&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35780;&#20272;&#25512;&#29702;&#38142;&#26041;&#38754;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#21152;&#23458;&#35266;&#12289;&#31995;&#32479;&#21644;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-step reasoning ability is fundamental to many natural language tasks, yet it is unclear what constitutes a good reasoning chain and how to evaluate them. Most existing methods focus solely on whether the reasoning chain leads to the correct conclusion, but this answer-oriented view may confound the quality of reasoning with other spurious shortcuts to predict the answer. To bridge this gap, we evaluate reasoning chains by viewing them as informal proofs that derive the final answer. Specifically, we propose ReCEval (Reasoning Chain Evaluation), a framework that evaluates reasoning chains through two key properties: (1) correctness, i.e., each step makes a valid inference based on the information contained within the step, preceding steps, and input context, and (2) informativeness, i.e., each step provides new information that is helpful towards deriving the generated answer. We implement ReCEval using natural language inference models and information-theoretic measures. On multi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#35789;&#27719;&#22806;&#38382;&#39064;&#30340;&#20004;&#31181;&#31574;&#30053;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#26356;&#22909;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#35821;&#20041;&#27169;&#22411;&#65292;&#26088;&#22312;&#20811;&#26381;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21644;&#22522;&#20110;&#35268;&#21017;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2304.10663</link><description>&lt;p&gt;
&#20803;&#35821;&#20041;&#23398;&#65306;&#36808;&#21521;&#26356;&#22909;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Meta Semantics: Towards better natural language understanding and reasoning. (arXiv:2304.10663v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10663
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#35789;&#27719;&#22806;&#38382;&#39064;&#30340;&#20004;&#31181;&#31574;&#30053;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#26356;&#22909;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#35821;&#20041;&#27169;&#22411;&#65292;&#26088;&#22312;&#20811;&#26381;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21644;&#22522;&#20110;&#35268;&#21017;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22359;&#65288;LLM&#65289;&#26041;&#27861;&#65292;&#22914;ChatGPT&#21644;GPT-3&#65292;&#20855;&#26377;&#37319;&#29992;&#38750;&#27491;&#24335;&#25991;&#26412;&#30340;&#24378;&#22823;&#28789;&#27963;&#24615;&#65292;&#20294;&#22312;&#36923;&#36753;&#25512;&#23548;&#19978;&#36739;&#24369;&#65292;&#24182;&#19988;&#21463;&#21040;&#35789;&#27719;&#22806;&#65288;OOV&#65289;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#22914;Mathematica&#12289;&#35821;&#20041;&#32593;&#32476;&#21644;Lean&#65292;&#22312;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26080;&#27861;&#22788;&#29702;&#22797;&#26434;&#21644;&#26131;&#21464;&#30340;&#38750;&#27491;&#24335;&#25991;&#26412;&#12290;&#21463;&#21040;&#35821;&#29992;&#23398;&#21644;&#32467;&#26500;&#20027;&#20041;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#35299;&#20915;OOV&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35821;&#20041;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language understanding is one of the most challenging topics in artificial intelligence. Deep neural network methods, particularly large language module (LLM) methods such as ChatGPT and GPT-3, have powerful flexibility to adopt informal text but are weak on logical deduction and suffer from the out-of-vocabulary (OOV) problem. On the other hand, rule-based methods such as Mathematica, Semantic web, and Lean, are excellent in reasoning but cannot handle the complex and changeable informal text. Inspired by pragmatics and structuralism, we propose two strategies to solve the OOV problem and a semantic model for better natural language understanding and reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;BERT&#20013;&#33976;&#39311;&#30693;&#35782;&#36827;&#34892;&#22810;&#35789;&#20041;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#22320;&#21033;&#29992;&#35789;&#30340;&#22810;&#20010;&#35821;&#20041;&#24863;&#30693;&#65292;&#22312;&#36164;&#28304;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#19982;&#26368;&#20808;&#36827;&#30340;&#22810;&#35789;&#20041;&#23884;&#20837;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.10642</link><description>&lt;p&gt;
&#20174;BERT&#20013;&#33976;&#39311;&#30693;&#35782;&#36827;&#34892;&#35789;&#20041;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Word Sense Induction with Knowledge Distillation from BERT. (arXiv:2304.10642v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;BERT&#20013;&#33976;&#39311;&#30693;&#35782;&#36827;&#34892;&#22810;&#35789;&#20041;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#22320;&#21033;&#29992;&#35789;&#30340;&#22810;&#20010;&#35821;&#20041;&#24863;&#30693;&#65292;&#22312;&#36164;&#28304;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#19982;&#26368;&#20808;&#36827;&#30340;&#22810;&#35789;&#20041;&#23884;&#20837;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#34987;&#24191;&#27867;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#20294;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#31995;&#32479;&#26469;&#35828;&#24182;&#19981;&#36866;&#29992;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38750;&#19978;&#19979;&#25991;&#35789;&#21521;&#37327;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#21033;&#29992;&#35789;&#30340;&#22810;&#20010;&#35821;&#20041;&#24863;&#30693;&#26469;&#20174;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;BERT&#65289;&#20013;&#33976;&#39311;&#22810;&#20010;&#35789;&#20041;&#65292;&#24182;&#23558;&#36825;&#31181;&#20449;&#24687;&#20256;&#36882;&#21040;&#31867;skip-gram&#26694;&#26550;&#19979;&#30340;&#22810;&#35789;&#20041;&#23884;&#20837;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained contextual language models are ubiquitously employed for language understanding tasks, but are unsuitable for resource-constrained systems. Noncontextual word embeddings are an efficient alternative in these settings. Such methods typically use one vector to encode multiple different meanings of a word, and incur errors due to polysemy. This paper proposes a two-stage method to distill multiple word senses from a pre-trained language model (BERT) by using attention over the senses of a word in a context and transferring this sense information to fit multi-sense embeddings in a skip-gram-like framework. We demonstrate an effective approach to training the sense disambiguation mechanism in our model with a distribution over word senses extracted from the output layer embeddings of BERT. Experiments on the contextual word similarity and sense induction tasks show that this method is superior to or competitive with state-of-the-art multi-sense embeddings on multiple benchmark d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#12289;&#38142;&#25509;&#21644;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#20998;&#31867;&#32454;&#31890;&#24230;&#21644;&#26032;&#20852;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2304.10637</link><description>&lt;p&gt;
IXA/Cogcomp&#22312;SemEval-2023&#20219;&#21153;2&#20013;&#30340;&#34920;&#29616;&#65306;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
IXA/Cogcomp at SemEval-2023 Task 2: Context-enriched Multilingual Named Entity Recognition using Knowledge Bases. (arXiv:2304.10637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#12289;&#38142;&#25509;&#21644;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#20998;&#31867;&#32454;&#31890;&#24230;&#21644;&#26032;&#20852;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#26159;&#19968;&#39033;&#26680;&#24515;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#22312;&#36825;&#26041;&#38754;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20687;CoNLL 2003&#31561;&#26631;&#20934;&#22522;&#20934;&#24182;&#27809;&#26377;&#35299;&#20915;&#37096;&#32626;NER&#31995;&#32479;&#38656;&#35201;&#38754;&#23545;&#30340;&#35768;&#22810;&#25361;&#25112;&#65292;&#20363;&#22914;&#38656;&#35201;&#20197;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#23545;&#26032;&#20852;&#25110;&#22797;&#26434;&#23454;&#20307;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NER&#32423;&#32852;&#26041;&#27861;&#65292;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;&#39318;&#20808;&#65292;&#35782;&#21035;&#36755;&#20837;&#21477;&#23376;&#20013;&#30340;&#23454;&#20307;&#20505;&#36873;&#39033;&#65307;&#20854;&#27425;&#65292;&#23558;&#27599;&#20010;&#20505;&#36873;&#39033;&#38142;&#25509;&#21040;&#29616;&#26377;&#30340;&#30693;&#35782;&#24211;&#65307;&#31532;&#19977;&#65292;&#39044;&#27979;&#27599;&#20010;&#23454;&#20307;&#20505;&#36873;&#39033;&#30340;&#32454;&#31890;&#24230;&#31867;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22806;&#37096;&#30693;&#35782;&#24211;&#22312;&#20934;&#30830;&#20998;&#31867;&#32454;&#31890;&#24230;&#21644;&#26032;&#20852;&#23454;&#20307;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;MultiCoNER2&#20849;&#20139;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#40065;&#26834;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#20063;&#33021;&#21033;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) is a core natural language processing task in which pre-trained language models have shown remarkable performance. However, standard benchmarks like CoNLL 2003 \cite{conll03} do not address many of the challenges that deployed NER systems face, such as having to classify emerging or complex entities in a fine-grained way. In this paper we present a novel NER cascade approach comprising three steps: first, identifying candidate entities in the input sentence; second, linking the each candidate to an existing knowledge base; third, predicting the fine-grained category for each entity candidate. We empirically demonstrate the significance of external knowledge bases in accurately classifying fine-grained and emerging entities. Our system exhibits robust performance in the MultiCoNER2 \cite{multiconer2-data} shared task, even in the low-resource language setting where we leverage knowledge bases of high-resource languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;ChatGPT&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#26377;&#23475;&#35780;&#35770;&#30340;&#21487;&#34892;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;ChatGPT&#21487;&#20197;&#36798;&#21040;&#32422;80%&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10619</link><description>&lt;p&gt;
&#8220;HOT&#8221; ChatGPT&#65306;ChatGPT&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26816;&#27979;&#21644;&#35782;&#21035;&#20196;&#20154;&#35752;&#21388;&#12289;&#20196;&#20154;&#19981;&#24742;&#21644;&#26377;&#23475;&#35780;&#35770;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
"HOT" ChatGPT: The promise of ChatGPT in detecting and discriminating hateful, offensive, and toxic comments on social media. (arXiv:2304.10619v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;ChatGPT&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#26377;&#23475;&#35780;&#35770;&#30340;&#21487;&#34892;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;ChatGPT&#21487;&#20197;&#36798;&#21040;&#32422;80%&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#21361;&#23475;&#24615;&#20869;&#23481;&#30340;&#23384;&#22312;&#23545;&#22312;&#32447;&#31038;&#21306;&#21644;&#21442;&#19982;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#24320;&#21457;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#30340;&#26816;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#36825;&#26679;&#30340;&#27169;&#22411;&#38656;&#35201;&#26333;&#38706;&#26631;&#27880;&#32773;&#20110;&#26377;&#23475;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#65292;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#26377;&#28508;&#21147;&#29702;&#35299;&#21644;&#26816;&#27979;&#26377;&#23475;&#20869;&#23481;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#28508;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;MTurker&#27880;&#37322;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#36825;&#20123;&#27880;&#37322;&#19982;&#26377;&#23475;&#20869;&#23481;&#30456;&#20851;&#30340;&#19977;&#20010;&#32463;&#24120;&#35752;&#35770;&#30340;&#27010;&#24565;&#65306;&#20196;&#20154;&#35752;&#21388;&#12289;&#20196;&#20154;&#19981;&#24742;&#21644;&#26377;&#23475;&#65288;HOT&#65289;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20116;&#20010;&#25552;&#31034;&#19982;ChatGPT&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#36827;&#34892;&#20102;&#22235;&#20010;&#23454;&#39564;&#26469;&#24341;&#20986;HOT&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;MTurker&#27880;&#37322;&#30456;&#27604;&#65292;ChatGPT&#21487;&#20197;&#36798;&#21040;&#32422;80&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#19982;HOT&#35780;&#35770;&#30456;&#27604;&#65292;&#27169;&#22411;&#23545;&#38750;HOT&#35780;&#35770;&#30340;&#20998;&#31867;&#26356;&#21152;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harmful content is pervasive on social media, poisoning online communities and negatively impacting participation. A common approach to address this issue is to develop detection models that rely on human annotations. However, the tasks required to build such models expose annotators to harmful and offensive content and may require significant time and cost to complete. Generative AI models have the potential to understand and detect harmful content. To investigate this potential, we used ChatGPT and compared its performance with MTurker annotations for three frequently discussed concepts related to harmful content: Hateful, Offensive, and Toxic (HOT). We designed five prompts to interact with ChatGPT and conducted four experiments eliciting HOT classifications. Our results show that ChatGPT can achieve an accuracy of approximately 80% when compared to MTurker annotations. Specifically, the model displays a more consistent classification for non-HOT comments than HOT comments compared 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#21644;&#24207;&#21015;&#32423;&#21035;&#30340;&#19981;&#21487;&#33021;&#24615;&#25439;&#22833;&#65292;&#20197;&#21450;&#22312;&#22521;&#35757;&#26399;&#38388;&#30340;&#37325;&#22797;&#24809;&#32602;&#12289;&#25512;&#29702;&#21644;&#21518;&#22788;&#29702;&#31561;&#22810;&#23618;&#38754;&#26041;&#27861;&#26469;&#25233;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37325;&#22797;&#65292;&#24182;&#36991;&#20813;&#29983;&#25104;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.10611</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26041;&#38754;&#37325;&#22797;&#25233;&#21046;&#21644;&#20869;&#23481;&#35843;&#25511;
&lt;/p&gt;
&lt;p&gt;
Multi-aspect Repetition Suppression and Content Moderation of Large Language Models. (arXiv:2304.10611v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#21644;&#24207;&#21015;&#32423;&#21035;&#30340;&#19981;&#21487;&#33021;&#24615;&#25439;&#22833;&#65292;&#20197;&#21450;&#22312;&#22521;&#35757;&#26399;&#38388;&#30340;&#37325;&#22797;&#24809;&#32602;&#12289;&#25512;&#29702;&#21644;&#21518;&#22788;&#29702;&#31561;&#22810;&#23618;&#38754;&#26041;&#27861;&#26469;&#25233;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37325;&#22797;&#65292;&#24182;&#36991;&#20813;&#29983;&#25104;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#22312;NLP&#39046;&#22495;&#26159;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#39046;&#22495;&#20043;&#19968;&#65292;&#36817;&#24180;&#26469;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24102;&#26469;&#30340;&#36827;&#27493;&#24471;&#21040;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#20316;&#20026;&#32534;&#20889;&#21161;&#25163;&#24212;&#29992;&#31243;&#24207;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#23427;&#20204;&#36890;&#24120;&#23481;&#26131;&#22797;&#21046;&#25110;&#25193;&#23637;&#36755;&#20837;&#20013;&#25552;&#20379;&#30340;&#20855;&#26377;&#25915;&#20987;&#24615;&#30340;&#20869;&#23481;&#12290;&#22312;&#20302;&#36164;&#28304;&#25968;&#25454;&#29615;&#22659;&#20013;&#65292;&#23427;&#20204;&#20063;&#21487;&#33021;&#23548;&#33268;&#36755;&#20986;&#37325;&#22797;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31934;&#30830;&#21644;&#38750;&#31934;&#30830;&#37325;&#22797;&#25233;&#21046;&#30340;&#32467;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#26631;&#35760;&#21644;&#24207;&#21015;&#32423;&#21035;&#30340;&#19981;&#21487;&#33021;&#24615;&#25439;&#22833;&#65292;&#22521;&#35757;&#26399;&#38388;&#30340;&#37325;&#22797;&#24809;&#32602;&#12289;&#25512;&#29702;&#21644;&#21518;&#22788;&#29702;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22810;&#32423;&#19981;&#21487;&#33021;&#24615;&#25439;&#22833;&#30340;&#33539;&#22260;&#65292;&#20197;&#36171;&#20104;&#27169;&#22411;&#36991;&#20813;&#20174;&#19968;&#24320;&#22987;&#20135;&#29983;&#25915;&#20987;&#24615;&#35789;&#27719;&#21644;&#30701;&#35821;&#30340;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#22312;&#22810;&#20010;&#24230;&#37327;&#26631;&#20934;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language generation is one of the most impactful fields in NLP, and recent years have witnessed its evolution brought about by large language models (LLMs). As the key instrument for writing assistance applications, they are generally prone to replicating or extending offensive content provided in the input. In low-resource data regime, they can also lead to repetitive outputs (Holtzman et al., 2019) [1]. Usually, offensive content and repetitions are mitigated with post-hoc methods, including n-gram level blocklists, top-k and nucleus sampling. In this paper, we introduce a combination of exact and non-exact repetition suppression using token and sequence level unlikelihood loss, repetition penalty during training, inference, and post-processing respectively. We further explore multi-level unlikelihood loss to the extent that it endows the model with abilities to avoid generating offensive words and phrases from the beginning. Finally, with comprehensive experiments, we demons
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25903;&#25345;&#23450;&#24615;&#20998;&#26512;&#20013;&#30340;&#28436;&#32462;&#32534;&#30721;&#12290;&#36890;&#36807;&#32467;&#21512;GPT-3&#21644;&#19987;&#23478;&#32534;&#20889;&#30340;&#32534;&#30721;&#26412;&#65292;&#30740;&#31350;&#20154;&#21592;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#19982;&#19987;&#23478;&#32534;&#30721;&#32467;&#26524;&#30456;&#36817;&#30340;&#26631;&#35760;&#32467;&#26524;&#65292;&#24182;&#19988;&#36824;&#20801;&#35768;&#36827;&#34892;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#32534;&#30721;&#26412;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.10548</link><description>&lt;p&gt;
&#32467;&#21512;&#32534;&#30721;&#26412;&#21644;GPT-3&#25903;&#25345;&#23450;&#24615;&#20998;&#26512;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Supporting Qualitative Analysis with Large Language Models: Combining Codebook with GPT-3 for Deductive Coding. (arXiv:2304.10548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25903;&#25345;&#23450;&#24615;&#20998;&#26512;&#20013;&#30340;&#28436;&#32462;&#32534;&#30721;&#12290;&#36890;&#36807;&#32467;&#21512;GPT-3&#21644;&#19987;&#23478;&#32534;&#20889;&#30340;&#32534;&#30721;&#26412;&#65292;&#30740;&#31350;&#20154;&#21592;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#19982;&#19987;&#23478;&#32534;&#30721;&#32467;&#26524;&#30456;&#36817;&#30340;&#26631;&#35760;&#32467;&#26524;&#65292;&#24182;&#19988;&#36824;&#20801;&#35768;&#36827;&#34892;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#32534;&#30721;&#26412;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25991;&#26412;&#20869;&#23481;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#36890;&#36807;&#32473;&#25968;&#25454;&#25171;&#19978;&#26631;&#31614;&#25581;&#31034;&#20102;&#20016;&#23500;&#32780;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#65292;&#36825;&#20010;&#36807;&#31243;&#24448;&#24448;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#20154;&#21147;&#36164;&#28304;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20855;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#65292;&#20294;&#30740;&#31350;&#20154;&#21592;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#29616;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#36164;&#28304;&#21644;&#25216;&#26415;&#65292;&#26356;&#19981;&#24517;&#35828;&#25361;&#25112;&#37027;&#20123;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30340;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#20102;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#25903;&#25345;&#28436;&#32462;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21487;&#33021;&#24615;&#12290;&#28436;&#32462;&#32534;&#30721;&#26159;&#23450;&#24615;&#20998;&#26512;&#30340;&#20027;&#35201;&#31867;&#21035;&#20043;&#19968;&#65292;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#39044;&#20808;&#30830;&#23450;&#30340;&#32534;&#30721;&#26412;&#23558;&#25968;&#25454;&#26631;&#35760;&#21040;&#19968;&#32452;&#22266;&#23450;&#30340;&#32534;&#30721;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#22522;&#20110;&#22909;&#22855;&#24515;&#39537;&#21160;&#30340;&#38382;&#39064;&#32534;&#30721;&#20219;&#21153;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#23558;GPT-3&#19982;&#19987;&#23478;&#21046;&#23450;&#30340;&#32534;&#30721;&#26412;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#19987;&#23478;&#32534;&#30721;&#32467;&#26524;&#23454;&#29616;&#20102;&#20844;&#24179;&#21040;&#30456;&#24403;&#22823;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#20801;&#35768;&#26377;&#25928;&#22320;&#36827;&#34892;&#32534;&#30721;&#26412;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#32467;&#21512;LLM&#21644;&#28436;&#32462;&#32534;&#30721;&#26159;&#23450;&#24615;&#20998;&#26512;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#24182;&#20855;&#26377;&#28508;&#22312;&#30340;&#23454;&#36341;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Qualitative analysis of textual contents unpacks rich and valuable information by assigning labels to the data. However, this process is often labor-intensive, particularly when working with large datasets. While recent AI-based tools demonstrate utility, researchers may not have readily available AI resources and expertise, let alone be challenged by the limited generalizability of those task-specific models. In this study, we explored the use of large language models (LLMs) in supporting deductive coding, a major category of qualitative analysis where researchers use pre-determined codebooks to label the data into a fixed set of codes. Instead of training task-specific models, a pre-trained LLM could be used directly for various tasks without fine-tuning through prompt learning. Using a curiosity-driven questions coding task as a case study, we found, by combining GPT-3 with expert-drafted codebooks, our proposed approach achieved fair to substantial agreements with expert-coded resu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#21019;&#24314;&#34913;&#37327;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31185;&#23398;&#29702;&#35299;&#30340;&#22522;&#20934;&#12290;&#20182;&#20204;&#20351;&#29992;&#20102;&#34892;&#20026;&#35266;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#38382;&#39064;&#20197;&#34913;&#37327;&#19981;&#21516;&#27700;&#24179;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27700;&#24179;&#21644;&#26041;&#27861;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.10327</link><description>&lt;p&gt;
&#21521;&#30528;&#20154;&#31867;&#21644;&#26426;&#22120;&#31185;&#23398;&#29702;&#35299;&#30340;&#22522;&#20934;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards a Benchmark for Scientific Understanding in Humans and Machines. (arXiv:2304.10327v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10327
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#21019;&#24314;&#34913;&#37327;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31185;&#23398;&#29702;&#35299;&#30340;&#22522;&#20934;&#12290;&#20182;&#20204;&#20351;&#29992;&#20102;&#34892;&#20026;&#35266;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#38382;&#39064;&#20197;&#34913;&#37327;&#19981;&#21516;&#27700;&#24179;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27700;&#24179;&#21644;&#26041;&#27861;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#29702;&#35299;&#26159;&#31185;&#23398;&#30340;&#22522;&#26412;&#30446;&#26631;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#35299;&#37322;&#19990;&#30028;&#12290;&#30446;&#21069;&#36824;&#27809;&#26377;&#22909;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#20195;&#29702;&#20154;&#30340;&#31185;&#23398;&#29702;&#35299;&#65292;&#26080;&#35770;&#23427;&#20204;&#26159;&#20154;&#31867;&#36824;&#26159;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#32570;&#20047;&#28165;&#26224;&#30340;&#22522;&#20934;&#65292;&#38590;&#20197;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27700;&#24179;&#21644;&#26041;&#27861;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;&#22312;&#27492;&#36335;&#32447;&#22270;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#31185;&#23398;&#21746;&#23398;&#24037;&#20855;&#21019;&#24314;&#31185;&#23398;&#29702;&#35299;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#37319;&#29992;&#34892;&#20026;&#35266;&#24565;&#65292;&#35748;&#20026;&#30495;&#27491;&#30340;&#29702;&#35299;&#24212;&#35813;&#34987;&#35748;&#20026;&#26159;&#25191;&#34892;&#26576;&#20123;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19968;&#32452;&#38382;&#39064;&#26469;&#25193;&#23637;&#36825;&#20010;&#27010;&#24565;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#34913;&#37327;&#19981;&#21516;&#27700;&#24179;&#30340;&#31185;&#23398;&#29702;&#35299;&#65292;&#21253;&#25324;&#20449;&#24687;&#26816;&#32034;&#65292;&#23433;&#25490;&#20449;&#24687;&#20197;&#29983;&#25104;&#35299;&#37322;&#30340;&#33021;&#21147;&#20197;&#21450;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#25512;&#26029;&#20107;&#29289;&#20250;&#26377;&#21738;&#20123;&#19981;&#21516;&#12290;Scientific Understanding Benchmark&#65288;SUB&#65289;&#30001;
&lt;/p&gt;
&lt;p&gt;
Scientific understanding is a fundamental goal of science, allowing us to explain the world. There is currently no good way to measure the scientific understanding of agents, whether these be humans or Artificial Intelligence systems. Without a clear benchmark, it is challenging to evaluate and compare different levels of and approaches to scientific understanding. In this Roadmap, we propose a framework to create a benchmark for scientific understanding, utilizing tools from philosophy of science. We adopt a behavioral notion according to which genuine understanding should be recognized as an ability to perform certain tasks. We extend this notion by considering a set of questions that can gauge different levels of scientific understanding, covering information retrieval, the capability to arrange information to produce an explanation, and the ability to infer how things would be different under different circumstances. The Scientific Understanding Benchmark (SUB), which is formed by 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#36866;&#24212;&#19981;&#21516;&#23376;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#21019;&#24314;&#19968;&#20010;&#20013;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#22312;&#20013;&#25991;&#35821;&#35328;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2304.07987</link><description>&lt;p&gt;
&#20013;&#25991;&#24320;&#25918;&#24335;&#25351;&#20196;&#24191;&#20041;&#35821;&#35328;&#27169;&#22411;&#65306;&#21021;&#27493;&#21457;&#24067;
&lt;/p&gt;
&lt;p&gt;
Chinese Open Instruction Generalist: A Preliminary Release. (arXiv:2304.07987v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#36866;&#24212;&#19981;&#21516;&#23376;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#21019;&#24314;&#19968;&#20010;&#20013;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#22312;&#20013;&#25991;&#35821;&#35328;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#26500;&#24314;&#24191;&#20041;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#38543;&#30528;InstructGPT&#21644;ChatGPT&#30340;&#21457;&#24067;&#65292;&#23427;&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#20844;&#20247;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#33521;&#35821;&#20026;&#22522;&#30784;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#36824;&#26410;&#25506;&#32034;&#33521;&#35821;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#20219;&#21153;&#19978;&#26159;&#21542;&#21487;&#20197;&#20687;&#33521;&#35821;&#20219;&#21153;&#37027;&#26679;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25351;&#20196;&#35843;&#25972;&#26469;&#25191;&#34892;&#65292;&#20197;&#21450;&#25105;&#20204;&#22914;&#20309;&#26500;&#24314;&#25152;&#38656;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#35843;&#25972;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39033;&#30446;&#65292;&#35797;&#22270;&#36890;&#36807;&#36866;&#24212;4&#20010;&#23376;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#37319;&#29992;&#21508;&#31181;&#26041;&#27861;&#21019;&#24314;&#19968;&#20010;&#20013;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#32422;20&#19975;&#20010;&#20013;&#25991;&#25351;&#20196;&#35843;&#25972;&#26679;&#26412;&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#26816;&#26597;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#33521;&#25991;&#21644;&#20013;&#25991;&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#24182;&#23545;&#19968;&#20123;&#28508;&#22312;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31616;&#35201;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning is widely recognized as a key technique for building generalist language models, which has attracted the attention of researchers and the public with the release of InstructGPT~\citep{ouyang2022training} and ChatGPT\footnote{\url{https://chat.openai.com/}}. Despite impressive progress in English-oriented large-scale language models (LLMs), it is still under-explored whether English-based foundation LLMs can perform similarly on multilingual tasks compared to English tasks with well-designed instruction tuning and how we can construct the corpora needed for the tuning.  To remedy this gap, we propose the project as an attempt to create a Chinese instruction dataset by various methods adapted to the intrinsic characteristics of 4 sub-tasks. We collect around 200k Chinese instruction tuning samples, which have been manually checked to guarantee high quality. We also summarize the existing English and Chinese instruction corpora and briefly describe some potential applic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31471;&#21040;&#31471;TTS&#31995;&#32479;&#20013;&#65292;&#21152;&#20837;&#35821;&#35843;&#26029;&#28857;&#39044;&#27979;&#27169;&#22411;&#26159;&#21542;&#26377;&#29992;&#20197;&#21450;&#22914;&#20309;&#34913;&#37327;&#20854;&#26377;&#25928;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#35821;&#35843;&#27169;&#22411;&#39044;&#27979;&#26029;&#28857;&#30340;&#25925;&#20107;&#27604;&#26410;&#20351;&#29992;&#39044;&#27979;&#26029;&#28857;&#30340;&#25925;&#20107;&#26356;&#21463;&#27426;&#36814;&#12290;</title><link>http://arxiv.org/abs/2304.04157</link><description>&lt;p&gt;
&#22312;&#31471;&#21040;&#31471;&#30340;TTS&#31995;&#32479;&#20013;&#65292;&#35828;&#35805;&#20154;&#29420;&#31435;&#35821;&#35843;&#26029;&#28857;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An investigation of speaker independent phrase break models in End-to-End TTS systems. (arXiv:2304.04157v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31471;&#21040;&#31471;TTS&#31995;&#32479;&#20013;&#65292;&#21152;&#20837;&#35821;&#35843;&#26029;&#28857;&#39044;&#27979;&#27169;&#22411;&#26159;&#21542;&#26377;&#29992;&#20197;&#21450;&#22914;&#20309;&#34913;&#37327;&#20854;&#26377;&#25928;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#35821;&#35843;&#27169;&#22411;&#39044;&#27979;&#26029;&#28857;&#30340;&#25925;&#20107;&#27604;&#26410;&#20351;&#29992;&#39044;&#27979;&#26029;&#28857;&#30340;&#25925;&#20107;&#26356;&#21463;&#27426;&#36814;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25105;&#20204;&#23545;&#20110;&#31471;&#21040;&#31471;TTS&#31995;&#32479;&#20013;&#35821;&#35843;&#26029;&#28857;&#39044;&#27979;&#30340;&#30740;&#31350;&#65292;&#30740;&#31350;&#21160;&#26426;&#26159;&#65306;&#65288;&#19968;&#65289;&#22312;&#31471;&#21040;&#31471;TTS&#31995;&#32479;&#20013;&#34701;&#20837;&#26126;&#30830;&#30340;&#35821;&#35843;&#27169;&#22411;&#26159;&#21542;&#26377;&#29992;&#65311;&#65288;&#20108;&#65289;&#22914;&#20309;&#35780;&#20272;&#31471;&#21040;&#31471;TTS&#31995;&#32479;&#30340;&#35821;&#35843;&#27169;&#22411;&#26159;&#21542;&#26377;&#25928;&#65311;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;&#23545;&#20799;&#31461;&#25925;&#20107;&#21512;&#25104;&#30340;&#35821;&#22659;&#19979;&#30701;&#35821;&#26029;&#28857;&#39044;&#27979;&#27169;&#22411;&#30340;&#25928;&#29992;&#21644;&#26377;&#25928;&#24615;&#36827;&#34892;&#35780;&#20272;&#65292;&#20351;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#20026;&#21548;&#20247;&#29702;&#35299;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#21548;&#21147;&#35780;&#20272;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#35757;&#32451;&#30340;&#35821;&#35843;&#27169;&#22411;&#39044;&#27979;&#30701;&#35821;&#26029;&#28857;&#20301;&#32622;&#21512;&#25104;&#30340;&#25925;&#20107;&#27604;&#30452;&#25509;&#21512;&#25104;&#30340;&#25925;&#20107;&#26356;&#21463;&#27426;&#36814;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our work on phrase break prediction in the context of end-to-end TTS systems, motivated by the following questions: (i) Is there any utility in incorporating an explicit phrasing model in an end-to-end TTS system?, and (ii) How do you evaluate the effectiveness of a phrasing model in an end-to-end TTS system? In particular, the utility and effectiveness of phrase break prediction models are evaluated in in the context of childrens story synthesis, using listener comprehension. We show by means of perceptual listening evaluations that there is a clear preference for stories synthesized after predicting the location of phrase breaks using a trained phrasing model, over stories directly synthesized without predicting the location of phrase breaks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#35777;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#37319;&#29992;ChatGPT&#36741;&#21161;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#20855;&#26377;&#19982;&#19987;&#19994;&#32763;&#35793;&#31995;&#32479;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#32763;&#35793;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.02182</link><description>&lt;p&gt;
&#21457;&#25496;ChatGPT&#32763;&#35793;&#30340;&#33021;&#21147;&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of ChatGPT for Translation: An Empirical Study. (arXiv:2304.02182v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#35777;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#37319;&#29992;ChatGPT&#36741;&#21161;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#20855;&#26377;&#19982;&#19987;&#19994;&#32763;&#35793;&#31995;&#32479;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#32763;&#35793;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#24067;&#30340;ChatGPT&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#26426;&#22120;&#32763;&#35793;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#24191;&#27867;&#30740;&#31350;&#30340;&#20219;&#21153;&#65292;&#23427;&#20005;&#37325;&#20381;&#36182;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#22914;&#20309;&#20351;&#29992;ChatGPT&#36741;&#21161;&#26426;&#22120;&#32763;&#35793;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#32763;&#35793;&#20013;&#37319;&#29992;&#20102;&#20960;&#20010;&#32763;&#35793;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#35774;&#35745;&#22909;&#30340;&#32763;&#35793;&#25552;&#31034;&#30340;ChatGPT&#21487;&#20197;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#32763;&#35793;&#20013;&#36798;&#21040;&#19982;&#19987;&#19994;&#32763;&#35793;&#31995;&#32479;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#32763;&#35793;&#19978;&#20005;&#37325;&#28382;&#21518;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#22810;&#20010;&#21442;&#32771;&#25991;&#26412;&#23545;&#32763;&#35793;&#36136;&#37327;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;ChatGPT&#30456;&#23545;&#20110;&#19987;&#19994;&#31995;&#32479;&#34920;&#29616;&#26356;&#21152;&#20248;&#24322;&#12290;&#25105;&#20204;&#36824;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#32763;&#35793;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#26368;&#32456;&#32467;&#26524;&#34920;&#26126;ChatGPT&#33021;&#22815;&#36798;&#21040;&#19982;&#19987;&#19994;&#32763;&#35793;&#31995;&#32479;&#30456;&#23218;&#32654;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently released ChatGPT has demonstrated surprising abilities in natural language understanding and natural language generation. Machine translation is an important and extensively studied task in the field of natural language processing, which heavily relies on the abilities of language understanding and generation. Thus, in this paper, we explore how to assist machine translation with ChatGPT. We adopt several translation prompts on a wide range of translations. Our experimental results show that ChatGPT with designed translation prompts can achieve comparable or better performance over professional translation systems for high-resource language translations but lags behind significantly on low-resource translations. We further evaluate the translation quality using multiple references, and ChatGPT achieves superior performance compared to the professional systems. We also conduct experiments on domain-specific translations, the final results show that ChatGPT is able to compre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#36136;&#37327;&#39184;&#21381;&#35780;&#35770;&#29983;&#25104;&#34394;&#20551;&#35780;&#35770;&#24182;&#24494;&#35843;GPT&#36755;&#20986;&#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39044;&#27979;&#34394;&#20551;&#35780;&#35770;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#39044;&#27979;&#38750;&#31934;&#33521;&#35780;&#35770;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20010;&#32500;&#24230;&#19978;&#23545;&#36825;&#20123;&#35780;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#27492;&#31867;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#26159;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#38754;&#20020;&#30340;&#25345;&#32493;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.07731</link><description>&lt;p&gt;
AI&#23545;&#25239;AI&#65306;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#25171;&#20987;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#39184;&#21381;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Combat AI With AI: Counteract Machine-Generated Fake Restaurant Reviews on Social Media. (arXiv:2302.07731v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#36136;&#37327;&#39184;&#21381;&#35780;&#35770;&#29983;&#25104;&#34394;&#20551;&#35780;&#35770;&#24182;&#24494;&#35843;GPT&#36755;&#20986;&#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39044;&#27979;&#34394;&#20551;&#35780;&#35770;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#39044;&#27979;&#38750;&#31934;&#33521;&#35780;&#35770;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20010;&#32500;&#24230;&#19978;&#23545;&#36825;&#20123;&#35780;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#27492;&#31867;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#26159;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#38754;&#20020;&#30340;&#25345;&#32493;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;GPT&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#21046;&#36896;&#20986;&#38590;&#20197;&#21306;&#20998;&#30340;&#34394;&#20551;&#39038;&#23458;&#35780;&#35770;&#65292;&#20174;&#32780;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26816;&#27979;&#36825;&#20123;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#36896;&#25104;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;Yelp&#39564;&#35777;&#30340;&#39640;&#36136;&#37327;&#30340;&#31934;&#33521;&#39184;&#21381;&#35780;&#35770;&#26469;&#29983;&#25104;OpenAI GPT&#35780;&#35770;&#29983;&#25104;&#22120;&#30340;&#34394;&#20551;&#35780;&#35770;&#65292;&#24182;&#26368;&#32456;&#24494;&#35843;GPT&#36755;&#20986;&#26816;&#27979;&#22120;&#26469;&#39044;&#27979;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#34394;&#20551;&#35780;&#35770;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#27169;&#22411;&#24212;&#29992;&#20110;&#39044;&#27979;&#38750;&#31934;&#33521;&#35780;&#35770;&#65292;&#24182;&#22312;&#20960;&#20010;&#32500;&#24230;&#65288;&#22914;&#35780;&#35770;&#12289;&#29992;&#25143;&#21644;&#39184;&#21381;&#29305;&#24449;&#20197;&#21450;&#20889;&#20316;&#39118;&#26684;&#65289;&#19978;&#35782;&#21035;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#27491;&#22312;&#19981;&#26029;&#38754;&#20020;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#30340;&#25361;&#25112;&#65292;&#23613;&#31649;&#20182;&#20204;&#21487;&#33021;&#23454;&#26045;&#26816;&#27979;&#31995;&#32479;&#20197;&#36807;&#28388;&#20986;&#21487;&#30097;&#30340;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative models such as GPT may be used to fabricate indistinguishable fake customer reviews at a much lower cost, thus posing challenges for social media platforms to detect these machine-generated fake reviews. We propose to leverage the high-quality elite restaurant reviews verified by Yelp to generate fake reviews from the OpenAI GPT review creator and ultimately fine-tune a GPT output detector to predict fake reviews that significantly outperform existing solutions. We further apply the model to predict non-elite reviews and identify the patterns across several dimensions, such as review, user and restaurant characteristics, and writing style. We show that social media platforms are continuously challenged by machine-generated fake reviews, although they may implement detection systems to filter out suspicious reviews.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20998;&#35299;&#22120;&#65292;&#35299;&#20915;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#21644;&#22797;&#26434;&#38382;&#39064;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.13808</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#22810;&#25165;&#22810;&#33402;&#30340;&#20998;&#35299;&#22120;&#65306;&#23558;&#35777;&#25454;&#21644;&#38382;&#39064;&#20998;&#35299;&#20026;&#34920;&#26684;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning. (arXiv:2301.13808v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13808
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20998;&#35299;&#22120;&#65292;&#35299;&#20915;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#21644;&#22797;&#26434;&#38382;&#39064;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#34920;&#26684;&#30340;&#25512;&#29702;&#24050;&#32463;&#22312;&#32467;&#21512;&#28145;&#24230;&#27169;&#22411;&#21644;&#31163;&#25955;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23427;&#38656;&#35201;&#23545;&#33258;&#30001;&#24418;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#21644;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#22522;&#20110;&#34920;&#26684;&#30340;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#20250;&#22312;&#28023;&#37327;&#35777;&#25454;&#65288;&#34920;&#26684;&#65289;&#19978;&#36973;&#36935;&#26174;&#33879;&#30340;&#24615;&#33021;&#36864;&#21270;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#26102;&#20063;&#38754;&#20020;&#22256;&#38590;&#65292;&#22240;&#20026;&#25152;&#38656;&#20449;&#24687;&#20998;&#25955;&#22312;&#19981;&#21516;&#30340;&#20301;&#32622;&#12290;&#20026;&#20102;&#32531;&#35299;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#26377;&#25928;&#30340;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#30340;&#20998;&#35299;&#22120;&#65292;&#23558;&#65288;i&#65289;&#24040;&#22823;&#30340;&#35777;&#25454;&#65288;&#19968;&#20010;&#24040;&#22823;&#30340;&#34920;&#26684;&#65289;&#20998;&#35299;&#25104;&#23376;&#35777;&#25454;&#65288;&#19968;&#20010;&#23567;&#34920;&#26684;&#65289;&#65292;&#20197;&#20943;&#36731;&#26080;&#29992;&#20449;&#24687;&#23545;&#34920;&#26684;&#25512;&#29702;&#30340;&#24178;&#25200;&#65307;&#21644;&#65288;ii&#65289;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#36827;&#34892;&#25991;&#26412;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;LLMs&#20998;&#35299;&#24403;&#21069;&#38382;&#39064;&#28041;&#21450;&#30340;&#35777;&#25454;&#65288;&#34920;&#26684;&#65289;&#65292;&#20445;&#30041;&#30456;&#20851;&#35777;&#25454;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#37096;&#20998;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#23558;&#22797;&#26434;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#26356;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#20197;&#20415;&#26356;&#31934;&#30830;&#22320;&#26816;&#32034;&#27599;&#20010;&#23376;&#38382;&#39064;&#30340;&#30456;&#24212;&#35777;&#25454;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#34920;&#26684;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Table-based reasoning has shown remarkable progress in combining deep models with discrete reasoning, which requires reasoning over both free-form natural language (NL) questions and structured tabular data. However, previous table-based reasoning solutions usually suffer from significant performance degradation on huge evidence (tables). In addition, most existing methods struggle to reason over complex questions since the required information is scattered in different places. To alleviate the above challenges, we exploit large language models (LLMs) as decomposers for effective table-based reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence (a small table) to mitigate the interference of useless information for table reasoning; and (ii) decompose complex questions into simpler sub-questions for text reasoning. Specifically, we first use the LLMs to break down the evidence (tables) involved in the current question, retaining the relevant evidence and excludin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;tieval&#65292;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#20449;&#24687;&#25277;&#21462;&#31995;&#32479;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#23427;&#25552;&#20379;&#20102;&#26631;&#20934;&#30340;&#25509;&#21475;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#20811;&#26381;&#19981;&#21516;&#25968;&#25454;&#38598;&#27880;&#37322;&#20307;&#31995;&#30340;&#24046;&#24322;&#12289;&#35299;&#26512;&#19981;&#21516;&#35821;&#26009;&#24211;&#30340;&#26684;&#24335;&#21644;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20960;&#20010;TIE&#31995;&#32479;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;tieval&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.04643</link><description>&lt;p&gt;
tieval&#65306;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#20449;&#24687;&#25277;&#21462;&#31995;&#32479;&#35780;&#20272;&#30340;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
tieval: An Evaluation Framework for Temporal Information Extraction Systems. (arXiv:2301.04643v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;tieval&#65292;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#20449;&#24687;&#25277;&#21462;&#31995;&#32479;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#23427;&#25552;&#20379;&#20102;&#26631;&#20934;&#30340;&#25509;&#21475;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#20811;&#26381;&#19981;&#21516;&#25968;&#25454;&#38598;&#27880;&#37322;&#20307;&#31995;&#30340;&#24046;&#24322;&#12289;&#35299;&#26512;&#19981;&#21516;&#35821;&#26009;&#24211;&#30340;&#26684;&#24335;&#21644;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20960;&#20010;TIE&#31995;&#32479;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;tieval&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20108;&#21313;&#24180;&#26469;&#65292;&#26102;&#38388;&#20449;&#24687;&#25277;&#21462;(TIE)&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#25512;&#21160;&#20102;&#22823;&#37327;&#25968;&#25454;&#38598;&#30340;&#24320;&#21457;&#12290;&#28982;&#32780;&#65292;&#25317;&#26377;&#22823;&#37327;&#35821;&#26009;&#24211;&#30340;&#22909;&#22788;&#19982;&#27492;&#21516;&#26102;&#20063;&#20351;&#24471;&#23545;TIE&#31995;&#32479;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21464;&#24471;&#22256;&#38590;&#12290;&#19981;&#21516;&#25968;&#25454;&#38598;&#20855;&#26377;&#19981;&#21516;&#30340;&#27880;&#37322;&#20307;&#31995;&#65292;&#36825;&#20351;&#24471;&#27604;&#36739;&#19981;&#21516;&#35821;&#26009;&#24211;&#20013;&#30340;&#31454;&#20105;&#23545;&#25163;&#21464;&#24471;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#35821;&#26009;&#24211;&#36890;&#24120;&#37319;&#29992;&#19981;&#21516;&#30340;&#26684;&#24335;&#36827;&#34892;&#20256;&#25773;&#65292;&#22240;&#27492;&#38656;&#35201;&#30740;&#31350;&#20154;&#21592;/&#20174;&#19994;&#20154;&#21592;&#22312;&#24320;&#21457;&#25152;&#26377;&#35821;&#26009;&#24211;&#30340;&#35299;&#26512;&#22120;&#26102;&#20184;&#20986;&#22823;&#37327;&#30340;&#24037;&#31243;&#21162;&#21147;&#12290;&#36825;&#31181;&#38480;&#21046;&#36843;&#20351;&#30740;&#31350;&#20154;&#21592;&#36873;&#25321;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#20182;&#20204;&#30340;&#31995;&#32479;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#31995;&#32479;&#30340;&#27604;&#36739;&#12290;&#21478;&#19968;&#20010;&#38459;&#30861;TIE&#31995;&#32479;&#21487;&#27604;&#24615;&#30340;&#38556;&#30861;&#26159;&#35780;&#20272;&#25351;&#26631;&#30340;&#37319;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;tieval&#65292;&#19968;&#31181;TIE&#31995;&#32479;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20026;&#35821;&#26009;&#24211;&#35775;&#38382;&#12289;&#40644;&#37329;&#26631;&#20934;&#34920;&#31034;&#21644;&#35780;&#20272;&#25351;&#26631;&#25552;&#20379;&#20102;&#26631;&#20934;&#25509;&#21475;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;tieval&#35780;&#20272;TIE&#31995;&#32479;&#65292;&#24182;&#23545;TIE&#31038;&#21306;&#20013;&#20351;&#29992;&#30340;&#20004;&#20010;&#40644;&#37329;&#26631;&#20934;&#30340;&#20960;&#20010;TIE&#31995;&#32479;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal information extraction (TIE) has attracted a great deal of interest over the last two decades, leading to the development of a significant number of datasets. Despite its benefits, having access to a large volume of corpora makes it difficult when it comes to benchmark TIE systems. On the one hand, different datasets have different annotation schemes, thus hindering the comparison between competitors across different corpora. On the other hand, the fact that each corpus is commonly disseminated in a different format requires a considerable engineering effort for a researcher/practitioner to develop parsers for all of them. This constraint forces researchers to select a limited amount of datasets to evaluate their systems which consequently limits the comparability of the systems. Yet another obstacle that hinders the comparability of the TIE systems is the evaluation metric employed. While most research works adopt traditional metrics such as precision, recall, and $F_1$, a fe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; IKD-MMT &#26694;&#26550;&#26469;&#25903;&#25345;&#26080;&#22270;&#20687;&#25512;&#26029;&#30340;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;&#65292;&#24182;&#36890;&#36807;&#22312;&#28304;&#25991;&#26412;&#20013;&#29983;&#25104;&#22810;&#27169;&#24335;&#29305;&#24449;&#26469;&#23454;&#29616;&#12290;&#35813;&#26694;&#26550;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20026;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#32570;&#20047;&#23545;&#40784;&#22270;&#20687;&#30340;&#26426;&#22120;&#32763;&#35793;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2210.04468</link><description>&lt;p&gt;
&#22833;&#21435;&#30446;&#26631;&#30340;&#22270;&#20687;&#21387;&#32553;&#65306;&#29992;&#21453;&#28436;&#30693;&#35782;&#33976;&#39311;&#25903;&#25345;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;&#30340;&#26080;&#22270;&#20687;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Distill the Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation. (arXiv:2210.04468v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; IKD-MMT &#26694;&#26550;&#26469;&#25903;&#25345;&#26080;&#22270;&#20687;&#25512;&#26029;&#30340;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;&#65292;&#24182;&#36890;&#36807;&#22312;&#28304;&#25991;&#26412;&#20013;&#29983;&#25104;&#22810;&#27169;&#24335;&#29305;&#24449;&#26469;&#23454;&#29616;&#12290;&#35813;&#26694;&#26550;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20026;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#32570;&#20047;&#23545;&#40784;&#22270;&#20687;&#30340;&#26426;&#22120;&#32763;&#35793;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;&#26041;&#38754;&#30340;&#30740;&#31350;&#36890;&#36807;&#34701;&#21512;&#22806;&#26469;&#35270;&#35273;&#20449;&#24687;&#25552;&#21319;&#20102;&#21452;&#35821;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#35201;&#27714;&#23545;&#40784;&#30340; [&#22270;&#20687;&#12289;&#28304;&#25991;&#26412;&#12289;&#30446;&#26631;&#25991;&#26412;] &#24418;&#24335;&#30340;&#22270;&#20687;&#24517;&#39035;&#22312;&#25512;&#26029;&#38454;&#27573;&#25552;&#20379;&#65292;&#36825;&#22312;&#24120;&#35268; NMT &#35774;&#32622;&#30340;&#26080;&#23545;&#40784;&#22270;&#20687;&#24773;&#20917;&#19979;&#29305;&#21035;&#40635;&#28902;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102; IKD-MMT&#65292;&#19968;&#31181;&#25903;&#25345;&#26080;&#22270;&#20687;&#25512;&#26029;&#30340;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#28436;&#30693;&#35782;&#33976;&#39311;&#26041;&#26696;&#25191;&#34892;&#22810;&#27169;&#24335;&#29305;&#24449;&#29983;&#25104;&#22120;&#65292;&#30452;&#25509;&#20174;&#65288;&#21482;&#26377;&#65289;&#28304;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#29983;&#25104;&#22810;&#27169;&#24335;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340; IKD-MMT &#26694;&#26550;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#26080;&#22270;&#20687;&#22522;&#32447;&#65292;&#24182;&#36798;&#21040;&#19982;&#22270;&#20687;&#24517;&#39035;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#20351;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;&#26356;&#21152;&#23454;&#29992;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past works on multimodal machine translation (MMT) elevate bilingual setup by incorporating additional aligned vision information. However, an image-must requirement of the multimodal dataset largely hinders MMT's development -namely that it demands an aligned form of [image, source text, target text]. This limitation is generally troublesome during the inference phase especially when the aligned image is not provided as in the normal NMT setup. Thus, in this work, we introduce IKD-MMT, a novel MMT framework to support the image-free inference phase via an inversion knowledge distillation scheme. In particular, a multimodal feature generator is executed with a knowledge distillation module, which directly generates the multimodal feature from (only) source texts as the input. While there have been a few prior works entertaining the possibility to support image-free inference for machine translation, their performances have yet to rival the image-must translation. In our experiments, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;377808&#39318;&#33521;&#25991;&#27468;&#26354;&#27468;&#35789;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#21450;&#26102;&#30340;&#24615;&#21035;&#27495;&#35270;&#30340;&#22686;&#21152;&#20197;&#21450;&#19981;&#21516;&#24615;&#21035;&#34920;&#28436;&#32773;&#30340;&#35821;&#35328;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2208.02052</link><description>&lt;p&gt;
&#27468;&#35789;&#20013;&#30340;&#24615;&#21035;&#27495;&#35270;&#21644;&#24615;&#21035;&#20559;&#35265;&#30340;&#22823;&#35268;&#27169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Large scale analysis of gender bias and sexism in song lyrics. (arXiv:2208.02052v3 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;377808&#39318;&#33521;&#25991;&#27468;&#26354;&#27468;&#35789;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#21450;&#26102;&#30340;&#24615;&#21035;&#27495;&#35270;&#30340;&#22686;&#21152;&#20197;&#21450;&#19981;&#21516;&#24615;&#21035;&#34920;&#28436;&#32773;&#30340;&#35821;&#35328;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20998;&#26512;&#20102;&#8220;Two Million Song Database&#8221;&#35821;&#26009;&#24211;&#20013;377808&#39318;&#33521;&#25991;&#27468;&#26354;&#27468;&#35789;&#65292;&#30528;&#37325;&#20998;&#26512;&#20102;&#20116;&#21313;&#24180;&#65288;1960-2010&#65289;&#38388;&#24615;&#21035;&#27495;&#35270;&#30340;&#34920;&#36798;&#65292;&#20197;&#21450;&#23545;&#24615;&#21035;&#20559;&#24046;&#30340;&#35780;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#24615;&#21035;&#27495;&#35270;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#22312;&#36739;&#22823;&#30340;&#35268;&#27169;&#19978;&#35782;&#21035;&#20102;&#24615;&#21035;&#27495;&#35270;&#27468;&#35789;&#65292;&#36828;&#36229;&#21069;&#20154;&#29992;&#25163;&#21160;&#26631;&#27880;&#27969;&#34892;&#27468;&#26354;&#30340;&#23567;&#26679;&#26412;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#27468;&#26354;&#27468;&#35789;&#19978;&#23398;&#20064;&#30340;&#35789;&#23884;&#20837;&#26469;&#34913;&#37327;&#20851;&#32852;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23588;&#20854;&#26159;&#30001;&#30007;&#24615;&#33402;&#26415;&#23478;&#28436;&#21809;&#30340;&#27969;&#34892;&#27468;&#26354;&#20013;&#30340;&#24615;&#21035;&#27495;&#35270;&#20869;&#23481;&#22312;&#26102;&#38388;&#19978;&#21576;&#36880;&#28176;&#22686;&#22810;&#30340;&#36235;&#21183;&#12290;&#26681;&#25454;&#34920;&#28436;&#32773;&#30340;&#24615;&#21035;&#19981;&#21516;&#65292;&#27468;&#26354;&#36824;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#35821;&#35328;&#20559;&#35265;&#65292;&#30007;&#24615;&#29420;&#21809;&#33402;&#26415;&#23478;&#30340;&#27468;&#26354;&#20013;&#21253;&#21547;&#26356;&#22810;&#21644;&#26356;&#24378;&#30340;&#20559;&#35265;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#36827;&#34892;&#36825;&#31181;&#22823;&#35268;&#27169;&#30340;&#20998;&#26512;&#65292;&#20026;&#25105;&#20204;&#25581;&#31034;&#20102;&#27969;&#34892;&#25991;&#21270;&#36825;&#19968;&#37325;&#35201;&#37096;&#20998;&#30340;&#35821;&#35328;&#29992;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We employ Natural Language Processing techniques to analyse 377808 English song lyrics from the "Two Million Song Database" corpus, focusing on the expression of sexism across five decades (1960-2010) and the measurement of gender biases. Using a sexism classifier, we identify sexist lyrics at a larger scale than previous studies using small samples of manually annotated popular songs. Furthermore, we reveal gender biases by measuring associations in word embeddings learned on song lyrics. We find sexist content to increase across time, especially from male artists and for popular songs appearing in Billboard charts. Songs are also shown to contain different language biases depending on the gender of the performer, with male solo artist songs containing more and stronger biases. This is the first large scale analysis of this type, giving insights into language usage in such an influential part of popular culture.
&lt;/p&gt;</description></item></channel></rss>