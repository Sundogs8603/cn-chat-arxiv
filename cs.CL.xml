<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;(SVG)&#26684;&#24335;&#22788;&#29702;&#22270;&#20687;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#30452;&#25509;&#29702;&#35299;&#21644;&#25805;&#20316;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#35270;&#35273;&#32452;&#20214;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.06094</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;&#39537;&#21160;&#30340;&#22270;&#20687;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding. (arXiv:2306.06094v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;(SVG)&#26684;&#24335;&#22788;&#29702;&#22270;&#20687;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#30452;&#25509;&#29702;&#35299;&#21644;&#25805;&#20316;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#35270;&#35273;&#32452;&#20214;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#28508;&#21147;&#20173;&#28982;&#24456;&#22823;&#31243;&#24230;&#19978;&#27809;&#26377;&#34987;&#24320;&#21457;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#24615;&#26041;&#27861;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#20351;&#29992;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;(SVG)&#26684;&#24335;&#22788;&#29702;&#22270;&#20687;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;XML&#30340;SVG&#34920;&#36848;&#30340;&#25991;&#26412;&#25551;&#36848;&#32780;&#19981;&#26159;&#20809;&#26629;&#22270;&#20687;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20351;LLMs&#33021;&#22815;&#30452;&#25509;&#29702;&#35299;&#21644;&#25805;&#20316;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#35270;&#35273;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#36827;&#34892;&#31616;&#21333;&#30340;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26041;&#27861;&#22312;&#21028;&#21035;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20248;&#24322;&#34920;&#29616;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#23427;(i)&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#40065;&#26834;&#24615;&#65292;(ii)&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#23454;&#29616;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#20197;&#21450;(iii)&#22270;&#20687;&#26434;&#20081;&#31243;&#24230;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) have made significant advancements in natural language understanding and generation. However, their potential in computer vision remains largely unexplored. In this paper, we introduce a new, exploratory approach that enables LLMs to process images using the Scalable Vector Graphics (SVG) format. By leveraging the XML-based textual descriptions of SVG representations instead of raster images, we aim to bridge the gap between the visual and textual modalities, allowing LLMs to directly understand and manipulate images without the need for parameterized visual components. Our method facilitates simple image classification, generation, and in-context learning using only LLM capabilities. We demonstrate the promise of our approach across discriminative and generative tasks, highlighting its (i) robustness against distribution shift, (ii) substantial improvements achieved by tapping into the in-context learning abilities of LLMs, and (iii) image unders
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22916;&#24819;&#30151;&#65292;&#25552;&#39640;&#23545;&#35805;&#20195;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06085</link><description>&lt;p&gt;
&#20351;&#29992;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#25429;&#25417;LLM&#22916;&#24819;&#30151;
&lt;/p&gt;
&lt;p&gt;
Trapping LLM Hallucinations Using Tagged Context Prompts. (arXiv:2306.06085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22916;&#24819;&#30151;&#65292;&#25552;&#39640;&#23545;&#35805;&#20195;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20363;&#22914;ChatGPT&#65292;&#24050;&#32463;&#23548;&#33268;&#39640;&#24230;&#22797;&#26434;&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#21463;&#21040;&#8220;&#22916;&#24819;&#30151;&#8221;&#30340;&#22256;&#25200;&#65292;&#21363;&#27169;&#22411;&#29983;&#25104;&#34394;&#20551;&#25110;&#25423;&#36896;&#30340;&#20449;&#24687;&#12290;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#38750;&#24120;&#20851;&#38190;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#20010;&#39046;&#22495;&#37319;&#29992;AI&#39537;&#21160;&#30340;&#24179;&#21488;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#26631;&#35760;LLM&#22312;&#20854;&#39046;&#22495;&#30693;&#35782;&#33539;&#22260;&#20043;&#22806;&#30340;&#38382;&#39064;&#65292;&#30830;&#20445;&#29992;&#25143;&#33719;&#24471;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32467;&#21512;&#23884;&#20837;&#30340;&#26631;&#35760;&#21644;&#19978;&#19979;&#25991;&#26469;&#23545;&#25239;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22916;&#24819;&#30151;&#21487;&#20197;&#25104;&#21151;&#22320;&#24212;&#23545;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#29983;&#25104;&#30340;URL&#20316;&#20026;&#26131;&#20110;&#27979;&#35797;&#30340;&#25423;&#36896;&#25968;&#25454;&#25351;&#26631;&#65292;&#22312;&#27809;&#26377;&#19978;&#19979;&#25991;&#25552;&#31034;-&#21709;&#24212;&#23545;&#30340;&#24773;&#20917;&#19979;&#22522;&#32447;&#20381;&#36182;&#24615;&#22916;&#24819;&#39057;&#29575;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#20026;&#27979;&#35797;&#30340;&#29983;&#25104;&#24341;&#25806;&#25552;&#20379;&#38382;&#39064;&#25552;&#31034;&#26102;&#25552;&#20379;&#19978;&#19979;&#25991;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24635;&#20307;&#22916;&#24819;&#30151;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#19978;&#19979;&#25991;&#25552;&#31034;&#20013;&#25918;&#32622;&#26631;&#35760;&#22914;&#20309;&#24433;&#21709;&#26816;&#27979;&#22916;&#24819;&#30151;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;LLM&#20013;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#25552;&#39640;&#20102;&#26816;&#27979;&#22916;&#24819;&#30151;&#30340;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs), such as ChatGPT, have led to highly sophisticated conversation agents. However, these models suffer from "hallucinations," where the model generates false or fabricated information. Addressing this challenge is crucial, particularly with AI-driven platforms being adopted across various sectors. In this paper, we propose a novel method to recognize and flag instances when LLMs perform outside their domain knowledge, and ensuring users receive accurate information.  We find that the use of context combined with embedded tags can successfully combat hallucinations within generative language models. To do this, we baseline hallucination frequency in no-context prompt-response pairs using generated URLs as easily-tested indicators of fabricated data. We observed a significant reduction in overall hallucination when context was supplied along with question prompts for tested generative engines. Lastly, we evaluated how placing tags within cont
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#25552;&#39640;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#20197;&#23545;&#25152;&#26377;&#20154;&#21475;&#32479;&#35745;&#23398;&#26041;&#38754;&#37117;&#26377;&#25552;&#39640;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.06083</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#25552;&#39640;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Fairness and Robustness in End-to-End Speech Recognition through unsupervised clustering. (arXiv:2306.06083v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06083
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#25552;&#39640;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#20197;&#23545;&#25152;&#26377;&#20154;&#21475;&#32479;&#35745;&#23398;&#26041;&#38754;&#37117;&#26377;&#25552;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#19981;&#33021;&#20026;&#25152;&#26377;&#20154;&#32676;&#23376;&#32452;&#32455;&#25552;&#20379;&#21516;&#26679;&#20986;&#33394;&#30340;&#24615;&#33021;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#20844;&#24179;&#24615;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#34429;&#28982;&#22312;&#25972;&#20307;&#35821;&#38899;&#35782;&#21035;&#36136;&#37327;&#26041;&#38754;&#24050;&#32463;&#26377;&#24456;&#22810;&#25913;&#36827;&#65292;&#20294;&#24182;&#27809;&#26377;&#29305;&#21035;&#20851;&#27880;&#20026;&#31995;&#32479;&#26080;&#27861;&#34920;&#29616;&#33391;&#22909;&#30340;&#25152;&#26377;&#29992;&#25143;&#32452;&#25512;&#36827;&#20844;&#24179;&#21644;&#24179;&#31561;&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;ASR&#30340;&#20844;&#24179;&#24615;&#20063;&#26159;&#19968;&#20010;&#40065;&#26834;&#24615;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25968;&#25454;&#38544;&#31169;&#22312;&#29983;&#20135;&#31995;&#32479;&#20013;&#20063;&#26159;&#39318;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20803;&#25968;&#25454;&#12289;&#37038;&#25919;&#32534;&#30721;&#29978;&#33267;&#19981;&#30452;&#25509;&#20351;&#29992;&#35828;&#35805;&#20154;&#25110;&#35805;&#35821;&#23884;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#31471;&#21040;&#31471;ASR&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#35828;&#35805;&#20154;ID&#27169;&#22411;&#26469;&#25552;&#21462;&#35805;&#35821;&#32423;&#21035;&#30340;&#23884;&#20837;&#65292;&#28982;&#21518;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20351;&#29992;&#23427;&#20204;&#26469;&#21019;&#24314;&#22768;&#23398;&#32858;&#31867;&#12290;&#25105;&#20204;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#32858;&#31867;ID&#32780;&#19981;&#26159;&#35828;&#35805;&#20154;&#35805;&#35821;&#23884;&#20837;&#20316;&#20026;&#39069;&#22806;&#30340;&#29305;&#24449;&#65292;&#36825;&#34920;&#29616;&#20986;&#20102;&#23545;&#25152;&#26377;&#20154;&#21475;&#32479;&#35745;&#23398;&#26041;&#38754;&#37117;&#26377;&#25552;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge of fairness arises when Automatic Speech Recognition (ASR) systems do not perform equally well for all sub-groups of the population. In the past few years there have been many improvements in overall speech recognition quality, but without any particular focus on advancing Equality and Equity for all user groups for whom systems do not perform well. ASR fairness is therefore also a robustness issue. Meanwhile, data privacy also takes priority in production systems. In this paper, we present a privacy preserving approach to improve fairness and robustness of end-to-end ASR without using metadata, zip codes, or even speaker or utterance embeddings directly in training. We extract utterance level embeddings using a speaker ID model trained on a public dataset, which we then use in an unsupervised fashion to create acoustic clusters. We use cluster IDs instead of speaker utterance embeddings as extra features during model training, which shows improvements for all demographic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;V-GLOSS&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.06077</link><description>&lt;p&gt;
&#35270;&#35273;&#35789;&#27719;&#25551;&#36848;&#25552;&#21319;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Visually-Grounded Descriptions Improve Zero-Shot Image Classification. (arXiv:2306.06077v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;V-GLOSS&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#22914;CLIP&#22312;&#38646;&#26679;&#26412;&#35270;&#35273;&#20219;&#21153;&#65288;&#20363;&#22914;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;ZSIC&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#20855;&#20307;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#31867;&#21035;&#25551;&#36848;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#31890;&#24230;&#21644;&#26631;&#31614;&#27495;&#20041;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;V-GLOSS&#65306;Visual Glosses&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#26469;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#20934;ZSIC&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;ImageNet&#21644;STL-10&#65289;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#26469;&#23637;&#31034;V-GLOSS&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30001;V-GLOSS&#29983;&#25104;&#30340;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20854;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#28304;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language-vision models like CLIP have made significant progress in zero-shot vision tasks, such as zero-shot image classification (ZSIC). However, generating specific and expressive class descriptions remains a major challenge. Existing approaches suffer from granularity and label ambiguity issues. To tackle these challenges, we propose V-GLOSS: Visual Glosses, a novel method leveraging modern language models and semantic knowledge bases to produce visually-grounded class descriptions. We demonstrate V-GLOSS's effectiveness by achieving state-of-the-art results on benchmark ZSIC datasets including ImageNet and STL-10. In addition, we introduce a silver dataset with class descriptions generated by V-GLOSS, and show its usefulness for vision tasks. We make available our code and dataset.
&lt;/p&gt;</description></item><item><title>Mind2Web&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#36890;&#29992;Web&#20195;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#25353;&#29031;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20219;&#24847;&#32593;&#31449;&#19978;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#27492;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19977;&#20010;&#24517;&#35201;&#20803;&#32032;&#65306;1&#65289;&#22810;&#26679;&#30340;&#39046;&#22495;&#12289;&#32593;&#31449;&#21644;&#20219;&#21153;&#65292;2&#65289;&#20351;&#29992;&#30495;&#23454;&#32593;&#31449;&#32780;&#38750;&#27169;&#25311;&#21644;&#31616;&#21270;&#32593;&#31449;&#65292;3&#65289;&#24191;&#27867;&#30340;&#29992;&#25143;&#20132;&#20114;&#27169;&#24335;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;Mind2Web&#36827;&#34892;&#20102;&#21021;&#27493;&#25506;&#32034;&#65292;&#20351;&#29992;&#23567;&#22411;LM&#36807;&#28388;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20219;&#21153;&#23436;&#25104;&#29575;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.06070</link><description>&lt;p&gt;
Mind2Web&#65306;&#38754;&#21521;Web&#30340;&#36890;&#29992;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Mind2Web: Towards a Generalist Agent for the Web. (arXiv:2306.06070v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06070
&lt;/p&gt;
&lt;p&gt;
Mind2Web&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#36890;&#29992;Web&#20195;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#25353;&#29031;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20219;&#24847;&#32593;&#31449;&#19978;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#27492;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19977;&#20010;&#24517;&#35201;&#20803;&#32032;&#65306;1&#65289;&#22810;&#26679;&#30340;&#39046;&#22495;&#12289;&#32593;&#31449;&#21644;&#20219;&#21153;&#65292;2&#65289;&#20351;&#29992;&#30495;&#23454;&#32593;&#31449;&#32780;&#38750;&#27169;&#25311;&#21644;&#31616;&#21270;&#32593;&#31449;&#65292;3&#65289;&#24191;&#27867;&#30340;&#29992;&#25143;&#20132;&#20114;&#27169;&#24335;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;Mind2Web&#36827;&#34892;&#20102;&#21021;&#27493;&#25506;&#32034;&#65292;&#20351;&#29992;&#23567;&#22411;LM&#36807;&#28388;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20219;&#21153;&#23436;&#25104;&#29575;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Mind2Web&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#36890;&#29992;Web&#20195;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#25353;&#29031;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20219;&#24847;&#32593;&#31449;&#19978;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;Web&#20195;&#29702;&#25968;&#25454;&#38598;&#35201;&#20040;&#20351;&#29992;&#27169;&#25311;&#32593;&#31449;&#65292;&#35201;&#20040;&#20165;&#35206;&#30422;&#26377;&#38480;&#30340;&#32593;&#31449;&#21644;&#20219;&#21153;&#65292;&#22240;&#27492;&#19981;&#36866;&#21512;&#36890;&#29992;Web&#20195;&#29702;&#12290;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;31&#20010;&#39046;&#22495;&#12289;137&#20010;&#32593;&#31449;&#30340;&#36229;&#36807;2,000&#20010;&#24320;&#25918;&#24335;&#20219;&#21153;&#21644;&#20219;&#21153;&#30340;&#20247;&#21253;&#25805;&#20316;&#24207;&#21015;&#65292;Mind2Web&#20026;&#26500;&#24314;&#36890;&#29992;Web&#20195;&#29702;&#25552;&#20379;&#20102;&#19977;&#20010;&#24517;&#35201;&#20803;&#32032;&#65306;1&#65289;&#22810;&#26679;&#30340;&#39046;&#22495;&#12289;&#32593;&#31449;&#21644;&#20219;&#21153;&#65292;2&#65289;&#20351;&#29992;&#30495;&#23454;&#32593;&#31449;&#32780;&#38750;&#27169;&#25311;&#21644;&#31616;&#21270;&#32593;&#31449;&#65292;3&#65289;&#24191;&#27867;&#30340;&#29992;&#25143;&#20132;&#20114;&#27169;&#24335;&#12290;&#22522;&#20110;Mind2Web&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#36890;&#29992;Web&#20195;&#29702;&#30340;&#21021;&#27493;&#25506;&#32034;&#12290;&#34429;&#28982;&#29616;&#23454;&#19990;&#30028;&#32593;&#31449;&#30340;&#21407;&#22987;HTML&#24448;&#24448;&#22826;&#22823;&#32780;&#26080;&#27861;&#25552;&#20379;&#32473;LLMs&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#39318;&#20808;&#29992;&#23567;&#22411;LM&#36807;&#28388;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20219;&#21153;&#23436;&#25104;&#29575;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;Trans-Lingual Definition Generation (TLDG)&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;&#27604;&#25552;&#31034;&#23398;&#20064;&#23454;&#29616;&#30340;&#33258;&#21160;&#36328;&#35821;&#35328;&#23450;&#20041;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#27597;&#35821;&#20351;&#29992;&#32773;&#30340;&#35821;&#35328;&#30340;&#23450;&#20041;&#65292;&#21327;&#21161;&#35821;&#35328;&#23398;&#20064;&#32773;&#29702;&#35299;&#29983;&#20731;&#35789;&#27719;&#65292;&#23454;&#39564;&#35777;&#26126;&#23545;&#20110;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#36328;&#35821;&#35328;&#23450;&#20041;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06058</link><description>&lt;p&gt;
&#21327;&#21161;&#35821;&#35328;&#23398;&#20064;&#32773;&#65306;&#36890;&#36807;&#23545;&#27604;&#25552;&#31034;&#23398;&#20064;&#23454;&#29616;&#30340;&#33258;&#21160;&#36328;&#35821;&#35328;&#23450;&#20041;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Assisting Language Learners: Automated Trans-Lingual Definition Generation via Contrastive Prompt Learning. (arXiv:2306.06058v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;Trans-Lingual Definition Generation (TLDG)&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;&#27604;&#25552;&#31034;&#23398;&#20064;&#23454;&#29616;&#30340;&#33258;&#21160;&#36328;&#35821;&#35328;&#23450;&#20041;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#27597;&#35821;&#20351;&#29992;&#32773;&#30340;&#35821;&#35328;&#30340;&#23450;&#20041;&#65292;&#21327;&#21161;&#35821;&#35328;&#23398;&#20064;&#32773;&#29702;&#35299;&#29983;&#20731;&#35789;&#27719;&#65292;&#23454;&#39564;&#35777;&#26126;&#23545;&#20110;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#36328;&#35821;&#35328;&#23450;&#20041;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#23450;&#20041;&#29983;&#25104;&#20219;&#21153;&#35201;&#27714;&#33258;&#21160;&#29983;&#25104;&#21333;&#35821;&#35328;&#23450;&#20041;&#65288;&#20363;&#22914;&#65292;&#33521;&#25991;&#21333;&#35789;&#30340;&#33521;&#25991;&#23450;&#20041;&#65289;&#65292;&#20294;&#24573;&#30053;&#20102;&#29983;&#25104;&#30340;&#23450;&#20041;&#23545;&#20110;&#35821;&#35328;&#23398;&#20064;&#32773;&#21516;&#26679;&#23384;&#22312;&#29983;&#20731;&#35789;&#27719;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#36328;&#35821;&#35328;&#23450;&#20041;&#29983;&#25104;&#65288;TLDG&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#29983;&#25104;&#21478;&#19968;&#31181;&#35821;&#35328;&#30340;&#23450;&#20041;&#65292;&#21363;&#27597;&#35821;&#20351;&#29992;&#32773;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#39318;&#20808;&#25506;&#32034;&#20102;&#36825;&#39033;&#20219;&#21153;&#30340;&#38750;&#30417;&#30563;&#26041;&#24335;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#23454;&#29616;&#26041;&#27861;&#65292;&#21363;&#24494;&#35843;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;Prompt Combination&#21644;Contrastive Prompt Learning&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#22522;&#32447;&#31649;&#36947;&#26041;&#27861;&#22312;&#20016;&#23500;&#21644;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#35777;&#26126;&#20102;&#23427;&#22312;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#36328;&#35821;&#35328;&#23450;&#20041;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard definition generation task requires to automatically produce mono-lingual definitions (e.g., English definitions for English words), but ignores that the generated definitions may also consist of unfamiliar words for language learners. In this work, we propose a novel task of Trans-Lingual Definition Generation (TLDG), which aims to generate definitions in another language, i.e., the native speaker's language. Initially, we explore the unsupervised manner of this task and build up a simple implementation of fine-tuning the multi-lingual machine translation model. Then, we develop two novel methods, Prompt Combination and Contrastive Prompt Learning, for further enhancing the quality of the generation. Our methods are evaluated against the baseline Pipeline method in both rich- and low-resource settings, and we empirically establish its superiority in generating higher-quality trans-lingual definitions.
&lt;/p&gt;</description></item><item><title>FinGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#21487;&#35775;&#38382;&#21644;&#36879;&#26126;&#30340;&#36164;&#28304;&#26469;&#24320;&#21457;&#37329;&#34701;LLMs&#65292;&#20854;&#37325;&#35201;&#24615;&#22312;&#20110;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#31649;&#36947;&#21644;&#36731;&#37327;&#32423;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2306.06031</link><description>&lt;p&gt;
FinGPT&#65306;&#24320;&#28304;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinGPT: Open-Source Financial Large Language Models. (arXiv:2306.06031v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06031
&lt;/p&gt;
&lt;p&gt;
FinGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#21487;&#35775;&#38382;&#21644;&#36879;&#26126;&#30340;&#36164;&#28304;&#26469;&#24320;&#21457;&#37329;&#34701;LLMs&#65292;&#20854;&#37325;&#35201;&#24615;&#22312;&#20110;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#31649;&#36947;&#21644;&#36731;&#37327;&#32423;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#21508;&#20010;&#39046;&#22495;&#38761;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#28508;&#21147;&#65292;&#24341;&#36215;&#20102;&#37329;&#34701;&#39046;&#22495;&#30340;&#27987;&#21402;&#20852;&#36259;&#12290;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#37329;&#34701;&#25968;&#25454;&#26159;&#37329;&#34701;LLMs&#65288;FinLLMs&#65289;&#30340;&#31532;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#37329;&#34701;&#39046;&#22495;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;FinGPT&#12290;&#19982;&#19987;&#26377;&#27169;&#22411;&#19981;&#21516;&#65292;FinGPT&#37319;&#29992;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#21487;&#35775;&#38382;&#21644;&#36879;&#26126;&#30340;&#36164;&#28304;&#26469;&#24320;&#21457;&#20182;&#20204;&#30340;&#37329;&#34701;LLMs&#12290;&#25105;&#20204;&#24378;&#35843;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#31649;&#36947;&#21644;&#36731;&#37327;&#32423;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#22312;&#24314;&#31435;FinGPT&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#20010;&#28508;&#22312;&#30340;&#24212;&#29992;&#20316;&#20026;&#29992;&#25143;&#30340;&#22522;&#30784;&#65292;&#22914;&#26426;&#22120;&#39038;&#38382;&#12289;&#31639;&#27861;&#20132;&#26131;&#21644;&#35770; &#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown the potential of revolutionizing natural language processing tasks in diverse domains, sparking great interest in finance. Accessing high-quality financial data is the first challenge for financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize Internet-scale financial data.  In this paper, we present an open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a data-centric approach, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we showcase several potential applications as stepping stones for users, such as robo-advising, algorithmic trading, and l
&lt;/p&gt;</description></item><item><title>HiTZ@Antidote&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#35770;&#35777;&#29702;&#35770;&#65292;&#25552;&#20379;&#25968;&#23383;&#21307;&#30103;&#39046;&#22495;&#39640;&#36136;&#37327;&#30340;&#21487;&#35299;&#37322;AI&#39044;&#27979;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.06029</link><description>&lt;p&gt;
HiTZ@Antidote: &#38754;&#21521;&#25968;&#23383;&#21307;&#30103;&#30340;&#22522;&#20110;&#35770;&#35777;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
HiTZ@Antidote: Argumentation-driven Explainable Artificial Intelligence for Digital Medicine. (arXiv:2306.06029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06029
&lt;/p&gt;
&lt;p&gt;
HiTZ@Antidote&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#35770;&#35777;&#29702;&#35770;&#65292;&#25552;&#20379;&#25968;&#23383;&#21307;&#30103;&#39046;&#22495;&#39640;&#36136;&#37327;&#30340;&#21487;&#35299;&#37322;AI&#39044;&#27979;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;AI&#39044;&#27979;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#20219;&#21153;&#12290;&#35201;&#20351;&#20854;&#27491;&#24120;&#24037;&#20316;&#65292;&#38656;&#35201;&#36873;&#25321;&#36866;&#24403;&#30340;&#35299;&#37322;&#27867;&#21270;/&#32454;&#21270;&#27700;&#24179;&#65307;&#32771;&#34385;&#35299;&#37322;&#21463;&#30410;&#32773;&#23545;&#25152;&#32771;&#34385;&#30340;AI&#20219;&#21153;&#30340;&#29087;&#24713;&#31243;&#24230;&#30340;&#20551;&#35774;&#65307;&#28041;&#21450;&#21040;&#23545;&#20419;&#25104;&#20915;&#31574;&#30340;&#20855;&#20307;&#20803;&#32032;&#30340;&#24341;&#29992;&#65307;&#21033;&#29992;&#21487;&#33021;&#19981;&#26159;&#39044;&#27979;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#30340;&#20854;&#20182;&#30693;&#35782;&#65288;&#20363;&#22914;&#19987;&#23478;&#35777;&#25454;&#65289;&#65307;&#24182;&#20197;&#28165;&#26224;&#26131;&#25026;&#12289;&#21487;&#33021;&#20855;&#35828;&#26381;&#21147;&#30340;&#26041;&#24335;&#34920;&#36848;&#35299;&#37322;&#12290;&#37492;&#20110;&#36825;&#20123;&#32771;&#34385;&#22240;&#32032;&#65292;ANTIDOTE&#22312;&#21487;&#35299;&#37322;AI&#26041;&#38754;&#22521;&#32946;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#35270;&#35282;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#36807;&#31243;&#30340;&#20302;&#32423;&#29305;&#24449;&#19982;&#20154;&#31867;&#35770;&#35777;&#33021;&#21147;&#30340;&#26356;&#39640;&#32423;&#21035;&#26041;&#26696;&#30456;&#32467;&#21512;&#12290;ANTIDOTE&#23558;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#35770;&#35777;&#29702;&#35770;&#30340;&#36328;&#23398;&#31185;&#31454;&#20105;&#20248;&#21183;&#65292;&#35299;&#20915;&#25968;&#23383;&#21307;&#23398;&#39046;&#22495;&#25552;&#20379;&#39640;&#36136;&#37327;AI&#39044;&#27979;&#35299;&#37322;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing high quality explanations for AI predictions based on machine learning is a challenging and complex task. To work well it requires, among other factors: selecting a proper level of generality/specificity of the explanation; considering assumptions about the familiarity of the explanation beneficiary with the AI task under consideration; referring to specific elements that have contributed to the decision; making use of additional knowledge (e.g. expert evidence) which might not be part of the prediction process; and providing evidence supporting negative hypothesis. Finally, the system needs to formulate the explanation in a clearly interpretable, and possibly convincing, way. Given these considerations, ANTIDOTE fosters an integrated vision of explainable AI, where low-level characteristics of the deep learning process are combined with higher level schemes proper of the human argumentation capacity. ANTIDOTE will exploit cross-disciplinary competences in deep learning and a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CheXpert&#26631;&#31614;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#24369;&#30417;&#30563;&#65292;&#26174;&#30528;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#65292;&#22312;&#33258;&#21160;&#26631;&#27880;&#24503;&#35821;&#33016;&#37096;X&#23556;&#32447;&#21307;&#23398;&#25253;&#21578;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.05997</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24503;&#35821;&#33016;&#37096;X&#23556;&#32447;&#21307;&#23398;&#25253;&#21578;&#33258;&#21160;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Automated Labeling of German Chest X-Ray Radiology Reports using Deep Learning. (arXiv:2306.05997v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CheXpert&#26631;&#31614;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#24369;&#30417;&#30563;&#65292;&#26174;&#30528;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#65292;&#22312;&#33258;&#21160;&#26631;&#27880;&#24503;&#35821;&#33016;&#37096;X&#23556;&#32447;&#21307;&#23398;&#25253;&#21578;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#33539;&#22260;&#20869;&#25918;&#23556;&#31185;&#21307;&#29983;&#30701;&#32570;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22521;&#35757;&#36825;&#26679;&#30340;&#27169;&#22411;&#24448;&#24448;&#38656;&#35201;&#32791;&#36153;&#26114;&#36149;&#21644;&#32791;&#26102;&#30340;&#25163;&#21160;&#26631;&#35760;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#33258;&#21160;&#25552;&#21462;&#26631;&#31614;&#21487;&#20197;&#20943;&#23569;&#33719;&#24471;&#26631;&#35760;&#25968;&#25454;&#38598;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#20294;&#30001;&#20110;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#35789;&#21644;&#32570;&#23569;&#27880;&#37322;&#25968;&#25454;&#32780;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26631;&#31614;&#22120;&#30340;&#24369;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26631;&#31614;&#39044;&#27979;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CheXpert&#26631;&#31614;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#30001;&#22522;&#20110;&#35268;&#21017;&#30340;&#24503;&#35821;CheXpert&#27169;&#22411;&#26631;&#35760;&#30340;&#25253;&#21578;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#23569;&#37327;&#25163;&#21160;&#26631;&#35760;&#30340;&#25253;&#21578;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#25152;&#26377;&#19977;&#20010;&#20219;&#21153;&#19978;&#26174;&#30528;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radiologists are in short supply globally, and deep learning models offer a promising solution to address this shortage as part of clinical decision-support systems. However, training such models often requires expensive and time-consuming manual labeling of large datasets. Automatic label extraction from radiology reports can reduce the time required to obtain labeled datasets, but this task is challenging due to semantically similar words and missing annotated data. In this work, we explore the potential of weak supervision of a deep learning-based label prediction model, using a rule-based labeler. We propose a deep learning-based CheXpert label prediction model, pre-trained on reports labeled by a rule-based German CheXpert model and fine-tuned on a small dataset of manually labeled reports. Our results demonstrate the effectiveness of our approach, which significantly outperformed the rule-based model on all three tasks. Our findings highlight the benefits of employing deep learni
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20998;&#24067;&#29305;&#24615;&#23398;&#20064;&#35268;&#21017;&#30340;&#20363;&#22806;&#24773;&#20917;&#65292;&#36890;&#36807;&#30740;&#31350;&#21160;&#35789;&#22312;&#20027;&#21160;&#35821;&#24577;&#21644;&#34987;&#21160;&#35821;&#24577;&#20013;&#30340;&#30456;&#23545;&#21487;&#25509;&#21463;&#24615;&#21644;&#30456;&#23545;&#20986;&#29616;&#39057;&#29575;&#30340;&#27491;&#30456;&#20851;&#24615;&#36827;&#34892;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.05969</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#35821;&#27861;&#35268;&#21017;&#30340;&#20363;&#22806;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Language Models Can Learn Exceptions to Syntactic Rules. (arXiv:2306.05969v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05969
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20998;&#24067;&#29305;&#24615;&#23398;&#20064;&#35268;&#21017;&#30340;&#20363;&#22806;&#24773;&#20917;&#65292;&#36890;&#36807;&#30740;&#31350;&#21160;&#35789;&#22312;&#20027;&#21160;&#35821;&#24577;&#21644;&#34987;&#21160;&#35821;&#24577;&#20013;&#30340;&#30456;&#23545;&#21487;&#25509;&#21463;&#24615;&#21644;&#30456;&#23545;&#20986;&#29616;&#39057;&#29575;&#30340;&#27491;&#30456;&#20851;&#24615;&#36827;&#34892;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#29983;&#20135;&#24615;&#22320;&#27010;&#25324;&#26032;&#30340;&#19978;&#19979;&#25991;&#12290;&#23427;&#20204;&#33021;&#21542;&#20063;&#23398;&#20064;&#36825;&#20123;&#29983;&#20135;&#24615;&#35268;&#21017;&#30340;&#20363;&#22806;&#24773;&#20917;&#65311;&#25105;&#20204;&#20197;&#33521;&#35821;&#34987;&#21160;&#24335;&#21477;&#27861;&#38480;&#21046;&#20026;&#20363;&#65288;&#20363;&#22914;&#65292;&#8220;The vacation lasted five days&#8221;&#26159;&#35821;&#27861;&#27491;&#30830;&#30340;&#65292;&#20294;&#8220;*Five days was lasted by the vacation&#8221;&#19981;&#26159;&#65289;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#22810;&#31181;&#21160;&#35789;&#26684;&#30340;&#20154;&#31867;&#25509;&#21463;&#24230;&#21028;&#26029;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;GPT-2&#23450;&#20041;&#30340;&#27010;&#29575;&#20998;&#24067;&#19982;&#20154;&#30340;&#25509;&#21463;&#24230;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#20010;&#21160;&#35789;&#22312;&#20027;&#21160;&#35821;&#24577;&#21644;&#34987;&#21160;&#35821;&#24577;&#20013;&#30340;&#30456;&#23545;&#21487;&#25509;&#21463;&#24615;&#19982;&#20854;&#22312;&#36825;&#20123;&#35821;&#24577;&#20013;&#30340;&#30456;&#23545;&#20986;&#29616;&#39057;&#29575;&#21576;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;&#36825;&#20123;&#32467;&#26524;&#21021;&#27493;&#25903;&#25345;&#25166;&#26681;&#20551;&#35774;&#65292;&#21363;&#23398;&#20064;&#32773;&#33021;&#22815;&#36319;&#36394;&#21644;&#20351;&#29992;&#20182;&#20204;&#36755;&#20837;&#30340;&#20998;&#24067;&#29305;&#24615;&#65292;&#20197;&#23398;&#20064;&#35268;&#21017;&#30340;&#20363;&#22806;&#24773;&#20917;&#12290;&#21516;&#26102;&#65292;&#36825;&#20010;&#20551;&#35774;&#26080;&#27861;&#35299;&#37322;&#36127;&#38754;&#20363;&#22806;&#24773;&#20917;&#30340;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks can generalize productively to novel contexts. Can they also learn exceptions to those productive rules? We explore this question using the case of restrictions on English passivization (e.g., the fact that "The vacation lasted five days" is grammatical, but "*Five days was lasted by the vacation" is not). We collect human acceptability judgments for passive sentences with a range of verbs, and show that the probability distribution defined by GPT-2, a language model, matches the human judgments with high correlation. We also show that the relative acceptability of a verb in the active vs. passive voice is positively correlated with the relative frequency of its occurrence in those voices. These results provide preliminary support for the entrenchment hypothesis, according to which learners track and uses the distributional properties of their input to learn negative exceptions to rules. At the same time, this hypothesis fails to explain the magnitude of unpa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24490;&#29615;&#34701;&#21512;&#25193;&#24352;&#21367;&#31215;&#21644;&#36890;&#36947;&#27880;&#24847;&#21147;&#30340;&#39640;&#25928;&#35821;&#38899;&#20998;&#31163;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#32467;&#26500;&#20026;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#34920;&#29616;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#21487;&#20316;&#20026;&#24403;&#21069;&#20027;&#27969;&#27169;&#22411;&#30340;&#26377;&#25928;&#26367;&#20195;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05887</link><description>&lt;p&gt;
&#22522;&#20110;&#24490;&#29615;&#34701;&#21512;&#25193;&#24352;&#21367;&#31215;&#21644;&#36890;&#36947;&#27880;&#24847;&#21147;&#30340;&#39640;&#25928;&#35821;&#38899;&#20998;&#31163;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
An Efficient Speech Separation Network Based on Recurrent Fusion Dilated Convolution and Channel Attention. (arXiv:2306.05887v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24490;&#29615;&#34701;&#21512;&#25193;&#24352;&#21367;&#31215;&#21644;&#36890;&#36947;&#27880;&#24847;&#21147;&#30340;&#39640;&#25928;&#35821;&#38899;&#20998;&#31163;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#32467;&#26500;&#20026;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#34920;&#29616;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#21487;&#20316;&#20026;&#24403;&#21069;&#20027;&#27969;&#27169;&#22411;&#30340;&#26377;&#25928;&#26367;&#20195;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35821;&#38899;&#20998;&#31163;&#31070;&#32463;&#32593;&#32476;&#65292;ARFDCN&#65292;&#23427;&#32467;&#21512;&#20102;&#25193;&#24352;&#21367;&#31215;&#65292;&#22810;&#23610;&#24230;&#34701;&#21512;&#65288;MSF&#65289;&#21644;&#36890;&#36947;&#27880;&#24847;&#21147;&#65292;&#20197;&#20811;&#26381;&#22522;&#20110;&#21367;&#31215;&#30340;&#32593;&#32476;&#30340;&#26377;&#38480;&#24863;&#21463;&#37326;&#21644;&#22522;&#20110;transformer&#30340;&#32593;&#32476;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#32467;&#26500;&#26159;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;&#36880;&#28176;&#22686;&#21152;&#30340;&#25193;&#24352;&#20540;&#30340;&#25193;&#24352;&#21367;&#31215;&#26469;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#24182;&#22312;&#30456;&#37051;&#38454;&#27573;&#36827;&#34892;&#34701;&#21512;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#20016;&#23500;&#30340;&#29305;&#24449;&#20869;&#23481;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#36890;&#36947;&#27880;&#24847;&#21147;&#27169;&#22359;&#28155;&#21152;&#21040;&#32593;&#32476;&#20013;&#65292;&#27169;&#22411;&#21487;&#20197;&#25552;&#21462;&#36890;&#36947;&#26435;&#37325;&#65292;&#23398;&#20064;&#26356;&#37325;&#35201;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#34920;&#29616;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#36798;&#21040;&#20102;&#19981;&#38169;&#30340;&#24179;&#34913;&#65292;&#25104;&#20026;&#24403;&#21069;&#23454;&#38469;&#24212;&#29992;&#20013;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an efficient speech separation neural network, ARFDCN, which combines dilated convolutions, multi-scale fusion (MSF), and channel attention to overcome the limited receptive field of convolution-based networks and the high computational cost of transformer-based networks. The suggested network architecture is encoder-decoder based. By using dilated convolutions with gradually increasing dilation value to learn local and global features and fusing them at adjacent stages, the model can learn rich feature content. Meanwhile, by adding channel attention modules to the network, the model can extract channel weights, learn more important features, and thus improve its expressive power and robustness. Experimental results indicate that the model achieves a decent balance between performance and computational efficiency, making it a promising alternative to current mainstream models for practical applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#19977;&#20010;&#21830;&#19994;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#36827;&#34892;&#20102;&#32454;&#33268;&#35780;&#20272;&#65292;&#22312;&#20851;&#27880;&#24615;&#21035;&#32763;&#35793;&#21644;&#20559;&#35265;&#30340;&#21069;&#25552;&#19979;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#24615;&#21035;&#32763;&#35793;&#26041;&#38754;&#30340;&#26174;&#30528;&#24046;&#24322;&#21644;&#20559;&#35265;&#65292;&#36825;&#19968;&#28857;&#19981;&#20250;&#22240;&#20026;&#23427;&#20204;&#25972;&#20307;&#32763;&#35793;&#36136;&#37327;&#30340;&#22909;&#22351;&#32780;&#26377;&#25152;&#25913;&#21464;&#12290;</title><link>http://arxiv.org/abs/2306.05882</link><description>&lt;p&gt;
&#22909;&#30340;&#65292;&#20294;&#24182;&#38750;&#24635;&#26159;&#20844;&#27491;&#65306;&#23545;&#19977;&#20010;&#21830;&#19994;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#24615;&#21035;&#20559;&#35265;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Good, but not always Fair: An Evaluation of Gender Bias for three commercial Machine Translation Systems. (arXiv:2306.05882v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#19977;&#20010;&#21830;&#19994;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#36827;&#34892;&#20102;&#32454;&#33268;&#35780;&#20272;&#65292;&#22312;&#20851;&#27880;&#24615;&#21035;&#32763;&#35793;&#21644;&#20559;&#35265;&#30340;&#21069;&#25552;&#19979;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#24615;&#21035;&#32763;&#35793;&#26041;&#38754;&#30340;&#26174;&#30528;&#24046;&#24322;&#21644;&#20559;&#35265;&#65292;&#36825;&#19968;&#28857;&#19981;&#20250;&#22240;&#20026;&#23427;&#20204;&#25972;&#20307;&#32763;&#35793;&#36136;&#37327;&#30340;&#22909;&#22351;&#32780;&#26377;&#25152;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#22312;&#36136;&#37327;&#26041;&#38754;&#32487;&#32493;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#65292;&#24182;&#19988;&#27491;&#22312;&#36234;&#26469;&#36234;&#22823;&#35268;&#27169;&#22320;&#34987;&#37319;&#29992;&#12290;&#22240;&#27492;&#65292;&#20998;&#26512;&#24050;&#36716;&#21521;&#26356;&#24494;&#22937;&#30340;&#26041;&#38754;&#12289;&#22797;&#26434;&#30340;&#29616;&#35937;&#20197;&#21450;&#21487;&#33021;&#20174;&#24191;&#27867;&#20351;&#29992;MT&#24037;&#20855;&#20013;&#20135;&#29983;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#38024;&#23545;&#19977;&#20010;&#21830;&#19994;MT&#31995;&#32479; - Google Translate&#12289;DeepL&#21644;Modern MT&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#35780;&#20272;&#65292;&#37325;&#28857;&#20851;&#27880;&#24615;&#21035;&#32763;&#35793;&#21644;&#20559;&#35265;&#12290;&#23545;&#20110;&#19977;&#31181;&#35821;&#35328;&#23545;&#65288;&#33521;&#35821;/&#35199;&#29677;&#29273;&#35821;&#12289;&#33521;&#35821;/&#24847;&#22823;&#21033;&#35821;&#21644;&#33521;&#35821;/&#27861;&#35821;&#65289;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#22659;&#19979;&#30340;&#24615;&#21035;&#29616;&#35937;&#30340;&#22810;&#20010;&#31890;&#24230;&#32423;&#21035;&#19978;&#23457;&#26597;&#20102;&#36825;&#20123;&#31995;&#32479;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32771;&#34385;&#20102;&#22312;&#32447;MT&#24037;&#20855;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#19977;&#20010;&#31995;&#32479;&#24615;&#21035;&#32763;&#35793;&#26041;&#38754;&#30340;&#26174;&#30528;&#24046;&#24322;&#65292;&#23613;&#31649;&#23427;&#20204;&#30340;&#25972;&#20307;&#32763;&#35793;&#36136;&#37327;&#19981;&#38169;&#65292;&#20294;&#26159;&#27599;&#20010;&#31995;&#32479;&#37117;&#34920;&#29616;&#20986;&#19981;&#21516;&#31243;&#24230;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Translation (MT) continues to make significant strides in quality and is increasingly adopted on a larger scale. Consequently, analyses have been redirected to more nuanced aspects, intricate phenomena, as well as potential risks that may arise from the widespread use of MT tools. Along this line, this paper offers a meticulous assessment of three commercial MT systems - Google Translate, DeepL, and Modern MT - with a specific focus on gender translation and bias. For three language pairs (English/Spanish, English/Italian, and English/French), we scrutinize the behavior of such systems at several levels of granularity and on a variety of naturally occurring gender phenomena in translation. Our study takes stock of the current state of online MT tools, by revealing significant discrepancies in the gender translation of the three systems, with each system displaying varying degrees of bias despite their overall translation quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27861;&#35821;&#25991;&#26412;&#30340; ChatGPT &#26816;&#27979;&#22120;&#24320;&#21457;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26816;&#27979;&#22120;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979; ChatGPT &#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#22312;&#22495;&#20869;&#29615;&#22659;&#19979;&#20855;&#26377;&#19968;&#23450;&#30340;&#25239;&#25915;&#20987;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#22312;&#22495;&#22806;&#29615;&#22659;&#20013;&#23384;&#22312;&#26126;&#26174;&#30340;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2306.05871</link><description>&lt;p&gt;
&#23454;&#29616;&#23545;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#40065;&#26834;&#26816;&#27979;&#65306;ChatGPT &#23481;&#26131;&#34987;&#21457;&#29616;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Towards a Robust Detection of Language Model Generated Text: Is ChatGPT that Easy to Detect?. (arXiv:2306.05871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27861;&#35821;&#25991;&#26412;&#30340; ChatGPT &#26816;&#27979;&#22120;&#24320;&#21457;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26816;&#27979;&#22120;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979; ChatGPT &#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#22312;&#22495;&#20869;&#29615;&#22659;&#19979;&#20855;&#26377;&#19968;&#23450;&#30340;&#25239;&#25915;&#20987;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#22312;&#22495;&#22806;&#29615;&#22659;&#20013;&#23384;&#22312;&#26126;&#26174;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#30340;&#26368;&#26032;&#36827;&#23637;&#25512;&#21160;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#21457;&#23637;&#65292;&#20363;&#22914; ChatGPT&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27861;&#35821;&#25991;&#26412;&#24320;&#21457;&#21644;&#35780;&#20272; ChatGPT &#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#30740;&#31350;&#23427;&#20204;&#23545;&#22495;&#22806;&#25968;&#25454;&#21644;&#24120;&#35265;&#25915;&#20987;&#26041;&#26696;&#30340;&#40065;&#26834;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#23558;&#33521;&#35821;&#25968;&#25454;&#38598;&#32763;&#35793;&#20026;&#27861;&#35821;&#24182;&#22312;&#32763;&#35793;&#25968;&#25454;&#19978;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26816;&#27979;&#22120;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979; ChatGPT &#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#22312;&#22495;&#20869;&#29615;&#22659;&#19979;&#20855;&#26377;&#19968;&#23450;&#30340;&#25239;&#25915;&#20987;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#22495;&#22806;&#29615;&#22659;&#20013;&#23384;&#22312;&#26126;&#26174;&#30340;&#28431;&#27934;&#65292;&#20984;&#26174;&#20102;&#26816;&#27979;&#23545;&#25239;&#24615;&#25991;&#26412;&#30340;&#25361;&#25112;&#24615;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;&#22495;&#20869;&#27979;&#35797;&#32467;&#26524;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#20869;&#23481;&#19978;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20316;&#20026;&#24320;&#28304;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in natural language processing (NLP) have led to the development of large language models (LLMs) such as ChatGPT. This paper proposes a methodology for developing and evaluating ChatGPT detectors for French text, with a focus on investigating their robustness on out-of-domain data and against common attack schemes. The proposed method involves translating an English dataset into French and training a classifier on the translated data. Results show that the detectors can effectively detect ChatGPT-generated text, with a degree of robustness against basic attack techniques in in-domain settings. However, vulnerabilities are evident in out-of-domain contexts, highlighting the challenge of detecting adversarial text. The study emphasizes caution when applying in-domain testing results to a wider variety of content. We provide our translated datasets and models as open-source resources. https://gitlab.inria.fr/wantoun/robust-chatgpt-detection
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#39640;&#25928;&#32534;&#30721;-&#35299;&#30721;&#21644;&#20840;&#38754;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#30340;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#25913;&#36827;&#30340;&#23494;&#38598;&#36830;&#25509;&#22359;&#12289;&#21452;&#36890;&#36335;&#27169;&#22359;&#12289;&#21367;&#31215;&#22686;&#24378;&#21464;&#21387;&#22120;&#12289;&#36890;&#36947;&#20851;&#27880;&#21644;&#31354;&#38388;&#20851;&#27880;&#31561;&#25216;&#26415;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;VCTK+DEMAND&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#27604;&#29616;&#26377;&#25216;&#26415;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.05861</link><description>&lt;p&gt;
&#39640;&#25928;&#32534;&#30721;-&#35299;&#30721;&#21644;&#21452;&#36890;&#36335;Conformer&#29992;&#20110;&#35821;&#38899;&#22686;&#24378;&#20013;&#30340;&#32508;&#21512;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Encoder-Decoder and Dual-Path Conformer for Comprehensive Feature Learning in Speech Enhancement. (arXiv:2306.05861v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#39640;&#25928;&#32534;&#30721;-&#35299;&#30721;&#21644;&#20840;&#38754;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#30340;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#25913;&#36827;&#30340;&#23494;&#38598;&#36830;&#25509;&#22359;&#12289;&#21452;&#36890;&#36335;&#27169;&#22359;&#12289;&#21367;&#31215;&#22686;&#24378;&#21464;&#21387;&#22120;&#12289;&#36890;&#36947;&#20851;&#27880;&#21644;&#31354;&#38388;&#20851;&#27880;&#31561;&#25216;&#26415;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;VCTK+DEMAND&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#27604;&#29616;&#26377;&#25216;&#26415;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#38899;&#22686;&#24378;(SE)&#30740;&#31350;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#36890;&#36947;&#20851;&#27880;&#21644;&#31354;&#38388;&#20851;&#27880;&#65292;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#32593;&#32476;&#20063;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#22914;&#20309;&#20026;&#20013;&#38388;&#30340;&#22686;&#24378;&#23618;&#25552;&#20379;&#39640;&#25928;&#30340;&#36755;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#39057;(T-F)&#39046;&#22495;&#30340;SE&#32593;&#32476;(DPCFCS-Net)&#65292;&#23427;&#21253;&#21547;&#20102;&#25913;&#36827;&#30340;&#23494;&#38598;&#36830;&#25509;&#22359;&#12289;&#21452;&#36890;&#36335;&#27169;&#22359;&#12289;&#21367;&#31215;&#22686;&#24378;&#21464;&#21387;&#22120;(conformers)&#12289;&#36890;&#36947;&#20851;&#27880;&#21644;&#31354;&#38388;&#20851;&#27880;&#12290;&#19982;&#20043;&#21069;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#24182;&#33021;&#22815;&#23398;&#20064;&#32508;&#21512;&#29305;&#24449;&#12290;&#22312;VCTK+DEMAND&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;SE&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#21457;&#23637;&#30340;&#25913;&#36827;&#30340;&#23494;&#38598;&#36830;&#25509;&#22359;&#21644;&#20108;&#32500;&#20851;&#27880;&#27169;&#22359;&#38750;&#24120;&#36866;&#24212;&#65292;&#26131;&#20110;&#38598;&#25104;&#21040;&#29616;&#26377;&#32593;&#32476;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current speech enhancement (SE) research has largely neglected channel attention and spatial attention, and encoder-decoder architecture-based networks have not adequately considered how to provide efficient inputs to the intermediate enhancement layer. To address these issues, this paper proposes a time-frequency (T-F) domain SE network (DPCFCS-Net) that incorporates improved densely connected blocks, dual-path modules, convolution-augmented transformers (conformers), channel attention, and spatial attention. Compared with previous models, our proposed model has a more efficient encoder-decoder and can learn comprehensive features. Experimental results on the VCTK+DEMAND dataset demonstrate that our method outperforms existing techniques in SE performance. Furthermore, the improved densely connected block and two dimensions attention module developed in this work are highly adaptable and easily integrated into existing networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65288;Corr2Cause&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#24456;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.05836</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#20174;&#30456;&#20851;&#24615;&#20013;&#25512;&#26029;&#20986;&#22240;&#26524;&#20851;&#31995;?
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Infer Causation from Correlation?. (arXiv:2306.05836v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65288;Corr2Cause&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#24456;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#20154;&#31867;&#26234;&#24935;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;&#34429;&#28982;CausalNLP&#39046;&#22495;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;NLP&#20013;&#29616;&#26377;&#30340;&#22240;&#26524;&#25512;&#26029;&#25968;&#25454;&#38598;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#32463;&#39564;&#30693;&#35782;&#65288;&#20363;&#22914;&#24120;&#35782;&#30693;&#35782;&#65289;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32431;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;Corr2Cause&#65292;&#23427;&#37319;&#29992;&#19968;&#32452;&#30456;&#20851;&#35821;&#21477;&#24182;&#30830;&#23450;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;400K&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#22312;&#20854;&#20013;&#35780;&#20272;&#20102;17&#20010;&#29616;&#26377;&#30340;LLMs&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;LLMs&#22312;&#22240;&#26524;&#25512;&#26029;&#25216;&#33021;&#26041;&#38754;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#38519;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20960;&#20046;&#25509;&#36817;&#38543;&#26426;&#12290;&#24403;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#24494;&#35843;&#23558;LLMs&#37325;&#26032;&#29992;&#20110;&#36825;&#31181;&#25216;&#33021;&#26102;&#65292;&#36825;&#31181;&#32570;&#38519;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#24471;&#21040;&#20102;&#32531;&#35299;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#22833;&#36133;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 400K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#24052;&#21202;&#26031;&#22374;&#21512;&#20316;&#31038;&#25552;&#20986;&#20102;&#19968;&#20010;&#27861;&#24459;&#38382;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22522;&#20110;LLM&#25216;&#26415;&#24320;&#21457;&#65292;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#31572;&#26696;&#21305;&#37197;&#24615;&#65292;&#21487;&#20026;&#29992;&#25143;&#25552;&#20379;&#27861;&#24459;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.05827</link><description>&lt;p&gt;
&#38754;&#21521;&#22522;&#20110;LLM&#25216;&#26415;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#20197;&#20026;&#24052;&#21202;&#26031;&#22374;&#21512;&#20316;&#31038;&#25552;&#20379;&#27861;&#24459;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Towards the Exploitation of LLM-based Chatbot for Providing Legal Support to Palestinian Cooperatives. (arXiv:2306.05827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#24052;&#21202;&#26031;&#22374;&#21512;&#20316;&#31038;&#25552;&#20986;&#20102;&#19968;&#20010;&#27861;&#24459;&#38382;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22522;&#20110;LLM&#25216;&#26415;&#24320;&#21457;&#65292;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#31572;&#26696;&#21305;&#37197;&#24615;&#65292;&#21487;&#20026;&#29992;&#25143;&#25552;&#20379;&#27861;&#24459;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#19981;&#26029;&#24212;&#29992;&#65292;&#25105;&#20204;&#36817;&#24180;&#26469;&#24320;&#22987;&#30446;&#30585;&#23545;&#20110;&#27861;&#24459;&#25991;&#20214;&#20132;&#20114;&#26041;&#24335;&#30340;&#37325;&#22823;&#21464;&#38761;&#12290;&#35813;&#25216;&#26415;&#25512;&#21160;&#20102;&#23545;&#22797;&#26434;&#27861;&#24459;&#26415;&#35821;&#21644;&#22330;&#26223;&#30340;&#20998;&#26512;&#21644;&#29702;&#35299;&#12290;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24320;&#21457;&#65292;&#29305;&#21035;&#26159;ChatGPT&#30340;&#20986;&#29616;&#65292;&#20063;&#23545;&#22788;&#29702;&#21644;&#29702;&#35299;&#27861;&#24459;&#25991;&#20214;&#30340;&#26041;&#24335;&#25552;&#20986;&#20102;&#38761;&#21629;&#24615;&#30340;&#36129;&#29486;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#20316;&#31038;&#27861;&#24459;&#38382;&#31572;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24037;&#20316;&#65292;&#20854;&#20013;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#32452;&#20851;&#20110;&#24052;&#21202;&#26031;&#22374;&#21512;&#20316;&#31038;&#21450;&#20854;&#35268;&#23450;&#30340;&#27861;&#24459;&#38382;&#39064;&#65292;&#24182;&#23558;&#32842;&#22825;&#26426;&#22120;&#20154;&#33258;&#21160;&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;&#30001;&#27861;&#24459;&#19987;&#23478;&#35774;&#35745;&#30340;&#30456;&#24212;&#31572;&#26696;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#30001;&#27861;&#24459;&#19987;&#23478;&#29983;&#25104;&#30340;50&#20010;&#26597;&#35810;&#65292;&#24182;&#23558;&#22270;&#34920;&#20135;&#29983;&#30340;&#31572;&#26696;&#19982;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
With the ever-increasing utilization of natural language processing (NLP), we started to witness over the past few years a significant transformation in our interaction with legal texts. This technology has advanced the analysis and enhanced the understanding of complex legal terminology and contexts. The development of recent large language models (LLMs), particularly ChatGPT, has also introduced a revolutionary contribution to the way that legal texts can be processed and comprehended. In this paper, we present our work on a cooperative-legal question-answering LLM-based chatbot, where we developed a set of legal questions about Palestinian cooperatives, associated with their regulations and compared the auto-generated answers by the chatbot to their correspondences that are designed by a legal expert. To evaluate the proposed chatbot, we have used 50 queries generated by the legal expert and compared the answers produced by the chart to their relevance judgments. Finding demonstrate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#20027;&#39064;&#24314;&#27169;&#21644;&#24773;&#24863;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#24314;&#31435;&#20102;&#20851;&#20110;&#21152;&#23494;&#36135;&#24065;&#30340;&#22810;&#20010;&#21465;&#36848;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#21465;&#36848;&#19982;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#20043;&#38388;&#23384;&#22312;&#24378;&#22823;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.05803</link><description>&lt;p&gt;
&#24773;&#24863;&#21644;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Causality between Sentiment and Cryptocurrency Prices. (arXiv:2306.05803v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#20027;&#39064;&#24314;&#27169;&#21644;&#24773;&#24863;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#24314;&#31435;&#20102;&#20851;&#20110;&#21152;&#23494;&#36135;&#24065;&#30340;&#22810;&#20010;&#21465;&#36848;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#21465;&#36848;&#19982;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#20043;&#38388;&#23384;&#22312;&#24378;&#22823;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#24494;&#21338;&#24179;&#21488;&#65288;Twitter&#65289;&#20256;&#36882;&#30340;&#21465;&#36848;&#19982;&#21152;&#23494;&#36164;&#20135;&#20215;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#25216;&#26415;&#65292;&#23558;&#30701;&#25991;&#26412;&#30340;&#20027;&#39064;&#24314;&#27169;&#19982;&#24773;&#24863;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#24314;&#31435;&#20102;&#20851;&#20110;&#21152;&#23494;&#36135;&#24065;&#30340;&#21465;&#36848;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;Twitter&#30340;&#22823;&#35268;&#27169;&#21644;&#22024;&#26434;&#25991;&#26412;&#25968;&#25454;&#20013;&#21457;&#29616;&#28508;&#22312;&#20027;&#39064;&#65292;&#28982;&#21518;&#25105;&#20204;&#25581;&#31034;&#20102;4-5&#20010;&#19982;&#21152;&#23494;&#36135;&#24065;&#30456;&#20851;&#30340;&#21465;&#36848;&#65292;&#21253;&#25324;&#19982;&#21152;&#23494;&#36135;&#24065;&#30456;&#20851;&#30340;&#37329;&#34701;&#25237;&#36164;&#12289;&#25216;&#26415;&#36827;&#27493;&#12289;&#37329;&#34701;&#21644;&#25919;&#27835;&#30417;&#31649;&#12289;&#21152;&#23494;&#36164;&#20135;&#21644;&#23186;&#20307;&#25253;&#36947;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#25105;&#20204;&#30340;&#21465;&#36848;&#19982;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#20043;&#38388;&#23384;&#22312;&#24378;&#22823;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#26368;&#26032;&#30340;&#32463;&#27982;&#23398;&#21019;&#26032;&#8212;&#8212;&#21465;&#20107;&#32463;&#27982;&#23398;&#19982;&#20027;&#39064;&#24314;&#27169;&#21644;&#24773;&#24863;&#20998;&#26512;&#30456;&#32467;&#21512;&#30340;&#26032;&#39046;&#22495;&#32852;&#31995;&#36215;&#26469;&#65292;&#20197;&#20851;&#32852;&#28040;&#36153;&#32773;&#34892;&#20026;&#21644;&#21465;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the relationship between narratives conveyed through microblogging platforms, namely Twitter, and the value of crypto assets. Our study provides a unique technique to build narratives about cryptocurrency by combining topic modelling of short texts with sentiment analysis. First, we used an unsupervised machine learning algorithm to discover the latent topics within the massive and noisy textual data from Twitter, and then we revealed 4-5 cryptocurrency-related narratives, including financial investment, technological advancement related to crypto, financial and political regulations, crypto assets, and media coverage. In a number of situations, we noticed a strong link between our narratives and crypto prices. Our work connects the most recent innovation in economics, Narrative Economics, to a new area of study that combines topic modelling and sentiment analysis to relate consumer behaviour to narratives.
&lt;/p&gt;</description></item><item><title>Xiezhi&#26159;&#19968;&#31181;&#20840;&#38754;&#32508;&#21512;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#35774;&#26377;516&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#35206;&#30422;&#20102;&#20174;13&#20010;&#19981;&#21516;&#23398;&#31185;&#36328;&#36234;&#30340;15&#20010;&#19987;&#19994;&#39046;&#22495;&#65292;&#24182;&#23545;47&#20010;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;&#22823;&#22810;&#25968;&#39046;&#22495;&#36229;&#36234;&#20154;&#31867;&#65292;&#20294;&#22312;&#19968;&#20123;&#39046;&#22495;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.05783</link><description>&lt;p&gt;
Xiezhi&#65306;&#19968;&#31181;&#20840;&#38754;&#26356;&#26032;&#30340;&#32508;&#21512;&#39046;&#22495;&#30693;&#35782;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. (arXiv:2306.05783v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05783
&lt;/p&gt;
&lt;p&gt;
Xiezhi&#26159;&#19968;&#31181;&#20840;&#38754;&#32508;&#21512;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#35774;&#26377;516&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#35206;&#30422;&#20102;&#20174;13&#20010;&#19981;&#21516;&#23398;&#31185;&#36328;&#36234;&#30340;15&#20010;&#19987;&#19994;&#39046;&#22495;&#65292;&#24182;&#23545;47&#20010;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;&#22823;&#22810;&#25968;&#39046;&#22495;&#36229;&#36234;&#20154;&#31867;&#65292;&#20294;&#22312;&#19968;&#20123;&#39046;&#22495;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24613;&#38656;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22522;&#20934;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Xiezhi&#65292;&#36825;&#26159;&#19968;&#20010;&#26368;&#20840;&#38754;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#26088;&#22312;&#35780;&#20272;&#32508;&#21512;&#39046;&#22495;&#30693;&#35782;&#12290;Xiezhi&#21253;&#25324;&#36328;&#36234;13&#20010;&#19981;&#21516;&#23398;&#31185;&#30340;516&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#21253;&#25324;22&#19975;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#38468;&#24102;Xiezhi-Specialty&#21644;Xiezhi-Interdiscipline&#65292;&#22343;&#26377;15,000&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;47&#20010;&#20808;&#36827;&#30340;LLM&#22312;Xiezhi&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#22312;&#31185;&#23398;&#12289;&#24037;&#31243;&#12289;&#20892;&#23398;&#12289;&#21307;&#23398;&#21644;&#33402;&#26415;&#26041;&#38754;&#36229;&#36807;&#20102;&#20154;&#31867;&#30340;&#24179;&#22343;&#34920;&#29616;&#65292;&#20294;&#22312;&#32463;&#27982;&#23398;&#12289;&#27861;&#23398;&#12289;&#25945;&#32946;&#23398;&#12289;&#25991;&#23398;&#12289;&#21382;&#21490;&#21644;&#31649;&#29702;&#26041;&#38754;&#21017;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#26399;&#26395;Xiezhi&#23558;&#26377;&#21161;&#20110;&#20998;&#26512;LLM&#30340;&#37325;&#35201;&#20248;&#28857;&#21644;&#19981;&#36275;&#20043;&#22788;&#65292;&#35813;&#22522;&#20934;&#24050;&#22312;https://github.com/MikeGu721/XiezhiBenchmark&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
New Natural Langauge Process~(NLP) benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present Xiezhi, the most comprehensive evaluation suite designed to assess holistic domain knowledge. Xiezhi comprises multiple-choice questions across 516 diverse disciplines ranging from 13 different subjects with 220,000 questions and accompanied by Xiezhi-Specialty and Xiezhi-Interdiscipline, both with 15k questions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results indicate that LLMs exceed average performance of humans in science, engineering, agronomy, medicine, and art, but fall short in economics, jurisprudence, pedagogy, literature, history, and management. We anticipate Xiezhi will help analyze important strengths and shortcomings of LLMs, and the benchmark is released in https://github.com/MikeGu721/XiezhiBenchmark .
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#30340;STRAFE&#27169;&#22411;&#65292;&#29992;&#20110;&#24930;&#24615;&#32958;&#33039;&#30142;&#30149;&#24694;&#21270;&#26102;&#38388;&#30340;&#29983;&#23384;&#20998;&#26512;&#39044;&#27979;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34920;&#26126;&#20854;&#22312;&#39044;&#27979;&#29305;&#23450;&#20107;&#20214;&#30340;&#30830;&#20999;&#26102;&#38388;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05779</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#24930;&#24615;&#32958;&#33039;&#30142;&#30149;&#24694;&#21270;&#26102;&#38388;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Time-to-Event Prediction for Chronic Kidney Disease Deterioration. (arXiv:2306.05779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#30340;STRAFE&#27169;&#22411;&#65292;&#29992;&#20110;&#24930;&#24615;&#32958;&#33039;&#30142;&#30149;&#24694;&#21270;&#26102;&#38388;&#30340;&#29983;&#23384;&#20998;&#26512;&#39044;&#27979;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34920;&#26126;&#20854;&#22312;&#39044;&#27979;&#29305;&#23450;&#20107;&#20214;&#30340;&#30830;&#20999;&#26102;&#38388;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;Transformer&#27169;&#22411;&#65292;&#24050;&#22312;&#22686;&#24378;&#32437;&#21521;&#20581;&#24247;&#35760;&#24405;&#30340;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22266;&#23450;&#26102;&#38388;&#39118;&#38505;&#39044;&#27979;&#19978;&#65292;&#20294;&#26102;&#38388;&#21040;&#36798;&#20107;&#20214;&#39044;&#27979;&#65288;&#20063;&#31216;&#20026;&#29983;&#23384;&#20998;&#26512;&#65289;&#24448;&#24448;&#26356;&#36866;&#21512;&#20110;&#20020;&#24202;&#22330;&#26223;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21517;&#20026;STRAFE&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#36890;&#29992;&#29983;&#23384;&#20998;&#26512;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;&#20351;&#29992;&#36229;&#36807;130,000&#20154;&#30340;&#30495;&#23454;&#19990;&#30028;&#32034;&#36180;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;STRAFE&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#20154;&#24739;&#26377;3&#26399;&#24930;&#24615;&#32958;&#33039;&#30142;&#30149;&#65288;CKD&#65289;&#65292;&#21457;&#29616;&#20854;&#22312;&#39044;&#27979;&#21040;&#36798;5&#26399;&#30340;&#24694;&#21270;&#30340;&#30830;&#20999;&#26102;&#38388;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26102;&#38388;&#21040;&#36798;&#20107;&#20214;&#39044;&#27979;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;STRAFE&#22312;&#39044;&#27979;&#22266;&#23450;&#26102;&#38388;&#39118;&#38505;&#26041;&#38754;&#20063;&#20248;&#20110;&#20108;&#20803;&#32467;&#26524;&#31639;&#27861;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#20854;&#33021;&#22815;&#23545;&#34987;&#23457;&#26597;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;STRAFE&#30340;&#39044;&#27979;&#21487;&#20197;&#25552;&#39640;&#38451;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep-learning techniques, particularly the transformer model, have shown great potential in enhancing the prediction performance of longitudinal health records. While previous methods have mainly focused on fixed-time risk prediction, time-to-event prediction (also known as survival analysis) is often more appropriate for clinical scenarios. Here, we present a novel deep-learning architecture we named STRAFE, a generalizable survival analysis transformer-based architecture for electronic health records. The performance of STRAFE was evaluated using a real-world claim dataset of over 130,000 individuals with stage 3 chronic kidney disease (CKD) and was found to outperform other time-to-event prediction algorithms in predicting the exact time of deterioration to stage 5. Additionally, STRAFE was found to outperform binary outcome algorithms in predicting fixed-time risk, possibly due to its ability to train on censored data. We show that STRAFE predictions can improve the positive predic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;35&#31687;&#35770;&#25991;&#30340;&#25991;&#29486;&#32508;&#36848;&#20197;&#21450;15&#21517;&#26234;&#33021;&#38899;&#31665;&#29992;&#25143;&#30340;&#35775;&#35848;&#65292;&#24635;&#32467;&#20986;&#20102;&#26234;&#33021;&#38899;&#31665;&#35774;&#35745;&#20013;&#30340;&#20116;&#20010;&#20027;&#39064;&#21644;&#22235;&#20010;&#35774;&#35745;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22235;&#20010;&#35774;&#35745;&#26426;&#36935;&#31354;&#38388;&#65292;&#22914;&#25552;&#20379;&#20449;&#24687;&#25903;&#25345;&#65292;&#22312;&#35774;&#35745;&#20013;&#34701;&#20837;&#29992;&#25143;&#30340;&#24515;&#29702;&#27169;&#22411;&#20197;&#21450;&#25972;&#21512;&#24179;&#38745;&#30340;&#35774;&#35745;&#21407;&#21017;&#31561;&#12290;</title><link>http://arxiv.org/abs/2306.05741</link><description>&lt;p&gt;
&#26234;&#33021;&#38899;&#31665;&#35774;&#35745;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Challenges and Opportunities for the Design of Smart Speakers. (arXiv:2306.05741v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;35&#31687;&#35770;&#25991;&#30340;&#25991;&#29486;&#32508;&#36848;&#20197;&#21450;15&#21517;&#26234;&#33021;&#38899;&#31665;&#29992;&#25143;&#30340;&#35775;&#35848;&#65292;&#24635;&#32467;&#20986;&#20102;&#26234;&#33021;&#38899;&#31665;&#35774;&#35745;&#20013;&#30340;&#20116;&#20010;&#20027;&#39064;&#21644;&#22235;&#20010;&#35774;&#35745;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22235;&#20010;&#35774;&#35745;&#26426;&#36935;&#31354;&#38388;&#65292;&#22914;&#25552;&#20379;&#20449;&#24687;&#25903;&#25345;&#65292;&#22312;&#35774;&#35745;&#20013;&#34701;&#20837;&#29992;&#25143;&#30340;&#24515;&#29702;&#27169;&#22411;&#20197;&#21450;&#25972;&#21512;&#24179;&#38745;&#30340;&#35774;&#35745;&#21407;&#21017;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#25216;&#26415;&#21644;&#35821;&#38899;&#29992;&#25143;&#30028;&#38754;&#65288;VUI&#65289;&#65292;&#22914;Alexa&#12289;Siri&#21644;Google Home&#31561;&#30340;&#36827;&#27493;&#65292;&#20026;&#35768;&#22810;&#26032;&#31867;&#22411;&#30340;&#20132;&#20114;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24066;&#22330;&#36234;&#26469;&#36234;&#22823;&#65292;VUI&#30740;&#31350;&#36234;&#26469;&#36234;&#22810;&#65292;&#20294;&#20154;&#20204;&#20173;&#28982;&#24863;&#21040;&#36825;&#39033;&#25216;&#26415;&#34987;&#20302;&#20272;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;35&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#23558;127&#20010;VUI&#35774;&#35745;&#25351;&#21335;&#32508;&#21512;&#25104;&#20102;&#20116;&#20010;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;15&#21517;&#26234;&#33021;&#38899;&#31665;&#29992;&#25143;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#23545;&#25216;&#26415;&#30340;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;&#24773;&#20917;&#12290;&#36890;&#36807;&#35775;&#35848;&#65292;&#25105;&#20204;&#24635;&#32467;&#20986;&#20102;&#22235;&#20010;&#23545;&#19981;&#20351;&#29992;&#25216;&#26415;&#20570;&#20986;&#26368;&#22823;&#36129;&#29486;&#30340;&#35774;&#35745;&#25361;&#25112;&#12290;&#22522;&#20110;&#20182;&#20204;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#35774;&#35745;&#26426;&#36935;&#31354;&#38388;&#65292;&#20363;&#22914;&#19987;&#27880;&#20110;&#22312;&#22810;&#20219;&#21153;&#65288;&#28921;&#39274;&#12289;&#39550;&#39542;&#12289;&#32946;&#20799;&#31561;&#65289;&#20013;&#25552;&#20379;&#20449;&#24687;&#25903;&#25345;&#65292;&#23558;&#29992;&#25143;&#30340;&#26234;&#33021;&#38899;&#31665;&#24515;&#29702;&#27169;&#22411;&#34701;&#20837;&#35774;&#35745;&#24403;&#20013;&#65292;&#20197;&#21450;&#25972;&#21512;&#24179;&#38745;&#30340;&#35774;&#35745;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in voice technology and voice user interfaces (VUIs) -- such as Alexa, Siri, and Google Home -- have opened up the potential for many new types of interaction. However, despite the potential of these devices reflected by the growing market and body of VUI research, there is a lingering sense that the technology is still underused. In this paper, we conducted a systematic literature review of 35 papers to identify and synthesize 127 VUI design guidelines into five themes. Additionally, we conducted semi-structured interviews with 15 smart speaker users to understand their use and non-use of the technology. From the interviews, we distill four design challenges that contribute the most to non-use. Based on their (non-)use, we identify four opportunity spaces for designers to explore such as focusing on information support while multitasking (cooking, driving, childcare, etc), incorporating users' mental models for smart speakers, and integrating calm design principles.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#23398;&#29983;&#31243;&#24207;&#21592;&#27714;&#21161;&#35831;&#27714;&#30340;&#22238;&#24212;&#33021;&#21147;&#65292;&#35780;&#20272;&#20102;&#20854;&#22312;&#35782;&#21035;&#23398;&#29983;&#38382;&#39064;&#20195;&#30721;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;GPT-3.5&#22312;&#22823;&#22810;&#25968;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;Codex&#65292;&#20004;&#20010;LLM&#32463;&#24120;&#22312;&#27599;&#20010;&#23398;&#29983;&#31243;&#24207;&#20013;&#25214;&#21040;&#33267;&#23569;&#19968;&#20010;&#23454;&#38469;&#38382;&#39064;&#65292;&#20294;&#37117;&#26410;&#33021;&#25214;&#21040;&#20840;&#37096;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.05715</link><description>&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21021;&#23398;&#32773;&#31243;&#24207;&#21592;&#27714;&#21161;&#35831;&#27714;&#30340;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
Exploring the Responses of Large Language Models to Beginner Programmers' Help Requests. (arXiv:2306.05715v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05715
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#23398;&#29983;&#31243;&#24207;&#21592;&#27714;&#21161;&#35831;&#27714;&#30340;&#22238;&#24212;&#33021;&#21147;&#65292;&#35780;&#20272;&#20102;&#20854;&#22312;&#35782;&#21035;&#23398;&#29983;&#38382;&#39064;&#20195;&#30721;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;GPT-3.5&#22312;&#22823;&#22810;&#25968;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;Codex&#65292;&#20004;&#20010;LLM&#32463;&#24120;&#22312;&#27599;&#20010;&#23398;&#29983;&#31243;&#24207;&#20013;&#25214;&#21040;&#33267;&#23569;&#19968;&#20010;&#23454;&#38469;&#38382;&#39064;&#65292;&#20294;&#37117;&#26410;&#33021;&#25214;&#21040;&#20840;&#37096;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#21644;&#32972;&#26223;&#65306;&#22312;&#36807;&#21435;&#30340;&#19968;&#24180;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24109;&#21367;&#20840;&#29699;&#12290;&#22312;&#35745;&#31639;&#26426;&#25945;&#32946;&#20013;&#65292;&#23601;&#20687;&#22312;&#29983;&#27963;&#30340;&#20854;&#20182;&#26041;&#38754;&#19968;&#26679;&#65292;&#35768;&#22810;&#26426;&#36935;&#21644;&#23041;&#32961;&#20986;&#29616;&#20102;&#12290;&#30446;&#26631;&#65306;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20013;&#30340;&#26426;&#36935;&#21644;&#23041;&#32961;&#65306;&#22238;&#24212;&#23398;&#29983;&#31243;&#24207;&#21592;&#30340;&#27714;&#21161;&#35831;&#27714;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLM&#22312;&#35782;&#21035;&#23398;&#29983;&#35831;&#27714;&#24110;&#21161;&#30340;&#38382;&#39064;&#20195;&#30721;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20174;&#22312;&#32447;&#32534;&#31243;&#35838;&#31243;&#20013;&#25910;&#38598;&#20102;&#27714;&#21161;&#35831;&#27714;&#21644;&#20195;&#30721;&#26679;&#26412;&#12290;&#28982;&#21518;&#20419;&#20351;&#20004;&#20010;&#19981;&#21516;&#30340;LLM&#65288;OpenAI Codex&#21644;GPT-3.5&#65289;&#35782;&#21035;&#21644;&#35299;&#37322;&#23398;&#29983;&#20195;&#30721;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#20102;LLM&#29983;&#25104;&#30340;&#31572;&#26696;&#12290;&#21457;&#29616;&#65306;GPT-3.5&#22312;&#22823;&#22810;&#25968;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;Codex&#12290;&#20004;&#20010;LLM&#32463;&#24120;&#22312;&#27599;&#20010;&#23398;&#29983;&#31243;&#24207;&#20013;&#25214;&#21040;&#33267;&#23569;&#19968;&#20010;&#23454;&#38469;&#38382;&#39064;&#65288;GPT-3.5&#22312;90&#65285;&#30340;&#24773;&#20917;&#19979;&#65289; &#12290;
&lt;/p&gt;
&lt;p&gt;
Background and Context: Over the past year, large language models (LLMs) have taken the world by storm. In computing education, like in other walks of life, many opportunities and threats have emerged as a consequence.  Objectives: In this article, we explore such opportunities and threats in a specific area: responding to student programmers' help requests. More specifically, we assess how good LLMs are at identifying issues in problematic code that students request help on.  Method: We collected a sample of help requests and code from an online programming course. We then prompted two different LLMs (OpenAI Codex and GPT-3.5) to identify and explain the issues in the students' code and assessed the LLM-generated answers both quantitatively and qualitatively.  Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently find at least one actual issue in each student program (GPT-3.5 in 90% of the cases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#25552;&#21462;&#22120;&#65292;&#29992;&#20110;&#20174;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#26377;&#25928;&#19988;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#30340;&#24773;&#24863;&#34920;&#31034;&#12290;&#24773;&#24863;&#25552;&#21462;&#22120;&#22312;&#19977;&#20010;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#26377;&#21033;&#20110;&#24773;&#24863;&#35821;&#38899;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.05709</link><description>&lt;p&gt;
&#22312;&#19981;&#24179;&#34913;&#30340;&#35821;&#38899;&#25968;&#25454;&#20013;&#23398;&#20064;&#24773;&#24863;&#34920;&#31034;&#20197;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#21644;&#24773;&#24863;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning Emotional Representations from Imbalanced Speech Data for Speech Emotion Recognition and Emotional Text-to-Speech. (arXiv:2306.05709v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#25552;&#21462;&#22120;&#65292;&#29992;&#20110;&#20174;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#26377;&#25928;&#19988;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#30340;&#24773;&#24863;&#34920;&#31034;&#12290;&#24773;&#24863;&#25552;&#21462;&#22120;&#22312;&#19977;&#20010;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#26377;&#21033;&#20110;&#24773;&#24863;&#35821;&#38899;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#35821;&#38899;&#24773;&#24863;&#34920;&#31034;&#23545;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#21644;&#24773;&#24863;&#35821;&#38899;&#21512;&#25104;&#65288;TTS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24773;&#24863;&#35821;&#38899;&#26679;&#26412;&#27604;&#20013;&#24615;&#26679;&#24335;&#35821;&#38899;&#26356;&#38590;&#20197;&#33719;&#21462;&#19988;&#25104;&#26412;&#26356;&#39640;&#65292;&#36825;&#23548;&#33268;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22823;&#22810;&#25968;&#30456;&#20851;&#24037;&#20316;&#19981;&#24184;&#22320;&#24573;&#30053;&#20102;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#23548;&#33268;&#27169;&#22411;&#21487;&#33021;&#36807;&#24230;&#25311;&#21512;&#20110;&#22823;&#22810;&#25968;&#20013;&#24615;&#31867;&#21035;&#24182;&#26080;&#27861;&#20135;&#29983;&#24378;&#20581;&#21644;&#26377;&#25928;&#30340;&#24773;&#24863;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#25552;&#21462;&#22120;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#22686;&#24378;&#26041;&#27861;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#20174;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20986;&#26377;&#25928;&#19988;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#30340;&#24773;&#24863;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;&#23545;&#20110;SER&#20219;&#21153;&#65292;&#35813;&#24773;&#24863;&#25552;&#21462;&#22120;&#22312;&#19977;&#20010;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#65307;&#65288;2&#65289;&#20174;&#25105;&#20204;&#30340;&#24773;&#24863;&#25552;&#21462;&#22120;&#20135;&#29983;&#30340;&#34920;&#31034;&#26377;&#21033;&#20110;TTS&#27169;&#22411;&#65292;&#24182;&#20351;&#20043;&#33021;&#22815;&#21512;&#25104;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective speech emotional representations play a key role in Speech Emotion Recognition (SER) and Emotional Text-To-Speech (TTS) tasks. However, emotional speech samples are more difficult and expensive to acquire compared with Neutral style speech, which causes one issue that most related works unfortunately neglect: imbalanced datasets. Models might overfit to the majority Neutral class and fail to produce robust and effective emotional representations. In this paper, we propose an Emotion Extractor to address this issue. We use augmentation approaches to train the model and enable it to extract effective and generalizable emotional representations from imbalanced datasets. Our empirical results show that (1) for the SER task, the proposed Emotion Extractor surpasses the state-of-the-art baseline on three imbalanced datasets; (2) the produced representations from our Emotion Extractor benefit the TTS model, and enable it to synthesize more expressive speech.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05685</link><description>&lt;p&gt;
&#29992;MT-Bench&#21644;Chatbot Arena&#35780;&#20272;&#20197;LLM&#20026;&#22522;&#30784;&#30340;&#32842;&#22825;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. (arXiv:2306.05685v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05685
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32842;&#22825;&#21161;&#25163;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#32780;&#29616;&#26377;&#30340;&#22522;&#20934;&#26080;&#27861;&#34913;&#37327;&#20154;&#31867;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#22312;&#26356;&#21152;&#24320;&#25918;&#30340;&#38382;&#39064;&#19978;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#30340;&#20351;&#29992;&#21644;&#23616;&#38480;&#24615;&#65292;&#22914;&#20301;&#32622;&#21644;&#20887;&#20313;&#20559;&#35265;&#20197;&#21450;&#26377;&#38480;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#26469;&#36801;&#31227;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#65288;&#19968;&#20010;&#22810;&#36718;&#38382;&#31572;&#38598;&#21644;&#19968;&#20010;&#20247;&#21253;&#31454;&#25216;&#24179;&#21488;&#65289;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#24378;&#22823;LLM&#35780;&#21028;&#21592;&#21487;&#20197;&#24456;&#22909;&#22320;&#21305;&#37197;&#21463;&#25511;&#21644;&#20247;&#21253;&#20154;&#31867;&#20559;&#22909;&#65292;&#36798;&#21040;&#20102;80&#65285;&#20197;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#19982;&#20154;&#31867;&#19968;&#33268;&#24615;&#27700;&#24179;&#30456;&#21516;&#12290;&#22240;&#27492;&#65292;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#21487;&#35299;&#37322;&#30340;&#36924;&#36817;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#24335;&#65292;&#32780;&#36825;&#20123;&#20559;&#22909;&#26159;&#38750;&#24120;&#26114;&#36149;&#33719;&#21462;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, such as position and verbosity biases and limited reasoning ability, and propose solutions to migrate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#24515;&#23545;&#25239;&#25915;&#20987;&#65292;&#38024;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#26495;&#22312;PLMs&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#65292;&#36890;&#36807;&#23383;&#31526;&#32423;&#21644;&#21333;&#35789;&#32423;&#30340;&#30772;&#22351;&#26041;&#27861;&#36827;&#34892;&#25915;&#20987;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.05659</link><description>&lt;p&gt;
COVER&#65306;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#24515;&#23545;&#25239;&#25915;&#20987;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models. (arXiv:2306.05659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#24515;&#23545;&#25239;&#25915;&#20987;&#65292;&#38024;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#26495;&#22312;PLMs&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#65292;&#36890;&#36807;&#23383;&#31526;&#32423;&#21644;&#21333;&#35789;&#32423;&#30340;&#30772;&#22351;&#26041;&#27861;&#36827;&#34892;&#25915;&#20987;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#26159;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20013;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#29305;&#21035;&#26159;&#22312;&#20687;&#23569;&#37327;&#26679;&#26412;&#22330;&#26223;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;PLMs&#30340;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#22312;&#22522;&#20110;&#27169;&#26495;&#30340;&#25552;&#31034;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#28508;&#22312;&#30340;&#28431;&#27934;&#65292;&#21487;&#33021;&#20250;&#35823;&#23548;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#24341;&#36215;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#25552;&#20986;&#22522;&#20110;&#25552;&#31034;&#30340;&#23545;&#25239;&#25915;&#20987;&#25163;&#27573;&#65292;&#25581;&#31034;&#20102;PLMs&#30340;&#19968;&#20123;&#28431;&#27934;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23383;&#31526;&#32423;&#21035;&#21644;&#21333;&#35789;&#32423;&#21035;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#30772;&#22351;&#25163;&#21160;&#27169;&#26495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#19978;&#36848;&#21551;&#21457;&#24335;&#30772;&#22351;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#24515;&#31639;&#27861;&#36827;&#34892;&#25915;&#20987;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;BERT&#31995;&#21015;&#27169;&#22411;&#30340;&#19977;&#20010;&#21464;&#31181;&#21644;&#20843;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#20219;&#21153;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based learning has been proved to be an effective way in pre-trained language models (PLMs), especially in low-resource scenarios like few-shot settings. However, the trustworthiness of PLMs is of paramount significance and potential vulnerabilities have been shown in prompt-based templates that could mislead the predictions of language models, causing serious security concerns. In this paper, we will shed light on some vulnerabilities of PLMs, by proposing a prompt-based adversarial attack on manual templates in black box scenarios. First of all, we design character-level and word-level heuristic approaches to break manual templates separately. Then we present a greedy algorithm for the attack based on the above heuristic destructive approaches. Finally, we evaluate our approach with the classification tasks on three variants of BERT series models and eight datasets. And comprehensive experimental results justify the effectiveness of our approach in terms of attack success rate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38382;&#31572;&#65288;QA&#65289;&#26041;&#27861;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#39118;&#38505;&#35780;&#20272;&#30340;&#25216;&#26415;&#65292;&#22312;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#30340;&#21069;&#25552;&#19979;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#29992;&#25143;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#19988;&#24212;&#29992;&#20110;&#20004;&#20010;&#22823;&#22411;&#24515;&#29702;&#20581;&#24247;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#24515;&#29702;&#20581;&#24247;&#29992;&#20363;&#23588;&#20854;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.05652</link><description>&lt;p&gt;
&#22312;&#32447;&#24515;&#29702;&#20581;&#24247;&#39118;&#38505;&#35780;&#20272;&#30340;&#38544;&#31169;&#24863;&#30693;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Privacy Aware Question-Answering System for Online Mental Health Risk Assessment. (arXiv:2306.05652v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38382;&#31572;&#65288;QA&#65289;&#26041;&#27861;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#39118;&#38505;&#35780;&#20272;&#30340;&#25216;&#26415;&#65292;&#22312;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#30340;&#21069;&#25552;&#19979;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#29992;&#25143;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#19988;&#24212;&#29992;&#20110;&#20004;&#20010;&#22823;&#22411;&#24515;&#29702;&#20581;&#24247;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#24515;&#29702;&#20581;&#24247;&#29992;&#20363;&#23588;&#20854;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20351;&#24739;&#26377;&#31934;&#31070;&#30142;&#30149;&#30340;&#20154;&#20998;&#20139;&#29983;&#27963;&#32463;&#21382;&#24182;&#25214;&#21040;&#22312;&#32447;&#25903;&#25345;&#20197;&#24212;&#23545;&#30142;&#30149;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29992;&#25143;&#26410;&#33021;&#33719;&#24471;&#30495;&#27491;&#30340;&#20020;&#24202;&#25903;&#25345;&#65292;&#20174;&#32780;&#21152;&#21095;&#20102;&#20182;&#20204;&#30340;&#30151;&#29366;&#12290;&#22522;&#20110;&#29992;&#25143;&#22312;&#32593;&#19978;&#21457;&#24067;&#30340;&#20869;&#23481;&#36827;&#34892;&#31579;&#36873;&#21487;&#20197;&#24110;&#21161;&#25552;&#20379;&#32773;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#21307;&#30103;&#20445;&#20581;&#65292;&#24182;&#23613;&#37327;&#20943;&#23569;&#34394;&#20551;&#38451;&#24615;&#12290;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#20197;&#35780;&#20272;&#29992;&#25143;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#20854;&#24515;&#29702;&#20581;&#24247;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32479;&#19968;&#38382;&#31572;&#65288;Unified-QA&#65289;&#27169;&#22411;&#30340;&#38382;&#31572;&#26041;&#27861;&#26469;&#35780;&#20272;&#24515;&#29702;&#20581;&#24247;&#39118;&#38505;&#65292;&#24212;&#29992;&#20110;&#20004;&#20010;&#22823;&#22411;&#24515;&#29702;&#20581;&#24247;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;Unified-QA&#65292;&#24182;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#23545;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#21311;&#21517;&#21270;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#39118;&#38505;&#35780;&#20272;&#24314;&#27169;&#20026;&#38382;&#31572;&#20219;&#21153;&#23545;&#20110;&#24515;&#29702;&#20581;&#24247;&#29992;&#20363;&#29305;&#21035;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#21253;&#21547;&#24046;&#20998;&#38544;&#31169;&#21518;&#65292;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#19981;&#21040;1%&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms have enabled individuals suffering from mental illnesses to share their lived experiences and find the online support necessary to cope. However, many users fail to receive genuine clinical support, thus exacerbating their symptoms. Screening users based on what they post online can aid providers in administering targeted healthcare and minimize false positives. Pre-trained Language Models (LMs) can assess users' social media data and classify them in terms of their mental health risk. We propose a Question-Answering (QA) approach to assess mental health risk using the Unified-QA model on two large mental health datasets. To protect user data, we extend Unified-QA by anonymizing the model training process using differential privacy. Our results demonstrate the effectiveness of modeling risk assessment as a QA task, specifically for mental health use cases. Furthermore, the model's performance decreases by less than 1% with the inclusion of differential privacy. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WSPAlign&#30340;&#26080;&#38656;&#25163;&#21160;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#35789;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29992;&#22823;&#35268;&#27169;&#24369;&#30417;&#30563;&#25968;&#25454;&#20013;&#30340;&#36328;&#24230;&#39044;&#27979;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#27604;&#24403;&#21069;&#26368;&#20339;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05644</link><description>&lt;p&gt;
WSPAlign: &#22823;&#35268;&#27169;&#24369;&#30417;&#30563;&#36328;&#24230;&#39044;&#27979;&#19979;&#30340;&#35789;&#23545;&#40784;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
WSPAlign: Word Alignment Pre-training via Large-Scale Weakly Supervised Span Prediction. (arXiv:2306.05644v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WSPAlign&#30340;&#26080;&#38656;&#25163;&#21160;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#35789;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29992;&#22823;&#35268;&#27169;&#24369;&#30417;&#30563;&#25968;&#25454;&#20013;&#30340;&#36328;&#24230;&#39044;&#27979;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#27604;&#24403;&#21069;&#26368;&#20339;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35789;&#23545;&#40784;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#23545;&#40784;&#25968;&#25454;&#38598;&#25110;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#23545;&#25163;&#21160;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#25105;&#20204;&#36890;&#36807;&#25918;&#23485;&#23545;&#27491;&#30830;&#12289;&#23436;&#20840;&#23545;&#40784;&#21644;&#24179;&#34892;&#21477;&#23376;&#30340;&#35201;&#27714;&#65292;&#25193;&#22823;&#20102;&#30417;&#30563;&#25968;&#25454;&#30340;&#26469;&#28304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#24102;&#26377;&#22122;&#22768;&#12289;&#37096;&#20998;&#23545;&#40784;&#21644;&#38750;&#24179;&#34892;&#27573;&#33853;&#20316;&#20026;&#22823;&#35268;&#27169;&#24369;&#30417;&#30563;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#36328;&#24230;&#39044;&#27979;&#23545;&#35789;&#23545;&#40784;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;WSPAlign&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#26080;&#38656;&#25163;&#21160;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#35789;&#23545;&#40784;&#26041;&#27861;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;fine-tuning&#26102;&#65292;WSPAlign&#22312;F1&#21644;AER&#20004;&#20010;&#25351;&#26631;&#19978;&#30340;&#26368;&#20339;&#30417;&#30563;&#22522;&#32447;&#20998;&#21035;&#25552;&#39640;&#20102;3.3~6.1&#21644;1.5~6.1&#20010;&#28857;&#65292;&#25104;&#20026;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;WSPAlign&#22312;&#23569;&#26679;&#26412;&#12289;&#38646;&#26679;&#26412;&#21644;&#36328;&#35821;&#35328;&#27979;&#35797;&#20013;&#20063;&#33719;&#24471;&#20102;&#19982;&#30456;&#24212;&#22522;&#32447;&#30456;&#21516;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing word alignment methods rely on manual alignment datasets or parallel corpora, which limits their usefulness. Here, to mitigate the dependence on manual data, we broaden the source of supervision by relaxing the requirement for correct, fully-aligned, and parallel sentences. Specifically, we make noisy, partially aligned, and non-parallel paragraphs. We then use such a large-scale weakly-supervised dataset for word alignment pre-training via span prediction. Extensive experiments with various settings empirically demonstrate that our approach, which is named WSPAlign, is an effective and scalable way to pre-train word aligners without manual data. When fine-tuned on standard benchmarks, WSPAlign has set a new state-of-the-art by improving upon the best-supervised baseline by 3.3~6.1 points in F1 and 1.5~6.1 points in AER. Furthermore, WSPAlign also achieves competitive performance compared with the corresponding baselines in few-shot, zero-shot and cross-lingual tests, whi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#20197;&#29992;&#20110;&#21307;&#30103;&#25253;&#21578;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#36731;&#37327;&#32423;&#26597;&#35810;Transformer&#36830;&#25509;&#20004;&#20010;FMs&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05642</link><description>&lt;p&gt;
&#38754;&#21521;&#21307;&#30103;&#25253;&#21578;&#29983;&#25104;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#33258;&#23450;&#20041;
&lt;/p&gt;
&lt;p&gt;
Customizing General-Purpose Foundation Models for Medical Report Generation. (arXiv:2306.05642v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05642
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#20197;&#29992;&#20110;&#21307;&#30103;&#25253;&#21578;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#36731;&#37327;&#32423;&#26597;&#35810;Transformer&#36830;&#25509;&#20004;&#20010;FMs&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#23383;&#24149;&#39044;&#27979;&#65292;&#20063;&#34987;&#35270;&#20026;&#21307;&#30103;&#25253;&#21578;&#29983;&#25104;&#65288;MRG&#65289;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20026;&#32473;&#23450;&#30340;&#21307;&#30103;&#22270;&#20687;&#33258;&#21160;&#29983;&#25104;&#36830;&#36143;&#20934;&#30830;&#30340;&#23383;&#24149;&#12290;&#28982;&#32780;&#65292;&#26631;&#35760;&#30340;&#21307;&#30103;&#22270;&#20687;-&#25253;&#21578;&#23545;&#30340;&#31232;&#32570;&#24615;&#22312;&#28145;&#24230;&#21644;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#24320;&#21457;&#20013;&#25552;&#20986;&#20102;&#24040;&#22823;&#25361;&#25112;&#65292;&#36825;&#20123;&#32593;&#32476;&#21487;&#20197;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36825;&#26679;&#30340;&#20154;&#24037;&#26234;&#33021;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#36890;&#29992;&#30340;&#38754;&#21521;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#23450;&#21046;&#65292;&#29305;&#21035;&#20851;&#27880;&#21307;&#30103;&#25253;&#21578;&#29983;&#25104;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26681;&#25454;BLIP-2&#25552;&#20986;&#20102;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;MRG&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#36731;&#37327;&#32423;&#26597;&#35810;Transformer&#36830;&#25509;&#20004;&#20010;FMs&#65306;&#24040;&#22411;&#35270;&#35273;Transformer EVA-ViT-g&#21644;&#21452;&#35821;LLM&#65292;&#35813;LLM&#34987;&#35757;&#32451;&#29992;&#20110;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#65288;&#31216;&#20026;T5-base-CN&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#21307;&#30103;&#25253;&#21578;&#29983;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#36825;&#34920;&#26126;&#20102;&#23558;&#22522;&#30784;&#27169;&#22411;&#36866;&#24212;&#20110;&#27492;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical caption prediction which can be regarded as a task of medical report generation (MRG), requires the automatic generation of coherent and accurate captions for the given medical images. However, the scarcity of labelled medical image-report pairs presents great challenges in the development of deep and large-scale neural networks capable of harnessing the potential artificial general intelligence power like large language models (LLMs). In this work, we propose customizing off-the-shelf general-purpose large-scale pre-trained models, i.e., foundation models (FMs), in computer vision and natural language processing with a specific focus on medical report generation. Specifically, following BLIP-2, a state-of-the-art vision-language pre-training approach, we introduce our encoder-decoder-based MRG model. This model utilizes a lightweight query Transformer to connect two FMs: the giant vision Transformer EVA-ViT-g and a bilingual LLM trained to align with human intentions (referred
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#24615;&#30340;Wav2vec2&#22768;&#38899;&#20266;&#36896;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#27880;&#20837;&#21487;&#35757;&#32451;&#31209;&#20998;&#35299;&#30697;&#38453;&#21040;&#21464;&#21387;&#22120;&#32467;&#26500;&#30340;&#27599;&#19968;&#23618;&#20013;&#26469;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#26102;&#36807;&#38271;&#35757;&#32451;&#26102;&#38388;&#21644;&#39640;&#20869;&#23384;&#28040;&#32791;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.05617</link><description>&lt;p&gt;
&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#24615;&#30340;Wav2vec2&#22768;&#38899;&#20266;&#36896;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Low-rank Adaptation Method for Wav2vec2-based Fake Audio Detection. (arXiv:2306.05617v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#24615;&#30340;Wav2vec2&#22768;&#38899;&#20266;&#36896;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#27880;&#20837;&#21487;&#35757;&#32451;&#31209;&#20998;&#35299;&#30697;&#38453;&#21040;&#21464;&#21387;&#22120;&#32467;&#26500;&#30340;&#27599;&#19968;&#23618;&#20013;&#26469;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#26102;&#36807;&#38271;&#35757;&#32451;&#26102;&#38388;&#21644;&#39640;&#20869;&#23384;&#28040;&#32791;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#26159;&#22768;&#38899;&#20266;&#36896;&#26816;&#27979;&#20013;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#20027;&#39064;&#12290;&#35768;&#22810;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#23398;&#20064;&#26356;&#20016;&#23500;&#21644;&#26356;&#39640;&#32423;&#21035;&#30340;&#35821;&#38899;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22312;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#26102;&#65292;&#36890;&#24120;&#23384;&#22312;&#36807;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#39640;&#20869;&#23384;&#28040;&#32791;&#30340;&#25361;&#25112;&#65292;&#32780;&#23436;&#20840;&#24494;&#35843;&#20063;&#38750;&#24120;&#26114;&#36149;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24212;&#29992;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#21040;wav2vec2&#27169;&#22411;&#20013;&#65292;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24182;&#23558;&#21487;&#35757;&#32451;&#31209;&#20998;&#35299;&#30697;&#38453;&#27880;&#20837;&#21040;&#21464;&#21387;&#22120;&#32467;&#26500;&#30340;&#27599;&#19968;&#23618;&#20013;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#12290;&#19982;&#20351;&#29992;&#21253;&#21547;317M&#35757;&#32451;&#21442;&#25968;&#30340;wav2vec2&#27169;&#22411;&#19978;&#30340;Adam&#24494;&#35843;&#30456;&#27604;&#65292;LoRA&#36890;&#36807;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;198&#20493;&#65292;&#23454;&#29616;&#20102;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised speech models are a rapidly developing research topic in fake audio detection. Many pre-trained models can serve as feature extractors, learning richer and higher-level speech features. However,when fine-tuning pre-trained models, there is often a challenge of excessively long training times and high memory consumption, and complete fine-tuning is also very expensive. To alleviate this problem, we apply low-rank adaptation(LoRA) to the wav2vec2 model, freezing the pre-trained model weights and injecting a trainable rank-decomposition matrix into each layer of the transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared with fine-tuning with Adam on the wav2vec2 model containing 317M training parameters, LoRA achieved similar performance by reducing the number of trainable parameters by 198 times.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35789;&#20041;&#25193;&#23637;&#65288;WSE&#65289;&#30340;&#33539;&#20363;&#65292;&#20351;&#35789;&#27719;&#33021;&#22815;&#21521;&#26032;&#35821;&#22659;&#20135;&#29983;&#26032;&#30340;&#24847;&#20041;&#65292;&#36890;&#36807;&#39318;&#20808;&#23558;&#22810;&#20041;&#35789;&#31867;&#22411;&#21010;&#20998;&#20026;&#20004;&#20010;&#20266;&#35760;&#21495;&#20197;&#26631;&#35760;&#20854;&#19981;&#21516;&#30340;&#24847;&#20041;&#65292;&#28982;&#21518;&#25512;&#26029;&#19968;&#20010;&#20266;&#31526;&#21495;&#30340;&#21547;&#20041;&#26159;&#21542;&#21487;&#20197;&#34987;&#25193;&#23637;&#20197;&#20256;&#36798;&#26469;&#33258;&#21516;&#19968;&#35789;&#31867;&#22411;&#21010;&#20998;&#20986;&#30340;&#31526;&#21495;&#25152;&#34920;&#31034;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.05609</link><description>&lt;p&gt;
&#35789;&#20041;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Word sense extension. (arXiv:2306.05609v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35789;&#20041;&#25193;&#23637;&#65288;WSE&#65289;&#30340;&#33539;&#20363;&#65292;&#20351;&#35789;&#27719;&#33021;&#22815;&#21521;&#26032;&#35821;&#22659;&#20135;&#29983;&#26032;&#30340;&#24847;&#20041;&#65292;&#36890;&#36807;&#39318;&#20808;&#23558;&#22810;&#20041;&#35789;&#31867;&#22411;&#21010;&#20998;&#20026;&#20004;&#20010;&#20266;&#35760;&#21495;&#20197;&#26631;&#35760;&#20854;&#19981;&#21516;&#30340;&#24847;&#20041;&#65292;&#28982;&#21518;&#25512;&#26029;&#19968;&#20010;&#20266;&#31526;&#21495;&#30340;&#21547;&#20041;&#26159;&#21542;&#21487;&#20197;&#34987;&#25193;&#23637;&#20197;&#20256;&#36798;&#26469;&#33258;&#21516;&#19968;&#35789;&#31867;&#22411;&#21010;&#20998;&#20986;&#30340;&#31526;&#21495;&#25152;&#34920;&#31034;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#24120;&#24120;&#29992;&#35789;&#35821;&#34920;&#36798;&#26032;&#39062;&#30340;&#24847;&#24605;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#22312;&#20851;&#27880;&#35789;&#20041;&#28040;&#23696;&#65288;WSD&#65289;&#65292;&#20294;&#23545;&#20110;&#22914;&#20309;&#23558;&#19968;&#20010;&#35789;&#30340;&#24847;&#20041;&#25193;&#23637;&#33267;&#26032;&#30340;&#21547;&#20041;&#65292;&#21364;&#40092;&#26377;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35789;&#20041;&#25193;&#23637;&#65288;WSE&#65289;&#30340;&#33539;&#20363;&#65292;&#20351;&#35789;&#27719;&#33021;&#22815;&#21521;&#26032;&#35821;&#22659;&#20135;&#29983;&#26032;&#30340;&#24847;&#20041;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#39318;&#20808;&#23558;&#22810;&#20041;&#35789;&#31867;&#22411;&#21010;&#20998;&#20026;&#20004;&#20010;&#20266;&#35760;&#21495;&#20197;&#26631;&#35760;&#20854;&#19981;&#21516;&#30340;&#24847;&#20041;&#65292;&#28982;&#21518;&#25512;&#26029;&#19968;&#20010;&#20266;&#31526;&#21495;&#30340;&#21547;&#20041;&#26159;&#21542;&#21487;&#20197;&#34987;&#25193;&#23637;&#20197;&#20256;&#36798;&#26469;&#33258;&#21516;&#19968;&#35789;&#31867;&#22411;&#21010;&#20998;&#20986;&#30340;&#31526;&#21495;&#25152;&#34920;&#31034;&#30340;&#24847;&#20041;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#38142;&#25509;&#30340;&#35748;&#30693;&#27169;&#22411;&#19982;&#23398;&#20064;&#26041;&#26696;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#36716;&#25442;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#31354;&#38388;&#20197;&#25903;&#25345;&#21508;&#31181;&#31867;&#22411;&#30340;&#35789;&#20041;&#25193;&#23637;&#12290;&#25105;&#20204;&#38024;&#23545;&#20960;&#20010;&#31454;&#20105;&#22522;&#32447;&#35780;&#20272;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#39044;&#27979;&#26032;&#39062;&#35789;&#20041;&#30340;&#21487;&#20449;&#32763;&#35793;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans often make creative use of words to express novel senses. A long-standing effort in natural language processing has been focusing on word sense disambiguation (WSD), but little has been explored about how the sense inventory of a word may be extended toward novel meanings. We present a paradigm of word sense extension (WSE) that enables words to spawn new senses toward novel context. We develop a framework that simulates novel word sense extension by first partitioning a polysemous word type into two pseudo-tokens that mark its different senses, and then inferring whether the meaning of a pseudo-token can be extended to convey the sense denoted by the token partitioned from the same word type. Our framework combines cognitive models of chaining with a learning scheme that transforms a language model embedding space to support various types of word sense extension. We evaluate our framework against several competitive baselines and show that it is superior in predicting plausible
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#26041;&#27861;&#26469;&#32479;&#19968;&#22788;&#29702;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#20135;&#21697;&#23646;&#24615;&#20540;&#35782;&#21035;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05605</link><description>&lt;p&gt;
&#20135;&#21697;&#23646;&#24615;&#20540;&#35782;&#21035;&#30340;&#32479;&#19968;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Unified Generative Approach to Product Attribute-Value Identification. (arXiv:2306.05605v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#26041;&#27861;&#26469;&#32479;&#19968;&#22788;&#29702;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#20135;&#21697;&#23646;&#24615;&#20540;&#35782;&#21035;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20135;&#21697;&#23646;&#24615;&#20540;&#35782;&#21035;&#65288;PAVI&#65289;&#26088;&#22312;&#20351;&#29992;&#20135;&#21697;&#25991;&#26412;&#20316;&#20026;&#32447;&#32034;&#23558;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;&#20135;&#21697;&#19982;&#20854;&#23646;&#24615;&#20540;&#65288;&#20363;&#22914;&lt;Material&#65292;Cotton&gt;&#65289;&#38142;&#25509;&#36215;&#26469;&#12290;&#29616;&#23454;&#19990;&#30028;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#30340;&#25216;&#26415;&#38656;&#27714;&#35201;&#27714;PAVI&#26041;&#27861;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#20540;&#12289;&#22810;&#23646;&#24615;&#20540;&#21644;&#35268;&#33539;&#21270;&#30340;&#20540;&#65292;&#32780;&#36825;&#20123;&#20165;&#22312;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#26041;&#27861;&#20013;&#37096;&#20998;&#24471;&#21040;&#35299;&#20915;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#26041;&#27861;&#26469;&#22788;&#29702;PAVI&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Product attribute-value identification (PAVI) has been studied to link products on e-commerce sites with their attribute values (e.g., &lt;Material, Cotton&gt;) using product text as clues. Technical demands from real-world e-commerce platforms require PAVI methods to handle unseen values, multi-attribute values, and canonicalized values, which are only partly addressed in existing extraction- and classification-based approaches. Motivated by this, we explore a generative approach to the PAVI task. We finetune a pre-trained generative model, T5, to decode a set of attribute-value pairs as a target sequence from the given product text. Since the attribute value pairs are unordered set elements, how to linearize them will matter; we, thus, explore methods of composing an attribute-value pair and ordering the pairs for the task. Experimental results confirm that our generation-based approach outperforms the existing extraction and classification-based methods on large-scale real-world datasets 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;LoST&#65306;Reddit&#24086;&#23376;&#20013;&#20302;&#33258;&#23562;&#24515;&#12289;&#21463;&#25387;&#30340;&#24402;&#23646;&#24863;&#21644;&#24863;&#30693;&#30340;&#36127;&#25285;&#30340;&#31934;&#31070;&#20581;&#24247;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#24110;&#21161;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#21487;&#33021;&#38656;&#35201;&#24178;&#39044;&#30340;&#20010;&#20307;&#12290;</title><link>http://arxiv.org/abs/2306.05596</link><description>&lt;p&gt;
LOST: Reddit&#24086;&#23376;&#20013;&#20302;&#33258;&#23562;&#24515;&#31934;&#31070;&#20581;&#24247;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
LOST: A Mental Health Dataset of Low Self-esteem in Reddit Posts. (arXiv:2306.05596v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05596
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;LoST&#65306;Reddit&#24086;&#23376;&#20013;&#20302;&#33258;&#23562;&#24515;&#12289;&#21463;&#25387;&#30340;&#24402;&#23646;&#24863;&#21644;&#24863;&#30693;&#30340;&#36127;&#25285;&#30340;&#31934;&#31070;&#20581;&#24247;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#24110;&#21161;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#21487;&#33021;&#38656;&#35201;&#24178;&#39044;&#30340;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#33258;&#23562;&#21644;&#20154;&#38469;&#38656;&#27714;&#65288;&#21363;&#21463;&#25387;&#30340;&#24402;&#23646;&#24863;&#21644;&#24863;&#30693;&#30340;&#36127;&#25285;&#65289;&#23545;&#25233;&#37057;&#21644;&#33258;&#26432;&#20225;&#22270;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#20010;&#20307;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#23547;&#27714;&#31038;&#20132;&#36830;&#32467;&#20197;&#22686;&#24378;&#21644;&#32531;&#35299;&#20182;&#20204;&#30340;&#23396;&#29420;&#24863;&#12290;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20351;&#20154;&#20204;&#33021;&#22815;&#34920;&#36798;&#20182;&#20204;&#30340;&#24819;&#27861;&#12289;&#32463;&#39564;&#12289;&#20449;&#20208;&#21644;&#24773;&#24863;&#12290;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24515;&#29702;&#20581;&#24247;&#30740;&#31350;&#38598;&#20013;&#22312;&#30151;&#29366;&#12289;&#21407;&#22240;&#21644;&#38556;&#30861;&#19978;&#12290;&#32780;&#21021;&#27493;&#30340;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#31579;&#36873;&#20986;&#20154;&#38469;&#39118;&#38505;&#22240;&#32032;&#21644;&#20302;&#33258;&#23562;&#24515;&#21487;&#33021;&#20250;&#24341;&#21457;&#24515;&#29702;&#38556;&#30861;&#32773;&#30340;&#26089;&#26399;&#35686;&#25253;&#65292;&#24182;&#25351;&#27966;&#27835;&#30103;&#24072;&#23545;&#20854;&#36827;&#34892;&#24178;&#39044;&#12290;&#26631;&#20934;&#21270;&#37327;&#34920;&#20351;&#29992;&#24515;&#29702;&#23398;&#29702;&#35770;&#21046;&#23450;&#30340;&#38382;&#39064;&#26469;&#34913;&#37327;&#33258;&#23562;&#24515;&#21644;&#20154;&#38469;&#38656;&#27714;&#12290;&#22312;&#30446;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#21644;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;LoST&#65306;&#20302;&#33258;&#23562;&#24515;&#65292;&#29992;&#20110;&#30740;&#31350;&#21644;&#26816;&#27979;Reddit&#19978;&#30340;&#20302;&#33258;&#23562;&#24515;&#12290;&#36890;&#36807;&#19968;&#31181;&#27880;&#37322;&#26041;&#27861;&#65292;&#35813;&#27880;&#37322;&#21253;&#25324;&#23545;&#27880;&#37322;&#30340;&#19968;&#33268;&#24615;&#12289;&#27491;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#26816;&#26597;&#65292;&#25105;&#20204;&#30830;&#20445;&#20102;&#25968;&#25454;&#38598;&#30340;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;10,000&#20010;Reddit&#24086;&#23376;&#65292;&#36825;&#20123;&#24086;&#23376;&#26631;&#27880;&#20102;&#20302;&#33258;&#23562;&#24515;&#12289;&#21463;&#25387;&#30340;&#24402;&#23646;&#24863;&#21644;&#24863;&#30693;&#30340;&#36127;&#25285;&#12290;&#25968;&#25454;&#38598;&#30001;&#24515;&#29702;&#23398;&#19987;&#23478;&#36827;&#34892;&#27880;&#37322;&#65292;&#20182;&#20204;&#36981;&#24490;&#20102;&#25104;&#29087;&#30340;&#24515;&#29702;&#23398;&#29702;&#35770;&#65292;&#29992;&#20110;&#20302;&#33258;&#23562;&#24515;&#35782;&#21035;&#21644;&#20154;&#38469;&#38656;&#27714;&#35780;&#20272;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#24182;&#35780;&#20272;&#20302;&#33258;&#23562;&#24515;&#26816;&#27979;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#38656;&#35201;&#38024;&#23545;&#20302;&#33258;&#23562;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#21487;&#33021;&#38656;&#35201;&#24178;&#39044;&#30340;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low self-esteem and interpersonal needs (i.e., thwarted belongingness (TB) and perceived burdensomeness (PB)) have a major impact on depression and suicide attempts. Individuals seek social connectedness on social media to boost and alleviate their loneliness. Social media platforms allow people to express their thoughts, experiences, beliefs, and emotions. Prior studies on mental health from social media have focused on symptoms, causes, and disorders. Whereas an initial screening of social media content for interpersonal risk factors and low self-esteem may raise early alerts and assign therapists to at-risk users of mental disturbance. Standardized scales measure self-esteem and interpersonal needs from questions created using psychological theories. In the current research, we introduce a psychology-grounded and expertly annotated dataset, LoST: Low Self esTeem, to study and detect low self-esteem on Reddit. Through an annotation approach involving checks on coherence, correctness,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#20266;&#21517;&#21270;&#25216;&#26415;&#22312;NLP&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#25968;&#25454;&#20445;&#25252;&#21644;&#25928;&#29992;&#20445;&#25252;&#20043;&#38388;&#26435;&#34913;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.05561</link><description>&lt;p&gt;
&#21311;&#21517;&#21270;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#25928;&#29992;&#20445;&#25252;&#65306;&#20266;&#21517;&#21270;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Privacy- and Utility-Preserving NLP with Anonymized Data: A case study of Pseudonymization. (arXiv:2306.05561v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#20266;&#21517;&#21270;&#25216;&#26415;&#22312;NLP&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#25968;&#25454;&#20445;&#25252;&#21644;&#25928;&#29992;&#20445;&#25252;&#20043;&#38388;&#26435;&#34913;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19981;&#21516;&#20266;&#21517;&#21270;&#25216;&#26415;&#65288;&#20174;&#22522;&#20110;&#35268;&#21017;&#30340;&#26367;&#20195;&#21040;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65289;&#22312;&#29992;&#20110;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;NLP&#20219;&#21153;&#65288;&#25991;&#26412;&#20998;&#31867;&#21644;&#25688;&#35201;&#65289;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#20851;&#20110;&#21407;&#22987;&#25968;&#25454;&#19982;&#21311;&#21517;&#21270;&#25968;&#25454;&#65288;&#37325;&#28857;&#25918;&#22312;&#20266;&#21517;&#21270;&#25216;&#26415;&#19978;&#65289;&#21644;&#27169;&#22411;&#36136;&#37327;&#20043;&#38388;&#24046;&#36317;&#30340;&#37325;&#35201;&#35265;&#35299;&#65292;&#24182;&#20419;&#36827;&#20102;&#26410;&#26469;&#23545;&#26356;&#39640;&#36136;&#37327;&#30340;&#21311;&#21517;&#21270;&#25216;&#26415;&#36827;&#34892;&#30740;&#31350;&#65292;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#25968;&#25454;&#20445;&#25252;&#21644;&#25928;&#29992;&#20445;&#25252;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#20844;&#24320;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#20266;&#21517;&#21270;&#25968;&#25454;&#38598;&#21644;&#19979;&#28216;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the effectiveness of different pseudonymization techniques, ranging from rule-based substitutions to using pre-trained Large Language Models (LLMs), on a variety of datasets and models used for two widely used NLP tasks: text classification and summarization. Our work provides crucial insights into the gaps between original and anonymized data (focusing on the pseudonymization technique) and model quality and fosters future research into higher-quality anonymization techniques to better balance the trade-offs between data protection and utility preservation. We make our code, pseudonymized datasets, and downstream models publicly available
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#21644;&#24773;&#32490;&#24341;&#23548;&#30340;&#25913;&#20889;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#35843;&#33410;&#65292;&#21487;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#65292;&#36890;&#36807;&#28155;&#21152;&#32454;&#31890;&#24230;&#24773;&#24863;&#26631;&#31614;&#21487;&#20197;&#25552;&#39640;&#32467;&#26524;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.05556</link><description>&lt;p&gt;
&#24773;&#24863;&#21644;&#24773;&#32490;&#24341;&#23548;&#30340;&#25913;&#20889;
&lt;/p&gt;
&lt;p&gt;
Emotion and Sentiment Guided Paraphrasing. (arXiv:2306.05556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#21644;&#24773;&#32490;&#24341;&#23548;&#30340;&#25913;&#20889;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#35843;&#33410;&#65292;&#21487;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#65292;&#36890;&#36807;&#28155;&#21152;&#32454;&#31890;&#24230;&#24773;&#24863;&#26631;&#31614;&#21487;&#20197;&#25552;&#39640;&#32467;&#26524;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#20889;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24120;&#35265;&#19988;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#23545;&#25991;&#26412;&#36827;&#34892;&#20462;&#25913;&#21516;&#26102;&#20445;&#30041;&#21407;&#26412;&#30340;&#24847;&#24605;&#12290;&#24773;&#32490;&#25913;&#20889;&#21487;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#65292;&#21253;&#25324;&#35843;&#33410;&#22312;&#32447;&#23545;&#35805;&#21644;&#38450;&#27490;&#32593;&#32476;&#27450;&#20940;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32454;&#31890;&#24230;&#24773;&#32490;&#25913;&#20889;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#21407;&#25991;&#24847;&#20041;&#30340;&#21516;&#26102;&#65292;&#24179;&#28369;&#22320;&#25913;&#21464;&#20854;&#24773;&#24863;&#24378;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#35843;&#33410;&#12290;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#32454;&#31890;&#24230;&#24773;&#24863;&#26631;&#31614;&#23545;&#20960;&#20010;&#24120;&#29992;&#30340;&#25913;&#20889;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#37325;&#24314;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26377;&#26465;&#20214;&#30340;&#25991;&#26412;&#29983;&#25104;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#21644;&#24773;&#32490;&#24341;&#23548;&#30340;&#25913;&#20889;&#26694;&#26550;&#12290;&#23545;&#35843;&#25972;&#21518;&#30340;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#23558;&#32454;&#31890;&#24230;&#24773;&#24863;&#26631;&#31614;&#21253;&#21547;&#22312;&#25913;&#20889;&#35757;&#32451;&#20013;&#21487;&#20197;&#25552;&#39640;&#32467;&#26524;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Paraphrase generation, a.k.a. paraphrasing, is a common and important task in natural language processing. Emotional paraphrasing, which changes the emotion embodied in a piece of text while preserving its meaning, has many potential applications, including moderating online dialogues and preventing cyberbullying. We introduce a new task of fine-grained emotional paraphrasing along emotion gradients, that is, altering the emotional intensities of the paraphrases in fine-grained settings following smooth variations in affective dimensions while preserving the meaning of the original text. We reconstruct several widely used paraphrasing datasets by augmenting the input and target texts with their fine-grained emotion labels. Then, we propose a framework for emotion and sentiment guided paraphrasing by leveraging pre-trained language models for conditioned text generation. Extensive evaluation of the fine-tuned models suggests that including fine-grained emotion labels in the paraphrase t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25991;&#26412;&#27169;&#31946;&#21270;&#26694;&#26550;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#24182;&#25193;&#23637;&#24515;&#29702;&#20445;&#20581;&#20132;&#20184;&#12290;&#32467;&#26524;&#34920;&#26126;&#21363;&#20351;&#19981;&#25552;&#20379;&#21407;&#22987;&#29992;&#25143;&#25991;&#26412;&#65292;ChatGPT&#30340;&#24314;&#35758;&#20173;&#28982;&#33021;&#22815;&#36866;&#24230;&#22320;&#26377;&#24110;&#21161;&#21644;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05552</link><description>&lt;p&gt;
ChatGPT for Us: &#36890;&#36807;&#23545;&#35805;&#25991;&#26412;&#27169;&#31946;&#21270;&#25193;&#23637;&#24515;&#29702;&#20445;&#20581;&#20132;&#20184;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Us: Preserving Data Privacy in ChatGPT via Dialogue Text Ambiguation to Expand Mental Health Care Delivery. (arXiv:2306.05552v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25991;&#26412;&#27169;&#31946;&#21270;&#26694;&#26550;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#24182;&#25193;&#23637;&#24515;&#29702;&#20445;&#20581;&#20132;&#20184;&#12290;&#32467;&#26524;&#34920;&#26126;&#21363;&#20351;&#19981;&#25552;&#20379;&#21407;&#22987;&#29992;&#25143;&#25991;&#26412;&#65292;ChatGPT&#30340;&#24314;&#35758;&#20173;&#28982;&#33021;&#22815;&#36866;&#24230;&#22320;&#26377;&#24110;&#21161;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25193;&#23637;&#24515;&#29702;&#20445;&#20581;&#20132;&#20184;&#38750;&#24120;&#26377;&#29992;&#12290;&#23588;&#20854;&#26159;ChatGPT&#22240;&#20854;&#29983;&#25104;&#20154;&#31867;&#23545;&#35805;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#25935;&#24863;&#30340;&#39046;&#22495;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#21307;&#30103;&#20445;&#20581;&#65292;&#30001;&#20110;&#38544;&#31169;&#21644;&#25968;&#25454;&#25152;&#26377;&#26435;&#38382;&#39064;&#65292;&#26080;&#27861;&#20351;&#29992;ChatGPT&#12290;&#20026;&#20102;&#20351;&#20854;&#24471;&#21040;&#21033;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25991;&#26412;&#27169;&#31946;&#21270;&#26694;&#26550;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#25105;&#20204;&#25226;&#36825;&#20010;&#26694;&#26550;&#29992;&#20110;&#35299;&#20915;&#29992;&#25143;&#25552;&#20379;&#30340;&#21387;&#21147;&#38382;&#39064;&#65292;&#20197;&#23637;&#31034;&#36825;&#31181;&#38544;&#31169;&#20445;&#25252;&#29983;&#25104;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#30410;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#25552;&#20379;&#21407;&#22987;&#29992;&#25143;&#25991;&#26412;&#65292;ChatGPT&#30340;&#24314;&#35758;&#20173;&#28982;&#33021;&#22815;&#36866;&#24230;&#22320;&#26377;&#24110;&#21161;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have been useful in expanding mental health care delivery. ChatGPT, in particular, has gained popularity for its ability to generate human-like dialogue. However, data-sensitive domains -- including but not limited to healthcare -- face challenges in using ChatGPT due to privacy and data-ownership concerns. To enable its utilization, we propose a text ambiguation framework that preserves user privacy. We ground this in the task of addressing stress prompted by user-provided texts to demonstrate the viability and helpfulness of privacy-preserved generations. Our results suggest that chatGPT recommendations are still able to be moderately helpful and relevant, even when the original user text is not provided.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33521;&#35821;&#39044;&#35757;&#32451;&#30340;Masked Language Models&#65288;MLMs&#65289;&#20197;&#21450;&#23427;&#20204;&#30340;&#19979;&#28216;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#23545;&#32654;&#22269;93&#20010;&#31038;&#20250;&#27745;&#21517;&#21270;&#32676;&#20307;&#30340;&#20559;&#35265;&#65292;&#20026;&#20102;&#35780;&#20272;93&#20010;&#27745;&#21517;&#21270;&#26465;&#20214;&#30340;&#20559;&#35265;&#23384;&#22312;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#25214;&#21040;&#20102;&#20559;&#35265;&#23384;&#22312;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.05550</link><description>&lt;p&gt;
&#23545;&#24102;&#8220;&#26631;&#35760;&#35821;&#35328;&#27169;&#22411;&#8221;&#20013;93&#20010;&#21463;&#27495;&#35270;&#32676;&#20307;&#30340;&#20559;&#35265;&#21450;&#20854;&#23545;&#19979;&#28216;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks. (arXiv:2306.05550v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33521;&#35821;&#39044;&#35757;&#32451;&#30340;Masked Language Models&#65288;MLMs&#65289;&#20197;&#21450;&#23427;&#20204;&#30340;&#19979;&#28216;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#23545;&#32654;&#22269;93&#20010;&#31038;&#20250;&#27745;&#21517;&#21270;&#32676;&#20307;&#30340;&#20559;&#35265;&#65292;&#20026;&#20102;&#35780;&#20272;93&#20010;&#27745;&#21517;&#21270;&#26465;&#20214;&#30340;&#20559;&#35265;&#23384;&#22312;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#25214;&#21040;&#20102;&#20559;&#35265;&#23384;&#22312;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24555;&#36895;&#37096;&#32626;&#38656;&#35201;&#23545;&#36825;&#20123;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#35265;&#21644;&#39118;&#38505;&#36827;&#34892;&#24443;&#24213;&#30340;&#35843;&#26597;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#35268;&#27169;&#31038;&#20250;&#27745;&#21517;&#21270;&#30340;&#20559;&#35265;&#36827;&#34892;&#30740;&#31350;&#65292;&#25193;&#23637;&#20102;&#24050;&#26377;&#24037;&#20316;&#23545;&#20559;&#35265;&#35780;&#20272;&#30340;&#28966;&#28857;&#12290;&#23427;&#20851;&#27880;&#32654;&#22269;93&#20010;&#31038;&#20250;&#27745;&#21517;&#21270;&#32676;&#20307;&#65292;&#21253;&#25324;&#19982;&#30142;&#30149;&#12289;&#27531;&#30142;&#12289;&#33647;&#29289;&#20351;&#29992;&#12289;&#24515;&#29702;&#30142;&#30149;&#12289;&#23447;&#25945;&#12289;&#24615;&#21462;&#21521;&#12289;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#21644;&#20854;&#20182;&#30456;&#20851;&#22240;&#32032;&#26377;&#20851;&#30340;&#19968;&#31995;&#21015;&#24773;&#20917;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#33521;&#35821;&#39044;&#35757;&#32451;&#30340;Masked Language Models&#65288;MLMs&#65289;&#20197;&#21450;&#23427;&#20204;&#30340;&#19979;&#28216;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#23545;&#36825;&#20123;&#32676;&#20307;&#30340;&#20559;&#35265;&#12290;&#20026;&#20102;&#35780;&#20272;93&#20010;&#27745;&#21517;&#21270;&#26465;&#20214;&#30340;&#20559;&#35265;&#23384;&#22312;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;29&#20010;&#38750;&#27745;&#21517;&#21270;&#26465;&#20214;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#12290;&#22522;&#20110;&#31038;&#20250;&#25490;&#26021;&#30340;&#24515;&#29702;&#23398;&#23610;&#24230;-&#31038;&#20250;&#36317;&#31163;&#37327;&#34920;&#65292;&#25105;&#20204;&#23545;&#20845;&#20010;MLMs&#36827;&#34892;&#20102;&#25552;&#31034;&#65306;RoBERTa-base&#12289;RoBERTa-large&#12289;XLNet-large&#12289;BERTweet-base&#12289;BERTweet-la&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid deployment of artificial intelligence (AI) models demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-la
&lt;/p&gt;</description></item><item><title>&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;DetectLLM-LRR&#21644;DetectLLM-NPR&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26085;&#24535;&#25490;&#21517;&#20449;&#24687;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#26816;&#27979;&#25928;&#26524;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05540</link><description>&lt;p&gt;
DetectLLM&#65306;&#22522;&#20110;&#23545;&#25968;&#31209;&#20449;&#24687;&#30340;&#38646;&#26679;&#26412;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text. (arXiv:2306.05540v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05540
&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;DetectLLM-LRR&#21644;DetectLLM-NPR&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26085;&#24535;&#25490;&#21517;&#20449;&#24687;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#26816;&#27979;&#25928;&#26524;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#36827;&#23637;&#21644;&#25152;&#29983;&#25104;&#30340;&#25991;&#26412;&#25968;&#37327;&#30340;&#24040;&#22823;&#22686;&#21152;&#65292;&#20154;&#24037;&#21306;&#20998;&#25991;&#26412;&#26159;&#21542;&#20026;&#26426;&#22120;&#29983;&#25104;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#20999;&#23454;&#38469;&#12290;&#32771;&#34385;&#21040;LLM&#22312;&#31038;&#20132;&#23186;&#20307;&#21644;&#25945;&#32946;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#38450;&#27490;&#24694;&#24847;&#20351;&#29992;&#65292;&#22914;&#25220;&#34989;&#12289;&#34394;&#20551;&#20449;&#24687;&#21644;&#23459;&#20256;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;&#20960;&#31181;&#38646;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#29992;&#20110;&#21033;&#29992;&#23545;&#25968;&#31209;&#20449;&#24687;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#12290;&#20854;&#20013;&#19968;&#20010;&#31216;&#20026;DetectLLM-LRR&#65292;&#24555;&#36895;&#39640;&#25928;&#65292;&#21478;&#19968;&#20010;&#31216;&#20026;DetectLLM-NPR&#65292;&#26356;&#31934;&#30830;&#65292;&#20294;&#38656;&#35201;&#25200;&#21160;&#65292;&#22240;&#27492;&#36895;&#24230;&#26356;&#24930;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#19971;&#20010;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid progress of large language models (LLMs) and the huge amount of text they generated, it becomes more and more impractical to manually distinguish whether a text is machine-generated. Given the growing use of LLMs in social media and education, it prompts us to develop methods to detect machine-generated text, preventing malicious usage such as plagiarism, misinformation, and propaganda. Previous work has studied several zero-shot methods, which require no training data. These methods achieve good performance, but there is still a lot of room for improvement. In this paper, we introduce two novel zero-shot methods for detecting machine-generated text by leveraging the log rank information. One is called DetectLLM-LRR, which is fast and efficient, and the other is called DetectLLM-NPR, which is more accurate, but slower due to the need for perturbations. Our experiments on three datasets and seven language models show that our proposed methods improve over the state of the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25351;&#20196;&#35843;&#33410;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#65288;STL&#65289;&#35774;&#32622;&#19979;&#65292;&#37197;&#22791;25&#65285;&#19979;&#28216;&#35757;&#32451;&#25968;&#25454;&#30340;&#25351;&#20196;&#35843;&#33410;&#27169;&#22411;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05539</link><description>&lt;p&gt;
&#25351;&#20196;&#35843;&#33410;&#27169;&#22411;&#26159;&#24555;&#36895;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuned Models are Quick Learners. (arXiv:2306.05539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25351;&#20196;&#35843;&#33410;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#65288;STL&#65289;&#35774;&#32622;&#19979;&#65292;&#37197;&#22791;25&#65285;&#19979;&#28216;&#35757;&#32451;&#25968;&#25454;&#30340;&#25351;&#20196;&#35843;&#33410;&#27169;&#22411;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#35843;&#33410;&#22914;&#20309;&#22686;&#24378;&#27169;&#22411;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27010;&#25324;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#30340;&#30417;&#30563;&#23398;&#20064;&#20173;&#38656;&#35201;&#22823;&#37327;&#30340;&#19979;&#28216;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#24494;&#35843;&#24448;&#24448;&#20165;&#26377;&#23569;&#37327;&#25968;&#25454;&#21487;&#29992;&#65292;&#20171;&#20110;&#23569;&#26679;&#26412;&#25512;&#29702;&#21644;&#23436;&#20840;&#30417;&#30563;&#24494;&#35843;&#20043;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25351;&#20196;&#35843;&#33410;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#20272;&#35745;&#20854;&#25191;&#34892;&#36716;&#31227;&#23398;&#20064;&#24182;&#21305;&#37197;&#26368;&#20808;&#36827;&#30417;&#30563;&#27169;&#22411;&#25152;&#38656;&#30340;&#26368;&#23567;&#19979;&#28216;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#65288;STL&#65289;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#35774;&#32622;&#19979;&#65292;&#22312;&#26469;&#33258;&#36229;&#32423;&#33258;&#28982;&#25351;&#20196;&#65288;SuperNI&#65289;&#30340;119&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;STL&#35774;&#32622;&#19979;&#65292;&#37197;&#22791;25&#65285;&#19979;&#28216;&#35757;&#32451;&#25968;&#25454;&#30340;&#25351;&#20196;&#35843;&#33410;&#27169;&#22411;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning of language models has demonstrated the ability to enhance model generalization to unseen tasks via in-context learning using a few examples. However, typical supervised learning still requires a plethora of downstream training data for finetuning. Often in real-world situations, there is a scarcity of data available for finetuning, falling somewhere between few shot inference and fully supervised finetuning. In this work, we demonstrate the sample efficiency of instruction tuned models over various tasks by estimating the minimal downstream training data required by them to perform transfer learning and match the performance of state-of-the-art (SOTA) supervised models. We conduct experiments on 119 tasks from Super Natural Instructions (SuperNI) in both the single task learning (STL) and multi task learning (MTL) settings. Our findings reveal that, in the STL setting, instruction tuned models equipped with 25% of the downstream train data surpass the SOTA performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#38754;&#33258;&#36866;&#24212;&#30340;&#30693;&#35782;&#39537;&#21160;&#24847;&#35265;&#25688;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20016;&#23500;&#30340;&#30693;&#35782;&#24211;&#26469;&#26377;&#25928;&#22320;&#29983;&#25104;&#31616;&#30701;&#20294;&#26174;&#33879;&#30340;&#25688;&#35201;&#65292;&#25552;&#39640;&#20102;&#24847;&#35265;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05537</link><description>&lt;p&gt;
AaKOS: &#26041;&#38754;&#33258;&#36866;&#24212;&#30693;&#35782;&#39537;&#21160;&#30340;&#24847;&#35265;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
AaKOS: Aspect-adaptive Knowledge-based Opinion Summarization. (arXiv:2306.05537v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#38754;&#33258;&#36866;&#24212;&#30340;&#30693;&#35782;&#39537;&#21160;&#24847;&#35265;&#25688;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20016;&#23500;&#30340;&#30693;&#35782;&#24211;&#26469;&#26377;&#25928;&#22320;&#29983;&#25104;&#31616;&#30701;&#20294;&#26174;&#33879;&#30340;&#25688;&#35201;&#65292;&#25552;&#39640;&#20102;&#24847;&#35265;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#20449;&#24687;&#30340;&#24555;&#36895;&#22686;&#38271;&#23548;&#33268;&#21508;&#31181;&#27963;&#21160;&#12289;&#20135;&#21697;&#21644;&#26381;&#21153;&#19978;&#26377;&#21387;&#20498;&#24615;&#30340;&#24847;&#35265;&#21644;&#35780;&#35770;&#12290;&#36825;&#20351;&#24471;&#29992;&#25143;&#22312;&#20570;&#20915;&#31574;&#26102;&#22788;&#29702;&#25152;&#26377;&#21487;&#29992;&#20449;&#24687;&#21464;&#24471;&#22256;&#38590;&#21644;&#32791;&#26102;&#12290;&#25991;&#26412;&#25688;&#35201;&#26159;&#19968;&#39033;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24050;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#21487;&#36890;&#36807;&#20174;&#38271;&#25991;&#26723;&#25110;&#22810;&#20010;&#25991;&#20214;&#20013;&#29983;&#25104;&#31616;&#30701;&#32780;&#26174;&#33879;&#30340;&#20869;&#23481;&#26469;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#12290;&#26368;&#36817;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#36164;&#28304;&#65292;&#24182;&#19988;&#38590;&#20197;&#20316;&#20026;&#31163;&#32447;&#24212;&#29992;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#24120;&#24120;&#32570;&#20047;&#8220;&#33258;&#36866;&#24212;&#8221;&#33021;&#21147;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#22810;&#26679;&#21270;&#30340;&#26041;&#38754;&#65292;&#36825;&#23545;&#20855;&#26377;&#29305;&#23450;&#35201;&#27714;&#25110;&#20559;&#22909;&#30340;&#29992;&#25143;&#29305;&#21035;&#19981;&#21033;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#38754;&#33258;&#36866;&#24212;&#30340;&#30693;&#35782;&#39537;&#21160;&#24847;&#35265;&#25688;&#35201;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of information on the Internet has led to an overwhelming amount of opinions and comments on various activities, products, and services. This makes it difficult and time-consuming for users to process all the available information when making decisions. Text summarization, a Natural Language Processing (NLP) task, has been widely explored to help users quickly retrieve relevant information by generating short and salient content from long or multiple documents. Recent advances in pre-trained language models, such as ChatGPT, have demonstrated the potential of Large Language Models (LLMs) in text generation. However, LLMs require massive amounts of data and resources and are challenging to implement as offline applications. Furthermore, existing text summarization approaches often lack the ``adaptive" nature required to capture diverse aspects in opinion summarization, which is particularly detrimental to users with specific requirements or preferences. In this paper, w
&lt;/p&gt;</description></item><item><title>&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#30340;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#21487;&#20197;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#26816;&#27979;&#21644;&#30830;&#35748;&#65292;&#36825;&#21487;&#24110;&#21161;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#36827;&#34892;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.05535</link><description>&lt;p&gt;
&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#26816;&#27979;&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;
&lt;/p&gt;
&lt;p&gt;
Detecting Check-Worthy Claims in Political Debates, Speeches, and Interviews Using Audio Data. (arXiv:2306.05535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05535
&lt;/p&gt;
&lt;p&gt;
&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#30340;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#21487;&#20197;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#26816;&#27979;&#21644;&#30830;&#35748;&#65292;&#36825;&#21487;&#24110;&#21161;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#36827;&#34892;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#30340;&#19968;&#22823;&#37096;&#20998;&#22242;&#32467;&#22312;&#30456;&#21516;&#30340;&#24895;&#26223;&#21644;&#24605;&#24819;&#21608;&#22260;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#33021;&#37327;&#12290;&#36825;&#27491;&#26159;&#25919;&#27835;&#20154;&#29289;&#24076;&#26395;&#20026;&#20182;&#20204;&#30340;&#20107;&#19994;&#25152;&#32047;&#31215;&#30340;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#20182;&#20204;&#26377;&#26102;&#20250;&#20351;&#29992;&#25197;&#26354;&#25110;&#38544;&#34255;&#30495;&#30456;&#30340;&#25163;&#27573;&#65292;&#26080;&#35770;&#26159;&#26080;&#24847;&#30340;&#36824;&#26159;&#26377;&#24847;&#30340;&#65292;&#36825;&#20026;&#38169;&#35823;&#20449;&#24687;&#21644;&#35823;&#23548;&#24320;&#20102;&#22823;&#38376;&#12290;&#33258;&#21160;&#26816;&#27979;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#30340;&#24037;&#20855;&#23558;&#23545;&#36777;&#35770;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#26377;&#24456;&#22823;&#24110;&#21161;&#12290;&#34429;&#28982;&#20197;&#21069;&#20851;&#20110;&#26816;&#27979;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#25991;&#26412;&#65292;&#20294;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#38899;&#39057;&#20449;&#21495;&#20316;&#20026;&#39069;&#22806;&#20449;&#24687;&#28304;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65288;&#33521;&#35821;&#25991;&#26412;&#21644;&#38899;&#39057;&#65289;&#65292;&#21253;&#21547;48&#23567;&#26102;&#30340;&#28436;&#35762;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#28436;&#35762;&#32773;&#30340;&#24773;&#20917;&#19979;&#65292;&#38899;&#39057;&#27169;&#24577;&#19982;&#25991;&#26412;&#32467;&#21512;&#20351;&#29992;&#27604;&#20165;&#20351;&#29992;&#25991;&#26412;&#20855;&#26377;&#25913;&#36827;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#21333;&#22768;&#36947;&#38899;&#39057;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#21333;&#22768;&#36947;&#25991;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large portion of society united around the same vision and ideas carries enormous energy. That is precisely what political figures would like to accumulate for their cause. With this goal in mind, they can sometimes resort to distorting or hiding the truth, unintentionally or on purpose, which opens the door for misinformation and disinformation. Tools for automatic detection of check-worthy claims would be of great help to moderators of debates, journalists, and fact-checking organizations. While previous work on detecting check-worthy claims has focused on text, here we explore the utility of the audio signal as an additional information source. We create a new multimodal dataset (text and audio in English) containing 48 hours of speech. Our evaluation results show that the audio modality together with text yields improvements over text alone in the case of multiple speakers. Moreover, an audio-only model could outperform a text-only one for a single speaker.
&lt;/p&gt;</description></item><item><title>FACTIFY3M&#26159;&#19968;&#20010;&#20197;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#39564;&#35777;&#20026;&#30446;&#26631;&#30340;&#25968;&#25454;&#38598;&#12290;&#34394;&#20551;&#20449;&#24687;&#22914;&#20170;&#24050;&#25104;&#20026;&#24403;&#19979;&#37325;&#22823;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#36825;&#19968;&#25968;&#25454;&#38598;&#26088;&#22312;&#36890;&#36807;&#22810;&#27169;&#24335;&#39564;&#35777;&#26469;&#21450;&#26102;&#35782;&#21035;&#21644;&#32531;&#35299;&#34394;&#20551;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.05523</link><description>&lt;p&gt;
FACTIFY3M: &#36890;&#36807;5W&#38382;&#31572;&#35299;&#37322;&#30340;&#22810;&#27169;&#24335;&#20107;&#23454;&#39564;&#35777;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability through 5W Question-Answering. (arXiv:2306.05523v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05523
&lt;/p&gt;
&lt;p&gt;
FACTIFY3M&#26159;&#19968;&#20010;&#20197;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#39564;&#35777;&#20026;&#30446;&#26631;&#30340;&#25968;&#25454;&#38598;&#12290;&#34394;&#20551;&#20449;&#24687;&#22914;&#20170;&#24050;&#25104;&#20026;&#24403;&#19979;&#37325;&#22823;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#36825;&#19968;&#25968;&#25454;&#38598;&#26088;&#22312;&#36890;&#36807;&#22810;&#27169;&#24335;&#39564;&#35777;&#26469;&#21450;&#26102;&#35782;&#21035;&#21644;&#32531;&#35299;&#34394;&#20551;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#26159;&#24403;&#21069;&#20127;&#24453;&#35299;&#20915;&#30340;&#31038;&#20250;&#21361;&#26426;&#20043;&#19968;&#8212;&#8212;&#22823;&#32422;67%&#30340;&#32654;&#22269;&#20154;&#35748;&#20026;&#34394;&#20551;&#20449;&#24687;&#20250;&#20135;&#29983;&#22823;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20854;&#20013;&#26377;10%&#30340;&#20154;&#26377;&#24847;&#35782;&#22320;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#12290;&#35777;&#25454;&#34920;&#26126;&#65292;&#34394;&#20551;&#20449;&#24687;&#21487;&#20197;&#25805;&#32437;&#27665;&#20027;&#36827;&#31243;&#21644;&#20844;&#20247;&#33286;&#35770;&#65292;&#24182;&#22312;&#21361;&#26426;&#26399;&#38388;&#24341;&#36215;&#32929;&#24066;&#21160;&#33633;&#12289;&#31038;&#20250;&#24656;&#24908;&#29978;&#33267;&#27515;&#20129;&#12290;&#22240;&#27492;&#65292;&#24212;&#21450;&#26102;&#35782;&#21035;&#24182;&#23613;&#21487;&#33021;&#32531;&#35299;&#34394;&#20551;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#27599;&#22825;&#20998;&#20139;&#22823;&#32422;32&#20159;&#24352;&#22270;&#20687;&#21644;720,000 &#23567;&#26102;&#30340;&#35270;&#39057;&#65292;&#22240;&#27492;&#23545;&#20110;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#30340;&#21487;&#25193;&#23637;&#24615;&#26816;&#27979;&#38656;&#35201;&#39640;&#25928;&#30340;&#20107;&#23454;&#39564;&#35777;&#12290;&#23613;&#31649;&#22312;&#25991;&#26412;&#27169;&#24335;&#19979;&#33258;&#21160;&#20107;&#23454;&#39564;&#35777;&#21462;&#24471;&#20102;&#36827;&#23637;(&#20363;&#22914;&#65292;FEVER, LIAR)&#65292;&#20294;&#23398;&#26415;&#30028;&#22312;&#22810;&#27169;&#24335;&#20107;&#23454;&#39564;&#35777;&#26041;&#38754;&#32570;&#20047;&#23454;&#36136;&#24615;&#30340;&#21162;&#21147;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FACTIFY3M&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;300&#19975;&#20010;&#26679;&#26412;&#65292;&#36890;&#36807;&#22810;&#31181;&#27169;&#24335;&#21644;5W&#38382;&#31572;&#25552;&#39640;&#20102;&#20107;&#23454;&#39564;&#35777;&#39046;&#22495;&#30340;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combating disinformation is one of the burning societal crises -- about 67% of the American population believes that disinformation produces a lot of uncertainty, and 10% of them knowingly propagate disinformation. Evidence shows that disinformation can manipulate democratic processes and public opinion, causing disruption in the share market, panic and anxiety in society, and even death during crises. Therefore, disinformation should be identified promptly and, if possible, mitigated. With approximately 3.2 billion images and 720,000 hours of video shared online daily on social media platforms, scalable detection of multimodal disinformation requires efficient fact verification. Despite progress in automatic text-based fact verification (e.g., FEVER, LIAR), the research community lacks substantial effort in multimodal fact verification. To address this gap, we introduce FACTIFY 3M, a dataset of 3 million samples that pushes the boundaries of the domain of fact verification via a multi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#23569;&#25968;&#26063;&#35028;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23631;&#34109;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#25552;&#31034;&#20013;&#21333;&#35789;&#24471;&#20998;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#20197;&#35782;&#21035;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;</title><link>http://arxiv.org/abs/2306.05500</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#20998;&#26512;&#20559;&#35265;&#30340;&#21333;&#35789;&#32423;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Word-Level Explanations for Analyzing Bias in Text-to-Image Models. (arXiv:2306.05500v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#23569;&#25968;&#26063;&#35028;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23631;&#34109;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#25552;&#31034;&#20013;&#21333;&#35789;&#24471;&#20998;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#20197;&#35782;&#21035;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#25509;&#25910;&#19968;&#21477;&#35805;&#65288;&#21363;&#25552;&#31034;&#65289;&#24182;&#29983;&#25104;&#19982;&#35813;&#36755;&#20837;&#25552;&#31034;&#30456;&#20851;&#32852;&#30340;&#22270;&#20687;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#22522;&#20110;&#31181;&#26063;&#21644;&#24615;&#21035;&#32780;&#20559;&#34962;&#23569;&#25968;&#26063;&#35028;&#30340;&#22270;&#20687;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36755;&#20837;&#25552;&#31034;&#20013;&#21738;&#20010;&#21333;&#35789;&#23548;&#33268;&#29983;&#25104;&#22270;&#20687;&#20986;&#29616;&#20559;&#35265;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35745;&#31639;&#25552;&#31034;&#20013;&#27599;&#20010;&#21333;&#35789;&#24471;&#20998;&#30340;&#26041;&#27861;&#65307;&#36825;&#20123;&#24471;&#20998;&#20195;&#34920;&#20854;&#22312;&#27169;&#22411;&#36755;&#20986;&#20559;&#24046;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36981;&#24490;&#8220;&#21024;&#38500;&#35299;&#37322;&#8221;&#30340;&#21407;&#21017;&#65292;&#21033;&#29992;&#23631;&#34109;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#24433;&#21709;&#24471;&#20998;&#12290;&#25105;&#20204;&#22312;&#31283;&#23450;&#25193;&#25955;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#22797;&#21046;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image models take a sentence (i.e., prompt) and generate images associated with this input prompt. These models have created award wining-art, videos, and even synthetic datasets. However, text-to-image (T2I) models can generate images that underrepresent minorities based on race and sex. This paper investigates which word in the input prompt is responsible for bias in generated images. We introduce a method for computing scores for each word in the prompt; these scores represent its influence on biases in the model's output. Our method follows the principle of \emph{explaining by removing}, leveraging masked language models to calculate the influence scores. We perform experiments on Stable Diffusion to demonstrate that our method identifies the replication of societal stereotypes in generated images.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#22797;&#26434;&#24615;&#21644;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#25216;&#26415;HouYi&#65292;&#24182;&#25581;&#31034;&#20102;&#24212;&#29992;&#31243;&#24207;&#25552;&#31034;&#26426;&#21046;&#20013;&#20197;&#21069;&#26410;&#30693;&#21644;&#20005;&#37325;&#20302;&#20272;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21628;&#21505;&#36827;&#19968;&#27493;&#24320;&#21457;&#20840;&#38754;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#20197;&#25269;&#24481;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.05499</link><description>&lt;p&gt;
LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Prompt Injection attack against LLM-integrated Applications. (arXiv:2306.05499v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#22797;&#26434;&#24615;&#21644;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#25216;&#26415;HouYi&#65292;&#24182;&#25581;&#31034;&#20102;&#24212;&#29992;&#31243;&#24207;&#25552;&#31034;&#26426;&#21046;&#20013;&#20197;&#21069;&#26410;&#30693;&#21644;&#20005;&#37325;&#20302;&#20272;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21628;&#21505;&#36827;&#19968;&#27493;&#24320;&#21457;&#20840;&#38754;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#20197;&#25269;&#24481;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#22240;&#20854;&#21331;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#32780;&#22312;&#23427;&#20204;&#21608;&#22260;&#21050;&#28608;&#20102;&#19968;&#20010;&#20805;&#28385;&#27963;&#21147;&#30340;&#24212;&#29992;&#29983;&#24577;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21508;&#31181;&#26381;&#21153;&#20013;&#30340;&#24191;&#27867;&#34701;&#21512;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#23558;&#35299;&#26500;&#23454;&#38469;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#22797;&#26434;&#24615;&#21644;&#24433;&#21709;&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#23545;&#21313;&#20010;&#21830;&#19994;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#30446;&#21069;&#25915;&#20987;&#31574;&#30053;&#22312;&#23454;&#36341;&#20013;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#21463;&#36825;&#20123;&#38480;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#38543;&#21518;&#21046;&#23450;&#20102;HouYi&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#25216;&#26415;&#65292;&#23427;&#20511;&#37492;&#20102;&#20256;&#32479;&#30340;Web&#27880;&#20837;&#25915;&#20987;&#12290;HouYi&#20998;&#20026;&#19977;&#20010;&#20851;&#38190;&#20803;&#32032;: &#19968;&#20010;&#26080;&#32541;&#38598;&#25104;&#30340;&#39044;&#26500;&#24314;&#25552;&#31034;&#12289;&#19968;&#20010;&#27880;&#20837;&#25552;&#31034;&#35825;&#23548;&#19978;&#19979;&#25991;&#20998;&#21306;&#20197;&#21450;&#19968;&#20010;&#24694;&#24847;&#36733;&#33655;&#65292;&#26088;&#22312;&#23454;&#29616;&#25915;&#20987;&#30446;&#26631;&#12290;&#21033;&#29992;HouYi&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24212;&#29992;&#31243;&#24207;&#25552;&#31034;&#26426;&#21046;&#20013;&#20197;&#21069;&#26410;&#30693;&#21644;&#20005;&#37325;&#20302;&#20272;&#30340;&#28431;&#27934;&#65292;&#24182;&#28436;&#31034;&#20102;&#32469;&#36807;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21628;&#21505;&#36827;&#19968;&#27493;&#30740;&#31350;&#24320;&#21457;&#20840;&#38754;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#20197;&#25269;&#24481;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HouYi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HouYi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HouYi, we unveil previously unknown and sev
&lt;/p&gt;</description></item><item><title>&#20845;&#36793;&#24418;&#26631;&#27880;&#22120;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20381;&#23384;&#20998;&#26512;&#22120;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#26102;&#23454;&#29616;&#23436;&#20840;&#24182;&#34892;&#21270;&#65292;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312; Penn Treebank &#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05477</link><description>&lt;p&gt;
&#20845;&#36793;&#24418;&#26631;&#27880;&#65306;&#23558;&#25237;&#24433;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#20316;&#20026;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Hexatagging: Projective Dependency Parsing as Tagging. (arXiv:2306.05477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05477
&lt;/p&gt;
&lt;p&gt;
&#20845;&#36793;&#24418;&#26631;&#27880;&#22120;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20381;&#23384;&#20998;&#26512;&#22120;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#26102;&#23454;&#29616;&#23436;&#20840;&#24182;&#34892;&#21270;&#65292;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312; Penn Treebank &#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20381;&#23384;&#20998;&#26512;&#22120;&#8212;&#8212;&#20845;&#36793;&#24418;&#26631;&#27880;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#21477;&#23376;&#20013;&#30340;&#21333;&#35789;&#26631;&#35760;&#20026;&#26469;&#33258;&#21487;&#33021;&#26631;&#35760;&#26377;&#38480;&#38598;&#21512;&#20013;&#30340;&#20803;&#32032;&#26469;&#26500;&#24314;&#20381;&#23384;&#26641;&#12290;&#19982;&#35768;&#22810;&#22788;&#29702;&#20381;&#23384;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#26102;&#26159;&#23436;&#20840;&#21487;&#24182;&#34892;&#21270;&#30340;&#65292;&#21363;&#29992;&#20110;&#26500;&#24314;&#20381;&#23384;&#20998;&#26512;&#25152;&#38656;&#30340;&#32467;&#26500;&#26500;&#24314;&#25805;&#20316;&#21487;&#20197;&#30456;&#20114;&#24182;&#34892;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#30830;&#20999;&#35299;&#30721;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#37117;&#26159;&#32447;&#24615;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#20381;&#23384;&#20998;&#26512;&#22120;&#65292;&#23427;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#24449;&#26469;&#39044;&#27979;&#20845;&#36793;&#26631;&#35760;&#65292;&#32780;&#19981;&#38656;&#35201;&#19987;&#20026;&#27492;&#20219;&#21153;&#26126;&#30830;&#35774;&#35745;&#30340;&#23450;&#21046;&#20307;&#31995;&#32467;&#26500;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#24615;&#21644;&#31616;&#21333;&#24615;&#65292;&#20294;&#22312; Penn Treebank &#27979;&#35797;&#38598;&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102; 96.4 LAS &#21644; 97.4 UAS &#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#22120;&#30340;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#24182;&#34892;&#24615;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;&#22823;&#32422;&#21313;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel dependency parser, the hexatagger, that constructs dependency trees by tagging the words in a sentence with elements from a finite set of possible tags. In contrast to many approaches to dependency parsing, our approach is fully parallelizable at training time, i.e., the structure-building actions needed to build a dependency parse can be predicted in parallel to each other. Additionally, exact decoding is linear in time and space complexity. Furthermore, we derive a probabilistic dependency parser that predicts hexatags using no more than a linear model with features from a pretrained language model, i.e., we forsake a bespoke architecture explicitly designed for the task. Despite the generality and simplicity of our approach, we achieve state-of-the-art performance of 96.4 LAS and 97.4 UAS on the Penn Treebank test set. Additionally, our parser's linear time complexity and parallelism significantly improve computational efficiency, with a roughly 10-times speed-u
&lt;/p&gt;</description></item><item><title>&#24739;&#26377;&#35821;&#35328;&#38556;&#30861;&#30340;&#20154;&#22312;&#29616;&#26377;&#28040;&#36153;&#31867;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#30701;&#35821;&#35782;&#21035;&#31995;&#32479;&#29992;&#20110;&#36866;&#24212;&#20854;&#38750;&#20856;&#22411;&#30340;&#35821;&#38899;&#27169;&#24335;&#65292;&#20854;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.05446</link><description>&lt;p&gt;
&#24739;&#35821;&#35328;&#38556;&#30861;&#32773;&#30340;&#28508;&#22312;&#30701;&#35821;&#21305;&#37197;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Latent Phrase Matching for Dysarthric Speech. (arXiv:2306.05446v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05446
&lt;/p&gt;
&lt;p&gt;
&#24739;&#26377;&#35821;&#35328;&#38556;&#30861;&#30340;&#20154;&#22312;&#29616;&#26377;&#28040;&#36153;&#31867;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#30701;&#35821;&#35782;&#21035;&#31995;&#32479;&#29992;&#20110;&#36866;&#24212;&#20854;&#38750;&#20856;&#22411;&#30340;&#35821;&#38899;&#27169;&#24335;&#65292;&#20854;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24456;&#22810;&#28040;&#36153;&#31867;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#19981;&#33021;&#20026;&#35821;&#35328;&#38556;&#30861;&#24739;&#32773;&#25552;&#20379;&#33391;&#22909;&#30340;&#20307;&#39564;&#21644;&#35782;&#21035;&#25928;&#26524;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35821;&#35328;&#38556;&#30861;&#24773;&#20917;&#26356;&#20026;&#20005;&#37325;&#30340;&#20154;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#20010;&#24615;&#21270;&#35821;&#38899;&#27169;&#22411;&#30340;&#25506;&#32034;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#36866;&#24212;&#38750;&#20856;&#22411;&#30340;&#35821;&#38899;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31034;&#20363;&#30340;&#20010;&#24615;&#21270;&#30701;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#23569;&#37327;&#30340;&#35821;&#38899;&#36827;&#34892;&#35757;&#32451;&#65292;&#19981;&#20381;&#36182;&#20110;&#20256;&#32479;&#21457;&#38899;&#35789;&#20856;&#65292;&#19981;&#21463;&#19981;&#21516;&#35821;&#35328;&#24433;&#21709;&#65292;&#19988;&#22312;&#21508;&#31181;&#35821;&#38899;&#38556;&#30861;&#24773;&#20917;&#19979;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;32&#21517;&#24739;&#26377;&#35328;&#35821;&#22256;&#38590;&#30340;&#20154;&#21592;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#19981;&#21463;&#20005;&#37325;&#31243;&#24230;&#24433;&#21709;&#65292;&#19982;&#21830;&#19994;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30456;&#27604;&#65292;&#21484;&#22238;&#29575;&#25552;&#39640;&#20102;60%&#12290;&#22312;&#20844;&#20849;EasyCall&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#31934;&#24230;&#25552;&#39640;&#20102;30.5%&#12290;&#24403;&#35757;&#32451;50&#20010;&#29420;&#29305;&#30701;&#35821;&#26102;&#65292;&#24615;&#33021;&#38543;&#30701;&#35821;&#25968;&#37327;&#22686;&#21152;&#32780;&#19979;&#38477;&#65292;&#20294;&#22987;&#32456;&#20248;&#20110;ASR&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many consumer speech recognition systems are not tuned for people with speech disabilities, resulting in poor recognition and user experience, especially for severe speech differences. Recent studies have emphasized interest in personalized speech models from people with atypical speech patterns. We propose a query-by-example-based personalized phrase recognition system that is trained using small amounts of speech, is language agnostic, does not assume a traditional pronunciation lexicon, and generalizes well across speech difference severities. On an internal dataset collected from 32 people with dysarthria, this approach works regardless of severity and shows a 60% improvement in recall relative to a commercial speech recognition system. On the public EasyCall dataset of dysarthric speech, our approach improves accuracy by 30.5%. Performance degrades as the number of phrases increases, but consistently outperforms ASR systems when trained with 50 unique phrases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;PIXIU&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;Fine-tuning LLaMA&#30340;&#31532;&#19968;&#20010;&#37329;&#34701;&#35821;&#35328;&#27169;&#22411;&#12289;&#21253;&#21547;136K&#25968;&#25454;&#26679;&#26412;&#29992;&#20110;Fine-tuning&#30340;&#31532;&#19968;&#20010;&#25351;&#20196;&#25968;&#25454;&#65292;&#20197;&#21450;&#20855;&#26377;5&#20010;&#20219;&#21153;&#21644;9&#20010;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#37329;&#34701;&#20219;&#21153;&#12289;&#37329;&#34701;&#25991;&#26723;&#31867;&#22411;&#21644;&#37329;&#34701;&#25968;&#25454;&#27169;&#24577;&#32771;&#34385;&#65292;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#22810;&#20219;&#21153;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#21517;&#20026;FinMA&#30340;&#37329;&#34701;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20415;&#23545;&#21508;&#31181;&#37329;&#34701;&#20219;&#21153;&#36827;&#34892;&#25351;&#20196;&#36319;&#38543;&#12290;</title><link>http://arxiv.org/abs/2306.05443</link><description>&lt;p&gt;
PIXIU&#65306;&#38754;&#21521;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#25351;&#20196;&#25968;&#25454;&#21644;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance. (arXiv:2306.05443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PIXIU&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;Fine-tuning LLaMA&#30340;&#31532;&#19968;&#20010;&#37329;&#34701;&#35821;&#35328;&#27169;&#22411;&#12289;&#21253;&#21547;136K&#25968;&#25454;&#26679;&#26412;&#29992;&#20110;Fine-tuning&#30340;&#31532;&#19968;&#20010;&#25351;&#20196;&#25968;&#25454;&#65292;&#20197;&#21450;&#20855;&#26377;5&#20010;&#20219;&#21153;&#21644;9&#20010;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#37329;&#34701;&#20219;&#21153;&#12289;&#37329;&#34701;&#25991;&#26723;&#31867;&#22411;&#21644;&#37329;&#34701;&#25968;&#25454;&#27169;&#24577;&#32771;&#34385;&#65292;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#22810;&#20219;&#21153;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#21517;&#20026;FinMA&#30340;&#37329;&#34701;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20415;&#23545;&#21508;&#31181;&#37329;&#34701;&#20219;&#21153;&#36827;&#34892;&#25351;&#20196;&#36319;&#38543;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#32570;&#20047;&#20844;&#24320;&#30340;&#38754;&#21521;&#37329;&#34701;&#30340;&#35821;&#35328;&#27169;&#22411;&#12289;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#22522;&#20934;&#65292;&#36825;&#23545;&#20110;&#19981;&#26029;&#25512;&#36827;&#37329;&#34701;&#20154;&#24037;&#26234;&#33021;&#24320;&#28304;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;PIXIU&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;Fine-tuning LLaMA&#30340;&#31532;&#19968;&#20010;&#37329;&#34701;&#35821;&#35328;&#27169;&#22411;&#12289;&#21253;&#21547;136K&#25968;&#25454;&#26679;&#26412;&#29992;&#20110;Fine-tuning&#30340;&#31532;&#19968;&#20010;&#25351;&#20196;&#25968;&#25454;&#65292;&#20197;&#21450;&#20855;&#26377;5&#20010;&#20219;&#21153;&#21644;9&#20010;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#21508;&#31181;&#37329;&#34701;&#20219;&#21153;&#12289;&#37329;&#34701;&#25991;&#26723;&#31867;&#22411;&#21644;&#37329;&#34701;&#25968;&#25454;&#27169;&#24577;&#65292;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#22810;&#20219;&#21153;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#36890;&#36807;Fine-tuning LLaMA&#25552;&#20986;&#20102;&#21517;&#20026;FinMA&#30340;&#37329;&#34701;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20415;&#23545;&#21508;&#31181;&#37329;&#34701;&#20219;&#21153;&#36827;&#34892;&#25351;&#20196;&#36319;&#38543;&#12290;&#20026;&#20102;&#25903;&#25345;&#37329;&#34701;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Although large language models (LLMs) has shown great performance on natural language processing (NLP) in the financial domain, there are no publicly available financial tailtored LLMs, instruction tuning datasets, and evaluation benchmarks, which is critical for continually pushing forward the open-source development of financial artificial intelligence (AI). This paper introduces PIXIU, a comprehensive framework including the first financial LLM based on fine-tuning LLaMA with instruction data, the first instruction data with 136K data samples to support the fine-tuning, and an evaluation benchmark with 5 tasks and 9 datasets. We first construct the large-scale multi-task instruction data considering a variety of financial tasks, financial document types, and financial data modalities. We then propose a financial LLM called FinMA by fine-tuning LLaMA with the constructed dataset to be able to follow instructions for various financial tasks. To support the evaluation of financial LLMs
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#22312;&#29615;&#33410;&#20013;&#20107;&#20214;&#20849;&#25351;&#20851;&#31995;&#27880;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21482;&#24314;&#35758;&#21487;&#33021;&#30340;&#20849;&#25351;&#20107;&#20214;&#23545;&#65292;&#23558;&#23436;&#20840;&#25163;&#21160;&#27880;&#37322;&#36807;&#31243;&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;&#22823;&#24133;&#24230;&#20943;&#23569;&#65292;&#21516;&#26102;&#23454;&#29616;97&#65285;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.05434</link><description>&lt;p&gt;
&#27169;&#22411;&#22312;&#29615;&#33410;&#20869;&#20107;&#20214;&#20849;&#25351;&#20851;&#31995;&#27880;&#37322;&#20013;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Good is the Model in Model-in-the-loop Event Coreference Resolution Annotation?. (arXiv:2306.05434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#22312;&#29615;&#33410;&#20013;&#20107;&#20214;&#20849;&#25351;&#20851;&#31995;&#27880;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21482;&#24314;&#35758;&#21487;&#33021;&#30340;&#20849;&#25351;&#20107;&#20214;&#23545;&#65292;&#23558;&#23436;&#20840;&#25163;&#21160;&#27880;&#37322;&#36807;&#31243;&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;&#22823;&#24133;&#24230;&#20943;&#23569;&#65292;&#21516;&#26102;&#23454;&#29616;97&#65285;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#37322;&#36328;&#25991;&#26723;&#20107;&#20214;&#20849;&#25351;&#38142;&#25509;&#26159;&#19968;&#39033;&#36153;&#26102;&#19988;&#35748;&#30693;&#35201;&#27714;&#39640;&#30340;&#20219;&#21153;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#27880;&#37322;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#22312;&#29615;&#33410;&#20013;&#20107;&#20214;&#20849;&#25351;&#20851;&#31995;&#27880;&#37322;&#26041;&#27861;&#65292;&#20854;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21482;&#24314;&#35758;&#21487;&#33021;&#30340;&#20849;&#25351;&#20107;&#20214;&#23545;&#12290;&#25105;&#20204;&#36890;&#36807;&#39318;&#20808;&#27169;&#25311;&#27880;&#37322;&#36807;&#31243;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#37322;&#32773;&#20026;&#20013;&#24515;&#30340;&#21484;&#22238;-&#27880;&#37322;&#24037;&#20316;&#37327;&#24179;&#34913;&#24230;&#37327;&#26469;&#27604;&#36739;&#19981;&#21516;&#22522;&#30784;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#65292;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#24133;&#20943;&#23569;&#23436;&#20840;&#25163;&#21160;&#27880;&#37322;&#36807;&#31243;&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;97&#65285;&#30340;&#21484;&#22238;&#29575;&#12290; &#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312; https://github.com/ahmeshaf/model_in_coref &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotating cross-document event coreference links is a time-consuming and cognitively demanding task that can compromise annotation quality and efficiency. To address this, we propose a model-in-the-loop annotation approach for event coreference resolution, where a machine learning model suggests likely corefering event pairs only. We evaluate the effectiveness of this approach by first simulating the annotation process and then, using a novel annotator-centric Recall-Annotation effort trade-off metric, we compare the results of various underlying models and datasets. We finally present a method for obtaining 97\% recall while substantially reducing the workload required by a fully manual annotation process. Code and data can be found at https://github.com/ahmeshaf/model_in_coref
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36808;&#21521;&#31471;&#21040;&#31471;&#35821;&#38899;&#21040;&#25991;&#26412;&#25688;&#35201;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#36807;&#28388;&#21644;&#36319;&#36394;&#27599;&#22825;&#19978;&#20256;&#22312;&#32447;&#30340;&#24191;&#25773;&#26032;&#38395;&#65292;&#24182;&#33021;&#22815;&#20135;&#29983;&#20016;&#23500;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26469;&#25552;&#39640;&#20854;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05432</link><description>&lt;p&gt;
&#36808;&#21521;&#31471;&#21040;&#31471;&#35821;&#38899;&#21040;&#25991;&#26412;&#25688;&#35201;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Towards End-to-end Speech-to-text Summarization. (arXiv:2306.05432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36808;&#21521;&#31471;&#21040;&#31471;&#35821;&#38899;&#21040;&#25991;&#26412;&#25688;&#35201;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#36807;&#28388;&#21644;&#36319;&#36394;&#27599;&#22825;&#19978;&#20256;&#22312;&#32447;&#30340;&#24191;&#25773;&#26032;&#38395;&#65292;&#24182;&#33021;&#22815;&#20135;&#29983;&#20016;&#23500;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26469;&#25552;&#39640;&#20854;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21040;&#25991;&#26412;&#65288;S2T&#65289;&#25688;&#35201;&#26159;&#19968;&#31181;&#33410;&#30465;&#26102;&#38388;&#30340;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#36807;&#28388;&#21644;&#36319;&#36394;&#27599;&#22825;&#19978;&#20256;&#22312;&#32447;&#30340;&#24191;&#25773;&#26032;&#38395;&#12290;&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#20013;&#20986;&#29616;&#20102;&#20855;&#26377;&#20986;&#33394;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#36215;&#20102;&#23545;&#20135;&#29983;&#31616;&#27905;&#25991;&#26723;&#29256;&#26412;&#30340;&#25688;&#35201;&#31995;&#32479;&#30340;&#30740;&#31350;&#37325;&#28857;&#65292;&#20063;&#31216;&#20026;&#25277;&#35937;&#25688;&#35201;&#12290;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#24314;&#27169;&#30340;S2T&#25277;&#35937;&#25688;&#35201;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#23427;&#25552;&#20379;&#20102;&#20135;&#29983;&#20016;&#23500;&#28508;&#22312;&#34920;&#31034;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#20123;&#34920;&#31034;&#21033;&#29992;&#20102;&#38750;&#35821;&#35328;&#21644;&#22768;&#23398;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#20165;&#20351;&#29992;&#32423;&#32852;&#31995;&#32479;&#20013;&#33258;&#21160;&#29983;&#25104;&#30340;&#36716;&#24405;&#25991;&#26412;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#20851;&#20110;&#36825;&#39033;&#20219;&#21153;&#30340;E2E&#24314;&#27169;&#24456;&#23569;&#25506;&#32034;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#24191;&#25773;&#26032;&#38395;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#65292;&#27599;&#22825;&#21521;&#29992;&#25143;&#21576;&#29616;&#22823;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#37319;&#29992;&#32423;&#32852;&#32763;&#35793;&#26041;&#27861;&#21644;&#31471;&#21040;&#31471;&#26041;&#27861;&#24314;&#27169;S2T&#25688;&#35201;&#65292;&#24182;&#22312;&#24191;&#25773;&#26032;&#38395;&#30340;&#26032;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;E2E&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#28508;&#21147;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26469;&#25552;&#39640;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech-to-text (S2T) summarization is a time-saving technique for filtering and keeping up with the broadcast news uploaded online on a daily basis. The rise of large language models from deep learning with impressive text generation capabilities has placed the research focus on summarization systems that produce paraphrased compact versions of the document content, also known as abstractive summaries. End-to-end (E2E) modelling of S2T abstractive summarization is a promising approach that offers the possibility of generating rich latent representations that leverage non-verbal and acoustic information, as opposed to the use of only linguistic information from automatically generated transcripts in cascade systems. However, the few literature on E2E modelling of this task fails on exploring different domains, namely broadcast news, which is challenging domain where large and diversified volumes of data are presented to the user every day. We model S2T summarization both with a cascade 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#26500;&#24314;&#19987;&#38376;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#21457;&#23637;&#22522;&#21069;&#25552;&#27169;&#22411;&#26159;&#20026;&#20102;&#22312;&#27861;&#24459;&#39046;&#22495;&#24320;&#21457;&#26410;&#26469;&#30340;&#24212;&#29992;&#65292;&#22914;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#36827;&#19968;&#27493;&#22521;&#35757;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#8220;&#26080;&#20195;&#30721;&#8221;&#26041;&#27861;&#65292;&#27861;&#24459;&#19987;&#19994;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#22320;&#20026;&#19979;&#28216;&#20219;&#21153;&#21019;&#24314;&#23450;&#21046;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#34429;&#28982;&#24615;&#33021;&#20302;&#20110;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#19981;&#20462;&#25913;&#27169;&#22411;&#25110;&#20854;&#28304;&#20195;&#30721;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;LexGPT&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05431</link><description>&lt;p&gt;
LexGPT 0.1&#65306;&#22522;&#20110;Pile of Law&#30340;&#39044;&#35757;&#32451;GPT-J&#27169;&#22411;&#65288;arXiv:2306.05431v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
LexGPT 0.1: pre-trained GPT-J models with Pile of Law. (arXiv:2306.05431v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#26500;&#24314;&#19987;&#38376;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#21457;&#23637;&#22522;&#21069;&#25552;&#27169;&#22411;&#26159;&#20026;&#20102;&#22312;&#27861;&#24459;&#39046;&#22495;&#24320;&#21457;&#26410;&#26469;&#30340;&#24212;&#29992;&#65292;&#22914;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#36827;&#19968;&#27493;&#22521;&#35757;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#8220;&#26080;&#20195;&#30721;&#8221;&#26041;&#27861;&#65292;&#27861;&#24459;&#19987;&#19994;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#22320;&#20026;&#19979;&#28216;&#20219;&#21153;&#21019;&#24314;&#23450;&#21046;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#34429;&#28982;&#24615;&#33021;&#20302;&#20110;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#19981;&#20462;&#25913;&#27169;&#22411;&#25110;&#20854;&#28304;&#20195;&#30721;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;LexGPT&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#26500;&#24314;&#19987;&#38376;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;GPT-J&#27169;&#22411;&#24182;&#20351;&#29992;Pile of Law&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;LexGPT&#27169;&#22411;&#30340;&#24320;&#21457;&#36807;&#31243;&#12290;&#26412;&#25991;&#25152;&#26500;&#24314;&#30340;&#22522;&#30784;&#27169;&#22411;&#26159;&#26410;&#26469;&#22312;&#27861;&#24459;&#39046;&#22495;&#24320;&#21457;&#24212;&#29992;&#30340;&#21021;&#22987;&#27493;&#39588;&#65292;&#20363;&#22914;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#19968;&#27493;&#22521;&#35757;&#12290;&#26412;&#25991;&#30340;&#21478;&#19968;&#20010;&#30446;&#26631;&#26159;&#36890;&#36807;&#8220;&#26080;&#20195;&#30721;&#8221;&#26041;&#27861;&#24110;&#21161;&#27861;&#24459;&#19987;&#19994;&#20154;&#21592;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#25968;&#25454;&#36827;&#34892;&#31934;&#35843;&#24182;&#19988;&#19981;&#20462;&#25913;&#20219;&#20309;&#28304;&#20195;&#30721;&#65292;&#27861;&#24459;&#19987;&#19994;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#22320;&#20026;&#19979;&#28216;&#20219;&#21153;&#21019;&#24314;&#23450;&#21046;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20943;&#23569;&#25216;&#26415;&#30693;&#35782;&#21644;&#25237;&#20837;&#30340;&#26368;&#23567;&#21270;&#12290;&#26412;&#25991;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#26159;&#23558;LexGPT&#27169;&#22411;&#36716;&#25442;&#20026;&#20998;&#31867;&#22120;&#65292;&#23613;&#31649;&#24615;&#33021;&#26126;&#26174;&#20302;&#20110;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#22914;&#20309;&#22312;&#19981;&#20462;&#25913;&#27169;&#22411;&#25110;&#20854;&#28304;&#20195;&#30721;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#26159;&#19968;&#20010;&#30740;&#31350;&#37325;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research aims to build generative language models specialized for the legal domain. The manuscript presents the development of LexGPT models based on GPT-J models and pre-trained with Pile of Law. The foundation model built in this manuscript is the initial step for the development of future applications in the legal domain, such as further training with reinforcement learning from human feedback. Another objective of this manuscript is to assist legal professionals in utilizing language models through the ``No Code'' approach. By fine-tuning models with specialized data and without modifying any source code, legal professionals can create custom language models for downstream tasks with minimum effort and technical knowledge. The downstream task in this manuscript is to turn a LexGPT model into a classifier, although the performance is notably lower than the state-of-the-art result. How to enhance downstream task performance without modifying the model or its source code is a res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;RRWKV&#26550;&#26500;&#65292;&#23427;&#22312;&#20445;&#25345;&#35760;&#24518;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#21152;&#20837;&#22238;&#39038;&#33021;&#21147;&#26377;&#25928;&#22320;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.05176</link><description>&lt;p&gt;
RRWKV&#65306;&#22312;RWKV&#20013;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
RRWKV: Capturing Long-range Dependencies in RWKV. (arXiv:2306.05176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;RRWKV&#26550;&#26500;&#65292;&#23427;&#22312;&#20445;&#25345;&#35760;&#24518;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#21152;&#20837;&#22238;&#39038;&#33021;&#21147;&#26377;&#25928;&#22320;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;Transformer&#24778;&#20154;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#65292;&#23427;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#20027;&#35201;&#26550;&#26500;&#12290;&#26368;&#36817;&#65292;Receptance Weighted Key Value&#65288;RWKV&#65289;&#26550;&#26500;&#36981;&#24490;&#38750;Transformer&#26550;&#26500;&#65292;&#28040;&#38500;&#20102;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#32570;&#28857;&#65292;&#20854;&#20013;&#23384;&#20648;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#21576;&#20108;&#27425;&#25193;&#23637;&#12290;&#23613;&#31649;RWKV&#21033;&#29992;&#20102;&#32447;&#24615;&#24352;&#37327;&#31215;&#27880;&#24847;&#26426;&#21046;&#24182;&#36890;&#36807;&#37096;&#32626;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#23454;&#29616;&#20102;&#24182;&#34892;&#35745;&#31639;&#65292;&#20294;&#19982;&#26631;&#20934;Transformer&#20013;&#30452;&#25509;&#20132;&#20114;&#33719;&#24471;&#30340;&#23436;&#25972;&#20449;&#24687;&#30456;&#27604;&#65292;&#23427;&#26080;&#27861;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#22240;&#20026;&#20854;&#21463;&#38480;&#20110;&#21521;&#21518;&#26597;&#30475;&#20808;&#21069;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#23558;&#22238;&#39038;&#33021;&#21147;&#32435;&#20837;RWKV&#20013;&#26469;&#35774;&#35745;Retrospected Receptance Weighted Key Value&#65288;RRWKV&#65289;&#26550;&#26500;&#65292;&#20197;&#26377;&#25928;&#22320;&#21560;&#25910;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#35760;&#24518;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to the impressive dot-product attention, the Transformers have been the dominant architectures in various natural language processing (NLP) tasks. Recently, the Receptance Weighted Key Value (RWKV) architecture follows a non-transformer architecture to eliminate the drawbacks of dot-product attention, where memory and computational complexity exhibits quadratic scaling with sequence length. Although RWKV has exploited a linearly tensor-product attention mechanism and achieved parallelized computations by deploying the time-sequential mode, it fails to capture long-range dependencies because of its limitation on looking back at previous information, compared with full information obtained by direct interactions in the standard transformer. Therefore, the paper devises the Retrospected Receptance Weighted Key Value (RRWKV) architecture via incorporating the retrospecting ability into the RWKV to effectively absorb information, which maintains memory and computational efficiency as 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#22312;&#28151;&#21512;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.04634</link><description>&lt;p&gt;
&#35770;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Reliability of Watermarks for Large Language Models. (arXiv:2306.04634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#22312;&#28151;&#21512;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24320;&#22987;&#24212;&#29992;&#20110;&#26085;&#24120;&#20351;&#29992;&#65292;&#24182;&#26377;&#33021;&#21147;&#22312;&#26410;&#26469;&#30340;&#21313;&#24180;&#20869;&#20135;&#29983;&#22823;&#37327;&#30340;&#25991;&#26412;&#12290;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#20250;&#21462;&#20195;&#20114;&#32852;&#32593;&#19978;&#30340;&#20154;&#31867;&#20889;&#20316;&#25991;&#26412;&#65292;&#24182;&#26377;&#21487;&#33021;&#34987;&#29992;&#20110;&#24694;&#24847;&#30446;&#30340;&#65292;&#22914;&#38035;&#40060;&#25915;&#20987;&#21644;&#31038;&#20132;&#23186;&#20307;&#26426;&#22120;&#20154;&#12290;&#27700;&#21360;&#26159;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#26816;&#27979;&#21644;&#21487;&#35760;&#24405;&#65292;&#26469;&#38477;&#20302;&#36825;&#20123;&#20260;&#23475;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;&#22312;&#29616;&#23454;&#20013;&#28151;&#21512;&#20102;&#20854;&#20182;&#30340;&#25991;&#26412;&#26469;&#28304;&#65292;&#34987;&#20154;&#31867;&#20889;&#20316;&#32773;&#25110;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#25913;&#20889;&#65292;&#34987;&#29992;&#20110;&#31038;&#20132;&#21644;&#25216;&#26415;&#39046;&#22495;&#30340;&#21508;&#31181;&#24212;&#29992;&#26102;&#65292;&#27700;&#21360;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#30340;&#21487;&#38752;&#24615;&#22914;&#20309;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#26816;&#27979;&#26041;&#26696;&#65292;&#37327;&#21270;&#20102;&#23427;&#20204;&#26816;&#27979;&#27700;&#21360;&#30340;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#22312;&#27599;&#20010;&#24773;&#20917;&#19979;&#38656;&#35201;&#35266;&#23519;&#22810;&#23569;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#25165;&#33021;&#21487;&#38752;&#22320;&#26816;&#27979;&#27700;&#21360;&#12290;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#20102;&#24403;&#27700;&#21360;&#19982;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#28151;&#21512;&#26102;&#27700;&#21360;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#27700;&#21360;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are now deployed to everyday use and positioned to produce large quantities of text in the coming decade. Machine-generated text may displace human-written text on the internet and has the potential to be used for malicious purposes, such as spearphishing attacks and social media bots. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet, a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text might be mixed with other text sources, paraphrased by human writers or other language models, and used for applications in a broad number of domains, both social and technical. In this paper, we explore different detection schemes, quantify their power at detecting watermarks, and determine how much machine-generated text needs to be observed in each scenario to reliably detect the watermark. We especially highlight o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#36136;&#37327;&#27880;&#37322;&#24037;&#20316;&#27969;&#31243;&#31216;&#20026;DQA&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#22320;&#35780;&#20272;&#23545;&#35805;&#36136;&#37327;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#19968;&#20123;&#23458;&#35266;&#23545;&#35805;&#23646;&#24615;&#30340;&#21028;&#26029;&#12290;</title><link>http://arxiv.org/abs/2306.03984</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#30340;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#30340;&#35780;&#20272;&#25351;&#26631;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs. (arXiv:2306.03984v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#36136;&#37327;&#27880;&#37322;&#24037;&#20316;&#27969;&#31243;&#31216;&#20026;DQA&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#22320;&#35780;&#20272;&#23545;&#35805;&#36136;&#37327;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#19968;&#20123;&#23458;&#35266;&#23545;&#35805;&#23646;&#24615;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20132;&#20114;&#36136;&#37327;&#23545;&#20110;&#25913;&#36827;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#23545;&#35805;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#35201;&#20040;&#20391;&#37325;&#20110;&#35780;&#20272;&#21333;&#20010;&#23545;&#35805;&#36718;&#27425;&#30340;&#36136;&#37327;&#65292;&#35201;&#20040;&#20174;&#32456;&#31471;&#29992;&#25143;&#31435;&#21363;&#22312;&#20132;&#20114;&#20043;&#21518;&#25910;&#38598;&#23545;&#35805;&#32423;&#21035;&#30340;&#36136;&#37327;&#27979;&#37327;&#25968;&#25454;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#32423;&#21035;&#27880;&#37322;&#24037;&#20316;&#27969;&#31243;&#31216;&#20026;&#23545;&#35805;&#36136;&#37327;&#27880;&#37322;&#65288;DQA&#65289;&#12290;DQA&#19987;&#23478;&#27880;&#37322;&#21592;&#35780;&#20272;&#25972;&#20010;&#23545;&#35805;&#30340;&#36136;&#37327;&#65292;&#24182;&#26631;&#35760;&#23545;&#35805;&#30340;&#30446;&#26631;&#23436;&#25104;&#21644;&#29992;&#25143;&#24773;&#24863;&#31561;&#23646;&#24615;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;&#65288;i&#65289;&#23613;&#31649;&#23545;&#35805;&#36136;&#37327;&#19981;&#33021;&#23436;&#20840;&#20998;&#35299;&#25104;&#23545;&#35805;&#32423;&#21035;&#23646;&#24615;&#65292;&#20294;&#26576;&#20123;&#23458;&#35266;&#23545;&#35805;&#23646;&#24615;&#19982;&#23545;&#35805;&#36136;&#37327;&#30340;&#21028;&#26029;&#20043;&#38388;&#23384;&#22312;&#30528;&#24378;&#20851;&#31995;&#65307;&#65288;ii&#65289;&#23545;&#20110;&#23545;&#35805;&#32423;&#21035;&#36136;&#37327;&#20272;&#35745;&#20219;&#21153;&#65292;&#19968;&#20010;&#22312;&#23545;&#35805;&#32423;&#21035;&#27880;&#37322;&#19978;&#35757;&#32451;&#30340;&#30417;&#30563;&#27169;&#22411;&#20248;&#20110;&#20165;&#22522;&#20110;&#32858;&#21512;&#36718;&#27425;&#32423;&#21035;&#29305;&#24449;&#30340;&#26041;&#27861;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#20351;&#29992;DQA&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#30340;&#23545;&#35805;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measurement of interaction quality is a critical task for the improvement of spoken dialog systems. Existing approaches to dialog quality estimation either focus on evaluating the quality of individual turns, or collect dialog-level quality measurements from end users immediately following an interaction. In contrast to these approaches, we introduce a new dialog-level annotation workflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate the quality of dialogs as a whole, and also label dialogs for attributes such as goal completion and user sentiment. In this contribution, we show that: (i) while dialog quality cannot be completely decomposed into dialog-level attributes, there is a strong relationship between some objective dialog attributes and judgments of dialog quality; (ii) for the task of dialog-level quality estimation, a supervised model trained on dialog-level annotations outperforms methods based purely on aggregating turn-level features; and (iii) the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; CELDA &#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#31934;&#32454;&#30340;&#20266;&#26631;&#31614;&#25968;&#25454;&#38598;&#24182;&#21033;&#29992;&#32858;&#31867;&#22686;&#24378;&#30340;&#21028;&#21035;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.02693</link><description>&lt;p&gt;
CELDA: &#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#40657;&#21283;&#23376;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22686;&#24378;&#20998;&#31867;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CELDA: Leveraging Black-box Language Model as Enhanced Classifier without Labels. (arXiv:2306.02693v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; CELDA &#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#31934;&#32454;&#30340;&#20266;&#26631;&#31614;&#25968;&#25454;&#38598;&#24182;&#21033;&#29992;&#32858;&#31867;&#22686;&#24378;&#30340;&#21028;&#21035;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;API&#20844;&#24320;&#30340;&#29616;&#20195;&#21270;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#20219;&#21153;&#27491;&#22312;&#21463;&#21040;&#30740;&#31350;&#20154;&#21592;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#26631;&#31614;&#25968;&#25454;&#31232;&#32570;&#25110;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#24050;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#20294;&#20854;&#25928;&#26524;&#20173;&#19981;&#22914;&#23436;&#20840;&#30417;&#30563;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#24494;&#23567;&#21464;&#21270;&#19979;&#34920;&#29616;&#27424;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;Clustering-enhanced Linear Discriminative Analysis&#65292;&#36890;&#36807;&#25552;&#21462;&#31934;&#32454;&#30340;&#20266;&#26631;&#31614;&#25968;&#25454;&#38598;&#24182;&#21033;&#29992;&#32858;&#31867;&#22686;&#24378;&#30340;&#21028;&#21035;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#24120;&#24369;&#30340;&#30417;&#30563;&#20449;&#21495;&#26465;&#20214;&#19979;&#65288;&#21363;&#26631;&#31614;&#21517;&#65289;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utilizing language models (LMs) without internal access is becoming an attractive paradigm in the field of NLP as many cutting-edge LMs are released through APIs and boast a massive scale. The de-facto method in this type of black-box scenario is known as prompting, which has shown progressive performance enhancements in situations where data labels are scarce or unavailable. Despite their efficacy, they still fall short in comparison to fully supervised counterparts and are generally brittle to slight modifications. In this paper, we propose Clustering-enhanced Linear Discriminative Analysis, a novel approach that improves the text classification accuracy with a very weak-supervision signal (i.e., name of the labels). Our framework draws a precise decision boundary without accessing weights or gradients of the LM model or data labels. The core ideas of CELDA are twofold: (1) extracting a refined pseudo-labeled dataset from an unlabeled dataset, and (2) training a lightweight and robus
&lt;/p&gt;</description></item><item><title>COBRA&#26694;&#26550;&#26159;&#31532;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#24418;&#24335;&#20027;&#20041;&#65292;&#29992;&#20110;&#35299;&#37322;&#25915;&#20987;&#24615;&#25110;&#26377;&#20559;&#35265;&#38472;&#36848;&#30340;&#24847;&#22270;&#12289;&#21453;&#24212;&#21644;&#21361;&#23475;&#65292;COBRACORPUS&#25968;&#25454;&#38598;&#21253;&#21547;3.3&#19975;&#20010;&#28508;&#22312;&#26377;&#23475;&#38472;&#36848;&#21644;&#26426;&#22120;&#19978;&#19979;&#25991;&#21450;&#20854;&#26377;&#23475;&#24615;&#12289;&#38544;&#21547;&#20559;&#35265;&#12289;&#35828;&#35805;&#24847;&#22270;&#21644;&#21548;&#20247;&#21453;&#24212;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#25915;&#20987;&#24615;&#38472;&#36848;&#21487;&#20197;&#20256;&#36798;&#19981;&#21516;&#30340;&#20559;&#35265;&#12289;&#24847;&#22270;&#21644;&#21361;&#23475;&#65292;&#21462;&#20915;&#20110;&#31038;&#20132;&#21644;&#24773;&#22659;&#32972;&#26223;&#65292;&#20984;&#26174;&#20102;&#22312;&#26377;&#23475;&#35821;&#35328;&#20390;&#27979;&#21644;&#31649;&#29702;&#20013;&#38656;&#35201;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.01985</link><description>&lt;p&gt;
COBRA&#26694;&#26550;&#65306;&#26377;&#20851;&#25915;&#20987;&#24615;&#38472;&#36848;&#24433;&#21709;&#21644;&#21361;&#23475;&#30340;&#24773;&#22659;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements. (arXiv:2306.01985v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01985
&lt;/p&gt;
&lt;p&gt;
COBRA&#26694;&#26550;&#26159;&#31532;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#24418;&#24335;&#20027;&#20041;&#65292;&#29992;&#20110;&#35299;&#37322;&#25915;&#20987;&#24615;&#25110;&#26377;&#20559;&#35265;&#38472;&#36848;&#30340;&#24847;&#22270;&#12289;&#21453;&#24212;&#21644;&#21361;&#23475;&#65292;COBRACORPUS&#25968;&#25454;&#38598;&#21253;&#21547;3.3&#19975;&#20010;&#28508;&#22312;&#26377;&#23475;&#38472;&#36848;&#21644;&#26426;&#22120;&#19978;&#19979;&#25991;&#21450;&#20854;&#26377;&#23475;&#24615;&#12289;&#38544;&#21547;&#20559;&#35265;&#12289;&#35828;&#35805;&#24847;&#22270;&#21644;&#21548;&#20247;&#21453;&#24212;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#25915;&#20987;&#24615;&#38472;&#36848;&#21487;&#20197;&#20256;&#36798;&#19981;&#21516;&#30340;&#20559;&#35265;&#12289;&#24847;&#22270;&#21644;&#21361;&#23475;&#65292;&#21462;&#20915;&#20110;&#31038;&#20132;&#21644;&#24773;&#22659;&#32972;&#26223;&#65292;&#20984;&#26174;&#20102;&#22312;&#26377;&#23475;&#35821;&#35328;&#20390;&#27979;&#21644;&#31649;&#29702;&#20013;&#38656;&#35201;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35686;&#21578;&#65306;&#26412;&#25991;&#21253;&#21547;&#21487;&#33021;&#20196;&#20154;&#19981;&#36866;&#25110;&#20882;&#29359;&#30340;&#20869;&#23481;&#12290;&#29702;&#35299;&#38472;&#36848;&#30340;&#21361;&#23475;&#21644;&#20882;&#29359;&#24615;&#38656;&#35201;&#23545;&#38472;&#36848;&#30340;&#31038;&#20132;&#21644;&#24773;&#22659;&#32972;&#26223;&#36827;&#34892;&#25512;&#29702;&#12290;&#20363;&#22914;&#65292;"&#20320;&#30340;&#33521;&#35821;&#38750;&#24120;&#22909;"&#36825;&#21477;&#35805;&#22312;&#30333;&#20154;&#23545;&#38750;&#30333;&#33394;&#21516;&#20107;&#35828;&#20986;&#26102;&#21487;&#33021;&#26263;&#31034;&#30528;&#20398;&#36785;&#65292;&#20294;&#22312;ESL&#32769;&#24072;&#23545;&#23398;&#29983;&#35828;&#20986;&#26102;&#21017;&#34987;&#35299;&#37322;&#20026;&#30495;&#35802;&#30340;&#36190;&#32654;&#12290;&#20808;&#21069;&#30340;&#26377;&#20851;&#26377;&#23475;&#35821;&#35328;&#20390;&#27979;&#30340;&#26041;&#27861;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#36825;&#20123;&#24773;&#22659;&#22240;&#32032;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;COBRA&#26694;&#26550;&#65292;&#31532;&#19968;&#20010;&#22522;&#20110;&#31038;&#20132;&#21644;&#24773;&#22659;&#32972;&#26223;&#35299;&#37322;&#25915;&#20987;&#24615;&#25110;&#26377;&#20559;&#35265;&#38472;&#36848;&#24847;&#22270;&#65292;&#21453;&#24212;&#21644;&#21361;&#23475;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#24418;&#24335;&#20027;&#20041;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;COBRACORPUS&#65292;&#19968;&#20010;&#21253;&#21547;3.3&#19975;&#20010;&#28508;&#22312;&#26377;&#23475;&#38472;&#36848;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#21450;&#20854;&#26377;&#23475;&#24615;&#12289;&#38544;&#21547;&#20559;&#35265;&#12289;&#35828;&#35805;&#24847;&#22270;&#21644;&#21548;&#20247;&#21453;&#24212;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#30740;&#31350;&#26377;&#23475;&#35821;&#35328;&#30340;&#24773;&#22659;&#21160;&#24577;&#65292;&#25105;&#20204;&#20351;&#29992;COBRA&#26694;&#26550;&#20998;&#26512;&#24182;&#27604;&#36739;&#19981;&#21516;&#24773;&#22659;&#19979;&#25915;&#20987;&#24615;&#38472;&#36848;&#30340;&#21361;&#23475;&#21644;&#35299;&#37322;&#65292;&#21253;&#25324;&#20010;&#20154;&#20132;&#35848;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;&#26032;&#38395;&#25991;&#31456;&#31561;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25915;&#20987;&#24615;&#38472;&#36848;&#21487;&#20197;&#20256;&#36798;&#19981;&#21516;&#30340;&#20559;&#35265;&#12289;&#24847;&#22270;&#21644;&#21361;&#23475;&#65292;&#21462;&#20915;&#20110;&#31038;&#20132;&#21644;&#24773;&#22659;&#32972;&#26223;&#65292;&#20984;&#26174;&#20102;&#22312;&#26377;&#23475;&#35821;&#35328;&#20390;&#27979;&#21644;&#31649;&#29702;&#20013;&#38656;&#35201;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Warning: This paper contains content that may be offensive or upsetting. Understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance "your English is very good" may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. We introduce COBRA frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create COBRACORPUS, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. To study the contextual dynamics of o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#22330;&#26223;&#20013;&#21033;&#29992;&#26377;&#26356;&#22909;&#35206;&#30422;&#29575;&#30340;&#35757;&#32451;&#25968;&#25454;&#65306;&#21160;&#24577;&#35268;&#21010;&#25552;&#31034;&#21644;&#31243;&#24207;&#33976;&#39311;&#12290;&#36825;&#33021;&#22815;&#22312;&#25968;&#23398;&#35789;&#38382;&#39064; (MWP) &#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#36890;&#36807;&#31243;&#24207;&#25191;&#34892;&#26469;&#39564;&#35777;&#31572;&#26696;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18170</link><description>&lt;p&gt;
&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#23569;&#26679;&#26412;&#21551;&#21457;&#24335;&#25968;&#20540;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning. (arXiv:2305.18170v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#22330;&#26223;&#20013;&#21033;&#29992;&#26377;&#26356;&#22909;&#35206;&#30422;&#29575;&#30340;&#35757;&#32451;&#25968;&#25454;&#65306;&#21160;&#24577;&#35268;&#21010;&#25552;&#31034;&#21644;&#31243;&#24207;&#33976;&#39311;&#12290;&#36825;&#33021;&#22815;&#22312;&#25968;&#23398;&#35789;&#38382;&#39064; (MWP) &#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#36890;&#36807;&#31243;&#24207;&#25191;&#34892;&#26469;&#39564;&#35777;&#31572;&#26696;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;CoT prompting&#24050;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#65292;&#20294;&#22312;&#24191;&#27867;&#38382;&#39064;&#31867;&#22411;&#19978;&#35774;&#35745;&#33021;&#22815;&#24456;&#22909;&#27010;&#25324;&#30340;&#25552;&#31034;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#23398;&#35789;&#38382;&#39064; (MWP) &#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#12290;&#27492;&#22806;&#65292;&#36890;&#24120;&#26377;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#26356;&#22909;&#30340;&#22810;&#26679;&#24615;&#35206;&#30422;&#29575;&#65292;&#20294;&#32570;&#20047;CoT&#27880;&#37322;&#65292;&#36825;&#38480;&#21046;&#20102;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#20197;&#22312;&#23569;&#25968;&#26679;&#26412;&#25552;&#31034;&#22330;&#26223;&#20013;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#65306;&#21160;&#24577;&#35268;&#21010;&#25552;&#31034;&#21644;&#31243;&#24207;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought (CoT) prompting with large language models has proven effective in numerous natural language processing tasks, but designing prompts that generalize well to diverse problem types can be challenging, especially in the context of math word problem (MWP) solving. Additionally, it is common to have a large amount of training data that have a better diversity coverage but CoT annotations are not available, which limits the use of supervised learning techniques. To address these issues, we investigate two approaches to leverage the training data in a few-shot prompting scenario: dynamic program prompting and program distillation. Our approach is largely inspired by Gao et al., (2022), where they proposed to replace the CoT with the programs as the intermediate reasoning step. Such a prompting strategy allows us to accurately verify the answer correctness through program execution in MWP solving. Our dynamic program prompting involves annotating the training data by sampling 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#30340;&#31616;&#27905;&#30340;&#27169;&#22411; HiTIN&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#26080;&#26435;&#26641;&#32467;&#26500;&#30340;&#32534;&#30721;&#26641;&#26469;&#22686;&#24378;&#25991;&#26412;&#34920;&#31034;&#65292;&#19981;&#38656;&#35201;&#20808;&#21069;&#30340;&#32479;&#35745;&#25968;&#25454;&#25110;&#26631;&#31614;&#35821;&#20041;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2305.15182</link><description>&lt;p&gt;
HiTIN: &#38024;&#23545;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#30340;&#20998;&#23618;&#26641;&#21516;&#26500;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HiTIN: Hierarchy-aware Tree Isomorphism Network for Hierarchical Text Classification. (arXiv:2305.15182v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#30340;&#31616;&#27905;&#30340;&#27169;&#22411; HiTIN&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#26080;&#26435;&#26641;&#32467;&#26500;&#30340;&#32534;&#30721;&#26641;&#26469;&#22686;&#24378;&#25991;&#26412;&#34920;&#31034;&#65292;&#19981;&#38656;&#35201;&#20808;&#21069;&#30340;&#32479;&#35745;&#25968;&#25454;&#25110;&#26631;&#31614;&#35821;&#20041;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#26159;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#20219;&#21153;&#65292;&#22240;&#20026;&#26631;&#31614;&#24418;&#25104;&#20102;&#22797;&#26434;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#29616;&#26377;&#30340;&#21452;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;HTC&#20013;&#21462;&#24471;&#20102;&#24369;&#30340;&#24615;&#33021;&#22686;&#30410;&#65292;&#20294;&#20855;&#26377;&#24040;&#22823;&#30340;&#20869;&#23384;&#24320;&#38144;&#65292;&#20854;&#32467;&#26500;&#32534;&#30721;&#22120;&#20005;&#37325;&#20381;&#36182;&#39046;&#22495;&#30693;&#35782;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20542;&#21521;&#20110;&#30740;&#31350;&#19968;&#31181;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#20869;&#23384;&#21451;&#22909;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#20808;&#21069;&#30340;&#32479;&#35745;&#25968;&#25454;&#25110;&#26631;&#31614;&#35821;&#20041;&#21363;&#21487;&#25552;&#39640;HTC&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#21516;&#26500;&#32593;&#32476;&#65288;HiTIN&#65289;&#65292;&#20197;&#21482;&#20351;&#29992;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#30340;&#21477;&#27861;&#20449;&#24687;&#26469;&#22686;&#24378;&#25991;&#26412;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#32467;&#26500;&#29109;&#22312;&#25351;&#23548;&#19979;&#23558;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#36716;&#25442;&#20026;&#19968;&#20010;&#26080;&#26435;&#26641;&#32467;&#26500;&#65292;&#31216;&#20026;&#32534;&#30721;&#26641;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32467;&#26500;&#32534;&#30721;&#22120;&#65292;&#23558;&#32534;&#30721;&#26641;&#20013;&#30340;&#23618;&#27425;&#24863;&#30693;&#20449;&#24687;&#19982;&#25991;&#26412;&#34920;&#31034;&#30456;&#32467;&#21512;&#12290;&#38500;&#25991;&#26412;&#32534;&#30721;&#22120;&#22806;&#65292;HiTIN&#20165;&#21253;&#21547;&#19968;&#20010;&#29305;&#24449;&#32534;&#30721;&#22120;&#65292;&#26174;&#30528;&#38477;&#20302;&#20102;&#20869;&#23384;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical text classification (HTC) is a challenging subtask of multi-label classification as the labels form a complex hierarchical structure. Existing dual-encoder methods in HTC achieve weak performance gains with huge memory overheads and their structure encoders heavily rely on domain knowledge. Under such observation, we tend to investigate the feasibility of a memory-friendly model with strong generalization capability that could boost the performance of HTC without prior statistics or label semantics. In this paper, we propose Hierarchy-aware Tree Isomorphism Network (HiTIN) to enhance the text representations with only syntactic information of the label hierarchy. Specifically, we convert the label hierarchy into an unweighted tree structure, termed coding tree, with the guidance of structural entropy. Then we design a structure encoder to incorporate hierarchy-aware information in the coding tree into text representations. Besides the text encoder, HiTIN only contains a fe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SPECTRA&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24212;&#29992;&#20102;&#26032;&#30340;&#26102;&#38388;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#26469;&#25429;&#25417;&#35821;&#38899;&#25991;&#26412;&#23545;&#40784;&#65292;&#21516;&#26102;&#23558;&#22238;&#31572;&#36873;&#25321;&#20219;&#21153;&#25512;&#24191;&#21040;&#26381;&#21153;&#20110;&#21475;&#35821;&#23545;&#35805;&#29702;&#35299;&#65292;&#29992;&#20110;&#20016;&#23500;&#35805;&#35821;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.11579</link><description>&lt;p&gt;
&#24102;&#26377;&#26174;&#24335;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#29992;&#20110;&#21475;&#35821;&#23545;&#35805;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Speech-Text Dialog Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment. (arXiv:2305.11579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SPECTRA&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24212;&#29992;&#20102;&#26032;&#30340;&#26102;&#38388;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#26469;&#25429;&#25417;&#35821;&#38899;&#25991;&#26412;&#23545;&#40784;&#65292;&#21516;&#26102;&#23558;&#22238;&#31572;&#36873;&#25321;&#20219;&#21153;&#25512;&#24191;&#21040;&#26381;&#21153;&#20110;&#21475;&#35821;&#23545;&#35805;&#29702;&#35299;&#65292;&#29992;&#20110;&#20016;&#23500;&#35805;&#35821;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#35768;&#22810;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#24120;&#38024;&#23545;&#19968;&#20010;&#25110;&#20004;&#20010;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#20102;&#23450;&#21046;&#65292;&#20294;&#26410;&#33021;&#24449;&#26381;&#21508;&#31181;&#35821;&#38899;&#25991;&#26412;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#26410;&#33021;&#25506;&#32034;&#23545;&#35805;&#20013;&#30340;&#24773;&#22659;&#20449;&#24687;&#20197;&#20016;&#23500;&#35805;&#35821;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#26174;&#24335;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;SPECTRA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#32771;&#34385;&#35821;&#38899;&#27169;&#24577;&#30340;&#26102;&#38388;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#39033;&#26032;&#30340;&#26102;&#38388;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#26469;&#25429;&#25417;&#35821;&#38899;&#25991;&#26412;&#23545;&#40784;&#12290;&#36825;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#26088;&#22312;&#39044;&#27979;&#30456;&#24212;&#35821;&#38899;&#27874;&#24418;&#20013;&#27599;&#20010;&#25991;&#26412;&#21333;&#35789;&#30340;&#24320;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23398;&#20064;&#21475;&#35821;&#23545;&#35805;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#23558;&#22238;&#31572;&#36873;&#25321;&#20219;&#21153;&#25512;&#24191;&#21040;&#26381;&#21153;&#20110;&#21475;&#35821;&#23545;&#35805;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, speech-text pre-training methods have shown remarkable success in many speech and natural language processing tasks. However, most previous pre-trained models are usually tailored for one or two specific tasks, but fail to conquer a wide range of speech-text tasks. In addition, existing speech-text pre-training methods fail to explore the contextual information within a dialogue to enrich utterance representations. In this paper, we propose Speech-text dialog Pre-training for spoken dialog understanding with ExpliCiT cRoss-Modal Alignment (SPECTRA), which is the first-ever speech-text dialog pre-training model. Concretely, to consider the temporality of speech modality, we design a novel temporal position prediction task to capture the speech-text alignment. This pre-training task aims to predict the start and end time of each textual word in the corresponding speech waveform. In addition, to learn the characteristics of spoken dialogs, we generalize a response selection task
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#32034;&#24341;&#30340;&#38544;&#24335;&#24773;&#24863;&#25512;&#26029;&#26694;&#26550;&#65288;THOR&#65289;&#65292;&#36890;&#36807;&#19977;&#27425;&#36339;&#25512;&#29702;&#27169;&#20223;&#31867;&#20154;&#25512;&#29702;&#36807;&#31243;&#65292;&#25903;&#25345;&#24120;&#35782;&#21644;&#22810;&#36339;&#25512;&#29702;&#20197;&#25512;&#26029;&#24847;&#35265;&#30340;&#28508;&#22312;&#24847;&#22270;&#65292;&#24182;&#36880;&#27493;&#35825;&#23548;&#38544;&#24335;&#26041;&#38754;&#12289;&#24847;&#35265;&#21644;&#26368;&#32456;&#24773;&#24863;&#26497;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#19978;&#22823;&#24133;&#25552;&#39640;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.11255</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#38142;&#32034;&#24341;&#30340;&#38544;&#24335;&#24773;&#24863;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Reasoning Implicit Sentiment with Chain-of-Thought Prompting. (arXiv:2305.11255v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#32034;&#24341;&#30340;&#38544;&#24335;&#24773;&#24863;&#25512;&#26029;&#26694;&#26550;&#65288;THOR&#65289;&#65292;&#36890;&#36807;&#19977;&#27425;&#36339;&#25512;&#29702;&#27169;&#20223;&#31867;&#20154;&#25512;&#29702;&#36807;&#31243;&#65292;&#25903;&#25345;&#24120;&#35782;&#21644;&#22810;&#36339;&#25512;&#29702;&#20197;&#25512;&#26029;&#24847;&#35265;&#30340;&#28508;&#22312;&#24847;&#22270;&#65292;&#24182;&#36880;&#27493;&#35825;&#23548;&#38544;&#24335;&#26041;&#38754;&#12289;&#24847;&#35265;&#21644;&#26368;&#32456;&#24773;&#24863;&#26497;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#19978;&#22823;&#24133;&#25552;&#39640;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#31995;&#32479;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#20851;&#38190;&#35266;&#28857;&#34920;&#36798;&#26469;&#30830;&#23450;&#32473;&#23450;&#30446;&#26631;&#30340;&#24773;&#24863;&#26497;&#24615;&#65292;&#32780;&#22312;&#38544;&#24335;&#24773;&#24863;&#20998;&#26512;&#65288;ISA&#65289;&#20013;&#65292;&#35266;&#28857;&#25552;&#31034;&#20197;&#19968;&#31181;&#38544;&#21547;&#21644;&#27169;&#31946;&#30340;&#26041;&#24335;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#38544;&#24335;&#24773;&#24863;&#38656;&#35201;&#24120;&#35782;&#21644;&#22810;&#36339;&#25512;&#29702;&#33021;&#21147;&#26469;&#25512;&#26029;&#24847;&#35265;&#30340;&#28508;&#22312;&#24847;&#22270;&#12290;&#21463;&#26368;&#36817;&#24605;&#32500;&#38142;&#32034;&#24341;&#65288;CoT&#65289;&#24605;&#24819;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#19977;&#27425;&#36339;&#25512;&#29702;&#65288;THOR&#65289;CoT&#26694;&#26550;&#65292;&#27169;&#20223;ISA&#30340;&#31867;&#20154;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#20026;THOR&#35774;&#35745;&#20102;&#19968;&#20010;&#19977;&#27493;&#25552;&#31034;&#21407;&#21017;&#65292;&#20197;&#36880;&#27493;&#35825;&#23548;&#38544;&#24335;&#26041;&#38754;&#12289;&#24847;&#35265;&#21644;&#26368;&#32456;&#24773;&#24863;&#26497;&#24615;&#12290;&#25105;&#20204;&#30340;THOR+Flan-T5&#65288;11B&#65289;&#22312;&#30417;&#30563;&#35774;&#32622;&#19978;&#23558;&#25216;&#26415;&#27700;&#24179;&#25512;&#36827;&#20102;&#36229;&#36807;6&#65285;&#30340;F1&#20540;&#12290;&#26356;&#20026;&#26174;&#33879;&#30340;&#26159;&#65292;THOR+GPT3&#65288;175B&#65289;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19978;&#23558;&#25216;&#26415;&#27700;&#24179;&#25552;&#21319;&#20102;&#36229;&#36807;50&#65285;&#30340;F1&#20540;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20301;&#20110;https://github.com/scofield7419/THOR-ISA &#12290;
&lt;/p&gt;
&lt;p&gt;
While sentiment analysis systems try to determine the sentiment polarities of given targets based on the key opinion expressions in input texts, in implicit sentiment analysis (ISA) the opinion cues come in an implicit and obscure manner. Thus detecting implicit sentiment requires the common-sense and multi-hop reasoning ability to infer the latent intent of opinion. Inspired by the recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop Reasoning (THOR) CoT framework to mimic the human-like reasoning process for ISA. We design a three-step prompting principle for THOR to step-by-step induce the implicit aspect, opinion, and finally the sentiment polarity. Our THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on supervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50% F1 on zero-shot setting. Our code is at https://github.com/scofield7419/THOR-ISA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CoFe&#27979;&#35797;&#22871;&#20214;&#26469;&#35843;&#26597;&#19978;&#19979;&#25991;&#32452;&#21512;&#27867;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#24212;&#35813;&#22312;&#32467;&#26500;&#19978;&#19982;&#27979;&#35797;&#29992;&#20363;&#31867;&#20284;&#65292;&#30456;&#20114;&#20043;&#38388;&#24212;&#35813;&#19981;&#21516;&#65292;&#32780;&#19988;&#21333;&#29420;&#22320;&#31616;&#21333;&#12290;</title><link>http://arxiv.org/abs/2305.04835</link><description>&lt;p&gt;
&#22914;&#20309;&#24433;&#21709;&#19978;&#19979;&#25991;&#33539;&#20363;&#22312;&#32452;&#21512;&#36890;&#29992;&#24615;&#20013;&#30340;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Do In-Context Examples Affect Compositional Generalization?. (arXiv:2305.04835v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CoFe&#27979;&#35797;&#22871;&#20214;&#26469;&#35843;&#26597;&#19978;&#19979;&#25991;&#32452;&#21512;&#27867;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#24212;&#35813;&#22312;&#32467;&#26500;&#19978;&#19982;&#27979;&#35797;&#29992;&#20363;&#31867;&#20284;&#65292;&#30456;&#20114;&#20043;&#38388;&#24212;&#35813;&#19981;&#21516;&#65292;&#32780;&#19988;&#21333;&#29420;&#22320;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#27867;&#21270;&#8212;&#8212;&#29702;&#35299;&#30475;&#19981;&#35265;&#30340;&#24050;&#30693;&#21407;&#22987;&#32452;&#21512;&#8212;&#8212;&#26159;&#20154;&#31867;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25512;&#29702;&#33021;&#21147;&#12290;AI&#31038;&#21306;&#20027;&#35201;&#36890;&#36807;&#22312;&#35768;&#22810;&#35757;&#32451;&#26679;&#26412;&#19978;&#24494;&#35843;&#31070;&#32463;&#32593;&#32476;&#26469;&#30740;&#31350;&#36825;&#31181;&#33021;&#21147;&#65292;&#28982;&#32780;&#36824;&#19981;&#28165;&#26970;&#19978;&#19979;&#25991;&#23398;&#20064;&#8212;&#8212;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#35201;&#23569;&#26679;&#26412;&#33539;&#24335;&#8212;&#8212;&#26159;&#21542;&#23637;&#31034;&#32452;&#21512;&#27867;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoFe&#65292;&#19968;&#20010;&#27979;&#35797;&#22871;&#20214;&#26469;&#35843;&#26597;&#19978;&#19979;&#25991;&#32452;&#21512;&#27867;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32452;&#21512;&#27867;&#21270;&#24615;&#33021;&#24456;&#23481;&#26131;&#21463;&#21040;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#30740;&#31350;&#38382;&#39064;&#65306;&#20160;&#20040;&#26159;&#22312;&#32452;&#21512;&#27867;&#21270;&#20013;&#21046;&#20316;&#22909;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#20010;&#28508;&#22312;&#22240;&#32032;&#65306;&#30456;&#20284;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#32452;&#21512;&#36890;&#29992;&#24615;&#20013;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#24212;&#35813;&#22312;&#32467;&#26500;&#19978;&#19982;&#27979;&#35797;&#29992;&#20363;&#31867;&#20284;&#65292;&#30456;&#20114;&#20043;&#38388;&#24212;&#35813;&#19981;&#21516;&#65292;&#32780;&#19988;&#21333;&#29420;&#22320;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositional generalization--understanding unseen combinations of seen primitives--is an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning--the prevailing few-shot paradigm based on large language models--exhibits compositional generalization. In this paper, we present CoFe, a test suite to investigate in-context compositional generalization. We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization. We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;Bootstrap&#23398;&#20064;&#29992;&#20110;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#32852;&#21512;&#25277;&#21462;&#12290;&#36890;&#36807;&#25506;&#32034;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#21644;&#33258;&#25105;&#38598;&#25104;&#27491;&#21017;&#21270;&#22120;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#26089;&#26399;&#24555;&#36895;&#25910;&#25947;&#24182;&#19988;&#32531;&#35299;&#20102;&#22122;&#22768;&#26631;&#31614;&#20135;&#29983;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#23454;&#20307;&#23545;&#25277;&#21462;&#21644;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;F1&#24471;&#20998;&#20998;&#21035;&#25552;&#39640;&#20102;4.43%&#21644;4.92%&#12290;</title><link>http://arxiv.org/abs/2305.03827</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;Bootstrap&#23398;&#20064;&#29992;&#20110;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#32852;&#21512;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Bootstrap Learning for Joint Extraction on Distantly-Supervised Data. (arXiv:2305.03827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;Bootstrap&#23398;&#20064;&#29992;&#20110;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#32852;&#21512;&#25277;&#21462;&#12290;&#36890;&#36807;&#25506;&#32034;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#21644;&#33258;&#25105;&#38598;&#25104;&#27491;&#21017;&#21270;&#22120;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#26089;&#26399;&#24555;&#36895;&#25910;&#25947;&#24182;&#19988;&#32531;&#35299;&#20102;&#22122;&#22768;&#26631;&#31614;&#20135;&#29983;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#23454;&#20307;&#23545;&#25277;&#21462;&#21644;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;F1&#24471;&#20998;&#20998;&#21035;&#25552;&#39640;&#20102;4.43%&#21644;4.92%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#24102;&#26377;&#27169;&#31946;&#25110;&#22122;&#22768;&#26631;&#31614;&#30340;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#26102;&#65292;&#32852;&#21512;&#25277;&#21462;&#23454;&#20307;&#23545;&#21450;&#20854;&#20851;&#31995;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;Bootstrap&#23398;&#20064;&#65292;&#20854;&#21160;&#26426;&#26159;&#26681;&#25454;&#30452;&#35273;&#65292;&#19968;&#20010;&#23454;&#20363;&#30340;&#19981;&#30830;&#23450;&#24615;&#36234;&#39640;&#65292;&#27169;&#22411;&#32622;&#20449;&#24230;&#19982;&#30495;&#23454;&#26631;&#31614;&#19981;&#19968;&#33268;&#30340;&#21487;&#33021;&#24615;&#23601;&#36234;&#22823;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#32034;&#23454;&#20363;&#32423;&#21035;&#30340;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#65292;&#21019;&#24314;&#19968;&#20010;&#39640;&#32622;&#20449;&#30340;&#21021;&#22987;&#26679;&#20363;&#38598;&#12290;&#36825;&#26679;&#30340;&#23376;&#38598;&#29992;&#20110;&#36807;&#28388;&#22122;&#22768;&#23454;&#20363;&#65292;&#24182;&#26377;&#21161;&#20110;&#27169;&#22411;&#22312;&#26089;&#26399;&#24555;&#36895;&#25910;&#25947;&#12290;&#22312;Bootstrap&#23398;&#20064;&#26399;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#33258;&#25105;&#38598;&#25104;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#20943;&#36731;&#22122;&#22768;&#26631;&#31614;&#20135;&#29983;&#30340;&#27169;&#22411;&#38388;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23450;&#20041;&#32852;&#21512;&#26631;&#35760;&#27010;&#29575;&#30340;&#27010;&#29575;&#26041;&#24046;&#65292;&#20197;&#20272;&#35745;&#20869;&#37096;&#27169;&#22411;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#29992;&#20110;&#36873;&#25321;&#21644;&#24314;&#31435;&#26032;&#30340;&#21487;&#38752;&#35757;&#32451;&#23454;&#20363;&#36827;&#34892;&#19979;&#19968;&#27425;&#36845;&#20195;&#12290;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#22312;&#23454;&#20307;&#23545;&#25277;&#21462;&#21644;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;F1&#24471;&#20998;&#20998;&#21035;&#25552;&#39640;&#20102;4.43%&#21644;4.92%&#12290;
&lt;/p&gt;
&lt;p&gt;
Jointly extracting entity pairs and their relations is challenging when working on distantly-supervised data with ambiguous or noisy labels. To mitigate such impact, we propose uncertainty-aware bootstrap learning, which is motivated by the intuition that the higher uncertainty of an instance, the more likely the model confidence is inconsistent with the ground truths. Specifically, we first explore instance-level data uncertainty to create an initial high-confident examples. Such subset serves as filtering noisy instances and facilitating the model to converge fast at the early stage. During bootstrap learning, we propose self-ensembling as a regularizer to alleviate inter-model uncertainty produced by noisy labels. We further define probability variance of joint tagging probabilities to estimate inner-model parametric uncertainty, which is used to select and build up new reliable training instances for the next iteration. Experimental results on two large datasets reveal that our app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01555</link><description>&lt;p&gt;
&#22914;&#20309;&#21457;&#25381;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#36824;&#27809;&#26377;&#24471;&#21040;&#20840;&#38754;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#35814;&#32454;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#30340;&#22522;&#26412;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25968;&#25454;&#29983;&#25104;&#12290;&#20026;&#20102;&#22686;&#24378;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;&#20197;&#21069;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#32780;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25512;&#21160;&#20197;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#22312;&#22235;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#28608;&#21457;&#26410;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312; \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25910;&#38598;&#21644;&#35780;&#20272;&#22810;&#26041;&#23545;&#35805;&#24773;&#20917;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22312;&#32676;&#20307;&#23545;&#35805;&#20013;&#38656;&#35201;&#20855;&#22791;&#30340;&#25216;&#33021;&#65292;&#21457;&#29616;&#26032;&#25968;&#25454;&#38598;MultiLIGHT&#21487;&#20197;&#22312;&#36825;&#20010;&#39046;&#22495;&#24102;&#26469;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.13835</link><description>&lt;p&gt;
&#22810;&#26041;&#32842;&#22825;&#65306;&#20154;&#31867;&#21644;&#27169;&#22411;&#20013;&#30340;&#32676;&#32842;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models. (arXiv:2304.13835v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25910;&#38598;&#21644;&#35780;&#20272;&#22810;&#26041;&#23545;&#35805;&#24773;&#20917;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22312;&#32676;&#20307;&#23545;&#35805;&#20013;&#38656;&#35201;&#20855;&#22791;&#30340;&#25216;&#33021;&#65292;&#21457;&#29616;&#26032;&#25968;&#25454;&#38598;MultiLIGHT&#21487;&#20197;&#22312;&#36825;&#20010;&#39046;&#22495;&#24102;&#26469;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#23545;&#35805;&#30740;&#31350;&#20027;&#35201;&#30740;&#31350;&#25104;&#23545;&#65288;&#21452;&#26041;&#65289;&#23545;&#35805;&#65292;&#24182;&#27809;&#26377;&#28041;&#21450;&#21040;&#22810;&#20110;&#20004;&#20010;&#20154;&#22312;&#19968;&#36215;&#23545;&#35805;&#30340;&#26085;&#24120;&#24773;&#26223;&#12290;&#26412;&#25991;&#20351;&#29992;LIGHT&#29615;&#22659;&#26500;&#24314;&#25509;&#22320;&#23545;&#35805;&#26469;&#25910;&#38598;&#21644;&#35780;&#20272;&#22810;&#26041;&#23545;&#35805;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#27604;&#22312;&#26032;&#25968;&#25454;&#38598;MultiLIGHT&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#29616;&#26377;&#30340;&#25104;&#23545;&#35757;&#32451;&#30340;&#23545;&#35805;&#27169;&#22411;&#20197;&#21450;&#24102;&#26377;&#23569;&#37327;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#23558;&#20844;&#24320;&#21457;&#24067;MultiLIGHT&#25968;&#25454;&#38598;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#22312;&#32676;&#20307;&#35774;&#32622;&#20013;&#24102;&#26469;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current dialogue research primarily studies pairwise (two-party) conversations, and does not address the everyday setting where more than two speakers converse together. In this work, we both collect and evaluate multi-party conversations to study this more general case. We use the LIGHT environment to construct grounded conversations, where each participant has an assigned character to role-play. We thus evaluate the ability of language models to act as one or more characters in such conversations. Models require two skills that pairwise-trained models appear to lack: (1) being able to decide when to talk; (2) producing coherent utterances grounded on multiple characters. We compare models trained on our new dataset to existing pairwise-trained dialogue models, as well as large language models with few-shot prompting. We find that our new dataset, MultiLIGHT, which we will publicly release, can help bring significant improvements in the group setting.
&lt;/p&gt;</description></item><item><title>AUTODIAL&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#23545;&#35805;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#24182;&#34892;&#35299;&#30721;&#22120;&#26469;&#25191;&#34892;&#23545;&#35805;&#20219;&#21153;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#24182;&#23454;&#29616;&#26356;&#24555;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;&#19982;&#29616;&#26377;&#30340;&#29983;&#25104;&#26041;&#27861;&#30456;&#27604;&#65292;AUTODIAL&#22312;&#19977;&#20010;&#23545;&#35805;&#20219;&#21153;&#19978;&#25552;&#20379;&#20102;3-6&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#20855;&#26377;11&#20493;&#30340;&#21442;&#25968;&#20943;&#23569;&#12290;&#36825;&#34920;&#26126;&#23558;&#24403;&#21069;&#30340;&#23545;&#35805;&#27169;&#22411;&#25193;&#23637;&#20026;&#20855;&#26377;&#24182;&#34892;&#35299;&#30721;&#22120;&#21487;&#20197;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#37096;&#32626;&#23427;&#20204;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.06245</link><description>&lt;p&gt;
AUTODIAL: &#39640;&#25928;&#24322;&#27493;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AUTODIAL: Efficient Asynchronous Task-Oriented Dialogue Model. (arXiv:2303.06245v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06245
&lt;/p&gt;
&lt;p&gt;
AUTODIAL&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#23545;&#35805;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#24182;&#34892;&#35299;&#30721;&#22120;&#26469;&#25191;&#34892;&#23545;&#35805;&#20219;&#21153;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#24182;&#23454;&#29616;&#26356;&#24555;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;&#19982;&#29616;&#26377;&#30340;&#29983;&#25104;&#26041;&#27861;&#30456;&#27604;&#65292;AUTODIAL&#22312;&#19977;&#20010;&#23545;&#35805;&#20219;&#21153;&#19978;&#25552;&#20379;&#20102;3-6&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#20855;&#26377;11&#20493;&#30340;&#21442;&#25968;&#20943;&#23569;&#12290;&#36825;&#34920;&#26126;&#23558;&#24403;&#21069;&#30340;&#23545;&#35805;&#27169;&#22411;&#25193;&#23637;&#20026;&#20855;&#26377;&#24182;&#34892;&#35299;&#30721;&#22120;&#21487;&#20197;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#37096;&#32626;&#23427;&#20204;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
AUTODIAL is a multi-task dialogue model that significantly reduces memory footprint and achieves faster inference times by using parallel decoders to perform dialogue tasks. Compared to existing generative approach, AUTODIAL provides 3-6x speedups during inference while having 11x fewer parameters on three dialogue tasks. This suggests that extending current dialogue models to have parallel decoders can be a viable alternative for deploying them in resource-constrained environments.
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#23545;&#35805;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#21464;&#24471;&#26222;&#36941;&#65292;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#26356;&#22823;&#30340;&#20869;&#23384;&#21344;&#29992;&#30340;&#39640;&#35745;&#31639;&#35201;&#27714;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AUTODIAL&#65292;&#19968;&#31181;&#22810;&#20219;&#21153;&#23545;&#35805;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#37096;&#32626;&#23545;&#35805;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;AUTODIAL&#21033;&#29992;&#24182;&#34892;&#35299;&#30721;&#22120;&#25191;&#34892;&#35832;&#22914;&#23545;&#35805;&#34892;&#20026;&#39044;&#27979;&#12289;&#39046;&#22495;&#39044;&#27979;&#12289;&#24847;&#22270;&#39044;&#27979;&#21644;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#31561;&#20219;&#21153;&#12290;&#20351;&#29992;&#20998;&#31867;&#35299;&#30721;&#22120;&#32780;&#19981;&#26159;&#29983;&#25104;&#35299;&#30721;&#22120;&#20351;AUTODIAL&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#38388;&#19978;&#23454;&#29616;&#27604;&#29616;&#26377;&#29983;&#25104;&#26041;&#27861;&#65288;&#21363;SimpleTOD&#65289;&#26356;&#24555;&#30340;&#36895;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23558;&#24403;&#21069;&#30340;&#23545;&#35805;&#27169;&#22411;&#25193;&#23637;&#20026;&#20855;&#26377;&#24182;&#34892;&#35299;&#30721;&#22120;&#21487;&#20197;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#37096;&#32626;&#23427;&#20204;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large dialogue models become commonplace in practice, the problems surrounding high compute requirements for training, inference and larger memory footprint still persists. In this work, we present AUTODIAL, a multi-task dialogue model that addresses the challenges of deploying dialogue model. AUTODIAL utilizes parallel decoders to perform tasks such as dialogue act prediction, domain prediction, intent prediction, and dialogue state tracking. Using classification decoders over generative decoders allows AUTODIAL to significantly reduce memory footprint and achieve faster inference times compared to existing generative approach namely SimpleTOD. We demonstrate that AUTODIAL provides 3-6x speedups during inference while having 11x fewer parameters on three dialogue tasks compared to SimpleTOD. Our results show that extending current dialogue models to have parallel decoders can be a viable alternative for deploying them in resource-constrained environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20219;&#21153;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#38382;&#39064;&#65292;&#34429;&#28982;&#36824;&#27809;&#26377;&#22312;ARC&#19978;&#20987;&#36133;&#26368;&#20808;&#36827;&#30340;DSL&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#20808;&#21069;&#26410;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.04091</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#23454;&#29616;&#35270;&#35273;&#25277;&#35937;&#21644;&#25512;&#29702;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Visual Abstraction and Reasoning through Language. (arXiv:2303.04091v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20219;&#21153;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#38382;&#39064;&#65292;&#34429;&#28982;&#36824;&#27809;&#26377;&#22312;ARC&#19978;&#20987;&#36133;&#26368;&#20808;&#36827;&#30340;DSL&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#20808;&#21069;&#26410;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#22312;&#23616;&#38480;&#24212;&#29992;&#20013;&#24050;&#32463;&#36798;&#21040;&#20102;&#20154;&#31867;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#23637;&#29616;&#26356;&#24191;&#27867;&#21644;&#26356;&#28789;&#27963;&#30340;&#26234;&#33021;&#12290;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#26088;&#22312;&#35780;&#20272;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#65288;DSL&#65289;&#65292;&#29992;&#20110;&#26292;&#21147;&#35299;&#20915;ARC&#20013;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;ARC&#38382;&#39064;&#12290;&#34429;&#28982;&#36824;&#27809;&#26377;&#22312;ARC&#19978;&#20987;&#36133;&#26368;&#20808;&#36827;&#30340;DSL&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#20808;&#21069;&#26410;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Artificial Intelligence (AI) models have achieved human or even superhuman performance in narrowly defined applications, they still struggle to show signs of broader and more flexible intelligence. The Abstraction and Reasoning Corpus (ARC), introduced by Fran\c{c}ois Chollet, aims to assess how close AI systems are to human-like cognitive abilities. Most current approaches rely on carefully handcrafted domain-specific languages (DSLs), which are used to brute-force solutions to the tasks present in ARC. In this work, we propose a general framework for solving ARC based on natural language descriptions of the tasks. While not yet beating state-of-the-art DSL models on ARC, we demonstrate the immense potential of our approach hinted at by the ability to solve previously unsolved tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#23545;&#19981;&#21516;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#26816;&#27979;&#36229;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#36229;&#20998;&#24067;&#40065;&#26834;&#24615;&#25552;&#20379;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2301.11660</link><description>&lt;p&gt;
&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#36229;&#20998;&#24067;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning. (arXiv:2301.11660v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#23545;&#19981;&#21516;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#26816;&#27979;&#36229;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#36229;&#20998;&#24067;&#40065;&#26834;&#24615;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#21152;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24357;&#34917;&#24494;&#35843;&#25104;&#26412;&#30340;&#24040;&#22823;&#20195;&#20215;&#12290;&#23613;&#31649;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21644;&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#65288;PETL&#65289;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#33021;&#26377;&#25928;&#22320;&#22788;&#29702;&#24050;&#20998;&#24067;&#25913;&#21464;&#30340;&#36755;&#20837;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#38543;&#30528;PLM&#22823;&#23567;&#22686;&#38271;&#25110;&#25913;&#21464;&#20256;&#36755;&#26041;&#27861;&#65292;&#26816;&#27979;&#36229;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#33021;&#21147;&#22914;&#20309;&#25913;&#21464;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;PETL&#25216;&#26415;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;Adapter&#12289;LoRA&#21644;&#21069;&#32512;&#35843;&#25972;&#65292;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#20351;&#29992;&#19981;&#21516;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of the pre-trained language model (PLM) continues to increase, numerous parameter-efficient transfer learning methods have been proposed recently to compensate for the tremendous cost of fine-tuning. Despite the impressive results achieved by large pre-trained language models (PLMs) and various parameter-efficient transfer learning (PETL) methods on sundry benchmarks, it remains unclear if they can handle inputs that have been distributionally shifted effectively. In this study, we systematically explore how the ability to detect out-of-distribution (OOD) changes as the size of the PLM grows or the transfer methods are altered. Specifically, we evaluated various PETL techniques, including fine-tuning, Adapter, LoRA, and prefix-tuning, on three different intention classification tasks, each utilizing various language models with different scales.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38544;&#31169;&#20445;&#25252;&#30340;&#35821;&#20041;&#35299;&#26512;&#22120;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#30495;&#23454;&#29992;&#25143;&#35821;&#21477;&#26469;&#24110;&#21161;&#22686;&#21152;&#31995;&#32479;&#30340;&#35821;&#35328;&#21644;&#21151;&#33021;&#35206;&#30422;&#33539;&#22260;&#65292;&#21516;&#26102;&#19981;&#20250;&#21361;&#21450;&#23454;&#38469;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2212.10520</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#35821;&#20041;&#35299;&#26512;&#22120;&#39046;&#22495;&#33258;&#36866;&#24212;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Domain Adaptation of Semantic Parsers. (arXiv:2212.10520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38544;&#31169;&#20445;&#25252;&#30340;&#35821;&#20041;&#35299;&#26512;&#22120;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#30495;&#23454;&#29992;&#25143;&#35821;&#21477;&#26469;&#24110;&#21161;&#22686;&#21152;&#31995;&#32479;&#30340;&#35821;&#35328;&#21644;&#21151;&#33021;&#35206;&#30422;&#33539;&#22260;&#65292;&#21516;&#26102;&#19981;&#20250;&#21361;&#21450;&#23454;&#38469;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#36890;&#24120;&#36741;&#21161;&#29992;&#25143;&#22788;&#29702;&#20010;&#20154;&#25110;&#26426;&#23494;&#20107;&#21153;&#12290;&#22240;&#27492;&#65292;&#27492;&#31867;&#31995;&#32479;&#30340;&#24320;&#21457;&#20154;&#21592;&#36890;&#24120;&#34987;&#31105;&#27490;&#35266;&#23519;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#12290;&#37027;&#20040;&#65292;&#20182;&#20204;&#22914;&#20309;&#30693;&#36947;&#31995;&#32479;&#22312;&#21738;&#20123;&#26041;&#38754;&#23384;&#22312;&#22833;&#36133;&#24182;&#38656;&#35201;&#26356;&#22810;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#26032;&#21151;&#33021;&#21602;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#21512;&#25104;&#30495;&#23454;&#29992;&#25143;&#35821;&#21477;&#26469;&#24110;&#21161;&#22686;&#21152;&#31995;&#32479;&#30340;&#35821;&#35328;&#21644;&#21151;&#33021;&#35206;&#30422;&#33539;&#22260;&#65292;&#21516;&#26102;&#19981;&#20250;&#21361;&#21450;&#23454;&#38469;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#24046;&#20998;&#38544;&#31169;&#29983;&#25104;&#26041;&#27861;&#65292;&#39318;&#20808;&#29983;&#25104;&#28508;&#22312;&#30340;&#35821;&#20041;&#35299;&#26512;&#65292;&#28982;&#21518;&#26681;&#25454;&#35299;&#26512;&#29983;&#25104;&#35821;&#21477;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31169;&#26377;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#30456;&#23545;&#20110;&#24403;&#21069;&#26041;&#27861;&#25552;&#39640;&#20102;MAUVE 2.5&#20493;&#21644;&#35821;&#20041;&#35206;&#30422;1.3&#20493;&#65292;&#25552;&#39640;&#20102;&#27969;&#30021;&#24615;&#21644;&#35821;&#20041;&#35206;&#30422;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#19968;&#20010;&#23454;&#38469;&#30340;&#39046;&#22495;&#36866;&#24212;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#28155;&#21152;&#26032;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogue systems often assist users with personal or confidential matters. For this reason, the developers of such a system are generally prohibited from observing actual usage. So how can they know where the system is failing and needs more training data or new functionality? In this work, we study ways in which realistic user utterances can be generated synthetically, to help increase the linguistic and functional coverage of the system, without compromising the privacy of actual users. To this end, we propose a two-stage Differentially Private (DP) generation method which first generates latent semantic parses, and then generates utterances based on the parses. Our proposed approach improves MAUVE by 2.5X and parse tree function type overlap by 1.3X relative to current approaches for private synthetic data generation, improving both on fluency and semantic coverage. We further validate our approach on a realistic domain adaptation task of adding new functionality from 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#38382;&#39064;&#39537;&#21160;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;Socratic&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#25552;&#39640;&#25688;&#35201;&#20219;&#21153;&#30340;&#21487;&#25511;&#24615;&#65292;&#24182;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#22810;&#20010;&#25511;&#21046;&#31574;&#30053;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#24471;&#20986;&#30340;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.10449</link><description>&lt;p&gt;
Socratic&#39044;&#35757;&#32451;&#65306;&#38754;&#21521;&#21487;&#25511;&#25688;&#35201;&#30340;&#38382;&#39064;&#39537;&#21160;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization. (arXiv:2212.10449v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#38382;&#39064;&#39537;&#21160;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;Socratic&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#25552;&#39640;&#25688;&#35201;&#20219;&#21153;&#30340;&#21487;&#25511;&#24615;&#65292;&#24182;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#22810;&#20010;&#25511;&#21046;&#31574;&#30053;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#24471;&#20986;&#30340;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#38271;&#31687;&#25991;&#26723;&#30340;&#21487;&#25511;&#25688;&#35201;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#24456;&#38590;&#36866;&#24212;&#20219;&#21153;&#24182;&#26377;&#25928;&#22320;&#21709;&#24212;&#29992;&#25143;&#26597;&#35810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Socratic&#39044;&#35757;&#32451;&#65292;&#19968;&#31181;&#38754;&#21521;&#38382;&#39064;&#39537;&#21160;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#25688;&#35201;&#20219;&#21153;&#30340;&#21487;&#25511;&#24615;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#21644;&#22238;&#31572;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#30456;&#20851;&#38382;&#39064;&#65292;Socratic&#39044;&#35757;&#32451;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#36981;&#24490;&#29992;&#25143;&#25552;&#20379;&#30340;&#26597;&#35810;&#65292;&#24182;&#30830;&#23450;&#38656;&#35201;&#25688;&#35201;&#30340;&#30456;&#20851;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20004;&#20010;&#25688;&#35201;&#22495;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#21363;&#30701;&#31687;&#25925;&#20107;&#21644;&#23545;&#35805;&#65292;&#24182;&#20351;&#29992;&#20851;&#38190;&#35789;&#12289;&#38382;&#39064;&#21644;&#20107;&#23454;QA&#23545;&#22810;&#20010;&#25511;&#21046;&#31574;&#30053;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#21482;&#20381;&#36182;&#20110;&#26080;&#26631;&#27880;&#25991;&#26723;&#21644;&#38382;&#39064;&#29983;&#25104;&#31995;&#32479;&#65292;&#34920;&#29616;&#20248;&#20110;&#20351;&#29992;&#39069;&#22806;&#30340;&#30417;&#30563;&#25968;&#25454;&#30340;&#39044;&#31934;&#35843;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Socratic&#39044;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#29983;&#25104;&#36981;&#23432;&#29992;&#25143;&#25351;&#23450;&#32422;&#26463;&#26465;&#20214;&#30340;&#25688;&#35201;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In long document controllable summarization, where labeled data is scarce, pretrained models struggle to adapt to the task and effectively respond to user queries. In this paper, we introduce Socratic pretraining, a question-driven, unsupervised pretraining objective specifically designed to improve controllability in summarization tasks. By training a model to generate and answer relevant questions in a given context, Socratic pretraining enables the model to more effectively adhere to user-provided queries and identify relevant content to be summarized. We demonstrate the effectiveness of this approach through extensive experimentation on two summarization domains, short stories and dialogue, and multiple control strategies: keywords, questions, and factoid QA pairs. Our pretraining method relies only on unlabeled documents and a question generation system and outperforms pre-finetuning approaches that use additional supervised data. Furthermore, our results show that Socratic pretra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36731;&#24230;&#19978;&#19979;&#25991;&#25935;&#24863;&#35821;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#38388;&#26029;&#24615;&#32452;&#22359;&#21477;&#27861;&#20998;&#26512;&#30340;&#35821;&#27861;&#24402;&#32435;&#12290;&#22312;&#20445;&#35777;&#35268;&#21017;&#32467;&#26500;&#30340;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#29992;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#23545;&#22823;&#37327;&#38750;&#32456;&#32467;&#31526;&#20855;&#26377;&#25193;&#23637;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.09140</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#38388;&#26029;&#24615;&#32452;&#22359;&#21477;&#27861;&#20998;&#26512;&#21450;&#20854;&#19982;&#36731;&#24230;&#19978;&#19979;&#25991;&#26377;&#20851;&#30340;&#35821;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Discontinuous Constituency Parsing with Mildly Context-Sensitive Grammars. (arXiv:2212.09140v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36731;&#24230;&#19978;&#19979;&#25991;&#25935;&#24863;&#35821;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#38388;&#26029;&#24615;&#32452;&#22359;&#21477;&#27861;&#20998;&#26512;&#30340;&#35821;&#27861;&#24402;&#32435;&#12290;&#22312;&#20445;&#35777;&#35268;&#21017;&#32467;&#26500;&#30340;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#29992;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#23545;&#22823;&#37327;&#38750;&#32456;&#32467;&#31526;&#20855;&#26377;&#25193;&#23637;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36731;&#24230;&#19978;&#19979;&#25991;&#25935;&#24863;&#35821;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#38388;&#26029;&#24615;&#32452;&#22359;&#21477;&#27861;&#20998;&#26512;&#30340;&#35821;&#27861;&#24402;&#32435;&#12290;&#25105;&#20204;&#20351;&#29992;&#27010;&#29575;&#32447;&#24615;&#19978;&#19979;&#25991;&#26080;&#20851;&#37325;&#20889;&#31995;&#32479; (LCFRS) &#24418;&#24335;&#65292;&#22312;&#20445;&#35777;&#35268;&#21017;&#32467;&#26500;&#30340;&#21069;&#25552;&#19979;&#65292;&#37325;&#28857;&#20851;&#27880;&#26368;&#22823;&#20284;&#28982;&#30340;&#21442;&#25968;&#23398;&#20064;&#12290;&#20026;&#20102;&#38477;&#20302;&#35299;&#26512;&#21644;&#21442;&#25968;&#20272;&#35745;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#25105;&#20204;&#23558;&#35821;&#27861;&#24418;&#24335;&#38480;&#21046;&#20026; LCFRS-2 (&#21363;&#65292;&#20855;&#26377;&#20004;&#20010;&#25159;&#20986;&#30340;&#20108;&#20803; LCFRS) &#24182;&#19988;&#20002;&#24323;&#38656;&#35201; O(n^6) &#30340;&#35299;&#26512;&#26102;&#38388;&#30340;&#35268;&#21017;&#65292;&#23558;&#25512;&#29702;&#38477;&#20302;&#21040; O(n^5)&#12290;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22823;&#37327;&#38750;&#32456;&#32467;&#31526;&#26159;&#26377;&#30410;&#30340;&#65292;&#22240;&#27492;&#21033;&#29992;&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340;&#31209;&#31354;&#38388;&#21160;&#24577;&#35268;&#21010;&#21644;&#22522;&#20110;&#23884;&#20837;&#30340;&#35268;&#21017;&#27010;&#29575;&#21442;&#25968;&#21270;&#26469;&#25193;&#22823;&#38750;&#32456;&#32467;&#31526;&#30340;&#25968;&#37327;&#12290;&#23545;&#24503;&#35821;&#21644;&#33655;&#20848;&#35821;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#35825;&#23548;&#20986;&#20855;&#26377;&#36830;&#32493;&#21644;&#19981;&#36830;&#32493;&#32467;&#26500;&#30340;&#35821;&#35328;&#23398;&#24847;&#20041;&#30340;&#26641;&#29366;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study grammar induction with mildly context-sensitive grammars for unsupervised discontinuous parsing. Using the probabilistic linear context-free rewriting system (LCFRS) formalism, our approach fixes the rule structure in advance and focuses on parameter learning with maximum likelihood. To reduce the computational complexity of both parsing and parameter estimation, we restrict the grammar formalism to LCFRS-2 (i.e., binary LCFRS with fan-out two) and further discard rules that require O(n^6) time to parse, reducing inference to O(n^5). We find that using a large number of nonterminals is beneficial and thus make use of tensor decomposition-based rank-space dynamic programming with an embedding-based parameterization of rule probabilities to scale up the number of nonterminals. Experiments on German and Dutch show that our approach is able to induce linguistically meaningful trees with continuous and discontinuous structures
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#23637;&#39044;&#35757;&#32451;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#31639;&#26415;&#30340;&#39044;&#35757;&#32451;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#25968;&#23383;&#34920;&#36798;&#33021;&#21147;&#21644;&#25968;&#20540;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#26550;&#26500;&#21464;&#21270;&#25110;&#37325;&#26032;&#39044;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#31639;&#26415;&#30340;&#39044;&#35757;&#32451;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#37117;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2205.06733</link><description>&lt;p&gt;
&#22522;&#20110;&#31639;&#26415;&#30340;&#39044;&#35757;&#32451;&#8212;&#8212;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23383;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Arithmetic-Based Pretraining -- Improving Numeracy of Pretrained Language Models. (arXiv:2205.06733v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.06733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#23637;&#39044;&#35757;&#32451;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#31639;&#26415;&#30340;&#39044;&#35757;&#32451;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#25968;&#23383;&#34920;&#36798;&#33021;&#21147;&#21644;&#25968;&#20540;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#26550;&#26500;&#21464;&#21270;&#25110;&#37325;&#26032;&#39044;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#31639;&#26415;&#30340;&#39044;&#35757;&#32451;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#37117;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#29702;&#35299;&#21644;&#20351;&#29992;&#25968;&#23383;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20027;&#35201;&#26377;&#20004;&#20010;&#21407;&#22240;&#65306;(1)&#24120;&#29992;&#30340;&#20998;&#35789;&#31639;&#27861;&#23545;&#25968;&#23383;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;(2)&#24120;&#35265;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#19981;&#38024;&#23545;&#25968;&#20540;&#33021;&#21147;&#12290;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#26550;&#26500;&#21464;&#21270;&#25110;&#37325;&#26032;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#23637;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#31639;&#26415;&#30340;&#39044;&#35757;&#32451;&#65292;&#23427;&#21487;&#20197;&#22312;&#19968;&#20010;&#25193;&#23637;&#30340;&#39044;&#35757;&#32451;&#27493;&#39588;&#20013;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#32570;&#28857;&#65292;&#32780;&#19981;&#38656;&#35201;&#26550;&#26500;&#21464;&#21270;&#25110;&#37325;&#26032;&#39044;&#35757;&#32451;&#12290;&#22522;&#20110;&#31639;&#26415;&#30340;&#39044;&#35757;&#32451;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#26032;&#30340;&#21487;&#25512;&#26029;&#25968;&#39044;&#27979;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#30456;&#32467;&#21512;&#65292;&#20197;&#25913;&#36827;&#25968;&#23383;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#31639;&#26415;&#30340;&#39044;&#35757;&#32451;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#37117;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art pretrained language models tend to perform below their capabilities when applied out-of-the-box on tasks that require understanding and working with numbers. Recent work suggests two main reasons for this: (1) popular tokenisation algorithms have limited expressiveness for numbers, and (2) common pretraining objectives do not target numeracy. Approaches that address these shortcomings usually require architectural changes or pretraining from scratch. In this paper, we propose a new extended pretraining approach called Arithmetic-Based Pretraining that jointly addresses both in one extended pretraining step without requiring architectural changes or pretraining from scratch. Arithmetic-Based Pretraining combines contrastive learning to improve the number representation, and a novel extended pretraining objective called Inferable Number Prediction Task to improve numeracy. Our experiments show the effectiveness of Arithmetic-Based Pretraining in three different tasks tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#36328;&#35821;&#35328;VQA&#30340;&#20960;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#24314;&#27169;&#26041;&#27861;&#21644;&#23398;&#20064;&#20559;&#24046;&#65292;&#21457;&#29616;&#31616;&#21333;&#20462;&#25913;&#21487;&#20197;&#20943;&#23569;&#21040;&#21333;&#35821;&#33521;&#35821;&#24615;&#33021;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2202.07630</link><description>&lt;p&gt;
&#26356;&#28145;&#20837;&#22320;&#25506;&#31350;&#36328;&#35821;&#35328;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Delving Deeper into Cross-lingual Visual Question Answering. (arXiv:2202.07630v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#36328;&#35821;&#35328;VQA&#30340;&#20960;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#24314;&#27169;&#26041;&#27861;&#21644;&#23398;&#20064;&#20559;&#24046;&#65292;&#21457;&#29616;&#31616;&#21333;&#20462;&#25913;&#21487;&#20197;&#20943;&#23569;&#21040;&#21333;&#35821;&#33521;&#35821;&#24615;&#33021;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26159;&#20851;&#38190;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#20219;&#21153;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;VQA&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#65292;&#22240;&#32570;&#20047;&#21512;&#36866;&#30340;&#35780;&#20272;&#36164;&#28304;&#12290;&#20808;&#21069;&#30340;&#36328;&#35821;&#35328;VQA&#30740;&#31350;&#25253;&#36947;&#20102;&#24403;&#21069;&#22810;&#35821;&#35328;&#22810;&#27169;&#24577;Transformer&#30340;&#38646;&#26679;&#26412;&#36716;&#31227;&#24615;&#33021;&#24046;&#65292;&#19982;&#21333;&#35821;&#24615;&#33021;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#65292;&#20294;&#27809;&#26377;&#36827;&#34892;&#28145;&#20837;&#30340;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26356;&#28145;&#20837;&#22320;&#25506;&#31350;&#20102;&#36328;&#35821;&#35328;VQA&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#26088;&#22312;&#20102;&#35299;1&#65289;&#24314;&#27169;&#26041;&#27861;&#21644;&#36873;&#25321;&#65292;&#21253;&#25324;&#20307;&#31995;&#32467;&#26500;&#12289;&#24402;&#32435;&#20559;&#35265;&#21644;&#24494;&#35843;&#65307;2&#65289;&#23398;&#20064;&#20559;&#24046;&#65306;&#21253;&#25324;&#36328;&#35821;&#35328;&#35774;&#32622;&#20013;&#30340;&#38382;&#39064;&#31867;&#22411;&#21644;&#27169;&#24577;&#20559;&#24046;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65306;1&#65289;&#25105;&#20204;&#26174;&#31034;&#20986;&#26631;&#20934;&#35757;&#32451;&#35774;&#32622;&#30340;&#31616;&#21333;&#20462;&#25913;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#36716;&#31227;&#21040;&#21333;&#35821;&#33521;&#35821;&#24615;&#33021;&#30340;&#24046;&#36317;&#65292;&#20174;&#29616;&#26377;&#26041;&#27861;&#20013;&#33719;&#24471;+10&#30340;&#20934;&#30830;&#24230;&#65307;2&#65289;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#38382;&#39064;&#31867;&#22411;&#30340;&#36328;&#35821;&#35328;VQA&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) is one of the crucial vision-and-language tasks. Yet, existing VQA research has mostly focused on the English language, due to a lack of suitable evaluation resources. Previous work on cross-lingual VQA has reported poor zero-shot transfer performance of current multilingual multimodal Transformers with large gaps to monolingual performance, without any deeper analysis. In this work, we delve deeper into the different aspects of cross-lingual VQA, aiming to understand the impact of 1) modeling methods and choices, including architecture, inductive bias, fine-tuning; 2) learning biases: including question types and modality biases in cross-lingual setups. The key results of our analysis are: 1) We show that simple modifications to the standard training setup can substantially reduce the transfer gap to monolingual English performance, yielding +10 accuracy points over existing methods. 2) We analyze cross-lingual VQA across different question types of var
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#26426;&#22120;&#31526;&#21495;&#23398;&#30340;&#24605;&#24819;&#65292;&#21363;&#26426;&#22120;&#19981;&#38656;&#35201;&#29702;&#35299;&#35805;&#35821;&#30340;&#20256;&#32479;&#24847;&#20041;&#65292;&#32780;&#26159;&#33021;&#22815;&#20197;&#26684;&#37324;&#26031;&#35821;&#29992;&#23398;&#30340;&#26041;&#24335;&#32472;&#21046;&#20250;&#35805;&#34164;&#28085;&#35828;&#26126;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#24418;&#24335;&#21270;&#20026;&#35805;&#35821;-&#24847;&#20041;&#23545;&#65288;UMP&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2008.10522</link><description>&lt;p&gt;
&#26426;&#22120;&#31526;&#21495;&#23398;
&lt;/p&gt;
&lt;p&gt;
Machine Semiotics. (arXiv:2008.10522v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.10522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#26426;&#22120;&#31526;&#21495;&#23398;&#30340;&#24605;&#24819;&#65292;&#21363;&#26426;&#22120;&#19981;&#38656;&#35201;&#29702;&#35299;&#35805;&#35821;&#30340;&#20256;&#32479;&#24847;&#20041;&#65292;&#32780;&#26159;&#33021;&#22815;&#20197;&#26684;&#37324;&#26031;&#35821;&#29992;&#23398;&#30340;&#26041;&#24335;&#32472;&#21046;&#20250;&#35805;&#34164;&#28085;&#35828;&#26126;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#24418;&#24335;&#21270;&#20026;&#35805;&#35821;-&#24847;&#20041;&#23545;&#65288;UMP&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#35782;&#21040;&#20154;&#31867;&#21644;&#26426;&#22120;&#31526;&#21495;&#23398;&#30340;&#22522;&#26412;&#24046;&#24322;&#20026;&#20811;&#26381;&#24403;&#21069;&#35821;&#38899;&#36741;&#21161;&#35774;&#22791;&#30340;&#32570;&#28857;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;&#23545;&#20110;&#26426;&#22120;&#32780;&#35328;&#65292;&#65288;&#20154;&#31867;&#65289;&#35805;&#35821;&#30340;&#24847;&#20041;&#30001;&#20854;&#33258;&#36523;&#30340;&#34892;&#21160;&#33539;&#22260;&#23450;&#20041;&#12290;&#22240;&#27492;&#65292;&#26426;&#22120;&#19981;&#38656;&#35201;&#29702;&#35299;&#35805;&#35821;&#30340;&#20256;&#32479;&#24847;&#20041;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#20197;&#65288;&#26032;&#65289;&#26684;&#37324;&#26031;&#35821;&#29992;&#23398;&#30340;&#24847;&#20041;&#32472;&#21046;&#20250;&#35805;&#34164;&#28085;&#35828;&#26126;&#12290;&#23545;&#20110;&#35821;&#38899;&#36741;&#21161;&#35774;&#22791;&#32780;&#35328;&#65292;&#23398;&#20064;&#20154;&#31867;&#35805;&#35821;&#30340;&#26426;&#22120;&#29305;&#23450;&#21547;&#20041;&#65292;&#21363;&#36890;&#36807;&#35789;&#27719;&#21270;&#30340;&#23581;&#35797;&#21644;&#35823;&#24046;&#23558;&#20250;&#35805;&#34164;&#28085;&#35828;&#26126;&#21464;&#25104;&#20256;&#32479;&#21270;&#30340;&#21547;&#20041;&#65292;&#20284;&#20046;&#24050;&#32463;&#36275;&#22815;&#20102;&#12290;&#36890;&#36807;&#19968;&#20010;&#30456;&#24403;&#29712;&#30862;&#30340;&#35748;&#30693;&#21152;&#28909;&#35774;&#22791;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22522;&#20110;&#21160;&#24577;&#35821;&#20041;&#23398;&#65292;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#24418;&#24335;&#21270;&#20026;&#35805;&#35821;-&#24847;&#20041;&#23545;&#65288;UMP&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing a basic difference between the semiotics of humans and machines presents a possibility to overcome the shortcomings of current speech assistive devices. For the machine, the meaning of a (human) utterance is defined by its own scope of actions. Machines, thus, do not need to understand the conventional meaning of an utterance. Rather, they draw conversational implicatures in the sense of (neo-)Gricean pragmatics. For speech assistive devices, the learning of machine-specific meanings of human utterances, i.e. the fossilization of conversational implicatures into conventionalized ones by trial and error through lexicalization appears to be sufficient. Using the quite trivial example of a cognitive heating device, we show that - based on dynamic semantics - this process can be formalized as the reinforcement learning of utterance-meaning pairs (UMP).
&lt;/p&gt;</description></item></channel></rss>