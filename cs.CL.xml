<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;CPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#65292;&#25903;&#25345;&#33258;&#21160;&#26435;&#37325;&#37327;&#21270;&#21644;&#20248;&#21270;&#20869;&#26680;&#65292;&#22312;&#27969;&#34892;&#30340;LLMs&#19978;&#23637;&#31034;&#20102;&#26497;&#39640;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.00502</link><description>&lt;p&gt;
&#22312;CPU&#19978;&#39640;&#25928;&#30340;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Efficient LLM Inference on CPUs. (arXiv:2311.00502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;CPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#65292;&#25903;&#25345;&#33258;&#21160;&#26435;&#37325;&#37327;&#21270;&#21644;&#20248;&#21270;&#20869;&#26680;&#65292;&#22312;&#27969;&#34892;&#30340;LLMs&#19978;&#23637;&#31034;&#20102;&#26497;&#39640;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#21644;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#24222;&#22823;&#25968;&#37327;&#65292;LLMs&#30340;&#37096;&#32626;&#19968;&#30452;&#38754;&#20020;&#25361;&#25112;&#65292;&#23545;&#22823;&#20869;&#23384;&#23481;&#37327;&#21644;&#39640;&#20869;&#23384;&#24102;&#23485;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;LLMs&#30340;&#37096;&#32626;&#26356;&#39640;&#25928;&#12290;&#25105;&#20204;&#25903;&#25345;&#33258;&#21160;&#30340;INT4&#26435;&#37325;&#37327;&#21270;&#27969;&#31243;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#27530;&#30340;LLM&#36816;&#34892;&#26102;&#65292;&#20855;&#26377;&#39640;&#24230;&#20248;&#21270;&#30340;&#20869;&#26680;&#65292;&#20197;&#21152;&#36895;&#22312;CPU&#19978;&#30340;LLM&#25512;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27969;&#34892;&#30340;LLMs&#19978;&#30340;&#26222;&#36866;&#24615;&#65292;&#21253;&#25324;Llama2&#65292;Llama&#65292;GPT-NeoX&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;CPU&#19978;&#30340;&#26497;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#20110;: https://github.com/intel/intel-extension-for-transformers.
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable performance and tremendous potential across a wide range of tasks. However, deploying these models has been challenging due to the astronomical amount of model parameters, which requires a demand for large memory capacity and high memory bandwidth. In this paper, we propose an effective approach that can make the deployment of LLMs more efficiently. We support an automatic INT4 weight-only quantization flow and design a special LLM runtime with highly-optimized kernels to accelerate the LLM inference on CPUs. We demonstrate the general applicability of our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase the extreme inference efficiency on CPUs. The code is publicly available at: https://github.com/intel/intel-extension-for-transformers.
&lt;/p&gt;</description></item><item><title>PromptAgent&#26159;&#19968;&#20010;&#36890;&#36807;&#25112;&#30053;&#35268;&#21010;&#23558;&#30446;&#26631;&#20219;&#21153;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#23454;&#29616;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#20027;&#35774;&#35745;&#19982;&#19987;&#23478;&#25163;&#24037;&#25171;&#36896;&#30340;&#25552;&#31034;&#30456;&#24403;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16427</link><description>&lt;p&gt;
PromptAgent: &#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#25112;&#30053;&#35268;&#21010;&#23454;&#29616;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization. (arXiv:2310.16427v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16427
&lt;/p&gt;
&lt;p&gt;
PromptAgent&#26159;&#19968;&#20010;&#36890;&#36807;&#25112;&#30053;&#35268;&#21010;&#23558;&#30446;&#26631;&#20219;&#21153;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#23454;&#29616;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#20027;&#35774;&#35745;&#19982;&#19987;&#23478;&#25163;&#24037;&#25171;&#36896;&#30340;&#25552;&#31034;&#30456;&#24403;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#36890;&#24120;&#38656;&#35201;&#32463;&#36807;&#19987;&#23478;&#30340;&#31934;&#24515;&#35774;&#35745;&#65292;&#38656;&#35201;&#32467;&#21512;&#35814;&#32454;&#30340;&#25351;&#23548;&#21644;&#39046;&#22495;&#35265;&#35299;&#65292;&#22522;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30446;&#26631;&#20219;&#21153;&#32454;&#33410;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#21270;&#29983;&#25104;&#19987;&#23478;&#32423;&#25552;&#31034;&#30340;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#12290;&#29616;&#26377;&#30340;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#39046;&#22495;&#30693;&#35782;&#30340;&#28145;&#24230;&#65292;&#24182;&#19988;&#38590;&#20197;&#39640;&#25928;&#22320;&#25506;&#32034;&#19987;&#23478;&#32423;&#25552;&#31034;&#30340;&#24191;&#38420;&#31354;&#38388;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptAgent&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#20027;&#35774;&#35745;&#19982;&#19987;&#23478;&#25163;&#24037;&#25171;&#36896;&#30340;&#25552;&#31034;&#30456;&#24403;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;PromptAgent&#23558;&#25552;&#31034;&#20248;&#21270;&#35270;&#20026;&#19968;&#20010;&#25112;&#30053;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#26681;&#25454;Monte Carlo&#26641;&#25628;&#32034;&#30340;&#35268;&#21017;&#35745;&#31639;&#31639;&#27861;&#22312;&#19987;&#23478;&#32423;&#25552;&#31034;&#31354;&#38388;&#20013;&#36827;&#34892;&#26377;&#25928;&#23548;&#33322;&#12290;PromptAgent&#36890;&#36807;&#31867;&#20154;&#31867;&#30340;&#21453;&#22797;&#35797;&#38169;&#25506;&#32034;&#65292;&#24341;&#21457;&#31934;&#30830;&#30340;&#19987;&#23478;&#32423;&#35265;&#35299;&#21644;&#28145;&#20837;&#30340;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Highly effective, task-specific prompts are often heavily engineered by experts to integrate detailed instructions and domain insights based on a deep understanding of both instincts of large language models (LLMs) and the intricacies of the target task. However, automating the generation of such expert-level prompts remains elusive. Existing prompt optimization methods tend to overlook the depth of domain knowledge and struggle to efficiently explore the vast space of expert-level prompts. Addressing this, we present PromptAgent, an optimization method that autonomously crafts prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm, rooted in Monte Carlo tree search, to strategically navigate the expert-level prompt space. Inspired by human-like trial-and-error exploration, PromptAgent induces precise expert-level insights and in-depth instructions by r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;g&#30340;&#23384;&#22312;&#65292;&#24182;&#21457;&#29616;&#20102;&#35813;&#22240;&#23376;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#26041;&#24046;&#30340;85%&#65292;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.11616</link><description>&lt;p&gt;
&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;&#65306;&#19968;&#31181;&#24515;&#29702;&#27979;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unveiling the General Intelligence Factor in Language Models: A Psychometric Approach. (arXiv:2310.11616v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;g&#30340;&#23384;&#22312;&#65292;&#24182;&#21457;&#29616;&#20102;&#35813;&#22240;&#23376;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#26041;&#24046;&#30340;85%&#65292;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;g&#30340;&#23384;&#22312;&#65292;&#24182;&#25193;&#23637;&#20102;&#35813;&#29702;&#35770;&#22312;&#20154;&#31867;&#21644;&#26576;&#20123;&#21160;&#29289;&#29289;&#31181;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;Open LLM Leaderboard&#65288;&#21253;&#21547;1,232&#20010;&#27169;&#22411;&#65289;&#21644;General Language Understanding Evaluation&#65288;GLUE&#65289;Leaderboard&#65288;&#21253;&#21547;88&#20010;&#27169;&#22411;&#65289;&#36827;&#34892;&#22240;&#23376;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20855;&#26377;&#19968;&#32500;&#24615;&#21644;&#39640;&#24230;&#31283;&#23450;&#24615;&#30340;g&#22240;&#23376;&#65292;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#26041;&#24046;&#30340;85%&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;g&#20043;&#38388;&#30340;&#20013;&#24230;&#30456;&#20851;&#24615;&#20026;0.48&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;g&#22240;&#23376;&#20026;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25351;&#26631;&#65292;&#20026;&#26356;&#24378;&#22823;&#12289;&#22522;&#20110;g&#22240;&#23376;&#30340;&#27169;&#22411;&#33021;&#21147;&#35780;&#20272;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#20174;&#24515;&#29702;&#27979;&#37327;&#30340;&#35282;&#24230;&#29702;&#35299;&#21644;&#26410;&#26469;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#24182;&#23545;&#27169;&#22411;&#35780;&#20272;&#21644;&#24320;&#21457;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study uncovers the factor of general intelligence, or g, in language models, extending the psychometric theory traditionally applied to humans and certain animal species. Utilizing factor analysis on two extensive datasets Open LLM Leaderboard with 1,232 models and General Language Understanding Evaluation (GLUE) Leaderboard with 88 models - we find compelling evidence for a unidimensional, highly stable g factor that accounts for 85% of the variance in model performance. The study also finds a moderate correlation of .48 between model size and g. The discovery of g in language models offers a unified metric for model evaluation and opens new avenues for more robust, g-based model ability assessment. These findings lay the foundation for understanding and future research on artificial general intelligence from a psychometric perspective and have practical implications for model evaluation and development.
&lt;/p&gt;</description></item><item><title>UPAR&#25552;&#31034;&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#32467;&#26500;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#21644;&#21487;&#26816;&#26597;&#30340;&#25512;&#29702;&#36712;&#36857;&#65292;&#22686;&#24378;&#20102;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#20026;&#29616;&#26377;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#20379;&#20102;&#35748;&#35782;&#35770;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2310.01441</link><description>&lt;p&gt;
UPAR&#65306;&#19968;&#31181;&#21463;&#24247;&#24503;&#21551;&#21457;&#30340;&#20419;&#36827;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities. (arXiv:2310.01441v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01441
&lt;/p&gt;
&lt;p&gt;
UPAR&#25552;&#31034;&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#32467;&#26500;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#21644;&#21487;&#26816;&#26597;&#30340;&#25512;&#29702;&#36712;&#36857;&#65292;&#22686;&#24378;&#20102;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#20026;&#29616;&#26377;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#20379;&#20102;&#35748;&#35782;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#25552;&#31034;&#25552;&#21319;&#36825;&#31181;&#33021;&#21147;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#21162;&#21147;&#65292;&#32479;&#19968;&#30340;&#35748;&#35782;&#35770;&#22522;&#30784;&#20173;&#28982;&#26126;&#26174;&#32570;&#22833;&#12290;&#21463;&#24247;&#24503;&#30340;&#20808;&#39564;&#21746;&#23398;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UPAR&#25552;&#31034;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;LLMs&#20013;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#32467;&#26500;&#12290;UPAR&#26694;&#26550;&#20998;&#20026;&#22235;&#20010;&#38454;&#27573;&#65306;&#8220;&#29702;&#35299;&#8221;&#12289;&#8220;&#35745;&#21010;&#8221;&#12289;&#8220;&#34892;&#21160;&#8221;&#21644;&#8220;&#21453;&#24605;&#8221;&#65292;&#20351;&#24471;&#33021;&#22815;&#20174;&#22797;&#26434;&#32972;&#26223;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#20107;&#20808;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#65292;&#25353;&#35745;&#21010;&#25191;&#34892;&#65292;&#24182;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;&#36825;&#20010;&#32467;&#26500;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#20135;&#29983;&#20102;&#20154;&#31867;&#21487;&#29702;&#35299;&#21644;&#21487;&#26816;&#26597;&#30340;&#25512;&#29702;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#29616;&#26377;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#20379;&#20102;&#35748;&#35782;&#35770;&#22522;&#30784;&#65292;&#21487;&#33021;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#30340;&#31995;&#32479;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive inferential capabilities, with numerous research endeavors devoted to enhancing this capacity through prompting. Despite these efforts, a unified epistemological foundation is still conspicuously absent. Drawing inspiration from Kant's a priori philosophy, we propose the UPAR prompting framework, designed to emulate the structure of human cognition within LLMs. The UPAR framework is delineated into four phases: "Understand", "Plan", "Act", and "Reflect", enabling the extraction of structured information from complex contexts, prior planning of solutions, execution according to plan, and self-reflection. This structure significantly augments the explainability and accuracy of LLM inference, producing a human-understandable and inspectable inferential trajectory. Furthermore, our work offers an epistemological foundation for existing prompting techniques, allowing for a possible systematic integration of these methods. With GPT-4,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#27861;&#20064;&#24471;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#35757;&#32451;&#30340;&#19968;&#20010;&#30701;&#26242;&#31383;&#21475;&#20869;&#65292;&#27169;&#22411;&#31361;&#28982;&#33719;&#24471;&#20102;&#35821;&#27861;&#27880;&#24847;&#32467;&#26500;(SAS)&#65292;&#24182;&#20276;&#38543;&#30528;&#25439;&#22833;&#30340;&#38497;&#23789;&#19979;&#38477;&#12290;SAS&#23545;&#38543;&#21518;&#20064;&#24471;&#35821;&#35328;&#33021;&#21147;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20419;&#36827;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.07311</link><description>&lt;p&gt;
&#25439;&#22833;&#31361;&#28982;&#19979;&#38477;&#65306;&#35821;&#27861;&#20064;&#24471;&#12289;&#30456;&#21464;&#21644;MLM&#20013;&#30340;&#31616;&#21270;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs. (arXiv:2309.07311v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#27861;&#20064;&#24471;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#35757;&#32451;&#30340;&#19968;&#20010;&#30701;&#26242;&#31383;&#21475;&#20869;&#65292;&#27169;&#22411;&#31361;&#28982;&#33719;&#24471;&#20102;&#35821;&#27861;&#27880;&#24847;&#32467;&#26500;(SAS)&#65292;&#24182;&#20276;&#38543;&#30528;&#25439;&#22833;&#30340;&#38497;&#23789;&#19979;&#38477;&#12290;SAS&#23545;&#38543;&#21518;&#20064;&#24471;&#35821;&#35328;&#33021;&#21147;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20419;&#36827;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#30340;&#22823;&#22810;&#25968;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20391;&#37325;&#20110;&#29702;&#35299;&#23436;&#20840;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#21644;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#35266;&#23519;&#35757;&#32451;&#36807;&#31243;&#30340;&#36712;&#36857;&#65292;&#21487;&#33021;&#25165;&#33021;&#33719;&#24471;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#26576;&#20123;&#27934;&#23519;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;(MLMs)&#20013;&#30340;&#35821;&#27861;&#20064;&#24471;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20998;&#26512;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#28436;&#21270;&#26469;&#21152;&#28145;&#25105;&#20204;&#23545;&#26032;&#20852;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#27861;&#27880;&#24847;&#32467;&#26500;(SAS)&#65292;&#36825;&#26159;MLMs&#20013;&#33258;&#28982;&#24418;&#25104;&#30340;&#19968;&#20010;&#29305;&#24615;&#65292;&#20854;&#20013;&#29305;&#23450;&#30340;Transformer&#22836;&#20542;&#21521;&#20110;&#20851;&#27880;&#29305;&#23450;&#30340;&#21477;&#27861;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#30340;&#19968;&#20010;&#30701;&#26242;&#31383;&#21475;&#20869;&#65292;&#27169;&#22411;&#31361;&#28982;&#33719;&#24471;&#20102;SAS&#65292;&#24182;&#21457;&#29616;&#36825;&#20010;&#31383;&#21475;&#19982;&#25439;&#22833;&#30340;&#38497;&#23789;&#19979;&#38477;&#21516;&#26102;&#21457;&#29983;&#12290;&#27492;&#22806;&#65292;SAS&#20419;&#20351;&#20102;&#38543;&#21518;&#23545;&#35821;&#35328;&#33021;&#21147;&#30340;&#20064;&#24471;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#26469;&#25805;&#32437;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;SAS&#65292;&#26469;&#30740;&#31350;SAS&#30340;&#22240;&#26524;&#20316;&#29992;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. In this paper, we present a case study of syntax acquisition in masked language models (MLMs). Our findings demonstrate how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in training when models abruptly acquire SAS and find that this window is concurrent with a steep drop in loss. Moreover, SAS precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by introducing a regularizer to manipulate SAS during training, and demonstrate
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20248;&#21270;&#20219;&#21153;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#36229;&#36807;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.03409</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Optimizers. (arXiv:2309.03409v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20248;&#21270;&#20219;&#21153;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#36229;&#36807;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#12290;&#34429;&#28982;&#22522;&#20110;&#23548;&#25968;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#26159;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#26159;&#27809;&#26377;&#26799;&#24230;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#20854;&#20013;&#20248;&#21270;&#20219;&#21153;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#25551;&#36848;&#12290;&#22312;&#27599;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#20013;&#65292;LLM&#20174;&#21253;&#21547;&#20808;&#21069;&#29983;&#25104;&#30340;&#35299;&#19982;&#20854;&#20540;&#30340;&#25552;&#31034;&#20013;&#29983;&#25104;&#26032;&#30340;&#35299;&#65292;&#28982;&#21518;&#23545;&#26032;&#30340;&#35299;&#36827;&#34892;&#35780;&#20272;&#24182;&#28155;&#21152;&#21040;&#25552;&#31034;&#20013;&#65292;&#29992;&#20110;&#19979;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;OPRO&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#65292;&#28982;&#21518;&#36716;&#21521;&#25552;&#31034;&#20248;&#21270;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#33021;&#26368;&#22823;&#21270;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#25351;&#20196;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;LLM&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;OPRO&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#22312;GSM8K&#19978;&#20987;&#36133;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#39640;&#36798;8%&#65292;&#22312;Big-Bench Hard&#20219;&#21153;&#19978;&#20987;&#36133;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#20809;&#27969;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#30456;&#32467;&#21512;&#65292;&#20016;&#23500;&#20102;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#27969;&#31243;&#20013;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#22312;&#25163;&#35821;&#35782;&#21035;&#20219;&#21153;&#20013;&#38477;&#20302;&#20102;WER 0.9&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27979;&#35797;&#38598;&#19978;&#22823;&#22810;&#25968;BLEU&#20998;&#25968;&#32422;0.6&#12290;</title><link>http://arxiv.org/abs/2309.01860</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#65306;&#22686;&#24378;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation. (arXiv:2309.01860v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#20809;&#27969;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#30456;&#32467;&#21512;&#65292;&#20016;&#23500;&#20102;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#27969;&#31243;&#20013;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#22312;&#25163;&#35821;&#35782;&#21035;&#20219;&#21153;&#20013;&#38477;&#20302;&#20102;WER 0.9&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27979;&#35797;&#38598;&#19978;&#22823;&#22810;&#25968;BLEU&#20998;&#25968;&#32422;0.6&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#29992;&#20110;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#19982;&#29616;&#26377;&#30340;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#27969;&#31243;&#30456;&#32467;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#20809;&#27969;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#32467;&#21512;&#65292;&#20197;&#20016;&#23500;&#20855;&#26377;&#19982;&#36816;&#21160;&#30456;&#20851;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;&#35813;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30740;&#31350;&#20102;&#36825;&#31181;&#27169;&#24577;&#21253;&#21547;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#30340;&#25554;&#20214;&#38750;&#24120;&#36731;&#37327;&#32423;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20026;&#26032;&#27169;&#24577;&#21253;&#25324;&#19968;&#20010;&#21333;&#29420;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#25105;&#20204;&#22312;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#20013;&#24212;&#29992;&#20102;&#36825;&#20123;&#25913;&#21464;&#65292;&#25913;&#21892;&#20102;&#27599;&#20010;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;RWTH-PHOENIX-2014&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#24615;&#33021;&#65292;&#29992;&#20110;&#25163;&#35821;&#35782;&#21035;&#65292;&#24182;&#22312;RWTH-PHOENIX-2014T&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#32763;&#35793;&#20219;&#21153;&#12290;&#22312;&#35782;&#21035;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;WER&#38477;&#20302;&#20102;0.9&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22823;&#37096;&#20998;BLEU&#20998;&#25968;&#22312;&#27979;&#35797;&#38598;&#19978;&#25552;&#39640;&#20102;&#32422;0.6&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we devise a mechanism for the addition of multi-modal information with an existing pipeline for continuous sign language recognition and translation. In our procedure, we have incorporated optical flow information with RGB images to enrich the features with movement-related information. This work studies the feasibility of such modality inclusion using a cross-modal encoder. The plugin we have used is very lightweight and doesn't need to include a separate feature extractor for the new modality in an end-to-end manner. We have applied the changes in both sign language recognition and translation, improving the result in each case. We have evaluated the performance on the RWTH-PHOENIX-2014 dataset for sign language recognition and the RWTH-PHOENIX-2014T dataset for translation. On the recognition task, our approach reduced the WER by 0.9, and on the translation task, our approach increased most of the BLEU scores by ~0.6 on the test set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#32452;&#21512;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#30784;&#65292;&#24182;&#37319;&#29992;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#35780;&#20272;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09778</link><description>&lt;p&gt;
&#36861;&#27714;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22522;&#20110;&#23454;&#38469;&#30340;&#35270;&#35273;&#31354;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models. (arXiv:2308.09778v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#32452;&#21512;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#30784;&#65292;&#24182;&#37319;&#29992;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#35780;&#20272;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#21508;&#31181;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#35745;&#25968;&#12289;&#25351;&#28041;&#34920;&#36798;&#21644;&#19968;&#33324;&#30340;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#65289;&#19978;&#30340;&#34920;&#29616;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#29702;&#35299;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#65292;&#20154;&#20204;&#23581;&#35797;&#20351;&#29992;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#65288;Liu, Emerson, and Collier 2022) &#25110;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#26469;&#22788;&#29702;&#27492;&#38382;&#39064;&#65292;&#20294;&#37117;&#34920;&#29616;&#20986;&#24615;&#33021;&#19981;&#20339;&#24182;&#19988;&#19982;&#20154;&#31867;&#24615;&#33021;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#32452;&#21512;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#30784;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#26469;&#23545;&#31354;&#38388;&#20174;&#21477;&#36827;&#34892;&#25490;&#21517;&#24182;&#35780;&#20272;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#32467;&#21512;&#21644;&#22320;&#38754;&#21270;&#29289;&#20307;&#23545;&#24212;&#30340;&#21517;&#35789;&#30701;&#35821;&#21644;&#23427;&#20204;&#30340;&#20301;&#32622;&#30340;&#35777;&#25454;&#26469;&#35745;&#31639;&#31354;&#38388;&#20174;&#21477;&#30340;&#26368;&#32456;&#25490;&#21517;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#24615;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advances in large scale vision-and-language models (VLMs) it is of interest to assess their performance on various visual reasoning tasks such as counting, referring expressions and general visual question answering. The focus of this work is to study the ability of these models to understanding spatial relations. Previously, this has been tackled using image-text matching (Liu, Emerson, and Collier 2022) or visual question answering task, both showing poor performance and a large gap compared to human performance. To better understand the gap, we present fine-grained compositional grounding of spatial relationships and propose a bottom up approach for ranking spatial clauses and evaluating the performance of spatial relationship reasoning task. We propose to combine the evidence from grounding noun phrases corresponding to objects and their locations to compute the final rank of the spatial clause. We demonstrate the approach on representative vision-language models (Tan and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20687;&#32032;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#27169;&#25311;&#20154;&#31867;&#27010;&#24565;&#30028;&#38754;&#21644;&#28151;&#21512;&#21160;&#20316;&#31354;&#38388;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;GUI&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#30340;MiniWob++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20154;&#31867;&#24037;&#20316;&#32773;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.00245</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#21040;&#29992;&#25143;&#30028;&#38754;&#25805;&#20316;&#65306;&#36890;&#36807;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#23398;&#20064;&#36981;&#24490;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces. (arXiv:2306.00245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20687;&#32032;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#27169;&#25311;&#20154;&#31867;&#27010;&#24565;&#30028;&#38754;&#21644;&#28151;&#21512;&#21160;&#20316;&#31354;&#38388;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;GUI&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#30340;MiniWob++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20154;&#31867;&#24037;&#20316;&#32773;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20026;&#20102;&#26500;&#24314;&#25805;&#20316;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#30340;&#25968;&#23383;&#21270;&#20195;&#29702;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#20381;&#36182;&#22522;&#20110;&#25991;&#26412;&#30340;&#34920;&#31034;&#65288;&#20174;HTML&#25110;&#20854;&#20182;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#27966;&#29983;&#65289;&#65292;&#36825;&#20123;&#34920;&#31034;&#24182;&#19981;&#24635;&#26159;&#23481;&#26131;&#33719;&#21462;&#12290;&#36825;&#20123;&#36755;&#20837;&#34920;&#31034;&#36890;&#24120;&#19982;&#33258;&#23450;&#20041;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#31354;&#38388;&#30456;&#20851;&#32852;&#12290;&#26412;&#25991;&#26088;&#22312;&#21019;&#24314;&#20351;&#29992;&#19982;&#20154;&#31867;&#36890;&#24120;&#20351;&#29992;&#30340;&#30456;&#21516;&#27010;&#24565;&#30028;&#38754;-&#36890;&#36807;&#22522;&#20110;&#20687;&#32032;&#30340;&#23631;&#24149;&#25130;&#22270;&#21644;&#23545;&#24212;&#20110;&#38190;&#30424;&#21644;&#40736;&#26631;&#25805;&#20316;&#30340;&#36890;&#29992;&#21160;&#20316;&#31354;&#38388;&#19982;&#25968;&#23383;&#19990;&#30028;&#20132;&#20114;&#30340;&#20195;&#29702;&#12290;&#22312;&#36817;&#26399;&#20851;&#20110;&#20687;&#32032;&#32423;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#20195;&#29702;&#22312;GUI&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#30340;MiniWob ++&#22522;&#20934;&#27979;&#35797;&#20013;&#33021;&#22815;&#36229;&#36234;&#20154;&#31867;&#24037;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Much of the previous work towards digital agents for graphical user interfaces (GUIs) has relied on text-based representations (derived from HTML or other structured data sources), which are not always readily available. These input representations have been often coupled with custom, task-specific action spaces. This paper focuses on creating agents that interact with the digital world using the same conceptual interface that humans commonly use -via pixel-based screenshots and a generic action space corresponding to keyboard and mouse actions. Building upon recent progress in pixel-based pretraining, we show, for the first time, that it is possible for such agents to outperform human crowdworkers on the MiniWob++ benchmark of GUI-based instruction following tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;AMPLIFY&#65292;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#33258;&#21160;&#21270;&#29983;&#25104;&#21407;&#22240;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11426</link><description>&lt;p&gt;
&#21518;&#39564;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Post Hoc Explanations of Language Models Can Improve Language Models. (arXiv:2305.11426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;AMPLIFY&#65292;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#33258;&#21160;&#21270;&#29983;&#25104;&#21407;&#22240;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#20837;&#20154;&#31867;&#27880;&#37322;&#30340;&#21407;&#29702;&#65288;&#20363;&#22914;&#65292;&#24605;&#32500;&#38142;&#25552;&#31034;&#65289;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#25512;&#29702;&#33021;&#21147;&#30340;&#20219;&#21153;&#19978;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#21407;&#29702;&#21152;&#20837;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#39640;&#24230;&#30340;&#20154;&#24037;&#21442;&#19982;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#21363;&#36890;&#36807;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25918;&#22823;&#27169;&#22411;&#24615;&#33021;&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#36755;&#20986;&#31216;&#20026;&#23646;&#24615;&#20998;&#25968;&#65288;&#35299;&#37322;&#65289;&#30340;&#20540;&#65292;&#29992;&#20110;&#25429;&#33719;&#27599;&#20010;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#33258;&#21160;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#21407;&#29702;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;&#23646;&#24615;&#20998;&#25968;&#20013;&#33719;&#24471;&#30340;&#20449;&#24687;&#65292;&#20197;&#20415;&#29992;&#25143;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AMPLIFY&#21487;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of- Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#20132;&#20114;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#30452;&#25509;&#32534;&#36753;&#19968;&#27493;&#27493;&#35299;&#37322;&#38169;&#35823;SQL&#20197;&#20462;&#22797;SQL&#38169;&#35823;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#25552;&#39640;&#20102;31.6&#65285;&#30340;&#25191;&#34892;&#20934;&#30830;&#24615;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#24110;&#21161;&#29992;&#25143;&#20197;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#20449;&#24515;&#35299;&#20915;&#20102;&#26356;&#22810;&#30340;SQL&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.07372</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#32534;&#36753;&#30340;&#36880;&#27493;&#35299;&#37322;&#23454;&#29616;&#20132;&#20114;&#24335;&#25991;&#26412;&#36716;SQL
&lt;/p&gt;
&lt;p&gt;
Interactive Text-to-SQL Generation via Editable Step-by-Step Explanations. (arXiv:2305.07372v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#20132;&#20114;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#30452;&#25509;&#32534;&#36753;&#19968;&#27493;&#27493;&#35299;&#37322;&#38169;&#35823;SQL&#20197;&#20462;&#22797;SQL&#38169;&#35823;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#25552;&#39640;&#20102;31.6&#65285;&#30340;&#25191;&#34892;&#20934;&#30830;&#24615;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#24110;&#21161;&#29992;&#25143;&#20197;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#20449;&#24515;&#35299;&#20915;&#20102;&#26356;&#22810;&#30340;SQL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25968;&#25454;&#24211;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#38750;&#19987;&#23478;&#24456;&#38590;&#23436;&#20840;&#37322;&#25918;&#20851;&#31995;&#25968;&#25454;&#24211;&#30340;&#20998;&#26512;&#33021;&#21147;&#65292;&#22240;&#20026;&#20182;&#20204;&#19981;&#29087;&#24713;SQL&#31561;&#25968;&#25454;&#24211;&#35821;&#35328;&#12290;&#35768;&#22810;&#25216;&#26415;&#24050;&#34987;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#33258;&#21160;&#29983;&#25104;SQL&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#20197;&#19979;&#20004;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#23545;&#20110;&#22797;&#26434;&#26597;&#35810;&#23427;&#20204;&#20173;&#20250;&#29359;&#24456;&#22810;&#38169;&#35823;&#65292;&#65288;2&#65289;&#23427;&#20204;&#19981;&#25552;&#20379;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#24335;&#65292;&#35753;&#38750;&#19987;&#23478;&#29992;&#25143;&#39564;&#35777;&#21644;&#25913;&#36827;&#19981;&#27491;&#30830;&#30340;&#26597;&#35810;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#30452;&#25509;&#32534;&#36753;&#19968;&#27493;&#27493;&#35299;&#37322;&#38169;&#35823;SQL&#20197;&#20462;&#22797;SQL&#38169;&#35823;&#12290;&#22312;Spider&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33267;&#23569;&#27604;&#19977;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#25191;&#34892;&#20934;&#30830;&#24615;&#26041;&#38754;&#25552;&#39640;&#20102;31.6&#65285;&#12290;24&#21517;&#21442;&#19982;&#32773;&#30340;&#29992;&#25143;&#30740;&#31350;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24110;&#21161;&#29992;&#25143;&#20197;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#20449;&#24515;&#35299;&#20915;&#20102;&#26356;&#22810;&#30340;SQL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relational databases play an important role in this Big Data era. However, it is challenging for non-experts to fully unleash the analytical power of relational databases, since they are not familiar with database languages such as SQL. Many techniques have been proposed to automatically generate SQL from natural language, but they suffer from two issues: (1) they still make many mistakes, particularly for complex queries, and (2) they do not provide a flexible way for non-expert users to validate and refine the incorrect queries. To address these issues, we introduce a new interaction mechanism that allows users directly edit a step-by-step explanation of an incorrect SQL to fix SQL errors. Experiments on the Spider benchmark show that our approach outperforms three SOTA approaches by at least 31.6% in terms of execution accuracy. A user study with 24 participants further shows that our approach helped users solve significantly more SQL tasks with less time and higher confidence, demo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#21019;&#24314;&#30340;&#23376;&#22270;&#34920;&#31034;&#35805;&#35821;&#21450;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#26469;&#36827;&#34892;&#20250;&#35805;&#35821;&#20041;&#35299;&#26512;&#65292;&#24182;&#21033;&#29992;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#65292;&#21487;&#34920;&#31034;&#22823;&#37327;&#30475;&#19981;&#35265;&#30340;&#33410;&#28857;&#65292;&#27604;&#38745;&#24577;&#26041;&#27861;&#26356;&#20026;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2305.06164</link><description>&lt;p&gt;
&#21160;&#24577;&#19978;&#19979;&#25991;&#22270;&#24418;&#23454;&#29616;&#23545;&#19975;&#29289;&#30693;&#35782;&#22270;&#35889;&#30340;&#20250;&#35805;&#35821;&#20041;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Conversational Semantic Parsing using Dynamic Context Graphs. (arXiv:2305.06164v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#21019;&#24314;&#30340;&#23376;&#22270;&#34920;&#31034;&#35805;&#35821;&#21450;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#26469;&#36827;&#34892;&#20250;&#35805;&#35821;&#20041;&#35299;&#26512;&#65292;&#24182;&#21033;&#29992;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#65292;&#21487;&#34920;&#31034;&#22823;&#37327;&#30475;&#19981;&#35265;&#30340;&#33410;&#28857;&#65292;&#27604;&#38745;&#24577;&#26041;&#27861;&#26356;&#20026;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#25317;&#26377;&#25968;&#30334;&#19975;&#20010;&#23454;&#20307;&#21644;&#25968;&#21315;&#31181;&#20851;&#31995;&#31867;&#22411;&#30340;&#36890;&#29992;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#20250;&#35805;&#35821;&#20041;&#35299;&#26512;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#33021;&#22815;&#20132;&#20114;&#22320;&#23558;&#29992;&#25143;&#35821;&#35328;&#26144;&#23556;&#20026;&#21487;&#25191;&#34892;&#36923;&#36753;&#24418;&#24335;&#65288;&#20363;&#22914;SPARQL&#65289;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#23545;&#35805;&#21382;&#21490;&#30340;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#36890;&#36807;&#19968;&#20010;&#21160;&#24577;&#21019;&#24314;&#30340;&#23376;&#22270;&#26469;&#34920;&#31034;&#26377;&#20851;&#35805;&#35821;&#21450;&#20854;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#65292;&#21363;&#27599;&#20010;&#35805;&#35821;&#30340;&#33410;&#28857;&#25968;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#32780;&#19988;&#65292;&#25105;&#20204;&#21033;&#29992;&#23376;&#22270;&#30340;&#22522;&#26412;&#32467;&#26500;&#65292;&#32780;&#19981;&#26159;&#23558;&#20854;&#35270;&#20026;&#24207;&#21015;&#65292;&#20351;&#29992;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#20801;&#35768;&#25105;&#20204;&#34920;&#31034;&#22823;&#37327;&#65288;&#30475;&#19981;&#35265;&#30340;&#65289;&#33410;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21160;&#24577;&#24314;&#27169;&#19978;&#19979;&#25991;&#20248;&#20110;&#38745;&#24577;&#26041;&#27861;&#65292;&#21487;&#22312;&#21508;&#20010;&#26041;&#38754;&#65288;&#21363;&#31616;&#21333;&#21644;&#22797;&#26434;&#38382;&#39064;&#65289;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#35777;&#23454;&#65292;&#27169;&#22411;&#21270;&#19978;&#19979;&#25991;&#32467;&#26500;&#27604;&#20165;&#32771;&#34385;&#21333;&#20010;&#35805;&#35821;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we consider the task of conversational semantic parsing over general purpose knowledge graphs (KGs) with millions of entities, and thousands of relation-types. We are interested in developing models capable of interactively mapping user utterances into executable logical forms (e.g., SPARQL) in the context of the conversational history. Our key idea is to represent information about an utterance and its context via a subgraph which is created dynamically, i.e., the number of nodes varies per utterance. Moreover, rather than treating the subgraph as a sequence we exploit its underlying structure, and thus encode it using a graph neural network which further allows us to represent a large number of (unseen) nodes. Experimental results show that modeling context dynamically is superior to static approaches, delivering performance improvements across the board (i.e., for simple and complex questions). Our results further confirm that modeling the structure of context is bette
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#27809;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#33258;&#30001;&#24418;&#24335;&#32534;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20170;&#21518;&#21487;&#20197;&#20316;&#20026;&#38646;-shot&#26816;&#27979;&#24037;&#20855;&#36827;&#34892;&#20351;&#29992;&#65292;</title><link>http://arxiv.org/abs/2305.03514</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#25913;&#21464;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Transform Computational Social Science?. (arXiv:2305.03514v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#27809;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#33258;&#30001;&#24418;&#24335;&#32534;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20170;&#21518;&#21487;&#20197;&#20316;&#20026;&#38646;-shot&#26816;&#27979;&#24037;&#20855;&#36827;&#34892;&#20351;&#29992;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#25104;&#21151;&#22320;&#22312;&#35768;&#22810;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#36827;&#34892;&#38646;-shot&#25805;&#20316;&#65288;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#65289;&#12290;&#22914;&#26524;&#36825;&#31181;&#33021;&#21147;&#20063;&#36866;&#29992;&#20110;&#23545;&#35828;&#26381;&#21147;&#21644;&#25919;&#27835;&#24847;&#35782;&#24418;&#24577;&#31561;&#31038;&#20250;&#29616;&#35937;&#30340;&#32534;&#30721;&#65292;&#37027;&#20040;LLMs&#23601;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21464;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;(CSS)&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;CSS&#24037;&#20855;&#30340;&#36335;&#32447;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#32452;&#20248;&#31168;&#30340;&#25552;&#31034;&#23454;&#36341;&#20197;&#21450;&#19968;&#20010;&#24191;&#27867;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#20197;&#27979;&#37327;13&#31181;&#35821;&#35328;&#27169;&#22411;&#22312;24&#20010;&#20195;&#34920;&#24615;&#30340;CSS&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;LLMs&#26080;&#27861;&#36229;&#36234;&#26368;&#20339;&#24494;&#35843;&#27169;&#22411;&#65292;&#20294;&#20173;&#28982;&#19982;&#20154;&#31867;&#36798;&#25104;&#20102;&#20844;&#24179;&#30340;&#21327;&#35758;&#27700;&#24179;&#12290;&#22312;&#33258;&#30001;&#24418;&#24335;&#30340;&#32534;&#30721;&#20219;&#21153;&#65288;&#29983;&#25104;&#65289;&#19978;&#65292;LLMs&#29983;&#25104;&#30340;&#35299;&#37322;&#24120;&#24120;&#36229;&#36807;&#20102;&#24037;&#20316;&#32773;&#30340;&#40644;&#37329;&#21442;&#32771;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#20170;&#22825;&#30340;LLMs&#21487;&#20197;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#20174;&#26681;&#26412;&#19978;&#22686;&#24378;CSS&#30740;&#31350;&#27969;&#31243;&#65306;(1)&#20316;&#20026;&#38646;-shot&#26816;&#27979;&#24037;&#20855;&#36827;&#34892;&#26080;&#32541;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like ChatGPT are capable of successfully performing many language processing tasks zero-shot (without the need for training data). If this capacity also applies to the coding of social phenomena like persuasiveness and political ideology, then LLMs could effectively transform Computational Social Science (CSS). This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 24 representative CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that today's LLMs can radically augment the CSS research pipeline in two ways: (1) serving as zero-shot d
&lt;/p&gt;</description></item><item><title>ZeroNLG&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#12290;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65292;&#26725;&#25509;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.06458</link><description>&lt;p&gt;
ZeroNLG: &#23558;&#39046;&#22495;&#23545;&#40784;&#21644;&#33258;&#32534;&#30721;&#29992;&#20110;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#21644;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation. (arXiv:2303.06458v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06458
&lt;/p&gt;
&lt;p&gt;
ZeroNLG&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#12290;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65292;&#26725;&#25509;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
ZeroNLG is a zero-shot learning framework that can handle multiple NLG tasks, including image-to-text, video-to-text, and text-to-text, across English, Chinese, German, and French. It does not require any labeled downstream pairs for training, and bridges the differences between different domains by projecting them to corresponding coordinates in a shared common latent space.
&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#25509;&#21463;&#20197;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#24418;&#24335;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20316;&#20026;&#36755;&#20986;&#12290;&#29616;&#26377;&#30340;NLG&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#32806;&#21512;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#23545;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#26377;&#38024;&#23545;&#24615;&#30340;&#22330;&#26223;&#21644;&#38750;&#33521;&#35821;&#35821;&#35328;&#65292;&#24448;&#24448;&#27809;&#26377;&#36275;&#22815;&#25968;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#20102;&#25918;&#26494;&#23545;&#19979;&#28216;&#20219;&#21153;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#26377;&#25928;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;ZeroNLG&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#65288;&#22270;&#20687;&#23383;&#24149;&#65289;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#65288;&#35270;&#39057;&#23383;&#24149;&#65289;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65288;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65289;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20869;&#12290;ZeroNLG&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;ZeroNLG&#65288;i&#65289;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#65288;&#36328;&#27169;&#24577;&#21644;&#35821;&#35328;&#65289;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65307;&#65288;ii&#65289;&#26725;&#25509;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Natural Language Generation (NLG) accepts input data in the form of images, videos, or text and generates corresponding natural language text as output. Existing NLG methods mainly adopt a supervised approach and rely heavily on coupled data-to-text pairs. However, for many targeted scenarios and for non-English languages, sufficient quantities of labeled data are often not available. To relax the dependency on labeled data of downstream tasks, we propose an intuitive and effective zero-shot learning framework, ZeroNLG, which can deal with multiple NLG tasks, including image-to-text (image captioning), video-to-text (video captioning), and text-to-text (neural machine translation), across English, Chinese, German, and French within a unified framework. ZeroNLG does not require any labeled downstream pairs for training. During training, ZeroNLG (i) projects different domains (across modalities and languages) to corresponding coordinates in a shared common latent space; (ii) bridges diff
&lt;/p&gt;</description></item></channel></rss>