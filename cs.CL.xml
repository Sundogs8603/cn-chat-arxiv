<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#29983;&#25104;&#26032;&#38395;&#25991;&#31456;&#25688;&#35201;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#29305;&#26391;&#26222;&#30456;&#27604;&#65292;&#26356;&#22810;&#30340;&#32654;&#22269;&#25919;&#24220;&#38598;&#20307;&#26426;&#26500;&#65288;&#21363;&#25919;&#24220;&#65289;&#19982;&#25308;&#30331;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#30340;&#25688;&#35201;&#20559;&#35265;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.02321</link><description>&lt;p&gt;
&#33258;&#21160;&#25688;&#35201;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#29305;&#24449;&#20998;&#26512;&#65306;&#20197;&#29305;&#26391;&#26222;&#21644;&#25308;&#30331;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Characterizing Political Bias in Automatic Summaries: A Case Study of Trump and Biden. (arXiv:2305.02321v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#29983;&#25104;&#26032;&#38395;&#25991;&#31456;&#25688;&#35201;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#29305;&#26391;&#26222;&#30456;&#27604;&#65292;&#26356;&#22810;&#30340;&#32654;&#22269;&#25919;&#24220;&#38598;&#20307;&#26426;&#26500;&#65288;&#21363;&#25919;&#24220;&#65289;&#19982;&#25308;&#30331;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#30340;&#25688;&#35201;&#20559;&#35265;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;NLP&#31995;&#32479;&#21487;&#33021;&#23545;&#31038;&#20250;&#20559;&#35265;&#36827;&#34892;&#32534;&#30721;&#65307;&#28982;&#32780;&#65292;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#30340;&#25919;&#27835;&#20559;&#35265;&#20173;&#30456;&#23545;&#26410;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23454;&#20307;&#26367;&#25442;&#26041;&#27861;&#30740;&#31350;&#20102;&#26032;&#38395;&#25991;&#31456;&#33258;&#21160;&#29983;&#25104;&#25688;&#35201;&#20013;&#30340;&#25919;&#27835;&#23478;&#25551;&#32472;&#12290;&#25105;&#20204;&#22522;&#20110;&#25919;&#27835;&#23454;&#20307;&#21644;&#35789;&#27719;&#36164;&#28304;&#24320;&#21457;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#35780;&#20272;&#25277;&#21462;&#24335;&#21644;&#25277;&#35937;&#24335;&#25688;&#35201;&#27169;&#22411;&#20013;&#26377;&#20851;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#21644;&#20052;&#183;&#25308;&#30331;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#19968;&#33268;&#30340;&#24046;&#24322;&#65292;&#20363;&#22914;&#22312;&#19982;&#29305;&#26391;&#26222;&#30456;&#27604;&#65292;&#26356;&#22810;&#30340;&#32654;&#22269;&#25919;&#24220;&#38598;&#20307;&#26426;&#26500;&#65288;&#21363;&#25919;&#24220;&#65289;&#19982;&#25308;&#30331;&#30456;&#20851;&#32852;&#12290;&#24403;&#23454;&#20307;&#22312;&#28304;&#25991;&#31456;&#20013;&#37325;&#28857;&#20986;&#29616;&#26102;&#65292;&#36825;&#20123;&#25688;&#35201;&#24046;&#24322;&#26368;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21270;&#29305;&#24449;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#20010;&#26410;&#26469;&#30740;&#31350;&#25688;&#35201;&#20559;&#35265;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Growing literature has shown that powerful NLP systems may encode social biases; however, the political bias of summarization models remains relatively unknown. In this work, we use an entity replacement method to investigate the portrayal of politicians in automatically generated summaries of news articles. We develop a computational framework based on political entities and lexical resources, and use it to assess biases about Donald Trump and Joe Biden in both extractive and abstractive summarization models. We find consistent differences, such as stronger associations of a collective US government (i.e., administration) with Biden than with Trump. These summary dissimilarities are most prominent when the entity is heavily featured in the source article. Our systematic characterization provides a framework for future studies of bias in summarization.
&lt;/p&gt;</description></item><item><title>VCoT&#26159;&#19968;&#31181;&#20351;&#29992;&#24605;&#32500;&#38142;&#28608;&#21169;&#21644;&#35270;&#35273;&#35821;&#35328;&#32452;&#21512;&#36882;&#24402;&#22320;&#24357;&#21512;&#26102;&#24207;&#25968;&#25454;&#20013;&#36923;&#36753;&#24046;&#36317;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20854;&#20351;&#29992;&#35270;&#35273;&#24341;&#23548;&#29983;&#25104;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#22635;&#20805;&#20197;&#28155;&#21152;&#19968;&#33268;&#19988;&#26032;&#39062;&#30340;&#20449;&#24687;&#65292;&#24182;&#20943;&#23569;&#38656;&#35201;&#26102;&#24207;&#25512;&#29702;&#30340;&#36923;&#36753;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2305.02317</link><description>&lt;p&gt;
&#35270;&#35273;&#24605;&#32500;&#38142;&#65306;&#22810;&#27169;&#24577;&#22635;&#20805;&#25216;&#26415;&#24357;&#21512;&#36923;&#36753;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings. (arXiv:2305.02317v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02317
&lt;/p&gt;
&lt;p&gt;
VCoT&#26159;&#19968;&#31181;&#20351;&#29992;&#24605;&#32500;&#38142;&#28608;&#21169;&#21644;&#35270;&#35273;&#35821;&#35328;&#32452;&#21512;&#36882;&#24402;&#22320;&#24357;&#21512;&#26102;&#24207;&#25968;&#25454;&#20013;&#36923;&#36753;&#24046;&#36317;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20854;&#20351;&#29992;&#35270;&#35273;&#24341;&#23548;&#29983;&#25104;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#22635;&#20805;&#20197;&#28155;&#21152;&#19968;&#33268;&#19988;&#26032;&#39062;&#30340;&#20449;&#24687;&#65292;&#24182;&#20943;&#23569;&#38656;&#35201;&#26102;&#24207;&#25512;&#29702;&#30340;&#36923;&#36753;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#65292;&#33021;&#20197;&#20154;&#31867;&#26041;&#24335;&#20998;&#35299;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35813;&#33539;&#20363;&#30001;&#20110;&#20854;&#21333;&#27169;&#24577;&#24615;&#36136;&#24182;&#19988;&#20027;&#35201;&#24212;&#29992;&#20110;&#38382;&#31572;&#20219;&#21153;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#35748;&#20026;&#23558;&#35270;&#35273;&#22686;&#24378;&#20869;&#23481;&#32435;&#20837;&#25512;&#29702;&#26159;&#24517;&#35201;&#30340;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#22797;&#26434;&#24819;&#35937;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;VCoT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#24605;&#32500;&#38142;&#28608;&#21169;&#21644;&#35270;&#35273;&#35821;&#35328;&#32452;&#21512;&#26469;&#36882;&#24402;&#22320;&#24357;&#21512;&#26102;&#24207;&#25968;&#25454;&#20013;&#30340;&#36923;&#36753;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#35270;&#35273;&#24341;&#23548;&#29983;&#25104;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#22635;&#20805;&#65292;&#20197;&#28155;&#21152;&#19968;&#33268;&#19988;&#26032;&#39062;&#30340;&#20449;&#24687;&#65292;&#24182;&#20943;&#23569;&#19979;&#28216;&#20219;&#21153;&#20013;&#38656;&#35201;&#26102;&#24207;&#25512;&#29702;&#30340;&#36923;&#36753;&#24046;&#36317;&#65292;&#21516;&#26102;&#25552;&#20379;&#27169;&#22411;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23558;VCoT&#24212;&#29992;&#20110;&#35270;&#35273;&#21465;&#20107;&#21644;WikiHow&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models elicit reasoning in a chain of thought that allows models to decompose problems in a human-like fashion. Though this paradigm improves multi-step reasoning ability in language models, it is limited by being unimodal and applied mainly to question-answering tasks. We claim that incorporating visual augmentation into reasoning is essential, especially for complex, imaginative tasks. Consequently, we introduce VCoT, a novel method that leverages chain of thought prompting with vision-language grounding to recursively bridge the logical gaps within sequential data. Our method uses visual guidance to generate synthetic multimodal infillings that add consistent and novel information to reduce the logical gaps for downstream tasks that can benefit from temporal reasoning, as well as provide interpretability into models' multi-step reasoning. We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Distilling Step-by-Step&#26426;&#21046;&#65292;&#36890;&#36807;&#25552;&#21462;LLM&#22522;&#30784;&#20449;&#24687;&#20026;&#23567;&#22411;&#27169;&#22411;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#32988;&#36807;&#26356;&#22823;&#30340;LLM&#27169;&#22411;&#65292;&#24182;&#38656;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.02301</link><description>&lt;p&gt;
Distilling Step-by-Step&#65281;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26356;&#23567;&#30340;&#27169;&#22411;&#23610;&#23544;&#32988;&#36807;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. (arXiv:2305.02301v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Distilling Step-by-Step&#26426;&#21046;&#65292;&#36890;&#36807;&#25552;&#21462;LLM&#22522;&#30784;&#20449;&#24687;&#20026;&#23567;&#22411;&#27169;&#22411;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#32988;&#36807;&#26356;&#22823;&#30340;LLM&#27169;&#22411;&#65292;&#24182;&#38656;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38754;&#20020;&#20869;&#23384;&#25928;&#29575;&#20302;&#21644;&#35745;&#31639;&#23494;&#38598;&#24230;&#39640;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#24494;&#35843;&#25110;&#31934;&#28860;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#26631;&#31614;&#26469;&#35757;&#32451;&#36739;&#23567;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#35201;&#24819;&#36798;&#21040;LLM&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Distilling Step-by-Step&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292; (a)&#35757;&#32451;&#36739;&#23567;&#30340;&#27169;&#22411;&#27604;LLM&#34920;&#29616;&#26356;&#22909;&#65292;(b)&#24182;&#36890;&#36807;&#21033;&#29992;&#24494;&#35843;&#25110;&#31934;&#28860;&#25152;&#38656;&#30340;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#35757;&#32451;&#26694;&#26550;&#20013;&#25552;&#21462;LLM&#22522;&#30784;&#65292;&#24182;&#20316;&#20026;&#39069;&#22806;&#30340;&#30417;&#30563;&#26469;&#35757;&#32451;&#23567;&#22411;&#27169;&#22411;&#12290;&#22312;&#22235;&#20010;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#21457;&#29616;&#65306;&#31532;&#19968;&#65292;&#19982;&#24494;&#35843;&#21644;&#31934;&#28860;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26426;&#21046;&#20351;&#29992;&#36739;&#23569;&#30340;&#26631;&#35760;/&#26410;&#26631;&#35760;&#35757;&#32451;&#31034;&#20363;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#31532;&#20108;&#65292;&#19982;LLM&#30456;&#27604;&#65292;&#21363;&#20351;&#20351;&#29992;&#26356;&#23567;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20063;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for small models within a multi-task training framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to LLMs, we achieve better performance using substantially smaller m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#38271;&#24230;&#21487;&#25511;&#26426;&#22120;&#32763;&#35793;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#21457;&#29616;BLEURT&#21644;COMET&#26159;&#26368;&#36866;&#21512;&#20316;&#20026;&#20854;&#35780;&#20272;&#25351;&#26631;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.02300</link><description>&lt;p&gt;
&#35780;&#20272;&#38271;&#24230;&#21487;&#25511;&#26426;&#22120;&#32763;&#35793;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Efficacy of Length-Controllable Machine Translation. (arXiv:2305.02300v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#38271;&#24230;&#21487;&#25511;&#26426;&#22120;&#32763;&#35793;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#21457;&#29616;BLEURT&#21644;COMET&#26159;&#26368;&#36866;&#21512;&#20316;&#20026;&#20854;&#35780;&#20272;&#25351;&#26631;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24230;&#21487;&#25511;&#26426;&#22120;&#32763;&#35793;&#26159;&#19968;&#31181;&#32422;&#26463;&#32763;&#35793;&#65292;&#26088;&#22312;&#22312;&#25511;&#21046;&#32763;&#35793;&#38271;&#24230;&#30340;&#21516;&#26102;&#23613;&#21487;&#33021;&#20445;&#30041;&#21407;&#22987;&#21547;&#20041;&#12290;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#21160;&#25688;&#35201;&#25110;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#38271;&#24230;&#21487;&#25511;&#26426;&#22120;&#32763;&#35793;&#65292;&#20294;&#36825;&#24182;&#19981;&#19968;&#23450;&#36866;&#29992;&#21644;&#20934;&#30830;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#27425;&#31995;&#32479;&#35780;&#20272;&#38271;&#24230;&#21487;&#25511;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#32763;&#35793;&#26041;&#21521;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#24182;&#35780;&#20272;&#20102;18&#20010;&#25688;&#35201;&#25110;&#32763;&#35793;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;BLEURT&#21644;COMET&#19982;&#20154;&#24037;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;&#26368;&#39640;&#65292;&#26368;&#36866;&#21512;&#20316;&#20026;&#38271;&#24230;&#21487;&#25511;&#26426;&#22120;&#32763;&#35793;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length-controllable machine translation is a type of constrained translation. It aims to contain the original meaning as much as possible while controlling the length of the translation. We can use automatic summarization or machine translation evaluation metrics for length-controllable machine translation, but this is not necessarily suitable and accurate. This work is the first attempt to evaluate the automatic metrics for length-controllable machine translation tasks systematically. We conduct a rigorous human evaluation on two translation directions and evaluate 18 summarization or translation evaluation metrics. We find that BLEURT and COMET have the highest correlation with human evaluation and are most suitable as evaluation metrics for length-controllable machine translation.
&lt;/p&gt;</description></item><item><title>M2-CTTS&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#22810;&#23610;&#24230;&#22810;&#27169;&#24577;&#30340;&#20250;&#35805;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#31995;&#32479;&#65292;&#23427;&#37319;&#29992;&#20102;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#24314;&#27169;&#26469;&#20840;&#38754;&#21033;&#29992;&#21382;&#21490;&#23545;&#35805;&#65292;&#21516;&#26102;&#36824;&#32771;&#34385;&#20102;&#22768;&#23398;&#29305;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20250;&#35805;&#24335;TTS&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02269</link><description>&lt;p&gt;
M2-CTTS&#65306;&#31471;&#21040;&#31471;&#22810;&#23610;&#24230;&#22810;&#27169;&#24577;&#20250;&#35805;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
M2-CTTS: End-to-End Multi-scale Multi-modal Conversational Text-to-Speech Synthesis. (arXiv:2305.02269v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02269
&lt;/p&gt;
&lt;p&gt;
M2-CTTS&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#22810;&#23610;&#24230;&#22810;&#27169;&#24577;&#30340;&#20250;&#35805;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#31995;&#32479;&#65292;&#23427;&#37319;&#29992;&#20102;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#24314;&#27169;&#26469;&#20840;&#38754;&#21033;&#29992;&#21382;&#21490;&#23545;&#35805;&#65292;&#21516;&#26102;&#36824;&#32771;&#34385;&#20102;&#22768;&#23398;&#29305;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20250;&#35805;&#24335;TTS&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65288;TTS&#65289;&#26088;&#22312;&#26681;&#25454;&#21382;&#21490;&#23545;&#35805;&#21512;&#25104;&#20855;&#26377;&#36866;&#24403;&#35821;&#35843;&#30340;&#22238;&#22797;&#35821;&#38899;&#12290;&#28982;&#32780;&#65292;&#20840;&#38754;&#24314;&#27169;&#23545;&#35805;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22823;&#22810;&#25968;&#20250;&#35805;&#24335;TTS&#31995;&#32479;&#21482;&#20851;&#27880;&#25552;&#21462;&#20840;&#23616;&#20449;&#24687;&#24182;&#30465;&#30053;&#26412;&#22320;&#35821;&#35843;&#29305;&#24449;&#65292;&#32780;&#21518;&#32773;&#21253;&#21547;&#20851;&#38190;&#35789;&#21644;&#24378;&#35843;&#31561;&#37325;&#35201;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20165;&#32771;&#34385;&#25991;&#26412;&#29305;&#24449;&#26159;&#19981;&#36275;&#30340;&#65292;&#22768;&#23398;&#29305;&#24449;&#20063;&#21253;&#21547;&#21508;&#31181;&#35821;&#35843;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M2-CTTS&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#22810;&#23610;&#24230;&#22810;&#27169;&#24577;&#30340;&#20250;&#35805;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#65292;&#26088;&#22312;&#20840;&#38754;&#21033;&#29992;&#21382;&#21490;&#23545;&#35805;&#24182;&#22686;&#24378;&#35821;&#35843;&#34920;&#36798;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25991;&#26412;&#19978;&#19979;&#25991;&#27169;&#22359;&#21644;&#19968;&#20010;&#22768;&#23398;&#19978;&#19979;&#25991;&#27169;&#22359;&#65292;&#20108;&#32773;&#37117;&#36827;&#34892;&#20102;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#24314;&#27169;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32467;&#21512;&#32454;&#31890;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#39069;&#22806;&#32771;&#34385;&#22768;&#23398;&#29305;&#24449;&#21487;&#20197;&#26377;&#25928;&#22686;&#24378;&#20250;&#35805;&#24335;TTS&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational text-to-speech (TTS) aims to synthesize speech with proper prosody of reply based on the historical conversation. However, it is still a challenge to comprehensively model the conversation, and a majority of conversational TTS systems only focus on extracting global information and omit local prosody features, which contain important fine-grained information like keywords and emphasis. Moreover, it is insufficient to only consider the textual features, and acoustic features also contain various prosody information. Hence, we propose M2-CTTS, an end-to-end multi-scale multi-modal conversational text-to-speech system, aiming to comprehensively utilize historical conversation and enhance prosodic expression. More specifically, we design a textual context module and an acoustic context module with both coarse-grained and fine-grained modeling. Experimental results demonstrate that our model mixed with fine-grained context information and additionally considering acoustic fea
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#20998;&#27835;&#25512;&#29702;&#26694;&#26550;NDCR&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#22797;&#26434;&#25991;&#26412;&#35270;&#20026;&#30001;&#22810;&#20010;&#31616;&#21333;&#21629;&#39064;&#21477;&#32452;&#25104;&#30340;&#22797;&#21512;&#21629;&#39064;&#25991;&#26412;&#65292;&#23558;&#22270;&#20687;&#26816;&#32034;&#38382;&#39064;&#20998;&#20026;&#19977;&#20010;&#27493;&#39588;&#65306;&#20998;&#27835;&#12289;&#24449;&#26381;&#21644;&#32452;&#21512;&#12290;&#35813;&#26694;&#26550;&#22312;&#35299;&#20915;&#35821;&#35328;&#22797;&#26434;&#25991;&#26412;&#38382;&#39064;&#26102;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02265</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#35821;&#35328;&#22797;&#26434;&#30340;&#22270;&#20687;&#26816;&#32034;&#30340;&#31070;&#32463;&#20998;&#27835;&#25512;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text. (arXiv:2305.02265v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02265
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#20998;&#27835;&#25512;&#29702;&#26694;&#26550;NDCR&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#22797;&#26434;&#25991;&#26412;&#35270;&#20026;&#30001;&#22810;&#20010;&#31616;&#21333;&#21629;&#39064;&#21477;&#32452;&#25104;&#30340;&#22797;&#21512;&#21629;&#39064;&#25991;&#26412;&#65292;&#23558;&#22270;&#20687;&#26816;&#32034;&#38382;&#39064;&#20998;&#20026;&#19977;&#20010;&#27493;&#39588;&#65306;&#20998;&#27835;&#12289;&#24449;&#26381;&#21644;&#32452;&#21512;&#12290;&#35813;&#26694;&#26550;&#22312;&#35299;&#20915;&#35821;&#35328;&#22797;&#26434;&#25991;&#26412;&#38382;&#39064;&#26102;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#22270;&#20687;&#26816;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#24403;&#38754;&#23545;&#38590;&#20197;&#29702;&#35299;&#30340;&#35821;&#35328;&#22797;&#26434;&#25991;&#26412;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#24613;&#21095;&#19979;&#38477;&#12290;&#26412;&#25991;&#21463;&#21040;&#20998;&#27835;&#31639;&#27861;&#21644;&#21452;&#36807;&#31243;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#23558;&#35821;&#35328;&#22797;&#26434;&#25991;&#26412;&#35270;&#20026;&#30001;&#22810;&#20010;&#31616;&#21333;&#21629;&#39064;&#21477;&#32452;&#25104;&#30340;&#22797;&#21512;&#21629;&#39064;&#25991;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#20998;&#27835;&#25512;&#29702;&#26694;&#26550;NDCR&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;1&#65289;&#20998;&#27835;&#65306;&#21629;&#39064;&#29983;&#25104;&#22120;&#23558;&#22797;&#21512;&#21629;&#39064;&#25991;&#26412;&#20998;&#20026;&#31616;&#21333;&#21629;&#39064;&#21477;&#65292;&#24182;&#29983;&#25104;&#23427;&#20204;&#30340;&#23545;&#24212;&#34920;&#31034;&#65292;2&#65289;&#24449;&#26381;&#65306;&#22522;&#20110;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#20132;&#20114;&#22120;&#23454;&#29616;&#20998;&#35299;&#21629;&#39064;&#21477;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;3&#65289;&#32452;&#21512;&#65306;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#22120;&#37319;&#29992;&#31070;&#32463;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#23558;&#19978;&#36848;&#25512;&#29702;&#29366;&#24577;&#32452;&#21512;&#65292;&#33719;&#24471;&#26368;&#32456;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Vision-Language Models (VLMs) have achieved remarkable performance in image retrieval from text. However, their performance drops drastically when confronted with linguistically complex texts that they struggle to comprehend. Inspired by the Divide-and-Conquer algorithm and dual-process theory, in this paper, we regard linguistically complex texts as compound proposition texts composed of multiple simple proposition sentences and propose an end-to-end Neural Divide-and-Conquer Reasoning framework, dubbed NDCR. It contains three main components: 1)Divide: a proposition generator divides the compound proposition text into simple proposition sentences and produces their corresponding representations, 2)Conquer: a pretrained VLMs-based visual-linguistic interactor achieves the interaction between decomposed proposition sentences and images, 3)Combine: a neural-symbolic reasoner combines the above reasoning states to obtain the final solution via a neural logic reasoning approach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#37197;&#32622;&#20102;&#25913;&#36827;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#21363;&#22522;&#20110;&#20013;&#36716;&#30340;&#32423;&#32852;&#32763;&#35793;&#27169;&#22411;&#65292;&#20351;&#29992;&#21152;&#26435;&#20013;&#36716;&#35821;&#35328;&#23884;&#20837;&#36755;&#20837;&#27169;&#22411;&#65292;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#32531;&#35299;&#26631;&#35760;&#21644;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.02261</link><description>&lt;p&gt;
&#22522;&#20110;&#20013;&#36716;&#30340;&#32423;&#32852;&#32763;&#35793;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#19982;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-end Training and Decoding for Pivot-based Cascaded Translation Model. (arXiv:2305.02261v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#37197;&#32622;&#20102;&#25913;&#36827;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#21363;&#22522;&#20110;&#20013;&#36716;&#30340;&#32423;&#32852;&#32763;&#35793;&#27169;&#22411;&#65292;&#20351;&#29992;&#21152;&#26435;&#20013;&#36716;&#35821;&#35328;&#23884;&#20837;&#36755;&#20837;&#27169;&#22411;&#65292;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#32531;&#35299;&#26631;&#35760;&#21644;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20013;&#36716;&#35821;&#35328;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#25928;&#26524;&#12290;&#36890;&#24120;&#65292;&#28304;&#35821;&#35328;&#21040;&#20013;&#36716;&#35821;&#35328;&#27169;&#22411;&#21644;&#20013;&#36716;&#35821;&#35328;&#21040;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#20998;&#21035;&#35757;&#32451;&#65292;&#27809;&#26377;&#21033;&#29992;&#26377;&#38480;&#30340;&#65288;&#28304;&#35821;&#35328;&#65292;&#30446;&#26631;&#35821;&#35328;&#65289;&#24182;&#34892;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32423;&#32852;&#32763;&#35793;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#37197;&#32622;&#20102;&#25913;&#36827;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#20013;&#36716;&#35821;&#35328;&#21040;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#26681;&#25454;&#28304;&#35821;&#35328;&#21040;&#20013;&#36716;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#20998;&#24067;&#20462;&#25913;&#20026;&#21152;&#26435;&#20013;&#36716;&#35821;&#35328;&#23884;&#20837;&#65292;&#20174;&#32780;&#21487;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#20013;&#36716;&#35821;&#35328;&#35299;&#30721;&#20013;&#30340;&#27874;&#26463;&#25628;&#32034;&#26102;&#65292;&#25105;&#20204;&#32531;&#35299;&#20102;&#26631;&#35760;&#21644;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utilizing pivot language effectively can significantly improve low-resource machine translation. Usually, the two translation models, source-pivot and pivot-target, are trained individually and do not utilize the limited (source, target) parallel data. This work proposes an end-to-end training method for the cascaded translation model and configures an improved decoding algorithm. The input of the pivot-target model is modified to weighted pivot embedding based on the probability distribution output by the source-pivot model. This allows the model to be trained end-to-end. In addition, we mitigate the inconsistency between tokens and probability distributions while using beam search in pivot decoding. Experiments demonstrate that our method enhances the quality of translation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26631;&#27880;&#25551;&#36848;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#20013;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#26356;&#40065;&#26834;&#22320;&#22788;&#29702;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.02239</link><description>&lt;p&gt;
&#26631;&#27880;&#25551;&#36848;&#35757;&#32451;&#22312;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
The Benefits of Label-Description Training for Zero-Shot Text Classification. (arXiv:2305.02239v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26631;&#27880;&#25551;&#36848;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#20013;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#26356;&#40065;&#26834;&#22320;&#22788;&#29702;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20801;&#35768;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#36716;&#31227;&#35821;&#20041;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#38646;&#26679;&#26412;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#23567;&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25551;&#36848;&#20219;&#21153;&#26631;&#31614;&#12290;&#19982;&#36890;&#24120;&#26377;&#25991;&#26412;&#26631;&#27880;&#26631;&#31614;&#30340;&#24494;&#35843;&#25968;&#25454;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#21482;&#26159;&#29992;&#35821;&#35328;&#25551;&#36848;&#26631;&#31614;&#65292;&#20363;&#22914;&#20351;&#29992;&#19968;&#20123;&#30456;&#20851;&#26415;&#35821;&#12289;&#35789;&#20856;/&#30334;&#31185;&#20840;&#20070;&#26465;&#30446;&#21644;&#30701;&#27169;&#26495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20027;&#39064;&#21644;&#24773;&#24863;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#27604;&#38646;&#26679;&#26412;&#39640;15-17&#65285;&#32477;&#23545;&#20540;&#12290;&#23427;&#36824;&#26356;&#20855;&#26377;&#38646;&#26679;&#26412;&#20998;&#31867;&#25152;&#38656;&#36873;&#25321;&#30340;&#40065;&#26834;&#24615;&#65292;&#20363;&#22914;&#25552;&#31034;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#30340;&#27169;&#24335;&#20197;&#21450;&#20174;&#26631;&#31614;&#26144;&#23556;&#21040;&#27169;&#22411;&#35789;&#27719;&#34920;&#20013;&#30340;&#20196;&#29260;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25105;&#20204;&#30340;&#25968;&#25454;&#20165;&#25551;&#36848;&#26631;&#31614;&#20294;&#19981;&#20351;&#29992;&#36755;&#20837;&#25991;&#26412;&#65292;&#22240;&#27492;&#22312;&#20854;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#20998;&#31867;&#30340;&#37325;&#28857;&#26356;&#19987;&#27880;&#20110;&#26631;&#31614;&#32780;&#19981;&#26159;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have improved zero-shot text classification by allowing the transfer of semantic knowledge from the training data in order to classify among specific label sets in downstream tasks. We propose a simple way to further improve zero-shot accuracies with minimal effort. We curate small finetuning datasets intended to describe the labels for a task. Unlike typical finetuning data, which has texts annotated with labels, our data simply describes the labels in language, e.g., using a few related terms, dictionary/encyclopedia entries, and short templates. Across a range of topic and sentiment datasets, our method is more accurate than zero-shot by 15-17% absolute. It is also more robust to choices required for zero-shot classification, such as patterns for prompting the model to classify and mappings from labels to tokens in the model's vocabulary. Furthermore, since our data merely describes the labels but does not use input texts, finetuning on it yields a model that p
&lt;/p&gt;</description></item><item><title>AttenWalker&#26159;&#19968;&#31181;&#22522;&#20110;Attention&#30340;&#22270;&#36941;&#21382;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#26080;&#30417;&#30563;&#38271;&#25991;&#26723;&#38382;&#31572;&#12290;&#23427;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;span collector&#12289;span linker&#21644;answer aggregator&#12290;&#36825;&#20123;&#27169;&#22359;&#32467;&#21512;&#20351;&#29992;&#26469;&#36873;&#25321;&#26377;&#20449;&#24687;&#37327;&#30340;&#20505;&#36873;span&#65292;&#23558;&#36825;&#20123;span&#20018;&#32852;&#36215;&#26469;&#24418;&#25104;&#23436;&#25972;&#31572;&#26696;&#65292;&#24182;&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#12290;&#22312;CoQA&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02235</link><description>&lt;p&gt;
AttenWalker: &#20351;&#29992;&#22522;&#20110;Attention&#30340;&#22270;&#36941;&#21382;&#23454;&#29616;&#26080;&#30417;&#30563;&#38271;&#25991;&#26723;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
AttenWalker: Unsupervised Long-Document Question Answering via Attention-based Graph Walking. (arXiv:2305.02235v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02235
&lt;/p&gt;
&lt;p&gt;
AttenWalker&#26159;&#19968;&#31181;&#22522;&#20110;Attention&#30340;&#22270;&#36941;&#21382;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#26080;&#30417;&#30563;&#38271;&#25991;&#26723;&#38382;&#31572;&#12290;&#23427;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;span collector&#12289;span linker&#21644;answer aggregator&#12290;&#36825;&#20123;&#27169;&#22359;&#32467;&#21512;&#20351;&#29992;&#26469;&#36873;&#25321;&#26377;&#20449;&#24687;&#37327;&#30340;&#20505;&#36873;span&#65292;&#23558;&#36825;&#20123;span&#20018;&#32852;&#36215;&#26469;&#24418;&#25104;&#23436;&#25972;&#31572;&#26696;&#65292;&#24182;&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#12290;&#22312;CoQA&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#27880;&#38271;&#25991;&#26723;&#38382;&#31572;&#26159;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#33021;&#21487;&#20197;&#36890;&#36807;&#26080;&#30417;&#30563;&#38382;&#31572;&#26041;&#27861;&#29983;&#25104;&#38271;&#25991;&#26723;&#38382;&#31572;&#23545;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;UQA&#20219;&#21153;&#22522;&#20110;&#30701;&#25991;&#26723;&#65292;&#38590;&#20197;&#32435;&#20837;&#38271;&#36317;&#31163;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21517;&#20026;&#26080;&#30417;&#30563;&#38271;&#25991;&#26723;&#38382;&#31572;&#65288;ULQA&#65289;&#65292;&#26088;&#22312;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#38271;&#25991;&#26723;QA&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AttenWalker&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#32858;&#21512;&#21644;&#29983;&#25104;&#20855;&#26377;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#31572;&#26696;&#65292;&#20197;&#26500;&#24314;&#38271;&#25991;&#26723;QA&#23545;&#12290;&#22312;CoQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26080;&#30417;&#30563;&#38271;&#25991;&#26723;QA&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotating long-document question answering (long-document QA) pairs is time-consuming and expensive. To alleviate the problem, it might be possible to generate long-document QA pairs via unsupervised question answering (UQA) methods. However, existing UQA tasks are based on short documents, and can hardly incorporate long-range information. To tackle the problem, we propose a new task, named unsupervised long-document question answering (ULQA), aiming to generate high-quality long-document QA instances in an unsupervised manner. Besides, we propose AttenWalker, a novel unsupervised method to aggregate and generate answers with long-range dependency so as to construct long-document QA pairs. Specifically, AttenWalker is composed of three modules, i.e., span collector, span linker and answer aggregator. Firstly, the span collector takes advantage of constituent parsing and reconstruction loss to select informative candidate spans for constructing answers. Secondly, by going through the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#33258;&#21160;&#29983;&#25104;&#20020;&#24202;&#31508;&#35760;&#30340;&#30740;&#31350;&#65292;&#37319;&#29992;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#27861;&#25152;&#29983;&#25104;&#31508;&#35760;&#34920;&#29616;&#20248;&#31168;&#65292;&#19988;&#21487;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#31508;&#35760;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2305.02220</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#29983;&#25104;&#20020;&#24202;&#31508;&#35760;&#65306;&#26469;&#33258;MEDIQA-Chat&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Clinical Note Generation from Doctor-Patient Conversations using Large Language Models: Insights from MEDIQA-Chat. (arXiv:2305.02220v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#33258;&#21160;&#29983;&#25104;&#20020;&#24202;&#31508;&#35760;&#30340;&#30740;&#31350;&#65292;&#37319;&#29992;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#27861;&#25152;&#29983;&#25104;&#31508;&#35760;&#34920;&#29616;&#20248;&#31168;&#65292;&#19988;&#21487;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#31508;&#35760;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;MEDIQA-Chat 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#25552;&#20132;&#30340;&#33258;&#21160;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#26041;&#26696;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#32467;&#26524;&#65306;&#31532;&#19968;&#31181;&#26159;&#22312;&#20849;&#20139;&#20219;&#21153;&#25968;&#25454;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#31532;&#20108;&#31181;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#12290;&#20004;&#31181;&#26041;&#27861;&#37117;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#65292;&#22914;&#36890;&#36807;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65288;&#20363;&#22914;ROUGE&#65292;BERTScore&#65289;&#27979;&#37327;&#65292;&#24182;&#20998;&#21035;&#22312;&#25152;&#26377;&#25552;&#20132;&#30340;&#26041;&#26696;&#20013;&#25490;&#21517;&#31532;&#20108;&#21644;&#31532;&#19968;&#12290;&#19987;&#23478;&#23457;&#26680;&#34920;&#26126;&#65292;&#36890;&#36807;&#22522;&#20110;ICL&#30340;&#26041;&#27861;&#20351;&#29992;GPT-4&#29983;&#25104;&#30340;&#31508;&#35760;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#31508;&#35760;&#19968;&#26679;&#21463;&#27426;&#36814;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#33258;&#21160;&#29983;&#25104;&#31508;&#35760;&#30340;&#26377;&#21069;&#36884;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission to the MEDIQA-Chat 2023 shared task for automatic clinical note generation from doctor-patient conversations. We report results for two approaches: the first fine-tunes a pre-trained language model (PLM) on the shared task data, and the second uses few-shot in-context learning (ICL) with a large language model (LLM). Both achieve high performance as measured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and first, respectively, of all submissions to the shared task. Expert human scrutiny indicates that notes generated via the ICL-based approach with GPT-4 are preferred about as often as human-written notes, making it a promising path toward automated note generation from doctor-patient conversations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35821;&#35328;&#20998;&#31867;&#26041;&#27861;&#25506;&#31350;&#21333;&#35821;BERT&#30340;&#35821;&#35328;&#23646;&#24615;&#65292;&#26680;&#24515;&#21457;&#29616;&#20026;BERT&#27491;&#22312;&#22797;&#21046;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.02215</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#20998;&#31867;&#25506;&#31350;&#21333;&#35821;BERT&#30340;&#35821;&#35328;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages. (arXiv:2305.02215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35821;&#35328;&#20998;&#31867;&#26041;&#27861;&#25506;&#31350;&#21333;&#35821;BERT&#30340;&#35821;&#35328;&#23646;&#24615;&#65292;&#26680;&#24515;&#21457;&#29616;&#20026;BERT&#27491;&#22312;&#22797;&#21046;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#30340;&#24040;&#22823;&#25104;&#21151;&#20351;&#20154;&#20204;&#20135;&#29983;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#20123;&#26426;&#22120;&#26159;&#22312;&#22797;&#21046;&#26576;&#20123;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36824;&#26159;&#21457;&#29616;&#20102;&#26681;&#26412;&#24615;&#30340;&#26032;&#29702;&#35770;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35266;&#28857;&#65292;&#20351;&#29992;&#35821;&#35328;&#20043;&#38388;&#30340;&#31867;&#22411;&#30456;&#20284;&#24615;&#26469;&#23545;&#27604;&#19981;&#21516;&#35821;&#35328;&#30340;transformer&#27169;&#22411;&#65292;&#35266;&#23519;&#36825;&#20123;&#30456;&#20284;&#24615;&#26159;&#21542;&#20986;&#29616;&#22312;&#29305;&#23450;&#30340;&#23618;&#27425;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20013;&#24515;&#26680;&#23545;&#40784;&#30340;&#26435;&#37325;&#30697;&#38453;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21477;&#27861;&#31867;&#22411;&#23398;&#30456;&#20284;&#24615;&#19982;&#20013;&#38388;&#23618;&#26435;&#37325;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26159;&#19968;&#33268;&#30340;&#12290;&#36825;&#19968;&#21457;&#29616;&#30830;&#35748;&#20102;&#36890;&#36807;&#21477;&#27861;&#25506;&#38024;&#26041;&#27861;&#33719;&#24471;&#30340;BERT&#30340;&#32467;&#26524;&#65292;&#24182;&#22240;&#27492;&#37325;&#35201;&#22320;&#35777;&#26126;&#20102;BERT&#27491;&#22312;&#22797;&#21046;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The overwhelming success of transformers is a real conundrum stimulating a compelling question: are these machines replicating some traditional linguistic models or discovering radically new theories? In this paper, we propose a novel standpoint to investigate this important question. Using typological similarities among languages, we aim to layer-wise compare transformers for different languages to observe whether these similarities emerge for particular layers. For this investigation, we propose to use Centered kernel alignment to measure similarity among weight matrices. We discovered that syntactic typological similarity is consistent with the similarity among weights in the middle layers. This finding confirms results obtained by syntactically probing BERT and, thus, gives an important confirmation that BERT is replicating traditional linguistic models.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#31232;&#30095;&#28608;&#27963;Transformer&#27169;&#22411;&#65292;&#21487;&#20197;&#21160;&#24577;&#20998;&#37197;&#19981;&#21516;&#20196;&#29260;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;Mixture-of-experts&#27169;&#22411;&#21442;&#25968;&#20302;&#25928;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02176</link><description>&lt;p&gt;
&#21521;&#21442;&#25968;&#25928;&#29575;&#36808;&#36827;&#65306;&#20855;&#26377;&#21160;&#24577;&#33021;&#21147;&#30340;&#20998;&#23618;&#31232;&#30095;&#28608;&#27963;Transformer
&lt;/p&gt;
&lt;p&gt;
Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity. (arXiv:2305.02176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#31232;&#30095;&#28608;&#27963;Transformer&#27169;&#22411;&#65292;&#21487;&#20197;&#21160;&#24577;&#20998;&#37197;&#19981;&#21516;&#20196;&#29260;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;Mixture-of-experts&#27169;&#22411;&#21442;&#25968;&#20302;&#25928;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#31232;&#30095;&#28608;&#27963;&#30340;Mixture-of-experts&#65288;MoE&#65289;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#22312;&#20445;&#25345;&#20302;&#27599;&#20010;&#20196;&#29260;&#35745;&#31639;&#35201;&#27714;&#30340;&#21516;&#26102;&#26174;&#30528;&#22686;&#21152;&#21442;&#25968;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;MoE&#27169;&#22411;&#26412;&#36136;&#19978;&#26159;&#21442;&#25968;&#20302;&#25928;&#30340;&#65292;&#22240;&#20026;&#38543;&#30528;&#19987;&#23478;&#25968;&#37327;&#22686;&#21152;&#65292;&#24615;&#33021;&#30340;&#25552;&#39640;&#20250;&#21464;&#23567;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#31181;&#21442;&#25968;&#20302;&#25928;&#26159;&#25152;&#26377;&#19987;&#23478;&#20855;&#26377;&#30456;&#21516;&#33021;&#21147;&#30340;&#32467;&#26524;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#28385;&#36275;&#19981;&#21516;&#20196;&#29260;&#25110;&#20219;&#21153;&#30340;&#19981;&#21516;&#22797;&#26434;&#24230;&#35201;&#27714;&#65292;&#20363;&#22914;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#65292;&#22522;&#20110;&#20854;&#36164;&#28304;&#27700;&#24179;&#30340;&#35821;&#35328;&#21487;&#33021;&#38656;&#35201;&#19981;&#21516;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#20998;&#23618;&#32467;&#26500;&#24182;&#21487;&#20197;&#20026;&#19981;&#21516;&#20196;&#29260;&#20998;&#37197;&#21160;&#24577;&#33021;&#21147;&#30340;Stratified Mixture of Experts&#65288;SMoE&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;SMoE&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;MoE&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-experts (MoE) models that employ sparse activation have demonstrated effectiveness in significantly increasing the number of parameters while maintaining low computational requirements per token. However, recent studies have established that MoE models are inherently parameter-inefficient as the improvement in performance diminishes with an increasing number of experts. We hypothesize this parameter inefficiency is a result of all experts having equal capacity, which may not adequately meet the varying complexity requirements of different tokens or tasks, e.g., in a multilingual setting, languages based on their resource levels might require different capacities. In light of this, we propose Stratified Mixture of Experts(SMoE) models, which feature a stratified structure and can assign dynamic capacity to different tokens. We demonstrate the effectiveness of SMoE on two multilingual machine translation benchmarks, where it outperforms multiple state-of-the-art MoE models. On
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#39564;&#35777;&#25991;&#26412;&#20998;&#32452;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#25991;&#26412;&#25506;&#32034;&#30340;&#27969;&#31243;&#65292;&#24182;&#22312;&#22307;&#32463;&#30340;&#21069;&#20004;&#21367;&#20070;&#20013;&#24212;&#29992;&#27492;&#27969;&#31243;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#24182;&#25506;&#32034;&#20102;&#21496;&#31085;&#27966;&#21035;&#21644;&#38750;&#21496;&#31085;&#27966;&#21035;&#20043;&#38388;&#30340;&#32479;&#35745;&#26126;&#26174;&#30340;&#25991;&#20307;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.02170</link><description>&lt;p&gt;
&#19968;&#31181;&#25991;&#26412;&#20998;&#32452;&#30340;&#32479;&#35745;&#25506;&#32034;&#65306;&#12298;&#21019;&#19990;&#35760;&#12299;&#21644;&#12298;&#20986;&#22467;&#21450;&#35760;&#12299;&#20013;&#21496;&#31085;&#27966;&#21035;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
A Statistical Exploration of Text Partition Into Constituents: The Case of the Priestly Source in the Books of Genesis and Exodus. (arXiv:2305.02170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02170
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39564;&#35777;&#25991;&#26412;&#20998;&#32452;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#25991;&#26412;&#25506;&#32034;&#30340;&#27969;&#31243;&#65292;&#24182;&#22312;&#22307;&#32463;&#30340;&#21069;&#20004;&#21367;&#20070;&#20013;&#24212;&#29992;&#27492;&#27969;&#31243;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#24182;&#25506;&#32034;&#20102;&#21496;&#31085;&#27966;&#21035;&#21644;&#38750;&#21496;&#31085;&#27966;&#21035;&#20043;&#38388;&#30340;&#32479;&#35745;&#26126;&#26174;&#30340;&#25991;&#20307;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#25991;&#26412;&#25506;&#32034;&#30340;&#27969;&#31243;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#20307;&#23398;&#30340;&#35299;&#37322;&#65292;&#24182;&#23545;&#25991;&#26412;&#30340;&#20551;&#35774;&#20998;&#32452;&#36827;&#34892;&#20102;&#32479;&#35745;&#39564;&#35777;&#12290;&#32473;&#23450;&#25991;&#26412;&#30340;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#30340;&#27969;&#31243;&#65306;&#65288;1&#65289;&#26816;&#27979;&#25991;&#23398;&#29305;&#24449;&#65292;&#20197;&#20135;&#29983;&#20551;&#35774;&#20998;&#32452;&#21644;&#26080;&#30417;&#30563;&#20998;&#32452;&#20043;&#38388;&#30340;&#26368;&#20339;&#37325;&#21472;&#65292;&#65288;2&#65289;&#25191;&#34892;&#20551;&#35774;&#26816;&#39564;&#20998;&#26512;&#65292;&#37327;&#21270;&#26368;&#20339;&#37325;&#21472;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#26356;&#21487;&#33021;&#34987;&#20998;&#32452;&#30340;&#25991;&#26412;&#21333;&#20301;&#20043;&#38388;&#30340;&#38544;&#24335;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#65288;3&#65289;&#25552;&#21462;&#21644;&#37327;&#21270;&#23545;&#20998;&#31867;&#26368;&#36127;&#36131;&#30340;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20272;&#35745;&#23427;&#20204;&#30340;&#32479;&#35745;&#31283;&#23450;&#24615;&#21644;&#32858;&#31867;-wise&#20016;&#24230;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#27969;&#31243;&#24212;&#29992;&#20110;&#22307;&#32463;&#20013;&#30340;&#21069;&#20004;&#21367;&#20070;&#65292;&#22307;&#32463;&#23398;&#32773;&#20204;&#35748;&#20026;&#65292;&#20854;&#20013;&#19968;&#31181;&#25991;&#20307;&#25104;&#20998;&#29305;&#21035;&#31361;&#20986;&#65292;&#21363;&#21496;&#31085;&#27966;&#21035;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#25506;&#32034;&#20102;&#21496;&#31085;&#27966;&#21035;&#21644;&#38750;&#21496;&#31085;&#27966;&#21035;&#20043;&#38388;&#30340;&#32479;&#35745;&#26126;&#26174;&#30340;&#25991;&#20307;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a pipeline for a statistical textual exploration, offering a stylometry-based explanation and statistical validation of a hypothesized partition of a text. Given a parameterization of the text, our pipeline: (1) detects literary features yielding the optimal overlap between the hypothesized and unsupervised partitions, (2) performs a hypothesis-testing analysis to quantify the statistical significance of the optimal overlap, while conserving implicit correlations between units of text that are more likely to be grouped, and (3) extracts and quantifies the importance of features most responsible for the classification, estimates their statistical stability and cluster-wise abundance.  We apply our pipeline to the first two books in the Bible, where one stylistic component stands out in the eyes of biblical scholars, namely, the Priestly component. We identify and explore statistically significant stylistic differences between the Priestly and non-Priestly components.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#25512;&#24191;&#21040; NLP &#39046;&#22495;&#12290;&#36890;&#36807;&#21518;&#26399;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#20013;&#25552;&#21462;&#39044;&#27979;&#39640;&#32423;&#29305;&#24449;&#65288;&#27010;&#24565;&#65289;&#65292;&#24182;&#20248;&#21270;&#20855;&#26377;&#39640;&#24433;&#21709;&#21147;&#30340;&#29305;&#24449;&#23384;&#22312;&#65292;&#20351;&#20854;&#33021;&#22815;&#20934;&#30830;&#22320;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.02160</link><description>&lt;p&gt;
&#21033;&#29992;&#39640;&#24433;&#21709;&#27010;&#24565;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explaining Language Models' Predictions with High-Impact Concepts. (arXiv:2305.02160v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#25512;&#24191;&#21040; NLP &#39046;&#22495;&#12290;&#36890;&#36807;&#21518;&#26399;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#20013;&#25552;&#21462;&#39044;&#27979;&#39640;&#32423;&#29305;&#24449;&#65288;&#27010;&#24565;&#65289;&#65292;&#24182;&#20248;&#21270;&#20855;&#26377;&#39640;&#24433;&#21709;&#21147;&#30340;&#29305;&#24449;&#23384;&#22312;&#65292;&#20351;&#20854;&#33021;&#22815;&#20934;&#30830;&#22320;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#23545;&#25512;&#26029;&#27169;&#22411;&#30340;&#35299;&#37322;&#25552;&#20986;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;&#35821;&#35328;&#30340;&#32452;&#21512;&#24615;&#36136;&#65292;&#34394;&#20551;&#30456;&#20851;&#32487;&#32493;&#21066;&#24369;&#30528; NLP &#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#65292;&#23548;&#33268;&#27169;&#22411;&#30340;&#35299;&#37322;&#20165;&#20165;&#19982;&#36755;&#20986;&#39044;&#27979;&#30456;&#20851;&#65292;&#26080;&#27861;&#38752;&#35889;&#22320;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#20419;&#36827;&#20844;&#24179;&#21644;&#36879;&#26126;&#24230;&#65292;&#22312; NLP &#39046;&#22495;&#26377;&#30528;&#36843;&#20999;&#38656;&#27714;&#21487;&#38752;&#30340;&#35299;&#37322;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19968;&#33268;&#22320;&#29702;&#35299;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#25512;&#24191;&#21040; NLP &#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#20013;&#25552;&#21462;&#39044;&#27979;&#39640;&#32423;&#29305;&#24449;&#65288;&#27010;&#24565;&#65289;&#30340;&#21518;&#26399;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#20248;&#21270;&#20855;&#26377;&#39640;&#24433;&#21709;&#21147;&#30340;&#29305;&#24449;&#23384;&#22312;&#65292;&#36825;&#20123;&#29305;&#24449;&#20250;&#23548;&#33268;&#36755;&#20986;&#39044;&#27979;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-scale pretrained language models has posed unprecedented challenges in deriving explanations of why the model has made some predictions. Stemmed from the compositional nature of languages, spurious correlations have further undermined the trustworthiness of NLP systems, leading to unreliable model explanations that are merely correlated with the output predictions. To encourage fairness and transparency, there exists an urgent demand for reliable explanations that allow users to consistently understand the model's behavior. In this work, we propose a complete framework for extending concept-based interpretability methods to NLP. Specifically, we propose a post-hoc interpretability method for extracting predictive high-level features (concepts) from the pretrained model's hidden layer activations. We optimize for features whose existence causes the output predictions to change substantially, \ie generates a high impact. Moreover, we devise several evaluation metri
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20102;&#35821;&#35328;&#29305;&#24449;&#23545;&#22810;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21021;&#27493;&#25552;&#20379;&#20102;&#26041;&#27861;&#20197;&#22686;&#24378;&#23545;&#35821;&#35328;&#19978;&#30456;&#36317;&#36739;&#36828;&#30340;&#35821;&#35328;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.02151</link><description>&lt;p&gt;
&#35821;&#35328;&#36317;&#31163;&#19982;&#22810;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#30340;&#30456;&#20851;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space. (arXiv:2305.02151v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02151
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#35821;&#35328;&#29305;&#24449;&#23545;&#22810;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21021;&#27493;&#25552;&#20379;&#20102;&#26041;&#27861;&#20197;&#22686;&#24378;&#23545;&#35821;&#35328;&#19978;&#30456;&#36317;&#36739;&#36828;&#30340;&#35821;&#35328;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#19981;&#21516;&#35821;&#35328;&#29305;&#24449;&#23545;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#31181;&#25928;&#24212;&#22914;&#20309;&#26144;&#23556;&#21040;&#34920;&#31034;&#31354;&#38388;&#20013;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24494;&#35843;&#26399;&#38388;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#23545;&#40784;&#19978;&#30340;&#24433;&#21709;&#65292;&#32780;&#26412;&#30740;&#31350;&#30740;&#31350;&#30340;&#26159;&#30001;MLLMs&#29983;&#25104;&#30340;&#30456;&#24212;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#30340;&#32477;&#23545;&#28436;&#21464;&#12290;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#35821;&#35328;&#29305;&#24449;&#30340;&#20316;&#29992;&#65292;&#24182;&#35843;&#26597;&#20854;&#19982;&#34920;&#31034;&#31354;&#38388;&#21644;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#65292;&#35828;&#26126;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#22686;&#24378;&#23545;&#35821;&#35328;&#19978;&#30456;&#36317;&#36739;&#36828;&#30340;&#35821;&#35328;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior research has investigated the impact of various linguistic features on cross-lingual transfer performance. In this study, we investigate the manner in which this effect can be mapped onto the representation space. While past studies have focused on the impact on cross-lingual alignment in multilingual language models during fine-tuning, this study examines the absolute evolution of the respective language representation spaces produced by MLLMs. We place a specific emphasis on the role of linguistic characteristics and investigate their inter-correlation with the impact on representation spaces and cross-lingual transfer performance. Additionally, this paper provides preliminary evidence of how these findings can be leveraged to enhance transfer to linguistically distant languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#35838;&#31243;&#35270;&#35282;&#65292;&#20197;&#26356;&#30452;&#35266;&#30340;&#26041;&#24335;&#20998;&#26512;&#35757;&#32451;&#21160;&#24577;&#65292;&#25351;&#20986;&#27424;&#25311;&#21512;&#30340;&#21407;&#22240;&#26159;&#30001;&#20110;&#24179;&#22343;&#26679;&#26412;&#26435;&#37325;&#30340;&#38477;&#20302;&#32780;&#24341;&#36215;&#30340;&#65292;&#23545;&#22122;&#22768;&#40065;&#26834;&#24615;&#30340;&#20248;&#21270;&#21017;&#26159;&#36890;&#36807;&#23545;&#24178;&#20928;&#26679;&#26412;&#36171;&#20104;&#26356;&#22823;&#30340;&#26679;&#26412;&#26435;&#37325;&#26469;&#23454;&#29616;&#30340;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#35838;&#31243;&#20462;&#27491;&#21487;&#20197;&#25552;&#39640;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#24615;&#33021;&#65292;&#32780;&#35757;&#32451;&#36827;&#24230;&#23545;&#20110;&#40065;&#26834;&#24615;&#20063;&#26377;&#30528;&#38750;&#24120;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.02139</link><description>&lt;p&gt;
&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#35838;&#31243;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Curriculum View of Robust Loss Functions. (arXiv:2305.02139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#35838;&#31243;&#35270;&#35282;&#65292;&#20197;&#26356;&#30452;&#35266;&#30340;&#26041;&#24335;&#20998;&#26512;&#35757;&#32451;&#21160;&#24577;&#65292;&#25351;&#20986;&#27424;&#25311;&#21512;&#30340;&#21407;&#22240;&#26159;&#30001;&#20110;&#24179;&#22343;&#26679;&#26412;&#26435;&#37325;&#30340;&#38477;&#20302;&#32780;&#24341;&#36215;&#30340;&#65292;&#23545;&#22122;&#22768;&#40065;&#26834;&#24615;&#30340;&#20248;&#21270;&#21017;&#26159;&#36890;&#36807;&#23545;&#24178;&#20928;&#26679;&#26412;&#36171;&#20104;&#26356;&#22823;&#30340;&#26679;&#26412;&#26435;&#37325;&#26469;&#23454;&#29616;&#30340;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#35838;&#31243;&#20462;&#27491;&#21487;&#20197;&#25552;&#39640;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#24615;&#33021;&#65292;&#32780;&#35757;&#32451;&#36827;&#24230;&#23545;&#20110;&#40065;&#26834;&#24615;&#20063;&#26377;&#30528;&#38750;&#24120;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#26088;&#22312;&#24212;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#20854;&#40065;&#26834;&#24615;&#36890;&#24120;&#30001;&#19982;&#35757;&#32451;&#21160;&#24577;&#26080;&#20851;&#30340;&#29702;&#35770;&#30028;&#38480;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30028;&#38480;&#21487;&#33021;&#26080;&#27861;&#34920;&#24449;&#23454;&#35777;&#24615;&#33021;&#65292;&#22240;&#20026;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#20026;&#20160;&#20040;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#20250;&#27424;&#25311;&#21512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#37325;&#20889;&#25104;&#20855;&#26377;&#30456;&#21516;&#31867;-&#20998;&#25968;&#38388;&#38548;&#21644;&#19981;&#21516;&#26679;&#26412;&#21152;&#26435;&#20989;&#25968;&#24418;&#24335;&#30340;&#24418;&#24335;&#12290;&#25152;&#24471;&#21040;&#30340;&#35838;&#31243;&#35270;&#35282;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#21160;&#24577;&#30340;&#30452;&#35266;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#23558;&#27424;&#25311;&#21512;&#24402;&#22240;&#20110;&#24179;&#22343;&#26679;&#26412;&#26435;&#37325;&#30340;&#38477;&#20302;&#21644;&#23558;&#22122;&#22768;&#40065;&#26834;&#24615;&#24402;&#22240;&#20110;&#23545;&#24178;&#20928;&#26679;&#26412;&#36171;&#20104;&#36739;&#22823;&#30340;&#26679;&#26412;&#26435;&#37325;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23545;&#35838;&#31243;&#35270;&#35282;&#36827;&#34892;&#31616;&#21333;&#30340;&#20462;&#27491;&#21487;&#20197;&#20351;&#27424;&#25311;&#21512;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#31454;&#20105;&#65292;&#32780;&#35757;&#32451;&#36827;&#24230;&#21487;&#20197;&#26497;&#22823;&#22320;&#24433;&#21709;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#21363;&#20351;&#37319;&#29992;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#12290;&#20195;&#30721;&#21487;&#22312;\url{github}&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust loss functions are designed to combat the adverse impacts of label noise, whose robustness is typically supported by theoretical bounds agnostic to the training dynamics. However, these bounds may fail to characterize the empirical performance as it remains unclear why robust loss functions can underfit. We show that most loss functions can be rewritten into a form with the same class-score margin and different sample-weighting functions. The resulting curriculum view provides a straightforward analysis of the training dynamics, which helps attribute underfitting to diminished average sample weights and noise robustness to larger weights for clean samples. We show that simple fixes to the curriculums can make underfitting robust loss functions competitive with the state-of-the-art, and training schedules can substantially affect the noise robustness even with robust loss functions. Code is available at \url{github}.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;RE-KBQA&#65292;&#21033;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#20851;&#31995;&#22686;&#24378;&#23454;&#20307;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#39069;&#22806;&#30417;&#30563;&#12290;&#22312;&#19977;&#20010;&#26041;&#38754;&#25506;&#32034;&#20851;&#31995;&#25351;&#23548;&#65292;&#21253;&#25324;&#21306;&#20998;&#30456;&#20284;&#23454;&#20307;&#12289;&#25506;&#32034;&#39069;&#22806;&#30417;&#30563;&#20197;&#21450;&#36827;&#34892;&#21518;&#22788;&#29702;&#30340;&#22522;&#20110;&#20851;&#31995;&#25351;&#23548;&#30340;&#37325;&#25490;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02118</link><description>&lt;p&gt;
&#20851;&#27880;&#20851;&#31995;&#25506;&#32034;&#65292;&#25552;&#21319;&#30693;&#35782;&#24211;&#38382;&#31572;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Pay More Attention to Relation Exploration for Knowledge Base Question Answering. (arXiv:2305.02118v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02118
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;RE-KBQA&#65292;&#21033;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#20851;&#31995;&#22686;&#24378;&#23454;&#20307;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#39069;&#22806;&#30417;&#30563;&#12290;&#22312;&#19977;&#20010;&#26041;&#38754;&#25506;&#32034;&#20851;&#31995;&#25351;&#23548;&#65292;&#21253;&#25324;&#21306;&#20998;&#30456;&#20284;&#23454;&#20307;&#12289;&#25506;&#32034;&#39069;&#22806;&#30417;&#30563;&#20197;&#21450;&#36827;&#34892;&#21518;&#22788;&#29702;&#30340;&#22522;&#20110;&#20851;&#31995;&#25351;&#23548;&#30340;&#37325;&#25490;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#26159;&#19968;&#39033;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#27491;&#30830;&#31572;&#26696;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#23454;&#20307;&#34920;&#31034;&#21644;&#26368;&#32456;&#31572;&#26696;&#25512;&#29702;&#65292;&#23548;&#33268;&#23545;&#27492;&#20219;&#21153;&#30340;&#38480;&#21046;&#24615;&#30417;&#30563;&#12290;&#27492;&#22806;&#65292;&#20851;&#31995;&#22312;&#26368;&#36817;&#30340;&#25216;&#26415;&#20013;&#24182;&#26410;&#34987;&#20805;&#20998;&#32771;&#34385;&#65292;&#32780;&#20851;&#31995;&#23454;&#38469;&#19978;&#20915;&#23450;&#20102;&#25512;&#29702;&#36335;&#24452;&#30340;&#36873;&#25321;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;RE-KBQA&#65292;&#21033;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#20851;&#31995;&#22686;&#24378;&#23454;&#20307;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#39069;&#22806;&#30340;&#30417;&#30563;&#12290;&#25105;&#20204;&#20174;&#19977;&#20010;&#26041;&#38754;&#25506;&#32034;&#20851;&#31995;&#25351;&#23548;&#65292;&#21253;&#25324;&#65288;1&#65289;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#20851;&#31995;&#37325;&#35201;&#24615;&#26469;&#21306;&#20998;&#30456;&#20284;&#23454;&#20307;&#65307;&#65288;2&#65289;&#36890;&#36807;&#39044;&#27979;&#20851;&#31995;&#20998;&#24067;&#20316;&#20026;&#36719;&#26631;&#31614;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#26041;&#26696;&#25506;&#32034;&#39069;&#22806;&#30417;&#30563;&#65307;&#65288;3&#65289;&#35774;&#35745;&#22522;&#20110;&#20851;&#31995;&#25351;&#23548;&#30340;&#37325;&#25490;&#31639;&#27861;&#36827;&#34892;&#21518;&#22788;&#29702;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge base question answering (KBQA) is a challenging task that aims to retrieve correct answers from large-scale knowledge bases. Existing attempts primarily focus on entity representation and final answer reasoning, which results in limited supervision for this task. Moreover, the relations, which empirically determine the reasoning path selection, are not fully considered in recent advancements. In this study, we propose a novel framework, RE-KBQA, that utilizes relations in the knowledge base to enhance entity representation and introduce additional supervision. We explore guidance from relations in three aspects, including (1) distinguishing similar entities by employing a variational graph auto-encoder to learn relation importance; (2) exploring extra supervision by predicting relation distributions as soft labels with a multi-task scheme; (3) designing a relation-guided re-ranking algorithm for post-processing. Experimental results on two benchmark datasets demonstrate the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GPT-RE&#65292;&#36890;&#36807;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#23454;&#20307;&#34920;&#31034;&#21644;&#20351;&#29992;&#37329;&#26631;&#31614;&#35825;&#23548;&#30340;&#25512;&#29702;&#36923;&#36753;&#20016;&#23500;&#28436;&#31034;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;&#20302;&#30456;&#20851;&#24615;&#21644;&#20542;&#21521;&#20110;&#38169;&#35823;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#25104;&#26524;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#36229;&#36807;&#29616;&#26377;&#22522;&#20934;&#65292;&#20854;&#20013;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#32467;&#26524;&#36798;&#21040;&#20102;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02105</link><description>&lt;p&gt;
GPT-RE: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20851;&#31995;&#25277;&#21462;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPT-RE: In-context Learning for Relation Extraction using Large Language Models. (arXiv:2305.02105v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GPT-RE&#65292;&#36890;&#36807;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#23454;&#20307;&#34920;&#31034;&#21644;&#20351;&#29992;&#37329;&#26631;&#31614;&#35825;&#23548;&#30340;&#25512;&#29702;&#36923;&#36753;&#20016;&#23500;&#28436;&#31034;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;&#20302;&#30456;&#20851;&#24615;&#21644;&#20542;&#21521;&#20110;&#38169;&#35823;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#25104;&#26524;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#36229;&#36807;&#29616;&#26377;&#22522;&#20934;&#65292;&#20854;&#20013;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#32467;&#26524;&#36798;&#21040;&#20102;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-3&#65289;&#26377;&#21487;&#33021;&#21462;&#24471;&#31361;&#30772;&#24615;&#25104;&#23601;&#65292;&#20294;&#23427;&#20204;&#22312;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#20173;&#36828;&#36828;&#33853;&#21518;&#20110;&#23436;&#20840;&#30417;&#30563;&#30340;&#22522;&#20934;&#65288;&#20363;&#22914;fine-tuned BERT&#65289;&#12290;  &#36825;&#26159;&#30001;&#20110;LLMs&#22312;RE&#26041;&#38754;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;:(1)&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#26816;&#32034;&#21040;&#30340;&#28436;&#31034;&#20013;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;; (2)&#24378;&#28872;&#20542;&#21521;&#20110;&#38169;&#35823;&#22320;&#23558;NULL&#31034;&#20363;&#20998;&#31867;&#20026;&#20854;&#20182;&#39044;&#23450;&#20041;&#30340;&#26631;&#31614;&#12290;  &#26412;&#25991;&#25552;&#20986;&#20102;GPT-RE&#26469;&#24357;&#21512;LLMs&#21644;&#23436;&#20840;&#30417;&#30563;&#30340;&#22522;&#20934;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290; GPT-RE&#36890;&#36807;&#65288;1&#65289;&#22312;&#28436;&#31034;&#26816;&#32034;&#20013;&#21152;&#20837;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#23454;&#20307;&#34920;&#31034;; &#65288;2&#65289;&#20351;&#29992;&#37329;&#26631;&#31614;&#35825;&#23548;&#30340;&#25512;&#29702;&#36923;&#36753;&#20016;&#23500;&#28436;&#31034;&#26469;&#25104;&#21151;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290; &#25105;&#20204;&#22312;&#22235;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;RE&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;GPT-RE&#65292;&#24182;&#35266;&#23519;&#21040;GPT-RE&#19981;&#20165;&#25913;&#21892;&#20102;&#29616;&#26377;&#30340;GPT-3&#22522;&#20934;&#65292;&#32780;&#19988;&#25913;&#21892;&#20102;&#23436;&#20840;&#30417;&#30563;&#30340;&#22522;&#20934;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;GPT-RE&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#19977;&#20010;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In spite of the potential for ground-breaking achievements offered by large language models (LLMs) (e.g., GPT-3), they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of LLMs in RE: (1) low relevance regarding entity and relation in retrieved demonstrations for in-context learning; and (2) the strong inclination to wrongly classify NULL examples into other pre-defined labels.  In this paper, we propose GPT-RE to bridge the gap between LLMs and fully-supervised baselines. GPT-RE successfully addresses the aforementioned issues by (1) incorporating task-specific entity representations in demonstration retrieval; and (2) enriching the demonstrations with gold label-induced reasoning logic. We evaluate GPT-RE on four widely-used RE datasets, and observe that GPT-RE achieves improvements over not only existing GPT-3 baselines, but also fully-supervised baselines. Specifically, GPT-RE ach
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#32972;&#26223;&#30693;&#35782;&#28304;&#25991;&#20214;&#21487;&#20197;&#25552;&#39640;&#26222;&#21450;&#25688;&#35201;&#30340;&#30456;&#20851;&#24615;&#21644;&#21487;&#35835;&#24615;&#65292;&#20294;&#19981;&#33021;&#25552;&#39640;&#20854;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02104</link><description>&lt;p&gt;
&#21487;&#35835;&#24615;&#12289;&#30456;&#20851;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#29983;&#29289;&#21307;&#23398;&#26222;&#21450;&#25688;&#35201;&#30340;&#32972;&#26223;&#30693;&#35782;&#23637;&#24320;
&lt;/p&gt;
&lt;p&gt;
Background Knowledge Grounding for Readable, Relevant, and Factual Biomedical Lay Summaries. (arXiv:2305.02104v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02104
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32972;&#26223;&#30693;&#35782;&#28304;&#25991;&#20214;&#21487;&#20197;&#25552;&#39640;&#26222;&#21450;&#25688;&#35201;&#30340;&#30456;&#20851;&#24615;&#21644;&#21487;&#35835;&#24615;&#65292;&#20294;&#19981;&#33021;&#25552;&#39640;&#20854;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#20844;&#20247;&#20256;&#25773;&#31185;&#23398;&#21457;&#29616;&#23545;&#20110;&#20445;&#25345;&#38750;&#19987;&#23478;&#20102;&#35299;&#32039;&#24613;&#21307;&#30103;&#27835;&#30103;&#31561;&#21457;&#23637;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#20174;&#31185;&#23398;&#25991;&#26723;&#20013;&#29983;&#25104;&#21487;&#35835;&#30340;&#26222;&#21450;&#25688;&#35201;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24182;&#19988;&#30446;&#21069;&#65292;&#36825;&#20123;&#25688;&#35201;&#23384;&#22312;&#20005;&#37325;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;&#25552;&#39640;&#20107;&#23454;&#24615;&#30340;&#19968;&#31181;&#27969;&#34892;&#24178;&#39044;&#25514;&#26045;&#26159;&#20351;&#29992;&#20854;&#20182;&#22806;&#37096;&#30693;&#35782;&#26469;&#25552;&#20379;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#26816;&#32034;&#12289;&#36873;&#25321;&#25110;&#25972;&#21512;&#36825;&#20123;&#32972;&#26223;&#30693;&#35782;&#65292;&#20197;&#21450;&#34917;&#20805;&#32972;&#26223;&#30693;&#35782;&#25991;&#26723;&#22914;&#20309;&#24433;&#21709;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#21487;&#35835;&#24615;&#25110;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#36873;&#25321;&#32972;&#26223;&#30693;&#35782;&#28304;&#65292;&#24182;&#23558;&#20854;&#19982;&#28304;&#25991;&#26723;&#38598;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;BioLaySum&#27719;&#24635;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#19981;&#21516;&#32972;&#26223;&#30693;&#35782;&#28304;&#23545;&#25688;&#35201;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32972;&#26223;&#30693;&#35782;&#28304;&#25991;&#20214;&#21487;&#20197;&#25552;&#39640;&#26222;&#21450;&#25688;&#35201;&#30340;&#30456;&#20851;&#24615;&#21644;&#21487;&#35835;&#24615;&#65292;&#20294;&#19981;&#33021;&#25552;&#39640;&#26222;&#21450;&#25688;&#35201;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication of scientific findings to the public is important for keeping non-experts informed of developments such as life-saving medical treatments. However, generating readable lay summaries from scientific documents is challenging, and currently, these summaries suffer from critical factual errors. One popular intervention for improving factuality is using additional external knowledge to provide factual grounding. However, it is unclear how these grounding sources should be retrieved, selected, or integrated, and how supplementary grounding documents might affect the readability or relevance of the generated summaries. We develop a simple method for selecting grounding sources and integrating them with source documents. We then use the BioLaySum summarization dataset to evaluate the effects of different grounding sources on summary quality. We found that grounding source documents improves the relevance and readability of lay summaries but does not improve factuality of lay summ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;Voice Activity Projection&#27169;&#22411;&#65292;&#25506;&#31350;&#22635;&#20805;&#20572;&#39039;&#23545;&#25345;&#32493;&#21457;&#35328;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#22635;&#20805;&#20572;&#39039;&#30830;&#23454;&#20855;&#26377;&#25345;&#32493;&#21457;&#35328;&#30340;&#25928;&#26524;&#65292;&#20294;&#21487;&#33021;&#19981;&#20687;&#20154;&#20204;&#39044;&#26399;&#30340;&#37027;&#20040;&#24378;&#28872;&#12290;</title><link>http://arxiv.org/abs/2305.02101</link><description>&lt;p&gt;
&#20160;&#20040;&#26679;&#30340;&#20572;&#39039;&#26159;&#22909;&#30340;&#21602;?&#8212;&#8212;&#25506;&#31350;&#35821;&#38899;&#20572;&#39039;&#23545;&#25345;&#32493;&#21457;&#35328;&#25928;&#24212;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
What makes a good pause? Investigating the turn-holding effects of fillers. (arXiv:2305.02101v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;Voice Activity Projection&#27169;&#22411;&#65292;&#25506;&#31350;&#22635;&#20805;&#20572;&#39039;&#23545;&#25345;&#32493;&#21457;&#35328;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#22635;&#20805;&#20572;&#39039;&#30830;&#23454;&#20855;&#26377;&#25345;&#32493;&#21457;&#35328;&#30340;&#25928;&#26524;&#65292;&#20294;&#21487;&#33021;&#19981;&#20687;&#20154;&#20204;&#39044;&#26399;&#30340;&#37027;&#20040;&#24378;&#28872;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22635;&#20805;&#20572;&#39039;&#65288;&#25110;&#21344;&#20301;&#31526;&#65289;&#65292;&#20363;&#22914;&#8220;&#21999;&#8221;&#21644;&#8220;&#21834;&#8221;&#65292;&#22312;&#21475;&#35821;&#20013;&#39057;&#32321;&#20986;&#29616;&#65292;&#21487;&#20197;&#20316;&#20026;&#21548;&#20247;&#30340;&#25345;&#32493;&#21457;&#35328;&#25552;&#31034;&#65292;&#34920;&#26126;&#24403;&#21069;&#28436;&#35762;&#32773;&#23578;&#26410;&#23436;&#25104;&#12290;&#26412;&#25991;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;Voice Activity Projection&#65288;VAP&#65289;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#23545;&#35805;&#30340;&#21160;&#24577;&#65292;&#20998;&#26512;&#22635;&#20805;&#20572;&#39039;&#23545;&#39044;&#26399;&#25345;&#32493;&#21457;&#35328;&#27010;&#29575;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#22635;&#20805;&#20572;&#39039;&#30830;&#23454;&#20855;&#26377;&#25345;&#32493;&#21457;&#35328;&#30340;&#25928;&#26524;&#65292;&#20294;&#21487;&#33021;&#19981;&#20687;&#20154;&#20204;&#39044;&#26399;&#30340;&#37027;&#20040;&#24378;&#28872;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#20854;&#20182;&#25552;&#31034;&#30340;&#20887;&#20313;&#24615;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22635;&#20805;&#29289;&#30340;&#38901;&#24459;&#29305;&#24615;&#21644;&#20301;&#32622;&#23545;&#25345;&#32493;&#21457;&#35328;&#30340;&#27010;&#29575;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#25152;&#25552;&#20986;&#30340;&#19981;&#21516;&#65292;&#36825;&#26041;&#38754;&#8220;&#21999;&#8221;&#21644;&#8220;&#21834;&#8221;&#27809;&#26377;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Filled pauses (or fillers), such as "uh" and "um", are frequent in spontaneous speech and can serve as a turn-holding cue for the listener, indicating that the current speaker is not done yet. In this paper, we use the recently proposed Voice Activity Projection (VAP) model, which is a deep learning model trained to predict the dynamics of conversation, to analyse the effects of filled pauses on the expected turn-hold probability. The results show that, while filled pauses do indeed have a turn-holding effect, it is perhaps not as strong as could be expected, probably due to the redundancy of other cues. We also find that the prosodic properties and position of the filler has a significant effect on the turn-hold probability. However, contrary to what has been suggested in previous work, there is no difference between "uh" and "um" in this regard.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#21709;&#24212;&#26465;&#20214;&#27169;&#22411;&#32467;&#21512;&#20102;&#23545;&#35805;&#21382;&#21490;&#21644;&#19979;&#19968;&#20010;&#21457;&#35328;&#32773;&#24819;&#35201;&#34920;&#36798;&#30340;&#20869;&#23481;&#26469;&#39044;&#27979;&#19968;&#36718;&#23545;&#35805;&#20309;&#26102;&#32467;&#26463;&#65292;&#24182;&#22312;Stanford&#23545;&#35805;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20339;&#30340;&#39044;&#27979;&#21644;&#21709;&#24212;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02036</link><description>&lt;p&gt;
&#21709;&#24212;&#26465;&#20214;&#30340;&#20132;&#26367;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Response-conditioned Turn-taking Prediction. (arXiv:2305.02036v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#21709;&#24212;&#26465;&#20214;&#27169;&#22411;&#32467;&#21512;&#20102;&#23545;&#35805;&#21382;&#21490;&#21644;&#19979;&#19968;&#20010;&#21457;&#35328;&#32773;&#24819;&#35201;&#34920;&#36798;&#30340;&#20869;&#23481;&#26469;&#39044;&#27979;&#19968;&#36718;&#23545;&#35805;&#20309;&#26102;&#32467;&#26463;&#65292;&#24182;&#22312;Stanford&#23545;&#35805;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20339;&#30340;&#39044;&#27979;&#21644;&#21709;&#24212;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#35299;&#20915;&#23545;&#35805;&#31995;&#32479;&#20013;&#36716;&#25442;&#21450;&#21709;&#24212;&#29983;&#25104;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#20854;&#35270;&#20026;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#65306;&#39318;&#20808;&#36890;&#36807;&#23545;&#35805;&#21382;&#21490;&#26469;&#26816;&#27979;&#19968;&#36718;&#23545;&#35805;&#26159;&#21542;&#24050;&#32467;&#26463;&#65292;&#28982;&#21518;&#31995;&#32479;&#29983;&#25104;&#19968;&#20010;&#21512;&#36866;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#19981;&#20165;&#20165;&#22240;&#20026;&#24456;&#21487;&#33021;&#36718;&#21040;&#33258;&#24049;&#20102;&#23601;&#21457;&#35328;&#65292;&#36824;&#35201;&#32771;&#34385;&#33258;&#24049;&#24819;&#35828;&#30340;&#35805;&#26159;&#21542;&#36866;&#21512;&#24403;&#21069;&#20301;&#32622;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#65288;TurnGPT&#30340;&#25193;&#23637;&#65289;&#65292;&#23427;&#22522;&#20110;&#23545;&#35805;&#21382;&#21490;&#21644;&#19979;&#19968;&#20010;&#21457;&#35328;&#32773;&#35201;&#35828;&#30340;&#20869;&#23481;&#26469;&#39044;&#27979;&#19968;&#36718;&#23545;&#35805;&#20309;&#26102;&#32467;&#26463;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#25351;&#26631;&#26041;&#38754;&#19968;&#30452;&#34920;&#29616;&#20248;&#24322;&#12290;&#22312;&#20004;&#31181;&#24773;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#25913;&#36827;&#23588;&#20026;&#26174;&#33879;&#65292;&#36825;&#20004;&#31181;&#24773;&#26223;&#19979;&#36718;&#25442;&#39044;&#27979;&#20165;&#30001;&#23545;&#35805;&#21382;&#21490;&#21487;&#33021;&#34920;&#29616;&#20026;&#27169;&#26865;&#20004;&#21487;&#30340;&#65306;1&#65289;&#24403;&#21069;&#35805;&#35821;&#20013;&#21253;&#21547;&#38472;&#36848;&#21477;&#21644;&#32039;&#25509;&#30528;&#30340;&#30097;&#38382;&#21477;&#65307;2&#65289;&#24403;&#21069;&#35805;&#35821;&#30340;&#32467;&#23614;&#21644;&#25152;&#38656;&#21709;&#24212;&#22312;&#35821;&#20041;&#19978;&#21305;&#37197;&#12290;&#23558;&#20132;&#26367;&#39044;&#27979;&#21644;&#21709;&#24212;&#25490;&#24207;&#35270;&#20026;&#19968;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Stanford&#23545;&#35805;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26368;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous approaches to turn-taking and response generation in conversational systems have treated it as a two-stage process: First, the end of a turn is detected (based on conversation history), then the system generates an appropriate response. Humans, however, do not take the turn just because it is likely, but also consider whether what they want to say fits the position. In this paper, we present a model (an extension of TurnGPT) that conditions the end-of-turn prediction on both conversation history and what the next speaker wants to say. We found that our model consistently outperforms the baseline model in a variety of metrics. The improvement is most prominent in two scenarios where turn predictions can be ambiguous solely from the conversation history: 1) when the current utterance contains a statement followed by a question; 2) when the end of the current utterance semantically matches the response. Treating the turn-prediction and response-ranking as a one-stage process, our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#21387;&#32553;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#20197;&#36866;&#24212;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#20266;&#30446;&#26631;&#35757;&#32451;&#25216;&#26415;&#38024;&#23545;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02031</link><description>&lt;p&gt;
&#31995;&#32479;&#30740;&#31350;&#22522;&#20110;&#20266;&#30446;&#26631;&#35757;&#32451;&#30340;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training. (arXiv:2305.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#21387;&#32553;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#20197;&#36866;&#24212;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#20266;&#30446;&#26631;&#35757;&#32451;&#25216;&#26415;&#38024;&#23545;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#12290;&#26412;&#25991;&#30740;&#31350;&#21387;&#32553;&#36825;&#20123;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#36825;&#23545;&#20110;&#26381;&#21153;&#25968;&#30334;&#19975;&#29992;&#25143;&#30340;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#25216;&#26415;&#65292;&#20854;&#20013;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#23398;&#20064;&#27169;&#20223;&#22823;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#20351;&#24471;&#21487;&#20197;&#20174;&#25945;&#24072;&#21521;&#23398;&#29983;&#20256;&#36882;&#30693;&#35782;&#12290;&#19982;&#20043;&#21069;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#38024;&#23545;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#20248;&#21270;&#27169;&#22411;&#12290;&#36890;&#24120;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#38500;&#20102;&#26377;&#26631;&#35760;&#25968;&#25454;&#22806;&#65292;&#36824;&#26377;&#22823;&#37327;&#30340;&#26410;&#26631;&#35760;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#65292;&#36825;&#23545;&#20110;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#33719;&#24471;&#39640;&#21387;&#32553;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#29616;&#23454;&#30340;&#20551;&#35774;&#19979;&#65292;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#33976;&#39311;&#30740;&#31350;&#65292;&#24182;&#35752;&#35770;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#33976;&#39311;&#30340;&#29305;&#27530;&#29305;&#24449;&#65292;&#23588;&#20854;&#26159;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#31995;&#21015;&#20266;&#30446;&#26631;&#35757;&#32451;&#65288;PTT&#65289;&#25216;&#26415;&#65292;&#32531;&#35299;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern Natural Language Generation (NLG) models come with massive computational and storage requirements. In this work, we study the potential of compressing them, which is crucial for real-world applications serving millions of users. We focus on Knowledge Distillation (KD) techniques, in which a small student model learns to imitate a large teacher model, allowing to transfer knowledge from the teacher to the student. In contrast to much of the previous work, our goal is to optimize the model for a specific NLG task and a specific dataset. Typically, in real-world applications, in addition to labeled data there is abundant unlabeled task-specific data, which is crucial for attaining high compression rates via KD. In this work, we conduct a systematic study of task-specific KD techniques for various NLG tasks under realistic assumptions. We discuss the special characteristics of NLG distillation and particularly the exposure bias problem. Following, we derive a family of Pseudo-Target
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#24773;&#24863;&#20998;&#26512;&#12289;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#24212;&#29992;&#20110;&#23458;&#25143;&#30340;B2B&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#33021;&#22815;&#33258;&#21160;&#25552;&#21462;&#20986;&#27491;&#30830;&#24773;&#24863;&#20449;&#24687;&#24182;&#20998;&#25104;&#19981;&#21516;&#30340;&#20027;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02029</link><description>&lt;p&gt;
&#23458;&#25143;&#22791;&#27880;&#25968;&#25454;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Natural language processing on customer note data. (arXiv:2305.02029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#24773;&#24863;&#20998;&#26512;&#12289;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#24212;&#29992;&#20110;&#23458;&#25143;&#30340;B2B&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#33021;&#22815;&#33258;&#21160;&#25552;&#21462;&#20986;&#27491;&#30830;&#24773;&#24863;&#20449;&#24687;&#24182;&#20998;&#25104;&#19981;&#21516;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20998;&#26512;&#23458;&#25143;&#25968;&#25454;&#23545;&#19994;&#21153;&#20844;&#21496;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#39046;&#22495;&#12290;&#30001;&#20110;&#36825;&#20123;&#25935;&#24863;&#20449;&#24687;&#30340;&#29305;&#27530;&#24615;&#36136;&#65292;&#19994;&#21153;&#23545;&#19994;&#21153;&#25968;&#25454;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#20197;&#21152;&#36895;&#20998;&#26512;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#23558;&#24773;&#24863;&#20998;&#26512;&#12289;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#24212;&#29992;&#20110;&#19968;&#20221;B2B&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20174;&#22791;&#27880;&#20013;&#33258;&#21160;&#25552;&#21462;&#20934;&#30830;&#30340;&#24773;&#24863;&#20449;&#24687;&#65292;&#24182;&#23558;&#22791;&#27880;&#25353;&#29031;&#30456;&#20851;&#24615;&#20998;&#25104;&#19981;&#21516;&#30340;&#20027;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27809;&#26377;&#26126;&#30830;&#30340;&#20027;&#39064;&#20998;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#20027;&#39064;&#21487;&#33021;&#32570;&#20047;&#19994;&#21153;&#19978;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic analysis of customer data for businesses is an area that is of interest to companies. Business to business data is studied rarely in academia due to the sensitive nature of such information. Applying natural language processing can speed up the analysis of prohibitively large sets of data. This paper addresses this subject and applies sentiment analysis, topic modelling and keyword extraction to a B2B data set. We show that accurate sentiment can be extracted from the notes automatically and the notes can be sorted by relevance into different topics. We see that without clear separation topics can lack relevance to a business context.
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#38899;&#39057;&#36136;&#37327;&#23545;&#33258;&#28982;&#24405;&#38899;&#20013;&#23156;&#20799;&#35821;&#38899;&#30740;&#31350;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#38899;&#39057;&#36136;&#37327;&#23545;&#23156;&#20799;&#23548;&#21521;&#35821;&#38899;&#21644;&#25104;&#20154;&#23548;&#21521;&#35821;&#38899;&#20998;&#26512;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.01965</link><description>&lt;p&gt;
&#20998;&#26512;&#38899;&#39057;&#36136;&#37327;&#23545;&#33258;&#28982;&#38271;&#31687;&#24405;&#38899;&#22312;&#23156;&#20799;&#35821;&#38899;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Analysing the Impact of Audio Quality on the Use of Naturalistic Long-Form Recordings for Infant-Directed Speech Research. (arXiv:2305.01965v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01965
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#38899;&#39057;&#36136;&#37327;&#23545;&#33258;&#28982;&#24405;&#38899;&#20013;&#23156;&#20799;&#35821;&#38899;&#30740;&#31350;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#38899;&#39057;&#36136;&#37327;&#23545;&#23156;&#20799;&#23548;&#21521;&#35821;&#38899;&#21644;&#25104;&#20154;&#23548;&#21521;&#35821;&#38899;&#20998;&#26512;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#35821;&#35328;&#20064;&#24471;&#30340;&#24314;&#27169;&#26088;&#22312;&#29702;&#35299;&#23156;&#20799;&#22914;&#20309;&#24341;&#23548;&#20182;&#20204;&#30340;&#35821;&#35328;&#25216;&#33021;&#12290;&#24314;&#27169;&#28085;&#30422;&#20102;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20837;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#27491;&#22312;&#27979;&#35797;&#30340;&#35748;&#30693;&#20551;&#35774;&#21450;&#20854;&#31639;&#27861;&#23454;&#29616;&#65292;&#20197;&#21450;&#29992;&#20110;&#23558;&#27169;&#22411;&#19982;&#20154;&#31867;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#20351;&#24471;&#35745;&#31639;&#27169;&#22411;&#20351;&#29992;&#26356;&#33258;&#28982;&#30340;&#35757;&#32451;&#25968;&#25454;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#20063;&#28608;&#21457;&#20102;&#24320;&#21457;&#26356;&#33258;&#28982;&#30340;&#27169;&#22411;&#34892;&#20026;&#27979;&#35797;&#30340;&#21160;&#26426;&#12290;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#20851;&#38190;&#27493;&#39588;&#26159;&#24320;&#21457;&#20195;&#34920;&#24615;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#23156;&#20799;&#22312;&#20854;&#33258;&#28982;&#29615;&#22659;&#20013;&#21548;&#21040;&#30340;&#35821;&#38899;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24405;&#38899;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#36890;&#24120;&#26159;&#22024;&#26434;&#30340;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22768;&#38899;&#36136;&#37327;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#30340;&#20998;&#26512;&#21644;&#24314;&#27169;&#23454;&#39564;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23156;&#20799;&#23548;&#21521;&#35821;&#38899;&#65288;IDS&#65289;&#21644;&#25104;&#20154;&#23548;&#21521;&#35821;&#38899;&#65288;ADS&#65289;&#20998;&#26512;&#30340;&#36825;&#19968;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modelling of early language acquisition aims to understand how infants bootstrap their language skills. The modelling encompasses properties of the input data used for training the models, the cognitive hypotheses and their algorithmic implementations being tested, and the evaluation methodologies to compare models to human data. Recent developments have enabled the use of more naturalistic training data for computational models. This also motivates development of more naturalistic tests of model behaviour. A crucial step towards such an aim is to develop representative speech datasets consisting of speech heard by infants in their natural environments. However, a major drawback of such recordings is that they are typically noisy, and it is currently unclear how the sound quality could affect analyses and modelling experiments conducted on such data. In this paper, we explore this aspect for the case of infant-directed speech (IDS) and adult-directed speech (ADS) analysis. First, we ma
&lt;/p&gt;</description></item><item><title>NorQuAD&#26159;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#31532;&#19968;&#20010;&#25386;&#23041;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4,752&#23545;&#25163;&#24037;&#21019;&#24314;&#30340;&#38382;&#31572;&#23545;&#65292;&#24182;&#22522;&#20110;&#35813;&#25968;&#25454;&#38598;&#23545;&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2305.01957</link><description>&lt;p&gt;
NorQuAD&#65306;&#25386;&#23041;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
NorQuAD: Norwegian Question Answering Dataset. (arXiv:2305.01957v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01957
&lt;/p&gt;
&lt;p&gt;
NorQuAD&#26159;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#31532;&#19968;&#20010;&#25386;&#23041;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4,752&#23545;&#25163;&#24037;&#21019;&#24314;&#30340;&#38382;&#31572;&#23545;&#65292;&#24182;&#22522;&#20110;&#35813;&#25968;&#25454;&#38598;&#23545;&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NorQuAD&#65306;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#31532;&#19968;&#20010;&#25386;&#23041;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;4,752&#23545;&#25163;&#24037;&#21019;&#24314;&#30340;&#38382;&#31572;&#23545;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#22522;&#20110;&#25968;&#25454;&#38598;&#23545;&#20960;&#31181;&#22810;&#35821;&#35328;&#21644;&#25386;&#23041;&#21333;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#34987;&#20813;&#36153;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present NorQuAD: the first Norwegian question answering dataset for machine reading comprehension. The dataset consists of 4,752 manually created question-answer pairs. We here detail the data collection procedure and present statistics of the dataset. We also benchmark several multilingual and Norwegian monolingual language models on the dataset and compare them against human performance. The dataset will be made freely available.
&lt;/p&gt;</description></item><item><title>SeqAug&#26159;&#19968;&#31181;&#27169;&#24577;&#19981;&#21487;&#30693;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#25104;&#21151;&#24212;&#29992;&#20110;&#21333;&#27169;&#24577;&#25110;&#22810;&#27169;&#24577;&#65292;&#36890;&#36807;&#20174;&#22522;&#30784;&#29305;&#24449;&#20998;&#24067;&#20013;&#37325;&#26032;&#37319;&#26679;&#26469;&#22686;&#24378;&#24207;&#21015;&#65292;&#24182;&#19988;&#19982;&#24490;&#29615;&#21644;Transformer&#26550;&#26500;&#20860;&#23481;&#65292;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01954</link><description>&lt;p&gt;
SeqAug: &#24207;&#21015;&#29305;&#24449;&#37325;&#37319;&#26679;&#25216;&#26415;&#8212;&#8212;&#19968;&#31181;&#27169;&#24577;&#19981;&#21487;&#30693;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SeqAug: Sequential Feature Resampling as a modality agnostic augmentation method. (arXiv:2305.01954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01954
&lt;/p&gt;
&lt;p&gt;
SeqAug&#26159;&#19968;&#31181;&#27169;&#24577;&#19981;&#21487;&#30693;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#25104;&#21151;&#24212;&#29992;&#20110;&#21333;&#27169;&#24577;&#25110;&#22810;&#27169;&#24577;&#65292;&#36890;&#36807;&#20174;&#22522;&#30784;&#29305;&#24449;&#20998;&#24067;&#20013;&#37325;&#26032;&#37319;&#26679;&#26469;&#22686;&#24378;&#24207;&#21015;&#65292;&#24182;&#19988;&#19982;&#24490;&#29615;&#21644;Transformer&#26550;&#26500;&#20860;&#23481;&#65292;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#25552;&#39640;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#24615;&#33021;&#30340;&#24120;&#29992;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SeqAug&#65292;&#19968;&#31181;&#38024;&#23545;&#20174;&#24207;&#21015;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;SeqAug&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#20174;&#22522;&#30784;&#29305;&#24449;&#20998;&#24067;&#20013;&#37325;&#26032;&#37319;&#26679;&#26469;&#22686;&#24378;&#24207;&#21015;&#12290;&#37325;&#26032;&#37319;&#26679;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#29305;&#24449;&#32500;&#24230;&#24182;&#27839;&#26102;&#38388;&#36724;&#23545;&#23427;&#20204;&#36827;&#34892;&#32622;&#25442;&#26469;&#23454;&#29616;&#12290;&#22312;CMU-MOSEI&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;SeqAug&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#24615;&#65307;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#21333;&#27169;&#24577;&#25110;&#22810;&#27169;&#24577;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#23427;&#19982;&#24490;&#29615;&#21644;Transformer&#26550;&#26500;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#32467;&#26524;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is a prevalent technique for improving performance in various machine learning applications. We propose SeqAug, a modality-agnostic augmentation method that is tailored towards sequences of extracted features. The core idea of SeqAug is to augment the sequence by resampling from the underlying feature distribution. Resampling is performed by randomly selecting feature dimensions and permuting them along the temporal axis. Experiments on CMU-MOSEI verify that SeqAug is modality agnostic; it can be successfully applied to a single modality or multiple modalities. We further verify its compatibility with both recurrent and transformer architectures, and also demonstrate comparable to state-of-the-art results.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102; TempoSum &#25277;&#35937;&#25688;&#35201;&#30340;&#26102;&#38388;&#27867;&#21270;&#33021;&#21147;&#22522;&#20934;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#35777;&#26126;&#20102;&#25688;&#35201;&#27169;&#22411;&#20013;&#23384;&#20648;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#23545;&#26410;&#26469;&#25968;&#25454;&#19978;&#29983;&#25104;&#30340;&#25688;&#35201;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.01951</link><description>&lt;p&gt;
TempoSum&#65306;&#35780;&#20272;&#25277;&#35937;&#25688;&#35201;&#30340;&#26102;&#38388;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
TempoSum: Evaluating the Temporal Generalization of Abstractive Summarization. (arXiv:2305.01951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102; TempoSum &#25277;&#35937;&#25688;&#35201;&#30340;&#26102;&#38388;&#27867;&#21270;&#33021;&#21147;&#22522;&#20934;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#35777;&#26126;&#20102;&#25688;&#35201;&#27169;&#22411;&#20013;&#23384;&#20648;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#23545;&#26410;&#26469;&#25968;&#25454;&#19978;&#29983;&#25104;&#30340;&#25688;&#35201;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#26377;&#30340;&#25277;&#35937;&#25688;&#35201;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26377; promising &#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25688;&#35201;&#22522;&#20934;&#19982;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21644;&#24494;&#35843;&#25968;&#25454;&#38598;&#22312;&#26102;&#38388;&#19978;&#37325;&#21472;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#24615;&#33021;&#21487;&#33021;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20013;&#25152;&#35760;&#24518;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25152;&#35760;&#24518;&#30340;&#30693;&#35782;&#21487;&#33021;&#24456;&#24555;&#23601;&#36807;&#26102;&#65292;&#36825;&#20250;&#24433;&#21709;&#21040;&#23427;&#20204;&#22312;&#26410;&#26469;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#20102;&#35299;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#30340;&#26102;&#38388;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; TempoSum&#65292;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20174; 2010 &#24180;&#21040; 2022 &#24180;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25688;&#35201;&#27169;&#22411;&#20013;&#23384;&#20648;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#23545;&#26410;&#26469;&#25968;&#25454;&#19978;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#26041;&#27861;&#19981;&#33021;&#21487;&#38752;&#22320;&#25552;&#39640;&#25688;&#35201;&#27169;&#22411;&#22312;&#26410;&#26469;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent pre-trained language models (PLMs) achieve promising results in existing abstractive summarization datasets. However, existing summarization benchmarks overlap in time with the standard pre-training corpora and finetuning datasets. Hence, the strong performance of PLMs may rely on the parametric knowledge that is memorized during pre-training and fine-tuning. Moreover, the knowledge memorized by PLMs may quickly become outdated, which affects the generalization performance of PLMs on future data. In this work, we propose TempoSum, a novel benchmark that contains data samples from 2010 to 2022, to understand the temporal generalization ability of abstractive summarization models. Through extensive human evaluation, we show that parametric knowledge stored in summarization models significantly affects the faithfulness of the generated summaries on future data. Moreover, existing faithfulness enhancement methods cannot reliably improve the faithfulness of summarization models on fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; Doc2SoarGraph &#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#32467;&#26500;&#20013;&#20803;&#32032;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#30456;&#20851;&#24615;&#65292;&#22312;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26412;&#30340;TAT-DQA&#38382;&#39064;&#19979;&#23454;&#29616;&#20102;&#31163;&#25955;&#25512;&#29702;&#65292;&#34920;&#29616;&#20986;&#20102;&#26368;&#20339;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01938</link><description>&lt;p&gt;
Doc2SoarGraph&#65306;&#22522;&#20110;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#30340;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26723;&#30340;&#31163;&#25955;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs. (arXiv:2305.01938v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Doc2SoarGraph &#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#32467;&#26500;&#20013;&#20803;&#32032;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#30456;&#20851;&#24615;&#65292;&#22312;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26412;&#30340;TAT-DQA&#38382;&#39064;&#19979;&#23454;&#29616;&#20102;&#31163;&#25955;&#25512;&#29702;&#65292;&#34920;&#29616;&#20986;&#20102;&#26368;&#20339;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20004;&#24180;&#26469;&#65292;&#23545;&#20110;&#34920;&#26684;&#25991;&#26412;&#25991;&#26723;&#65288;&#20363;&#22914;&#36130;&#21153;&#25253;&#21578;&#65289;&#30340;&#31163;&#25955;&#25512;&#29702;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#22823;&#22810;&#36890;&#36807;&#25163;&#21160;&#36873;&#25321;&#21644;&#36716;&#25442;&#25991;&#26723;&#39029;&#38754;&#21040;&#32467;&#26500;&#21270;&#30340;&#34920;&#26684;&#21644;&#27573;&#33853;&#26469;&#31616;&#21270;&#36825;&#19968;&#25361;&#25112;&#65292;&#20174;&#32780;&#38459;&#30861;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#19968;&#31181;&#26356;&#20026;&#29616;&#23454;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#21363;&#20197; TAT-DQA &#30340;&#24418;&#24335;&#22238;&#31572;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26412;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; Doc2SoarGraph &#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#32467;&#26500;&#20013;&#19981;&#21516;&#20803;&#32032;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#20102;&#20854;&#31163;&#25955;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545; TAT-DQA &#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#31934;&#30830;&#21305;&#37197;&#65288;EM&#65289;&#21644; F1 &#24471;&#20998;&#26041;&#38754;&#20998;&#21035;&#27604;&#26368;&#20339;&#22522;&#32447;&#27169;&#22411;&#20998;&#21035;&#25552;&#39640;&#20102; 17.73% &#21644; 16.91%&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete reasoning over table-text documents (e.g., financial reports) gains increasing attention in recent two years. Existing works mostly simplify this challenge by manually selecting and transforming document pages to structured tables and paragraphs, hindering their practical application. In this work, we explore a more realistic problem setting in the form of TAT-DQA, i.e. to answer the question over a visually-rich table-text document. Specifically, we propose a novel Doc2SoarGraph framework with enhanced discrete reasoning capability by harnessing the differences and correlations among different elements (e.g., quantities, dates) of the given question and document with Semantic-oriented hierarchical Graph structures. We conduct extensive experiments on TAT-DQA dataset, and the results show that our proposed framework outperforms the best baseline model by 17.73% and 16.91% in terms of Exact Match (EM) and F1 score respectively on the test set, achieving the new state-of-the-art
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#26367;&#20195;&#20154;&#31867;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#20004;&#20010;NLP&#20219;&#21153;&#19978;&#65292;&#20351;&#29992;LLM&#35780;&#20272;&#21644;&#20154;&#31867;&#35780;&#20272;&#24471;&#21040;&#30340;&#32467;&#26524;&#26159;&#19968;&#33268;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.01937</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#26367;&#20195;&#20154;&#31867;&#35780;&#20272;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Be an Alternative to Human Evaluations?. (arXiv:2305.01937v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#26367;&#20195;&#20154;&#31867;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#20004;&#20010;NLP&#20219;&#21153;&#19978;&#65292;&#20351;&#29992;LLM&#35780;&#20272;&#21644;&#20154;&#31867;&#35780;&#20272;&#24471;&#21040;&#30340;&#32467;&#26524;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35780;&#20272;&#23545;&#20110;&#35780;&#20272;&#30001;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#25110;&#30001;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#36136;&#37327;&#26159;&#24517;&#19981;&#21487;&#23569;&#21644;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#35780;&#20272;&#38750;&#24120;&#38590;&#20197;&#37325;&#29616;&#65292;&#20854;&#36136;&#37327;&#20063;&#26159;&#38750;&#24120;&#19981;&#31283;&#23450;&#30340;&#65292;&#36825;&#38459;&#30861;&#20102;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#21644;&#31639;&#27861;&#20043;&#38388;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;&#26368;&#36817;&#65292;&#24403;&#21482;&#25552;&#20379;&#20219;&#21153;&#35828;&#26126;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24322;&#24120;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#30340;&#36825;&#31181;&#33021;&#21147;&#33021;&#21542;&#29992;&#20316;&#20154;&#31867;&#35780;&#20272;&#30340;&#26367;&#20195;&#21697;&#12290;&#25105;&#20204;&#21521;LLMs&#25552;&#20379;&#19982;&#36827;&#34892;&#20154;&#31867;&#35780;&#20272;&#30456;&#21516;&#30340;&#35828;&#26126;&#12289;&#24453;&#35780;&#20272;&#26679;&#26412;&#21644;&#38382;&#39064;&#65292;&#24182;&#35201;&#27714;LLMs&#23545;&#36825;&#20123;&#38382;&#39064;&#29983;&#25104;&#21709;&#24212;&#65307;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;LLM&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#31867;&#35780;&#20272;&#21644;LLM&#35780;&#20272;&#26469;&#35780;&#20272;&#20004;&#20010;NLP&#20219;&#21153;&#20013;&#30340;&#25991;&#26412;&#65306;&#24320;&#25918;&#24335;&#25925;&#20107;&#29983;&#25104;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#34920;&#26126;LLM&#35780;&#20272;&#30340;&#32467;&#26524;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks. We show that the result of LLM evaluation is consiste
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20219;&#21153;&#24863;&#30693;&#30340;&#29983;&#25104;&#24335;&#27169;&#22411;&#21644;&#19977;&#31181;&#38024;&#23545;&#20856;&#22411;&#20803;&#23398;&#20064;&#33539;&#30068;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#36798;&#21040;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01920</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;&#30340;&#29983;&#25104;&#24335;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generative Meta-Learning for Zero-Shot Relation Triplet Extraction. (arXiv:2305.01920v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01920
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20219;&#21153;&#24863;&#30693;&#30340;&#29983;&#25104;&#24335;&#27169;&#22411;&#21644;&#19977;&#31181;&#38024;&#23545;&#20856;&#22411;&#20803;&#23398;&#20064;&#33539;&#30068;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#36798;&#21040;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;&#20219;&#21153;&#26088;&#22312;&#20174;&#19968;&#20010;&#21253;&#21547;&#26410;&#35265;&#36807;&#20851;&#31995;&#31867;&#22411;&#30340;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#31995;&#19977;&#20803;&#32452;&#12290;&#27604;&#36739;&#26377;&#20195;&#34920;&#24615;&#30340;&#24037;&#20316;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#27169;&#22411;&#20026;&#26032;&#20851;&#31995;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#29983;&#25104;&#24335;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#32570;&#20047;&#23545;&#20110;&#27169;&#22411;&#27867;&#21270;&#21040;&#19981;&#21516;&#20219;&#21153;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#22240;&#27492;&#20855;&#26377;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20803;&#23398;&#20064;&#30340;&#8220;&#23398;&#20064;&#22914;&#20309;&#23398;&#20064;&#8221;&#30340;&#33021;&#21147;&#25552;&#39640;&#29983;&#25104;&#24335;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#20219;&#21153;&#24863;&#30693;&#30340;&#29983;&#25104;&#24335;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#24378;&#21046;&#36827;&#34892;&#20248;&#21270;&#36807;&#31243;&#26469;&#23398;&#20064;&#19968;&#33324;&#24615;&#30693;&#35782;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#38024;&#23545;&#19977;&#31867;&#20856;&#22411;&#20803;&#23398;&#20064;&#33539;&#30068;&#30340;&#29983;&#25104;&#24335;&#20803;&#23398;&#20064;&#26041;&#27861;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#38646;&#26679;&#26412;&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The zero-shot relation triplet extraction (ZeroRTE) task aims to extract relation triplets from a piece of text with unseen relation types. The seminal work adopts the pre-trained generative model to generate synthetic samples for new relations. However, current generative models lack the optimization process of model generalization on different tasks during training, and thus have limited generalization capability. For this reason, we propose a novel generative meta-learning framework which exploits the `learning-to-learn' ability of meta-learning to boost the generalization capability of generative models. Specifically, we first design a task-aware generative model which can learn the general knowledge by forcing the optimization process to be conducted across multiple tasks. Based on it, we then present three generative meta-learning approaches designated for three typical meta-learning categories. Extensive experimental results demonstrate that our framework achieves a new state-of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#65292;&#24182;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2305.01918</link><description>&lt;p&gt;
&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Improving Contrastive Learning of Sentence Embeddings from AI Feedback. (arXiv:2305.01918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#65292;&#24182;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#20013;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#30340;&#31163;&#25955;&#24615;&#20351;&#24471;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29983;&#25104;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#38590;&#20197;&#20445;&#35777;&#12290;&#34429;&#28982;&#26377;&#30417;&#30563;&#30340;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26631;&#31614;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26679;&#26412;&#23545;&#65292;&#20294;&#20173;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#21453;&#39304;&#26469;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;CLAIF&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;AI&#21453;&#39304;&#26500;&#24314;&#24102;&#26377;&#32454;&#31890;&#24230;&#26679;&#26412;&#30456;&#20284;&#24230;&#20998;&#25968;&#30340;&#26679;&#26412;&#23545;&#65292;&#20197;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32467;&#21512;&#20154;&#24037;&#21453;&#39304;&#21644;AI&#21453;&#39304;&#20026;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#26356;&#22909;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has become a popular approach in natural language processing, particularly for the learning of sentence embeddings. However, the discrete nature of natural language makes it difficult to ensure the quality of positive and negative sample pairs generated through data augmentation methods. Although supervised contrastive learning can produce more accurate sample pairs with human feedback labels, it still lacks fine-grained training signals. In this paper, we propose to improve \textbf{C}ontrastive \textbf{L}earning of sentence embeddings from \textbf{AI} \textbf{F}eedback \textbf{(CLAIF)}. Our method utilizes AI feedback from large pre-trained language models (LLMs) to construct sample pairs with fine-grained sample similarity scores to improve contrastive learning. Besides, we combine human feedback and AI feedback to provide better supervision signals for supervised contrastive learning of sentence embeddings. Experimental results show that our method achieves stat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24178;&#39044;&#30340;&#23567;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22686;&#37327;&#23398;&#20064;&#26469;&#20171;&#20837;&#21407;&#22411;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#24536;&#21364;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#19978;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01914</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#24178;&#39044;&#30340;&#23567;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Causal Interventions-based Few-Shot Named Entity Recognition. (arXiv:2305.01914v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24178;&#39044;&#30340;&#23567;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22686;&#37327;&#23398;&#20064;&#26469;&#20171;&#20837;&#21407;&#22411;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#24536;&#21364;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#19978;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#26088;&#22312;&#22522;&#20110;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65292;&#35782;&#21035;&#26032;&#31867;&#21035;&#30340;&#23454;&#20307;&#12290;&#22312;&#23567;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#19982;&#22823;&#37327;&#26679;&#26412;&#20219;&#21153;&#30456;&#27604;&#65292;&#31995;&#32479;&#23481;&#26131;&#20986;&#29616;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#36807;&#24230;&#25311;&#21512;&#20027;&#35201;&#26159;&#30001;&#23569;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23548;&#33268;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#24341;&#36215;&#30340;&#12290;&#20026;&#20102;&#32531;&#35299;&#23567;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24178;&#39044;&#30340;&#23567;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#36890;&#36807;&#21518;&#38376;&#35843;&#25972;&#20171;&#20837;&#19978;&#19979;&#25991;&#21644;&#21407;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#20171;&#20837;&#19968;&#20010;&#26679;&#26412;&#22330;&#26223;&#30340;&#19978;&#19979;&#25991;&#38750;&#24120;&#22256;&#38590;&#65292;&#22240;&#27492;&#25105;&#20204;&#36890;&#36807;&#22686;&#37327;&#23398;&#20064;&#26469;&#20171;&#20837;&#21407;&#22411;&#65292;&#36825;&#20063;&#21487;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#24536;&#21364;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65288;&#23545;&#20110;&#25152;&#26377;&#20219;&#21153;&#65292;&#24179;&#22343;&#33021;&#22815;&#23454;&#29616;29&#65285;&#30340;&#32477;&#23545;&#25552;&#39640;&#21644;12&#65285;&#30340;&#25552;&#39640;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot named entity recognition (NER) systems aims at recognizing new classes of entities based on a few labeled samples. A significant challenge in the few-shot regime is prone to overfitting than the tasks with abundant samples. The heavy overfitting in few-shot learning is mainly led by spurious correlation caused by the few samples selection bias. To alleviate the problem of the spurious correlation in the few-shot NER, in this paper, we propose a causal intervention-based few-shot NER method. Based on the prototypical network, the method intervenes in the context and prototype via backdoor adjustment during training. In particular, intervening in the context of the one-shot scenario is very difficult, so we intervene in the prototype via incremental learning, which can also avoid catastrophic forgetting. Our experiments on different benchmarks show that our approach achieves new state-of-the-art results (achieving up to 29% absolute improvement and 12% on average for all tasks).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20004;&#20010;&#23454;&#29992;&#30340;&#35774;&#32622;&#20986;&#21457;&#65292;&#20998;&#26512;&#27604;&#36739;&#20102;&#21313;&#31181;&#20195;&#34920;&#24615;&#30340;&#23567;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#24402;&#32435;&#24635;&#32467;&#20986;&#20102;&#21407;&#22411;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#36234;&#24615;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01901</link><description>&lt;p&gt;
&#23567;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#65306;&#32463;&#39564;&#30740;&#31350;&#21644;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Few-shot Event Detection: An Empirical Study and a Unified View. (arXiv:2305.01901v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20004;&#20010;&#23454;&#29992;&#30340;&#35774;&#32622;&#20986;&#21457;&#65292;&#20998;&#26512;&#27604;&#36739;&#20102;&#21313;&#31181;&#20195;&#34920;&#24615;&#30340;&#23567;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#24402;&#32435;&#24635;&#32467;&#20986;&#20102;&#21407;&#22411;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#36234;&#24615;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979; (ED) &#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#28982;&#32780;&#36825;&#20063;&#24102;&#26469;&#20102;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#20363;&#22914;&#21508;&#31181;&#21160;&#26426;&#12289;&#20219;&#21153;&#21644;&#23454;&#39564;&#35774;&#32622;&#65292;&#36825;&#20123;&#24046;&#24322;&#22952;&#30861;&#20102;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#26410;&#26469;&#36827;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#24443;&#24213;&#30340;&#32463;&#39564;&#30740;&#31350;&#12289;&#19968;&#20010;ED&#27169;&#22411;&#30340;&#32479;&#19968;&#35270;&#35282;&#21644;&#19968;&#20010;&#26356;&#22909;&#30340;&#32479;&#19968;&#22522;&#20934;&#32447;&#12290;&#20026;&#20102;&#20844;&#24179;&#35780;&#20272;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#20004;&#20010;&#23454;&#29992;&#30340;&#35774;&#32622;&#65306;&#20302;&#36164;&#28304;&#35774;&#32622;&#26469;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#21644;&#31867;&#36716;&#31227;&#35774;&#32622;&#26469;&#35780;&#20272;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21313;&#31181;&#20195;&#34920;&#24615;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22823;&#33268;&#34987;&#20998;&#20026;&#22522;&#20110;&#25552;&#31034;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#27169;&#22411;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#12290;&#20026;&#20102;&#35843;&#26597;&#22522;&#20110;&#21407;&#22411;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#25105;&#20204;&#20998;&#35299;&#20102;&#35774;&#35745;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#19981;&#20165;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#22312;&#20302;&#36164;&#28304;&#35774;&#32622;&#19979;&#33719;&#24471;2.7&#65285;F1&#25910;&#30410;&#65289;&#65292;&#32780;&#19988;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35768;&#22810;&#26377;&#20215;&#20540;&#30340;&#30740;&#31350;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot event detection (ED) has been widely studied, while this brings noticeable discrepancies, e.g., various motivations, tasks, and experimental settings, that hinder the understanding of models for future progress. This paper presents a thorough empirical study, a unified view of ED models, and a better unified baseline. For fair evaluation, we choose two practical settings: low-resource setting to assess generalization ability and class-transfer setting for transferability. We compare ten representative methods on three datasets, which are roughly grouped into prompt-based and prototype-based models for detailed analysis. To investigate the superior performance of prototype-based methods, we break down the design and build a unified framework. Based on that, we not only propose a simple yet effective method (e.g., 2.7% F1 gains under low-resource setting) but also offer many valuable research insights for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24544;&#23454;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#27604;&#25945;&#24072;&#27169;&#22411;&#22823;&#25968;&#20493;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#19968;&#20010;&#23567;&#30340;&#12289;&#33258;&#25105;&#19968;&#33268;&#30340;&#24605;&#36335;&#20018;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35777;&#26126;&#20915;&#31574;&#24182;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.01879</link><description>&lt;p&gt;
SCOTT: &#33258;&#25105;&#19968;&#33268;&#24615;&#24605;&#36335;&#20018;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
SCOTT: Self-Consistent Chain-of-Thought Distillation. (arXiv:2305.01879v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24544;&#23454;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#27604;&#25945;&#24072;&#27169;&#22411;&#22823;&#25968;&#20493;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#19968;&#20010;&#23567;&#30340;&#12289;&#33258;&#25105;&#19968;&#33268;&#30340;&#24605;&#36335;&#20018;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35777;&#26126;&#20915;&#31574;&#24182;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20986;&#19968;&#23450;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#36890;&#36807;&#19968;&#31995;&#21015;&#36830;&#32493;&#30340;&#24605;&#32771;&#36807;&#31243;&#33719;&#24471;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#30340;&#31361;&#20986;&#33021;&#21147;&#12290;&#34429;&#28982;&#24605;&#36335;&#20018;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20165;&#22312;&#36275;&#22815;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25165;&#33021;&#35266;&#23519;&#21040;&#36825;&#31181;&#25910;&#30410;&#12290;&#26356;&#20196;&#20154;&#25285;&#24551;&#30340;&#26159;&#65292;&#29983;&#25104;&#30340;&#29702;&#30001;&#24456;&#23569;&#20445;&#35777;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#20445;&#25345;&#19968;&#33268;&#25110;&#32773;&#24544;&#23454;&#22320;&#35777;&#26126;&#20915;&#31574;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24544;&#23454;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#27604;&#25945;&#24072;&#27169;&#22411;&#22823;&#25968;&#20493;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#19968;&#20010;&#23567;&#30340;&#12289;&#33258;&#25105;&#19968;&#33268;&#30340;&#24605;&#36335;&#20018;&#27169;&#22411;&#12290;&#20026;&#20102;&#24418;&#25104;&#26356;&#22909;&#30340;&#30417;&#30563;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#35299;&#30721;&#24341;&#23548;&#25945;&#24072;&#27169;&#22411;&#20135;&#29983;&#25903;&#25345;&#27491;&#30830;&#31572;&#26696;&#30340;&#29702;&#30001;&#65292;&#36825;&#40723;&#21169;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#30340;token&#21482;&#22312;&#32771;&#34385;&#21040;&#31572;&#26696;&#26102;&#25165;&#26356;&#21152;&#21487;&#20449;&#12290;&#20026;&#20102;&#20445;&#35777;&#24544;&#23454;&#30340;&#33976;&#39311;&#65292;&#25105;&#20204;&#20351;&#29992;&#25945;&#24072;&#29983;&#25104;&#30340;&#29702;&#30001;&#26469;&#23398;&#20064;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21453;&#20107;&#23454;&#25512;&#29702;&#30446;&#26631;&#65292;&#21363;&#26681;&#25454;&#20855;&#26377;&#33258;&#25105;&#19968;&#33268;&#24615;&#19988;&#24544;&#23454;&#20110;&#25945;&#24072;&#39044;&#27979;&#30340;&#24605;&#36335;&#20018;&#29702;&#30001;&#39044;&#27979;&#20915;&#31574;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#25277;&#35937;&#25688;&#35201;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23398;&#29983;&#27169;&#22411;&#20013;&#30340;&#33258;&#25105;&#19968;&#33268;&#24615;&#26377;&#21161;&#20110;&#35777;&#26126;&#20915;&#31574;&#24182;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (CoT) prompting. While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM's predictions or faithfully justify the decisions. In this work, we propose a faithful knowledge distillation method to learn a small, self-consistent CoT model from a teacher model that is orders of magnitude larger. To form better supervision, we elicit rationales supporting the gold answers from a large LM (teacher) by contrastive decoding, which encourages the teacher to generate tokens that become more plausible only when the answer is considered. To ensure faithful distillation, we use the teacher-generated rationales to learn a student LM with a counterfactual reasoning objective, which pre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01876</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#21477;&#23376;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Causality-aware Concept Extraction based on Knowledge-guided Prompting. (arXiv:2305.01876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#26377;&#21161;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20294;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#36828;&#26410;&#23436;&#21892;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#27010;&#24565;&#25552;&#21462;&#65288;CE&#65289;&#12290;&#28982;&#32780;&#65292;PLM&#24448;&#24448;&#20174;&#22823;&#37327;&#35821;&#26009;&#24211;&#30340;&#20849;&#29616;&#20851;&#32852;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#30693;&#35782;&#25366;&#25496;&#65292;&#32780;&#38750;Token&#20043;&#38388;&#30340;&#30495;&#23454;&#22240;&#26524;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#30693;&#35782;&#28151;&#28102;&#20102;PLM&#65292;&#23548;&#33268;&#25552;&#21462;&#22522;&#20110;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#30340;&#26377;&#20559;&#27010;&#24565;&#65292;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20302;&#31934;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;PLM&#30340;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#20943;&#36731;&#27010;&#24565;&#20559;&#24046;&#12290;&#25552;&#31034;&#37319;&#29992;&#29616;&#26377;KG&#20013;&#30340;&#32473;&#23450;&#23454;&#20307;&#20027;&#39064;&#26469;&#32531;&#35299;&#23454;&#20307;&#21644;&#26377;&#20559;&#27010;&#24565;&#20043;&#38388;&#30340;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#26174;&#33879;&#25913;&#36827;&#20102;&#25552;&#21462;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concepts benefit natural language understanding but are far from complete in existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs) have been widely used in text-based concept extraction (CE). However, PLMs tend to mine the co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens.As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations, inevitably resulting in low precision. In this paper, through the lens of a Structural Causal Model (SCM), we propose equipping the PLM-based extractor with a knowledge-guided prompt as an intervention to alleviate concept bias. The prompt adopts the topic of the given entity from the existing knowledge in KGs to mitigate the spurious co-occurrence correlations between entities and biased concepts. Our extensive experiments on representative multilingual KG datasets justify that our proposed prompt 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GPTutor&#30340;ChatGPT&#21160;&#21147;&#32534;&#31243;&#24037;&#20855;&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;ChatGPT API&#30340;Visual Studio Code&#25193;&#23637;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#35789;&#65292;&#21487;&#20197;&#23545;&#25152;&#36873;&#20195;&#30721;&#36827;&#34892;&#31934;&#31616;&#12289;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.01863</link><description>&lt;p&gt;
GPTutor: &#19968;&#31181;&#30001;ChatGPT&#39537;&#21160;&#30340;&#32534;&#31243;&#24037;&#20855;&#65292;&#29992;&#20110;&#31243;&#24207;&#20195;&#30721;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
GPTutor: a ChatGPT-powered programming tool for code explanation. (arXiv:2305.01863v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GPTutor&#30340;ChatGPT&#21160;&#21147;&#32534;&#31243;&#24037;&#20855;&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;ChatGPT API&#30340;Visual Studio Code&#25193;&#23637;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#35789;&#65292;&#21487;&#20197;&#23545;&#25152;&#36873;&#20195;&#30721;&#36827;&#34892;&#31934;&#31616;&#12289;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26032;&#30340;&#32534;&#31243;&#25216;&#33021;&#38656;&#35201;&#20010;&#24615;&#21270;&#25351;&#23548;&#12290;&#38543;&#30528;ChatGPT API&#31561;&#39640;&#32423;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#29616;&#22312;&#26377;&#21487;&#33021;&#21019;&#24314;&#19968;&#20010;&#26041;&#20415;&#30340;&#12289;&#20010;&#24615;&#21270;&#30340;AI&#32534;&#31243;&#25945;&#32946;&#36741;&#23548;&#31995;&#32479;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GPTutor&#30340;ChatGPT&#21160;&#21147;&#32534;&#31243;&#24037;&#20855;&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;ChatGPT API&#30340;Visual Studio Code&#25193;&#23637;&#65292;&#29992;&#20110;&#25552;&#20379;&#32534;&#31243;&#20195;&#30721;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning new programming skills requires tailored guidance. With the emergence of advanced Natural Language Generation models like the ChatGPT API, there is now a possibility of creating a convenient and personalized tutoring system with AI for computer science education. This paper presents GPTutor, a ChatGPT-powered programming tool, which is a Visual Studio Code extension using the ChatGPT API to provide programming code explanations. By integrating Visual Studio Code API, GPTutor can comprehensively analyze the provided code by referencing the relevant source codes. As a result, GPTutor can use designed prompts to explain the selected code with a pop-up message. GPTutor is now published at the Visual Studio Code Extension Marketplace, and its source code is openly accessible on GitHub. Preliminary evaluation indicates that GPTutor delivers the most concise and accurate explanations compared to vanilla ChatGPT and GitHub Copilot. Moreover, the feedback from students and teachers ind
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20877;&#27425;&#23581;&#35797;&#25918;&#24323;&#23454;&#20363;&#30340;&#21518;&#26399;&#22788;&#29702;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#35206;&#30422;&#33539;&#22260;&#32780;&#19981;&#26174;&#30528;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01812</link><description>&lt;p&gt;
&#21518;&#25918;&#24323;&#65306;&#20851;&#20110;&#22312;&#38382;&#31572;&#20013;&#21487;&#38752;&#22320;&#37325;&#26032;&#23581;&#35797;&#25918;&#24323;&#23454;&#20363;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA. (arXiv:2305.01812v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20877;&#27425;&#23581;&#35797;&#25918;&#24323;&#23454;&#20363;&#30340;&#21518;&#26399;&#22788;&#29702;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#35206;&#30422;&#33539;&#22260;&#32780;&#19981;&#26174;&#30528;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20063;&#32463;&#24120;&#20570;&#20986;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#36825;&#20123;&#39044;&#27979;&#20250;&#24433;&#21709;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#38480;&#21046;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#37319;&#29992;&#12290;&#36873;&#25321;&#24615;&#39044;&#27979;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#20854;&#39044;&#27979;&#21487;&#33021;&#19981;&#27491;&#30830;&#26102;&#25918;&#24323;&#22238;&#31572;&#26469;&#37096;&#20998;&#22320;&#35299;&#20915;&#20102;&#19978;&#36848;&#38382;&#39064;&#12290;&#34429;&#28982;&#36873;&#25321;&#24615;&#39044;&#27979;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#23427;&#30041;&#19979;&#20102;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#8220;&#25918;&#24323;&#21518;&#35813;&#24590;&#20040;&#21150;&#8221;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#20851;&#20110;&#8220;&#21518;&#25918;&#24323;&#8221;&#30340;&#30740;&#31350;&#65292;&#35813;&#20219;&#21153;&#20801;&#35768;&#37325;&#26032;&#23581;&#35797;&#25918;&#24323;&#23454;&#20363;&#65292;&#20197;&#22686;&#21152;&#31995;&#32479;&#30340;&#8220;&#35206;&#30422;&#29575;&#8221;&#32780;&#19981;&#26174;&#30528;&#29306;&#29298;&#20854;&#8220;&#20934;&#30830;&#24615;&#8221;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#35813;&#20219;&#21153;&#30340;&#25968;&#23398;&#20844;&#24335;&#65292;&#28982;&#21518;&#25506;&#35752;&#20102;&#20960;&#31181;&#35299;&#20915;&#26041;&#27861;&#12290;&#23545;11&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#23548;&#33268;&#20102;&#30456;&#24403;&#22823;&#30340;&#39118;&#38505;&#25913;&#36827;&#8212;&#8212;&#21518;&#25918;&#24323;&#20219;&#21153;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite remarkable progress made in natural language processing, even the state-of-the-art models often make incorrect predictions. Such predictions hamper the reliability of systems and limit their widespread adoption in real-world applications. 'Selective prediction' partly addresses the above concern by enabling models to abstain from answering when their predictions are likely to be incorrect. While selective prediction is advantageous, it leaves us with a pertinent question 'what to do after abstention'. To this end, we present an explorative study on 'Post-Abstention', a task that allows re-attempting the abstained instances with the aim of increasing 'coverage' of the system without significantly sacrificing its 'accuracy'. We first provide mathematical formulation of this task and then explore several methods to solve it. Comprehensive experiments on 11 QA datasets show that these methods lead to considerable risk improvements -- performance metric of the Post-Abstention task -
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;KEPLET&#65292;&#22312;&#39044;&#35757;&#32451;&#35821;&#26009;&#20013;&#21152;&#20837;&#20027;&#39064;&#23454;&#20307;&#24863;&#30693;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#23454;&#20307;&#20132;&#20114;&#21644;&#35789;&#35821;&#35821;&#20041;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.01810</link><description>&lt;p&gt;
KEPLET: &#19968;&#31181;&#24102;&#26377;&#20027;&#39064;&#23454;&#20307;&#24863;&#30693;&#30340;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
KEPLET: Knowledge-Enhanced Pretrained Language Model with Topic Entity Awareness. (arXiv:2305.01810v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;KEPLET&#65292;&#22312;&#39044;&#35757;&#32451;&#35821;&#26009;&#20013;&#21152;&#20837;&#20027;&#39064;&#23454;&#20307;&#24863;&#30693;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#23454;&#20307;&#20132;&#20114;&#21644;&#35789;&#35821;&#35821;&#20041;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#36890;&#36807;&#22312;&#26410;&#32467;&#26500;&#21270;&#25991;&#26412;&#35821;&#26009;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#20197;&#26174;&#31034;&#20854;&#20248;&#36234;&#24615;&#12290;&#22312;&#23454;&#20307;&#20016;&#23500;&#30340;&#25991;&#26412;&#36164;&#28304;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#65289;&#20013;&#65292;&#30693;&#35782;&#22686;&#24378;&#30340;PLM&#65288;KEPLMs&#65289;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;&#26631;&#35760;&#19982;&#25152;&#25552;&#21450;&#30340;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#32467;&#21512;&#36215;&#26469;&#65292;&#22240;&#27492;&#22312;&#23454;&#20307;&#20013;&#24515;&#20219;&#21153;&#65288;&#22914;&#23454;&#20307;&#38142;&#25509;&#21644;&#20851;&#31995;&#20998;&#31867;&#65289;&#19978;&#26356;&#26377;&#25928;&#12290;&#34429;&#28982;&#20256;&#32479;KEPLM&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21033;&#29992;&#20102;&#32500;&#22522;&#30334;&#31185;&#30340;&#20016;&#23500;&#32467;&#26500;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#24573;&#30053;&#20102;&#27599;&#20010;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#22260;&#32469;&#19968;&#20010;&#20027;&#39064;&#23454;&#20307;&#30340;&#29420;&#29305;&#24067;&#23616;&#65288;&#30001;&#39029;&#38754;URL&#26631;&#35782;&#24182;&#22312;&#39029;&#38754;&#26631;&#39064;&#20013;&#26174;&#31034;&#65289;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;&#22914;&#26524;&#19981;&#21152;&#20837;&#20027;&#39064;&#23454;&#20307;&#65292;KEPLM&#23558;&#23548;&#33268;&#23454;&#20307;&#20132;&#20114;&#19981;&#36275;&#21644;&#20559;&#24046;&#65288;&#20851;&#31995;&#65289;&#35789;&#35821;&#35821;&#20041;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KEPLET&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#20027;&#39064;&#23454;&#20307;&#24863;&#30693;&#30340;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#65292;KEPLET&#30830;&#23450;&#39044;&#35757;&#32451;&#35821;&#26009;&#20013;&#30340;&#20027;&#39064;&#23454;&#20307;&#65292;&#24182;&#25913;&#36827;&#20102;KEPLM&#30340;&#23454;&#20307;&#20132;&#20114;&#21644;&#35789;&#35821;&#35821;&#20041;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Pre-trained Language Models (PLMs) have shown their superiority by pre-training on unstructured text corpus and then fine-tuning on downstream tasks. On entity-rich textual resources like Wikipedia, Knowledge-Enhanced PLMs (KEPLMs) incorporate the interactions between tokens and mentioned entities in pre-training, and are thus more effective on entity-centric tasks such as entity linking and relation classification. Although exploiting Wikipedia's rich structures to some extent, conventional KEPLMs still neglect a unique layout of the corpus where each Wikipedia page is around a topic entity (identified by the page URL and shown in the page title). In this paper, we demonstrate that KEPLMs without incorporating the topic entities will lead to insufficient entity interaction and biased (relation) word semantics. We thus propose KEPLET, a novel Knowledge-Enhanced Pre-trained LanguagE model with Topic entity awareness. In an end-to-end manner, KEPLET identifies where to a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MPP&#30340;&#20219;&#21153;&#65292;&#29992;&#20110;&#20351;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#35745;&#21010;&#65292;&#24110;&#21161;&#20154;&#31867;&#23436;&#25104;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36328;&#27169;&#24577;&#30340;&#35745;&#21010;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27169;&#24577;&#25552;&#31034;&#26041;&#27861;TIP&#65292;&#23427;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.01795</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#25991;&#26412;-&#22270;&#20687;&#25552;&#31034;&#30340;&#22810;&#27169;&#24577;&#36807;&#31243;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Multimodal Procedural Planning via Dual Text-Image Prompting. (arXiv:2305.01795v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01795
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MPP&#30340;&#20219;&#21153;&#65292;&#29992;&#20110;&#20351;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#35745;&#21010;&#65292;&#24110;&#21161;&#20154;&#31867;&#23436;&#25104;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36328;&#27169;&#24577;&#30340;&#35745;&#21010;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27169;&#24577;&#25552;&#31034;&#26041;&#27861;TIP&#65292;&#23427;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#20195;&#29702;&#24050;&#32463;&#22312;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#23436;&#25104;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26480;&#20986;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#25552;&#20379;&#25351;&#23548;&#20197;&#24110;&#21161;&#20154;&#31867;&#23436;&#25104;&#20219;&#21153;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#36807;&#31243;&#35268;&#21010;&#65288;MPP&#65289;&#20219;&#21153;&#65292;&#20854;&#20013;&#27169;&#22411;&#32473;&#20986;&#20102;&#39640;&#32423;&#30446;&#26631;&#24182;&#29983;&#25104;&#37197;&#23545;&#30340;&#25991;&#26412;-&#22270;&#20687;&#27493;&#39588;&#30340;&#35745;&#21010;&#65292;&#25552;&#20379;&#27604;&#21333;&#27169;&#24577;&#35745;&#21010;&#26356;&#20016;&#23500;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#25351;&#23548;&#12290;MPP&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#30830;&#20445;&#36328;&#27169;&#24577;&#30340;&#35745;&#21010;&#22312;&#20449;&#24687;&#24615;&#65292;&#26102;&#38388;&#36830;&#36143;&#24615;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;-&#22270;&#20687;&#25552;&#31034;&#65288;TIP&#65289;&#65292;&#19968;&#31181;&#21452;&#27169;&#24577;&#25552;&#31034;&#26041;&#27861;&#65292;&#23427;&#21516;&#26102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#30340;&#38646;-shot&#25512;&#29702;&#33021;&#21147;&#21644;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#20013;&#20196;&#20154;&#20449;&#26381;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#12290;TIP&#20351;&#29992;&#25991;&#26412;-&#22270;&#20687;&#26725;&#21644;&#22270;&#20687;-&#25991;&#26412;&#26725;&#25913;&#21892;&#20102;&#21452;&#27169;&#24577;&#20132;&#20114;&#65292;&#20351;LLMs&#24341;&#23548;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied agents have achieved prominent performance in following human instructions to complete tasks. However, the potential of providing instructions informed by texts and images to assist humans in completing tasks remains underexplored. To uncover this capability, we present the multimodal procedural planning (MPP) task, in which models are given a high-level goal and generate plans of paired text-image steps, providing more complementary and informative guidance than unimodal plans. The key challenges of MPP are to ensure the informativeness, temporal coherence,and accuracy of plans across modalities. To tackle this, we propose Text-Image Prompting (TIP), a dual-modality prompting method that jointly leverages zero-shot reasoning ability in large language models (LLMs) and compelling text-to-image generation ability from diffusion-based models. TIP improves the interaction in the dual modalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs to guide the textua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#35789;&#27719;&#30693;&#35782;&#24211;&#30340;&#35789;&#20041;&#20449;&#24687;&#26469;&#35299;&#20915;&#21407;&#26469;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#20013;&#30340;&#22810;&#20041;&#35789;&#38382;&#39064;&#12290;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#21152;&#20837;&#35789;&#20041;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340; GPT-3 &#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#35789;&#20856;&#22806;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01788</link><description>&lt;p&gt;
&#35270;&#35273;&#19982;&#23450;&#20041;&#30456;&#36935;&#65306;&#34701;&#21512;&#35789;&#20041;&#20449;&#24687;&#30340;&#26080;&#30417;&#30563;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;
&lt;/p&gt;
&lt;p&gt;
Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information. (arXiv:2305.01788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#35789;&#27719;&#30693;&#35782;&#24211;&#30340;&#35789;&#20041;&#20449;&#24687;&#26469;&#35299;&#20915;&#21407;&#26469;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#20013;&#30340;&#22810;&#20041;&#35789;&#38382;&#39064;&#12290;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#21152;&#20837;&#35789;&#20041;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340; GPT-3 &#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#35789;&#20856;&#22806;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26159;&#19968;&#39033;&#20219;&#21153;&#65292;&#26088;&#22312;&#25214;&#21040;&#26368;&#20934;&#30830;&#22320;&#25551;&#36848;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#30446;&#26631;&#35789;&#27491;&#30830;&#24847;&#20041;&#30340;&#22270;&#20687;&#12290;&#20197;&#24448;&#30340;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#24448;&#24448;&#21463;&#21040;&#35789;&#20041;&#22810;&#20041;&#24615;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22806;&#37096;&#35789;&#27719;&#30693;&#35782;&#24211;&#30340;&#35789;&#27719;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#35789;&#20041;&#23450;&#20041;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#27809;&#26377;&#25552;&#20379;&#31572;&#26696;&#30340;&#35789;&#20041;&#20449;&#24687;&#26102;&#65292;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#21152;&#20837;&#35789;&#20041;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25913;&#36827;&#35789;&#20856;&#22806;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;GPT-3&#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#26126;&#26174;&#25552;&#39640;&#20102;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#22312;&#35789;&#20856;&#22806;&#20363;&#23376;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words. This paper introduces an unsupervised VWSD approach that uses gloss information of an external lexical knowledge-base, especially the sense definitions. Specifically, we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we propose a context-aware definition generation with GPT-3. Experimental results show that the VWSD performance significantly increased with our Bayesian inference-based approach. In addition, our context-aware definition generation achieved prominent performance improvement in OOD examples exhibiting better performance than the existing definition generation method. We will publish source 
&lt;/p&gt;</description></item><item><title>SLTUNET&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#20026;&#22810;&#20010;&#25163;&#35821;&#32763;&#35793;&#20219;&#21153;&#25552;&#20379;&#25903;&#25345;&#12290;&#36890;&#36807;&#32852;&#21512;&#24314;&#27169;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;SLTUNET&#21487;&#20197;&#25506;&#32034;&#36328;&#20219;&#21153;&#30456;&#20851;&#24615;&#20197;&#32553;&#23567;&#27169;&#24577;&#24046;&#36317;&#65292;&#24182;&#20511;&#21161;&#22806;&#37096;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#31454;&#20105;&#29978;&#33267;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01778</link><description>&lt;p&gt;
SLTUNET&#65306;&#19968;&#31181;&#31616;&#21333;&#30340;&#32479;&#19968;&#27169;&#22411;&#36827;&#34892;&#25163;&#35821;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
SLTUNET: A Simple Unified Model for Sign Language Translation. (arXiv:2305.01778v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01778
&lt;/p&gt;
&lt;p&gt;
SLTUNET&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#20026;&#22810;&#20010;&#25163;&#35821;&#32763;&#35793;&#20219;&#21153;&#25552;&#20379;&#25903;&#25345;&#12290;&#36890;&#36807;&#32852;&#21512;&#24314;&#27169;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;SLTUNET&#21487;&#20197;&#25506;&#32034;&#36328;&#20219;&#21153;&#30456;&#20851;&#24615;&#20197;&#32553;&#23567;&#27169;&#24577;&#24046;&#36317;&#65292;&#24182;&#20511;&#21161;&#22806;&#37096;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#31454;&#20105;&#29978;&#33267;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#31070;&#32463;&#27169;&#22411;&#22312;&#25163;&#35821;&#32763;&#35793;&#65288;SLT&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#25163;&#35821;&#35270;&#39057;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#65292;&#32763;&#35793;&#36136;&#37327;&#20173;&#28982;&#33853;&#21518;&#20110;&#21475;&#35821;&#35821;&#35328;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#27169;&#24577;&#34920;&#31034;&#20849;&#20139;&#30340;&#31574;&#30053;&#65292;&#25552;&#20986;&#20102;SLTUNET&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#25903;&#25345;&#22810;&#20010;SLT&#30456;&#20851;&#20219;&#21153;&#30340;&#31616;&#21333;&#32479;&#19968;&#31070;&#32463;&#27169;&#22411;&#65292;&#20363;&#22914;&#25163;&#35821;&#21040;&#25163;&#35821;&#32534;&#30721;&#12289;&#25163;&#35821;&#32534;&#30721;&#21040;&#25991;&#26412;&#12289;&#25163;&#35821;&#21040;&#25991;&#26412;&#32763;&#35793;&#12290;&#32852;&#21512;&#24314;&#27169;&#19981;&#21516;&#20219;&#21153;&#36171;&#20104;SLTUNET&#25506;&#32034;&#26377;&#21161;&#20110;&#32553;&#23567;&#27169;&#24577;&#24046;&#36317;&#30340;&#36328;&#20219;&#21153;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#22806;&#37096;&#36164;&#28304;&#30340;&#30693;&#35782;&#65292;&#20363;&#22914;&#29992;&#20110;&#21475;&#35821;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#30340;&#20016;&#23500;&#24179;&#34892;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SLTUNET&#22312;&#22686;&#21152;MT&#25968;&#25454;&#24182;&#37197;&#22791;&#19968;&#32452;&#20248;&#21270;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#22312;PHOENIX-2014T&#21644;CSL-Daily&#19978;&#23454;&#29616;&#31454;&#20105;&#29978;&#33267;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent successes with neural models for sign language translation (SLT), translation quality still lags behind spoken languages because of the data scarcity and modality gap between sign video and text. To address both problems, we investigate strategies for cross-modality representation sharing for SLT. We propose SLTUNET, a simple unified neural model designed to support multiple SLTrelated tasks jointly, such as sign-to-gloss, gloss-to-text and sign-to-text translation. Jointly modeling different tasks endows SLTUNET with the capability to explore the cross-task relatedness that could help narrow the modality gap. In addition, this allows us to leverage the knowledge from external resources, such as abundant parallel data used for spoken-language machine translation (MT). We show in experiments that SLTUNET achieves competitive and even state-of-the-art performance on PHOENIX-2014T and CSL-Daily when augmented with MT data and equipped with a set of optimization techniques. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#22240;&#26524;&#25552;&#31034;&#35821;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#20154;&#31867;&#30340;&#24515;&#29702;&#36807;&#31243;&#12290;&#36825;&#20123;&#25552;&#31034;&#35821;&#21487;&#20197;&#29992;&#26469;&#20135;&#29983;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.01764</link><description>&lt;p&gt;
&#21463;&#24515;&#29702;&#23398;&#21551;&#21457;&#30340;&#22240;&#26524;&#25552;&#31034;&#35821;
&lt;/p&gt;
&lt;p&gt;
Psychologically-Inspired Causal Prompts. (arXiv:2305.01764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#22240;&#26524;&#25552;&#31034;&#35821;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#20154;&#31867;&#30340;&#24515;&#29702;&#36807;&#31243;&#12290;&#36825;&#20123;&#25552;&#31034;&#35821;&#21487;&#20197;&#29992;&#26469;&#20135;&#29983;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#25968;&#25454;&#38598;&#19981;&#20165;&#20165;&#21547;&#26377;&#36755;&#20837;&#36755;&#20986;&#23545;&#65292;&#36824;&#21253;&#21547;&#36755;&#20837;&#21644;&#36755;&#20986;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#26412;&#25991;&#20197;&#24773;&#24863;&#20998;&#31867;&#20026;&#20363;&#65292;&#25506;&#35752;&#35780;&#35770;&#65288;X&#65289;&#21644;&#24773;&#24863;&#65288;Y&#65289;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#24515;&#29702;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#21487;&#20197;&#24433;&#21709;&#24773;&#32490;&#65292;&#24403;&#19968;&#20010;&#20154;&#39318;&#27425;&#36827;&#34892;&#35780;&#20998;&#24182;&#22312;&#35780;&#35770;&#20013;&#36827;&#34892;&#33258;&#25105;&#21512;&#29702;&#21270;&#26102;&#65288;&#24773;&#24863;&#24341;&#36215;&#35780;&#35770;&#65292;&#21363;Y-&gt;X&#65289;&#65292;&#19982;&#39318;&#20808;&#25551;&#36848;&#33258;&#24049;&#30340;&#32463;&#21382;&#24182;&#26435;&#34913;&#21033;&#24330;&#20197;&#20570;&#20986;&#26368;&#21518;&#35780;&#20998;&#26102;&#65288;&#35780;&#35770;&#24341;&#36215;&#24773;&#24863;&#65292;&#21363;X-&gt;Y&#65289;&#65292;&#20250;&#24341;&#21457;&#19981;&#21516;&#30340;&#24515;&#29702;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#35780;&#27880;&#32773;&#36890;&#36807;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#25512;&#26029;&#29992;&#25143;&#30340;&#21407;&#22987;&#35780;&#20998;&#65292;&#21017;&#36825;&#20063;&#26159;&#23436;&#20840;&#19981;&#21516;&#30340;&#24515;&#29702;&#36807;&#31243;&#65288;&#35780;&#35770;&#24341;&#36215;&#35780;&#20998;&#65292;&#21363;X-ToM-&gt; Y&#65289;&#12290;&#26412;&#25991;&#23558;&#36825;&#19977;&#31181;&#24773;&#24863;&#20998;&#31867;&#30340;&#20154;&#31867;&#24515;&#29702;&#36807;&#31243;&#30340;&#22240;&#26524;&#26426;&#21046;&#36716;&#21270;&#20026;&#19977;&#20010;&#25552;&#31034;&#35821;&#65292;&#24182;&#22312;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#24212;&#29992;&#36825;&#20123;&#25552;&#31034;&#35821;&#65292;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLP datasets are richer than just input-output pairs; rather, they carry causal relations between the input and output variables. In this work, we take sentiment classification as an example and look into the causal relations between the review (X) and sentiment (Y). As psychology studies show that language can affect emotion, different psychological processes are evoked when a person first makes a rating and then self-rationalizes their feeling in a review (where the sentiment causes the review, i.e., Y -&gt; X), versus first describes their experience, and weighs the pros and cons to give a final rating (where the review causes the sentiment, i.e., X -&gt; Y ). Furthermore, it is also a completely different psychological process if an annotator infers the original rating of the user by theory of mind (ToM) (where the review causes the rating, i.e., X -ToM-&gt; Y ). In this paper, we verbalize these three causal mechanisms of human psychological processes of sentiment classification into three
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;VoicePrivacy 2020 Challenge&#20013;&#28436;&#35762;&#32773;&#21311;&#21517;&#21270;&#23545;&#24773;&#24863;&#35821;&#38899;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20854;&#26410;&#33021;&#26377;&#25928;&#22320;&#20445;&#25252;&#28436;&#35762;&#32773;&#30340;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2305.01759</link><description>&lt;p&gt;
&#28436;&#35762;&#32773;&#21311;&#21517;&#21270;&#23545;&#24773;&#24863;&#35821;&#38899;&#30340;&#24433;&#21709;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Speaker Anonymization on Emotional Speech. (arXiv:2305.01759v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;VoicePrivacy 2020 Challenge&#20013;&#28436;&#35762;&#32773;&#21311;&#21517;&#21270;&#23545;&#24773;&#24863;&#35821;&#38899;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20854;&#26410;&#33021;&#26377;&#25928;&#22320;&#20445;&#25252;&#28436;&#35762;&#32773;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#25968;&#25454;&#25658;&#24102;&#30528;&#20010;&#20154;&#30340;&#20449;&#24687;&#65292;&#22914;&#28436;&#35762;&#32773;&#30340;&#36523;&#20221;&#21644;&#24773;&#24863;&#29366;&#24577;&#12290;&#36825;&#20123;&#23646;&#24615;&#21487;&#33021;&#34987;&#29992;&#20110;&#24694;&#24847;&#30446;&#30340;&#12290;&#38543;&#30528;&#34394;&#25311;&#21161;&#25163;&#30340;&#21457;&#23637;&#65292;&#20986;&#29616;&#20102;&#26032;&#19968;&#20195;&#30340;&#38544;&#31169;&#23041;&#32961;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;&#20445;&#25252;&#35821;&#38899;&#38544;&#31169;&#30340;&#35805;&#39064;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;VoicePrivacy&#20513;&#35758;&#65292;&#26088;&#22312;&#20419;&#36827;&#20026;&#35821;&#38899;&#25216;&#26415;&#24320;&#21457;&#38544;&#31169;&#20445;&#25252;&#24037;&#20855;&#12290;VoicePrivacy 2020&#25361;&#25112;&#36187;&#65288;VPC&#65289;&#36873;&#23450;&#30340;&#20219;&#21153;&#26159;&#28436;&#35762;&#32773;&#21311;&#21517;&#21270;&#12290;&#30446;&#26631;&#26159;&#38544;&#34255;&#28304;&#28436;&#35762;&#32773;&#30340;&#36523;&#20221;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#35328;&#20449;&#24687;&#12290;VPC&#30340;&#22522;&#20934;&#32447;&#20351;&#29992;&#20102;&#35821;&#38899;&#36716;&#25442;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;VPC&#22522;&#20934;&#32447;&#31995;&#32479;&#23545;&#35821;&#38899;&#35805;&#35821;&#20013;&#24773;&#24863;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#25353;&#29031;&#25915;&#20987;&#32773;&#23545;&#21311;&#21517;&#21270;&#31995;&#32479;&#30340;&#20102;&#35299;&#65292;&#25191;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;VPC&#22522;&#20934;&#32447;&#31995;&#32479;&#26410;&#33021;&#26377;&#25928;&#22320;&#21311;&#21517;&#21270;&#35821;&#38899;&#20013;&#30340;&#24773;&#24863;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#28436;&#35762;&#32773;&#30340;&#38544;&#31169;&#36896;&#25104;&#28508;&#22312;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech data carries a range of personal information, such as the speaker's identity and emotional state. These attributes can be used for malicious purposes. With the development of virtual assistants, a new generation of privacy threats has emerged. Current studies have addressed the topic of preserving speech privacy. One of them, the VoicePrivacy initiative aims to promote the development of privacy preservation tools for speech technology. The task selected for the VoicePrivacy 2020 Challenge (VPC) is about speaker anonymization. The goal is to hide the source speaker's identity while preserving the linguistic information. The baseline of the VPC makes use of a voice conversion. This paper studies the impact of the speaker anonymization baseline system of the VPC on emotional information present in speech utterances. Evaluation is performed following the VPC rules regarding the attackers' knowledge about the anonymization system. Our results show that the VPC baseline system does n
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;KB-BINDER&#26694;&#26550;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#30693;&#35782;&#24211;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#32972;&#26223;&#23398;&#20064;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;KBQA&#38382;&#39064;&#30340;&#21487;&#35299;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01750</link><description>&lt;p&gt;
&#22522;&#20110;&#23569;&#26679;&#26412;&#32972;&#26223;&#23398;&#20064;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Few-shot In-context Learning for Knowledge Base Question Answering. (arXiv:2305.01750v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;KB-BINDER&#26694;&#26550;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#30693;&#35782;&#24211;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#32972;&#26223;&#23398;&#20064;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;KBQA&#38382;&#39064;&#30340;&#21487;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#38590;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#24212;&#23545;&#21508;&#31181;&#21487;&#33021;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#30693;&#35782;&#24211;&#26550;&#26500;&#39033;&#20043;&#38388;&#30340;&#24322;&#26500;&#24615;&#36890;&#24120;&#38656;&#35201;&#38024;&#23545;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#19987;&#38376;&#30340;&#35757;&#32451;&#12290;&#20026;&#20102;&#22788;&#29702;&#22810;&#31181;KBQA&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KB-BINDER&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#36827;&#34892;&#23569;&#37327;&#26679;&#26412;&#30340;&#32972;&#26223;&#23398;&#20064;&#65292;&#24182;&#23558;&#19981;&#21516;&#30340;KBQA&#25968;&#25454;&#38598;&#32479;&#19968;&#12290;&#39318;&#20808;&#65292;KB-BINDER&#21033;&#29992;&#20687;Codex&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#27169;&#20223;&#23569;&#37327;&#28436;&#31034;&#26469;&#29983;&#25104;&#29305;&#23450;&#38382;&#39064;&#30340;&#36923;&#36753;&#24418;&#24335;&#20316;&#20026;&#33609;&#31295;&#12290;&#20854;&#27425;&#65292;KB-BINDER&#22522;&#20110;&#30693;&#35782;&#24211;&#26469;&#32465;&#23450;&#29983;&#25104;&#30340;&#33609;&#31295;&#33267;&#21487;&#25191;&#34892;&#24418;&#24335;&#65292;&#36890;&#36807;BM25&#20998;&#25968;&#21305;&#37197;&#12290;&#22312;&#22235;&#20010;&#20844;&#24320;&#30340;&#24322;&#26500;KBQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KB-BINDER&#21487;&#20197;&#22312;&#23569;&#37327;&#19978;&#19979;&#25991;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over knowledge bases is considered a difficult problem due to the challenge of generalizing to a wide variety of possible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets. To handle questions over diverse KBQA datasets with a unified training-free framework, we propose KB-BINDER, which for the first time enables few-shot in-context learning over KBQA tasks. Firstly, KB-BINDER leverages large language models like Codex to generate logical forms as the draft for a specific question by imitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge base to bind the generated draft to an executable one with BM25 score matching. The experimental results on four public heterogeneous KBQA datasets show that KB-BINDER can achieve a strong performance with only a few in-context demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25688;&#35201;&#25552;&#21462;&#26041;&#27861;DiffuSum&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25277;&#21462;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01735</link><description>&lt;p&gt;
DiffuSum&#65306;&#22522;&#20110;&#25193;&#25955;&#22686;&#24378;&#30340;&#25688;&#35201;&#25552;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffuSum: Generation Enhanced Extractive Summarization with Diffusion. (arXiv:2305.01735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25688;&#35201;&#25552;&#21462;&#26041;&#27861;DiffuSum&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25277;&#21462;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#21462;&#24335;&#25688;&#35201;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#20174;&#28304;&#25991;&#20214;&#20013;&#25552;&#21462;&#21477;&#23376;&#26469;&#24418;&#25104;&#25688;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DiffuSum&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#25152;&#38656;&#30340;&#25688;&#35201;&#21477;&#23376;&#34920;&#31034;&#65292;&#24182;&#22522;&#20110;&#21477;&#23376;&#34920;&#31034;&#21305;&#37197;&#25552;&#21462;&#21477;&#23376;&#12290;&#27492;&#22806;&#65292;DiffuSum&#20849;&#21516;&#20248;&#21270;&#20102;&#23545;&#27604;&#21477;&#23376;&#32534;&#30721;&#22120;&#21644;&#21305;&#37197;&#25439;&#22833;&#65292;&#29992;&#20110;&#21477;&#23376;&#34920;&#31034;&#23545;&#40784;&#65292;&#20197;&#21450;&#29992;&#20110;&#34920;&#31034;&#22810;&#26679;&#24615;&#30340;&#22810;&#31867;&#23545;&#27604;&#25439;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DiffuSum&#22312;CNN/DailyMail&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#25277;&#21462;&#32467;&#26524;&#65292;&#22312;ROUGE&#20998;&#25968;&#26041;&#38754;&#36798;&#21040;&#20102; $44.83/22.56/40.56$&#12290;&#23545;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#25688;&#35201;&#38271;&#24230;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#20063;&#35777;&#26126;&#20102;DiffuSum&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#24378;&#22823;&#24615;&#33021;&#34920;&#26126;&#20102;&#36866;&#24212;&#24403;&#21069;&#24773;&#20917;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extractive summarization aims to form a summary by directly extracting sentences from the source document. Existing works mostly formulate it as a sequence labeling problem by making individual sentence label predictions. This paper proposes DiffuSum, a novel paradigm for extractive summarization, by directly generating the desired summary sentence representations with diffusion models and extracting sentences based on sentence representation matching. In addition, DiffuSum jointly optimizes a contrastive sentence encoder with a matching loss for sentence representation alignment and a multi-class contrastive loss for representation diversity. Experimental results show that DiffuSum achieves the new state-of-the-art extractive results on CNN/DailyMail with ROUGE scores of $44.83/22.56/40.56$. Experiments on the other two datasets with different summary lengths also demonstrate the effectiveness of DiffuSum. The strong performance of our framework shows the great potential of adapting g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#36827;&#34892;&#31435;&#22330;&#26816;&#27979;&#65306;&#26377;&#30417;&#30563;&#20998;&#31867;&#12289;&#38646;&#26679;&#26412;&#20998;&#31867;&#19982;NLI&#20998;&#31867;&#22120;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35821;&#35328;&#20998;&#31867;&#22120;&#21487;&#20197;&#26367;&#20195;&#20154;&#24037;&#26631;&#31614;&#32773;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#36816;&#29992;&#23427;&#20204;&#26469;&#25191;&#34892;&#31435;&#22330;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.01723</link><description>&lt;p&gt;
&#21033;&#29992;&#26377;&#30417;&#30563;&#12289;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24212;&#29992;&#36827;&#34892;&#31435;&#22330;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Stance Detection With Supervised, Zero-Shot, and Few-Shot Applications. (arXiv:2305.01723v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#36827;&#34892;&#31435;&#22330;&#26816;&#27979;&#65306;&#26377;&#30417;&#30563;&#20998;&#31867;&#12289;&#38646;&#26679;&#26412;&#20998;&#31867;&#19982;NLI&#20998;&#31867;&#22120;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35821;&#35328;&#20998;&#31867;&#22120;&#21487;&#20197;&#26367;&#20195;&#20154;&#24037;&#26631;&#31614;&#32773;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#36816;&#29992;&#23427;&#20204;&#26469;&#25191;&#34892;&#31435;&#22330;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31435;&#22330;&#26816;&#27979;&#26159;&#35782;&#21035;&#20316;&#32773;&#23545;&#19968;&#20010;&#20027;&#39064;&#30340;&#20449;&#20208;&#30340;&#36807;&#31243;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#20381;&#36182;&#24773;&#24863;&#20998;&#26512;&#26469;&#23436;&#25104;&#36825;&#19968;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#24773;&#24863;&#20998;&#26512;&#21482;&#19982;&#31435;&#22330;&#23384;&#22312;&#26494;&#25955;&#30340;&#30456;&#20851;&#24615;&#65292;&#22914;&#26524;&#26377;&#30340;&#35805;&#12290;&#26412;&#25991;&#36890;&#36807;&#31934;&#30830;&#23450;&#20041;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#12289;&#25552;&#20379;&#20219;&#21153;&#30340;&#26222;&#36866;&#26694;&#26550;&#65292;&#24182;&#38024;&#23545;&#25191;&#34892;&#31435;&#22330;&#26816;&#27979;&#30340;&#19977;&#31181;&#19981;&#21516;&#26041;&#27861;&#65288;&#26377;&#30417;&#30563;&#20998;&#31867;&#12289;&#38646;&#26679;&#26412;&#20998;&#31867;&#19982;NLI&#20998;&#31867;&#22120;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#25552;&#20986;&#26032;&#30340;&#30740;&#31350;&#26041;&#27861;&#20197;&#25512;&#36827;&#25991;&#26412;&#20998;&#26512;&#26041;&#24335;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#26412;&#25991;&#35828;&#26126;&#20102;&#22914;&#20309;&#20351;&#29992;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35821;&#35328;&#20998;&#31867;&#22120;&#26367;&#20195;&#20154;&#24037;&#26631;&#31614;&#32773;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#21644;&#23616;&#38480;&#24615;&#19982;&#26377;&#30417;&#30563;&#20998;&#31867;&#22120;&#19981;&#21516;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#36890;&#36807;&#22797;&#21046;Block Jr&#31561;&#20154; (2022)&#30340;&#26041;&#27861;&#65292;&#28436;&#31034;&#20102;&#38646;&#26679;&#26412;&#31435;&#22330;&#26816;&#27979;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stance detection is the identification of an author's beliefs about a subject from a document. Researchers widely rely on sentiment analysis to accomplish this. However, recent research has show that sentiment analysis is only loosely correlated with stance, if at all. This paper advances methods in text analysis by precisely defining the task of stance detection, providing a generalized framework for the task, and then presenting three distinct approaches for performing stance detection: supervised classification, zero-shot classification with NLI classifiers, and in-context learning. In doing so, I demonstrate how zero-shot and few-shot language classifiers can replace human labelers for a variety of tasks and discuss how their application and limitations differ from supervised classifiers. Finally, I demonstrate an application of zero-shot stance detection by replicating Block Jr et al. (2022).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23558;BERT-GPT2&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#21487;&#20998;&#31163;&#30340;&#35821;&#20041;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01713</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#35299;&#37322;&#30340;&#38750;&#20132;&#20114;&#35821;&#20041;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks. (arXiv:2305.01713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23558;BERT-GPT2&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#21487;&#20998;&#31163;&#30340;&#35821;&#20041;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32454;&#21270;&#36830;&#32493;&#31354;&#38388;&#30340;&#21477;&#23376;&#34920;&#24449;&#19978;&#36827;&#34892;&#35299;&#32806;&#21487;&#20197;&#22312;&#23450;&#20301;&#26126;&#30830;&#21457;&#29983;&#30340;&#29983;&#25104;&#22240;&#32032;&#30340;&#21516;&#26102;&#65292;&#25913;&#36827;&#21487;&#35299;&#37322;&#24615;&#21644;&#35821;&#20041;&#25511;&#21046;&#65292;&#36825;&#20026;&#22522;&#20110;&#31070;&#32463;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#20102;&#19968;&#20123;&#31526;&#21495;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#28789;&#27963;&#24615;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#23558;BERT-GPT2&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#21487;&#20998;&#31163;&#30340;&#35821;&#20041;&#31354;&#38388;&#26469;&#35299;&#38500;&#32534;&#30721;&#30340;&#38544;&#34255;&#31354;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;INN&#33021;&#22815;&#23558;&#20998;&#24067;&#24335;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#22909;&#30340;&#35821;&#20041;&#19978;&#35299;&#32806;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangling sentence representations over continuous spaces can be a critical process in improving interpretability and semantic control by localising explicit generative factors. Such process confers to neural-based language models some of the advantages that are characteristic of symbolic models, while keeping their flexibility. This work presents a methodology for disentangling the hidden space of a BERT-GPT2 autoencoder by transforming it into a more separable semantic space with the support of a flow-based invertible neural network (INN). Experimental results indicate that the INN can transform the distributed hidden space into a better semantically disentangled latent space, resulting in better interpretability and controllability, when compared to recent state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#20110;&#24494;&#35843;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20256;&#32479;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#19981;&#33021;&#20445;&#35777;&#19968;&#33268;&#30340;&#25552;&#39640;&#24615;&#33021;&#65292;&#29978;&#33267;&#20250;&#23545;&#19968;&#20123;&#20219;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#26088;&#22312;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#21521;LM&#23637;&#31034;&#20219;&#21153;&#30456;&#20851;&#25991;&#26412;&#21644;&#25552;&#31034;&#27169;&#26495;&#65292;&#20174;&#32780;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01711</link><description>&lt;p&gt;
&#19981;&#20572;&#27490;&#39044;&#35757;&#32451;&#65311;&#35753;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26356;&#21152;&#24378;&#22823;
&lt;/p&gt;
&lt;p&gt;
Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner. (arXiv:2305.01711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#20110;&#24494;&#35843;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20256;&#32479;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#19981;&#33021;&#20445;&#35777;&#19968;&#33268;&#30340;&#25552;&#39640;&#24615;&#33021;&#65292;&#29978;&#33267;&#20250;&#23545;&#19968;&#20123;&#20219;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#26088;&#22312;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#21521;LM&#23637;&#31034;&#20219;&#21153;&#30456;&#20851;&#25991;&#26412;&#21644;&#25552;&#31034;&#27169;&#26495;&#65292;&#20174;&#32780;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#26080;&#26631;&#27880;&#25968;&#25454;&#30340;&#35757;&#32451;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290; &#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;NLP&#20013;&#24191;&#20026;&#25509;&#21463;&#30340;LM&#20219;&#21153;&#30456;&#20851;&#25991;&#26412;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24494;&#35843;&#24615;&#33021;&#30340;&#29702;&#35770;&#12290;&#36890;&#36807;&#22312;&#21322;&#30417;&#30563;&#21644;&#20840;&#30417;&#30563;&#35774;&#32622;&#19979;&#23545;&#20843;&#20010;&#21333;&#21477;&#20219;&#21153;&#21644;&#20843;&#20010;&#21477;&#23545;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#19981;&#33021;&#20445;&#35777;&#19968;&#33268;&#30340;&#25552;&#39640;&#24615;&#33021;&#65292;&#29978;&#33267;&#21487;&#33021;&#23545;&#21477;&#23545;&#20219;&#21153;&#25110;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26041;&#24335;&#26102;&#20250;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65288;PCP&#65289;&#65292;&#23558;&#25351;&#23548;&#35843;&#25972;&#30340;&#24605;&#24819;&#19982;&#20256;&#32479;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#22312;&#24494;&#35843;&#30446;&#26631;&#20043;&#21069;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#21521;LM&#23637;&#31034;&#20219;&#21153;&#30456;&#20851;&#25991;&#26412;&#21644;&#25552;&#31034;&#27169;&#26495;&#65292;&#20174;&#32780;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;FT&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) trained on vast quantities of unlabelled data have greatly advanced the field of natural language processing (NLP). In this study, we re-visit the widely accepted notion in NLP that continued pre-training LMs on task-related texts improves the performance of fine-tuning (FT) in downstream tasks. Through experiments on eight single-sentence tasks and eight sentence-pair tasks in both semi-supervised and fully-supervised settings, we find that conventional continued pre-training does not consistently provide benefits and can even be detrimental for sentence-pair tasks or when prompt-based FT is used. To tackle these issues, we propose Prompt-based Continued Pre-training (PCP), which combines the idea of instruction tuning with conventional continued pre-training. Our approach aims to improve the performance of prompt-based FT by presenting both task-related texts and prompt templates to LMs through unsupervised pre-training objectives before fine-tuning for the targ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26723;&#32423;&#31471;&#21040;&#31471;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#65292;&#23454;&#29616;&#26041;&#38754;&#26816;&#27979;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#35780;&#20998;&#39044;&#27979;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01710</link><description>&lt;p&gt;
&#26143;&#36784;&#21363;&#20320;&#25152;&#38656;&#65306;&#29992;&#36828;&#31243;&#30417;&#30563;&#37329;&#23383;&#22612;&#32593;&#32476;&#36827;&#34892;&#25991;&#26723;&#32423;&#31471;&#21040;&#31471;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Stars Are All You Need: A Distantly Supervised Pyramid Network for Document-Level End-to-End Sentiment Analysis. (arXiv:2305.01710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26723;&#32423;&#31471;&#21040;&#31471;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#65292;&#23454;&#29616;&#26041;&#38754;&#26816;&#27979;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#35780;&#20998;&#39044;&#27979;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25991;&#26723;&#32423;&#31471;&#21040;&#31471;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#23545;&#22312;&#32447;&#35780;&#35770;&#20013;&#34920;&#36798;&#30340;&#26041;&#38754;&#21644;&#35780;&#35770;&#24773;&#24863;&#36827;&#34892;&#26377;&#25928;&#30340;&#32479;&#19968;&#20998;&#26512;&#12290;&#25105;&#20204;&#20551;&#35774;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#26159;&#35780;&#35770;&#20013;&#21508;&#26041;&#38754;&#35780;&#20998;&#30340;&#8220;&#31895;&#31890;&#24230;&#32508;&#21512;&#8221;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#30417;&#30563;&#30340;&#37329;&#23383;&#22612;&#32593;&#32476;&#65288;DSPN&#65289;&#65292;&#21482;&#29992;&#25991;&#26723;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#65292;&#21363;&#21487;&#26377;&#25928;&#22320;&#25191;&#34892;&#26041;&#38754;-&#31867;&#21035;&#26816;&#27979;&#12289;&#26041;&#38754;-&#31867;&#21035;&#24773;&#24863;&#20998;&#26512;&#21644;&#35780;&#20998;&#39044;&#27979;&#12290;&#36890;&#36807;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#25191;&#34892;&#36825;&#19977;&#20010;&#30456;&#20851;&#30340;&#24773;&#24863;&#23376;&#20219;&#21153;&#65292;DSPN&#21487;&#20197;&#25552;&#21462;&#35780;&#35770;&#20013;&#25552;&#21040;&#30340;&#26041;&#38754;&#65292;&#30830;&#23450;&#30456;&#24212;&#30340;&#24773;&#24863;&#65292;&#24182;&#39044;&#27979;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;&#33521;&#25991;&#21644;&#27721;&#35821;&#22810;&#26041;&#38754;&#35780;&#35770;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;DSPN&#65292;&#21457;&#29616;&#20165;&#20351;&#29992;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#36827;&#34892;&#30417;&#30563;&#65292;DSPN&#30340;&#24615;&#33021;&#19982;&#21508;&#31181;&#22522;&#20934;&#27169;&#22411;&#30456;&#24403;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;DSPN&#22312;&#35780;&#35770;&#19978;&#30340;&#21487;&#35299;&#37322;&#24615;&#36755;&#20986;&#65292;&#20197;&#35828;&#26126;&#37329;&#23383;&#22612;&#32593;&#32476;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose document-level end-to-end sentiment analysis to efficiently understand aspect and review sentiment expressed in online reviews in a unified manner. In particular, we assume that star rating labels are a "coarse-grained synthesis" of aspect ratings across in the review. We propose a Distantly Supervised Pyramid Network (DSPN) to efficiently perform Aspect-Category Detection, Aspect-Category Sentiment Analysis, and Rating Prediction using only document star rating labels for training. By performing these three related sentiment subtasks in an end-to-end manner, DSPN can extract aspects mentioned in the review, identify the corresponding sentiments, and predict the star rating labels. We evaluate DSPN on multi-aspect review datasets in English and Chinese and find that with only star rating labels for supervision, DSPN can perform comparably well to a variety of benchmark models. We also demonstrate the interpretability of DSPN's outputs on reviews to show the py
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21387;&#32553;&#27169;&#22411;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#25163;&#21160;&#26631;&#27880;&#26356;&#22810;&#30340;&#24494;&#35843;&#25968;&#25454;&#20197;&#30452;&#25509;&#35757;&#32451;&#21387;&#32553;&#27169;&#22411;&#30456;&#27604;&#65292;&#20174;T5-XXL&#33976;&#39311;&#21040;T5-Small&#20960;&#20046;&#24635;&#26159;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2305.01645</link><description>&lt;p&gt;
&#21387;&#32553;&#27169;&#22411;&#30340;&#39640;&#25928;&#24494;&#35843;&#65306;&#33976;&#39311;&#36824;&#26159;&#26631;&#27880;&#65311;
&lt;/p&gt;
&lt;p&gt;
Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models. (arXiv:2305.01645v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21387;&#32553;&#27169;&#22411;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#25163;&#21160;&#26631;&#27880;&#26356;&#22810;&#30340;&#24494;&#35843;&#25968;&#25454;&#20197;&#30452;&#25509;&#35757;&#32451;&#21387;&#32553;&#27169;&#22411;&#30456;&#27604;&#65292;&#20174;T5-XXL&#33976;&#39311;&#21040;T5-Small&#20960;&#20046;&#24635;&#26159;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#30340;&#24494;&#35843;&#34429;&#28982;&#25928;&#26524;&#26174;&#33879;&#65292;&#20294;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#25104;&#26412;&#39640;&#19988;&#20250;&#20135;&#29983;&#30899;&#25490;&#25918;&#12290;&#30693;&#35782;&#33976;&#39311;&#24050;&#34987;&#35777;&#26126;&#26159;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#33976;&#39311;&#36807;&#31243;&#26412;&#36523;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#26368;&#26377;&#25928;&#22320;&#20351;&#29992;&#22266;&#23450;&#39044;&#31639;&#26500;&#24314;&#21387;&#32553;&#27169;&#22411;&#12290;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#20174;T5-XXL&#65288;11B&#65289;&#33976;&#39311;&#21040;T5-Small&#65288;60M&#65289;&#20960;&#20046;&#24635;&#26159;&#27604;&#25163;&#21160;&#26631;&#27880;&#26356;&#22810;&#30340;&#24494;&#35843;&#25968;&#25454;&#20197;&#30452;&#25509;&#35757;&#32451;&#19968;&#20010;&#21387;&#32553;&#27169;&#22411;&#65288;T5-Small&#65288;60M&#65289;&#65289;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#26368;&#22823;&#21270;&#25928;&#29992;&#30340;&#26368;&#20339;&#33976;&#39311;&#37327;&#22240;&#20219;&#21153;&#32780;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning large models is highly effective, however, inference using these models can be expensive and produces carbon emissions. Knowledge distillation has been shown to be a practical solution to reduce inference costs, but the distillation process itself requires significant computational resources. Rather than buying or renting GPUs to fine-tune, then distill a large model, an NLP practitioner who needs a compact model might also choose to simply allocate an available budget to hire annotators and manually label additional fine-tuning data. In this paper, we investigate how to most efficiently use a fixed budget to build a compact model. Through our extensive experiments on six diverse NLP tasks, we find that distilling from T5-XXL (11B) to T5-Small (60M) leads to almost always a cost-efficient option compared to annotating more data to directly train a compact model (T5-Small (60M)). We further demonstrate that the optimal amount of distillation that maximizes utility varies acr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01555</link><description>&lt;p&gt;
&#22914;&#20309;&#21457;&#25381;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#36824;&#27809;&#26377;&#24471;&#21040;&#20840;&#38754;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#35814;&#32454;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#30340;&#22522;&#26412;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25968;&#25454;&#29983;&#25104;&#12290;&#20026;&#20102;&#22686;&#24378;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;&#20197;&#21069;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#32780;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25512;&#21160;&#20197;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#22312;&#22235;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#28608;&#21457;&#26410;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312; \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.01219</link><description>&lt;p&gt;
&#35302;&#21457;&#35789;&#20316;&#20026;&#21518;&#38376;&#25915;&#20987;&#30340;&#35302;&#21457;&#22120;&#65306;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#33539;&#20363;&#24357;&#21512;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#20960;&#20010;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#27880;&#20837;&#35302;&#21457;&#22120;&#24182;&#20462;&#25913;&#26631;&#31614;&#26469;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#26377;&#38024;&#23545;&#24615;&#30340;&#28431;&#27934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35302;&#21457;&#22120;&#30340;&#23384;&#22312;&#21644;&#27602;&#30244;&#25968;&#25454;&#26631;&#27880;&#19981;&#27491;&#30830;&#31561;&#32570;&#38519;&#65292;&#36825;&#31181;&#25915;&#20987;&#23384;&#22312;&#24322;&#24120;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#65292;&#22522;&#20110;&#25552;&#31034;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#12290;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#36164;&#28304;&#21644;&#23569;&#26679;&#26412;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ProAttack&#26041;&#27861;&#22312;&#20445;&#25345;&#24178;&#20928;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose {\bf ProAttack}, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text c
&lt;/p&gt;</description></item><item><title>CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.00969</link><description>&lt;p&gt;
CryCeleb: &#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00969
&lt;/p&gt;
&lt;p&gt;
CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;Ubenwa CryCeleb&#25968;&#25454;&#38598;&#8212;&#8212;&#19968;&#20010;&#26631;&#35760;&#30340;&#23156;&#20799;&#21741;&#22768;&#25910;&#38598;&#65292;&#20197;&#21450;&#38468;&#24102;&#30340;CryCeleb 2023&#20219;&#21153;&#8212;&#8212;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#20844;&#20849;&#35828;&#35805;&#20154;&#39564;&#35777;&#25361;&#25112;&#12290;&#25105;&#20204;&#37322;&#25918;&#20986;786&#21517;&#26032;&#29983;&#20799;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#20197;&#40723;&#21169;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the Ubenwa CryCeleb dataset - a labeled collection of infant cries, and the accompanying CryCeleb 2023 task - a public speaker verification challenge based on infant cry sounds. We release for academic usage more than 6 hours of manually segmented cry sounds from 786 newborns to encourage research in infant cry analysis.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SearChain&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#36861;&#28335;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;SearChain&#36890;&#36807;&#28145;&#24230;&#38598;&#25104;LLM&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#23454;&#29616;&#65292;&#20854;&#24605;&#36335;&#26159;&#36890;&#36807;&#26500;&#36896;&#26597;&#35810;&#38142;&#65292;&#23558;&#22810;&#36339;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#65292;&#26368;&#32456;&#25351;&#23548;LLM&#29983;&#25104;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.14732</link><description>&lt;p&gt;
&#22522;&#20110;SearChain&#30340;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#31934;&#30830;&#12289;&#21487;&#20449;&#21644;&#21487;&#36861;&#28335;&#20869;&#23481;&#29983;&#25104;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks. (arXiv:2304.14732v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SearChain&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#36861;&#28335;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;SearChain&#36890;&#36807;&#28145;&#24230;&#38598;&#25104;LLM&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#23454;&#29616;&#65292;&#20854;&#24605;&#36335;&#26159;&#36890;&#36807;&#26500;&#36896;&#26597;&#35810;&#38142;&#65292;&#23558;&#22810;&#36339;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#65292;&#26368;&#32456;&#25351;&#23548;LLM&#29983;&#25104;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#22914;&#20309;&#20351;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#20934;&#30830;&#21487;&#20449;&#22312;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Search-in-the-Chain&#65288;SearChain&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#31561;&#20856;&#22411;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#36861;&#28335;&#24615;&#12290;SearChain&#26159;&#19968;&#20010;&#28145;&#24230;&#38598;&#25104;LLM&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#30340;&#26694;&#26550;&#12290;&#22312;SearChain&#20013;&#65292;LLM&#26500;&#24314;&#26597;&#35810;&#38142;&#65292;&#20316;&#20026;&#22810;&#36339;&#38382;&#39064;&#30340;&#20998;&#35299;&#12290;&#38142;&#30340;&#27599;&#20010;&#33410;&#28857;&#37117;&#26159;&#30001;IR&#23548;&#21521;&#30340;&#26597;&#35810;-&#31572;&#26696;&#23545;&#65292;&#20197;&#21450;&#30001;LLM&#29983;&#25104;&#30340;&#35813;&#26597;&#35810;&#30340;&#31572;&#26696;&#12290;IR&#39564;&#35777;&#12289;&#23436;&#21892;&#21644;&#36319;&#36394;&#38142;&#20013;&#27599;&#20010;&#33410;&#28857;&#30340;&#20449;&#24687;&#65292;&#20197;&#25351;&#23548;LLM&#26500;&#24314;&#27491;&#30830;&#30340;&#26597;&#35810;&#38142;&#65292;&#24182;&#26368;&#32456;&#22238;&#31572;&#22810;&#36339;&#38382;&#39064;&#12290;SearChain&#20351;LLM&#20174;&#19968;&#27425;&#24615;&#31572;&#26696;&#36716;&#21464;&#20026;&#22810;&#27493;&#31572;&#26696;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SearChain&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the wide application of Large Language Models (LLMs) such as ChatGPT, how to make the contents generated by LLM accurate and credible becomes very important, especially in complex knowledge-intensive tasks. In this paper, we propose a novel framework called Search-in-the-Chain (SearChain) to improve the accuracy, credibility and traceability of LLM-generated content for multi-hop question answering, which is a typical complex knowledge-intensive task. SearChain is a framework that deeply integrates LLM and information retrieval (IR). In SearChain, LLM constructs a chain-of-query, which is the decomposition of the multi-hop question. Each node of the chain is a query-answer pair consisting of an IR-oriented query and the answer generated by LLM for this query. IR verifies, completes, and traces the information of each node of the chain, so as to guide LLM to construct the correct chain-of-query, and finally answer the multi-hop question. SearChain makes LLM change from trying to gi
&lt;/p&gt;</description></item><item><title>ImpressionGPT&#26159;&#19968;&#20010;&#21033;&#29992;LLMs&#26500;&#24314;&#21160;&#24577;&#19978;&#19979;&#25991;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#29983;&#25104;&#12290;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;ImpressionGPT&#22312;&#20855;&#26377;&#36739;&#22909;&#27867;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.08448</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#29983;&#25104;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65306;ImpressionGPT
&lt;/p&gt;
&lt;p&gt;
ImpressionGPT: An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT. (arXiv:2304.08448v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08448
&lt;/p&gt;
&lt;p&gt;
ImpressionGPT&#26159;&#19968;&#20010;&#21033;&#29992;LLMs&#26500;&#24314;&#21160;&#24577;&#19978;&#19979;&#25991;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#29983;&#25104;&#12290;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;ImpressionGPT&#22312;&#20855;&#26377;&#36739;&#22909;&#27867;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;"Impression"&#37096;&#20998;&#26159;&#25918;&#23556;&#31185;&#21307;&#24072;&#21644;&#20854;&#20182;&#21307;&#29983;&#20132;&#27969;&#30340;&#37325;&#35201;&#22522;&#30784;&#65292;&#36890;&#24120;&#26159;&#22522;&#20110;"Findings"&#37096;&#20998;&#32534;&#20889;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25918;&#23556;&#31185;&#21307;&#24072;&#26469;&#35828;&#65292;&#32534;&#20889;&#22823;&#37327;&#30340;&#21360;&#35937;&#25551;&#36848;&#21487;&#33021;&#26159;&#36153;&#26102;&#36153;&#21147;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#22823;&#35268;&#27169;&#21307;&#23398;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21360;&#35937;&#29983;&#25104;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#20294;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#21307;&#23398;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#24046;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#34429;&#28982;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#29305;&#23450;&#39046;&#22495;&#65288;&#22914;&#25918;&#23556;&#23398;&#65289;&#20013;&#30340;&#34920;&#29616;&#20173;&#28982;&#26410;&#32463;&#35843;&#26597;&#65292;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ImpressionGPT&#65292;&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#20010;&#24615;&#21270;&#25968;&#25454;&#26500;&#24314;&#21160;&#24577;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20102;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 'Impression' section of a radiology report is a critical basis for communication between radiologists and other physicians, and it is typically written by radiologists based on the 'Findings' section. However, writing numerous impressions can be laborious and error-prone for radiologists. Although recent studies have achieved promising results in automatic impression generation using large-scale medical text data for pre-training and fine-tuning pre-trained language models, such models often require substantial amounts of medical text data and have poor generalization performance. While large language models (LLMs) like ChatGPT have shown strong generalization capabilities and performance, their performance in specific domains, such as radiology, remains under-investigated and potentially limited. To address this limitation, we propose ImpressionGPT, which leverages the in-context learning capability of LLMs by constructing dynamic contexts using domain-specific, individualized dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#25918;&#23556;&#24615;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359; X-REM&#65292;&#23427;&#20351;&#29992;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20998;&#25968;&#26469;&#34913;&#37327;&#33016;&#37096; X &#20809;&#22270;&#20687;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#36827;&#34892;&#25253;&#21578;&#26816;&#32034;&#65292;&#20854;&#22312;&#22810;&#20010;&#20808;&#21069;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#33258;&#21160;&#29983;&#25104;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17579</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20248;&#21270;&#22522;&#20110;&#26816;&#32034;&#30340;&#33016;&#37096; X &#23556;&#32447;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multimodal Image-Text Matching Improves Retrieval-based Chest X-Ray Report Generation. (arXiv:2303.17579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#25918;&#23556;&#24615;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359; X-REM&#65292;&#23427;&#20351;&#29992;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20998;&#25968;&#26469;&#34913;&#37327;&#33016;&#37096; X &#20809;&#22270;&#20687;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#36827;&#34892;&#25253;&#21578;&#26816;&#32034;&#65292;&#20854;&#22312;&#22810;&#20010;&#20808;&#21069;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#33258;&#21160;&#29983;&#25104;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#29983;&#25104;&#20020;&#24202;&#20934;&#30830;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#12290;&#20197;&#21069;&#20381;&#36182;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#30340;&#25253;&#21578;&#29983;&#25104;&#26041;&#27861;&#30001;&#20110;&#32570;&#20047;&#30456;&#20851;&#39046;&#22495;&#30693;&#35782;&#32780;&#32463;&#24120;&#29983;&#25104;&#19981;&#36830;&#36143;&#21644;&#19981;&#27491;&#30830;&#30340;&#25991;&#26412;&#65292;&#32780;&#22522;&#20110;&#26816;&#32034;&#30340;&#23581;&#35797;&#32463;&#24120;&#26816;&#32034;&#21040;&#19982;&#36755;&#20837;&#22270;&#20687;&#19981;&#30456;&#20851;&#30340;&#25253;&#21578;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Contrastive X-Ray REport Match&#65288;X-REM&#65289;&#30340;&#26032;&#22411;&#22522;&#20110;&#26816;&#32034;&#30340;&#25918;&#23556;&#24615;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#20351;&#29992;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20998;&#25968;&#26469;&#34913;&#37327;&#33016;&#37096; X &#20809;&#22270;&#20687;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#20197;&#36827;&#34892;&#25253;&#21578;&#26816;&#32034;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20351;&#29992;&#35821;&#35328;&#22270;&#20687;&#27169;&#22411;&#35745;&#31639;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20998;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#22312;&#20351;&#29992;&#20313;&#24358;&#30456;&#20284;&#24615;&#26102;&#32463;&#24120;&#20002;&#22833;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#20132;&#20114;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#21644;&#20020;&#24202;&#24230;&#37327;&#26041;&#38754;&#65292;X-REM&#22312;&#22810;&#20010;&#20808;&#21069;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#36890;&#36807;&#23545;&#29983;&#25104;&#30340;&#25253;&#21578;&#36827;&#34892;&#20154;&#31867;&#35780;&#20272;&#65292;&#34920;&#26126; X-R...
&lt;/p&gt;
&lt;p&gt;
Automated generation of clinically accurate radiology reports can improve patient care. Previous report generation methods that rely on image captioning models often generate incoherent and incorrect text due to their lack of relevant domain knowledge, while retrieval-based attempts frequently retrieve reports that are irrelevant to the input image. In this work, we propose Contrastive X-Ray REport Match (X-REM), a novel retrieval-based radiology report generation module that uses an image-text matching score to measure the similarity of a chest X-ray image and radiology report for report retrieval. We observe that computing the image-text matching score with a language-image model can effectively capture the fine-grained interaction between image and text that is often lost when using cosine similarity. X-REM outperforms multiple prior radiology report generation modules in terms of both natural language and clinical metrics. Human evaluation of the generated reports suggests that X-R
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#20294;&#36825;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2303.10475</link><description>&lt;p&gt;
&#20165;&#20165;&#25552;&#31034;&#36275;&#22815;&#20102;&#21527;&#65311;&#19981;&#26159;&#30340;&#12290;&#25351;&#23548;&#23398;&#20064;&#30340;&#20840;&#38754;&#21644;&#26356;&#24191;&#38420;&#35270;&#35282;&#65288;arXiv&#65306;2303.10475v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning. (arXiv:2303.10475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10475
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#20294;&#36825;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#35821;&#20041;&#21487;&#20197;&#36890;&#36807;&#19968;&#32452;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#25110;&#19968;&#26465;&#25991;&#26412;&#25351;&#20196;&#26469;&#34920;&#36798;&#12290;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#30340;&#21487;&#29992;&#24615;&#12290;&#36825;&#24341;&#36215;&#20102;&#20004;&#20010;&#38382;&#39064;&#65306;&#39318;&#20808;&#65292;&#25910;&#38598;&#20219;&#21153;&#29305;&#23450;&#26631;&#35760;&#31034;&#20363;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#65292;&#25110;&#32773;&#31995;&#32479;&#38656;&#35201;&#31435;&#21363;&#22788;&#29702;&#26032;&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#36825;&#19981;&#26159;&#29992;&#25143;&#21451;&#22909;&#30340;&#65292;&#22240;&#20026;&#26368;&#32456;&#29992;&#25143;&#21487;&#33021;&#26356;&#24895;&#24847;&#22312;&#20351;&#29992;&#31995;&#32479;&#20043;&#21069;&#25552;&#20379;&#20219;&#21153;&#25551;&#36848;&#32780;&#19981;&#26159;&#19968;&#32452;&#31034;&#20363;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#31038;&#21306;&#20173;&#28982;&#38754;&#20020;&#30528;&#19968;&#20123;&#20849;&#21516;&#30340;&#38382;&#39064;&#12290;&#26412;&#27425;&#35843;&#26597;&#26088;&#22312;&#24635;&#32467;&#25351;&#23548;&#23398;&#20064;&#30340;&#24403;&#21069;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;
&lt;/p&gt;
&lt;p&gt;
Task semantics can be expressed by a set of input-to-output examples or a piece of textual instruction. Conventional machine learning approaches for natural language processing (NLP) mainly rely on the availability of large-scale sets of task-specific examples. Two issues arise: first, collecting task-specific labeled examples does not apply to scenarios where tasks may be too complicated or costly to annotate, or the system is required to handle a new task immediately; second, this is not user-friendly since end-users are probably more willing to provide task description rather than a set of examples before using the system. Therefore, the community is paying increasing interest in a new supervision-seeking paradigm for NLP: learning from task instructions. Despite its impressive progress, there are some common issues that the community struggles with. This survey paper tries to summarize the current research on instruction learning, particularly, by answering the following questions:
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#25506;&#35752;COVID-19&#24863;&#26579;&#21644;&#25233;&#37057;&#30151;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26089;&#26399;&#39044;&#27979;&#25233;&#37057;&#30151;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2302.12044</link><description>&lt;p&gt;
&#25506;&#31350;&#31038;&#20132;&#23186;&#20307;&#22312;COVID-19&#24739;&#32773;&#26089;&#26399;&#26816;&#27979;&#25233;&#37057;&#30151;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Social Media for Early Detection of Depression in COVID-19 Patients. (arXiv:2302.12044v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#25506;&#35752;COVID-19&#24863;&#26579;&#21644;&#25233;&#37057;&#30151;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26089;&#26399;&#39044;&#27979;&#25233;&#37057;&#30151;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#22823;&#27969;&#34892;&#23545;&#20840;&#29699;&#20581;&#24247;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#30772;&#22351;&#12290;&#23613;&#31649;&#19977;&#24180;&#36807;&#21435;&#20102;&#65292;&#19990;&#30028;&#20173;&#22312;&#19982;&#36825;&#31181;&#30149;&#27602;&#26007;&#20105;&#12290;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;COVID-19&#23545;&#24863;&#26579;&#32773;&#24515;&#29702;&#20581;&#24247;&#30340;&#24433;&#21709;&#65292;&#20182;&#20204;&#26356;&#23481;&#26131;&#24739;&#19978;&#25233;&#37057;&#30151;&#65292;&#36825;&#20250;&#23545;&#21463;&#24433;&#21709;&#30340;&#20010;&#20307;&#21644;&#19990;&#30028;&#20135;&#29983;&#38271;&#26399;&#21518;&#26524;&#12290;&#22312;COVID-19&#24739;&#32773;&#20013;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#21487;&#20197;&#38477;&#20302;&#24739;&#25233;&#37057;&#30151;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#25506;&#35752;&#20102;COVID-19&#24863;&#26579;&#21644;&#25233;&#37057;&#30151;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31649;&#29702;&#20102;&#19968;&#20010;&#21253;&#21547;COVID-19&#24739;&#32773;&#31038;&#20132;&#23186;&#20307;&#27963;&#21160;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#36825;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#20197;&#30740;&#31350;COVID-19&#24863;&#26579;&#39118;&#38505;&#36739;&#39640;&#30340;&#24739;&#32773;&#30340;&#29305;&#28857;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26089;&#26399;&#39044;&#27979;&#25233;&#37057;&#30151;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has caused substantial damage to global health. Even though three years have passed, the world continues to struggle with the virus. Concerns are growing about the impact of COVID-19 on the mental health of infected individuals, who are more likely to experience depression, which can have long-lasting consequences for both the affected individuals and the world. Detection and intervention at an early stage can reduce the risk of depression in COVID-19 patients. In this paper, we investigated the relationship between COVID-19 infection and depression through social media analysis. Firstly, we managed a dataset of COVID-19 patients that contains information about their social media activity both before and after infection. Secondly,We conducted an extensive analysis of this dataset to investigate the characteristic of COVID-19 patients with a higher risk of depression. Thirdly, we proposed a deep neural network for early prediction of depression risk. This model con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20219;&#21153;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25903;&#25345;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#32858;&#31867;&#12289;&#25490;&#24207;&#32452;&#20214;&#65292;&#20174;&#25191;&#34892;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#30340;&#25945;&#23398;&#35270;&#39057;&#25991;&#26412;&#35760;&#24405;&#20013;&#29983;&#25104;&#20219;&#21153;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;ProceL&#21644;CrossTask&#25968;&#25454;&#38598;&#19978;&#27604;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#30340;&#20219;&#21153;&#22270;&#26356;&#21152;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2302.09173</link><description>&lt;p&gt;
&#20174;&#25945;&#23398;&#35270;&#39057;&#25991;&#26412;&#36716;&#24405;&#20013;&#26080;&#30417;&#30563;&#29983;&#25104;&#20219;&#21153;&#22270;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Task Graph Generation from Instructional Video Transcripts. (arXiv:2302.09173v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20219;&#21153;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25903;&#25345;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#32858;&#31867;&#12289;&#25490;&#24207;&#32452;&#20214;&#65292;&#20174;&#25191;&#34892;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#30340;&#25945;&#23398;&#35270;&#39057;&#25991;&#26412;&#35760;&#24405;&#20013;&#29983;&#25104;&#20219;&#21153;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;ProceL&#21644;CrossTask&#25968;&#25454;&#38598;&#19978;&#27604;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#30340;&#20219;&#21153;&#22270;&#26356;&#21152;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#20219;&#21153;&#22270;&#30340;&#38382;&#39064;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#32771;&#34385;&#25552;&#20379;&#25191;&#34892;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#65288;&#22914;&#21046;&#20316;&#21654;&#21857;&#65289;&#30340;&#25945;&#23398;&#35270;&#39057;&#25991;&#26412;&#35760;&#24405;&#65292;&#24182;&#26088;&#22312;&#30830;&#23450;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20851;&#38190;&#27493;&#39588;&#21450;&#20854;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#38754;&#21521;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;&#32858;&#31867;&#21644;&#25490;&#24207;&#32452;&#20214;&#65292;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20934;&#30830;&#30340;&#20219;&#21153;&#22270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;ProceL&#21644;CrossTask&#25968;&#25454;&#38598;&#19978;&#29983;&#25104;&#30340;&#20219;&#21153;&#22270;&#27604;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26356;&#21152;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work explores the problem of generating task graphs of real-world activities. Different from prior formulations, we consider a setting where text transcripts of instructional videos performing a real-world activity (e.g., making coffee) are provided and the goal is to identify the key steps relevant to the task as well as the dependency relationship between these key steps. We propose a novel task graph generation approach that combines the reasoning capabilities of instruction-tuned language models along with clustering and ranking components to generate accurate task graphs in a completely unsupervised manner. We show that the proposed approach generates more accurate task graphs compared to a supervised learning approach on tasks from the ProceL and CrossTask datasets.
&lt;/p&gt;</description></item><item><title>&#30005;&#21830;PQA&#30340;&#30740;&#31350;&#38754;&#20020;&#30528;&#38382;&#39064;&#22810;&#12289;&#25968;&#25454;&#38590;&#25910;&#38598;&#12289;&#31572;&#26696;&#19981;&#30830;&#23450;&#31561;&#29305;&#27530;&#25361;&#25112;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#32508;&#36848;&#20102;PQA&#30740;&#31350;&#30340;&#29616;&#29366;&#19982;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.08092</link><description>&lt;p&gt;
&#30005;&#21830;&#20135;&#21697;&#38382;&#31572;&#65306;&#19968;&#39033;&#32508;&#36848;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Product Question Answering in E-Commerce: A Survey. (arXiv:2302.08092v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08092
&lt;/p&gt;
&lt;p&gt;
&#30005;&#21830;PQA&#30340;&#30740;&#31350;&#38754;&#20020;&#30528;&#38382;&#39064;&#22810;&#12289;&#25968;&#25454;&#38590;&#25910;&#38598;&#12289;&#31572;&#26696;&#19981;&#30830;&#23450;&#31561;&#29305;&#27530;&#25361;&#25112;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#32508;&#36848;&#20102;PQA&#30740;&#31350;&#30340;&#29616;&#29366;&#19982;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20135;&#21697;&#38382;&#31572;&#65288;PQA&#65289;&#33268;&#21147;&#20110;&#22312;&#30005;&#21830;&#24179;&#21488;&#19978;&#33258;&#21160;&#24555;&#36895;&#22238;&#22797;&#23458;&#25143;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#19982;&#20856;&#22411;&#30340;&#38382;&#31572;&#38382;&#39064;&#30456;&#27604;&#65292;PQA&#34920;&#29616;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#30005;&#21830;&#24179;&#21488;&#19978;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30340;&#20027;&#35266;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#22240;&#27492;&#65292;&#21508;&#31181;&#38382;&#39064;&#35774;&#32622;&#21644;&#26032;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#25429;&#25417;&#36825;&#20123;&#29305;&#27530;&#29305;&#24449;&#12290;&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#23457;&#26597;&#29616;&#26377;&#20851;&#20110;PQA&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;PQA&#30740;&#31350;&#25353;&#25552;&#20379;&#30340;&#31572;&#26696;&#24418;&#24335;&#23558;&#20854;&#20998;&#31867;&#20026;&#22235;&#20010;&#38382;&#39064;&#35774;&#32622;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#27599;&#20010;&#35774;&#32622;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#21327;&#35758;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24635;&#32467;&#20102;&#34920;&#24449;PQA&#19982;&#19968;&#33324;QA&#24212;&#29992;&#30340;&#26368;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#20960;&#20010;&#26410;&#26469;&#26041;&#21521;&#26469;&#32467;&#26463;&#26412;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Product question answering (PQA), aiming to automatically provide instant responses to customer's questions in E-Commerce platforms, has drawn increasing attention in recent years. Compared with typical QA problems, PQA exhibits unique challenges such as the subjectivity and reliability of user-generated contents in E-commerce platforms. Therefore, various problem settings and novel methods have been proposed to capture these special characteristics. In this paper, we aim to systematically review existing research efforts on PQA. Specifically, we categorize PQA studies into four problem settings in terms of the form of provided answers. We analyze the pros and cons, as well as present existing datasets and evaluation protocols for each setting. We further summarize the most significant challenges that characterize PQA from general QA applications and discuss their corresponding solutions. Finally, we conclude this paper by providing the prospect on several future directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DocILE&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22823;&#37327;&#21830;&#21153;&#25991;&#20214;&#65292;&#21487;&#29992;&#20110;&#20851;&#38190;&#20449;&#24687;&#23450;&#20301;&#21644;&#25552;&#21462;&#20197;&#21450;&#34892;&#39033;&#30446;&#35782;&#21035;&#20219;&#21153;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;55&#20010;&#31867;&#21035;&#30340;&#27880;&#37322;&#65292;&#36229;&#36807;&#20197;&#24448;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21253;&#25324;&#20247;&#22810;&#19981;&#21516;&#24067;&#23616;&#21644;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#65292;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#30740;&#31350;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2302.05658</link><description>&lt;p&gt;
DocILE&#22522;&#20934;&#25968;&#25454;&#38598;&#29992;&#20110;&#25991;&#20214;&#20449;&#24687;&#23450;&#20301;&#21644;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
DocILE Benchmark for Document Information Localization and Extraction. (arXiv:2302.05658v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DocILE&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22823;&#37327;&#21830;&#21153;&#25991;&#20214;&#65292;&#21487;&#29992;&#20110;&#20851;&#38190;&#20449;&#24687;&#23450;&#20301;&#21644;&#25552;&#21462;&#20197;&#21450;&#34892;&#39033;&#30446;&#35782;&#21035;&#20219;&#21153;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;55&#20010;&#31867;&#21035;&#30340;&#27880;&#37322;&#65292;&#36229;&#36807;&#20197;&#24448;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21253;&#25324;&#20247;&#22810;&#19981;&#21516;&#24067;&#23616;&#21644;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#65292;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#30740;&#31350;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DocILE&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20851;&#38190;&#20449;&#24687;&#23450;&#20301;&#21644;&#25552;&#21462;&#20197;&#21450;&#34892;&#39033;&#30446;&#35782;&#21035;&#20219;&#21153;&#30340;&#21830;&#21153;&#25991;&#20214;&#30340;&#26368;&#22823;&#25968;&#25454;&#38598;&#12290; &#23427;&#21253;&#21547;6.7k&#20010;&#24102;&#27880;&#37322;&#30340;&#21830;&#21153;&#25991;&#20214;&#65292;100k&#20010;&#21512;&#25104;&#29983;&#25104;&#30340;&#25991;&#26723;&#20197;&#21450;&#36817;1M&#20010;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#12290; &#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#29305;&#23450;&#20110;&#39046;&#22495;&#21644;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#20855;&#26377;&#20197;&#19979;&#20851;&#38190;&#29305;&#24449;&#65306;&#65288;i&#65289;&#22312;55&#20010;&#31867;&#21035;&#20013;&#27880;&#37322;&#65292;&#20854;&#31890;&#24230;&#36828;&#36828;&#36229;&#36807;&#20197;&#21069;&#21457;&#24067;&#30340;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;; &#65288;ii&#65289;&#34892;&#39033;&#30446;&#35782;&#21035;&#34920;&#31034;&#19968;&#39033;&#26497;&#20855;&#23454;&#29992;&#24615;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#22312;&#34920;&#26684;&#20013;&#24517;&#39035;&#23558;&#20851;&#38190;&#20449;&#24687;&#20998;&#37197;&#32473;&#39033;&#30446;; &#65288;iii&#65289;&#25991;&#26723;&#26469;&#33258;&#20247;&#22810;&#24067;&#23616;&#65292;&#27979;&#35797;&#38598;&#21253;&#25324;&#38646;-shot&#21644;&#23569;-shot&#26696;&#20363;&#20197;&#21450;&#35757;&#32451;&#38598;&#20013;&#24120;&#35265;&#30340;&#24067;&#23616;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#37197;&#26377;&#22810;&#20010;&#22522;&#32447;&#65292;&#21253;&#25324;RoBERTa&#12289; LayoutLMv3&#21644;&#22522;&#20110;DETR&#30340;&#34920;&#26684;Transformer&#65307;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the DocILE benchmark with the largest dataset of business documents for the tasks of Key Information Localization and Extraction and Line Item Recognition. It contains 6.7k annotated business documents, 100k synthetically generated documents, and nearly~1M unlabeled documents for unsupervised pre-training. The dataset has been built with knowledge of domainand task-specific aspects, resulting in the following key features: (i) annotations in 55 classes, which surpasses the granularity of previously published key information extraction datasets by a large margin; (ii) Line Item Recognition represents a highly practical information extraction task, where key information has to be assigned to items in a table; (iii) documents come from numerous layouts and the test set includes zero- and few-shot cases as well as layouts commonly seen in the training set. The benchmark comes with several baselines, including RoBERTa, LayoutLMv3 and DETR-based Table Transformer; app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;APAM&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#21644;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#38271;&#23614;&#21644;&#22122;&#22768;&#26631;&#31614;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26368;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.03488</link><description>&lt;p&gt;
APAM&#65306;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#21644;&#33258;&#36866;&#24212;&#20803;&#23398;&#20064;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#22788;&#29702;&#22122;&#22768;&#26631;&#31614;&#21644;&#38271;&#23614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
APAM: Adaptive Pre-training and Adaptive Meta Learning in Language Model for Noisy Labels and Long-tailed Learning. (arXiv:2302.03488v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;APAM&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#21644;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#38271;&#23614;&#21644;&#22122;&#22768;&#26631;&#31614;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#20219;&#21153;&#36890;&#24120;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#21644;&#38271;&#23614;&#20998;&#24067;&#30340;&#29305;&#28857;&#65292;&#36825;&#20123;&#38382;&#39064;&#20250;&#25361;&#25112;&#22797;&#26434;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#12290;&#24120;&#29992;&#30340;&#37325;&#25277;&#26679;&#25216;&#26415;&#65288;&#22914;&#36807;&#37319;&#26679;&#25110;&#27424;&#37319;&#26679;&#65289;&#23481;&#26131;&#23548;&#33268;&#36807;&#25311;&#21512;&#12290;&#20511;&#21161;&#23569;&#37327;&#20803;&#25968;&#25454;&#23398;&#20064;&#25968;&#25454;&#26435;&#37325;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23545;&#20110;&#24369;&#34920;&#31034;&#25968;&#25454;&#30340;&#20248;&#28857;&#24840;&#21457;&#26126;&#26174;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#38271;&#23614;&#21644;&#22122;&#22768;&#26631;&#31614;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#24335;&#36866;&#24212;&#38382;&#39064;&#22495;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#37325;&#26032;&#21152;&#26435;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#21487;&#20197;&#23398;&#20064;&#26174;&#24335;&#21152;&#26435;&#20989;&#25968;&#24182;&#26681;&#25454;&#20803;&#25968;&#25454;&#36827;&#34892;&#36866;&#24212;&#12290;&#25105;&#20204;&#22312;&#25439;&#22833;&#20989;&#25968;&#30340;&#39033;&#26435;&#37325;&#19978;&#36827;&#19968;&#27493;&#37319;&#29992;&#20102;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#22810;&#39033;&#24335;&#25193;&#23637;&#21644;&#37325;&#28857;&#27491;&#21017;&#21270;&#30340;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#33258;&#36866;&#24212;&#20803;&#23398;&#20064;&#26041;&#27861;&#25972;&#21512;&#21040;&#39044;&#35757;&#32451;&#38454;&#27573;&#20013;&#65292;&#20197;&#20174;&#24369;&#34920;&#31034;&#31867;&#20013;&#23398;&#20064;&#26356;&#20855;&#21487;&#20256;&#36882;&#24615;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#38271;&#23614;&#21644;&#22122;&#38899;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Practical natural language processing (NLP) tasks are commonly long-tailed with noisy labels. Those problems challenge the generalization and robustness of complex models such as Deep Neural Networks (DNNs). Some commonly used resampling techniques, such as oversampling or undersampling, could easily lead to overfitting. It is growing popular to learn the data weights leveraging a small amount of metadata. Besides, recent studies have shown the advantages of self-supervised pre-training, particularly to the under-represented data. In this work, we propose a general framework to handle the problem of both long-tail and noisy labels. The model is adapted to the domain of problems in a contrastive learning manner. The re-weighting module is a feed-forward network that learns explicit weighting functions and adapts weights according to metadata. The framework further adapts weights of terms in the loss function through a combination of the polynomial expansion of cross-entropy loss and foc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#23454;&#65292;&#20197;&#23383;&#20307;&#20026;&#22522;&#30784;&#30340;&#23383;&#31526;&#32423;&#25991;&#26412;&#32534;&#30721;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#30340;&#35270;&#35273;&#25991;&#26412;&#25490;&#29256;&#36136;&#37327;&#65292;&#36798;&#21040;&#26356;&#39640;&#30340;&#25340;&#20889;&#20934;&#30830;&#29575;&#21644;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.10562</link><description>&lt;p&gt;
&#22522;&#20110;&#23383;&#31526;&#30340;&#27169;&#22411;&#25552;&#21319;&#20102;&#35270;&#35273;&#25991;&#26412;&#28210;&#26579;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Character-Aware Models Improve Visual Text Rendering. (arXiv:2212.10562v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#23454;&#65292;&#20197;&#23383;&#20307;&#20026;&#22522;&#30784;&#30340;&#23383;&#31526;&#32423;&#25991;&#26412;&#32534;&#30721;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#30340;&#35270;&#35273;&#25991;&#26412;&#25490;&#29256;&#36136;&#37327;&#65292;&#36798;&#21040;&#26356;&#39640;&#30340;&#25340;&#20889;&#20934;&#30830;&#29575;&#21644;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#33391;&#22909;&#25490;&#29256;&#30340;&#35270;&#35273;&#25991;&#26412;&#26041;&#38754;&#23384;&#22312;&#36739;&#22823;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#32771;&#23519;&#20102;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#30446;&#21069;&#20027;&#27969;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#32570;&#20047;&#23383;&#31526;&#32423;&#21035;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#20351;&#24471;&#20197;&#23383;&#20307;&#20026;&#22522;&#30784;&#30340;&#35270;&#35273;&#25991;&#26412;&#21576;&#29616;&#26356;&#21152;&#22256;&#38590;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#23383;&#31526;&#30340;&#25991;&#26412;&#32534;&#30721;&#27169;&#22411;&#23545;&#20110;&#26032;&#39062;&#30340;&#25340;&#20889;&#20219;&#21153;&#65288;WikiSpell&#65289;&#25552;&#20379;&#20102;&#24456;&#22823;&#30340;&#24110;&#21161;&#12290;&#23558;&#36825;&#20123;&#32467;&#26524;&#24212;&#29992;&#20110;&#35270;&#35273;&#39046;&#22495;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#22871;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22522;&#20110;&#23383;&#31526;&#30340;&#21464;&#20307;&#22312;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25991;&#26412;&#28210;&#26579;&#20219;&#21153;&#65288;&#21363;DrawText benchmark&#65289;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#26080;&#23383;&#31526;&#36755;&#20837;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35270;&#35273;&#25340;&#20889;&#26041;&#38754;&#21462;&#24471;&#20102;&#36828;&#39640;&#20110;&#21516;&#39046;&#22495;&#23545;&#25163;&#30340;&#25104;&#26524;&#65292;&#22312;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#36229;&#36807;30&#20010;&#30334;&#20998;&#28857;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current image generation models struggle to reliably produce well-formed visual text. In this paper, we investigate a key contributing factor: popular text-to-image models lack character-level input features, making it much harder to predict a word's visual makeup as a series of glyphs. To quantify this effect, we conduct a series of experiments comparing character-aware vs. character-blind text encoders. In the text-only domain, we find that character-aware models provide large gains on a novel spelling task (WikiSpell). Applying our learnings to the visual domain, we train a suite of image generation models, and show that character-aware variants outperform their character-blind counterparts across a range of novel text rendering tasks (our DrawText benchmark). Our models set a much higher state-of-the-art on visual spelling, with 30+ point accuracy gains over competitors on rare words, despite training on far fewer examples.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21407;&#21017;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#26426;&#21046;&#24110;&#21161;&#27599;&#20010;&#26679;&#26412;&#25214;&#21040;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#25490;&#21015;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#34920;&#29616;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#22312;8&#20010;&#19981;&#21516;&#30340;NLP&#25968;&#25454;&#38598;&#19978;&#65292;&#33258;&#36866;&#24212;ICL&#26041;&#27861;&#30456;&#23545;&#20110;&#24120;&#35268;&#35774;&#32622;&#25552;&#39640;&#20102;40%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2212.10375</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;&#22522;&#20110;&#20449;&#24687;&#21387;&#32553;&#35270;&#35282;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#21462;&#21644;&#25490;&#24207;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering. (arXiv:2212.10375v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10375
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21407;&#21017;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#26426;&#21046;&#24110;&#21161;&#27599;&#20010;&#26679;&#26412;&#25214;&#21040;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#25490;&#21015;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#34920;&#29616;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#22312;8&#20010;&#19981;&#21516;&#30340;NLP&#25968;&#25454;&#38598;&#19978;&#65292;&#33258;&#36866;&#24212;ICL&#26041;&#27861;&#30456;&#23545;&#20110;&#24120;&#35268;&#35774;&#32622;&#25552;&#39640;&#20102;40%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064; (ICL) &#20013;&#20855;&#26377;&#24778;&#20154;&#30340;&#23569;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#20173;&#28982;&#26222;&#36941;&#37319;&#29992;&#38543;&#26426;&#36873;&#21462;&#26679;&#26412;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#20570;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ICL&#21407;&#21017;&#65306;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#24341;&#20837;&#33258;&#36866;&#24212;&#26426;&#21046;&#26469;&#24110;&#21161;&#27599;&#20010;&#26679;&#26412;&#25214;&#21040;&#19968;&#20010;&#33021;&#22815;&#24471;&#21040;&#27491;&#30830;&#39044;&#27979;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#25490;&#21015;&#65288;&#21363;&#36873;&#21462;&#21644;&#25490;&#24207;&#65289;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#34920;&#29616;&#12290;&#20026;&#20102;&#39564;&#35777;&#33258;&#36866;&#24212;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36873;&#25321;-&#25490;&#24207;&#26694;&#26550;&#65292;&#24182;&#23558;&#20854;&#29992;&#26032;&#30340;&#36873;&#25321;&#21644;&#25490;&#24207;&#31639;&#27861;&#23454;&#20363;&#21270;&#12290;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;NLP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;ICL&#26041;&#27861;&#30456;&#23545;&#20110;&#24120;&#35268;&#35774;&#32622;&#25552;&#39640;&#20102;40%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#25581;&#31034;&#20102;&#33258;&#36866;&#24212;ICL&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#21363;&#21487;&#33021;&#36890;&#36807;&#26356;&#20808;&#36827;&#30340;&#31639;&#27861;&#26469;&#32553;&#23567;ICL&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#21457;&#24067;&#20195;&#30721;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65306;https://github.com/jxlr/SAICL_iclr22&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the surprising few-shot performance of in-context learning (ICL), it is still a common practice to randomly sample examples to serve as context. This paper advocates a new principle for ICL: self-adaptive in-context learning. The self-adaption mechanism is introduced to help each sample find an in-context example permutation (i.e., selection and ordering) that can derive the correct prediction, thus maximizing performance. To validate the effectiveness of self-adaptive ICL, we propose a general select-then-rank framework and instantiate it with new selection and ranking algorithms. Upon extensive evaluation on eight different NLP datasets, our self-adaptive ICL method achieves a 40% relative improvement over the common practice setting. Further analysis reveals the enormous potential of self-adaptive ICL that it might be able to close the gap between ICL and finetuning given more advanced algorithms. Our code is released to facilitate future research in this area: https://githu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SeqDiffuSeq&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#65292;&#37319;&#29992;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26550;&#26500;&#21644;&#33258;&#36866;&#24212;&#22122;&#22768;&#35843;&#24230;&#25216;&#26415;&#65292;&#26088;&#22312;&#25506;&#32034;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.10325</link><description>&lt;p&gt;
SeqDiffuSeq: &#19968;&#31181;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SeqDiffuSeq: Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation. (arXiv:2212.10325v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SeqDiffuSeq&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#65292;&#37319;&#29992;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26550;&#26500;&#21644;&#33258;&#36866;&#24212;&#22122;&#22768;&#35843;&#24230;&#25216;&#26415;&#65292;&#26088;&#22312;&#25506;&#32034;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24314;&#27169;&#33539;&#24335;&#65292;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#25991;&#26412;&#30340;&#31163;&#25955;&#20998;&#31867;&#24615;&#36136;&#65292;&#23558;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#25193;&#23637;&#21040;&#33258;&#28982;&#35821;&#35328;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#32780;&#19988;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#30740;&#31350;&#36739;&#23569;&#12290;&#24207;&#21015;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#35805;&#39064;&#20043;&#19968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#65292;&#25506;&#32034;&#25193;&#25955;&#27169;&#22411;&#30340;&#20248;&#36234;&#29983;&#25104;&#24615;&#33021;&#33021;&#21542;&#36716;&#31227;&#21040;&#33258;&#28982;&#35821;&#35328;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;SeqDiffuSeq&#65292;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#12290;SeqDiffuSeq&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26550;&#26500;&#26469;&#24314;&#27169;&#21435;&#22122;&#20989;&#25968;&#12290;&#20026;&#20102;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;SeqDiffuSeq&#32467;&#21512;&#20102;&#33258;&#25105;&#35843;&#33410;&#25216;&#26415;&#21644;&#19968;&#20010;&#26032;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#22122;&#22768;&#35843;&#24230;&#25216;&#26415;&#12290;&#33258;&#36866;&#24212;&#22122;&#22768;&#35843;&#24230;&#20855;&#26377;&#22343;&#21248;&#21435;&#22122;&#30340;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Diffusion model, a new generative modelling paradigm, has achieved great success in image, audio, and video generation. However, considering the discrete categorical nature of text, it is not trivial to extend continuous diffusion models to natural language, and text diffusion models are less studied. Sequence-to-sequence text generation is one of the essential natural language processing topics. In this work, we apply diffusion models to approach sequence-to-sequence text generation, and explore whether the superiority generation performance of diffusion model can transfer to natural language domain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequence generation. SeqDiffuSeq uses an encoder-decoder Transformers architecture to model denoising function. In order to improve generation quality, SeqDiffuSeq combines the self-conditioning technique and a newly proposed adaptive noise schedule technique. The adaptive noise schedule has the difficulty of denoising evenly 
&lt;/p&gt;</description></item><item><title>Pangu&#26159;&#19968;&#20010;&#27867;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#19982;&#29616;&#23454;&#29615;&#22659;&#30340;&#25509;&#36712;&#65292;&#23427;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#36776;&#21035;&#33021;&#21147;&#32780;&#38750;&#29983;&#25104;&#33021;&#21147;&#65292;&#30001;&#19968;&#20010;&#31526;&#21495;&#20195;&#29702;&#21644;&#19968;&#20010;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#24037;&#20316;&#12290;&#36825;&#19968;&#26041;&#26696;&#24050;&#32463;&#22312;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.09736</link><description>&lt;p&gt;
&#19981;&#29983;&#25104;&#65292;&#36776;&#21035;&#65306;&#19968;&#31181;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#25509;&#36712;&#30340;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments. (arXiv:2212.09736v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09736
&lt;/p&gt;
&lt;p&gt;
Pangu&#26159;&#19968;&#20010;&#27867;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#19982;&#29616;&#23454;&#29615;&#22659;&#30340;&#25509;&#36712;&#65292;&#23427;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#36776;&#21035;&#33021;&#21147;&#32780;&#38750;&#29983;&#25104;&#33021;&#21147;&#65292;&#30001;&#19968;&#20010;&#31526;&#21495;&#20195;&#29702;&#21644;&#19968;&#20010;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#24037;&#20316;&#12290;&#36825;&#19968;&#26041;&#26696;&#24050;&#32463;&#22312;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#35821;&#35328;&#27169;&#22411;&#26368;&#32570;&#22833;&#30340;&#23601;&#26159;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#30340;&#25509;&#36712;&#24615;&#12290;&#24050;&#26377;&#30340;&#30456;&#20851;&#24037;&#20316;&#23558;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#30452;&#25509;&#29983;&#25104;&#35745;&#21010;&#65292;&#20197;&#20415;&#22312;&#29615;&#22659;&#20013;&#25191;&#34892;&#20197;&#36798;&#21040;&#39044;&#26399;&#30340;&#25928;&#26524;&#65292;&#36825;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#36127;&#25285;&#20102;&#30830;&#20445;&#35821;&#27861;&#27491;&#30830;&#24615;&#12289;&#24544;&#23454;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;&#37325;&#25285;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27867;&#29992;&#30340;&#26694;&#26550;Pangu&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#19982;&#29616;&#23454;&#29615;&#22659;&#30340;&#25509;&#36712;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#36776;&#21035;&#33021;&#21147;&#32780;&#38750;&#29983;&#25104;&#33021;&#21147;&#65292;&#30001;&#19968;&#20010;&#31526;&#21495;&#20195;&#29702;&#21644;&#19968;&#20010;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#24037;&#20316;&#65306;&#20195;&#29702;&#22312;&#29615;&#22659;&#20013;&#25506;&#32034;&#20197;&#36880;&#27493;&#26500;&#24314;&#26377;&#25928;&#30340;&#35745;&#21010;&#65292;&#32780;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22791;&#36873;&#35745;&#21010;&#30340;&#21512;&#29702;&#24615;&#20197;&#24341;&#23548;&#25628;&#32034;&#36807;&#31243;&#12290;&#38024;&#23545;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#38382;&#39064;&#20855;&#26377;&#24222;&#22823;&#30340;&#29615;&#22659;&#65292;&#32467;&#26524;&#34920;&#26126;&#20102;Pangu&#30340;&#26174;&#33879;&#26377;&#25928;&#24615;&#21644;&#28789;&#27963;&#24615;&#65306;BERT&#22522;&#35821;&#35328;&#27169;&#22411;&#24050;&#36275;&#22815;&#24212;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key missing capacity of current language models (LMs) is grounding to real-world environments. Most existing work for grounded language understanding uses LMs to directly generate plans that can be executed in the environment to achieve the desired effects. It thereby casts the burden of ensuring grammaticality, faithfulness, and controllability all on the LMs. We propose Pangu, a generic framework for grounded language understanding that capitalizes on the discriminative ability of LMs instead of their generative ability. Pangu consists of a symbolic agent and a neural LM working in a concerted fashion: The agent explores the environment to incrementally construct valid plans, and the LM evaluates the plausibility of the candidate plans to guide the search process. A case study on the challenging problem of knowledge base question answering (KBQA), which features a massive environment, demonstrates the remarkable effectiveness and flexibility of Pangu: A BERT-base LM is sufficient f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#21327;&#21516;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#26032;&#30340;&#34394;&#20551;&#20449;&#24687;&#22768;&#26126;&#24182;&#35782;&#21035;&#25903;&#25345;&#23427;&#20204;&#30340;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#12290;&#22312;COVID-19&#27835;&#30103;&#30340;&#26696;&#20363;&#20013;&#65292;&#22522;&#20110;&#29616;&#20195;NLP&#26041;&#27861;&#24320;&#21457;&#22522;&#32447;&#31995;&#32479;&#65292;&#24182;&#23637;&#31034;&#20102;&#20154;&#31867;&#20107;&#23454;&#26680;&#26597;&#20154;&#21592;&#27599;&#23567;&#26102;&#21487;&#20197;&#35782;&#21035;&#20986;&#36829;&#21453;Twitter&#20851;&#20110;COVID-19&#34394;&#20551;&#20449;&#24687;&#26041;&#38024;&#30340;124&#26465;&#25512;&#25991;&#12290;</title><link>http://arxiv.org/abs/2212.09683</link><description>&lt;p&gt;
&#20154;&#26426;&#21327;&#21516;&#35780;&#20272;&#26089;&#26399;&#35823;&#20256;&#20449;&#24687;&#26816;&#27979;&#65306;COVID-19&#27835;&#30103;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments. (arXiv:2212.09683v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09683
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#21327;&#21516;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#26032;&#30340;&#34394;&#20551;&#20449;&#24687;&#22768;&#26126;&#24182;&#35782;&#21035;&#25903;&#25345;&#23427;&#20204;&#30340;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#12290;&#22312;COVID-19&#27835;&#30103;&#30340;&#26696;&#20363;&#20013;&#65292;&#22522;&#20110;&#29616;&#20195;NLP&#26041;&#27861;&#24320;&#21457;&#22522;&#32447;&#31995;&#32479;&#65292;&#24182;&#23637;&#31034;&#20102;&#20154;&#31867;&#20107;&#23454;&#26680;&#26597;&#20154;&#21592;&#27599;&#23567;&#26102;&#21487;&#20197;&#35782;&#21035;&#20986;&#36829;&#21453;Twitter&#20851;&#20110;COVID-19&#34394;&#20551;&#20449;&#24687;&#26041;&#38024;&#30340;124&#26465;&#25512;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#26426;&#21327;&#21516;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#20107;&#23454;&#26680;&#26597;&#26032;&#30340;&#34394;&#20551;&#20449;&#24687;&#22768;&#26126;&#24182;&#35782;&#21035;&#25903;&#25345;&#23427;&#20204;&#30340;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21462;&#20540;&#24471;&#26680;&#26597;&#30340;&#22768;&#26126;&#65292;&#36825;&#20123;&#22768;&#26126;&#34987;&#32858;&#21512;&#24182;&#25490;&#21517;&#20197;&#20415;&#22797;&#23457;&#12290;&#28982;&#21518;&#20351;&#29992;&#31435;&#22330;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#25903;&#25345;&#26032;&#34394;&#20551;&#20449;&#24687;&#30003;&#36848;&#30340;&#25512;&#25991;&#65292;&#36827;&#19968;&#27493;&#26816;&#26597;&#20197;&#30830;&#23450;&#23427;&#20204;&#26159;&#21542;&#36829;&#21453;&#30456;&#20851;&#25919;&#31574;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#22312;COVID-19&#27835;&#30103;&#39046;&#22495;&#22522;&#20110;&#29616;&#20195;NLP&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#32447;&#31995;&#32479;&#29992;&#20110;&#20154;&#26426;&#21327;&#21516;&#20107;&#23454;&#26680;&#26597;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#22522;&#32447;&#31995;&#32479;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20154;&#31867;&#20107;&#23454;&#26680;&#26597;&#20154;&#21592;&#27599;&#23567;&#26102;&#33021;&#22815;&#35782;&#21035;&#20986;&#36829;&#21453;Twitter&#20851;&#20110;COVID-19&#34394;&#20551;&#20449;&#24687;&#26041;&#38024;&#30340;124&#26465;&#25512;&#25991;&#12290;&#25105;&#20204;&#23558;&#25552;&#20379;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#25968;&#25454;&#12289;&#22522;&#32447;&#27169;&#22411;&#21644;&#35814;&#32454;&#27880;&#37322;&#25351;&#21335;&#26469;&#25903;&#25345;&#20154;&#26426;&#21327;&#21516;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#30452;&#25509;&#20174;&#21407;&#22987;&#29992;&#25143;&#29983;&#25104;&#30340;&#20869;&#23481;&#20013;&#35782;&#21035;&#26032;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a human-in-the-loop evaluation framework for fact-checking novel misinformation claims and identifying social media messages that support them. Our approach extracts check-worthy claims, which are aggregated and ranked for review. Stance classifiers are then used to identify tweets supporting novel misinformation claims, which are further reviewed to determine whether they violate relevant policies. To demonstrate the feasibility of our approach, we develop a baseline system based on modern NLP methods for human-in-the-loop fact-checking in the domain of COVID-19 treatments. Using our baseline system, we show that human fact-checkers can identify 124 tweets per hour that violate Twitter's policies on COVID-19 misinformation. We will make our code, data, baseline models, and detailed annotation guidelines available to support the evaluation of human-in-the-loop systems that identify novel misinformation directly from raw user-generated content.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#25552;&#31034;(PARC)&#31649;&#36947;&#65292;&#22312;&#38646;-shot&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#36890;&#36807;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#26816;&#32034;&#20986;&#30340;&#35821;&#20041;&#19978;&#31867;&#20284;&#30340;&#21477;&#23376;&#26469;&#25913;&#21892;&#24615;&#33021;&#65292;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110; fine-tuning &#22522;&#32447;&#65292;&#21516;&#26102;&#19982;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#20302;&#36164;&#28304;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#23384;&#22312;&#26174;&#33879;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2212.09651</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages. (arXiv:2212.09651v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#25552;&#31034;(PARC)&#31649;&#36947;&#65292;&#22312;&#38646;-shot&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#36890;&#36807;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#26816;&#32034;&#20986;&#30340;&#35821;&#20041;&#19978;&#31867;&#20284;&#30340;&#21477;&#23376;&#26469;&#25913;&#21892;&#24615;&#33021;&#65292;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110; fine-tuning &#22522;&#32447;&#65292;&#21516;&#26102;&#19982;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#20302;&#36164;&#28304;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#23384;&#22312;&#26174;&#33879;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(MPLMs)&#22312;&#26368;&#36817;&#30340;&#32463;&#39564;&#36328;&#35821;&#35328;&#36716;&#31227;&#30740;&#31350;&#20013;&#23637;&#29616;&#20102;&#20854;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;(PARC)&#31649;&#36947;&#65292;&#36890;&#36807;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;(HRL)&#20013;&#26816;&#32034;&#20986;&#30340;&#35821;&#20041;&#19978;&#31867;&#20284;&#30340;&#21477;&#23376;&#20316;&#20026;&#25552;&#31034;&#26469;&#25913;&#21892;&#38646;-shot&#20302;&#36164;&#28304;&#35821;&#35328;(LRLs)&#30340;&#24615;&#33021;&#12290;PARC&#36890;&#36807;&#22810;&#35821;&#35328;&#24182;&#34892;&#27979;&#35797;&#38598;&#22312;&#19977;&#20010;&#19979;&#28216;&#20219;&#21153;(&#20108;&#20803;&#24773;&#24863;&#20998;&#31867;&#12289;&#20027;&#39064;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;)&#19978;&#25552;&#39640;&#20102;&#38646;-shot&#30340;&#24615;&#33021;&#65292;&#35206;&#30422;&#20102;10&#20010;LRLs&#65292;&#28085;&#30422;&#20102;6&#31181;&#35821;&#35328;&#23478;&#26063;&#65292;&#22312;&#26410;&#26631;&#35760;&#30340;&#35774;&#32622;&#20013;&#25552;&#39640;&#20102;(+5.1%)&#65292;&#22312;&#26631;&#35760;&#30340;&#35774;&#32622;&#20013;&#25552;&#39640;&#20102;(+16.3%)&#12290;PARC&#26631;&#35760;&#36824;&#36229;&#36234;&#20102; fine-tuning &#22522;&#32447;3.7%&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#24615;&#33021;&#22312;&#19968;&#26041;&#38754;&#19982;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#20302;&#36164;&#28304;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual Pretrained Language Models (MPLMs) have shown their strong multilinguality in recent empirical cross-lingual transfer studies. In this paper, we propose the Prompts Augmented by Retrieval Crosslingually (PARC) pipeline to improve the zero-shot performance on low-resource languages (LRLs) by augmenting the context with semantically similar sentences retrieved from a high-resource language (HRL) as prompts. PARC improves the zero-shot performance on three downstream tasks (binary sentiment classification, topic categorization and natural language inference) with multilingual parallel test sets across 10 LRLs covering 6 language families in both unlabeled settings (+5.1%) and labeled settings (+16.3%). PARC-labeled also outperforms the finetuning baseline by 3.7%. We find a significant positive correlation between cross-lingual transfer performance on one side, and the similarity between the high- and low-resource languages as well as the amount of low-resource pretraining da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26041;&#27861;&#26469;&#20174;&#20219;&#21153;&#35828;&#26126;&#20013;&#23398;&#20064;&#65292;&#20197;&#22788;&#29702;&#35828;&#26126;&#30340;&#21464;&#21270;&#24182;&#25552;&#39640;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.03813</link><description>&lt;p&gt;
&#20174;&#20219;&#21153;&#35828;&#26126;&#20070;&#20013;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustness of Learning from Task Instructions. (arXiv:2212.03813v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26041;&#27861;&#26469;&#20174;&#20219;&#21153;&#35828;&#26126;&#20013;&#23398;&#20064;&#65292;&#20197;&#22788;&#29702;&#35828;&#26126;&#30340;&#21464;&#21270;&#24182;&#25552;&#39640;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#30417;&#30563;&#23398;&#20064;&#22823;&#22810;&#22312;&#20010;&#21035;&#20219;&#21153;&#19978;&#36827;&#34892;&#65292;&#24182;&#38656;&#35201;&#22312;&#22823;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#19978;&#35757;&#32451;&#12290;&#36825;&#31181;&#33539;&#24335;&#20005;&#37325;&#38459;&#30861;&#20102;&#20219;&#21153;&#27010;&#25324;&#30340;&#21457;&#23637;&#65292;&#22240;&#20026;&#20934;&#22791;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#38598;&#26159;&#26114;&#36149;&#30340;&#12290;&#20026;&#20102;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#24555;&#36895;&#36731;&#26494;&#22320;&#25512;&#24191;&#21040;&#26032;&#20219;&#21153;&#30340;&#31995;&#32479;&#65292;&#26368;&#36817;&#37319;&#29992;&#20102;&#20219;&#21153;&#35828;&#26126;&#20316;&#20026;&#30417;&#30563;&#30340;&#26032;&#20852;&#36235;&#21183;&#12290;&#36825;&#20123;&#35828;&#26126;&#32473;&#27169;&#22411;&#23450;&#20041;&#20102;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#27169;&#22411;&#26681;&#25454;&#35828;&#26126;&#21644;&#36755;&#20837;&#36755;&#20986;&#36866;&#24403;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#20219;&#21153;&#35828;&#26126;&#36890;&#24120;&#20197;&#19981;&#21516;&#24418;&#24335;&#34920;&#36798;&#65292;&#21487;&#20197;&#20174;&#20004;&#20010;&#32447;&#32034;&#20013;&#35299;&#37322;&#65306;&#39318;&#20808;&#65292;&#19968;&#20123;&#35828;&#26126;&#26159;&#30701;&#21477;&#65292;&#24182;&#19988;&#26159;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#23548;&#21521;&#65292;&#20363;&#22914;&#25552;&#31034;&#65292;&#32780;&#20854;&#20182;&#35828;&#26126;&#26159;&#27573;&#33853;&#65292;&#24182;&#19988;&#26159;&#20154;&#20026;&#23548;&#21521;&#30340;&#65292;&#20363;&#22914;&#20122;&#39532;&#36874;&#30340;MTurk; &#20854;&#27425;&#65292;&#19981;&#21516;&#30340;&#26368;&#32456;&#29992;&#25143;&#24456;&#21487;&#33021;&#29992;&#19981;&#21516;&#30340;&#25991;&#26412;&#34920;&#36798;&#26041;&#24335;&#35299;&#37322;&#30456;&#21516;&#30340;&#20219;&#21153;&#12290;&#38656;&#35201;&#19968;&#31181;&#40065;&#26834;&#30340;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#20219;&#21153;&#35828;&#26126;&#30340;&#21487;&#21464;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26041;&#27861;&#26469;&#20174;&#20219;&#21153;&#35828;&#26126;&#20013;&#23398;&#20064;&#65292;&#21487;&#20197;&#22788;&#29702;&#35828;&#26126;&#30340;&#21464;&#21270;&#24182;&#25913;&#21892;&#23545;&#26032;&#20219;&#21153;&#30340;&#27010;&#25324;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional supervised learning mostly works on individual tasks and requires training on a large set of task-specific examples. This paradigm seriously hinders the development of task generalization since preparing a task-specific example set is costly. To build a system that can quickly and easily generalize to new tasks, task instructions have been adopted as an emerging trend of supervision recently. These instructions give the model the definition of the task and allow the model to output the appropriate answer based on the instructions and inputs. However, task instructions are often expressed in different forms, which can be interpreted from two threads: first, some instructions are short sentences and are pretrained language model (PLM) oriented, such as prompts, while other instructions are paragraphs and are human-oriented, such as those in Amazon MTurk; second, different end-users very likely explain the same task with instructions of different textual expressions. A robust 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29992;&#25143;&#21382;&#21490;&#35821;&#35328;&#24314;&#27169;&#21487;&#20197;&#22312;&#19981;&#21516;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#24322;&#32467;&#26524;&#65292;&#24182;&#19988;&#21033;&#29992;&#20219;&#21153;&#26080;&#20851;&#30340;&#29992;&#25143;&#21382;&#21490;&#36824;&#21487;&#20197;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.03760</link><description>&lt;p&gt;
&#35821;&#35328;&#24314;&#27169;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65306;&#20016;&#23500;&#20219;&#21153;&#29305;&#23450;&#21644;&#20219;&#21153;&#26080;&#20851;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning. (arXiv:2212.03760v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29992;&#25143;&#21382;&#21490;&#35821;&#35328;&#24314;&#27169;&#21487;&#20197;&#22312;&#19981;&#21516;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#24322;&#32467;&#26524;&#65292;&#24182;&#19988;&#21033;&#29992;&#20219;&#21153;&#26080;&#20851;&#30340;&#29992;&#25143;&#21382;&#21490;&#36824;&#21487;&#20197;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#30340;&#32479;&#19968;&#29992;&#25143;&#24314;&#27169;&#26694;&#26550;&#12290;&#20854;&#20013;&#35768;&#22810;&#21463;&#30410;&#20110;&#23558;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#20316;&#20026;&#32431;&#25991;&#26412;&#20351;&#29992;&#65292;&#20195;&#34920;&#30528;&#20219;&#20309;&#39046;&#22495;&#25110;&#31995;&#32479;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#32780;&#19981;&#22833;&#36890;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#38382;&#39064;&#20135;&#29983;&#20102;&#65306;&#29992;&#25143;&#21382;&#21490;&#35821;&#35328;&#24314;&#27169;&#33021;&#21542;&#24110;&#21161;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#65311;&#34429;&#28982;&#35821;&#35328;&#24314;&#27169;&#30340;&#22810;&#21151;&#33021;&#24615;&#24050;&#22312;&#35768;&#22810;&#39046;&#22495;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20854;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#20173;&#26410;&#28145;&#20837;&#25506;&#35752;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30452;&#25509;&#24212;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#21382;&#21490;&#30340;&#35821;&#35328;&#24314;&#27169;&#22312;&#19981;&#21516;&#30340;&#25512;&#33616;&#20219;&#21153;&#19978;&#21487;&#20197;&#21462;&#24471;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#20219;&#21153;&#26080;&#20851;&#30340;&#29992;&#25143;&#21382;&#21490;&#36824;&#21487;&#20197;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20026;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#26377;&#21069;&#36884;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#26410;&#30693;&#22495;&#21644;&#26381;&#21153;&#19978;&#20063;&#21487;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have proposed unified user modeling frameworks that leverage user behavior data from various applications. Many of them benefit from utilizing users' behavior sequences as plain texts, representing rich information in any domain or system without losing generality. Hence, a question arises: Can language modeling for user history corpus help improve recommender systems? While its versatile usability has been widely investigated in many domains, its applications to recommender systems still remain underexplored. We show that language modeling applied directly to task-specific user histories achieves excellent results on diverse recommendation tasks. Also, leveraging additional task-agnostic user histories delivers significant performance benefits. We further demonstrate that our approach can provide promising transfer learning capabilities for a broad spectrum of real-world recommender systems, even on unseen domains and services.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#20013;&#30340;&#38899;&#39057;&#21644;&#35270;&#39057;&#36827;&#34892;&#22122;&#22768;&#25233;&#21046;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#33258;&#25105;&#20013;&#24515;&#35270;&#35273;&#20449;&#24687;&#23545;&#20110;&#29983;&#25104;&#21152;&#24615;&#26657;&#27491;&#25513;&#30721;&#26368;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2211.03643</link><description>&lt;p&gt;
&#33258;&#25105;&#20013;&#24515;&#30340;&#38899;&#35270;&#39057;&#22122;&#22768;&#25233;&#21046;
&lt;/p&gt;
&lt;p&gt;
Egocentric Audio-Visual Noise Suppression. (arXiv:2211.03643v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#20013;&#30340;&#38899;&#39057;&#21644;&#35270;&#39057;&#36827;&#34892;&#22122;&#22768;&#25233;&#21046;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#33258;&#25105;&#20013;&#24515;&#35270;&#35273;&#20449;&#24687;&#23545;&#20110;&#29983;&#25104;&#21152;&#24615;&#26657;&#27491;&#25513;&#30721;&#26368;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#65288;&#21363;&#26410;&#25429;&#33719;&#21040;&#35270;&#39057;&#20013;&#30340;&#35762;&#35805;&#32773;&#65289;&#30340;&#38899;&#35270;&#39057;&#22122;&#22768;&#25233;&#21046;&#12290;&#30456;&#27604;&#20110;&#20197;&#21069;&#20381;&#36182;&#20110;&#21767;&#37096;&#21644;&#38754;&#37096;&#35270;&#35273;&#30340;&#38899;&#35270;&#39057;&#22686;&#24378;&#25216;&#26415;&#65292;&#26412;&#25991;&#20013;&#25152;&#20351;&#29992;&#30340;&#25668;&#20687;&#22836;&#22312;&#30011;&#38754;&#20013;&#25429;&#25417;&#21040;&#30340;&#26159;&#28508;&#22312;&#22122;&#22768;&#28304;&#25152;&#30475;&#21040;&#30340;&#22806;&#37096;&#19990;&#30028;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#20013;&#24515;&#35270;&#35273;&#20449;&#24687;&#23545;&#20110;&#22122;&#22768;&#25233;&#21046;&#26377;&#24110;&#21161;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#29289;&#20307;&#35782;&#21035;&#21644;&#21160;&#20316;&#20998;&#31867;&#30340;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#30740;&#31350;&#20102;&#23558;&#38899;&#39057;&#21644;&#35270;&#35273;&#34920;&#31034;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#20197;&#21450;&#22312;&#22122;&#22768;&#25233;&#21046;&#27169;&#22411;&#20013;&#25972;&#21512;&#35270;&#35273;&#20449;&#24687;&#30340;&#20301;&#32622;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35270;&#35273;&#29305;&#24449;&#23545;&#20110;&#29983;&#25104;&#21152;&#24615;&#26657;&#27491;&#25513;&#30721;&#26368;&#26377;&#24110;&#21161;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#30830;&#20445;&#35270;&#35273;&#29305;&#24449;&#22312;&#19981;&#21516;&#22122;&#22768;&#26465;&#20214;&#19979;&#26377;&#36776;&#35782;&#24230;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22122;&#22768;&#33258;&#36866;&#24212;&#30340;&#35757;&#32451;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies audio-visual noise suppression for egocentric videos -where the speaker is not captured in the video. Instead, potential noise sources are visible on screen with the camera emulating the off-screen speaker's view of the outside world. This setting is different from prior work in audio-visual speech enhancement that relies on lip and facial visuals. In this paper, we first demonstrate that egocentric visual information is helpful for noise suppression. We compare object recognition and action classification-based visual feature extractors and investigate methods to align audio and visual representations. Then, we examine different fusion strategies for the aligned features, and locations within the noise suppression model to incorporate visual information. Experiments demonstrate that visual features are most helpful when used to generate additive correction masks. Finally, in order to ensure that the visual features are discriminative with respect to different nois
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#23545;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#24378;&#38544;&#31169;&#20445;&#25252;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#25991;&#26412;&#65292;&#24182;&#19988;&#19982;&#38750;&#38544;&#31169;&#29256;&#26412;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2210.14348</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#21512;&#25104;&#25991;&#26412;&#29983;&#25104;&#65306;&#19968;&#20010;&#31616;&#21333;&#32780;&#23454;&#29992;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe. (arXiv:2210.14348v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14348
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#23545;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#24378;&#38544;&#31169;&#20445;&#25252;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#25991;&#26412;&#65292;&#24182;&#19988;&#19982;&#38750;&#38544;&#31169;&#29256;&#26412;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#25935;&#24863;&#25968;&#25454;&#30340;&#35760;&#24518;&#20542;&#21521;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#38544;&#31169;&#38382;&#39064;&#25104;&#20026;&#25968;&#25454;&#39537;&#21160;&#20135;&#21697;&#30340;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#29983;&#25104;&#20855;&#26377;&#24418;&#24335;&#38544;&#31169;&#20445;&#35777;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#22914;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#65292;&#20026;&#32531;&#35299;&#36825;&#20123;&#38544;&#31169;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#20294;&#26159;&#65292;&#20197;&#24448;&#30340;&#25351;&#21521;&#27492;&#26041;&#21521;&#30340;&#26041;&#27861;&#36890;&#24120;&#26410;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24046;&#20998;&#38544;&#31169;&#19979;&#25991;&#26412;&#39046;&#22495;&#30340;&#19968;&#20010;&#31616;&#21333;&#23454;&#29992;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#23545;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#24182;&#21152;&#20837;DP&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#24378;&#38544;&#31169;&#20445;&#25252;&#30340;&#26377;&#29992;&#30340;&#21512;&#25104;&#25991;&#26412;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#25968;&#25454;&#21644;&#31169;&#20154;&#23458;&#25143;&#25968;&#25454;&#30340;&#24191;&#27867;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#19982;&#38750;&#38544;&#31169;&#29256;&#26412;&#30456;&#20284;&#30340;&#23454;&#29992;&#24615;&#30340;&#21512;&#25104;&#25991;&#26412;&#65292;&#21516;&#26102;&#25552;&#20379;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#36991;&#20813;&#28508;&#22312;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy concerns have attracted increasing attention in data-driven products due to the tendency of machine learning models to memorize sensitive training data. Generating synthetic versions of such data with a formal privacy guarantee, such as differential privacy (DP), provides a promising path to mitigating these privacy concerns, but previous approaches in this direction have typically failed to produce synthetic data of high quality. In this work, we show that a simple and practical recipe in the text domain is effective: simply fine-tuning a pretrained generative language model with DP enables the model to generate useful synthetic text with strong privacy protection. Through extensive empirical analyses on both benchmark and private customer data, we demonstrate that our method produces synthetic text that is competitive in terms of utility with its non-private counterpart, meanwhile providing strong protection against potential privacy leakages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#21516;&#20041;&#35789;&#26367;&#25442;&#25915;&#20987;&#30340;&#29616;&#29366;&#65292;&#21457;&#29616;&#24403;&#21069;&#26041;&#27861;&#23384;&#22312;&#26080;&#35299;&#20915;&#30340;&#38556;&#30861;&#65292;&#29983;&#25104;&#30340;&#25932;&#23545;&#26679;&#26412;&#25928;&#26524;&#19981;&#20339;&#65292;&#38656;&#35201;&#22312;&#26410;&#26469;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.02844</link><description>&lt;p&gt;
&#31163;&#30495;&#27491;&#30340;&#21516;&#20041;&#35789;&#26367;&#25442;&#25915;&#20987;&#36824;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Are We from Real Synonym Substitution Attacks?. (arXiv:2210.02844v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#21516;&#20041;&#35789;&#26367;&#25442;&#25915;&#20987;&#30340;&#29616;&#29366;&#65292;&#21457;&#29616;&#24403;&#21069;&#26041;&#27861;&#23384;&#22312;&#26080;&#35299;&#20915;&#30340;&#38556;&#30861;&#65292;&#29983;&#25104;&#30340;&#25932;&#23545;&#26679;&#26412;&#25928;&#26524;&#19981;&#20339;&#65292;&#38656;&#35201;&#22312;&#26410;&#26469;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#36317;&#31163;&#30495;&#27491;&#30340;&#21516;&#20041;&#35789;&#26367;&#25442;&#25915;&#20987;&#26377;&#22810;&#36828;&#65311;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#21516;&#20041;&#35789;&#26367;&#25442;&#25915;&#20987;&#22914;&#20309;&#26367;&#25442;&#21407;&#22987;&#21477;&#23376;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#26174;&#31034;&#24403;&#21069;&#21516;&#20041;&#35789;&#26367;&#25442;&#25915;&#20987;&#20173;&#23384;&#22312;&#26410;&#35299;&#20915;&#30340;&#38556;&#30861;&#20351;&#24471;&#29983;&#25104;&#30340;&#25932;&#23545;&#26679;&#26412;&#26159;&#26080;&#25928;&#30340;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#22235;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21333;&#35789;&#26367;&#25442;&#26041;&#27861;&#29983;&#25104;&#22823;&#37327;&#26080;&#25928;&#26367;&#25442;&#35789;&#65292;&#36825;&#20123;&#35789;&#26159;&#19981;&#21512;&#35821;&#27861;&#30340;&#25110;&#19981;&#31526;&#21512;&#21407;&#22987;&#21477;&#23376;&#30340;&#35821;&#20041;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26816;&#27979;&#26080;&#25928;&#25932;&#23545;&#26679;&#26412;&#26041;&#38754;&#65292;SSA&#25152;&#20351;&#29992;&#30340;&#35821;&#20041;&#21644;&#35821;&#27861;&#32422;&#26463;&#26159;&#39640;&#24230;&#19981;&#36275;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#26410;&#26469;&#26500;&#24314;&#26356;&#22909;&#30340;SSA&#30340;&#37325;&#35201;&#22522;&#30707;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the following question: how far are we from real synonym substitution attacks (SSAs). We approach this question by examining how SSAs replace words in the original sentence and show that there are still unresolved obstacles that make current SSAs generate invalid adversarial samples. We reveal that four widely used word substitution methods generate a large fraction of invalid substitution words that are ungrammatical or do not preserve the original sentence's semantics. Next, we show that the semantic and grammatical constraints used in SSAs for detecting invalid word replacements are highly insufficient in detecting invalid adversarial samples. Our work is an important stepping stone to constructing better SSAs in the future.
&lt;/p&gt;</description></item><item><title>ContraCLM&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#22686;&#24378;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#21306;&#20998;&#24615;&#24182;&#36866;&#29992;&#20110;&#36229;&#20986;&#35821;&#35328;&#29983;&#25104;&#30340;&#20219;&#21153;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;ContraCLM&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.01185</link><description>&lt;p&gt;
ContraCLM&#65306;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ContraCLM: Contrastive Learning For Causal Language Model. (arXiv:2210.01185v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01185
&lt;/p&gt;
&lt;p&gt;
ContraCLM&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#22686;&#24378;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#21306;&#20998;&#24615;&#24182;&#36866;&#29992;&#20110;&#36229;&#20986;&#35821;&#35328;&#29983;&#25104;&#30340;&#20219;&#21153;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;ContraCLM&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#65292;&#20294;&#20854;&#34920;&#31034;&#30340;&#34920;&#29616;&#21147;&#21463;&#21040;&#36739;&#24046;&#30340;&#21306;&#20998;&#33021;&#21147;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; ContraCLM&#65292;&#23427;&#22312;&#26631;&#35760;&#32423;&#21035;&#21644;&#24207;&#21015;&#32423;&#21035;&#19978;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102; ContraCLM&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#22686;&#24378;&#20102;&#34920;&#31034;&#30340;&#21306;&#20998;&#24615;&#65292;&#24182;&#24357;&#21512;&#20102;&#20165;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#24046;&#36317;&#65292;&#36825;&#20351;&#24471;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#21512;&#20110;&#36229;&#20986;&#35821;&#35328;&#29983;&#25104;&#30340;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;44%&#30340;&#30456;&#23545;&#25913;&#36827;&#65292;&#22312;&#20195;&#30721;&#23545;&#20195;&#30721;&#25628;&#32034;&#20219;&#21153;&#19978;&#33719;&#24471;&#20102;34%&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25552;&#39640;&#34920;&#31034;&#30340;&#34920;&#29616;&#21147;&#65292;ContraCLM &#36824;&#25552;&#39640;&#20102;&#28304;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#65292;&#22312; HumanEval &#22522;&#20934;&#27979;&#35797;&#20013;&#25191;&#34892;&#31934;&#24230;&#30456;&#23545;&#25552;&#39640;&#20102;9%&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite exciting progress in causal language models, the expressiveness of the representations is largely limited due to poor discrimination ability. To remedy this issue, we present ContraCLM, a novel contrastive learning framework at both token-level and sequence-level. We assess ContraCLM on a variety of downstream tasks. We show that ContraCLM enhances discrimination of the representations and bridges the gap with the encoder-only models, which makes causal language models better suited for tasks beyond language generation. Specifically, we attain $44\%$ relative improvement on the Semantic Textual Similarity tasks and $34\%$ on Code-to-Code Search tasks. Furthermore, by improving the expressiveness of the representations, ContraCLM also boosts the source code generation capability with $9\%$ relative improvement on execution accuracy on the HumanEval benchmark.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#25353;&#38656;&#38598;&#25104;&#21040;&#26680;&#24515;&#27169;&#22411;&#20013;&#30340;&#29420;&#31435;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#65292;&#22312;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#32531;&#35299;&#20559;&#24046;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.15171</link><description>&lt;p&gt;
&#24102;&#26377;&#23646;&#24615;&#21024;&#38500;&#23376;&#32593;&#32476;&#30340;&#27169;&#22359;&#21270;&#21644;&#25353;&#38656;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks. (arXiv:2205.15171v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15171
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#25353;&#38656;&#38598;&#25104;&#21040;&#26680;&#24515;&#27169;&#22411;&#20013;&#30340;&#29420;&#31435;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#65292;&#22312;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#32531;&#35299;&#20559;&#24046;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#20559;&#35265;&#21453;&#26144;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24494;&#35843;&#29256;&#26412;&#20013;&#12290;&#24120;&#35265;&#30340;&#22788;&#29702;&#20559;&#24046;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#20248;&#21270;&#26631;&#20934;&#65292;&#24182;&#26356;&#26032;&#27169;&#22411;&#20197;&#36798;&#21040;&#26032;&#30340;&#21435;&#20559;&#32622;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#26368;&#32456;&#29992;&#25143;&#21644;&#20174;&#19994;&#20154;&#21592;&#21487;&#33021;&#26356;&#21916;&#27426;&#20999;&#25442;&#22238;&#21407;&#22987;&#27169;&#22411;&#65292;&#25110;&#20165;&#23545;&#29305;&#23450;&#23376;&#38598;&#30340;&#20445;&#25252;&#23646;&#24615;&#24212;&#29992;&#21435;&#20559;&#32622;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#21253;&#25324;&#29420;&#31435;&#39640;&#24230;&#31232;&#30095;&#30340;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#65292;&#20854;&#20013;&#27599;&#20010;&#21435;&#20559;&#32622;&#27169;&#22359;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#38388;&#25353;&#38656;&#38598;&#25104;&#21040;&#26680;&#24515;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20511;&#37492;&#20102;&#8220;diff&#8221;&#21098;&#26525;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#21512;&#20110;&#21508;&#31181;&#34920;&#31034;&#20998;&#31163;&#20248;&#21270;&#30340;&#26032;&#22411;&#35757;&#32451;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#22312;&#32531;&#35299;&#20559;&#24046;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of \emph{diff} pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#31185;&#23398;&#35282;&#24230;&#20986;&#21457;&#65292;&#35780;&#20272;&#20102;14&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;12&#20010;&#19979;&#28216;&#31185;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#25968;&#25454;&#25110;&#35745;&#31639;&#26102;&#38388;&#24182;&#19981;&#24635;&#26159;&#20250;&#23548;&#33268;&#26174;&#30528;&#25552;&#39640;&#65292;&#21487;&#33021;&#20250;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2205.11342</link><description>&lt;p&gt;
&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#20013;&#30340;&#20943;&#24369;&#25910;&#30410;
&lt;/p&gt;
&lt;p&gt;
The Diminishing Returns of Masked Language Models to Science. (arXiv:2205.11342v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#31185;&#23398;&#35282;&#24230;&#20986;&#21457;&#65292;&#35780;&#20272;&#20102;14&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;12&#20010;&#19979;&#28216;&#31185;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#25968;&#25454;&#25110;&#35745;&#31639;&#26102;&#38388;&#24182;&#19981;&#24635;&#26159;&#20250;&#23548;&#33268;&#26174;&#30528;&#25552;&#39640;&#65292;&#21487;&#33021;&#20250;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;BERT&#65292;&#22312;&#19968;&#33324;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#21518;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36824;&#24050;&#32463;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#21487;&#20197;&#36890;&#36807;&#22312;&#26356;&#22810;&#25968;&#25454;&#19978;&#26356;&#38271;&#26102;&#38388;&#22320;&#39044;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#26469;&#25552;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35780;&#20272;&#20102;&#36825;&#20123;&#32467;&#26524;&#22312;&#31185;&#23398;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;14&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65288;&#21253;&#25324;ScholarBERT&#65292;&#19968;&#20010;&#26032;&#30340; 7.7 &#20159;&#21442;&#25968;&#30340;&#31185;&#23398;&#32858;&#28966;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#39044;&#20808;&#35757;&#32451;&#20102; &#39640;&#36798; 225B &#20010;&#20196;&#29260;&#65289;&#65292;&#20197;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26102;&#38388;&#23545;12&#20010;&#19979;&#28216;&#31185;&#23398;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#25968;&#25454;&#25110;&#35745;&#31639;&#26102;&#38388;&#24182;&#19981;&#24635;&#26159;&#20250;&#23548;&#33268;&#26174;&#30528;&#30340;&#25552;&#39640;&#65288;&#21363; &gt;1% F1&#65289;&#65292;&#22914;&#26524;&#26377;&#30340;&#35805;&#65292;&#21487;&#33021;&#20250;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#33021;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based masked language models such as BERT, trained on general corpora, have shown impressive performance on downstream tasks. It has also been demonstrated that the downstream task performance of such models can be improved by pretraining larger models for longer on more data. In this work, we empirically evaluate the extent to which these results extend to tasks in science. We use 14 domain-specific transformer-based models (including ScholarBERT, a new 770M-parameter science-focused masked language model pretrained on up to 225B tokens) to evaluate the impact of training data, model size, pretraining and finetuning time on 12 downstream scientific tasks. Interestingly, we find that increasing model sizes, training data, or compute time does not always lead to significant improvements (i.e., &gt;1% F1), if at all, in scientific information extraction tasks and offered possible explanations for the surprising performance differences.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26469;&#33258;BERT&#27169;&#22411;&#30340;&#26368;&#26032;&#39044;&#35757;&#32451;&#19978;&#19979;&#25991;&#34920;&#31034;&#26469;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#35782;&#21035;&#32597;&#35265;&#30149;&#65292;&#26080;&#38656;&#39046;&#22495;&#19987;&#23478;&#30340;&#25968;&#25454;&#27880;&#37322;&#65292;&#23454;&#29616;&#20102;&#20174;&#25991;&#26412;&#21040;&#32597;&#35265;&#30149;&#26412;&#20307;&#30340;&#33258;&#21160;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2205.05656</link><description>&lt;p&gt;
&#22522;&#20110;&#26412;&#20307;&#21644;&#24369;&#30417;&#30563;&#30340;&#20020;&#24202;&#35760;&#24405;&#20013;&#32597;&#35265;&#30149;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Ontology-Driven and Weakly Supervised Rare Disease Identification from Clinical Notes. (arXiv:2205.05656v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.05656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26469;&#33258;BERT&#27169;&#22411;&#30340;&#26368;&#26032;&#39044;&#35757;&#32451;&#19978;&#19979;&#25991;&#34920;&#31034;&#26469;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#35782;&#21035;&#32597;&#35265;&#30149;&#65292;&#26080;&#38656;&#39046;&#22495;&#19987;&#23478;&#30340;&#25968;&#25454;&#27880;&#37322;&#65292;&#23454;&#29616;&#20102;&#20174;&#25991;&#26412;&#21040;&#32597;&#35265;&#30149;&#26412;&#20307;&#30340;&#33258;&#21160;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#25991;&#26412;&#34920;&#22411;&#26159;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#35782;&#21035;&#24739;&#26377;&#26576;&#20123;&#30142;&#30149;&#21644;&#29305;&#24449;&#30340;&#23454;&#36341;&#12290;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#21487;&#29992;&#30340;&#26696;&#20363;&#24456;&#23569;&#65292;&#19988;&#38656;&#35201;&#39046;&#22495;&#19987;&#23478;&#30340;&#25968;&#25454;&#27880;&#37322;&#65292;&#22240;&#27492;&#32597;&#35265;&#30149;&#38590;&#20197;&#34987;&#35782;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26412;&#20307;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26469;&#33258;Bi-directional Transformers&#65288;&#20363;&#22914;BERT&#65289;&#30340;&#26368;&#26032;&#39044;&#35757;&#32451;&#19978;&#19979;&#25991;&#34920;&#31034;&#12290;&#22522;&#20110;&#26412;&#20307;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;i&#65289;&#25991;&#26412;&#21040;UMLS&#65292;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#24037;&#20855;SemEHR&#65292;&#22312;&#19978;&#19979;&#25991;&#20013;&#38142;&#25509;&#25552;&#21450;&#21040;Unified Medical Language System&#65288;UMLS&#65289;&#20013;&#30340;&#27010;&#24565;&#26469;&#25552;&#21462;&#34920;&#22411;&#65292;&#24182;&#20351;&#29992;&#33258;&#23450;&#20041;&#35268;&#21017;&#21644;&#19978;&#19979;&#25991;&#25552;&#21450;&#34920;&#31034;&#36827;&#34892;&#24369;&#30417;&#30563;&#65307;&#65288;ii&#65289;UMLS&#21040;ORDO&#65292;&#23558;UMLS&#27010;&#24565;&#19982;Orphanet&#32597;&#35265;&#30149;&#26412;&#20307;&#65288;ORDO&#65289;&#20013;&#30340;&#32597;&#35265;&#30149;&#21305;&#37197;&#12290;&#25552;&#20986;&#20102;&#24369;&#30417;&#30563;&#26041;&#27861;&#26469;&#23398;&#20064;&#34920;&#22411;&#30830;&#35748;&#27169;&#22411;&#65292;&#20197;&#25913;&#36827;&#25991;&#26412;&#21040;UMLS&#30340;&#38142;&#25509;&#65292;&#32780;&#26080;&#38656;&#27880;&#37322;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational text phenotyping is the practice of identifying patients with certain disorders and traits from clinical notes. Rare diseases are challenging to be identified due to few cases available for machine learning and the need for data annotation from domain experts. We propose a method using ontologies and weak supervision, with recent pre-trained contextual representations from Bi-directional Transformers (e.g. BERT). The ontology-based framework includes two steps: (i) Text-to-UMLS, extracting phenotypes by contextually linking mentions to concepts in Unified Medical Language System (UMLS), with a Named Entity Recognition and Linking (NER+L) tool, SemEHR, and weak supervision with customised rules and contextual mention representation; (ii) UMLS-to-ORDO, matching UMLS concepts to rare diseases in Orphanet Rare Disease Ontology (ORDO). The weakly supervised approach is proposed to learn a phenotype confirmation model to improve Text-to-UMLS linking, without annotated data from
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#38024;&#23545;&#25991;&#26412;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#23545;&#25239;&#20928;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#27880;&#20837;&#22122;&#22768;&#12289;&#25513;&#30422;&#36755;&#20837;&#25991;&#26412;&#24182;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#37325;&#26500;&#25513;&#30422;&#25991;&#26412;&#65292;&#25104;&#21151;&#22320;&#22312;&#19981;&#38656;&#35201;&#20102;&#35299;&#25915;&#20987;&#24418;&#24335;&#30340;&#24773;&#20917;&#19979;&#23545;&#25239;&#21333;&#35789;&#26367;&#25442;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2203.14207</link><description>&lt;p&gt;
&#25991;&#26412;&#23545;&#25239;&#20928;&#21270;&#20316;&#20026;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Text Adversarial Purification as Defense against Adversarial Attacks. (arXiv:2203.14207v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.14207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#38024;&#23545;&#25991;&#26412;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#23545;&#25239;&#20928;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#27880;&#20837;&#22122;&#22768;&#12289;&#25513;&#30422;&#36755;&#20837;&#25991;&#26412;&#24182;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#37325;&#26500;&#25513;&#30422;&#25991;&#26412;&#65292;&#25104;&#21151;&#22320;&#22312;&#19981;&#38656;&#35201;&#20102;&#35299;&#25915;&#20987;&#24418;&#24335;&#30340;&#24773;&#20917;&#19979;&#23545;&#25239;&#21333;&#35789;&#26367;&#25442;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#20928;&#21270;&#26159;&#19968;&#31181;&#25104;&#21151;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20102;&#35299;&#20837;&#20405;&#25915;&#20987;&#24418;&#24335;&#30340;&#24773;&#20917;&#19979;&#23545;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#36890;&#24120;&#65292;&#23545;&#25239;&#24615;&#20928;&#21270;&#26088;&#22312;&#28040;&#38500;&#23545;&#25239;&#25200;&#21160;&#65292;&#20174;&#32780;&#21487;&#20197;&#22522;&#20110;&#24674;&#22797;&#30340;&#24178;&#20928;&#26679;&#26412;&#36827;&#34892;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#23613;&#31649;&#23545;&#25239;&#20928;&#21270;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#21253;&#25324;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23558;&#20928;&#21270;&#20316;&#20026;&#38024;&#23545;&#25991;&#26412;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38450;&#24481;&#31574;&#30053;&#21364;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26032;&#22411;&#23545;&#25239;&#20928;&#21270;&#26041;&#27861;&#12290;&#20511;&#21161;&#35821;&#35328;&#27169;&#22411;&#30340;&#24110;&#21161;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25513;&#30422;&#36755;&#20837;&#25991;&#26412;&#21644;&#22522;&#20110;&#25513;&#30422;&#30340;&#35821;&#35328;&#27169;&#22411;&#37325;&#26500;&#25513;&#30422;&#25991;&#26412;&#26469;&#27880;&#20837;&#22122;&#22768;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#38024;&#23545;&#26368;&#24120;&#29992;&#30340;&#21333;&#35789;&#26367;&#25442;&#23545;&#25239;&#25915;&#20987;&#26500;&#24314;&#20102;&#38024;&#23545;&#25991;&#26412;&#27169;&#22411;&#30340;&#23545;&#25239;&#20928;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial purification is a successful defense mechanism against adversarial attacks without requiring knowledge of the form of the incoming attack. Generally, adversarial purification aims to remove the adversarial perturbations therefore can make correct predictions based on the recovered clean samples. Despite the success of adversarial purification in the computer vision field that incorporates generative models such as energy-based models and diffusion models, using purification as a defense strategy against textual adversarial attacks is rarely explored. In this work, we introduce a novel adversarial purification method that focuses on defending against textual adversarial attacks. With the help of language models, we can inject noise by masking input texts and reconstructing the masked texts based on the masked language models. In this way, we construct an adversarial purification process for textual models against the most widely used word-substitution adversarial attacks. We
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31038;&#20132;&#23186;&#20307;&#20013;&#31038;&#20250;&#35821;&#29992;&#24847;&#20041;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2203.07648</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#20013;&#31038;&#20250;&#35821;&#29992;&#24847;&#20041;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning of Sociopragmatic Meaning in Social Media. (arXiv:2203.07648v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07648
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31038;&#20132;&#23186;&#20307;&#20013;&#31038;&#20250;&#35821;&#29992;&#24847;&#20041;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#31561;&#30740;&#31350;&#36827;&#23637;&#23578;&#26410;&#24191;&#27867;&#32771;&#34385;&#31038;&#20250;&#35821;&#29992;&#24847;&#20041;&#36825;&#19968;&#31867;&#21035;&#65288;&#21363;&#19981;&#21516;&#35821;&#35328;&#31038;&#21306;&#20869;&#30340;&#20132;&#27969;&#24847;&#20041;&#65289;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21487;&#36801;&#31227;&#33267;&#21508;&#31181;&#31038;&#20250;&#35821;&#29992;&#20219;&#21153;&#65288;&#22914;&#24773;&#24863;&#12289;&#20167;&#24680;&#35328;&#35770;&#12289;&#24189;&#40664;&#12289;&#35773;&#21050;&#65289;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#25968;&#25454;&#20197;&#21450;&#19968;&#33324;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#20363;&#22914;&#65292;&#19982;&#20004;&#20010;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#20165;&#29992;20&#20010;&#35757;&#32451;&#26679;&#26412;&#24494;&#35843;&#26102;&#65292;&#24179;&#22343;F1&#20540;&#22312;16&#20010;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;11.66&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in representation and contrastive learning in NLP has not widely considered the class of \textit{sociopragmatic meaning} (i.e., meaning in interaction within different language communities). To bridge this gap, we propose a novel framework for learning task-agnostic representations transferable to a wide range of sociopragmatic tasks (e.g., emotion, hate speech, humor, sarcasm). Our framework outperforms other contrastive learning frameworks for both in-domain and out-of-domain data, across both the general and few-shot settings. For example, compared to two popular pre-trained language models, our method obtains an improvement of $11.66$ average $F_1$ on $16$ datasets when fine-tuned on only $20$ training samples per dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt&#21644;&#35821;&#20041;&#30693;&#35782;&#22686;&#24378;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#35782;&#21035;&#26410;&#35265;&#20851;&#31995;&#12290;&#37319;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#21333;&#35789;&#32423;&#21035;&#30340;&#31867;&#27604;&#25512;&#29702;&#26041;&#27861;&#65292;&#29983;&#25104;&#20102;&#20855;&#26377;&#26410;&#35265;&#20851;&#31995;&#30340;&#22686;&#24378;&#23454;&#20363;&#12290;&#35774;&#35745;&#20102;&#22522;&#20110;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#30340;&#25552;&#31034;&#65292;&#22686;&#21152;&#20102;&#24050;&#35265;&#20851;&#31995;&#30340;&#35821;&#20041;&#30693;&#35782;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2112.04539</link><description>&lt;p&gt;
&#22522;&#20110;Prompt&#21644;&#35821;&#20041;&#30693;&#35782;&#22686;&#24378;&#30340;&#38646;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Prompt-based Zero-shot Relation Extraction with Semantic Knowledge Augmentation. (arXiv:2112.04539v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.04539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt&#21644;&#35821;&#20041;&#30693;&#35782;&#22686;&#24378;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#35782;&#21035;&#26410;&#35265;&#20851;&#31995;&#12290;&#37319;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#21333;&#35789;&#32423;&#21035;&#30340;&#31867;&#27604;&#25512;&#29702;&#26041;&#27861;&#65292;&#29983;&#25104;&#20102;&#20855;&#26377;&#26410;&#35265;&#20851;&#31995;&#30340;&#22686;&#24378;&#23454;&#20363;&#12290;&#35774;&#35745;&#20102;&#22522;&#20110;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#30340;&#25552;&#31034;&#65292;&#22686;&#21152;&#20102;&#24050;&#35265;&#20851;&#31995;&#30340;&#35821;&#20041;&#30693;&#35782;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;(RTE)&#20219;&#21153;&#20013;&#65292;&#35782;&#21035;&#27809;&#26377;&#35757;&#32451;&#23454;&#20363;&#30340;&#26410;&#35265;&#20851;&#31995;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#24050;&#32463;&#23581;&#35797;&#20351;&#29992;&#38382;&#31572;&#27169;&#22411;&#25110;&#20851;&#31995;&#25551;&#36848;&#26469;&#35782;&#21035;&#26410;&#35265;&#20851;&#31995;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#26377;&#20851;&#24050;&#35265;&#21644;&#26410;&#35265;&#20851;&#31995;&#20043;&#38388;&#32852;&#31995;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt&#21644;&#35821;&#20041;&#30693;&#35782;&#22686;&#24378;&#30340;&#27169;&#22411;&#65288;ZS-SKA&#65289;&#65292;&#29992;&#20110;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#35782;&#21035;&#26410;&#35265;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31867;&#27604;&#25512;&#29702;&#30340;&#21333;&#35789;&#32423;&#21477;&#23376;&#32763;&#35793;&#35268;&#21017;&#65292;&#24182;&#20351;&#29992;&#35813;&#35268;&#21017;&#20174;&#20855;&#26377;&#24050;&#35265;&#20851;&#31995;&#30340;&#23454;&#20363;&#20013;&#29983;&#25104;&#20855;&#26377;&#26410;&#35265;&#20851;&#31995;&#30340;&#22686;&#24378;&#23454;&#20363;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#30340;&#21152;&#26435;&#34394;&#25311;&#26631;&#31614;&#26500;&#24314;&#30340;&#25552;&#31034;&#20449;&#24687;&#65292;&#20197;&#25972;&#21512;&#20174;&#24050;&#35265;&#20851;&#31995;&#20013;&#23398;&#21040;&#30340;&#35821;&#20041;&#30693;&#35782;&#20449;&#24687;&#12290;&#25105;&#20204;&#26500;&#36896;&#20102;&#21152;&#26435;&#34394;&#25311;&#26631;&#31614;&#35789;&#65292;&#32780;&#19981;&#26159;&#22312;&#25552;&#31034;&#27169;&#26495;&#20013;&#20351;&#29992;&#23454;&#38469;&#30340;&#26631;&#31614;&#38598;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;RTE&#20219;&#21153;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;
&lt;/p&gt;
&lt;p&gt;
In relation triplet extraction (RTE), recognizing unseen (new) relations for which there are no training instances is a challenging task. Efforts have been made to recognize unseen relations based on question-answering models or relation descriptions. However, these approaches miss the semantic information about connections between seen and unseen relations. In this paper, We propose a prompt-based model with semantic knowledge augmentation (ZS-SKA) to recognize unseen relations under the zero-shot setting. We present a new word-level analogy-based sentence translation rule and generate augmented instances with unseen relations from instances with seen relations using that new rule. We design prompts with weighted virtual label construction based on an external knowledge graph to integrate semantic knowledge information learned from seen relations. Instead of using the actual label sets in the prompt template, we construct weighted virtual label words. We learn the representations of b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#35757;&#32451;&#23436;&#25104;&#30340;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#23618;&#38754;&#21644;&#31070;&#32463;&#20803;&#27700;&#24179;&#30340;&#20998;&#26512;&#65292;&#25506;&#32034;&#20102;&#20854;&#20013;&#20851;&#20110;&#35828;&#35805;&#20154;&#12289;&#35821;&#35328;&#21644;&#20449;&#36947;&#23646;&#24615;&#30340;&#20449;&#24687;&#25429;&#33719;&#24773;&#20917;&#12290;&#20854;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#35299;&#37322;&#27169;&#22411;&#23398;&#20064;&#30340;&#20851;&#38190;&#29305;&#24449;&#21450;&#20854;&#22312;&#23454;&#29616;&#20844;&#27491;&#24615;&#20915;&#31574;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2107.00439</link><description>&lt;p&gt;
&#31471;&#21040;&#31471;&#35821;&#38899;&#27169;&#22411;&#23398;&#20064;&#20102;&#21738;&#20123;&#20851;&#20110;&#35828;&#35805;&#20154;&#12289;&#35821;&#35328;&#21644;&#20449;&#36947;&#20449;&#24687;&#65311;&#19968;&#39033;&#23618;&#38754;&#21644;&#31070;&#32463;&#20803;&#27700;&#24179;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
What do End-to-End Speech Models Learn about Speaker, Language and Channel Information? A Layer-wise and Neuron-level Analysis. (arXiv:2107.00439v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.00439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#35757;&#32451;&#23436;&#25104;&#30340;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#23618;&#38754;&#21644;&#31070;&#32463;&#20803;&#27700;&#24179;&#30340;&#20998;&#26512;&#65292;&#25506;&#32034;&#20102;&#20854;&#20013;&#20851;&#20110;&#35828;&#35805;&#20154;&#12289;&#35821;&#35328;&#21644;&#20449;&#36947;&#23646;&#24615;&#30340;&#20449;&#24687;&#25429;&#33719;&#24773;&#20917;&#12290;&#20854;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#35299;&#37322;&#27169;&#22411;&#23398;&#20064;&#30340;&#20851;&#38190;&#29305;&#24449;&#21450;&#20854;&#22312;&#23454;&#29616;&#20844;&#27491;&#24615;&#20915;&#31574;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22825;&#29983;&#38590;&#20197;&#35299;&#37322;&#21644;&#29702;&#35299;&#12290;&#19982;&#25163;&#24037;&#29305;&#24449;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#38590;&#20197;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#20102;&#21738;&#20123;&#27010;&#24565;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#29702;&#35299;&#19981;&#20165;&#23545;&#20110;&#35843;&#35797;&#30446;&#30340;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#19988;&#23545;&#20110;&#30830;&#20445;&#36947;&#24503;&#20915;&#31574;&#20013;&#30340;&#20844;&#27491;&#24615;&#20063;&#24456;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25506;&#27979;&#26694;&#26550;[1]&#23545;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#20107;&#21518;&#21151;&#33021;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#38024;&#23545;&#19981;&#21516;&#20219;&#21153;&#65288;&#22914;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#26041;&#35328;&#35782;&#21035;&#65289;&#36827;&#34892;&#35757;&#32451;&#30340;&#35821;&#38899;&#27169;&#22411;&#30340;&#35805;&#35821;&#27700;&#24179;&#34920;&#31034;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23618;&#38754;&#21644;&#31070;&#32463;&#20803;&#27700;&#24179;&#30340;&#20998;&#26512;&#65292;&#25506;&#32034;&#35828;&#35805;&#20154;&#12289;&#35821;&#35328;&#21644;&#20449;&#36947;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;i&#65289;&#34920;&#31034;&#20013;&#25429;&#33719;&#20102;&#21738;&#20123;&#20449;&#24687;&#65311;ii&#65289;&#23427;&#26159;&#22914;&#20309;&#34920;&#31034;&#21644;&#20998;&#24067;&#30340;&#65311;&#20197;&#21450;iii&#65289;&#25105;&#20204;&#33021;&#21542;&#30830;&#23450;&#25317;&#26377;&#27492;&#20449;&#24687;&#30340;&#32593;&#32476;&#30340;&#26368;&#23567;&#23376;&#38598;&#65311;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#20123;&#26032;&#30340;&#21457;&#29616;&#65292;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are inherently opaque and challenging to interpret. Unlike hand-crafted feature-based models, we struggle to comprehend the concepts learned and how they interact within these models. This understanding is crucial not only for debugging purposes but also for ensuring fairness in ethical decision-making. In our study, we conduct a post-hoc functional interpretability analysis of pretrained speech models using the probing framework [1]. Specifically, we analyze utterance-level representations of speech models trained for various tasks such as speaker recognition and dialect identification. We conduct layer and neuron-wise analyses, probing for speaker, language, and channel properties. Our study aims to answer the following questions: i) what information is captured within the representations? ii) how is it represented and distributed? and iii) can we identify a minimal subset of the network that possesses this information?  Our results reveal several novel findings,
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21709;&#24212;&#36873;&#25321;&#33539;&#20363;Uni-Encoder&#65292;&#35299;&#20915;&#20102;Cross-Encoder&#22810;&#27425;&#32534;&#30721;&#30456;&#21516;&#19978;&#19979;&#25991;&#35745;&#31639;&#25104;&#26412;&#39640;&#21644;Poly-Encoder&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#35813;&#33539;&#20363;&#22312;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#23545;&#25152;&#26377;&#20505;&#36873;&#19982;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2106.01263</link><description>&lt;p&gt;
Uni-Encoder: &#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#24555;&#36895;&#20934;&#30830;&#21709;&#24212;&#36873;&#25321;&#33539;&#20363;
&lt;/p&gt;
&lt;p&gt;
Uni-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems. (arXiv:2106.01263v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.01263
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21709;&#24212;&#36873;&#25321;&#33539;&#20363;Uni-Encoder&#65292;&#35299;&#20915;&#20102;Cross-Encoder&#22810;&#27425;&#32534;&#30721;&#30456;&#21516;&#19978;&#19979;&#25991;&#35745;&#31639;&#25104;&#26412;&#39640;&#21644;Poly-Encoder&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#35813;&#33539;&#20363;&#22312;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#23545;&#25152;&#26377;&#20505;&#36873;&#19982;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26679;&#26412;&#19982;&#25490;&#24207;&#26159;&#29616;&#20195;&#29983;&#25104;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#20851;&#38190;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#20174;&#29983;&#25104;&#30340;&#23569;&#37327;&#20505;&#36873;&#31572;&#26696;&#20013;&#36873;&#25321;&#19968;&#20010;&#31572;&#26696;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25490;&#24207;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#31216;&#20026;&#20132;&#21449;&#32534;&#30721;&#22120;&#30340;&#32534;&#30721;&#33539;&#20363;&#65292;&#35813;&#32534;&#30721;&#22120;&#20998;&#21035;&#23545;&#27599;&#20010;&#19978;&#19979;&#25991;-&#20505;&#36873;&#23545;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#26681;&#25454;&#20854;&#36866;&#24212;&#24230;&#24471;&#20998;&#23545;&#20505;&#36873;&#36827;&#34892;&#25490;&#24207;&#12290;&#28982;&#32780;&#65292;&#20132;&#21449;&#32534;&#30721;&#22120;&#20026;&#27599;&#20010;&#20505;&#36873;&#37325;&#22797;&#32534;&#30721;&#30456;&#21516;&#30340;&#20887;&#38271;&#19978;&#19979;&#25991;&#65292;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;Poly-Encoder&#36890;&#36807;&#20943;&#23569;&#19978;&#19979;&#25991;&#21644;&#20505;&#36873;&#20043;&#38388;&#30340;&#20132;&#20114;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#20294;&#20195;&#20215;&#26159;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#31216;&#20026;Uni-Encoder&#65292;&#23427;&#20687;&#20132;&#21449;&#32534;&#30721;&#22120;&#19968;&#26679;&#23436;&#20840;&#20851;&#27880;&#27599;&#20010;&#20505;&#36873;&#23545;&#65292;&#21516;&#26102;&#20687;Poly-&#32534;&#30721;&#22120;&#19968;&#26679;&#21482;&#32534;&#30721;&#19968;&#27425;&#19978;&#19979;&#25991;&#12290;Uni-Encoder&#22312;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#23545;&#25152;&#26377;&#20505;&#36873;&#19982;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#38024;&#23545;&#25152;&#26377;&#20505;&#36873;&#20351;&#29992;&#30456;&#21516;&#30340;&#20301;&#32622;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sample-and-rank is a key decoding strategy for modern generation-based dialogue systems. It helps achieve diverse and high-quality responses by selecting an answer from a small pool of generated candidates. The current state-of-the-art ranking methods mainly use an encoding paradigm called Cross-Encoder, which separately encodes each context-candidate pair and ranks the candidates according to their fitness scores. However, Cross-Encoder repeatedly encodes the same lengthy context for each candidate, resulting in high computational costs. Poly-Encoder addresses the above problems by reducing the interaction between context and candidates, but with a price of performance drop. In this work, we develop a new paradigm called Uni-Encoder, that keeps the full attention over each pair as in Cross-Encoder while only encoding the context once, as in Poly-Encoder. Uni-Encoder encodes all the candidates with the context in one forward pass. We use the same positional embedding for all candidates
&lt;/p&gt;</description></item></channel></rss>