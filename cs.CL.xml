<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;BSDetector&#65292;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38169;&#35823;&#21644;&#25512;&#27979;&#24615;&#22238;&#31572;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20272;&#35745;&#32622;&#20449;&#24230;&#37327;&#21270;&#20102;&#22238;&#31572;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#38381;&#21512;&#22411;&#21644;&#24320;&#25918;&#22411;&#38382;&#31572;&#22522;&#20934;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#20934;&#30830;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16175</link><description>&lt;p&gt;
&#36890;&#36807;&#20869;&#22312;&#21644;&#22806;&#22312;&#32622;&#20449;&#24230;&#35780;&#20272;&#26469;&#37327;&#21270;&#20219;&#24847;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying Uncertainty in Answers from any Language Model via Intrinsic and Extrinsic Confidence Assessment. (arXiv:2308.16175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;BSDetector&#65292;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38169;&#35823;&#21644;&#25512;&#27979;&#24615;&#22238;&#31572;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20272;&#35745;&#32622;&#20449;&#24230;&#37327;&#21270;&#20102;&#22238;&#31572;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#38381;&#21512;&#22411;&#21644;&#24320;&#25918;&#22411;&#38382;&#31572;&#22522;&#20934;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#20934;&#30830;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;BSDetector&#65292;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20219;&#20309;&#36755;&#20986;&#30340;&#25968;&#20540;&#32622;&#20449;&#24230;&#26469;&#26816;&#27979;&#38169;&#35823;&#21644;&#25512;&#27979;&#24615;&#22238;&#31572;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#36866;&#29992;&#20110;&#20165;&#36890;&#36807;&#40657;&#30418;API&#35775;&#38382;&#30340;&#20219;&#20309;LLM&#65292;&#24182;&#23558;&#20869;&#22312;&#21644;&#22806;&#22312;&#35780;&#20272;&#30340;&#32622;&#20449;&#24230;&#32467;&#21512;&#20026;&#23545;&#32473;&#23450;&#25552;&#31034;&#19979;LLM&#21709;&#24212;&#30340;&#21333;&#20010;&#21487;&#20449;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#36890;&#29992;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#24403;&#20170;&#25152;&#26377;&#26368;&#22909;&#30340;LLM&#65288;&#20854;&#35757;&#32451;&#25968;&#25454;&#26410;&#30693;&#65289;&#12290;&#36890;&#36807;&#39069;&#22806;&#30340;&#35745;&#31639;&#65292;&#20219;&#20309;LLM API&#30340;&#29992;&#25143;&#29616;&#22312;&#21487;&#20197;&#33719;&#24471;&#19982;&#36890;&#24120;&#30456;&#21516;&#30340;&#21709;&#24212;&#65292;&#20197;&#21450;&#19968;&#20010;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#20197;&#20415;&#22312;&#19981;&#20449;&#20219;&#35813;&#21709;&#24212;&#26102;&#20445;&#25345;&#35880;&#24910;&#12290;&#23545;&#20110;&#38381;&#21512;&#22411;&#21644;&#24320;&#25918;&#22411;&#38382;&#31572;&#22522;&#20934;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;BSDetector&#27604;&#20854;&#20182;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65288;&#23545;&#20110;GPT-3&#21644;ChatGPT&#65289;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#38169;&#35823;&#30340;LLM&#21709;&#24212;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#21709;&#24212;&#36827;&#34892;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
We introduce BSDetector, a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated. Our uncertainty quantification technique works for any LLM accessible only via a black-box API, and combines intrinsic and extrinsic assessments of confidence into a single trustworthiness estimate for any LLM response to a given prompt. Our method is extremely general and can applied to all of the best LLMs available today (whose training data remains unknown). By expending a bit of extra computation, users of any LLM API can now get the same response as they would ordinarily, as well as a confidence estimate that caution when not to trust this response. Experiments on both closed and open-form Question-Answer benchmarks reveal that BSDetector more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple responses
&lt;/p&gt;</description></item><item><title>Jais&#21644;Jais-chat&#26159;&#26032;&#30340;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;13&#20159;&#21442;&#25968;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#33521;&#35821;&#26041;&#38754;&#20063;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#24067;&#26088;&#22312;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2308.16149</link><description>&lt;p&gt;
Jais&#21644;Jais-chat&#65306;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#22522;&#30784;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models. (arXiv:2308.16149v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16149
&lt;/p&gt;
&lt;p&gt;
Jais&#21644;Jais-chat&#26159;&#26032;&#30340;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;13&#20159;&#21442;&#25968;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#33521;&#35821;&#26041;&#38754;&#20063;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#24067;&#26088;&#22312;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Jais&#21644;Jais-chat&#65292;&#36825;&#26159;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#22522;&#30784;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;GPT-3&#30340;&#20165;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#22312;&#38463;&#25289;&#20271;&#35821;&#21644;&#33521;&#35821;&#25991;&#26412;&#30340;&#28151;&#21512;&#29289;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#21508;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#28304;&#20195;&#30721;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;130&#20159;&#20010;&#21442;&#25968;&#65292;&#26681;&#25454;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#24320;&#25918;&#24335;&#38463;&#25289;&#20271;&#35821;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#30340;&#33521;&#35821;&#25968;&#25454;&#35201;&#23569;&#24471;&#22810;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#33521;&#35821;&#26041;&#38754;&#19982;&#31867;&#20284;&#35268;&#27169;&#30340;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#24320;&#25918;&#27169;&#22411;&#30456;&#27604;&#20173;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#12289;&#35843;&#20248;&#12289;&#23433;&#20840;&#23545;&#40784;&#21644;&#35780;&#20272;&#30340;&#35814;&#32454;&#25551;&#36848;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#27169;&#22411;&#30340;&#20004;&#20010;&#24320;&#25918;&#29256;&#26412;--&#22522;&#30784;Jais&#27169;&#22411;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;Jais-chat&#21464;&#31181;--&#26088;&#22312;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#30740;&#31350;&#12290;&#35814;&#35265;https://hugging
&lt;/p&gt;
&lt;p&gt;
We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://hugging
&lt;/p&gt;</description></item><item><title>LM-Infinite&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#26102;&#25512;&#24191;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16137</link><description>&lt;p&gt;
LM-Infinite: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21363;&#26102;&#38271;&#24230;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16137
&lt;/p&gt;
&lt;p&gt;
LM-Infinite&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#26102;&#25512;&#24191;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;Transformer-based&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#38543;&#30528;&#36825;&#20123;LLM&#22312;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#30528;&#23545;&#38271;&#26102;&#38388;&#25512;&#29702;&#36807;&#31243;&#25110;&#29702;&#35299;&#26356;&#22823;&#19978;&#19979;&#25991;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;LLM&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#22823;&#22810;&#25968;&#39044;&#35757;&#32451;&#26041;&#26696;&#23558;&#35757;&#32451;&#24207;&#21015;&#25130;&#26029;&#21040;&#22266;&#23450;&#38271;&#24230;&#65288;&#20363;&#22914;LLaMa&#30340;2048&#65289;&#12290;&#21363;&#20351;&#20351;&#29992;&#20102;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26469;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;LLM&#22312;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20043;&#21518;&#24448;&#24448;&#38590;&#20197;&#29983;&#25104;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#26356;&#19981;&#29992;&#35828;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#20102;&#12290;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#22312;&#26356;&#38271;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#24448;&#24448;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#30340;&#30828;&#20214;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#38656;&#35201;&#36827;&#34892;&#20180;&#32454;&#30340;&#35757;&#32451;&#36807;&#31243;&#35774;&#35745;&#12290;&#20026;&#20102;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;LLM&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30740;&#31350;&#20102;&#20027;&#35201;&#30340;&#20998;&#24067;&#22806;(OOD) f
&lt;/p&gt;
&lt;p&gt;
In recent years, there have been remarkable advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for increasingly complex tasks, they often face the needs to conduct longer reasoning processes or understanding larger contexts. In these situations, the length generalization failure of LLMs on long sequences become more prominent. Most pre-training schemes truncate training sequences to a fixed length (such as 2048 for LLaMa). LLMs often struggle to generate fluent texts, let alone carry out downstream tasks, after longer contexts, even with relative positional encoding which is designed to cope with this problem. Common solutions such as finetuning on longer corpora often involves daunting hardware and time costs and requires careful training process design. To more efficiently leverage the generation capacity of existing LLMs, we theoretically and empirically investigate the main out-of-distribution (OOD) f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22238;&#24212;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32039;&#24613;&#31867;&#27604;&#25512;&#29702;&#30340;&#20027;&#24352;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#23383;&#31526;&#20018;&#31867;&#27604;&#30340;&#21453;&#20363;&#26469;&#21453;&#39539;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;GPT-3&#26080;&#27861;&#35299;&#20915;&#26368;&#31616;&#21333;&#30340;&#31867;&#27604;&#38382;&#39064;&#12290;&#20026;&#20102;&#21152;&#24378;&#38646;&#28857;&#25512;&#29702;&#31561;&#20154;&#31867;&#25512;&#29702;&#30340;&#20027;&#24352;&#65292;&#38656;&#35201;&#21457;&#23637;&#20986;&#25490;&#38500;&#25968;&#25454;&#35760;&#24518;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.16118</link><description>&lt;p&gt;
&#22238;&#24212;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32039;&#24613;&#31867;&#27604;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Response: Emergent analogical reasoning in large language models. (arXiv:2308.16118v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16118
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22238;&#24212;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32039;&#24613;&#31867;&#27604;&#25512;&#29702;&#30340;&#20027;&#24352;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#23383;&#31526;&#20018;&#31867;&#27604;&#30340;&#21453;&#20363;&#26469;&#21453;&#39539;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;GPT-3&#26080;&#27861;&#35299;&#20915;&#26368;&#31616;&#21333;&#30340;&#31867;&#27604;&#38382;&#39064;&#12290;&#20026;&#20102;&#21152;&#24378;&#38646;&#28857;&#25512;&#29702;&#31561;&#20154;&#31867;&#25512;&#29702;&#30340;&#20027;&#24352;&#65292;&#38656;&#35201;&#21457;&#23637;&#20986;&#25490;&#38500;&#25968;&#25454;&#35760;&#24518;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#12298;&#33258;&#28982;&#20154;&#31867;&#34892;&#20026;&#12299;&#35770;&#25991;&#20013;&#65292;&#8220;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32039;&#24613;&#31867;&#27604;&#25512;&#29702;&#8221;&#65288;Webb&#65292;Holyoak&#21644;Lu&#65292;2023&#65289;&#65292;&#20316;&#32773;&#20204;&#35748;&#20026;&#8220;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#33719;&#24471;&#20102;&#21457;&#29616;&#24191;&#27867;&#31867;&#27604;&#38382;&#39064;&#30340;&#38646;&#28857;&#35299;&#30340;&#32039;&#24613;&#33021;&#21147;&#8221;&#12290;&#22312;&#26412;&#22238;&#24212;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#23383;&#31526;&#20018;&#31867;&#27604;&#30340;&#21453;&#20363;&#12290;&#22312;&#25105;&#20204;&#30340;&#27979;&#35797;&#20013;&#65292;GPT-3&#29978;&#33267;&#26080;&#27861;&#35299;&#20915;&#21407;&#22987;&#35770;&#25991;&#20013;&#25552;&#20986;&#30340;&#26368;&#31616;&#21333;&#30340;&#21464;&#20307;&#38382;&#39064;&#12290;&#38646;&#28857;&#25512;&#29702;&#26159;&#19968;&#20010;&#38656;&#35201;&#38750;&#24120;&#20805;&#20998;&#35777;&#25454;&#25903;&#25345;&#30340;&#38750;&#20961;&#20027;&#24352;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#27809;&#26377;&#30475;&#21040;&#36825;&#26679;&#30340;&#35777;&#25454;&#12290;&#20026;&#20102;&#21152;&#24378;&#20687;&#38646;&#28857;&#25512;&#29702;&#36825;&#26679;&#31867;&#20284;&#20154;&#31867;&#25512;&#29702;&#30340;&#20027;&#24352;&#65292;&#37325;&#35201;&#30340;&#26159;&#35813;&#39046;&#22495;&#24320;&#21457;&#20986;&#33021;&#22815;&#25490;&#38500;&#25968;&#25454;&#35760;&#24518;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In their recent Nature Human Behaviour paper, "Emergent analogical reasoning in large language models," (Webb, Holyoak, and Lu, 2023) the authors argue that "large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems." In this response, we provide counterexamples of the letter string analogies. In our tests, GPT-3 fails to solve even the easiest variants of the problems presented in the original paper. Zero-shot reasoning is an extraordinary claim that requires extraordinary evidence. We do not see that evidence in our experiments. To strengthen claims of humanlike reasoning such as zero-shot reasoning, it is important that the field develop approaches that rule out data memorization.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25968;&#25454;&#30340;&#20849;&#20139;&#38754;&#20020;&#20010;&#20154;&#21644;&#25935;&#24863;&#20449;&#24687;&#20445;&#25252;&#30340;&#38382;&#39064;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#20551;&#21517;&#21270;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#21253;&#25324;&#23545;&#20551;&#21517;&#21270;&#23545;&#26080;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#20551;&#21517;&#21270;&#20316;&#20026;&#20445;&#25252;&#20316;&#32773;&#36523;&#20221;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#24320;&#21457;&#19978;&#19979;&#25991;&#25935;&#24863;&#31639;&#27861;&#29992;&#20110;&#22788;&#29702;&#20010;&#20154;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.16109</link><description>&lt;p&gt;
Grandma Karl&#20170;&#24180;27&#23681;&#8212;&#8212;&#30740;&#31350;&#21311;&#21517;&#21270;&#30740;&#31350;&#25968;&#25454;&#30340;&#35758;&#31243;
&lt;/p&gt;
&lt;p&gt;
Grandma Karl is 27 years old -- research agenda for pseudonymization of research data. (arXiv:2308.16109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16109
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25968;&#25454;&#30340;&#20849;&#20139;&#38754;&#20020;&#20010;&#20154;&#21644;&#25935;&#24863;&#20449;&#24687;&#20445;&#25252;&#30340;&#38382;&#39064;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#20551;&#21517;&#21270;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#21253;&#25324;&#23545;&#20551;&#21517;&#21270;&#23545;&#26080;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#20551;&#21517;&#21270;&#20316;&#20026;&#20445;&#25252;&#20316;&#32773;&#36523;&#20221;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#24320;&#21457;&#19978;&#19979;&#25991;&#25935;&#24863;&#31639;&#27861;&#29992;&#20110;&#22788;&#29702;&#20010;&#20154;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25968;&#25454;&#30340;&#21487;&#35775;&#38382;&#24615;&#23545;&#20110;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#30340;&#36827;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#20854;&#20013;&#21253;&#21547;&#20010;&#20154;&#21644;&#25935;&#24863;&#20449;&#24687;&#65288;&#22914;&#22995;&#21517;&#25110;&#25919;&#27835;&#35266;&#28857;&#65289;&#65292;&#25991;&#26412;&#25968;&#25454;&#36890;&#24120;&#19981;&#33021;&#34987;&#20849;&#20139;&#12290;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#65288;GDPR&#65289;&#24314;&#35758;&#20351;&#29992;&#20551;&#21517;&#21270;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#30830;&#20445;&#23545;&#30740;&#31350;&#25968;&#25454;&#30340;&#24320;&#25918;&#35775;&#38382;&#30340;&#23433;&#20840;&#24615;&#65292;&#20294;&#22312;&#37319;&#29992;&#20551;&#21517;&#21270;&#26041;&#27861;&#22788;&#29702;&#30740;&#31350;&#25968;&#25454;&#20043;&#21069;&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#22810;&#22320;&#20102;&#35299;&#20551;&#21517;&#21270;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#19968;&#20010;&#20851;&#20110;&#20551;&#21517;&#21270;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#21253;&#25324;&#23545;&#26080;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#20551;&#21517;&#21270;&#23545;&#21487;&#35835;&#24615;&#21644;&#35821;&#35328;&#35780;&#20272;&#31561;&#24433;&#21709;&#30340;&#30740;&#31350;&#38656;&#27714;&#65292;&#20197;&#21450;&#20551;&#21517;&#21270;&#20316;&#20026;&#20445;&#25252;&#20316;&#32773;&#36523;&#20221;&#30340;&#26041;&#24335;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#24320;&#21457;&#19978;&#19979;&#25991;&#25935;&#24863;&#31639;&#27861;&#29992;&#20110;&#26816;&#27979;&#12289;&#26631;&#35760;&#21644;&#26367;&#25442;&#26080;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#20010;&#20154;&#20449;&#24687;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#26368;&#36817;&#33719;&#24471;&#30340;&#20851;&#20110;&#20551;&#21517;&#21270;&#30340;&#39033;&#30446;&#8220;Grandma Karl&#20170;&#24180;27&#23681;&#8221;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#37096;&#20998;&#35758;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accessibility of research data is critical for advances in many research fields, but textual data often cannot be shared due to the personal and sensitive information which it contains, e.g names or political opinions. General Data Protection Regulation (GDPR) suggests pseudonymization as a solution to secure open access to research data, but we need to learn more about pseudonymization as an approach before adopting it for manipulation of research data. This paper outlines a research agenda within pseudonymization, namely need of studies into the effects of pseudonymization on unstructured data in relation to e.g. readability and language assessment, as well as the effectiveness of pseudonymization as a way of protecting writer identity, while also exploring different ways of developing context-sensitive algorithms for detection, labelling and replacement of personal information in unstructured data. The recently granted project on pseudonymization Grandma Karl is 27 years old address
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#31995;&#32479;&#20013;&#28155;&#21152;&#22270;&#20687;&#29305;&#24449;&#21487;&#33021;&#26159;&#22810;&#20313;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#21512;&#25104;&#22122;&#22768;&#26469;&#35780;&#20272;&#22270;&#20687;&#23545;&#22788;&#29702;&#25991;&#26412;&#22122;&#22768;&#30340;&#24110;&#21161;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#30053;&#20248;&#20110;&#25991;&#26412;&#27169;&#22411;&#65292;&#21363;&#20351;&#26159;&#38543;&#26426;&#22270;&#20687;&#12290;&#30740;&#31350;&#22312;&#33521;&#35821;&#32763;&#35793;&#20026;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#19988;&#35270;&#35273;&#32972;&#26223;&#23545;&#32763;&#35793;&#25928;&#26524;&#30340;&#24433;&#21709;&#19982;&#28304;&#25991;&#26412;&#22122;&#22768;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2308.16075</link><description>&lt;p&gt;
&#35270;&#35273;&#32972;&#26223;&#23545;&#22024;&#26434;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24433;&#21709;&#65306;&#23545;&#33521;&#21360;&#35821;&#35328;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages. (arXiv:2308.16075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16075
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#31995;&#32479;&#20013;&#28155;&#21152;&#22270;&#20687;&#29305;&#24449;&#21487;&#33021;&#26159;&#22810;&#20313;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#21512;&#25104;&#22122;&#22768;&#26469;&#35780;&#20272;&#22270;&#20687;&#23545;&#22788;&#29702;&#25991;&#26412;&#22122;&#22768;&#30340;&#24110;&#21161;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#30053;&#20248;&#20110;&#25991;&#26412;&#27169;&#22411;&#65292;&#21363;&#20351;&#26159;&#38543;&#26426;&#22270;&#20687;&#12290;&#30740;&#31350;&#22312;&#33521;&#35821;&#32763;&#35793;&#20026;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#19988;&#35270;&#35273;&#32972;&#26223;&#23545;&#32763;&#35793;&#25928;&#26524;&#30340;&#24433;&#21709;&#19982;&#28304;&#25991;&#26412;&#22122;&#22768;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#32780;&#26412;&#30740;&#31350;&#21017;&#32771;&#23519;&#20102;&#23558;&#22270;&#20687;&#29305;&#24449;&#28155;&#21152;&#21040;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20013;&#30340;&#32763;&#35793;&#25928;&#26524;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#22270;&#20687;&#21487;&#33021;&#26159;&#22810;&#20313;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#21512;&#25104;&#22122;&#22768;&#26469;&#35780;&#20272;&#22270;&#20687;&#26159;&#21542;&#26377;&#21161;&#20110;&#27169;&#22411;&#22788;&#29702;&#25991;&#26412;&#22122;&#22768;&#12290;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#65292;&#21363;&#20351;&#26159;&#38543;&#26426;&#22270;&#20687;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#30053;&#20248;&#20110;&#25991;&#26412;&#27169;&#22411;&#12290;&#23454;&#39564;&#23558;&#33521;&#35821;&#32763;&#35793;&#20026;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#65292;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#35270;&#35273;&#32972;&#26223;&#30340;&#24433;&#21709;&#19982;&#28304;&#25991;&#26412;&#22122;&#22768;&#26377;&#25152;&#19981;&#21516;&#65306;&#23545;&#20110;&#38750;&#22122;&#22768;&#32763;&#35793;&#65292;&#19981;&#20351;&#29992;&#35270;&#35273;&#32972;&#26223;&#25928;&#26524;&#26368;&#22909;&#65307;&#23545;&#20110;&#20302;&#22122;&#22768;&#65292;&#35009;&#21098;&#30340;&#22270;&#20687;&#29305;&#24449;&#26368;&#20339;&#65307;&#22312;&#39640;&#22122;&#22768;&#24773;&#20917;&#19979;&#65292;&#23436;&#25972;&#30340;&#22270;&#20687;&#29305;&#24449;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study investigates the effectiveness of utilizing multimodal information in Neural Machine Translation (NMT). While prior research focused on using multimodal data in low-resource scenarios, this study examines how image features impact translation when added to a large-scale, pre-trained unimodal NMT system. Surprisingly, the study finds that images might be redundant in this context. Additionally, the research introduces synthetic noise to assess whether images help the model deal with textual noise. Multimodal models slightly outperform text-only models in noisy settings, even with random images. The study's experiments translate from English to Hindi, Bengali, and Malayalam, outperforming state-of-the-art benchmarks significantly. Interestingly, the effect of visual context varies with source text noise: no visual context works best for non-noisy translations, cropped image features are optimal for low noise, and full image features work better in high-noise scenarios. This she
&lt;/p&gt;</description></item><item><title>Conti&#20844;&#21496;&#30340;&#32842;&#22825;&#35760;&#24405;&#27844;&#38706;&#32473;&#25105;&#20204;&#25552;&#20379;&#20102;&#20102;&#35299;&#21202;&#32034;&#36719;&#20214;&#26381;&#21153;&#36816;&#33829;&#21830;&#20869;&#37096;&#36816;&#20316;&#30340;&#26426;&#20250;&#12290;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#21487;&#35270;&#21270;&#31574;&#30053;&#65292;&#30740;&#31350;&#21457;&#29616;&#19994;&#21153;&#12289;&#25216;&#26415;&#12289;&#20869;&#37096;&#20219;&#21153;&#31649;&#29702;&#12289;&#24694;&#24847;&#36719;&#20214;&#21644;&#23458;&#25143;&#26381;&#21153;&#26159;Conti&#25104;&#21592;&#35752;&#35770;&#30340;&#20027;&#35201;&#20027;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.16061</link><description>&lt;p&gt;
Conti&#20844;&#21496;&#65306;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20102;&#35299;&#19968;&#20010;&#22823;&#22411;&#21202;&#32034;&#36719;&#20214;&#26381;&#21153;&#36816;&#33829;&#21830;&#30340;&#20869;&#37096;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
Conti Inc.: Understanding the Internal Discussions of a large Ransomware-as-a-Service Operator with Machine Learning. (arXiv:2308.16061v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16061
&lt;/p&gt;
&lt;p&gt;
Conti&#20844;&#21496;&#30340;&#32842;&#22825;&#35760;&#24405;&#27844;&#38706;&#32473;&#25105;&#20204;&#25552;&#20379;&#20102;&#20102;&#35299;&#21202;&#32034;&#36719;&#20214;&#26381;&#21153;&#36816;&#33829;&#21830;&#20869;&#37096;&#36816;&#20316;&#30340;&#26426;&#20250;&#12290;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#21487;&#35270;&#21270;&#31574;&#30053;&#65292;&#30740;&#31350;&#21457;&#29616;&#19994;&#21153;&#12289;&#25216;&#26415;&#12289;&#20869;&#37096;&#20219;&#21153;&#31649;&#29702;&#12289;&#24694;&#24847;&#36719;&#20214;&#21644;&#23458;&#25143;&#26381;&#21153;&#26159;Conti&#25104;&#21592;&#35752;&#35770;&#30340;&#20027;&#35201;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21202;&#32034;&#36719;&#20214;&#26381;&#21153;&#65288;RaaS&#65289;&#27491;&#22312;&#22686;&#21152;&#21202;&#32034;&#36719;&#20214;&#25915;&#20987;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#12290;&#20102;&#35299;RaaS&#32972;&#21518;&#30340;&#20869;&#37096;&#36816;&#20316;&#19968;&#30452;&#26159;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#27492;&#31867;&#27963;&#21160;&#26159;&#38750;&#27861;&#30340;&#12290;&#26368;&#36817;Conti&#20844;&#21496;&#27844;&#38706;&#30340;&#32842;&#22825;&#35760;&#24405;&#32473;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20102;&#35299;&#36825;&#31867;&#32452;&#32455;&#20869;&#37096;&#36816;&#20316;&#30340;&#33391;&#26426;&#12290;&#26412;&#25991;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#31561;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20197;&#21450;&#21487;&#35270;&#21270;&#31574;&#30053;&#65292;&#20998;&#26512;&#20102;Conti&#20844;&#21496;&#32842;&#22825;&#35760;&#24405;&#20013;&#30340;&#20027;&#35201;&#20027;&#39064;&#35752;&#35770;&#12290;&#21457;&#29616;&#20102;&#20116;&#20010;&#35752;&#35770;&#20027;&#39064;&#65306;1&#65289;&#19994;&#21153;&#65292;2&#65289;&#25216;&#26415;&#65292;3&#65289;&#20869;&#37096;&#20219;&#21153;/&#31649;&#29702;&#65292;4&#65289;&#24694;&#24847;&#36719;&#20214;&#65292;5&#65289;&#23458;&#25143;&#26381;&#21153;/&#38382;&#39064;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;Conti&#25104;&#21592;&#30340;&#20027;&#39064;&#20998;&#24067;&#26174;&#31034;&#65292;&#21482;&#26377;4%&#30340;&#20154;&#36827;&#34892;&#20102;&#19987;&#38376;&#30340;&#35752;&#35770;&#65292;&#32780;&#20960;&#20046;&#25152;&#26377;&#20154;&#65288;96%&#65289;&#37117;&#26159;&#20840;&#33021;&#22411;&#65292;&#24847;&#21619;&#30528;&#20182;&#20204;&#30340;&#35752;&#35770;&#37117;&#22260;&#32469;&#30528;&#36825;&#20116;&#20010;&#20027;&#39064;&#23637;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ransomware-as-a-service (RaaS) is increasing the scale and complexity of ransomware attacks. Understanding the internal operations behind RaaS has been a challenge due to the illegality of such activities. The recent chat leak of the Conti RaaS operator, one of the most infamous ransomware operators on the international scene, offers a key opportunity to better understand the inner workings of such organizations. This paper analyzes the main topic discussions in the Conti chat leak using machine learning techniques such as Natural Language Processing (NLP) and Latent Dirichlet Allocation (LDA), as well as visualization strategies. Five discussion topics are found: 1) Business, 2) Technical, 3) Internal tasking/Management, 4) Malware, and 5) Customer Service/Problem Solving. Moreover, the distribution of topics among Conti members shows that only 4% of individuals have specialized discussions while almost all individuals (96%) are all-rounders, meaning that their discussions revolve aro
&lt;/p&gt;</description></item><item><title>Text-to-OverpassQL&#26159;&#19968;&#20010;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;OpenStreetMap&#20013;&#22320;&#29702;&#25968;&#25454;&#30340;&#30028;&#38754;&#65292;&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#21046;&#23450;&#22797;&#26434;&#30340;&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#24182;&#20026;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#19968;&#20010;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.16060</link><description>&lt;p&gt;
Text-to-OverpassQL&#65306;&#19968;&#20010;&#29992;&#20110;&#26597;&#35810;OpenStreetMap&#22797;&#26434;&#22320;&#29702;&#25968;&#25454;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap. (arXiv:2308.16060v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16060
&lt;/p&gt;
&lt;p&gt;
Text-to-OverpassQL&#26159;&#19968;&#20010;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;OpenStreetMap&#20013;&#22320;&#29702;&#25968;&#25454;&#30340;&#30028;&#38754;&#65292;&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#21046;&#23450;&#22797;&#26434;&#30340;&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#24182;&#20026;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#19968;&#20010;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Text-to-OverpassQL&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;OpenStreetMap (OSM)&#20013;&#22320;&#29702;&#25968;&#25454;&#30340;&#20219;&#21153;&#12290;Overpass&#26597;&#35810;&#35821;&#35328;&#65288;OverpassQL&#65289;&#20801;&#35768;&#29992;&#25143;&#21046;&#23450;&#22797;&#26434;&#30340;&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#24182;&#22312;OSM&#29983;&#24577;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#20174;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#29983;&#25104;Overpass&#26597;&#35810;&#21487;&#20197;&#28385;&#36275;&#22810;&#20010;&#29992;&#20363;&#12290;&#23427;&#20351;&#26032;&#25163;&#29992;&#25143;&#33021;&#22815;&#22312;&#26080;&#38656;&#20808;&#21069;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;OverpassQL&#65292;&#24110;&#21161;&#26377;&#32463;&#39564;&#30340;&#29992;&#25143;&#21046;&#20316;&#39640;&#32423;&#26597;&#35810;&#65292;&#24182;&#20351;&#24037;&#20855;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#35775;&#38382;&#23384;&#20648;&#22312;OSM&#25968;&#25454;&#24211;&#20013;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35780;&#20272;&#24403;&#21069;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OverpassNL&#65292;&#19968;&#20010;&#21253;&#21547;8,352&#20010;&#26597;&#35810;&#21644;&#30456;&#24212;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#23545;OSM&#25968;&#25454;&#24211;&#25191;&#34892;&#26597;&#35810;&#26469;&#23545;Text-to-OverpassQL&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#24314;&#31435;&#20102;&#24378;&#22823;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Text-to-OverpassQL, a task designed to facilitate a natural language interface for querying geodata from OpenStreetMap (OSM). The Overpass Query Language (OverpassQL) allows users to formulate complex database queries and is widely adopted in the OSM ecosystem. Generating Overpass queries from natural language input serves multiple use-cases. It enables novice users to utilize OverpassQL without prior knowledge, assists experienced users with crafting advanced queries, and enables tool-augmented large language models to access information stored in the OSM database. In order to assess the performance of current sequence generation models on this task, we propose OverpassNL, a dataset of 8,352 queries with corresponding natural language inputs. We further introduce task specific evaluation metrics and ground the evaluation of the Text-to-OverpassQL task by executing the queries against the OSM database. We establish strong baselines by finetuning sequence-to-sequence models a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#37319;&#29992;&#24322;&#27493;&#23398;&#20064;&#26041;&#27861;&#24182;&#24341;&#20837;&#22810;&#20010;&#36741;&#21161;&#20851;&#31995;&#30340;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#31867;&#22411;&#25512;&#26029;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#31890;&#24230;&#30340;&#23454;&#20307;&#31867;&#22411;&#27169;&#24335;&#24314;&#27169;&#26041;&#38754;&#20855;&#26377;&#26356;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16055</link><description>&lt;p&gt;
&#24322;&#27493;&#23398;&#20064;&#30340;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#31867;&#22411;&#25512;&#26029;&#26041;&#27861;&#19982;&#36741;&#21161;&#20851;&#31995;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AsyncET: Asynchronous Learning for Knowledge Graph Entity Typing with Auxiliary Relations. (arXiv:2308.16055v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#37319;&#29992;&#24322;&#27493;&#23398;&#20064;&#26041;&#27861;&#24182;&#24341;&#20837;&#22810;&#20010;&#36741;&#21161;&#20851;&#31995;&#30340;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#31867;&#22411;&#25512;&#26029;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#31890;&#24230;&#30340;&#23454;&#20307;&#31867;&#22411;&#27169;&#24335;&#24314;&#27169;&#26041;&#38754;&#20855;&#26377;&#26356;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#31867;&#22411;&#25512;&#26029;&#65288;KGET&#65289;&#26159;&#39044;&#27979;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#23454;&#20307;&#31867;&#22411;&#30340;&#20219;&#21153;&#12290;&#20197;&#21069;&#65292;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#20851;&#31995;&#8220;hasType&#8221;&#26469;&#24314;&#27169;&#23454;&#20307;&#19982;&#20854;&#31867;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23581;&#35797;&#35299;&#20915;KGET&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#21333;&#20010;&#36741;&#21161;&#20851;&#31995;&#22312;&#34920;&#36798;&#22810;&#26679;&#30340;&#23454;&#20307;&#31867;&#22411;&#27169;&#24335;&#26041;&#38754;&#20855;&#26377;&#38480;&#21046;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22810;&#20010;&#36741;&#21161;&#20851;&#31995;&#26469;&#25552;&#39640;KGE&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#31867;&#20284;&#30340;&#23454;&#20307;&#31867;&#22411;&#34987;&#20998;&#32452;&#65292;&#20197;&#20943;&#23569;&#36741;&#21161;&#20851;&#31995;&#30340;&#25968;&#37327;&#65292;&#24182;&#25552;&#39640;&#23427;&#20204;&#23545;&#19981;&#21516;&#31890;&#24230;&#30340;&#23454;&#20307;&#31867;&#22411;&#27169;&#24335;&#24314;&#27169;&#30340;&#33021;&#21147;&#12290;&#22312;&#22810;&#20010;&#36741;&#21161;&#20851;&#31995;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AsyncET&#30340;&#23454;&#20307;&#31867;&#22411;&#24322;&#27493;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20132;&#26367;&#26356;&#26032;&#23454;&#20307;&#21644;&#31867;&#22411;&#23884;&#20837;&#26469;&#20445;&#25345;&#23398;&#21040;&#30340;&#23454;&#20307;&#23884;&#20837;&#23545;&#20110;&#23454;&#20307;&#31867;&#22411;&#25512;&#26029;&#30340;&#26368;&#26032;&#21644;&#20449;&#24687;&#20016;&#23500;&#12290;&#22312;&#20004;&#20010;&#24120;&#29992;&#30340;KGE&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph entity typing (KGET) is a task to predict the missing entity types in knowledge graphs (KG). Previously, KG embedding (KGE) methods tried to solve the KGET task by introducing an auxiliary relation, 'hasType', to model the relationship between entities and their types. However, a single auxiliary relation has limited expressiveness for diverse entity-type patterns. We improve the expressiveness of KGE methods by introducing multiple auxiliary relations in this work. Similar entity types are grouped to reduce the number of auxiliary relations and improve their capability to model entity-type patterns with different granularities. With the presence of multiple auxiliary relations, we propose a method adopting an Asynchronous learning scheme for Entity Typing, named AsyncET, which updates the entity and type embeddings alternatively to keep the learned entity embedding up-to-date and informative for entity type prediction. Experiments are conducted on two commonly used KGE
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#35757;&#32451;&#21518;&#37327;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;W4A8&#21644;W8A8&#20004;&#31181;&#26041;&#26696;&#30340;&#20248;&#28857;&#65292;&#36890;&#36807;&#36880;&#23618;&#28608;&#27963;&#37327;&#21270;&#21644;&#32454;&#31890;&#24230;&#26435;&#37325;&#37327;&#21270;&#26469;&#35299;&#20915;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15987</link><description>&lt;p&gt;
FPTQ&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#35757;&#32451;&#21518;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
FPTQ: Fine-grained Post-Training Quantization for Large Language Models. (arXiv:2308.15987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#35757;&#32451;&#21518;&#37327;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;W4A8&#21644;W8A8&#20004;&#31181;&#26041;&#26696;&#30340;&#20248;&#28857;&#65292;&#36890;&#36807;&#36880;&#23618;&#28608;&#27963;&#37327;&#21270;&#21644;&#32454;&#31890;&#24230;&#26435;&#37325;&#37327;&#21270;&#26469;&#35299;&#20915;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#20195;&#20013;&#65292;&#24222;&#22823;&#30340;&#21442;&#25968;&#22823;&#23567;&#32473;&#37096;&#32626;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20316;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#37327;&#21270;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20027;&#27969;&#23454;&#36341;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#20004;&#31181;&#26041;&#26696;W8A8&#21644;W4A16&#65288;&#21363;&#36825;&#20004;&#31181;&#20301;&#23485;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;W4A8&#35757;&#32451;&#21518;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29616;&#26377;&#30340;&#24320;&#25918;&#28304;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#36825;&#20004;&#31181;&#26041;&#26696;&#30340;&#20248;&#28857;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;4&#20301;&#26435;&#37325;&#37327;&#21270;&#30340;I/O&#21033;&#29992;&#29575;&#20248;&#21183;&#21644;8&#20301;&#30697;&#38453;&#35745;&#31639;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;W4A8&#38754;&#20020;&#30528;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36880;&#23618;&#28608;&#27963;&#37327;&#21270;&#31574;&#30053;&#65292;&#23545;&#20110;&#26368;&#26840;&#25163;&#30340;&#23618;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25968;&#22343;&#34913;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#32454;&#31890;&#24230;&#26435;&#37325;&#37327;&#21270;&#30456;&#32467;&#21512;&#12290;&#22312;&#27809;&#26377;&#39069;&#22806;&#30340;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#28040;&#38500;&#20102;&#36827;&#19968;&#27493;&#24494;&#35843;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of large-scale language models, the substantial parameter size poses significant challenges for deployment. Being a prevalent compression technique, quantization has emerged as the mainstream practice to tackle this issue, which is mainly centered on two recipes W8A8 and W4A16 (i.e. weights and activations in such bit widths). In this study, we propose a novel W4A8 post-training quantization method for the available open-sourced LLMs, which combines the advantages of both two recipes. Therefore, we can leverage the benefit in the I/O utilization of 4-bit weight quantization and the acceleration due to 8-bit matrix computation. Nevertheless, the W4A8 faces notorious performance degradation. As a remedy, we involve layerwise activation quantization strategies which feature a novel logarithmic equalization for most intractable layers, and we combine them with fine-grained weight quantization. Without whistles and bells, we eliminate the necessity for further fine-tuning and obt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MerA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#34701;&#21512;&#39640;&#25928;&#22320;&#23558;&#39044;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#21512;&#24182;&#21040;&#21333;&#20010;&#27169;&#22411;&#20013;&#65292;&#20197;&#35299;&#20915;&#23569;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MerA&#30456;&#27604;&#20110;&#21333;&#20010;&#36866;&#37197;&#22120;&#21644;AdapterFusion&#37117;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.15982</link><description>&lt;p&gt;
MerA: &#21512;&#24182;&#39044;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MerA: Merging Pretrained Adapters For Few-Shot Learning. (arXiv:2308.15982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MerA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#34701;&#21512;&#39640;&#25928;&#22320;&#23558;&#39044;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#21512;&#24182;&#21040;&#21333;&#20010;&#27169;&#22411;&#20013;&#65292;&#20197;&#35299;&#20915;&#23569;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MerA&#30456;&#27604;&#20110;&#21333;&#20010;&#36866;&#37197;&#22120;&#21644;AdapterFusion&#37117;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#37197;&#22120;&#35843;&#20248;&#26159;&#19968;&#31181;&#20027;&#27969;&#26041;&#27861;&#65292;&#20165;&#26356;&#26032;&#23569;&#37327;&#21442;&#25968;&#65292;&#29992;&#20110;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#32463;&#24120;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#12290;AdapterFusion&#26159;&#19968;&#31181;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#20351;&#29992;&#23450;&#21046;&#30340;&#32452;&#21512;&#23618;&#23558;&#39044;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#32452;&#21512;&#21040;&#29305;&#23450;&#20219;&#21153;&#20013;&#65292;&#20294;&#20250;&#26174;&#33879;&#22686;&#21152;&#21487;&#35757;&#32451;&#21442;&#25968;&#21644;&#37096;&#32626;&#25104;&#26412;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#21333;&#20010;&#36866;&#37197;&#22120;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#20063;&#21487;&#20197;&#32988;&#36807;AdapterFusion&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#25552;&#20986;&#20102;\textbf{\texttt{&#21512;&#24182;&#39044;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;}} (MerA)&#65292;&#36890;&#36807;&#27169;&#22411;&#34701;&#21512;&#23558;&#39044;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#39640;&#25928;&#22320;&#34701;&#20837;&#21040;&#21333;&#20010;&#27169;&#22411;&#20013;&#12290;&#22312;&#20004;&#20010;PLMs&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;MerA&#30456;&#27604;&#21333;&#20010;&#36866;&#37197;&#22120;&#21644;AdapterFusion&#37117;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;MerA&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#31216;&#20026;&#8220;\textit{same-track}&#8221;&#35774;&#32622;&#65292;&#23558;&#36866;&#37197;&#22120;&#34701;&#21512;&#21040;&#21516;&#19968;&#20010;&#36890;&#36947;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapter tuning, which updates only a few parameters, has become a mainstream method for fine-tuning pretrained language models to downstream tasks. However, it often yields subpar results in few-shot learning. AdapterFusion, which assembles pretrained adapters using composition layers tailored to specific tasks, is a possible solution but significantly increases trainable parameters and deployment costs. Despite this, our preliminary study reveals that even single adapters can outperform Adapterfusion in few-shot learning, urging us to propose \textbf{\texttt{Merging Pretrained Adapters}} (MerA) that efficiently incorporates pretrained adapters to a single model through model fusion. Extensive experiments on two PLMs demonstrate that MerA achieves substantial improvements compared to both single adapters and AdapterFusion. To further enhance the capacity of MerA, we also introduce a simple yet effective technique, referred to as the "\textit{same-track}" setting, that merges adapters f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#23545;&#24212;&#20110;&#35299;&#21078;&#32467;&#26500;&#30340;&#23616;&#37096;&#20196;&#29260;&#33021;&#21542;&#25913;&#36827;&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Faster R-CNN&#26041;&#27861;&#65292;&#24182;&#22312;&#35299;&#21078;&#32467;&#26500;&#23450;&#20301;&#26399;&#38388;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#23616;&#37096;&#20196;&#29260;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#25253;&#21578;&#30340;&#20934;&#30830;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.15961</link><description>&lt;p&gt;
&#23547;&#25214;&#24863;&#30693;&#33016;&#37096;X&#20809;&#25253;&#21578;&#20013;&#30340;&#35299;&#21078;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Finding-Aware Anatomical Tokens for Chest X-Ray Automated Reporting. (arXiv:2308.15961v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#23545;&#24212;&#20110;&#35299;&#21078;&#32467;&#26500;&#30340;&#23616;&#37096;&#20196;&#29260;&#33021;&#21542;&#25913;&#36827;&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Faster R-CNN&#26041;&#27861;&#65292;&#24182;&#22312;&#35299;&#21078;&#32467;&#26500;&#23450;&#20301;&#26399;&#38388;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#23616;&#37096;&#20196;&#29260;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#25253;&#21578;&#30340;&#20934;&#30830;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#20219;&#21153;&#21253;&#25324;&#25551;&#36848;&#21644;&#35299;&#37322;&#25918;&#23556;&#22270;&#20687;&#20013;&#30340;&#21307;&#23398;&#21457;&#29616;&#65292;&#21253;&#25324;&#20854;&#20301;&#32622;&#21644;&#22806;&#35266;&#30340;&#25551;&#36848;&#12290;&#33258;&#21160;&#21270;&#25918;&#23556;&#23398;&#25253;&#21578;&#38656;&#35201;&#23558;&#22270;&#20687;&#32534;&#30721;&#20026;&#36866;&#21512;&#36755;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#20196;&#29260;&#34920;&#31034;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23558;&#22270;&#20687;&#32534;&#30721;&#20026;&#19968;&#31995;&#21015;&#22270;&#20687;&#32423;&#29305;&#24449;&#22270;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#25253;&#21578;&#36890;&#24120;&#22312;&#36924;&#30495;&#26679;&#24335;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#20294;&#20934;&#30830;&#24230;&#19981;&#39640;&#12290;&#21463;&#26368;&#36817;&#22312;&#19968;&#33324;&#39046;&#22495;&#20013;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#27599;&#20010;&#35270;&#35273;&#20196;&#29260;&#23545;&#24212;&#20110;&#22270;&#20687;&#20013;&#26816;&#27979;&#21040;&#30340;&#23545;&#35937;&#65292;&#25105;&#20204;&#30740;&#31350;&#20351;&#29992;&#23545;&#24212;&#20110;&#35299;&#21078;&#32467;&#26500;&#30340;&#23616;&#37096;&#20196;&#29260;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#25253;&#21578;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Faster R-CNN&#30340;&#36866;&#24212;&#24615;&#26041;&#27861;&#65292;&#22312;&#35299;&#21078;&#32467;&#26500;&#23450;&#20301;&#26399;&#38388;&#20026;&#20505;&#36873;&#36793;&#30028;&#26694;&#25191;&#34892;&#20102;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#35813;&#32467;&#26524;&#20316;&#20026;&#20196;&#29260;&#36755;&#20837;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of radiology reporting comprises describing and interpreting the medical findings in radiographic images, including description of their location and appearance. Automated approaches to radiology reporting require the image to be encoded into a suitable token representation for input to the language model. Previous methods commonly use convolutional neural networks to encode an image into a series of image-level feature map representations. However, the generated reports often exhibit realistic style but imperfect accuracy. Inspired by recent works for image captioning in the general domain in which each visual token corresponds to an object detected in an image, we investigate whether using local tokens corresponding to anatomical structures can improve the quality of the generated reports. We introduce a novel adaptation of Faster R-CNN in which finding detection is performed for the candidate bounding boxes extracted during anatomical structure localisation. We use the resu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#22312;&#21513;&#23572;&#21513;&#26031;&#35821;&#20013;&#36827;&#34892;&#20027;&#39064;&#20998;&#31867;&#12290;&#22312;&#22810;&#26631;&#31614;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#25910;&#38598;&#21644;&#27880;&#37322;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35813;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.15952</link><description>&lt;p&gt;
&#22312;&#21513;&#23572;&#21513;&#26031;&#35821;&#20013;&#36827;&#34892;&#22810;&#26631;&#31614;&#20027;&#39064;&#20998;&#31867;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Multilabel Topic Classification in the Kyrgyz Language. (arXiv:2308.15952v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#22312;&#21513;&#23572;&#21513;&#26031;&#35821;&#20013;&#36827;&#34892;&#20027;&#39064;&#20998;&#31867;&#12290;&#22312;&#22810;&#26631;&#31614;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#25910;&#38598;&#21644;&#27880;&#37322;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35813;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#22312;&#21513;&#23572;&#21513;&#26031;&#35821;&#20013;&#36827;&#34892;&#20027;&#39064;&#20998;&#31867;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#26032;&#38395;&#32593;&#31449;24.KG&#30340;&#25910;&#38598;&#21644;&#27880;&#37322;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#26631;&#31614;&#35774;&#32622;&#19979;&#26032;&#38395;&#20998;&#31867;&#30340;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#32479;&#35745;&#27169;&#22411;&#21644;&#31070;&#32463;&#27169;&#22411;&#65292;&#24182;&#25253;&#21578;&#20102;&#24471;&#20998;&#65292;&#35752;&#35770;&#20102;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kyrgyz is a very underrepresented language in terms of modern natural language processing resources. In this work, we present a new public benchmark for topic classification in Kyrgyz, introducing a dataset based on collected and annotated data from the news site 24.KG and presenting several baseline models for news classification in the multilabel setting. We train and evaluate both classical statistical and neural models, reporting the scores, discussing the results, and proposing directions for future work.
&lt;/p&gt;</description></item><item><title>LLaSM&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;&#65292;&#20855;&#26377;&#36328;&#27169;&#24577;&#23545;&#35805;&#33021;&#21147;&#65292;&#36890;&#36807;&#36981;&#24490;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.15930</link><description>&lt;p&gt;
LLaSM: &#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaSM: Large Language and Speech Model. (arXiv:2308.15930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15930
&lt;/p&gt;
&lt;p&gt;
LLaSM&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;&#65292;&#20855;&#26377;&#36328;&#27169;&#24577;&#23545;&#35805;&#33021;&#21147;&#65292;&#36890;&#36807;&#36981;&#24490;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#35270;&#35273;-&#35821;&#35328;&#22810;&#27169;&#24577;&#27169;&#22411;&#19978;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#26469;&#36981;&#24490;&#35270;&#35273;&#21644;&#35821;&#35328;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#35821;&#38899;&#20063;&#26159;&#20154;&#31867;&#19982;&#19990;&#30028;&#20114;&#21160;&#30340;&#37325;&#35201;&#26041;&#24335;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#19968;&#20010;&#36890;&#29992;&#30340;&#21161;&#25163;&#26469;&#35828;&#65292;&#33021;&#22815;&#36981;&#24490;&#22810;&#27169;&#24577;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;&#65288;LLaSM&#65289;&#12290;LLaSM&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#38899;-&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#36328;&#27169;&#24577;&#23545;&#35805;&#33021;&#21147;&#65292;&#33021;&#22815;&#36981;&#24490;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;LLaSM&#23637;&#31034;&#20102;&#19968;&#31181;&#26356;&#26041;&#20415;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#12290;&#20026;&#20102;&#25903;&#25345;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#35821;&#38899;&#25351;&#20196;&#25968;&#25454;&#38598;LLaSM-Audio-Instructions&#12290;&#20195;&#30721;&#21644;&#28436;&#31034;&#21487;&#22312;https://github.com/LinkSoul-AI/LLaSM&#21644;ht&#19978;&#26597;&#30475;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models have garnered significant interest recently. Though, most of the works focus on vision-language multi-modal models providing strong capabilities in following vision-and-language instructions. However, we claim that speech is also an important modality through which humans interact with the world. Hence, it is crucial for a general-purpose assistant to be able to follow multi-modal speech-and-language instructions. In this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an end-to-end trained large multi-modal speech-language model with cross-modal conversational abilities, capable of following speech-and-language instructions. Our early experiments show that LLaSM demonstrates a more convenient and natural way for humans to interact with artificial intelligence. Specifically, we also release a large Speech Instruction Following dataset LLaSM-Audio-Instructions. Code and demo are available at https://github.com/LinkSoul-AI/LLaSM and ht
&lt;/p&gt;</description></item><item><title>&#32654;&#22269;&#27861;&#24459;&#38656;&#35201;&#21152;&#24378;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#25361;&#25112;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#31215;&#26497;&#12289;&#21487;&#23457;&#35745;&#30340;&#25351;&#23548;&#65292;&#20197;&#22635;&#34917;&#29616;&#26377;&#27861;&#24459;&#26694;&#26550;&#22312;&#20445;&#25252;&#22522;&#26412;&#20215;&#20540;&#35266;&#26041;&#38754;&#30340;&#31354;&#30333;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15906</link><description>&lt;p&gt;
&#32654;&#22269;&#27861;&#24459;&#20307;&#31995;&#26159;&#21542;&#20934;&#22791;&#22909;&#24212;&#23545;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#25361;&#25112;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is the U.S. Legal System Ready for AI's Challenges to Human Values?. (arXiv:2308.15906v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15906
&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#27861;&#24459;&#38656;&#35201;&#21152;&#24378;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#25361;&#25112;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#31215;&#26497;&#12289;&#21487;&#23457;&#35745;&#30340;&#25351;&#23548;&#65292;&#20197;&#22635;&#34917;&#29616;&#26377;&#27861;&#24459;&#26694;&#26550;&#22312;&#20445;&#25252;&#22522;&#26412;&#20215;&#20540;&#35266;&#26041;&#38754;&#30340;&#31354;&#30333;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#35843;&#26597;&#20102;&#32654;&#22269;&#27861;&#24459;&#22312;&#38754;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#25361;&#25112;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#19987;&#23478;&#30740;&#35752;&#20250;&#26399;&#38388;&#21046;&#23450;&#30340;&#22810;&#31181;&#20551;&#35774;&#24773;&#26223;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#27861;&#24459;&#26694;&#26550;&#22312;&#20445;&#25252;&#33258;&#20027;&#26435;&#12289;&#38544;&#31169;&#26435;&#12289;&#23562;&#20005;&#12289;&#22810;&#26679;&#24615;&#12289;&#24179;&#31561;&#20197;&#21450;&#36523;&#24515;&#20581;&#24247;&#31561;&#22522;&#26412;&#20215;&#20540;&#35266;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#31354;&#30333;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#23466;&#27861;&#21644;&#27665;&#26435;&#27861;&#20284;&#20046;&#26080;&#27861;&#23545;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#27495;&#35270;&#24615;&#20135;&#20986;&#25552;&#20379;&#36275;&#22815;&#30340;&#20445;&#25252;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#25105;&#20204;&#25490;&#38500;&#31532;230&#26465;&#27454;&#25552;&#20379;&#30340;&#36131;&#20219;&#20445;&#25252;&#65292;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22797;&#26434;&#21644;&#19981;&#36879;&#26126;&#24615;&#65292;&#35777;&#26126;&#35837;&#35876;&#21644;&#20135;&#21697;&#36131;&#20219;&#32034;&#36180;&#30340;&#22240;&#26524;&#20851;&#31995;&#20063;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24102;&#26469;&#30340;&#29420;&#29305;&#21644;&#38590;&#20197;&#39044;&#27979;&#30340;&#23041;&#32961;&#65292;&#25105;&#20204;&#20027;&#24352;&#24314;&#31435;&#33021;&#22815;&#36866;&#24212;&#26032;&#23041;&#32961;&#24182;&#20026;&#34892;&#19994;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#31215;&#26497;&#12289;&#21487;&#23457;&#35745;&#30340;&#25351;&#23548;&#30340;&#27861;&#24459;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our interdisciplinary study investigates how effectively U.S. laws confront the challenges posed by Generative AI to human values. Through an analysis of diverse hypothetical scenarios crafted during an expert workshop, we have identified notable gaps and uncertainties within the existing legal framework regarding the protection of fundamental values, such as autonomy, privacy, dignity, diversity, equality, and physical/mental well-being. Constitutional and civil rights, it appears, may not provide sufficient protection against AI-generated discriminatory outputs. Furthermore, even if we exclude the liability shield provided by Section 230, proving causation for defamation and product liability claims is a challenging endeavor due to the intricate and opaque nature of AI systems. To address the unique and unforeseeable threats posed by Generative AI, we advocate for legal frameworks that evolve to recognize new threat and provide proactive, auditable guidelines to industry stakeholders
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#19968;&#27425;&#24615;&#30340;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24120;&#35782;&#32972;&#26223;&#30693;&#35782;&#21644;&#20803;&#35299;&#37322;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#20013;&#23398;&#20064;&#20998;&#31867;&#35268;&#21017;&#65292;&#19988;&#22797;&#26434;&#24230;&#26356;&#39640;&#30340;&#26679;&#26412;&#21487;&#20197;&#36798;&#21040;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15885</link><description>&lt;p&gt;
&#20351;&#29992;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#30340;&#19968;&#27425;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards One-Shot Learning for Text Classification using Inductive Logic Programming. (arXiv:2308.15885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#19968;&#27425;&#24615;&#30340;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24120;&#35782;&#32972;&#26223;&#30693;&#35782;&#21644;&#20803;&#35299;&#37322;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#20013;&#23398;&#20064;&#20998;&#31867;&#35268;&#21017;&#65292;&#19988;&#22797;&#26434;&#24230;&#26356;&#39640;&#30340;&#26679;&#26412;&#21487;&#20197;&#36798;&#21040;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#25191;&#34892;&#20010;&#24615;&#21270;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#19981;&#26029;&#22686;&#38271;&#65292;&#24320;&#21457;&#25968;&#25454;&#39640;&#25928;&#19988;&#19981;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#19968;&#27425;&#24615;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20803;&#35299;&#37322;&#23398;&#20064;&#65288;MIL&#65289;&#26694;&#26550;&#65292;&#24182;&#21033;&#29992;&#20174;ConceptNet&#20013;&#25552;&#21462;&#30340;&#24120;&#35782;&#32972;&#26223;&#30693;&#35782;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;MIL&#21487;&#20197;&#20174;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#20013;&#23398;&#20064;&#25991;&#26412;&#20998;&#31867;&#35268;&#21017;&#12290;&#27492;&#22806;&#65292;&#36873;&#25321;&#30340;&#26679;&#26412;&#30340;&#22797;&#26434;&#24230;&#36234;&#39640;&#65292;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#20063;&#36234;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the ever-increasing potential of AI to perform personalised tasks, it is becoming essential to develop new machine learning techniques which are data-efficient and do not require hundreds or thousands of training data. In this paper, we explore an Inductive Logic Programming approach for one-shot text classification. In particular, we explore the framework of Meta-Interpretive Learning (MIL), along with using common-sense background knowledge extracted from ConceptNet. Results indicate that MIL can learn text classification rules from a small number of training examples. Moreover, the higher complexity of chosen examples, the higher accuracy of the outcome.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#33258;&#28982;&#35821;&#35328;&#21487;&#35299;&#37322;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;-&#29289;&#21697;&#29305;&#24449;&#20135;&#29983;&#22522;&#20110;&#20107;&#23454;&#30340;&#20010;&#24615;&#21270;&#35299;&#37322;&#65292;&#21516;&#26102;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#29992;&#25143;&#30340;&#20449;&#24515;&#21644;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2308.15813</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#33616;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Knowledge-grounded Natural Language Recommendation Explanation. (arXiv:2308.15813v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#33258;&#28982;&#35821;&#35328;&#21487;&#35299;&#37322;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;-&#29289;&#21697;&#29305;&#24449;&#20135;&#29983;&#22522;&#20110;&#20107;&#23454;&#30340;&#20010;&#24615;&#21270;&#35299;&#37322;&#65292;&#21516;&#26102;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#29992;&#25143;&#30340;&#20449;&#24515;&#21644;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#20915;&#31574;&#36741;&#21161;&#29992;&#25143;&#29702;&#35299;&#30340;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#29992;&#25143;&#23545;&#31995;&#32479;&#30340;&#20449;&#24515;&#21644;&#20449;&#20219;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20197;&#20154;&#31867;&#21487;&#35835;&#30340;&#24418;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#25552;&#20986;&#30340;&#26041;&#27861;&#24448;&#24448;&#21033;&#29992;&#29992;&#25143;&#32534;&#20889;&#30340;&#29289;&#21697;&#35780;&#35770;&#65292;&#36825;&#20123;&#35780;&#35770;&#36890;&#24120;&#20027;&#35266;&#65292;&#35821;&#35328;&#36139;&#20047;&#65292;&#24182;&#19988;&#26080;&#27861;&#28085;&#30422;&#26410;&#34987;&#36141;&#20080;&#25110;&#35780;&#35770;&#30340;&#26032;&#29289;&#21697;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26681;&#25454;&#29992;&#25143;&#30340;&#36141;&#20080;&#21382;&#21490;&#65292;&#29983;&#25104;&#20197;&#29289;&#21697;&#29305;&#24449;&#23458;&#35266;&#25551;&#36848;&#30340;&#22522;&#20110;&#20107;&#23454;&#30340;&#25512;&#33616;&#35299;&#37322;&#65292;&#24182;&#38544;&#21547;&#32771;&#34385;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#33258;&#28982;&#35821;&#35328;&#21487;&#35299;&#37322;&#25512;&#33616;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#65292;&#21033;&#29992;&#29992;&#25143;-&#29289;&#21697;&#29305;&#24449;&#20135;&#29983;&#22522;&#20110;&#20107;&#23454;&#30340;&#20010;&#24615;&#21270;&#35299;&#37322;&#65292;&#21516;&#26102;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanations accompanied by a recommendation can assist users in understanding the decision made by recommendation systems, which in turn increases a user's confidence and trust in the system. Recently, research has focused on generating natural language explanations in a human-readable format. Thus far, the proposed approaches leverage item reviews written by users, which are often subjective, sparse in language, and unable to account for new items that have not been purchased or reviewed before. Instead, we aim to generate fact-grounded recommendation explanations that are objectively described with item features while implicitly considering a user's preferences, based on the user's purchase history. To achieve this, we propose a knowledge graph (KG) approach to natural language explainable recommendation. Our approach draws on user-item features through a novel collaborative filtering-based KG representation to produce fact-grounded, personalized explanations, while jointly learning
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#20110;&#23545;&#40784;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#35774;&#35745;&#21453;&#39304;&#36873;&#25321;&#26159;&#35780;&#20998;&#36824;&#26159;&#25490;&#21517;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#19988;&#27880;&#37322;&#32773;&#30340;&#20559;&#35265;&#20063;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#20063;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.15812</link><description>&lt;p&gt;
&#36879;&#36807;&#20559;&#22909;&#30475;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#33719;&#21462;&#65306;&#25581;&#31034;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models. (arXiv:2308.15812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#20110;&#23545;&#40784;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#35774;&#35745;&#21453;&#39304;&#36873;&#25321;&#26159;&#35780;&#20998;&#36824;&#26159;&#25490;&#21517;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#19988;&#27880;&#37322;&#32773;&#30340;&#20559;&#35265;&#20063;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#20063;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#24847;&#22270;&#30340;&#23545;&#40784;&#25215;&#35834;&#28041;&#21450;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25110;&#20154;&#31867;&#21453;&#39304;&#12290;&#31264;&#23494;&#30340;&#21453;&#39304;&#27880;&#37322;&#33719;&#21462;&#21644;&#25972;&#21512;&#25104;&#26412;&#36739;&#39640;&#65292;&#32780;&#31232;&#30095;&#30340;&#21453;&#39304;&#21017;&#28041;&#21450;&#32467;&#26500;&#24615;&#35774;&#35745;&#36873;&#25321;&#65292;&#21363;&#35780;&#20998;&#65288;&#20363;&#22914;&#65292;&#22312;1-7&#30340;&#33539;&#22260;&#20869;&#23545;&#22238;&#31572;A&#36827;&#34892;&#35780;&#20998;&#65289;&#21644;&#25490;&#21517;&#65288;&#20363;&#22914;&#65292;&#22238;&#31572;A&#26159;&#21542;&#27604;&#22238;&#31572;B&#26356;&#22909;&#65311;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#23545;LLMs&#30340;&#23545;&#40784;&#21644;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#22312;&#20154;&#31867;&#21644;AI&#27880;&#37322;&#32773;&#20013;&#37117;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#36798;&#21040;&#20102;60%&#12290;&#25105;&#20204;&#30340;&#21518;&#32493;&#20998;&#26512;&#30830;&#23450;&#20102;&#35299;&#37322;&#36825;&#20010;&#29616;&#35937;&#30340;&#21508;&#31181;&#27880;&#37322;&#32773;&#20559;&#35265;&#26041;&#38754;&#65292;&#27604;&#22914;&#20154;&#31867;&#27880;&#37322;&#32773;&#26356;&#21916;&#27426;&#23494;&#38598;&#22238;&#31572;&#24182;&#22312;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#26356;&#38738;&#30544;&#20934;&#30830;&#24615;&#12290;&#20196;&#25105;&#20204;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#23545;&#23545;&#40784;&#30340;LLMs&#30340;&#35780;&#20272;&#20063;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#35780;&#20272;&#32467;&#26524;&#22240;&#20026;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena, such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments. To our surprise, we also observe that the choice of feedback protocol also has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs
&lt;/p&gt;</description></item><item><title>&#22312;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#20102;&#35299;&#20915;&#22522;&#20110;BERT&#27169;&#22411;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#22810;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#22312;&#26368;&#32456;&#39044;&#27979;&#20043;&#21069;&#65292;&#23545;&#32473;&#23450;&#25968;&#25454;&#36827;&#34892;&#39069;&#22806;&#30340;&#25513;&#30721;&#23454;&#20307;&#20256;&#36882;&#65292;&#20197;&#20415;&#22312;&#27169;&#22411;&#30693;&#36947;&#39044;&#27979;&#24773;&#24863;&#30340;&#30830;&#20999;&#23454;&#20307;&#21644;&#19981;&#30693;&#36947;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#21512;&#24182;&#26469;&#33258;&#27169;&#22411;&#30340;&#36923;&#36753;&#12290;</title><link>http://arxiv.org/abs/2308.15793</link><description>&lt;p&gt;
HAlf-MAsked&#27169;&#22411;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
HAlf-MAsked Model for Named Entity Sentiment analysis. (arXiv:2308.15793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15793
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#20102;&#35299;&#20915;&#22522;&#20110;BERT&#27169;&#22411;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#22810;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#22312;&#26368;&#32456;&#39044;&#27979;&#20043;&#21069;&#65292;&#23545;&#32473;&#23450;&#25968;&#25454;&#36827;&#34892;&#39069;&#22806;&#30340;&#25513;&#30721;&#23454;&#20307;&#20256;&#36882;&#65292;&#20197;&#20415;&#22312;&#27169;&#22411;&#30693;&#36947;&#39044;&#27979;&#24773;&#24863;&#30340;&#30830;&#20999;&#23454;&#20307;&#21644;&#19981;&#30693;&#36947;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#21512;&#24182;&#26469;&#33258;&#27169;&#22411;&#30340;&#36923;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#20998;&#26512;&#65288;NESA&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#26368;&#27963;&#36291;&#30340;&#24212;&#29992;&#39046;&#22495;&#20043;&#19968;&#12290;&#31038;&#20132;&#23186;&#20307;NESA&#26159;&#24847;&#35265;&#20998;&#26512;&#30340;&#37325;&#35201;&#39046;&#22495;&#65292;&#22240;&#20026;&#26816;&#27979;&#21644;&#36319;&#36394;&#26032;&#38395;&#27969;&#20013;&#30340;&#24773;&#24863;&#36235;&#21183;&#23545;&#20110;&#26500;&#24314;&#21508;&#31181;&#20998;&#26512;&#31995;&#32479;&#21644;&#30417;&#27979;&#29305;&#23450;&#20154;&#29289;&#25110;&#20844;&#21496;&#30340;&#23186;&#20307;&#24418;&#35937;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;transformer&#30340;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#22312;RuSentNE-23&#35780;&#20272;&#20013;&#30340;NESA&#12290;&#23613;&#31649;BERT&#31561;&#27169;&#22411;&#30340;&#25928;&#26524;&#24456;&#22909;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#21487;&#33021;&#22312;&#26576;&#20123;&#25361;&#25112;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#20363;&#22914;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36825;&#26159;&#22312;RuSentNE-23&#25968;&#25454;&#19978;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#22312;&#26368;&#32456;&#39044;&#27979;&#20043;&#21069;&#65292;&#23545;&#32473;&#23450;&#25968;&#25454;&#36827;&#34892;&#39069;&#22806;&#30340;&#25513;&#30721;&#23454;&#20307;&#20256;&#36882;&#65292;&#20197;&#20415;&#22312;&#27169;&#22411;&#30693;&#36947;&#39044;&#27979;&#24773;&#24863;&#30340;&#30830;&#20999;&#23454;&#20307;&#21644;&#19981;&#30693;&#36947;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#21512;&#24182;&#26469;&#33258;&#27169;&#22411;&#30340;&#36923;&#36753;&#12290;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Named Entity Sentiment analysis (NESA) is one of the most actively developing application domains in Natural Language Processing (NLP). Social media NESA is a significant field of opinion analysis since detecting and tracking sentiment trends in the news flow is crucial for building various analytical systems and monitoring the media image of specific people or companies. In this paper, we study different transformers-based solutions NESA in RuSentNE-23 evaluation. Despite the effectiveness of the BERT-like models, they can still struggle with certain challenges, such as overfitting, which appeared to be the main obstacle in achieving high accuracy on the RuSentNE-23 data. We present several approaches to overcome this problem, among which there is a novel technique of additional pass over given data with masked entity before making the final prediction so that we can combine logits from the model when it knows the exact entity it predicts sentiment for and when it does not. Utilizing 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65292;&#23558;&#20219;&#21153;&#20449;&#24687;&#19982;MoE&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#26032;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.15772</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#29992;&#20110;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Task-Based MoE for Multitask Multilingual Machine Translation. (arXiv:2308.15772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65292;&#23558;&#20219;&#21153;&#20449;&#24687;&#19982;MoE&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#26032;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#30340;&#22810;&#31181;&#24212;&#29992;&#20013;&#65292;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26550;&#26500;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;MoE&#23454;&#29616;&#26159;&#20219;&#21153;&#26080;&#20851;&#30340;&#65292;&#23558;&#19981;&#21516;&#20219;&#21153;&#30340;&#25152;&#26377;&#26631;&#35760;&#20197;&#30456;&#21516;&#26041;&#24335;&#22788;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#20139;&#30340;&#21160;&#24577;&#22522;&#20110;&#20219;&#21153;&#30340;&#36866;&#37197;&#22120;&#65292;&#22312;MoE&#27169;&#22411;&#30340;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19978;&#23558;&#20219;&#21153;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#19978;&#30340;&#20248;&#21183;&#12290;&#20511;&#21161;&#20219;&#21153;&#29305;&#23450;&#30340;&#36866;&#37197;&#22120;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#39640;&#25928;&#22320;&#25512;&#24191;&#21040;&#26032;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-experts (MoE) architecture has been proven a powerful method for diverse tasks in training deep models in many applications. However, current MoE implementations are task agnostic, treating all tokens from different tasks in the same manner. In this work, we instead design a novel method that incorporates task information into MoE models at different granular levels with shared dynamic task-based adapters. Our experiments and analysis show the advantages of our approaches over the dense and canonical MoE models on multi-task multilingual machine translations. With task-specific adapters, our models can additionally generalize to new tasks efficiently.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#33258;&#21160;&#26816;&#27979;&#32593;&#32476;&#27450;&#20940;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#24182;&#21457;&#29616;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.15745</link><description>&lt;p&gt;
&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;&#26041;&#35328;&#30340;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#65306;&#29616;&#29366;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Cyberbullying Detection for Low-resource Languages and Dialects: Review of the State of the Art. (arXiv:2308.15745v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#33258;&#21160;&#26816;&#27979;&#32593;&#32476;&#27450;&#20940;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#24182;&#21457;&#29616;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#21450;&#26102;&#22788;&#29702;&#20869;&#23481;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#40723;&#21169;&#29992;&#25143;&#28389;&#29992;&#36825;&#20123;&#24179;&#21488;&#20256;&#25773;&#31895;&#20439;&#25110;&#36785;&#39554;&#24615;&#35821;&#35328;&#65292;&#36825;&#31181;&#37325;&#22797;&#24615;&#30340;&#34892;&#20026;&#20250;&#23548;&#33268;&#32593;&#32476;&#27450;&#20940;&#65292;&#23427;&#26159;&#19968;&#31181;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#21457;&#29983;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#20294;&#20250;&#24102;&#26469;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#21518;&#26524;&#65292;&#22914;&#25233;&#37057;&#12289;&#36864;&#32553;&#65292;&#29978;&#33267;&#21463;&#23475;&#32773;&#33258;&#26432;&#20225;&#22270;&#12290;&#33258;&#21160;&#26816;&#27979;&#21644;&#32531;&#35299;&#32593;&#32476;&#27450;&#20940;&#30340;&#31995;&#32479;&#24050;&#32463;&#24320;&#21457;&#20986;&#26469;&#20102;&#65292;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;&#20854;&#20013;&#32477;&#22823;&#22810;&#25968;&#26159;&#38024;&#23545;&#33521;&#35821;&#65292;&#20165;&#26377;&#23569;&#25968;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#20026;&#20102;&#35780;&#20272;&#30446;&#21069;&#30340;&#30740;&#31350;&#29616;&#29366;&#24182;&#30830;&#23450;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#38656;&#27714;&#65292;&#26412;&#25991;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#33258;&#21160;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#26041;&#38754;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#31995;&#32479;&#30340;&#35843;&#30740;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25152;&#26377;&#21487;&#29992;&#30340;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;70&#22810;&#20010;&#20851;&#20110;&#33258;&#21160;&#26816;&#27979;&#32593;&#32476;&#27450;&#20940;&#25110;&#30456;&#20851;&#35821;&#35328;&#30340;&#24050;&#21457;&#34920;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The struggle of social media platforms to moderate content in a timely manner, encourages users to abuse such platforms to spread vulgar or abusive language, which, when performed repeatedly becomes cyberbullying a social problem taking place in virtual environments, yet with real-world consequences, such as depression, withdrawal, or even suicide attempts of its victims. Systems for the automatic detection and mitigation of cyberbullying have been developed but, unfortunately, the vast majority of them are for the English language, with only a handful available for low-resource languages. To estimate the present state of research and recognize the needs for further development, in this paper we present a comprehensive systematic survey of studies done so far for automatic cyberbullying detection in low-resource languages. We analyzed all studies on this topic that were available. We investigated more than seventy published studies on automatic detection of cyberbullying or related lan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#12289;&#23454;&#20307;&#32423;&#30340;&#23450;&#20041;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#25552;&#21462;&#25935;&#24863;&#23454;&#20307;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#37325;&#26500;&#25935;&#24863;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15727</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#21644;&#20998;&#26512;&#23454;&#20307;&#32423;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Quantifying and Analyzing Entity-level Memorization in Large Language Models. (arXiv:2308.15727v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#12289;&#23454;&#20307;&#32423;&#30340;&#23450;&#20041;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#25552;&#21462;&#25935;&#24863;&#23454;&#20307;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#37325;&#26500;&#25935;&#24863;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35777;&#26126;&#33021;&#22815;&#35760;&#24518;&#20854;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#35774;&#35745;&#30340;&#25552;&#31034;&#25552;&#21462;&#20986;&#26469;&#12290;&#38543;&#30528;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#30001;&#35760;&#24518;&#24341;&#36215;&#30340;&#38544;&#31169;&#39118;&#38505;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#33021;&#21147;&#26377;&#21161;&#20110;&#35780;&#20272;&#28508;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#20851;&#20110;&#37327;&#21270;&#35760;&#24518;&#30340;&#30740;&#31350;&#38656;&#35201;&#35775;&#38382;&#31934;&#30830;&#30340;&#21407;&#22987;&#25968;&#25454;&#25110;&#20135;&#29983;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#24456;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#12289;&#23454;&#20307;&#32423;&#30340;&#23450;&#20041;&#65292;&#29992;&#20110;&#20197;&#26356;&#25509;&#36817;&#23454;&#38469;&#22330;&#26223;&#30340;&#26465;&#20214;&#21644;&#24230;&#37327;&#26041;&#24335;&#26469;&#37327;&#21270;&#35760;&#24518;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#39640;&#25928;&#25552;&#21462;&#25935;&#24863;&#23454;&#20307;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22522;&#20110;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#37325;&#26500;&#25935;&#24863;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been proven capable of memorizing their training data, which can be extracted through specifically designed prompts. As the scale of datasets continues to grow, privacy risks arising from memorization have attracted increasing attention. Quantifying language model memorization helps evaluate potential privacy risks. However, prior works on quantifying memorization require access to the precise original data or incur substantial computational overhead, making it difficult for applications in real-world language models. To this end, we propose a fine-grained, entity-level definition to quantify memorization with conditions and metrics closer to real-world scenarios. In addition, we also present an approach for efficiently extracting sensitive entities from autoregressive language models. We conduct extensive experiments based on the proposed, probing language models' ability to reconstruct sensitive entities under different settings. We find that languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DKGen&#31995;&#32479;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#30693;&#35782;&#21442;&#32771;&#65292;&#28040;&#38500;&#20102;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#38750;&#20851;&#32852;&#21442;&#32771;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15711</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#30693;&#35782;&#36873;&#25321;&#20248;&#21270;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Optimizing Factual Accuracy in Text Generation through Dynamic Knowledge Selection. (arXiv:2308.15711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DKGen&#31995;&#32479;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#30693;&#35782;&#21442;&#32771;&#65292;&#28040;&#38500;&#20102;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#38750;&#20851;&#32852;&#21442;&#32771;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#25105;&#20204;&#19982;&#20449;&#24687;&#20114;&#21160;&#26041;&#38754;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20250;&#29983;&#25104;&#38750;&#20107;&#23454;&#24615;&#30340;&#25991;&#26412;&#65292;&#24341;&#21457;&#23545;&#20854;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#20316;&#20026;&#25991;&#26412;&#29983;&#25104;&#30340;&#21442;&#32771;&#26469;&#22686;&#24378;&#20107;&#23454;&#24615;&#65292;&#20294;&#24448;&#24448;&#22312;&#26080;&#20851;&#21442;&#32771;&#30340;&#30693;&#35782;&#28151;&#20081;&#65288;&#22914;&#23454;&#20307;&#19981;&#21305;&#37197;&#65289;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#36755;&#20986;&#25991;&#26412;&#30340;&#38271;&#24230;&#22686;&#21152;&#65292;&#38543;&#26426;&#37319;&#26679;&#30340;&#38543;&#24847;&#24615;&#20250;&#21152;&#21095;&#65292;&#23545;&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DKGen&#65292;&#23558;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#21010;&#20998;&#20026;&#36845;&#20195;&#36807;&#31243;&#12290;&#22312;&#27599;&#20010;&#36845;&#20195;&#20013;&#65292;DKGen&#23558;&#36755;&#20837;&#30340;&#26597;&#35810;&#12289;&#20808;&#21069;&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#19968;&#37096;&#20998;&#21442;&#32771;&#27573;&#33853;&#20316;&#20026;&#36755;&#20837;&#26469;&#29983;&#25104;&#30701;&#25991;&#26412;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#20808;&#21069;&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#26597;&#35810;&#19982;&#20043;&#30340;&#30456;&#20851;&#24615;&#65292;&#21160;&#24577;&#36873;&#25321;&#23376;&#38598;&#65292;&#20174;&#23436;&#25972;&#30340;&#27573;&#33853;&#38598;&#20013;&#22823;&#37327;&#28040;&#38500;&#20102;&#19981;&#30456;&#20851;&#30340;&#21442;&#32771;&#20869;&#23481;&#30340;&#36755;&#20837;&#12290;&#20026;&#36827;&#19968;&#27493;&#22686;&#24378;DKGen&#30340;&#33021;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have revolutionized the way we interact with information, but they often generate nonfactual text, raising concerns about their reliability. Previous methods use external knowledge as references for text generation to enhance factuality but often struggle with the knowledge mix-up(e.g., entity mismatch) of irrelevant references. Besides,as the length of the output text grows, the randomness of sampling can escalate, detrimentally impacting the factual accuracy of the generated text. In this paper, we present DKGen, which divide the text generation process into an iterative process. In each iteration, DKGen takes the input query, the previously generated text and a subset of the reference passages as input to generate short text. During the process, the subset is dynamically selected from the full passage set based on their relevance to the previously generated text and the query, largely eliminating the irrelevant references from input. To further enhance DKGen's 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22522;&#20110;Transformer&#12289;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21155;&#21183;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#32570;&#20047;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#26377;&#35821;&#35328;&#26080;&#20851;&#30340;Document AI&#27169;&#22411;&#65292;&#20294;&#20854;&#24615;&#33021;&#20173;&#23384;&#22312;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.15517</link><description>&lt;p&gt;
Document AI: &#22522;&#20110;Transformer&#12289;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Document AI: A Comparative Study of Transformer-Based, Graph-Based Models, and Convolutional Neural Networks For Document Layout Analysis. (arXiv:2308.15517v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22522;&#20110;Transformer&#12289;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21155;&#21183;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#32570;&#20047;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#26377;&#35821;&#35328;&#26080;&#20851;&#30340;Document AI&#27169;&#22411;&#65292;&#20294;&#20854;&#24615;&#33021;&#20173;&#23384;&#22312;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Document AI&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#33258;&#21160;&#20998;&#26512;&#25991;&#26723;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#20219;&#21153;&#26159;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#65292;&#36890;&#36807;&#35299;&#37322;&#24067;&#23616;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20869;&#23481;&#21644;&#31354;&#38388;&#20851;&#31995;&#26469;&#32467;&#26500;&#21270;&#25991;&#26723;&#39029;&#38754;&#12290;&#36825;&#20010;&#20219;&#21153;&#21487;&#20197;&#26159;&#20197;&#22270;&#20687;&#20026;&#20013;&#24515;&#65292;&#30446;&#30340;&#26159;&#35782;&#21035;&#21644;&#26631;&#35760;&#21508;&#31181;&#21306;&#22495;&#65292;&#22914;&#20316;&#32773;&#21644;&#27573;&#33853;&#65307;&#20063;&#21487;&#20197;&#26159;&#20197;&#25991;&#26412;&#20026;&#20013;&#24515;&#65292;&#37325;&#28857;&#26159;&#23545;&#25991;&#26723;&#20013;&#30340;&#21333;&#35789;&#36827;&#34892;&#20998;&#31867;&#12290;&#34429;&#28982;&#26377;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#24067;&#23616;&#20998;&#26512;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#20173;&#23384;&#22312;&#30097;&#34385;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#22522;&#20110;&#38750;&#24120;&#19981;&#21516;&#30340;&#26550;&#26500;&#24320;&#21457;&#20102;&#31995;&#32479;&#65292;&#22914;&#22522;&#20110;Transformer&#30340;&#12289;&#22522;&#20110;&#22270;&#30340;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#30740;&#31350;&#25552;&#21450;&#36825;&#20123;&#27169;&#22411;&#22312;&#27604;&#36739;&#20998;&#26512;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#23384;&#22312;&#33021;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#30340;&#35821;&#35328;&#26080;&#20851;&#30340;Document AI&#27169;&#22411;&#65292;&#20294;&#20854;&#24615;&#33021;&#20173;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document AI aims to automatically analyze documents by leveraging natural language processing and computer vision techniques. One of the major tasks of Document AI is document layout analysis, which structures document pages by interpreting the content and spatial relationships of layout, image, and text. This task can be image-centric, wherein the aim is to identify and label various regions such as authors and paragraphs, or text-centric, where the focus is on classifying individual words in a document. Although there are increasingly sophisticated methods for improving layout analysis, doubts remain about the extent to which their findings can be generalized to a broader context. Specifically, prior work developed systems based on very different architectures, such as transformer-based, graph-based, and CNNs. However, no work has mentioned the effectiveness of these models in a comparative analysis. Moreover, while language-independent Document AI models capable of knowledge transfe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#23545;&#35805;&#31995;&#32479;&#65292;&#23558;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#23545;&#35805;&#12289;&#33080;&#37096;&#34920;&#24773;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;GPT-3.5&#27169;&#22411;&#26469;&#29983;&#25104;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#65292;&#20197;&#25552;&#20379;&#20449;&#24687;&#24182;&#19982;&#35775;&#23458;&#36827;&#34892;&#33258;&#28982;&#20132;&#27969;&#12290;</title><link>http://arxiv.org/abs/2308.15214</link><description>&lt;p&gt;
FurChat: &#20351;&#29992;LLMs&#30340;&#20855;&#26377;&#33080;&#37096;&#34920;&#24773;&#30340;&#20132;&#20114;&#24335;&#23545;&#35805;&#31995;&#32479;&#65292;&#32467;&#21512;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
FurChat: An Embodied Conversational Agent using LLMs, Combining Open and Closed-Domain Dialogue with Facial Expressions. (arXiv:2308.15214v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#23545;&#35805;&#31995;&#32479;&#65292;&#23558;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#23545;&#35805;&#12289;&#33080;&#37096;&#34920;&#24773;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;GPT-3.5&#27169;&#22411;&#26469;&#29983;&#25104;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#65292;&#20197;&#25552;&#20379;&#20449;&#24687;&#24182;&#19982;&#35775;&#23458;&#36827;&#34892;&#33258;&#28982;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#23545;&#35805;&#31995;&#32479;&#65292;&#21487;&#20197;&#20316;&#20026;&#25509;&#24453;&#21592;&#65292;&#29983;&#25104;&#32467;&#21512;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#23545;&#35805;&#20197;&#21450;&#33080;&#37096;&#34920;&#24773;&#30340;&#28151;&#21512;&#23545;&#35805;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#24320;&#21457;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#23558;&#35813;&#31995;&#32479;&#37096;&#32626;&#21040;&#20102;&#19968;&#20010;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;Furhat&#26426;&#22120;&#20154;&#19978;&#65292;&#22312;&#20114;&#21160;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#21475;&#22836;&#21644;&#38750;&#35821;&#35328;&#25552;&#31034;&#12290;&#35813;&#31995;&#32479;&#19987;&#38376;&#20026;&#22269;&#23478;&#26426;&#22120;&#20154;&#23454;&#39564;&#23460;&#35774;&#35745;&#65292;&#36890;&#36807;&#33258;&#28982;&#23545;&#35805;&#19982;&#35775;&#23458;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#21521;&#20182;&#20204;&#25552;&#20379;&#26377;&#20851;&#35774;&#26045;&#12289;&#30740;&#31350;&#12289;&#26032;&#38395;&#12289;&#21363;&#23558;&#20030;&#34892;&#30340;&#27963;&#21160;&#31561;&#26041;&#38754;&#30340;&#20449;&#24687;&#12290;&#31995;&#32479;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;GPT-3.5&#27169;&#22411;&#26681;&#25454;&#25552;&#31034;&#29983;&#25104;&#36825;&#20123;&#20449;&#24687;&#65292;&#21516;&#26102;&#29983;&#25104;&#39046;&#22495;&#36890;&#29992;&#30340;&#23545;&#35805;&#21644;&#38754;&#37096;&#34920;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate an embodied conversational agent that can function as a receptionist and generate a mixture of open and closed-domain dialogue along with facial expressions, by using a large language model (LLM) to develop an engaging conversation. We deployed the system onto a Furhat robot, which is highly expressive and capable of using both verbal and nonverbal cues during interaction. The system was designed specifically for the National Robotarium to interact with visitors through natural conversations, providing them with information about the facilities, research, news, upcoming events, etc. The system utilises the state-of-the-art GPT-3.5 model to generate such information along with domain-general conversations and facial expressions based on prompt engineering.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeBERT&#30340;SNN&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;Spikformer&#26550;&#26500;&#21644;&#20351;&#29992;&#20004;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#20854;&#20182;SNN&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#29978;&#33267;&#36798;&#21040;&#20102;&#19982;BERT&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15122</link><description>&lt;p&gt;
SpikeBERT&#65306;&#19968;&#31181;&#37319;&#29992;&#20004;&#38454;&#27573;BERT&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#30340;&#35821;&#35328;Spikformer
&lt;/p&gt;
&lt;p&gt;
SpikeBERT: A Language Spikformer Trained with Two-Stage Knowledge Distillation from BERT. (arXiv:2308.15122v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeBERT&#30340;SNN&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;Spikformer&#26550;&#26500;&#21644;&#20351;&#29992;&#20004;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#20854;&#20182;SNN&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#29978;&#33267;&#36798;&#21040;&#20102;&#19982;BERT&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22312;&#20197;&#26356;&#33410;&#33021;&#30340;&#26041;&#24335;&#23454;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#35821;&#35328;&#20219;&#21153;&#30340;SNN&#32593;&#32476;&#26550;&#26500;&#36807;&#20110;&#31616;&#21333;&#65292;&#28145;&#24230;&#26550;&#26500;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#65292;&#19982;BERT&#31561;&#20027;&#27969;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#30456;&#27604;&#65292;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#33033;&#20914;Transformer&#65288;&#21363;Spikformer&#65289;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#35821;&#35328;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#26469;&#35757;&#32451;&#23427;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#36890;&#36807;&#20174;BERT&#21644;&#22823;&#37327;&#26410;&#26631;&#35760;&#25991;&#26412;&#20013;&#33976;&#39311;&#30693;&#35782;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#20877;&#27425;&#20174;&#22312;&#30456;&#21516;&#35757;&#32451;&#31034;&#20363;&#19978;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#23454;&#20363;&#30693;&#35782;&#33976;&#39311;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;SpikeBERT&#65292;&#22312;&#23454;&#29616;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;SNN&#65292;&#24182;&#19988;&#29978;&#33267;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#19982;BERT&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) offer a promising avenue to implement deep neural networks in a more energy-efficient way. However, the network architectures of existing SNNs for language tasks are too simplistic, and deep architectures have not been fully explored, resulting in a significant performance gap compared to mainstream transformer-based networks such as BERT. To this end, we improve a recently-proposed spiking transformer (i.e., Spikformer) to make it possible to process language tasks and propose a two-stage knowledge distillation method for training it, which combines pre-training by distilling knowledge from BERT with a large collection of unlabelled texts and fine-tuning with task-specific instances via knowledge distillation again from the BERT fine-tuned on the same training examples. Through extensive experimentation, we show that the models trained with our method, named SpikeBERT, outperform state-of-the-art SNNs and even achieve comparable results to BERTs on text 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#26500;&#24314;&#36866;&#24212;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#25991;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;&#36827;&#34892;&#30340;&#24037;&#31243;&#24037;&#20316;&#65292;&#21033;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#21644;&#25991;&#26412;&#23545;&#35805;&#31995;&#32479;&#23454;&#29616;&#20102;&#25554;&#27133;&#21644;&#20540;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.15053</link><description>&lt;p&gt;
&#36866;&#24212;&#21475;&#35821;&#23545;&#35805;&#30340;&#25991;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;
&lt;/p&gt;
&lt;p&gt;
Adapting text-based dialogue state tracker for spoken dialogues. (arXiv:2308.15053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#26500;&#24314;&#36866;&#24212;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#25991;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;&#36827;&#34892;&#30340;&#24037;&#31243;&#24037;&#20316;&#65292;&#21033;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#21644;&#25991;&#26412;&#23545;&#35805;&#31995;&#32479;&#23454;&#29616;&#20102;&#25554;&#27133;&#21644;&#20540;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36890;&#36807;&#23545;&#35805;&#31995;&#32479;&#25216;&#26415;&#31454;&#36187;&#65288;DSTC&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26500;&#24314;&#19968;&#20010;&#20855;&#26377;&#35821;&#38899;&#30028;&#38754;&#30340;&#31283;&#20581;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22823;&#37096;&#20998;&#36827;&#23637;&#37117;&#26159;&#38024;&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#22240;&#20026;&#26377;&#20016;&#23500;&#30340;&#20070;&#38754;&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#65292;&#32780;&#20855;&#26377;&#21475;&#35821;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#38750;&#24120;&#31232;&#32570;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;Siri&#21644;Alexa&#31561;&#35821;&#38899;&#21161;&#25163;&#31995;&#32479;&#25152;&#23637;&#31034;&#30340;&#65292;&#23558;&#36825;&#31181;&#25104;&#21151;&#36716;&#31227;&#21040;&#21475;&#35821;&#23545;&#35805;&#20013;&#20855;&#26377;&#23454;&#38469;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;DSTC11&#30340;&#20855;&#26377;&#35821;&#38899;&#24863;&#30693;&#30340;&#23545;&#35805;&#31995;&#32479;&#25216;&#26415;&#25361;&#25112;&#36187;&#20013;&#30340;&#39640;&#24230;&#25104;&#21151;&#27169;&#22411;&#30340;&#24037;&#31243;&#21162;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#65306;&#65288;1&#65289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#65292;&#20197;&#24357;&#21512;&#21475;&#35821;&#21644;&#25991;&#26412;&#35805;&#35821;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#65288;2&#65289;&#29992;&#20110;&#20272;&#35745;&#25554;&#27133;&#21644;&#20540;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#23545;&#35805;&#31995;&#32479;&#65288;D3ST&#65289;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#25554;&#27133;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although there have been remarkable advances in dialogue systems through the dialogue systems technology competition (DSTC), it remains one of the key challenges to building a robust task-oriented dialogue system with a speech interface. Most of the progress has been made for text-based dialogue systems since there are abundant datasets with written corpora while those with spoken dialogues are very scarce. However, as can be seen from voice assistant systems such as Siri and Alexa, it is of practical importance to transfer the success to spoken dialogues. In this paper, we describe our engineering effort in building a highly successful model that participated in the speech-aware dialogue systems technology challenge track in DSTC11. Our model consists of three major modules: (1) automatic speech recognition error correction to bridge the gap between the spoken and the text utterances, (2) text-based dialogue system (D3ST) for estimating the slots and values using slot descriptions, an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#27880;&#24847;&#21147;&#21644;&#33258;&#30417;&#30563;&#35821;&#38899;&#23884;&#20837;&#23545;&#38750;&#35821;&#20041;&#35821;&#38899;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#24773;&#24863;&#29702;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#36731;&#37327;&#32423;HuBERT-Large&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2308.14359</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#21644;&#33258;&#30417;&#30563;&#35821;&#38899;&#23884;&#20837;&#23545;&#38750;&#35821;&#20041;&#35821;&#38899;&#20219;&#21153;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effect of Attention and Self-Supervised Speech Embeddings on Non-Semantic Speech Tasks. (arXiv:2308.14359v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#27880;&#24847;&#21147;&#21644;&#33258;&#30417;&#30563;&#35821;&#38899;&#23884;&#20837;&#23545;&#38750;&#35821;&#20041;&#35821;&#38899;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#24773;&#24863;&#29702;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#36731;&#37327;&#32423;HuBERT-Large&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#20013;&#65292;&#20154;&#31867;&#24773;&#24863;&#29702;&#35299;&#22312;&#20351;&#23545;&#35805;&#25216;&#26415;&#25104;&#20026;&#20027;&#27969;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23558;&#35821;&#38899;&#24773;&#24863;&#29702;&#35299;&#35270;&#20026;&#19968;&#31181;&#30693;&#35273;&#20219;&#21153;&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#29616;&#23454;&#30340;&#24773;&#26223;&#12290;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#65288;&#35821;&#35328;&#65292;&#20154;&#21475;&#32479;&#35745;&#23398;&#31561;&#65289;&#65292;&#19981;&#21516;&#27604;&#20363;&#30340;&#20154;&#20250;&#23558;&#30456;&#21516;&#30340;&#35821;&#38899;&#29255;&#27573;&#35270;&#20026;&#38750;&#19968;&#33268;&#30340;&#24773;&#24863;&#12290;&#20316;&#20026;ACM&#22810;&#23186;&#20307;2023&#35745;&#31639;&#35821;&#38899;&#32852;&#26426;&#25361;&#25112;&#65288;ComParE&#65289;&#30340;&#19968;&#37096;&#20998;&#65292;&#22312;EMotion Share&#36712;&#36947;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#20182;&#20204;&#20016;&#23500;&#30340;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#21644;&#22810;&#26631;&#31614;&#22238;&#24402;&#30446;&#26631;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;&#8220;&#24773;&#24863;&#20998;&#20139;&#8221;&#25110;&#23545;&#35813;&#24773;&#24863;&#30340;&#24863;&#30693;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21516;&#22522;&#30784;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#26696;&#20915;&#23450;&#20102;&#23427;&#20204;&#22312;&#36229;&#36234;&#35821;&#38899;&#35782;&#21035;&#30340;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24773;&#24863;&#29702;&#35299;&#31561;&#38750;&#35821;&#20041;&#35821;&#38899;&#20219;&#21153;&#12290;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#28041;&#21450;&#21040;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#65292;&#30446;&#26631;&#26631;&#31614;&#30340;&#21464;&#21270;&#20197;&#21450;&#22238;&#24402;&#25968;&#25454;&#38598;&#20013;&#30340;&#22266;&#26377;&#19981;&#24179;&#34913;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#36731;&#37327;&#32423;HuBERT-Large&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human emotion understanding is pivotal in making conversational technology mainstream. We view speech emotion understanding as a perception task which is a more realistic setting. With varying contexts (languages, demographics, etc.) different share of people perceive the same speech segment as a non-unanimous emotion. As part of the ACM Multimedia 2023 Computational Paralinguistics ChallengE (ComParE) in the EMotion Share track, we leverage their rich dataset of multilingual speakers and multi-label regression target of 'emotion share' or perception of that emotion. We demonstrate that the training scheme of different foundation models dictates their effectiveness for tasks beyond speech recognition, especially for non-semantic speech tasks like emotion understanding. This is a very complex task due to multilingual speakers, variability in the target labels, and inherent imbalance in the regression dataset. Our results show that HuBERT-Large with a self-attention-based light-weight se
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#25552;&#21319;&#20013;&#31561;&#35268;&#27169;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27169;&#22411;&#23545;&#38476;&#29983;&#25351;&#20196;&#30340;&#22788;&#29702;&#33021;&#21147;&#26377;&#24453;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.14306</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Robustness to Instructions of Large Language Models. (arXiv:2308.14306v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#25552;&#21319;&#20013;&#31561;&#35268;&#27169;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27169;&#22411;&#23545;&#38476;&#29983;&#25351;&#20196;&#30340;&#22788;&#29702;&#33021;&#21147;&#26377;&#24453;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25351;&#20196;&#24494;&#35843;&#24050;&#25104;&#20026;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26032;&#20219;&#21153;&#20013;&#38646;-shot&#33021;&#21147;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;&#35813;&#25216;&#26415;&#26174;&#31034;&#20986;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#25552;&#21319;&#20013;&#31561;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#29978;&#33267;&#36798;&#21040;&#30456;&#24403;&#20110;&#26356;&#22823;&#27169;&#22411;&#21464;&#20307;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#30740;&#31350;&#20102;&#32463;&#36807;&#25351;&#20196;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#24050;&#30693;&#20219;&#21153;&#21644;&#26410;&#30693;&#20219;&#21153;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23545;&#20845;&#20010;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#21253;&#25324;Alpaca&#12289;Vicuna&#12289;WizardLM&#21644;&#20256;&#32479;&#30340;&#20219;&#21153;&#23548;&#21521;&#27169;&#22411;&#65288;Flan-T5-XL/XXL&#12289;T0++&#65289;&#65292;&#20197;&#30495;&#23454;&#19990;&#30028;&#30340;&#20851;&#31995;&#25552;&#21462;&#25968;&#25454;&#38598;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#36981;&#24490;&#25351;&#20196;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#22522;&#20110;&#24320;&#25918;&#22495;&#25351;&#20196;&#21644;&#20219;&#21153;&#23548;&#21521;&#25351;&#20196;&#36827;&#34892;&#24494;&#35843;&#30340;&#12290;&#20027;&#35201;&#35752;&#35770;&#30340;&#26159;&#23427;&#20204;&#22312;&#22788;&#29702;&#25351;&#20196;&#26102;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#38476;&#29983;&#25351;&#20196;&#26041;&#38754;&#30340;&#24615;&#33021;&#24448;&#24448;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Instruction fine-tuning has risen to prominence as a potential method for enhancing the zero-shot capabilities of Large Language Models (LLMs) on novel tasks. This technique has shown an exceptional ability to boost the performance of moderately sized LLMs, sometimes even reaching performance levels comparable to those of much larger model variants. The focus is on the robustness of instruction-tuned LLMs to seen and unseen tasks. We conducted an exploration of six models including Alpaca, Vicuna, WizardLM, and Traditional Task-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction datasets as case studies. We carried out a comprehensive evaluation of these instruction-following LLMs which have been tuned based on open-domain instructions and task-oriented instructions. The main discussion is their performance and robustness towards instructions. We have observed that in most cases, the model's performance in dealing with unfamiliar instructions tends to w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#20013;&#25552;&#21462;&#20855;&#26377;&#26368;&#39640;&#26465;&#20214;&#29109;&#30340;&#30701;&#35821;&#20316;&#20026;&#20851;&#38190;&#35789;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20851;&#38190;&#35789;&#25552;&#21462;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#19982;&#24120;&#29992;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13399</link><description>&lt;p&gt;
EntropyRank: &#36890;&#36807;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#21387;&#32553;&#30340;&#21103;&#20449;&#24687;&#20248;&#21270;&#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#20851;&#38190;&#35789;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression. (arXiv:2308.13399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#20013;&#25552;&#21462;&#20855;&#26377;&#26368;&#39640;&#26465;&#20214;&#29109;&#30340;&#30701;&#35821;&#20316;&#20026;&#20851;&#38190;&#35789;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20851;&#38190;&#35789;&#25552;&#21462;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#19982;&#24120;&#29992;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#21644;Shannon&#30340;&#20449;&#24687;&#26368;&#22823;&#21270;&#65292;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#38190;&#35789;&#21644;&#20851;&#38190;&#35789;&#30701;&#35821;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21462;&#22312;LM&#19979;&#20855;&#26377;&#26368;&#39640;&#26465;&#20214;&#29109;&#30340;&#30701;&#35821;&#12290;&#24471;&#21040;&#30340;&#20851;&#38190;&#35789;&#30701;&#35821;&#38598;&#21512;&#35299;&#20915;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#20449;&#24687;&#35770;&#38382;&#39064;&#65306;&#22914;&#26524;&#20316;&#20026;&#21103;&#20449;&#24687;&#25552;&#20379;&#65292;&#23427;&#20250;&#23548;&#33268;&#20351;&#29992;LM&#21644;&#29109;&#32534;&#30721;&#22120;&#23545;&#25991;&#26412;&#36827;&#34892;&#21387;&#32553;&#26102;&#30340;&#39044;&#26399;&#26368;&#23567;&#20108;&#36827;&#21046;&#30721;&#38271;&#24230;&#12290;&#21478;&#22806;&#65292;&#24471;&#21040;&#30340;&#38598;&#21512;&#26159;&#36890;&#36807;&#22240;&#26524;LM&#23545;&#22312;&#32473;&#23450;&#26465;&#20214;&#19979;&#26368;&#23567;&#21270;&#25991;&#26412;&#29109;&#30340;&#30701;&#35821;&#38598;&#21512;&#30340;&#36817;&#20284;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20851;&#38190;&#35789;&#25552;&#21462;&#22522;&#20934;&#25361;&#25112;&#20013;&#25552;&#20379;&#20102;&#19982;&#26368;&#24120;&#29992;&#26041;&#27861;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an unsupervised method to extract keywords and keyphrases from texts based on a pre-trained language model (LM) and Shannon's information maximization. Specifically, our method extracts phrases having the highest conditional entropy under the LM. The resulting set of keyphrases turns out to solve a relevant information-theoretic problem: if provided as side information, it leads to the expected minimal binary code length in compressing the text using the LM and an entropy encoder. Alternately, the resulting set is an approximation via a causal LM to the set of phrases that minimize the entropy of the text when conditioned upon it. Empirically, the method provides results comparable to the most commonly used methods in various keyphrase extraction benchmark challenges.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20154;&#21475;&#23398;&#39046;&#22495;&#30340;Agent Based Models (ABMs)&#30340;&#25968;&#23398;&#35268;&#33539;&#30340;&#21512;&#36866;&#24418;&#24335;&#26415;&#35821;&#65292;&#36825;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#29702;&#35299;&#65292;&#24182;&#19982;O.D.D.&#21327;&#35758;&#30456;&#32467;&#21512;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#22797;&#21046;&#36807;&#31243;&#20013;&#30340;&#27495;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.13081</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#21475;&#29305;&#24449;&#30340;&#22266;&#23450;&#27493;&#38271;&#21333;&#26102;&#38047;&#27169;&#25311;&#30340;Agent-Based&#27169;&#22411;&#30340;&#24418;&#24335;&#21270;&#35268;&#33539;&#26415;&#35821;
&lt;/p&gt;
&lt;p&gt;
Formal specification terminology for demographic agent-based models of fixed-step single-clocked simulations. (arXiv:2308.13081v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20154;&#21475;&#23398;&#39046;&#22495;&#30340;Agent Based Models (ABMs)&#30340;&#25968;&#23398;&#35268;&#33539;&#30340;&#21512;&#36866;&#24418;&#24335;&#26415;&#35821;&#65292;&#36825;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#29702;&#35299;&#65292;&#24182;&#19982;O.D.D.&#21327;&#35758;&#30456;&#32467;&#21512;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#22797;&#21046;&#36807;&#31243;&#20013;&#30340;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20154;&#21475;&#23398;&#39046;&#22495;&#30340;Agent Based Models (ABMs)&#30340;&#25968;&#23398;&#35268;&#33539;&#30340;&#21512;&#36866;&#24418;&#24335;&#26415;&#35821;&#12290;&#30446;&#26631;ABMs&#30340;&#27169;&#25311;&#36981;&#24490;&#22266;&#23450;&#27493;&#38271;&#21333;&#26102;&#38047;&#27169;&#24335;&#12290;&#25152;&#25552;&#20986;&#30340;&#26415;&#35821;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#29702;&#35299;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#29420;&#31435;&#30340;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#35268;&#33539;&#21644;&#36873;&#25321;&#24615;&#22320;&#35760;&#24405;&#19968;&#32452;&#37325;&#35201;&#30340;&#65288;&#20154;&#21475;&#65289;ABMs&#12290;&#28982;&#32780;&#65292;&#21487;&#20197;&#24819;&#35937;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#23637;&#65292;&#36825;&#31181;&#26415;&#35821;&#21487;&#33021;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#25991;&#26723;&#21644;&#36890;&#20449;O.D.D.&#21327;&#35758;[Grimm&#21644;et al.&#65292;2020&#65292;Amouroux&#31561;&#65292;2010]&#21512;&#24182;&#65292;&#20197;&#20943;&#23569;&#35768;&#22810;&#27169;&#22411;&#24314;&#27169;&#32773;&#30340;&#28304;&#28304;&#19981;&#26029;&#20135;&#29983;&#30340;&#27495;&#20041;&#65292;&#20174;&#32780;&#38459;&#30861;&#27169;&#22411;&#22797;&#21046;&#12290;&#24050;&#32463;&#20986;&#29256;&#30340;&#20154;&#21475;&#27169;&#22411;&#25991;&#26723;&#65292;&#21333;&#20146;&#27169;&#22411;&#30340;&#22823;&#22823;&#31616;&#21270;&#29256;&#26412;[Gostoli&#21644;Silverman&#65292;2020]&#20316;&#20026;&#24418;&#24335;&#26415;&#35821;&#30340;&#31034;&#20363;&#65292;&#21333;&#29420;&#21457;&#24067;&#22312;[Elsheikh&#65292;2023b]&#20013;&#12290;&#35813;&#27169;&#22411;&#24050;&#34987;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This document presents adequate formal terminology for the mathematical specification of a subset of Agent Based Models (ABMs) in the field of Demography. The simulation of the targeted ABMs follows a fixed-step single-clocked pattern. The proposed terminology further improves the model understanding and can act as a stand-alone methodology for the specification and optionally the documentation of a significant set of (demographic) ABMs. Nevertheless, it is imaginable the this terminology probably with further extensions can be merged with the largely-informal widely-used model documentation and communication O.D.D. protocol [Grimm and et al., 2020, Amouroux et al., 2010] to reduce many sources of ambiguity, hindering model replications by other modelers. A published demographic model documentation, largely simplified version of the Lone Parent Model [Gostoli and Silverman, 2020] is separately published in [Elsheikh, 2023b] as illustration for the formal terminology. The model was impl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LibriSQA&#65292;&#19968;&#20010;&#33258;&#30001;&#24418;&#24335;&#21644;&#24320;&#25918;&#24335;&#30340;&#21475;&#35821;&#38382;&#31572;&#25968;&#25454;&#38598;&#21644;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;ASR&#20219;&#21153;&#24182;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;LLMs&#19978;&#25191;&#34892;&#21475;&#35821;&#38382;&#31572;&#20219;&#21153;&#30340;&#26174;&#33879;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.10390</link><description>&lt;p&gt;
LibriSQA&#65306;&#36890;&#36807;&#26032;&#22411;&#25968;&#25454;&#38598;&#21644;&#26694;&#26550;&#25512;&#36827;&#33258;&#30001;&#24418;&#24335;&#21644;&#24320;&#25918;&#24335;&#30340;&#21475;&#35821;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
LibriSQA: Advancing Free-form and Open-ended Spoken Question Answering with a Novel Dataset and Framework. (arXiv:2308.10390v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LibriSQA&#65292;&#19968;&#20010;&#33258;&#30001;&#24418;&#24335;&#21644;&#24320;&#25918;&#24335;&#30340;&#21475;&#35821;&#38382;&#31572;&#25968;&#25454;&#38598;&#21644;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;ASR&#20219;&#21153;&#24182;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;LLMs&#19978;&#25191;&#34892;&#21475;&#35821;&#38382;&#31572;&#20219;&#21153;&#30340;&#26174;&#33879;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21487;&#31216;&#36190;&#30340;&#24615;&#33021;&#65292;&#20294;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#27169;&#24577;&#21151;&#33021;&#26041;&#38754;&#20173;&#23384;&#22312;&#26126;&#26174;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38656;&#35201;&#35821;&#38899;&#21644;&#25991;&#26412;&#29305;&#24449;&#20043;&#38388;&#31934;&#30830;&#23545;&#40784;&#21644;&#28145;&#24230;&#20132;&#20114;&#30340;&#21475;&#35821;&#38382;&#31572;&#65288;SQA&#65289;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;LLM&#19978;&#30340;SQA&#25361;&#25112;&#65292;&#25105;&#20204;&#20174;Librispeech&#21019;&#36896;&#20102;&#33258;&#30001;&#24418;&#24335;&#21644;&#24320;&#25918;&#24335;&#30340;LibriSQA&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#33258;&#28982;&#23545;&#35805;&#26684;&#24335;&#30340;&#31532;&#19968;&#37096;&#20998;&#21644;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#39064;&#21644;&#31572;&#26696;&#20197;&#21450;&#20998;&#26512;&#29255;&#27573;&#30340;&#31532;&#20108;&#37096;&#20998;&#12290;&#36825;&#20004;&#37096;&#20998;&#20849;&#21253;&#21547;107k&#20010;&#28085;&#30422;&#21508;&#31181;&#20027;&#39064;&#30340;SQA&#23545;&#12290;&#37492;&#20110;&#29616;&#26377;&#35821;&#38899;-&#25991;&#26412;LLM&#30340;&#26126;&#26174;&#21294;&#20047;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#22312;LibriSQA&#19978;&#25191;&#34892;SQA&#20219;&#21153;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#23558;ASR&#25913;&#20026;SQA&#26684;&#24335;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#25105;&#20204;&#26694;&#26550;&#22788;&#29702;ASR&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have demonstrated commendable performance across a myriad of domains and tasks, existing LLMs still exhibit a palpable deficit in handling multimodal functionalities, especially for the Spoken Question Answering (SQA) task which necessitates precise alignment and deep interaction between speech and text features. To address the SQA challenge on LLMs, we initially curated the free-form and open-ended LibriSQA dataset from Librispeech, comprising Part I with natural conversational formats and Part II encompassing multiple-choice questions followed by answers and analytical segments. Both parts collectively include 107k SQA pairs that cover various topics. Given the evident paucity of existing speech-text LLMs, we propose a lightweight, end-to-end framework to execute the SQA task on the LibriSQA, witnessing significant results. By reforming ASR into the SQA format, we further substantiate our framework's capability in handling ASR tasks. Our empirical f
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20197;&#32418;&#38431;&#35780;&#20272;&#30340;&#26041;&#24335;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#35780;&#20272;&#65292;&#21457;&#29616;&#21363;&#20415;&#26159;&#24191;&#27867;&#37096;&#32626;&#30340;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#36830;&#32493;&#21457;&#35328;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#36829;&#21453;&#20262;&#29702;&#22320;&#23545;&#26377;&#23475;&#26597;&#35810;&#20570;&#20986;&#22238;&#24212;&#12290;&#36890;&#36807;&#32418;&#38431;&#35780;&#20272;&#23581;&#35797;&#65292;&#21457;&#29616;&#22810;&#25968;&#24320;&#28304;LLM&#20063;&#20250;&#29983;&#25104;&#26377;&#23475;&#22238;&#24212;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;LLM&#23433;&#20840;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.09662</link><description>&lt;p&gt;
&#20351;&#29992;&#36830;&#32493;&#21457;&#35328;&#30340;&#26041;&#24335;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32418;&#38431;&#35780;&#20272;&#20197;&#23454;&#29616;&#23433;&#20840;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment. (arXiv:2308.09662v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09662
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20197;&#32418;&#38431;&#35780;&#20272;&#30340;&#26041;&#24335;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#35780;&#20272;&#65292;&#21457;&#29616;&#21363;&#20415;&#26159;&#24191;&#27867;&#37096;&#32626;&#30340;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#36830;&#32493;&#21457;&#35328;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#36829;&#21453;&#20262;&#29702;&#22320;&#23545;&#26377;&#23475;&#26597;&#35810;&#20570;&#20986;&#22238;&#24212;&#12290;&#36890;&#36807;&#32418;&#38431;&#35780;&#20272;&#23581;&#35797;&#65292;&#21457;&#29616;&#22810;&#25968;&#24320;&#28304;LLM&#20063;&#20250;&#29983;&#25104;&#26377;&#23475;&#22238;&#24212;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;LLM&#23433;&#20840;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#20248;&#21270;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#30446;&#26631;&#65292;&#20197;&#20854;&#24040;&#22823;&#30340;&#22810;&#20219;&#21153;&#33021;&#21147;&#38663;&#25788;&#19990;&#30028;&#12290;&#38543;&#30528;&#23427;&#20204;&#30340;&#23646;&#24615;&#21644;&#32534;&#30721;&#30693;&#35782;&#30340;&#20986;&#29616;&#65292;LLMs&#20135;&#29983;&#26377;&#23475;&#36755;&#20986;&#30340;&#39118;&#38505;&#22686;&#21152;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#21512;&#21487;&#25193;&#23637;&#22320;&#37096;&#32626;&#32473;&#20844;&#20247;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23433;&#20840;&#35780;&#20272;&#22522;&#20934;RED-EVAL&#65292;&#36827;&#34892;&#32418;&#38431;&#35780;&#20272;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20415;&#26159;&#24191;&#27867;&#37096;&#32626;&#30340;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#22522;&#20110;&#36830;&#32493;&#21457;&#35328;&#30340;(CoU)&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#20351;&#22522;&#20110;GPT-4&#21644;ChatGPT&#30340;&#38381;&#28304;LLM&#31995;&#32479;&#36829;&#21453;&#20262;&#29702;&#22320;&#23545;&#36229;&#36807;65%&#21644;73%&#30340;&#26377;&#23475;&#26597;&#35810;&#20570;&#20986;&#22238;&#24212;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;RED-EVAL&#22312;8&#20010;&#24320;&#28304;LLM&#20013;&#30340;&#19968;&#33268;&#24615;&#65292;&#36890;&#36807;&#32418;&#38431;&#35780;&#20272;&#23581;&#35797;&#29983;&#25104;86%&#20197;&#19978;&#30340;&#26377;&#23475;&#22238;&#24212;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RED-INSTRUCT--&#19968;&#31181;&#29992;&#20110;LLM&#23433;&#20840;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;1&#65289;HARMFULQA&#25968;&#25454;&#25910;&#38598;&#65306;&#21033;&#29992;CoU&#25552;&#31034;,
&lt;/p&gt;
&lt;p&gt;
Larger language models (LLMs) have taken the world by storm with their massive multi-tasking capabilities simply by optimizing over a next-word prediction objective. With the emergence of their properties and encoded knowledge, the risk of LLMs producing harmful outputs increases, making them unfit for scalable deployment for the public. In this work, we propose a new safety evaluation benchmark RED-EVAL that carries out red-teaming. We show that even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of harmful queries. We also demonstrate the consistency of the RED-EVAL across 8 open-source LLMs in generating harmful responses in more than 86% of the red-teaming attempts. Next, we propose RED-INSTRUCT--An approach for the safety alignment of LLMs. It constitutes two phases: 1) HARMFULQA data collection: Leveraging CoU prompting, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21028;&#26029;&#36829;&#27861;&#34892;&#20026;&#65292;&#24182;&#27604;&#36739;&#20102;&#30001;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#23545;&#38506;&#23457;&#22242;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#36739;&#24369;&#65292;&#20294;&#38543;&#30528;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#20854;&#28508;&#21147;&#26377;&#26395;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.06032</link><description>&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;ChatGPT&#33021;&#21542;&#21462;&#20195;&#24459;&#24072;&#65311;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?. (arXiv:2308.06032v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21028;&#26029;&#36829;&#27861;&#34892;&#20026;&#65292;&#24182;&#27604;&#36739;&#20102;&#30001;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#23545;&#38506;&#23457;&#22242;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#36739;&#24369;&#65292;&#20294;&#38543;&#30528;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#20854;&#28508;&#21147;&#26377;&#26395;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#22686;&#24378;&#23545;&#27861;&#24459;&#31995;&#32479;&#30340;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#22312;&#36827;&#34892;&#27861;&#24459;&#20219;&#21153;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#30740;&#31350;&#28041;&#21450;&#21152;&#23494;&#36135;&#24065;&#30340;&#35777;&#21048;&#26696;&#20214;&#65292;&#20316;&#20026;AI&#21487;&#20197;&#25903;&#25345;&#27861;&#24459;&#36807;&#31243;&#30340;&#20247;&#22810;&#24773;&#22659;&#20043;&#19968;&#65292;&#30740;&#31350;LLMs&#30340;&#27861;&#24459;&#25512;&#29702;&#21644;&#36215;&#33609;&#33021;&#21147;&#12290;&#25105;&#20204;&#26816;&#26597;&#20197;&#19979;&#20004;&#20010;&#26041;&#38754;&#65306;a&#65289;LLM&#33021;&#21542;&#20934;&#30830;&#30830;&#23450;&#20107;&#23454;&#27169;&#24335;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36829;&#27861;&#34892;&#20026;&#65292;b&#65289;&#22522;&#20110;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#65292;&#38506;&#23457;&#22242;&#30340;&#20915;&#31574;&#26159;&#21542;&#26377;&#25152;&#24046;&#24322;&#12290;&#25105;&#20204;&#23558;&#30495;&#23454;&#26696;&#20363;&#20013;&#30340;&#20107;&#23454;&#27169;&#24335;&#36755;&#20837;GPT-3.5&#65292;&#24182;&#35780;&#20272;&#20854;&#30830;&#23450;&#27491;&#30830;&#28508;&#22312;&#36829;&#27861;&#34892;&#20026;&#24182;&#25490;&#38500;&#34394;&#20551;&#36829;&#27861;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35831;&#27169;&#25311;&#38506;&#23457;&#21592;&#35780;&#20272;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#12290;GPT-3.5&#30340;&#27861;&#24459;&#25512;&#29702;&#33021;&#21147;&#36739;&#24369;&#65292;&#20294;&#25105;&#20204;&#39044;&#26399;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#23427;&#24314;&#35758;&#30340;&#36829;&#27861;&#34892;&#20026;&#24448;&#24448;&#26159;&#27491;&#30830;&#30340;&#65288;&#23427;&#20165;&#20165;&#36807;&#20110;&#20445;&#23432;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) could enhance access to the legal system. However, empirical research on their effectiveness in conducting legal tasks is scant. We study securities cases involving cryptocurrencies as one of numerous contexts where AI could support the legal process, studying LLMs' legal reasoning and drafting capabilities. We examine whether a) an LLM can accurately determine which laws are potentially being violated from a fact pattern, and b) whether there is a difference in juror decision-making based on complaints written by a lawyer compared to an LLM. We feed fact patterns from real-life cases to GPT-3.5 and evaluate its ability to determine correct potential violations from the scenario and exclude spurious violations. Second, we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely m
&lt;/p&gt;</description></item><item><title>WeaverBird&#26159;&#19968;&#20010;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;&#26234;&#33021;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#26412;&#22320;&#30693;&#35782;&#24211;&#21644;&#25628;&#32034;&#24341;&#25806;&#65292;&#33021;&#22815;&#29702;&#35299;&#22797;&#26434;&#30340;&#37329;&#34701;&#26597;&#35810;&#24182;&#25552;&#20379;&#26126;&#26234;&#30340;&#22238;&#31572;&#65292;&#20855;&#26377;&#22686;&#24378;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.05361</link><description>&lt;p&gt;
WeaverBird: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#30693;&#35782;&#24211;&#21644;&#25628;&#32034;&#24341;&#25806;&#22686;&#24378;&#37329;&#34701;&#20915;&#31574;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
WeaverBird: Empowering Financial Decision-Making with Large Language Model, Knowledge Base, and Search Engine. (arXiv:2308.05361v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05361
&lt;/p&gt;
&lt;p&gt;
WeaverBird&#26159;&#19968;&#20010;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;&#26234;&#33021;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#26412;&#22320;&#30693;&#35782;&#24211;&#21644;&#25628;&#32034;&#24341;&#25806;&#65292;&#33021;&#22815;&#29702;&#35299;&#22797;&#26434;&#30340;&#37329;&#34701;&#26597;&#35810;&#24182;&#25552;&#20379;&#26126;&#26234;&#30340;&#22238;&#31572;&#65292;&#20855;&#26377;&#22686;&#24378;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;WeaverBird&#65292;&#19968;&#20010;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;&#26234;&#33021;&#23545;&#35805;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21033;&#29992;GPT&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#37329;&#34701;&#30456;&#20851;&#25991;&#26412;&#30340;&#24191;&#27867;&#35821;&#26009;&#23545;&#20854;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#29702;&#35299;&#22797;&#26434;&#30340;&#37329;&#34701;&#26597;&#35810;&#65292;&#20363;&#22914;&#8220;&#22312;&#36890;&#36135;&#33192;&#32960;&#26399;&#38388;&#22914;&#20309;&#31649;&#29702;&#25105;&#30340;&#25237;&#36164;&#65311;&#8221;&#24182;&#25552;&#20379;&#26126;&#26234;&#30340;&#22238;&#31572;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#36824;&#38598;&#25104;&#20102;&#26412;&#22320;&#30340;&#30693;&#35782;&#24211;&#21644;&#25628;&#32034;&#24341;&#25806;&#20197;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#12290;&#26368;&#32456;&#30340;&#22238;&#31572;&#26159;&#22522;&#20110;&#25628;&#32034;&#32467;&#26524;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#30340;&#65292;&#24182;&#21253;&#21547;&#36866;&#24403;&#30340;&#24341;&#29992;&#26469;&#28304;&#65292;&#20174;&#32780;&#20855;&#26377;&#22686;&#24378;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#19982;&#37329;&#34701;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24050;&#32463;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#30456;&#27604;&#20854;&#20182;&#27169;&#22411;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#29992;&#25143;&#21487;&#20197;&#22312;&#25105;&#20204;&#30340;&#22312;&#32447;&#28436;&#31034;&#32593;&#31449;https://weaverbird.ttic.edu&#19982;&#25105;&#20204;&#30340;&#31995;&#32479;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#35266;&#30475;&#25105;&#20204;&#30340;2&#20998;&#38047;&#28436;&#31034;&#35270;&#39057;https://www.youtube.com/watch?v=yofgeq&#12290;
&lt;/p&gt;
&lt;p&gt;
We present WeaverBird, an intelligent dialogue system designed specifically for the finance domain. Our system harnesses a large language model of GPT architecture that has been tuned using extensive corpora of finance-related text. As a result, our system possesses the capability to understand complex financial queries, such as "How should I manage my investments during inflation?", and provide informed responses. Furthermore, our system incorporates a local knowledge base and a search engine to retrieve relevant information. The final responses are conditioned on the search results and include proper citations to the sources, thus enjoying an enhanced credibility. Through a range of finance-related questions, we have demonstrated the superior performance of our system compared to other models. To experience our system firsthand, users can interact with our live demo at https://weaverbird.ttic.edu, as well as watch our 2-min video illustration at https://www.youtube.com/watch?v=yofgeq
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#23545;&#22810;&#26679;&#21270;&#30340;&#33258;&#25105;&#32416;&#27491;&#31574;&#30053;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#34892;&#20026;&#12290;&#33258;&#21160;&#21270;&#21453;&#39304;&#25216;&#26415;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#23454;&#29992;&#21644;&#21487;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2308.03188</link><description>&lt;p&gt;
&#33258;&#21160;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#22810;&#26679;&#21270;&#33258;&#25105;&#32416;&#27491;&#31574;&#30053;&#30340;&#27010;&#36848;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies. (arXiv:2308.03188v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#23545;&#22810;&#26679;&#21270;&#30340;&#33258;&#25105;&#32416;&#27491;&#31574;&#30053;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#34892;&#20026;&#12290;&#33258;&#21160;&#21270;&#21453;&#39304;&#25216;&#26415;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#23454;&#29992;&#21644;&#21487;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21151;&#25928;&#21463;&#21040;&#20102;&#19981;&#21463;&#27426;&#36814;&#21644;&#19981;&#19968;&#33268;&#30340;&#34892;&#20026;&#30340;&#21066;&#24369;&#65292;&#21253;&#25324;&#24187;&#35273;&#12289;&#19981;&#24544;&#23454;&#30340;&#25512;&#29702;&#21644;&#26377;&#23475;&#20869;&#23481;&#12290;&#32416;&#27491;&#36825;&#20123;&#32570;&#38519;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#26159;&#33258;&#25105;&#32416;&#27491;&#65292;&#21363;&#24341;&#23548;&#25110;&#25351;&#23548;LLM&#33258;&#34892;&#20462;&#22797;&#36755;&#20986;&#38382;&#39064;&#12290;&#21033;&#29992;&#33258;&#21160;&#21453;&#39304;&#30340;&#25216;&#26415;--&#26080;&#35770;&#26159;&#30001;LLM&#33258;&#36523;&#20135;&#29983;&#36824;&#26159;&#30001;&#26576;&#20010;&#22806;&#37096;&#31995;&#32479;&#20135;&#29983;--&#23588;&#20854;&#26377;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#20351;&#22522;&#20110;LLM&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#23454;&#38469;&#21644;&#21487;&#37096;&#32626;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#24335;&#65292;&#19988;&#21482;&#38656;&#26368;&#23569;&#30340;&#20154;&#31867;&#21453;&#39304;&#12290;&#26412;&#25991;&#23545;&#36825;&#19968;&#26032;&#20852;&#25216;&#26415;&#31867;&#21035;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20998;&#26512;&#21644;&#20998;&#31867;&#20102;&#35768;&#22810;&#26368;&#36817;&#21033;&#29992;&#36825;&#20123;&#31574;&#30053;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;&#35757;&#32451;&#26102;&#12289;&#29983;&#25104;&#26102;&#21644;&#20107;&#21518;&#32416;&#27491;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#36825;&#19968;&#31574;&#30053;&#30340;&#20027;&#35201;&#24212;&#29992;&#65292;&#24182;&#22312;&#26368;&#21518;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.
&lt;/p&gt;</description></item><item><title>Context-VQA&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#38754;&#28385;&#36275;&#20154;&#20204;&#38656;&#27714;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30340;&#21019;&#26032;&#22312;&#20110;&#23558;&#22270;&#20687;&#19982;&#19981;&#21516;&#19978;&#19979;&#25991;&#37197;&#23545;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#19978;&#19979;&#25991;&#19979;&#38382;&#39064;&#31867;&#22411;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.15745</link><description>&lt;p&gt;
Context-VQA: &#38754;&#21521;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#26377;&#24847;&#20041;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering. (arXiv:2307.15745v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15745
&lt;/p&gt;
&lt;p&gt;
Context-VQA&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#38754;&#28385;&#36275;&#20154;&#20204;&#38656;&#27714;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30340;&#21019;&#26032;&#22312;&#20110;&#23558;&#22270;&#20687;&#19982;&#19981;&#21516;&#19978;&#19979;&#25991;&#37197;&#23545;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#19978;&#19979;&#25991;&#19979;&#38382;&#39064;&#31867;&#22411;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26377;&#28508;&#21147;&#20197;&#19968;&#31181;&#20114;&#21160;&#26041;&#24335;&#20351;&#20114;&#32852;&#32593;&#26356;&#20855;&#21487;&#35775;&#38382;&#24615;&#65292;&#20351;&#19981;&#33021;&#30475;&#21040;&#22270;&#20687;&#30340;&#20154;&#20204;&#33021;&#22815;&#23601;&#22270;&#20687;&#25552;&#20986;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#30450;&#20154;&#25110;&#35270;&#21147;&#20302;&#19979;&#30340;&#20154;&#26356;&#21916;&#27426;&#21253;&#21547;&#22270;&#20687;&#20986;&#29616;&#29615;&#22659;&#30340;&#22270;&#20687;&#35299;&#37322;&#65292;&#32780;&#24403;&#21069;&#30340;VQA&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#23396;&#31435;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#38500;&#38750;&#32771;&#34385;&#19978;&#19979;&#25991;&#65292;&#21542;&#21017;VQA&#27169;&#22411;&#23558;&#26080;&#27861;&#23436;&#20840;&#28385;&#36275;&#20154;&#20204;&#30340;&#38656;&#27714;&#12290;&#20026;&#36827;&#19968;&#27493;&#28608;&#21457;&#21644;&#20998;&#26512;&#19981;&#21516;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Context-VQA&#65292;&#19968;&#31181;&#23558;&#22270;&#20687;&#19982;&#19978;&#19979;&#25991;&#65288;&#22914;&#36141;&#29289;&#32593;&#31449;&#65289;&#37197;&#23545;&#30340;VQA&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#19978;&#19979;&#25991;&#19979;&#30340;&#38382;&#39064;&#31867;&#22411;&#23384;&#22312;&#31995;&#32479;&#24615;&#24046;&#24322;&#12290;&#20363;&#22914;&#65292;&#22312;&#26053;&#34892;&#19978;&#19979;&#25991;&#20013;&#21576;&#29616;&#30340;&#22270;&#20687;&#20135;&#29983;2&#20493;&#20110;&#24179;&#22343;&#25968;&#30340;&#8220;&#22312;&#21738;&#37324;&#65311;&#8221;&#38382;&#39064;&#65292;&#32780;&#31038;&#20132;&#23186;&#20307;&#21644;&#26032;&#38395;&#19978;&#30340;&#22270;&#20687;&#20135;&#29983;&#30340;&#8220;&#35841;&#65311;&#8221;&#38382;&#39064;&#20998;&#21035;&#20026;&#24179;&#22343;&#25968;&#30340;2.8&#20493;&#21644;1.8&#20493;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#19978;&#19979;&#25991;&#23545;&#20110;&#22238;&#31572;&#27491;&#30830;&#30340;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;&#19978;&#19979;&#25991;&#25552;&#20379;&#20102;&#25552;&#31034;&#26102;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;17.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) has the potential to make the Internet more accessible in an interactive way, allowing people who cannot see images to ask questions about them. However, multiple studies have shown that people who are blind or have low-vision prefer image explanations that incorporate the context in which an image appears, yet current VQA datasets focus on images in isolation. We argue that VQA models will not fully succeed at meeting people's needs unless they take context into account. To further motivate and analyze the distinction between different contexts, we introduce Context-VQA, a VQA dataset that pairs images with contexts, specifically types of websites (e.g., a shopping website). We find that the types of questions vary systematically across contexts. For example, images presented in a travel context garner 2 times more "Where?" questions, and images on social media and news garner 2.8 and 1.8 times more "Who?" questions than the average. We also find that c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08303</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#36716;&#21270;&#20026;&#23494;&#38598;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#27979;&#37327;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;DR&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;DR&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20174;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;MS MARCO&#65289;&#20013;&#23398;&#20064;&#65292;&#20294;&#35777;&#25454;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;DR&#27169;&#22411;&#21644;&#39046;&#22495;&#37117;&#33021;&#21516;&#31561;&#21463;&#30410;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;DR&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#37319;&#29992;&#30340;&#30828;&#25552;&#31034;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#24369;&#26597;&#35810;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#24378;DR&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;&#65288;SPTAR&#65289;&#65306;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#22312;&#26377;&#38480;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#25552;&#31034;&#24341;&#23548;LLMs&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#26631;&#35760;&#24369;&#26597;&#35810;&#65292;&#20174;&#32780;&#24471;&#21040;&#36275;&#22815;&#30340;&#24369;&#25991;&#26723;-&#26597;&#35810;&#23545;&#26469;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#22120;&#26102;&#23384;&#22312;&#30340;&#31995;&#32479;&#20559;&#24046;&#65292;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#20505;&#36873;&#21709;&#24212;&#30340;&#39034;&#24207;&#26469;&#25805;&#32437;&#35780;&#20272;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26657;&#20934;&#26694;&#26550;&#65292;&#21253;&#25324;&#22810;&#35777;&#25454;&#26657;&#20934;&#12289;&#22343;&#34913;&#20301;&#32622;&#26657;&#20934;&#21644;&#20154;&#26426;&#21327;&#21516;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2305.17926</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#19981;&#26159;&#20844;&#24179;&#30340;&#35780;&#20272;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are not Fair Evaluators. (arXiv:2305.17926v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#22120;&#26102;&#23384;&#22312;&#30340;&#31995;&#32479;&#20559;&#24046;&#65292;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#20505;&#36873;&#21709;&#24212;&#30340;&#39034;&#24207;&#26469;&#25805;&#32437;&#35780;&#20272;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26657;&#20934;&#26694;&#26550;&#65292;&#21253;&#25324;&#22810;&#35777;&#25454;&#26657;&#20934;&#12289;&#22343;&#34913;&#20301;&#32622;&#26657;&#20934;&#21644;&#20154;&#26426;&#21327;&#21516;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#37319;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65288;&#20363;&#22914;GPT-4&#65289;&#20316;&#20026;&#35009;&#21028;&#26469;&#35780;&#20998;&#21644;&#27604;&#36739;&#20505;&#36873;&#27169;&#22411;&#29983;&#25104;&#30340;&#21709;&#24212;&#36136;&#37327;&#30340;&#35780;&#20272;&#33539;&#24335;&#20013;&#23384;&#22312;&#30340;&#31995;&#32479;&#20559;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#31616;&#21333;&#22320;&#25913;&#21464;&#20505;&#36873;&#21709;&#24212;&#22312;&#19978;&#19979;&#25991;&#20013;&#20986;&#29616;&#30340;&#39034;&#24207;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#25805;&#32437;&#20505;&#36873;&#21709;&#24212;&#30340;&#36136;&#37327;&#25490;&#21517;&#12290;&#36825;&#31181;&#25805;&#32437;&#20351;&#24471;&#19968;&#20010;&#27169;&#22411;&#30475;&#36215;&#26469;&#27604;&#21478;&#19968;&#20010;&#27169;&#22411;&#35201;&#20248;&#36234;&#24471;&#22810;&#65292;&#20363;&#22914;&#65292;&#20351;&#29992;ChatGPT&#20316;&#20026;&#35780;&#20272;&#22120;&#65292;&#22312;80&#20010;&#27979;&#35797;&#26597;&#35810;&#20013;&#65292;Vicuna-13B&#21487;&#20197;&#20987;&#36133;ChatGPT&#30340;66&#20010;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26657;&#20934;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19977;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#65306;1&#65289;&#22810;&#35777;&#25454;&#26657;&#20934;&#65292;&#35201;&#27714;&#35780;&#20272;&#27169;&#22411;&#22312;&#20998;&#37197;&#35780;&#20998;&#20043;&#21069;&#29983;&#25104;&#22810;&#20010;&#35780;&#20272;&#35777;&#25454;&#65307;2&#65289;&#22343;&#34913;&#20301;&#32622;&#26657;&#20934;&#65292;&#22312;&#21508;&#31181;&#39034;&#24207;&#20013;&#32858;&#21512;&#32467;&#26524;&#20197;&#30830;&#23450;&#26368;&#32456;&#20998;&#25968;&#65307;3&#65289;&#20154;&#26426;&#21327;&#21516;&#26657;&#20934;&#65292;&#24341;&#20837;&#24179;&#34913;&#30340;&#20301;&#32622;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-3&#29983;&#25104;&#30340;&#38024;&#23545;&#20167;&#24680;&#20869;&#23481;&#30340;&#35299;&#37322;&#26159;&#21542;&#20934;&#30830;&#21644;&#26377;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3&#29983;&#25104;&#30340;&#35299;&#37322;&#26222;&#36941;&#23384;&#22312;&#36807;&#20110;&#27169;&#31946;&#12289;&#32858;&#28966;&#19981;&#24403;&#31561;&#32570;&#28857;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#20167;&#24680;&#35328;&#35770;&#29983;&#25104;&#30340;&#35299;&#37322;&#36136;&#37327;&#24046;&#24322;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17680</link><description>&lt;p&gt;
&#35780;&#20272;GPT-3&#29983;&#25104;&#30340;&#20167;&#24680;&#20869;&#23481;&#23457;&#26680;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-3 Generated Explanations for Hateful Content Moderation. (arXiv:2305.17680v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-3&#29983;&#25104;&#30340;&#38024;&#23545;&#20167;&#24680;&#20869;&#23481;&#30340;&#35299;&#37322;&#26159;&#21542;&#20934;&#30830;&#21644;&#26377;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3&#29983;&#25104;&#30340;&#35299;&#37322;&#26222;&#36941;&#23384;&#22312;&#36807;&#20110;&#27169;&#31946;&#12289;&#32858;&#28966;&#19981;&#24403;&#31561;&#32570;&#28857;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#20167;&#24680;&#35328;&#35770;&#29983;&#25104;&#30340;&#35299;&#37322;&#36136;&#37327;&#24046;&#24322;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;Fine-tune&#25110;&#25552;&#31034;&#29983;&#25104;&#20167;&#24680;&#35328;&#35770;&#30340;&#35299;&#37322;&#12290;&#23613;&#31649;&#36825;&#20010;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20294;&#36825;&#20123;&#29983;&#25104;&#35299;&#37322;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#22312;&#38480;&#21046;&#20173;&#28982;&#19981;&#20026;&#20154;&#20204;&#25152;&#20102;&#35299;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#65292;&#30001;LLMs&#29983;&#25104;&#30340;&#36825;&#20123;&#35299;&#37322;&#21487;&#33021;&#20250;&#23548;&#33268;&#29992;&#25143;&#21644;&#20869;&#23481;&#23457;&#26680;&#21592;&#23545;&#26631;&#35760;&#20869;&#23481;&#26412;&#36136;&#20570;&#20986;&#38169;&#35823;&#21028;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#26469;&#26816;&#26597;&#20167;&#24680;&#35328;&#35770;&#35299;&#37322;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#35843;&#26597;&#26469;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;GPT-3&#19978;&#36755;&#20837;&#20167;&#24680;&#21644;&#38750;&#20167;&#24680;&#20869;&#23481;&#65292;&#21457;&#29616;&#21463;&#35843;&#26597;&#32773;&#22312;&#20154;&#24037;&#23457;&#26680;GPT&#29983;&#25104;&#30340;&#35299;&#37322;&#26102;&#65292;&#23558;&#20167;&#24680;&#35328;&#35770;&#35299;&#37322;&#35780;&#20215;&#20026;&#19981;&#22815;&#20934;&#30830;&#21644;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has focused on using large language models (LLMs) to generate explanations for hate speech through fine-tuning or prompting. Despite the growing interest in this area, these generated explanations' effectiveness and potential limitations remain poorly understood. A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators. For instance, an LLM-generated explanation might inaccurately convince a content moderator that a benign piece of content is hateful. In light of this, we propose an analytical framework for examining hate speech explanations and conducted an extensive survey on evaluating such explanations. Specifically, we prompted GPT-3 to generate explanations for both hateful and non-hateful content, and a survey was conducted with 2,400 unique respondents to evaluate the generated explanations. Our findings reveal that (1) human evaluators rated the GPT-gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;MPI-rical&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#20195;&#30721;&#29255;&#27573;&#36827;&#34892;&#35757;&#32451;&#23454;&#29616;&#33258;&#21160;&#21270;MPI&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#24182;&#34892;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09438</link><description>&lt;p&gt;
MPI-rical&#65306;&#22522;&#20110;Transformer&#30340;&#25968;&#25454;&#39537;&#21160;MPI&#20998;&#24067;&#24335;&#24182;&#34892;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers. (arXiv:2305.09438v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;MPI-rical&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#20195;&#30721;&#29255;&#27573;&#36827;&#34892;&#35757;&#32451;&#23454;&#29616;&#33258;&#21160;&#21270;MPI&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#24182;&#34892;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#20013;&#65292;&#23558;&#20018;&#34892;&#20195;&#30721;&#33258;&#21160;&#24182;&#34892;&#21270;&#20197;&#25903;&#25345;&#20849;&#20139;&#20869;&#23384;&#21644;&#20998;&#24067;&#24335;&#20869;&#23384;&#31995;&#32479;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#35768;&#22810;&#23581;&#35797;&#23558;&#20018;&#34892;&#20195;&#30721;&#36716;&#25442;&#20026;&#20849;&#20139;&#20869;&#23384;&#29615;&#22659;&#30340;&#24182;&#34892;&#20195;&#30721;&#65288;&#36890;&#24120;&#20351;&#29992;OpenMP&#65289;&#65292;&#20294;&#27809;&#26377;&#20219;&#20309;&#19968;&#39033;&#23581;&#35797;&#25104;&#21151;&#23558;&#20854;&#36716;&#21270;&#20026;&#20998;&#24067;&#24335;&#20869;&#23384;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MPI-rical&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;Transformer&#27169;&#22411;&#23545;&#22823;&#32422;25,000&#20010;&#20018;&#34892;&#20195;&#30721;&#29255;&#27573;&#21450;&#20854;&#23545;&#24212;&#30340;&#24182;&#34892;MPI&#20195;&#30721;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#65288;MPICodeCorpus&#65289;&#30340;50,000&#22810;&#20010;&#20195;&#30721;&#29255;&#27573;&#20013;&#29983;&#25104;&#33258;&#21160;&#21270;MPI&#20195;&#30721;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20018;&#34892;&#20195;&#30721;&#36716;&#25442;&#20026;&#22522;&#20110;MPI&#30340;&#24182;&#34892;&#20195;&#30721;&#32763;&#35793;&#38382;&#39064;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#21046;&#23450;&#20004;&#20010;&#30740;&#31350;&#30446;&#26631;&#65306;&#20195;&#30721;&#34917;&#20840;&#65292;&#21363;&#22312;&#32473;&#23450;&#28304;&#20195;&#30721;&#20013;&#30340;&#26576;&#20010;&#20301;&#32622;&#65292;&#39044;&#27979;&#35813;&#20301;&#32622;&#30340;MPI&#20989;&#25968;&#65307;&#20195;&#30721;&#32763;&#35793;&#65292;&#21363;&#39044;&#27979;&#19968;&#20010;MPI&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic source-to-source parallelization of serial code for shared and distributed memory systems is a challenging task in high-performance computing. While many attempts were made to translate serial code into parallel code for a shared memory environment (usually using OpenMP), none has managed to do so for a distributed memory environment. In this paper, we propose a novel approach, called MPI-rical, for automated MPI code generation using a transformer-based model trained on approximately 25,000 serial code snippets and their corresponding parallelized MPI code out of more than 50,000 code snippets in our corpus (MPICodeCorpus). To evaluate the performance of the model, we first break down the serial code to MPI-based parallel code translation problem into two sub-problems and develop two research objectives: code completion defined as given a location in the source code, predict the MPI function for that location, and code translation defined as predicting an MPI function as wel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;GENRE&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#20041;&#30693;&#35782;&#20016;&#23500;&#26032;&#38395;&#25968;&#25454;&#65292;&#36890;&#36807;&#20174;&#27169;&#22411;&#35774;&#35745;&#36716;&#31227;&#21040;&#25552;&#31034;&#35774;&#35745;&#25552;&#20379;&#28789;&#27963;&#32780;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#26032;&#38395;&#29983;&#25104;&#12289;&#29992;&#25143;&#30011;&#20687;&#21644;&#26032;&#38395;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.06566</link><description>&lt;p&gt;
LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#21021;&#25506;
&lt;/p&gt;
&lt;p&gt;
A First Look at LLM-Powered Generative News Recommendation. (arXiv:2305.06566v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;GENRE&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#20041;&#30693;&#35782;&#20016;&#23500;&#26032;&#38395;&#25968;&#25454;&#65292;&#36890;&#36807;&#20174;&#27169;&#22411;&#35774;&#35745;&#36716;&#31227;&#21040;&#25552;&#31034;&#35774;&#35745;&#25552;&#20379;&#28789;&#27963;&#32780;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#26032;&#38395;&#29983;&#25104;&#12289;&#29992;&#25143;&#30011;&#20687;&#21644;&#26032;&#38395;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#20026;&#29992;&#25143;&#27983;&#35272;&#28023;&#37327;&#22312;&#32447;&#26032;&#38395;&#20869;&#23481;&#25152;&#24517;&#38656;&#30340;&#24037;&#20855;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30528;&#20919;&#21551;&#21160;&#38382;&#39064;&#12289;&#29992;&#25143;&#30011;&#20687;&#24314;&#27169;&#21644;&#26032;&#38395;&#20869;&#23481;&#29702;&#35299;&#31561;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#27169;&#22411;&#35774;&#35745;&#36981;&#24490;&#19968;&#31181;&#19981;&#28789;&#27963;&#30340;&#20363;&#34892;&#31243;&#24207;&#26469;&#35299;&#20915;&#29305;&#23450;&#30340;&#25361;&#25112;&#65292;&#20294;&#22312;&#29702;&#35299;&#26032;&#38395;&#20869;&#23481;&#21644;&#25429;&#25417;&#29992;&#25143;&#20852;&#36259;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GENRE&#65292;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#20041;&#30693;&#35782;&#26469;&#20016;&#23500;&#26032;&#38395;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20174;&#27169;&#22411;&#35774;&#35745;&#36716;&#31227;&#21040;&#25552;&#31034;&#35774;&#35745;&#26469;&#25552;&#20379;&#19968;&#31181;&#28789;&#27963;&#32780;&#32479;&#19968;&#30340;&#26032;&#38395;&#25512;&#33616;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GENRE&#22312;&#20010;&#24615;&#21270;&#26032;&#38395;&#29983;&#25104;&#12289;&#29992;&#25143;&#30011;&#20687;&#21644;&#26032;&#38395;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;&#12290;&#20351;&#29992;&#21508;&#31181;&#27969;&#34892;&#30340;&#25512;&#33616;&#27169;&#22411;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;GENRE&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized news recommendation systems have become essential tools for users to navigate the vast amount of online news content, yet existing news recommenders face significant challenges such as the cold-start problem, user profile modeling, and news content understanding. Previous works have typically followed an inflexible routine to address a particular challenge through model design, but are limited in their ability to understand news content and capture user interests. In this paper, we introduce GENRE, an LLM-powered generative news recommendation framework, which leverages pretrained semantic knowledge from large language models to enrich news data. Our aim is to provide a flexible and unified solution for news recommendation by moving from model design to prompt design. We showcase the use of GENRE for personalized news generation, user profiling, and news summarization. Extensive experiments with various popular recommendation models demonstrate the effectiveness of GENRE. 
&lt;/p&gt;</description></item><item><title>RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06767</link><description>&lt;p&gt;
RAFT: &#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#29992;&#20110;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06767
&lt;/p&gt;
&lt;p&gt;
RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#24191;&#27867;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#24102;&#26469;&#30340;&#38544;&#24335;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#23376;&#20248;&#26679;&#26412;&#12289;&#25197;&#26354;&#30340;&#32467;&#26524;&#21644;&#19981;&#20844;&#24179;&#65292;&#21487;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#30340;&#20262;&#29702;&#21644;&#20559;&#22909;&#23545;&#40784;&#26159;&#30830;&#20445;&#23427;&#20204;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#30340;&#37096;&#32626;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#37319;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288; RLHF&#65289;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#25163;&#27573;&#12290;&#22312; RL &#31639;&#27861;&#30340;&#25351;&#23548;&#19979;&#65292;&#29992;&#20154;&#31867;&#21453;&#39304;&#25351;&#23548;&#30340;&#22870;&#21169;&#27169;&#22411;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292; RL &#31639;&#27861;&#30340;&#20302;&#25928;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#24120;&#24120;&#20250;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25104;&#21151;&#23545;&#40784;&#20135;&#29983;&#37325;&#22823;&#38556;&#30861;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#26356;&#20026;&#24378;&#22823;&#21644;&#31616;&#21270;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#65288; RAFT &#65289;&#65292;&#26088;&#22312;&#23545;&#40784;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#29616;&#26377;&#30340;VL&#27169;&#22411;&#20013;&#21152;&#20837;&#21512;&#25104;&#25968;&#25454;&#38598;SyViC&#65292;&#25104;&#21151;&#23454;&#29616;&#23545;'&#21517;&#35789;&#20197;&#22806;'&#30340;&#29702;&#35299;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.17590</link><description>&lt;p&gt;
&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#31361;&#30772;&#21517;&#35789;&#23616;&#38480;
&lt;/p&gt;
&lt;p&gt;
Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data. (arXiv:2303.17590v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#29616;&#26377;&#30340;VL&#27169;&#22411;&#20013;&#21152;&#20837;&#21512;&#25104;&#25968;&#25454;&#38598;SyViC&#65292;&#25104;&#21151;&#23454;&#29616;&#23545;'&#21517;&#35789;&#20197;&#22806;'&#30340;&#29702;&#35299;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#65288;&#20960;&#20046;&#20219;&#24847;&#65289;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#36827;&#34892;&#38646;&#26679;&#26412;&#24320;&#25918;&#35789;&#27719;&#25512;&#29702;&#65292;&#21462;&#20195;&#20102;&#19968;&#32452;&#25903;&#25345;&#30340;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#20010;&#26681;&#26412;&#24615;&#24369;&#28857;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32431;&#21512;&#25104;&#25968;&#25454;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#25945;&#20250;&#36825;&#20123;&#27169;&#22411;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#32780;&#19981;&#25439;&#23475;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#20316;&#32773;&#36129;&#29486;&#20102;&#19968;&#20010;&#25968;&#30334;&#19975;&#35268;&#27169;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;SyViC&#65292;&#20197;&#21450;&#25968;&#25454;&#29983;&#25104;&#20195;&#30721;&#24211;&#65292;&#20801;&#35768;&#22312;&#29616;&#26377;&#30340;VL&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#39069;&#22806;&#30340;&#21512;&#36866;&#25968;&#25454;&#65292;&#23454;&#29616;'noun'&#20197;&#22806;&#30340;&#29702;&#35299;&#20219;&#21153;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained Vision &amp; Language (VL) models have shown remarkable performance in many applications, enabling replacing a fixed set of supported classes with zero-shot open vocabulary reasoning over (almost arbitrary) natural language prompts. However, recent works have uncovered a fundamental weakness of these models. For example, their difficulty to understand Visual Language Concepts (VLC) that go 'beyond nouns' such as the meaning of non-object words (e.g., attributes, actions, relations, states, etc.), or difficulty in performing compositional reasoning such as understanding the significance of the order of the words in a sentence. In this work, we investigate to which extent purely synthetic data could be leveraged to teach these models to overcome such shortcomings without compromising their zero-shot capabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scale synthetic dataset and data generation codebase allowing to generate additional suitable dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;STAR&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;Answer Set Programming&#30456;&#32467;&#21512;&#65292;&#20197;&#36798;&#21040;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25104;&#21151;&#24212;&#23545;&#38656;&#35201;&#25512;&#29702;&#30340;&#19981;&#21516;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.03780</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;Answer Set Programming&#23454;&#29616;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Reliable Natural Language Understanding with Large Language Models and Answer Set Programming. (arXiv:2302.03780v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;STAR&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;Answer Set Programming&#30456;&#32467;&#21512;&#65292;&#20197;&#36798;&#21040;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25104;&#21151;&#24212;&#23545;&#38656;&#35201;&#25512;&#29702;&#30340;&#19981;&#21516;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#20174;&#21477;&#23376;&#20013;&#25552;&#21462;&#20449;&#24687;&#65288;&#24847;&#20041;&#65289;&#65292;&#23558;&#20854;&#19982;&#24050;&#26377;&#30340;&#24120;&#35782;&#30693;&#35782;&#32467;&#21512;&#65292;&#24182;&#36827;&#34892;&#25512;&#29702;&#26469;&#29702;&#35299;&#35821;&#35328;&#12290;&#34429;&#28982;&#20687;GPT-3&#21644;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#25991;&#26412;&#20013;&#30340;&#27169;&#24335;&#26469;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#22312;&#38656;&#35201;&#25512;&#29702;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#23427;&#20204;&#20063;&#26080;&#27861;&#21487;&#38752;&#22320;&#35299;&#37322;&#29983;&#25104;&#30340;&#31572;&#26696;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STAR&#26694;&#26550;&#65292;&#23558;LLMs&#19982;Answer Set Programming (ASP) &#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;LLMs&#26377;&#25928;&#22320;&#20174;&#35821;&#35328;&#20013;&#25552;&#21462;&#20197;&#35859;&#35789;&#34920;&#31034;&#30340;&#30693;&#35782;&#12290;&#28982;&#21518;&#20351;&#29992;&#30446;&#26631;&#23548;&#21521;&#30340;ASP&#26469;&#21487;&#38752;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#23558;STAR&#26694;&#26550;&#24212;&#29992;&#20110;&#38656;&#35201;&#25512;&#29702;&#30340;&#19977;&#20010;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65306;&#23450;&#24615;&#25512;&#29702;&#65292;&#25968;&#23398;&#25512;&#29702;&#21644;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;STAR&#33021;&#22815;&#22635;&#34917;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#25512;&#29702;&#24046;&#36317;&#65292;&#25552;&#20379;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans understand language by extracting information (meaning) from sentences, combining it with existing commonsense knowledge, and then performing reasoning to draw conclusions. While large language models (LLMs) such as GPT-3 and ChatGPT are able to leverage patterns in the text to solve a variety of NLP tasks, they fall short in problems that require reasoning. They also cannot reliably explain the answers generated for a given question. In order to emulate humans better, we propose STAR, a framework that combines LLMs with Answer Set Programming (ASP). We show how LLMs can be used to effectively extract knowledge -- represented as predicates -- from language. Goal-directed ASP is then employed to reliably reason over this knowledge. We apply the STAR framework to three different NLU tasks requiring reasoning: qualitative reasoning, mathematical reasoning, and goal-directed conversation. Our experiments reveal that STAR is able to bridge the gap of reasoning in NLU tasks, leading t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#21487;&#30097;&#20551;&#35774;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#33258;&#28982;&#29983;&#25104;&#30340;&#26597;&#35810;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#35201;&#27714;&#31995;&#32479;&#33021;&#22815;&#26816;&#27979;&#21040;&#21644;&#22238;&#31572;&#26082;&#21253;&#21547;&#20856;&#22411;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#21448;&#21253;&#21547;&#21487;&#30097;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#20197;&#35780;&#20272;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10003</link><description>&lt;p&gt;
(QA)$^2$: &#24102;&#26377;&#21487;&#30097;&#20551;&#35774;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
(QA)$^2$: Question Answering with Questionable Assumptions. (arXiv:2212.10003v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#21487;&#30097;&#20551;&#35774;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#33258;&#28982;&#29983;&#25104;&#30340;&#26597;&#35810;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#35201;&#27714;&#31995;&#32479;&#33021;&#22815;&#26816;&#27979;&#21040;&#21644;&#22238;&#31572;&#26082;&#21253;&#21547;&#20856;&#22411;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#21448;&#21253;&#21547;&#21487;&#30097;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#20197;&#35780;&#20272;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#24448;&#24448;&#21253;&#21547;&#26377;&#21487;&#30097;&#30340;&#20551;&#35774;&#65292;&#36825;&#20123;&#20551;&#35774;&#21487;&#33021;&#26159;&#38169;&#35823;&#30340;&#25110;&#26080;&#27861;&#39564;&#35777;&#30340;&#12290;&#21253;&#21547;&#21487;&#30097;&#20551;&#35774;&#30340;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#19982;&#36890;&#24120;&#30340;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#30340;&#31572;&#26696;&#19981;&#21516;&#30340;&#31572;&#39064;&#31574;&#30053;&#12290;&#20363;&#22914;&#65292;&#38382;&#39064;&#8220;&#29595;&#20029;&#183;&#23621;&#37324;&#26159;&#20160;&#20040;&#26102;&#20505;&#21457;&#29616;&#38080;&#30340;&#65311;&#8221;&#19981;&#33021;&#20687;&#36890;&#24120;&#30340;&#8220;&#20160;&#20040;&#26102;&#20505;&#8221;&#38382;&#39064;&#19968;&#26679;&#22238;&#31572;&#65292;&#32780;&#26159;&#38656;&#35201;&#35299;&#20915;&#20551;&#35774;&#8220;&#29595;&#20029;&#183;&#23621;&#37324;&#21457;&#29616;&#20102;&#38080;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;(QA)$^2$&#65288;&#24102;&#26377;&#21487;&#30097;&#20551;&#35774;&#30340;&#38382;&#31572;&#31995;&#32479;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#33258;&#28982;&#29983;&#25104;&#30340;&#25628;&#32034;&#24341;&#25806;&#26597;&#35810;&#30340;&#24320;&#25918;&#39046;&#22495;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#26597;&#35810;&#21487;&#33021;&#21253;&#21547;&#25110;&#19981;&#21253;&#21547;&#21487;&#30097;&#20551;&#35774;&#12290;&#35201;&#22312;(QA)$^2$&#19978;&#21462;&#24471;&#25104;&#21151;&#65292;&#31995;&#32479;&#24517;&#39035;&#33021;&#22815;&#26816;&#27979;&#20986;&#21487;&#30097;&#20551;&#35774;&#65292;&#24182;&#19988;&#33021;&#22815;&#23545;&#26082;&#21253;&#21547;&#20856;&#22411;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#21448;&#21253;&#21547;&#21487;&#30097;&#20551;&#35774;&#30340;&#38382;&#39064;&#20135;&#29983;&#20805;&#20998;&#30340;&#22238;&#31572;&#12290;&#36890;&#36807;&#20154;&#24037;&#35780;&#20998;&#32773;&#30340;&#21487;&#25509;&#21463;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#26102;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Naturally occurring information-seeking questions often contain questionable assumptions -- assumptions that are false or unverifiable. Questions containing questionable assumptions are challenging because they require a distinct answer strategy that deviates from typical answers for information-seeking questions. For instance, the question "When did Marie Curie discover Uranium?" cannot be answered as a typical "when" question without addressing the false assumption "Marie Curie discovered Uranium". In this work, we propose (QA)$^2$ (Question Answering with Questionable Assumptions), an open-domain evaluation dataset consisting of naturally occurring search engine queries that may or may not contain questionable assumptions. To be successful on (QA)$^2$, systems must be able to detect questionable assumptions and also be able to produce adequate responses for both typical information-seeking questions and ones with questionable assumptions. Through human rater acceptability on end-to-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#30693;&#35782;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(KE-PLMs)&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#30693;&#35782;&#34701;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#20840;&#38754;&#20102;&#35299;&#65292;&#24182;&#20171;&#32461;&#20102;&#36866;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#20998;&#31867;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.05994</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Knowledge Enhanced Pre-trained Language Models. (arXiv:2211.05994v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#30693;&#35782;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(KE-PLMs)&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#30693;&#35782;&#34701;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#20840;&#38754;&#20102;&#35299;&#65292;&#24182;&#20171;&#32461;&#20102;&#36866;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#35821;&#26009;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;PLMs&#21487;&#20197;&#22312;&#31934;&#35843;&#38454;&#27573;&#26377;&#25928;&#22320;&#33719;&#24471;&#20174;&#22823;&#35268;&#27169;&#35757;&#32451;&#25991;&#26412;&#20013;&#23398;&#21040;&#30340;&#20016;&#23500;&#30693;&#35782;&#65292;&#24182;&#23545;&#19979;&#28216;&#20219;&#21153;&#20135;&#29983;&#22909;&#22788;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#32570;&#20047;&#22806;&#37096;&#30693;&#35782;&#23548;&#33268;&#25512;&#29702;&#33021;&#21147;&#36739;&#24046;&#12290;&#30740;&#31350;&#20154;&#21592;&#33268;&#21147;&#20110;&#23558;&#30693;&#35782;&#34701;&#20837;PLMs&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#30693;&#35782;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(KE-PLMs)&#65292;&#20197;&#25552;&#20379;&#23545;&#36825;&#19968;&#34028;&#21187;&#21457;&#23637;&#39046;&#22495;&#30340;&#28165;&#26224;&#35748;&#35782;&#12290;&#25105;&#20204;&#20998;&#21035;&#20171;&#32461;&#20102;&#36866;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;(NLU)&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;(NLG)&#30340;&#21512;&#36866;&#20998;&#31867;&#27861;&#65292;&#20197;&#31361;&#20986;NLP&#30340;&#36825;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#12290;&#23545;&#20110;NLU&#65292;&#25105;&#20204;&#23558;&#30693;&#35782;&#31867;&#22411;&#21010;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65306;&#35821;&#35328;&#30693;&#35782;&#65292;&#25991;&#26412;&#30693;&#35782;&#65292;&#30693;&#35782;&#24211;&#30693;&#35782;&#21644;&#24120;&#35782;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models (PLMs) which are trained on large text corpus via self-supervised learning method, have yielded promising performance on various tasks in Natural Language Processing (NLP). However, though PLMs with huge parameters can effectively possess rich knowledge learned from massive training text and benefit downstream tasks at the fine-tuning stage, they still have some limitations such as poor reasoning ability due to the lack of external knowledge. Research has been dedicated to incorporating knowledge into PLMs to tackle these issues. In this paper, we present a comprehensive review of Knowledge Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear insight into this thriving field. We introduce appropriate taxonomies respectively for Natural Language Understanding (NLU) and Natural Language Generation (NLG) to highlight these two main tasks of NLP. For NLU, we divide the types of knowledge into four categories: linguistic knowledge, text knowledge, kn
&lt;/p&gt;</description></item><item><title>CLSE&#35821;&#26009;&#24211;&#26159;&#20026;&#20102;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#21629;&#21517;&#23454;&#20307;&#22788;&#29702;&#30340;&#38382;&#39064;&#32780;&#21457;&#24067;&#30340;&#65292;&#35813;&#35821;&#26009;&#24211;&#30001;&#35821;&#35328;&#23398;&#19987;&#23478;&#27880;&#37322;&#65292;&#21253;&#25324;34&#31181;&#35821;&#35328;&#21644;74&#31181;&#19981;&#21516;&#30340;&#35821;&#20041;&#31867;&#22411;&#65292;&#33021;&#22815;&#25903;&#25345;&#22810;&#31181;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.02423</link><description>&lt;p&gt;
CLSE: &#36328;&#35821;&#35328;&#35821;&#22659;&#19979;&#20855;&#26377;&#35821;&#35328;&#23398;&#24847;&#20041;&#23454;&#20307;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
CLSE: Corpus of Linguistically Significant Entities. (arXiv:2211.02423v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02423
&lt;/p&gt;
&lt;p&gt;
CLSE&#35821;&#26009;&#24211;&#26159;&#20026;&#20102;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#21629;&#21517;&#23454;&#20307;&#22788;&#29702;&#30340;&#38382;&#39064;&#32780;&#21457;&#24067;&#30340;&#65292;&#35813;&#35821;&#26009;&#24211;&#30001;&#35821;&#35328;&#23398;&#19987;&#23478;&#27880;&#37322;&#65292;&#21253;&#25324;34&#31181;&#35821;&#35328;&#21644;74&#31181;&#19981;&#21516;&#30340;&#35821;&#20041;&#31867;&#22411;&#65292;&#33021;&#22815;&#25903;&#25345;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;(NLG)&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#26159;&#27491;&#30830;&#22788;&#29702;&#21629;&#21517;&#23454;&#20307;&#12290;&#21629;&#21517;&#23454;&#20307;&#32463;&#24120;&#26159;&#35821;&#27861;&#38169;&#35823;&#30340;&#28304;&#22836;&#65292;&#22914;&#38169;&#35823;&#30340;&#20171;&#35789;&#12289;&#38169;&#35823;&#30340;&#20896;&#35789;&#22788;&#29702;&#25110;&#38169;&#35823;&#30340;&#23454;&#20307;&#23624;&#25240;&#12290;&#22914;&#26524;&#19981;&#32771;&#34385;&#35821;&#35328;&#34920;&#31034;&#65292;&#24403;&#22312;&#19968;&#23567;&#32452;&#20219;&#24847;&#36873;&#25321;&#30340;&#21442;&#25968;&#20540;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#25110;&#23558;&#25968;&#25454;&#38598;&#20174;&#35821;&#35328;&#19978;&#36739;&#31616;&#21333;&#30340;&#35821;&#35328;(&#22914;&#33521;&#35821;)&#32763;&#35793;&#21040;&#35821;&#35328;&#19978;&#36739;&#22797;&#26434;&#30340;&#35821;&#35328;(&#22914;&#20420;&#35821;)&#26102;&#65292;&#36825;&#20123;&#38169;&#35823;&#30340;&#37325;&#35201;&#24615;&#24120;&#24120;&#34987;&#20302;&#20272;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26576;&#20123;&#24212;&#29992;&#26469;&#35828;&#65292;&#31934;&#30830;&#30340;&#35821;&#27861;&#27491;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#8212;&#8212;&#27597;&#35821;&#20351;&#29992;&#32773;&#21487;&#33021;&#20250;&#35273;&#24471;&#19982;&#23454;&#20307;&#30456;&#20851;&#30340;&#35821;&#27861;&#38169;&#35823;&#26159;&#24858;&#34850;&#30340;&#12289;&#19981;&#21644;&#35856;&#30340;&#29978;&#33267;&#20196;&#20154;&#19981;&#24555;&#30340;&#12290;&#20026;&#20102;&#33021;&#22815;&#21019;&#24314;&#26356;&#20855;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;NLG&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#30001;&#35821;&#35328;&#23398;&#19987;&#23478;&#27880;&#37322;&#30340;"&#20855;&#26377;&#35821;&#35328;&#23398;&#24847;&#20041;&#30340;&#23454;&#20307;&#35821;&#26009;&#24211;&#65288;CLSE&#65289;"&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;34&#31181;&#35821;&#35328;&#65292;&#28085;&#30422;&#20102;74&#31181;&#19981;&#21516;&#30340;&#35821;&#20041;&#31867;&#22411;&#65292;&#20197;&#25903;&#25345;&#33322;&#31354;&#20844;&#21496;&#26102;&#38388;&#34920;&#31561;&#21508;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the biggest challenges of natural language generation (NLG) is the proper handling of named entities. Named entities are a common source of grammar mistakes such as wrong prepositions, wrong article handling, or incorrect entity inflection. Without factoring linguistic representation, such errors are often underrepresented when evaluating on a small set of arbitrarily picked argument values, or when translating a dataset from a linguistically simpler language, like English, to a linguistically complex language, like Russian. However, for some applications, broadly precise grammatical correctness is critical -- native speakers may find entity-related grammar errors silly, jarring, or even offensive.  To enable the creation of more linguistically diverse NLG datasets, we release a Corpus of Linguistically Significant Entities (CLSE) annotated by linguist experts. The corpus includes 34 languages and covers 74 different semantic types to support various applications from airline ti
&lt;/p&gt;</description></item></channel></rss>