<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;&#26102;&#20986;&#29616;&#30340;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#29616;&#35937;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20043;&#21518;&#23545;&#38169;&#35823;&#28436;&#31034;&#30340;&#22788;&#29702;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#65292;&#24182;&#25351;&#20986;&#20102;&#38169;&#35823;&#24402;&#32435;&#22836;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2307.09476</link><description>&lt;p&gt;
&#36807;&#24230;&#24605;&#32771;&#30495;&#30456;&#65306;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09476
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;&#26102;&#20986;&#29616;&#30340;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#29616;&#35937;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20043;&#21518;&#23545;&#38169;&#35823;&#28436;&#31034;&#30340;&#22788;&#29702;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#65292;&#24182;&#25351;&#20986;&#20102;&#38169;&#35823;&#24402;&#32435;&#22836;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#31034;&#33539;&#36827;&#34892;&#22797;&#26434;&#27169;&#24335;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27169;&#20223;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#37325;&#29616;&#19981;&#20934;&#30830;&#25110;&#26377;&#23475;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#26469;&#30740;&#31350;&#26377;&#23475;&#30340;&#27169;&#20223;&#65292;&#24182;&#30830;&#23450;&#20102;&#20004;&#20010;&#30456;&#20851;&#29616;&#35937;&#65306;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#12290;&#31532;&#19968;&#20010;&#29616;&#35937;&#65292;&#36807;&#24230;&#24605;&#32771;&#65292;&#22312;&#32473;&#20986;&#27491;&#30830;&#19982;&#38169;&#35823;&#30340;&#23569;&#37327;&#31034;&#33539;&#26102;&#65292;&#25105;&#20204;&#20174;&#20013;&#38388;&#23618;&#35299;&#30721;&#39044;&#27979;&#12290;&#22312;&#26089;&#26399;&#23618;&#20013;&#65292;&#20004;&#31181;&#31034;&#33539;&#24341;&#36215;&#20102;&#30456;&#20284;&#30340;&#27169;&#22411;&#34892;&#20026;&#65292;&#20294;&#22312;&#26576;&#20010;&#8220;&#20851;&#38190;&#23618;&#8221;&#20043;&#21518;&#65292;&#32473;&#20986;&#38169;&#35823;&#31034;&#33539;&#30340;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#12290;&#31532;&#20108;&#20010;&#29616;&#35937;&#65292;&#38169;&#35823;&#24402;&#32435;&#22836;&#65292;&#21487;&#33021;&#26159;&#36807;&#24230;&#24605;&#32771;&#30340;&#19968;&#31181;&#26426;&#21046;&#24615;&#21407;&#22240;&#65306;&#36825;&#20123;&#26159;&#20301;&#20110;&#36739;&#26202;&#23618;&#30340;&#22836;&#37096;&#65292;&#23427;&#20204;&#20851;&#27880;&#24182;&#22797;&#21046;&#20808;&#21069;&#31034;&#33539;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#20854;&#21066;&#24369;&#20250;&#20943;&#23569;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#30830;&#30340;&#25351;&#31034;&#26041;&#27861;&#65292;&#21033;&#29992;&#28857;&#21644;&#26694;&#31561;&#22810;&#26679;&#21270;&#30340;&#24341;&#29992;&#34920;&#31034;&#26041;&#27861;&#26469;&#24341;&#29992;&#29305;&#27530;&#21306;&#22495;&#65292;&#20174;&#32780;&#20351;&#24471;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#33021;&#22815;&#23454;&#29616;&#26356;&#31934;&#32454;&#30340;&#20132;&#20114;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatSpot&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25903;&#25345;&#22810;&#31181;&#24418;&#24335;&#30340;&#20114;&#21160;&#65292;&#25552;&#20379;&#26356;&#28789;&#27963;&#21644;&#26080;&#32541;&#30340;&#20132;&#20114;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2307.09474</link><description>&lt;p&gt;
ChatSpot: &#36890;&#36807;&#31934;&#30830;&#30340;&#25351;&#31034;&#35843;&#25972;&#24341;&#23548;&#36827;&#34892;&#22810;&#27169;&#24577;LLMs&#24341;&#23548;&#21021;&#21019;
&lt;/p&gt;
&lt;p&gt;
ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning. (arXiv:2307.09474v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#30830;&#30340;&#25351;&#31034;&#26041;&#27861;&#65292;&#21033;&#29992;&#28857;&#21644;&#26694;&#31561;&#22810;&#26679;&#21270;&#30340;&#24341;&#29992;&#34920;&#31034;&#26041;&#27861;&#26469;&#24341;&#29992;&#29305;&#27530;&#21306;&#22495;&#65292;&#20174;&#32780;&#20351;&#24471;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#33021;&#22815;&#23454;&#29616;&#26356;&#31934;&#32454;&#30340;&#20132;&#20114;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatSpot&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25903;&#25345;&#22810;&#31181;&#24418;&#24335;&#30340;&#20114;&#21160;&#65292;&#25552;&#20379;&#26356;&#28789;&#27963;&#21644;&#26080;&#32541;&#30340;&#20132;&#20114;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#26159;&#21453;&#26144;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21487;&#29992;&#24615;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31471;&#21040;&#31471;MLLMs&#21482;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#35821;&#35328;&#25351;&#20196;&#19982;&#20854;&#20132;&#20114;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20132;&#20114;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31934;&#30830;&#30340;&#24341;&#29992;&#25351;&#20196;&#65292;&#21033;&#29992;&#28857;&#21644;&#26694;&#31561;&#22810;&#26679;&#21270;&#30340;&#24341;&#29992;&#34920;&#31034;&#26041;&#27861;&#26469;&#24341;&#29992;&#29305;&#27530;&#21306;&#22495;&#12290;&#36825;&#20351;&#24471;MLLMs&#33021;&#22815;&#32858;&#28966;&#20110;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#65292;&#24182;&#23454;&#29616;&#26356;&#31934;&#32454;&#30340;&#20132;&#20114;&#12290;&#22522;&#20110;&#31934;&#30830;&#30340;&#24341;&#29992;&#25351;&#20196;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatSpot&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25903;&#25345;&#21253;&#25324;&#40736;&#26631;&#28857;&#20987;&#12289;&#25302;&#25918;&#21644;&#32472;&#21046;&#26694;&#31561;&#22810;&#31181;&#24418;&#24335;&#30340;&#20114;&#21160;&#65292;&#25552;&#20379;&#26356;&#28789;&#27963;&#21644;&#26080;&#32541;&#30340;&#20114;&#21160;&#20307;&#39564;&#12290;&#25105;&#20204;&#36824;&#26681;&#25454;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;GPT-4&#29983;&#25104;&#20102;&#19968;&#20010;&#22810;&#23618;&#27425;&#30340;&#35270;&#35273;&#35821;&#35328;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we des
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20998;&#26512;&#20102;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;SR-GAN&#27169;&#22411;&#65292;&#32467;&#26524;&#21457;&#29616;EDSR&#27169;&#22411;&#22312;&#20445;&#25345;&#35270;&#35273;&#36136;&#37327;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#36755;&#20837;&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;PSNR&#21644;SSIM&#20540;&#65292;&#24182;&#19988;&#36820;&#22238;&#39640;&#36136;&#37327;&#30340;OCR&#32467;&#26524;&#65292;&#36825;&#34920;&#26126;EDSR&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.09456</link><description>&lt;p&gt;
SR-GAN&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A comparative analysis of SR-GAN models. (arXiv:2307.09456v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20998;&#26512;&#20102;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;SR-GAN&#27169;&#22411;&#65292;&#32467;&#26524;&#21457;&#29616;EDSR&#27169;&#22411;&#22312;&#20445;&#25345;&#35270;&#35273;&#36136;&#37327;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#36755;&#20837;&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;PSNR&#21644;SSIM&#20540;&#65292;&#24182;&#19988;&#36820;&#22238;&#39640;&#36136;&#37327;&#30340;OCR&#32467;&#26524;&#65292;&#36825;&#34920;&#26126;EDSR&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#36229;&#20998;&#36776;&#29575;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;SR GAN&#65289;&#27169;&#22411;&#65292;&#21253;&#25324;ESRGAN&#65292;Real-ESRGAN&#21644;EDSR&#65292;&#22312;&#19968;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#20197;&#21450;&#32463;&#36807;&#38477;&#32423;&#22788;&#29702;&#30340;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20123;&#27169;&#22411;&#20284;&#20046;&#22312;&#20445;&#25345;&#35270;&#35273;&#36136;&#37327;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#36755;&#20837;&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#65292;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;Tesseract OCR&#24341;&#25806;&#35780;&#20272;&#30340;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#26469;&#33258;huggingface&#30340;EDSR-BASE&#27169;&#22411;&#22312;&#23450;&#37327;&#25351;&#26631;&#21644;&#20027;&#35266;&#35270;&#35273;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20313;&#20505;&#36873;&#27169;&#22411;&#65292;&#24182;&#19988;&#35745;&#31639;&#24320;&#38144;&#26368;&#23567;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EDSR&#29983;&#25104;&#20855;&#26377;&#36739;&#39640;&#23792;&#20540;&#20449;&#22122;&#27604;&#65288;PSNR&#65289;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#65288;SSIM&#65289;&#20540;&#30340;&#22270;&#20687;&#65292;&#24182;&#22312;Tesseract OCR&#24341;&#25806;&#19979;&#36820;&#22238;&#39640;&#36136;&#37327;&#30340;OCR&#32467;&#26524;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;EDSR&#26159;&#19968;&#31181;&#31283;&#20581;&#26377;&#25928;&#30340;&#21333;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#21512;&#20110;&#38656;&#35201;OCR&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we evaluate the performance of multiple state-of-the-art SR GAN (Super Resolution Generative Adversarial Network) models, ESRGAN, Real-ESRGAN and EDSR, on a benchmark dataset of real-world images which undergo degradation using a pipeline. Our results show that some models seem to significantly increase the resolution of the input images while preserving their visual quality, this is assessed using Tesseract OCR engine. We observe that EDSR-BASE model from huggingface outperforms the remaining candidate models in terms of both quantitative metrics and subjective visual quality assessments with least compute overhead. Specifically, EDSR generates images with higher peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) values and are seen to return high quality OCR results with Tesseract OCR engine. These findings suggest that EDSR is a robust and effective approach for single-image super-resolution and may be particularly well-suited for applications wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20266;&#24322;&#24120;&#26292;&#38706;&#65288;POE&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39034;&#24207;&#23631;&#34109;&#19982;&#20869;&#20998;&#24067;&#31867;&#30456;&#20851;&#30340;&#20196;&#29260;&#26500;&#24314;&#26367;&#20195;&#30340;&#22806;&#20998;&#24067;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;&#26410;&#30693;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.09455</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#30340;&#20266;&#24322;&#24120;&#26292;&#38706;&#27861;&#29992;&#20110;&#26816;&#27979;&#26410;&#30693;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Pseudo Outlier Exposure for Out-of-Distribution Detection using Pretrained Transformers. (arXiv:2307.09455v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20266;&#24322;&#24120;&#26292;&#38706;&#65288;POE&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39034;&#24207;&#23631;&#34109;&#19982;&#20869;&#20998;&#24067;&#31867;&#30456;&#20851;&#30340;&#20196;&#29260;&#26500;&#24314;&#26367;&#20195;&#30340;&#22806;&#20998;&#24067;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;&#26410;&#30693;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#24212;&#29992;&#65292;&#26816;&#27979;&#26410;&#30693;&#20998;&#24067;&#30340;&#26679;&#26412;&#26377;&#21161;&#20110;&#25552;&#37266;&#29992;&#25143;&#25110;&#25298;&#32477;&#19981;&#21487;&#38752;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#36807;&#21442;&#25968;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20250;&#23545;&#20869;&#20998;&#24067;&#21644;&#22806;&#20998;&#24067;&#26679;&#26412;&#37117;&#20135;&#29983;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#29305;&#21035;&#26159;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#20869;&#20998;&#24067;&#26679;&#26412;&#20855;&#26377;&#30456;&#20284;&#35821;&#20041;&#34920;&#31034;&#30340;&#22806;&#20998;&#24067;&#26679;&#26412;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#36825;&#20123;&#22806;&#20998;&#24067;&#26679;&#26412;&#20301;&#20110;&#20869;&#20998;&#24067;&#27969;&#24418;&#38468;&#36817;&#12290;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20869;&#20998;&#24067;&#21644;&#22810;&#26679;&#30340;&#24322;&#24120;&#26679;&#26412;&#35757;&#32451;&#25298;&#32477;&#32593;&#32476;&#26469;&#26816;&#27979;&#27979;&#35797;&#30340;&#22806;&#20998;&#24067;&#26679;&#26412;&#65292;&#20294;&#26126;&#30830;&#25910;&#38598;&#36741;&#21161;&#30340;&#22806;&#20998;&#24067;&#25968;&#25454;&#38598;&#20250;&#22686;&#21152;&#25968;&#25454;&#25910;&#38598;&#30340;&#36127;&#25285;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20266;&#24322;&#24120;&#26292;&#38706;&#65288;POE&#65289;&#65292;&#36890;&#36807;&#39034;&#24207;&#23631;&#34109;&#19982;&#20869;&#20998;&#24067;&#31867;&#30456;&#20851;&#30340;&#20196;&#29260;&#26469;&#26500;&#24314;&#19968;&#20010;&#26367;&#20195;&#30340;&#22806;&#20998;&#24067;&#25968;&#25454;&#38598;&#12290;POE&#24341;&#20837;&#30340;&#26367;&#20195;&#22806;&#20998;&#24067;&#26679;&#26412;&#26174;&#31034;&#20986;&#19982;&#20869;&#37096;&#25968;&#25454;&#31867;&#20284;&#30340;&#34920;&#31034;&#65292;&#36825;&#23545;&#20110;&#35757;&#32451;&#25298;&#32477;&#32593;&#32476;&#26368;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;
&lt;/p&gt;
&lt;p&gt;
For real-world language applications, detecting an out-of-distribution (OOD) sample is helpful to alert users or reject such unreliable samples. However, modern over-parameterized language models often produce overconfident predictions for both in-distribution (ID) and OOD samples. In particular, language models suffer from OOD samples with a similar semantic representation to ID samples since these OOD samples lie near the ID manifold. A rejection network can be trained with ID and diverse outlier samples to detect test OOD samples, but explicitly collecting auxiliary OOD datasets brings an additional burden for data collection. In this paper, we propose a simple but effective method called Pseudo Outlier Exposure (POE) that constructs a surrogate OOD dataset by sequentially masking tokens related to ID classes. The surrogate OOD sample introduced by POE shows a similar representation to ID data, which is most effective in training a rejection network. Our method does not require any 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;ViCE&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#38382;&#31572;&#65292;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#26469;&#35780;&#20272;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.09416</link><description>&lt;p&gt;
&#35753;&#25105;&#20204;&#26469;ViCE&#65281;&#22312;&#22270;&#20687;&#29983;&#25104;&#35780;&#20272;&#20013;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Let's ViCE! Mimicking Human Cognitive Behavior in Image Generation Evaluation. (arXiv:2307.09416v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;ViCE&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#38382;&#31572;&#65292;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#26469;&#35780;&#20272;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#24341;&#20837;&#20102;&#33021;&#22815;&#26681;&#25454;&#25991;&#23383;&#36755;&#20837;&#20135;&#29983;&#39640;&#36136;&#37327;&#35270;&#35273;&#20869;&#23481;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#21160;&#19979;&#12290;&#23613;&#31649;&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#36924;&#30495;&#24230;&#26041;&#38754;&#19981;&#26029;&#21462;&#24471;&#36827;&#23637;&#65292;&#20294;&#30446;&#21069;&#23578;&#26410;&#23450;&#20041;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#23450;&#37327;&#34913;&#37327;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#21644;&#19982;&#25552;&#31034;&#35201;&#27714;&#30340;&#19968;&#33268;&#24615;&#65306;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#21482;&#26377;&#22522;&#20110;&#20154;&#31867;&#30340;&#35780;&#20272;&#34987;&#29992;&#20110;&#36136;&#37327;&#28385;&#24847;&#24230;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#36827;&#34892;&#35270;&#35273;&#27010;&#24565;&#35780;&#20272;&#65288;ViCE&#65289;&#65292;&#21363;&#35780;&#20272;&#29983;&#25104;/&#32534;&#36753;&#30340;&#22270;&#20687;&#19982;&#30456;&#24212;&#25552;&#31034;/&#25351;&#20196;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#36825;&#19968;&#36807;&#31243;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#34892;&#20026;&#30340;&#21551;&#21457;&#12290;ViCE&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#30340;&#20248;&#21183;&#32467;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#27969;&#31243;&#20013;&#65292;&#26088;&#22312;&#22797;&#21046;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#20013;&#30340;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research in Image Generation has recently made significant progress, particularly boosted by the introduction of Vision-Language models which are able to produce high-quality visual content based on textual inputs. Despite ongoing advancements in terms of generation quality and realism, no methodical frameworks have been defined yet to quantitatively measure the quality of the generated content and the adherence with the prompted requests: so far, only human-based evaluations have been adopted for quality satisfaction and for comparing different generative methods. We introduce a novel automated method for Visual Concept Evaluation (ViCE), i.e. to assess consistency between a generated/edited image and the corresponding prompt/instructions, with a process inspired by the human cognitive behaviour. ViCE combines the strengths of Large Language Models (LLMs) and Visual Question Answering (VQA) into a unified pipeline, aiming to replicate the human cognitive process in quality assessment.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;2021&#24180;&#21644;2022&#24180;&#25910;&#38598;&#30340;R&#35821;&#35328;&#21253;&#30340;&#24341;&#29992;&#26684;&#24335;&#30340;&#32437;&#21521;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#21644;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#36719;&#20214;&#24341;&#29992;&#26684;&#24335;&#22914;&#20309;&#38543;&#26102;&#38388;&#28436;&#21464;&#12290;&#30740;&#31350;&#21457;&#29616;&#36719;&#20214;&#30340;&#24341;&#29992;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#24341;&#29992;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#29702;&#35299;&#31185;&#23398;&#30740;&#31350;&#20013;&#36719;&#20214;&#24341;&#29992;&#30340;&#37325;&#35201;&#24615;&#21644;&#28436;&#21464;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.09390</link><description>&lt;p&gt;
&#36719;&#20214;&#24341;&#29992;&#26684;&#24335;&#22914;&#20309;&#38543;&#26102;&#38388;&#28436;&#21464;&#65311;&#23545;R&#32534;&#31243;&#35821;&#35328;&#21253;&#30340;&#32437;&#21521;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
How do software citation formats evolve over time? A longitudinal analysis of R programming language packages. (arXiv:2307.09390v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;2021&#24180;&#21644;2022&#24180;&#25910;&#38598;&#30340;R&#35821;&#35328;&#21253;&#30340;&#24341;&#29992;&#26684;&#24335;&#30340;&#32437;&#21521;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#21644;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#36719;&#20214;&#24341;&#29992;&#26684;&#24335;&#22914;&#20309;&#38543;&#26102;&#38388;&#28436;&#21464;&#12290;&#30740;&#31350;&#21457;&#29616;&#36719;&#20214;&#30340;&#24341;&#29992;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#24341;&#29992;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#29702;&#35299;&#31185;&#23398;&#30740;&#31350;&#20013;&#36719;&#20214;&#24341;&#29992;&#30340;&#37325;&#35201;&#24615;&#21644;&#28436;&#21464;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#30740;&#31350;&#33539;&#24335;&#19979;&#65292;&#30740;&#31350;&#36719;&#20214;&#22312;&#31185;&#23398;&#25506;&#31350;&#30340;&#20960;&#20046;&#27599;&#20010;&#38454;&#27573;&#37117;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23398;&#32773;&#20204;&#21628;&#21505;&#22312;&#23398;&#26415;&#20986;&#29256;&#29289;&#20013;&#27491;&#24335;&#24341;&#29992;&#36719;&#20214;&#65292;&#23558;&#20854;&#35270;&#20026;&#20256;&#32479;&#30740;&#31350;&#25104;&#26524;&#30340;&#24179;&#31561;&#23545;&#35937;&#12290;&#28982;&#32780;&#65292;&#36719;&#20214;&#30340;&#24341;&#29992;&#24182;&#19981;&#19968;&#33268;&#65306;&#19968;&#20010;&#36719;&#20214;&#23454;&#20307;&#21487;&#20197;&#34987;&#24341;&#29992;&#20026;&#19981;&#21516;&#30340;&#23545;&#35937;&#65292;&#24182;&#19988;&#24341;&#29992;&#21487;&#33021;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#29616;&#26377;&#30340;&#20851;&#20110;&#36719;&#20214;&#24341;&#29992;&#30340;&#32463;&#39564;&#24615;&#30740;&#31350;&#20013;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#12290;&#20026;&#20102;&#22635;&#34917;&#19978;&#36848;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#21644;&#20998;&#26512;&#22312;2021&#24180;&#21644;2022&#24180;&#25910;&#38598;&#30340;&#25152;&#26377;R&#36719;&#20214;&#21253;&#30340;&#24341;&#29992;&#26684;&#24335;&#30340;&#32437;&#21521;&#25968;&#25454;&#38598;&#65292;&#26469;&#20102;&#35299;R&#35821;&#35328;&#21253;&#30340;&#24341;&#29992;&#26684;&#24335;&#65292;&#36825;&#20123;&#21253;&#26159;&#24320;&#28304;&#36719;&#20214;&#23478;&#26063;&#20013;&#30340;&#37325;&#35201;&#25104;&#21592;&#65292;&#24182;&#30740;&#31350;&#24341;&#29992;&#26684;&#24335;&#22914;&#20309;&#38543;&#26102;&#38388;&#28436;&#21464;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24341;&#29992;&#32972;&#21518;&#30340;&#19981;&#21516;&#25991;&#26723;&#31867;&#22411;&#65292;&#20197;&#21450;&#24341;&#29992;&#26684;&#24335;&#20013;&#30340;&#21738;&#20123;&#20803;&#25968;&#25454;&#20803;&#32032;&#38543;&#26102;&#38388;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Under the data-driven research paradigm, research software has come to play crucial roles in nearly every stage of scientific inquiry. Scholars are advocating for the formal citation of software in academic publications, treating it on par with traditional research outputs. However, software is hardly consistently cited: one software entity can be cited as different objects, and the citations can change over time. These issues, however, are largely overlooked in existing empirical research on software citation. To fill the above gaps, the present study compares and analyzes a longitudinal dataset of citation formats of all R packages collected in 2021 and 2022, in order to understand the citation formats of R-language packages, important members in the open-source software family, and how the citations evolve over time. In particular, we investigate the different document types underlying the citations and what metadata elements in the citation formats changed over time. Furthermore, w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#26597;&#35810;&#37325;&#26500;&#65288;ZeQR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#23545;&#35805;&#25628;&#32034;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#27495;&#20041;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.09384</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#23545;&#35805;&#25628;&#32034;&#20013;&#30340;&#26597;&#35810;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Query Reformulation for Conversational Search. (arXiv:2307.09384v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09384
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#26597;&#35810;&#37325;&#26500;&#65288;ZeQR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#23545;&#35805;&#25628;&#32034;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#27495;&#20041;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#38899;&#21161;&#25163;&#30340;&#26222;&#21450;&#65292;&#23545;&#35805;&#25628;&#32034;&#22312;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#24341;&#36215;&#20102;&#26356;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#35805;&#25628;&#32034;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#20005;&#37325;&#38459;&#30861;&#20102;&#30417;&#30563;&#24335;&#23545;&#35805;&#25628;&#32034;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#26356;&#21152;&#20851;&#27880;&#38646;&#26679;&#26412;&#23545;&#35805;&#25628;&#32034;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25152;&#26377;&#30340;&#26816;&#32034;&#22120;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#32570;&#20047;&#36275;&#22815;&#30340;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#20182;&#20204;&#26080;&#27861;&#35299;&#20915;&#22240;&#30465;&#30053;&#32780;&#23548;&#33268;&#30340;&#24120;&#35265;&#23545;&#35805;&#27495;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#26597;&#35810;&#37325;&#26500;&#65288;ZeQR&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26681;&#25454;&#20808;&#21069;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#37325;&#26500;&#26597;&#35810;&#65292;&#32780;&#26080;&#38656;&#23545;&#35805;&#25628;&#32034;&#25968;&#25454;&#30340;&#30417;&#30563;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#35774;&#35745;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#26126;&#30830;&#35299;&#20915;&#20004;&#20010;&#24120;&#35265;&#30340;&#27495;&#20041;&#65306;&#21327;&#35843;&#21644;&#30465;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the popularity of voice assistants continues to surge, conversational search has gained increased attention in Information Retrieval. However, data sparsity issues in conversational search significantly hinder the progress of supervised conversational search methods. Consequently, researchers are focusing more on zero-shot conversational search approaches. Nevertheless, existing zero-shot methods face three primary limitations: they are not universally applicable to all retrievers, their effectiveness lacks sufficient explainability, and they struggle to resolve common conversational ambiguities caused by omission. To address these limitations, we introduce a novel Zero-shot Query Reformulation (ZeQR) framework that reformulates queries based on previous dialogue contexts without requiring supervision from conversational search data. Specifically, our framework utilizes language models designed for machine reading comprehension tasks to explicitly resolve two common ambiguities: cor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20027;&#35201;&#38024;&#23545;ASR&#22522;&#30784;&#27169;&#22411;Whisper&#30340;&#36755;&#20986;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65306;&#24494;&#35843;&#21644;&#36719;&#25552;&#31034;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#21464;Whisper&#30340;&#35299;&#30721;&#34892;&#20026;&#65292;&#29983;&#25104;&#20505;&#36873;&#20154;&#23454;&#38469;&#35828;&#20986;&#30340;&#21333;&#35789;&#12290;</title><link>http://arxiv.org/abs/2307.09378</link><description>&lt;p&gt;
&#20026;&#21475;&#35821;&#35780;&#20272;&#36866;&#24212;ASR&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Adapting an ASR Foundation Model for Spoken Language Assessment. (arXiv:2307.09378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20027;&#35201;&#38024;&#23545;ASR&#22522;&#30784;&#27169;&#22411;Whisper&#30340;&#36755;&#20986;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65306;&#24494;&#35843;&#21644;&#36719;&#25552;&#31034;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#21464;Whisper&#30340;&#35299;&#30721;&#34892;&#20026;&#65292;&#29983;&#25104;&#20505;&#36873;&#20154;&#23454;&#38469;&#35828;&#20986;&#30340;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#21487;&#38752;&#30340;&#21475;&#35821;&#35780;&#20272;&#31995;&#32479;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#24213;&#23618;&#30340;ASR&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;ASR&#22522;&#30784;&#27169;&#22411;&#22914;Whisper&#24050;&#32463;&#21487;&#29992;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#20154;&#31867;&#21487;&#35835;&#30340;&#65292;&#25152;&#20197;&#20250;&#28155;&#21152;&#26631;&#28857;&#31526;&#21495;&#65292;&#25968;&#23383;&#21576;&#29616;&#20026;&#38463;&#25289;&#20271;&#25968;&#23383;&#24418;&#24335;&#65292;&#21253;&#25324;&#32553;&#20889;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#20250;&#36339;&#36807;&#36755;&#20986;&#20013;&#30340;&#19981;&#27969;&#30021;&#21644;&#29369;&#35947;&#12290;&#34429;&#28982;&#23545;&#20110;&#21487;&#35835;&#24615;&#24456;&#26377;&#29992;&#65292;&#20294;&#36825;&#20123;&#23646;&#24615;&#23545;&#20110;&#35780;&#20272;&#20505;&#36873;&#20154;&#30340;&#33021;&#21147;&#21644;&#25552;&#20379;&#21453;&#39304;&#24182;&#19981;&#26377;&#29992;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;Whisper&#30340;&#36755;&#20986;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65306;&#24494;&#35843;&#21644;&#36719;&#25552;&#31034;&#24494;&#35843;&#12290;&#22312;&#20844;&#20849;&#35821;&#38899;&#35821;&#26009;&#24211;&#21644;&#33521;&#35821;&#23398;&#20064;&#32773;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21464;Whisper&#30340;&#35299;&#30721;&#34892;&#20026;&#65292;&#29983;&#25104;&#20505;&#36873;&#20154;&#23454;&#38469;&#35828;&#20986;&#30340;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
A crucial part of an accurate and reliable spoken language assessment system is the underlying ASR model. Recently, large-scale pre-trained ASR foundation models such as Whisper have been made available. As the output of these models is designed to be human readable, punctuation is added, numbers are presented in Arabic numeric form and abbreviations are included. Additionally, these models have a tendency to skip disfluencies and hesitations in the output. Though useful for readability, these attributes are not helpful for assessing the ability of a candidate and providing feedback. Here a precise transcription of what a candidate said is needed. In this paper, we give a detailed analysis of Whisper outputs and propose two solutions: fine-tuning and soft prompt tuning. Experiments are conducted on both public speech corpora and an English learner dataset. Results show that we can effectively alter the decoding behaviour of Whisper to generate the exact words spoken in the response.
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120; (mDT) &#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;mDT&#36890;&#36807;&#25972;&#20307;&#20998;&#26512;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#32467;&#21512;&#22270;&#21464;&#25442;&#22120;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20132;&#32455;&#34701;&#21512;&#23618;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#32452;&#21512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09312</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120;&#65306;&#25972;&#21512;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#21464;&#25442;&#22120;&#20197;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09312
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120; (mDT) &#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;mDT&#36890;&#36807;&#25972;&#20307;&#20998;&#26512;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#32467;&#21512;&#22270;&#21464;&#25442;&#22120;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20132;&#32455;&#34701;&#21512;&#23618;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#32452;&#21512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#20110;&#22270;&#30340;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21517;&#20026;&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120;&#65288;mDT&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#26631;&#35760;&#35780;&#35770;&#20026;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#22260;&#32469;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25972;&#20307;&#20998;&#26512;&#23637;&#24320;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#22270;&#21464;&#25442;&#22120;&#26469;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#20013;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#37319;&#29992;&#20132;&#32455;&#34701;&#21512;&#23618;&#26469;&#32452;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#22788;&#29702;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#65292;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#26395;&#20102;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#25552;&#20379;&#31038;&#20250;&#20215;&#20540;&#30340;&#26410;&#26469;&#24037;&#20316;&#65292;&#24182;&#35748;&#20026;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal graph-based transformer model for detecting hate speech in online social networks. In contrast to traditional text-only methods, our approach to labelling a comment as hate speech centers around the holistic analysis of text and images. This is done by leveraging graph transformers to capture the contextual relationships in the entire discussion that surrounds a comment, with interwoven fusion layers to combine text and image embeddings instead of processing different modalities separately. We compare the performance of our model to baselines that only process text; we also conduct extensive ablation studies. We conclude with future work for multimodal solutions to deliver social value in online contexts, arguing that capturing a holistic view of a conversation greatly advances the effort to detect anti-social behavior.
&lt;/p&gt;</description></item><item><title>Llama 2&#26159;&#19968;&#20010;&#20248;&#21270;&#30340;&#32842;&#22825;&#27169;&#22411;&#65292;&#36890;&#36807;fine-tuned&#25216;&#26415;&#21644;&#23433;&#20840;&#25913;&#36827;&#65292;&#34920;&#29616;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#24182;&#21487;&#20316;&#20026;&#38381;&#28304;&#27169;&#22411;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2307.09288</link><description>&lt;p&gt;
Llama 2: &#24320;&#25918;&#22522;&#30784;&#21644;&#20248;&#21270;&#32842;&#22825;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Llama 2: Open Foundation and Fine-Tuned Chat Models. (arXiv:2307.09288v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09288
&lt;/p&gt;
&lt;p&gt;
Llama 2&#26159;&#19968;&#20010;&#20248;&#21270;&#30340;&#32842;&#22825;&#27169;&#22411;&#65292;&#36890;&#36807;fine-tuned&#25216;&#26415;&#21644;&#23433;&#20840;&#25913;&#36827;&#65292;&#34920;&#29616;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#24182;&#21487;&#20316;&#20026;&#38381;&#28304;&#27169;&#22411;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#21457;&#24067;&#20102;Llama 2&#65292;&#19968;&#20010;&#21253;&#21547;&#39044;&#35757;&#32451;&#21644;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20854;&#35268;&#27169;&#20174;70&#20159;&#21040;700&#20159;&#21442;&#25968;&#19981;&#31561;&#12290;&#25105;&#20204;&#30340;&#20248;&#21270;LLM&#65292;&#31216;&#20026;Llama 2-Chat&#65292;&#22312;&#23545;&#35805;&#20351;&#29992;&#26696;&#20363;&#20013;&#34920;&#29616;&#20248;&#20110;&#24320;&#28304;&#32842;&#22825;&#27169;&#22411;&#12290;&#26681;&#25454;&#25105;&#20204;&#23545;&#26377;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#65292;&#23427;&#20204;&#21487;&#33021;&#26159;&#38381;&#28304;&#27169;&#22411;&#30340;&#21512;&#36866;&#26367;&#20195;&#21697;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;Llama 2-Chat&#30340;&#20248;&#21270;&#21644;&#23433;&#20840;&#24615;&#25913;&#36827;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#35753;&#31038;&#21306;&#33021;&#22815;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#26500;&#24314;&#24182;&#20026;LLM&#30340;&#36127;&#36131;&#20219;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#19977;&#32500;&#36830;&#20307;&#32593;&#32476;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#23618;&#27425;&#21270;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#32467;&#26500;&#26465;&#20214;&#26469;&#36827;&#34892;&#20840;&#38754;&#30340;&#19979;&#28216;&#24314;&#27169;&#31574;&#30053;&#27010;&#35272;&#12290;</title><link>http://arxiv.org/abs/2307.09274</link><description>&lt;p&gt;
&#36890;&#36807;3D&#36830;&#20307;&#32593;&#32476;&#25913;&#36827;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Improving Text Semantic Similarity Modeling through a 3D Siamese Network. (arXiv:2307.09274v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09274
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#19977;&#32500;&#36830;&#20307;&#32593;&#32476;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#23618;&#27425;&#21270;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#32467;&#26500;&#26465;&#20214;&#26469;&#36827;&#34892;&#20840;&#38754;&#30340;&#19979;&#28216;&#24314;&#27169;&#31574;&#30053;&#27010;&#35272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36830;&#20307;&#32593;&#32476;&#24050;&#25104;&#20026;&#24314;&#27169;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#27744;&#21270;&#25805;&#20316;&#26469;&#21387;&#32553;&#32534;&#30721;&#20013;Transformer&#22359;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#20174;&#32780;&#24471;&#21040;&#20108;&#32500;&#35821;&#20041;&#21521;&#37327;&#24182;&#20002;&#22833;Transformer&#22359;&#20013;&#30340;&#23618;&#27425;&#21270;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26377;&#38480;&#30340;&#35821;&#20041;&#21521;&#37327;&#32467;&#26500;&#31867;&#20284;&#20110;&#19968;&#20010;&#34987;&#38138;&#24179;&#30340;&#22320;&#24418;&#65292;&#38480;&#21046;&#20102;&#19979;&#28216;&#24314;&#27169;&#20013;&#21487;&#20197;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#33021;&#22312;&#36825;&#20010;&#24179;&#38754;&#19978;&#36827;&#34892;&#23548;&#33322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#24314;&#27169;&#30340;3D&#36830;&#20307;&#32593;&#32476;&#65292;&#23427;&#23558;&#35821;&#20041;&#20449;&#24687;&#26144;&#23556;&#21040;&#19968;&#20010;&#26356;&#39640;&#32500;&#30340;&#31354;&#38388;&#20013;&#12290;&#19977;&#32500;&#35821;&#20041;&#24352;&#37327;&#19981;&#20165;&#20445;&#30041;&#20102;&#26356;&#31934;&#30830;&#30340;&#31354;&#38388;&#21644;&#29305;&#24449;&#39046;&#22495;&#20449;&#24687;&#65292;&#36824;&#20026;&#20840;&#38754;&#30340;&#19979;&#28216;&#24314;&#27169;&#31574;&#30053;&#25552;&#20379;&#20102;&#24517;&#35201;&#30340;&#32467;&#26500;&#26465;&#20214;&#26469;&#25429;&#25417;&#36825;&#20123;&#20449;&#24687;&#12290;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#19978;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20010;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Siamese networks have gained popularity as a method for modeling text semantic similarity. Traditional methods rely on pooling operation to compress the semantic representations from Transformer blocks in encoding, resulting in two-dimensional semantic vectors and the loss of hierarchical semantic information from Transformer blocks. Moreover, this limited structure of semantic vectors is akin to a flattened landscape, which restricts the methods that can be applied in downstream modeling, as they can only navigate this flat terrain. To address this issue, we propose a novel 3D Siamese network for text semantic similarity modeling, which maps semantic information to a higher-dimensional space. The three-dimensional semantic tensors not only retains more precise spatial and feature domain information but also provides the necessary structural condition for comprehensive downstream modeling strategies to capture them. Leveraging this structural advantage, we introduce several modules to 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#21270;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;LRPE&#65289;&#23478;&#26063;&#65292;&#36890;&#36807;&#35268;&#33539;&#24418;&#24335;&#21644;&#37193;&#21464;&#25442;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#24320;&#21457;&#20445;&#30041;&#32447;&#24615;&#26102;&#31354;&#22797;&#26434;&#24230;&#30340;&#26032;&#30340;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#26377;&#25928;&#30340;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2307.09270</link><description>&lt;p&gt;
&#32447;&#24615;&#21270;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Linearized Relative Positional Encoding. (arXiv:2307.09270v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09270
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#21270;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;LRPE&#65289;&#23478;&#26063;&#65292;&#36890;&#36807;&#35268;&#33539;&#24418;&#24335;&#21644;&#37193;&#21464;&#25442;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#24320;&#21457;&#20445;&#30041;&#32447;&#24615;&#26102;&#31354;&#22797;&#26434;&#24230;&#30340;&#26032;&#30340;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#26377;&#25928;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#34987;&#24191;&#27867;&#29992;&#20110;&#26222;&#36890;&#21644;&#32447;&#24615; transformers &#20013;&#20197;&#34920;&#31034;&#20301;&#32622;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32534;&#30721;&#26041;&#27861;&#23545;&#20110;&#32447;&#24615; transformer &#19981;&#19968;&#23450;&#30452;&#25509;&#36866;&#29992;&#65292;&#22240;&#20026;&#21518;&#32773;&#38656;&#35201;&#23558;&#26597;&#35810;&#21644;&#38190;&#34920;&#31034;&#20998;&#35299;&#20026;&#21333;&#29420;&#30340;&#26680;&#20989;&#25968;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#35774;&#35745;&#36866;&#21512;&#32447;&#24615; transformer &#30340;&#32534;&#30721;&#26041;&#27861;&#30340;&#21407;&#21017;&#20173;&#28982;&#30740;&#31350;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#29616;&#26377;&#30340;&#32447;&#24615;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#24402;&#32467;&#20026;&#19968;&#31181;&#35268;&#33539;&#24418;&#24335;&#65292;&#24182;&#36827;&#19968;&#27493;&#36890;&#36807;&#37193;&#21464;&#25442;&#25552;&#20986;&#20102;&#19968;&#26063;&#32447;&#24615;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20844;&#24335;&#25512;&#23548;&#20986;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#24320;&#21457;&#20445;&#30041;&#32447;&#24615;&#26102;&#31354;&#22797;&#26434;&#24230;&#30340;&#26032;&#30340;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#12290;&#20511;&#21161;&#19981;&#21516;&#27169;&#22411;&#65292;&#25552;&#20986;&#30340;&#32447;&#24615;&#21270;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;LRPE&#65289;&#23478;&#26063;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relative positional encoding is widely used in vanilla and linear transformers to represent positional information. However, existing encoding methods of a vanilla transformer are not always directly applicable to a linear transformer, because the latter requires a decomposition of the query and key representations into separate kernel functions. Nevertheless, principles for designing encoding methods suitable for linear transformers remain understudied. In this work, we put together a variety of existing linear relative positional encoding approaches under a canonical form and further propose a family of linear relative positional encoding algorithms via unitary transformation. Our formulation leads to a principled framework that can be used to develop new relative positional encoding methods that preserve linear space-time complexity. Equipped with different models, the proposed linearized relative positional encoding (LRPE) family derives effective encoding for various applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;n-gram&#22256;&#24785;&#24230;&#30340;&#31616;&#21333;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#25991;&#26412;&#21521;&#37327;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#22312;&#35745;&#31639;&#26631;&#37327;&#22256;&#24785;&#24230;&#26102;&#30001;&#20110;&#20010;&#21035;&#26631;&#35760;&#30340;&#19981;&#22826;&#21487;&#33021;&#24615;&#23548;&#33268;&#27010;&#29575;&#30340;&#38477;&#20302;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#30041;&#25991;&#26412;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#30456;&#23545;&#22256;&#24785;&#24230;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.09255</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#35821;&#35328;&#27169;&#22411;&#21644;n-gram&#22256;&#24785;&#24230;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Text vectorization via transformer-based language models and n-gram perplexities. (arXiv:2307.09255v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09255
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;n-gram&#22256;&#24785;&#24230;&#30340;&#31616;&#21333;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#25991;&#26412;&#21521;&#37327;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#22312;&#35745;&#31639;&#26631;&#37327;&#22256;&#24785;&#24230;&#26102;&#30001;&#20110;&#20010;&#21035;&#26631;&#35760;&#30340;&#19981;&#22826;&#21487;&#33021;&#24615;&#23548;&#33268;&#27010;&#29575;&#30340;&#38477;&#20302;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#30041;&#25991;&#26412;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#30456;&#23545;&#22256;&#24785;&#24230;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25991;&#26412;&#30340;&#27010;&#29575;&#65288;&#21450;&#22256;&#24785;&#24230;&#65289;&#26159;&#22522;&#20110;&#21508;&#20010;&#26631;&#35760;&#30340;&#27010;&#29575;&#30340;&#20056;&#31215;&#35745;&#31639;&#30340;&#65292;&#22240;&#27492;&#21487;&#33021;&#20250;&#20986;&#29616;&#19968;&#20010;&#19981;&#22826;&#21487;&#33021;&#30340;&#26631;&#35760;&#26174;&#33879;&#38477;&#20302;&#19968;&#20123;&#21407;&#26412;&#39640;&#27010;&#29575;&#36755;&#20837;&#30340;&#27010;&#29575;&#65288;&#22686;&#21152;&#22256;&#24785;&#24230;&#65289;&#65292;&#21516;&#26102;&#21487;&#33021;&#34920;&#31034;&#19968;&#20010;&#31616;&#21333;&#30340;&#25171;&#23383;&#38169;&#35823;&#12290;&#21478;&#22806;&#65292;&#37492;&#20110;&#22256;&#24785;&#24230;&#26159;&#19968;&#20010;&#26631;&#37327;&#20540;&#65292;&#23427;&#25351;&#30340;&#26159;&#25972;&#20010;&#36755;&#20837;&#30340;&#27010;&#29575;&#20998;&#24067;&#20449;&#24687;&#22312;&#35745;&#31639;&#20013;&#20002;&#22833;&#20102;&#65288;&#19968;&#20010;&#30456;&#23545;&#36739;&#22909;&#30340;&#25991;&#26412;&#22914;&#26524;&#26377;&#19968;&#20010;&#19981;&#22826;&#21487;&#33021;&#30340;&#26631;&#35760;&#65292;&#20197;&#21450;&#27599;&#20010;&#26631;&#35760;&#31561;&#21487;&#33021;&#30340;&#21478;&#19968;&#20010;&#25991;&#26412;&#65292;&#23427;&#20204;&#21487;&#20197;&#20855;&#26377;&#30456;&#21516;&#30340;&#22256;&#24785;&#24230;&#20540;&#65289;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36739;&#38271;&#30340;&#25991;&#26412;&#12290;&#20316;&#20026;&#23545;&#26631;&#37327;&#22256;&#24785;&#24230;&#30340;&#26367;&#20195;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#22522;&#20110;&#36755;&#20837;&#20013;&#30340;n-gram&#22256;&#24785;&#24230;&#30340;&#21521;&#37327;&#20540;&#12290;&#36825;&#26679;&#30340;&#34920;&#31034;&#26041;&#27861;&#32771;&#34385;&#20102;&#21069;&#38754;&#25552;&#21040;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#32780;&#19981;&#26159;&#21807;&#19968;&#30340;&#20540;&#65292;&#35745;&#31639;&#20102;&#27599;&#20010;&#25991;&#26412;&#26631;&#35760;&#30340;&#30456;&#23545;&#22256;&#24785;&#24230;&#65292;
&lt;/p&gt;
&lt;p&gt;
As the probability (and thus perplexity) of a text is calculated based on the product of the probabilities of individual tokens, it may happen that one unlikely token significantly reduces the probability (i.e., increase the perplexity) of some otherwise highly probable input, while potentially representing a simple typographical error. Also, given that perplexity is a scalar value that refers to the entire input, information about the probability distribution within it is lost in the calculation (a relatively good text that has one unlikely token and another text in which each token is equally likely they can have the same perplexity value), especially for longer texts. As an alternative to scalar perplexity this research proposes a simple algorithm used to calculate vector values based on n-gram perplexities within the input. Such representations consider the previously mentioned aspects, and instead of a unique value, the relative perplexity of each text token is calculated, and the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;PAC&#31070;&#32463;&#39044;&#27979;&#38598;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#31181;&#35821;&#35328;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#22522;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;63&#65285;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09254</link><description>&lt;p&gt;
&#29992;&#20110;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;PAC&#31070;&#32463;&#39044;&#27979;&#38598;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models. (arXiv:2307.09254v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;PAC&#31070;&#32463;&#39044;&#27979;&#38598;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#31181;&#35821;&#35328;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#22522;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;63&#65285;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#22686;&#24378;&#27169;&#22411;&#21487;&#20449;&#24230;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#30001;&#20110;&#23545;&#29983;&#25104;&#34394;&#26500;&#20107;&#23454;&#30340;&#25285;&#24551;&#65292;&#26368;&#36817;&#20852;&#36215;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#29305;&#21035;&#24378;&#35843;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#39044;&#27979;&#38598;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;&#30340;&#26041;&#24335;&#37327;&#21270;GLM&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#39044;&#27979;&#38598;&#27169;&#22411;&#36890;&#36807;&#26631;&#37327;&#20540;&#21442;&#25968;&#21270;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#39044;&#27979;&#38598;&#65292;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20294;&#20173;&#28385;&#36275;PAC&#20445;&#35777;&#12290;&#36890;&#36807;&#22312;&#22235;&#31181;&#31867;&#22411;&#30340;&#35821;&#35328;&#25968;&#25454;&#38598;&#21644;&#20845;&#31181;&#31867;&#22411;&#30340;&#27169;&#22411;&#19978;&#23637;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#26631;&#20934;&#22522;&#20934;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;63&#65285;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty learning and quantification of models are crucial tasks to enhance the trustworthiness of the models. Importantly, the recent surge of generative language models (GLMs) emphasizes the need for reliable uncertainty quantification due to the concerns on generating hallucinated facts. In this paper, we propose to learn neural prediction set models that comes with the probably approximately correct (PAC) guarantee for quantifying the uncertainty of GLMs. Unlike existing prediction set models, which are parameterized by a scalar value, we propose to parameterize prediction sets via neural networks, which achieves more precise uncertainty quantification but still satisfies the PAC guarantee. We demonstrate the efficacy of our method on four types of language datasets and six types of models by showing that our method improves the quantified uncertainty by $63\%$ on average, compared to a standard baseline method.
&lt;/p&gt;</description></item><item><title>UniTabE&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#34920;&#26684;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#34920;&#26684;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#24182;&#20855;&#26377;&#23545;&#22810;&#26679;&#21270;&#19979;&#28216;&#24212;&#29992;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09249</link><description>&lt;p&gt;
UniTabE: &#38754;&#21521;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#34920;&#26684;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data. (arXiv:2307.09249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09249
&lt;/p&gt;
&lt;p&gt;
UniTabE&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#34920;&#26684;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#34920;&#26684;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#24182;&#20855;&#26377;&#23545;&#22810;&#26679;&#21270;&#19979;&#28216;&#24212;&#29992;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#26126;&#35777;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31361;&#30772;&#24615;&#24433;&#21709;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#23041;&#21147;&#25193;&#23637;&#21040;&#20256;&#32479;&#34987;&#24573;&#35270;&#30340;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#65292;&#35813;&#39046;&#22495;&#30001;&#20110;&#19981;&#21516;&#20219;&#21153;&#22266;&#26377;&#30340;&#20247;&#22810;&#34920;&#26684;&#27169;&#24335;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#24037;&#20316;&#30340;&#20027;&#35201;&#30740;&#31350;&#38382;&#39064;&#22260;&#32469;&#24322;&#26500;&#34920;&#26684;&#32467;&#26500;&#30340;&#36866;&#24212;&#24615;&#12289;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#21327;&#35758;&#30340;&#24314;&#31435;&#12289;&#23398;&#21040;&#30340;&#30693;&#35782;&#22312;&#20219;&#21153;&#20043;&#38388;&#30340;&#27867;&#21270;&#21644;&#21487;&#20256;&#36882;&#24615;&#12289;&#23545;&#22810;&#26679;&#21270;&#19979;&#28216;&#24212;&#29992;&#30340;&#36866;&#24212;&#24615;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#22686;&#37327;&#21015;&#30340;&#32435;&#20837;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UniTabE&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20197;&#19968;&#33268;&#30340;&#26041;&#24335;&#22788;&#29702;&#34920;&#26684;&#65292;&#25670;&#33073;&#20102;&#29305;&#23450;&#34920;&#26684;&#32467;&#26500;&#24378;&#21152;&#30340;&#32422;&#26463;&#12290;UniTabE&#30340;&#26680;&#24515;&#27010;&#24565;&#26159;&#23545;&#27599;&#20010;&#22522;&#26412;&#34920;&#26684;&#36827;&#34892;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks. This study seeks to extend the power of pretraining methodologies to tabular data, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks. The primary research questions underpinning this work revolve around the adaptation to heterogeneous table structures, the establishment of a universal pretraining protocol for tabular data, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time. In response to these challenges, we introduce UniTabE, a pioneering method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures. UniTabE's core concept relies on representing each basic tab
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#24773;&#24863;&#20998;&#26512;&#21644;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#65292;&#20197;&#25506;&#27979;&#23545;&#27531;&#38556;&#20154;&#22763;&#30340;&#26126;&#26174;&#20559;&#35265;&#12290;&#30740;&#31350;&#20351;&#29992;&#20559;&#35265;&#35782;&#21035;&#26694;&#26550;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#23545;&#35805;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#27979;&#35797;&#35821;&#26009;&#24211;&#26469;&#37327;&#21270;&#26126;&#26174;&#30340;&#27531;&#38556;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25152;&#30740;&#31350;&#30340;&#27169;&#22411;&#22343;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2307.09209</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;&#27531;&#38556;&#20027;&#20041;&#65306;&#25506;&#32034;&#24773;&#24863;&#20998;&#26512;&#21644;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#26126;&#26174;&#27531;&#38556;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models. (arXiv:2307.09209v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09209
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#24773;&#24863;&#20998;&#26512;&#21644;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#65292;&#20197;&#25506;&#27979;&#23545;&#27531;&#38556;&#20154;&#22763;&#30340;&#26126;&#26174;&#20559;&#35265;&#12290;&#30740;&#31350;&#20351;&#29992;&#20559;&#35265;&#35782;&#21035;&#26694;&#26550;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#23545;&#35805;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#27979;&#35797;&#35821;&#26009;&#24211;&#26469;&#37327;&#21270;&#26126;&#26174;&#30340;&#27531;&#38556;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25152;&#30740;&#31350;&#30340;&#27169;&#22411;&#22343;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#24773;&#24863;&#20998;&#26512;&#21644;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#65292;&#20197;&#26816;&#27979;&#23545;&#27531;&#38556;&#20154;&#22763;&#30340;&#26126;&#26174;&#20559;&#35265;&#12290;&#25105;&#20204;&#37319;&#29992;&#25200;&#21160;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#20559;&#35265;&#35782;&#21035;&#26694;&#26550;&#65292;&#30740;&#31350;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#19982;&#27531;&#38556;&#20154;&#22763;&#30456;&#20851;&#30340;&#23545;&#35805;&#65292;&#29305;&#21035;&#26159;Twitter&#21644;Reddit&#65292;&#22312;&#30495;&#23454;&#31038;&#20132;&#29615;&#22659;&#20013;&#20102;&#35299;&#27531;&#38556;&#20559;&#35265;&#26159;&#22914;&#20309;&#20256;&#25773;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#8220;&#24773;&#24863;&#20013;&#30340;&#20559;&#35265;&#35782;&#21035;&#27979;&#35797;&#8221;&#65288;BITS&#65289;&#35821;&#26009;&#24211;&#65292;&#20197;&#37327;&#21270;&#20219;&#20309;&#24773;&#24863;&#20998;&#26512;&#21644;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#26126;&#26174;&#27531;&#38556;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20351;&#29992;BITS&#25581;&#31034;&#20102;&#22235;&#31181;&#24320;&#25918;&#30340;AIaaS&#65288;AI&#21363;&#26381;&#21153;&#65289;&#24773;&#24863;&#20998;&#26512;&#24037;&#20855;&#65288;TextBlob&#65292;VADER&#65292;Google Cloud Natural Language API&#65292;DistilBERT&#65289;&#21644;&#20004;&#31181;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#65288;&#20004;&#20010;&#29256;&#26412;&#30340;Toxic-BERT&#65289;&#20013;&#23384;&#22312;&#26174;&#30528;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze sentiment analysis and toxicity detection models to detect the presence of explicit bias against people with disability (PWD). We employ the bias identification framework of Perturbation Sensitivity Analysis to examine conversations related to PWD on social media platforms, specifically Twitter and Reddit, in order to gain insight into how disability bias is disseminated in real-world social settings. We then create the \textit{Bias Identification Test in Sentiment} (BITS) corpus to quantify explicit disability bias in any sentiment analysis and toxicity detection models. Our study utilizes BITS to uncover significant biases in four open AIaaS (AI as a Service) sentiment analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API, DistilBERT and two toxicity detection models, namely two versions of Toxic-BERT. Our findings indicate that all of these models exhibit statistically significant explicit bias against PWD.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20197;GPT-2&#21644;GPT-3.5&#20026;&#20363;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#28145;&#20837;&#30340;&#23450;&#37327;&#20998;&#26512;&#25581;&#31034;&#20102;&#23384;&#22312;&#30340;&#24615;&#21035;&#21270;&#35789;&#35821;&#20851;&#32852;&#12289;&#35821;&#35328;&#20351;&#29992;&#21644;&#20559;&#35265;&#21465;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#24615;&#21035;&#20559;&#35265;&#21487;&#33021;&#23545;&#31038;&#20250;&#35748;&#30693;&#20135;&#29983;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.09162</link><description>&lt;p&gt;
&#25581;&#31034;&#22312;LLM&#20013;&#32844;&#19994;&#24615;&#21035;&#20559;&#35265;&#65306;&#20998;&#26512;&#21644;&#35299;&#20915;&#31038;&#20250;&#23398;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and Addressing Sociological Implications. (arXiv:2307.09162v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20197;GPT-2&#21644;GPT-3.5&#20026;&#20363;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#28145;&#20837;&#30340;&#23450;&#37327;&#20998;&#26512;&#25581;&#31034;&#20102;&#23384;&#22312;&#30340;&#24615;&#21035;&#21270;&#35789;&#35821;&#20851;&#32852;&#12289;&#35821;&#35328;&#20351;&#29992;&#21644;&#20559;&#35265;&#21465;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#24615;&#21035;&#20559;&#35265;&#21487;&#33021;&#23545;&#31038;&#20250;&#35748;&#30693;&#20135;&#29983;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#23545;&#31038;&#20250;&#35748;&#30693;&#21644;&#20559;&#35265;&#20135;&#29983;&#24433;&#21709;&#12290;&#36825;&#31687;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#37325;&#28857;&#27604;&#36739;&#20102;GPT-2&#21644;GPT-3.5&#36825;&#20123;&#33879;&#21517;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#24433;&#21709;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#35813;&#30740;&#31350;&#32771;&#23519;&#20102;&#29616;&#26377;&#20851;&#20110;AI&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#30740;&#31350;&#65292;&#24182;&#30830;&#23450;&#20102;&#24403;&#21069;&#30693;&#35782;&#30340;&#31354;&#30333;&#12290;&#30740;&#31350;&#26041;&#27861;&#21253;&#25324;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;GPT-2&#21644;GPT-3.5&#30340;&#25968;&#25454;&#65292;&#24182;&#36816;&#29992;&#28145;&#20837;&#30340;&#23450;&#37327;&#20998;&#26512;&#25216;&#26415;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#23384;&#22312;&#30340;&#20855;&#26377;&#24615;&#21035;&#33394;&#24425;&#30340;&#35789;&#35821;&#20851;&#32852;&#12289;&#35821;&#35328;&#20351;&#29992;&#21644;&#20559;&#35265;&#21465;&#36848;&#12290;&#35752;&#35770;&#37096;&#20998;&#25506;&#35752;&#20102;&#24615;&#21035;&#20559;&#35265;&#30340;&#20262;&#29702;&#24433;&#21709;&#20197;&#21450;&#20854;&#23545;&#31038;&#20250;&#35748;&#30693;&#30340;&#28508;&#22312;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gender bias in artificial intelligence (AI) and natural language processing has garnered significant attention due to its potential impact on societal perceptions and biases. This research paper aims to analyze gender bias in Large Language Models (LLMs) with a focus on multiple comparisons between GPT-2 and GPT-3.5, some prominent language models, to better understand its implications. Through a comprehensive literature review, the study examines existing research on gender bias in AI language models and identifies gaps in the current knowledge. The methodology involves collecting and preprocessing data from GPT-2 and GPT-3.5, and employing in-depth quantitative analysis techniques to evaluate gender bias in the generated text. The findings shed light on gendered word associations, language usage, and biased narratives present in the outputs of these Large Language Models. The discussion explores the ethical implications of gender bias and its potential consequences on social percepti
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#39044;&#35757;&#32451;&#21477;&#23376;&#23884;&#20837;&#21644;&#23567;&#27880;&#24847;&#21147;&#23618;&#36827;&#34892;&#38271;&#25991;&#26412;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09084</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#21477;&#23376;&#23884;&#20837;&#30340;&#38271;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Attention over pre-trained Sentence Embeddings for Long Document Classification. (arXiv:2307.09084v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09084
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#21477;&#23376;&#23884;&#20837;&#21644;&#23567;&#27880;&#24847;&#21147;&#23618;&#36827;&#34892;&#38271;&#25991;&#26412;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;transformer&#27169;&#22411;&#26159;&#22823;&#22810;&#25968;NLP&#20219;&#21153;&#20013;&#30340;&#40664;&#35748;&#27169;&#22411;&#65292;&#20294;&#30001;&#20110;&#20854;&#20851;&#20110;&#20196;&#29260;&#25968;&#37327;&#30340;&#20108;&#27425;&#27880;&#24847;&#21147;&#22797;&#26434;&#24230;&#65292;&#23427;&#20204;&#36890;&#24120;&#34987;&#38480;&#21046;&#22312;&#30701;&#24207;&#21015;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#19968;&#20123;&#23581;&#35797;&#65292;&#21253;&#25324;&#36890;&#36807;&#20943;&#23569;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#25104;&#26412;&#65292;&#36890;&#36807;&#23545;&#36739;&#23567;&#30340;&#24207;&#21015;&#24314;&#27169;&#24182;&#36890;&#36807;&#36882;&#24402;&#26426;&#21046;&#25110;&#20351;&#29992;&#26032;&#30340;transformer&#27169;&#22411;&#36827;&#34892;&#32467;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#21477;&#23376;&#23884;&#20837;&#26469;&#20174;&#21508;&#20010;&#21477;&#23376;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#23884;&#20837;&#24320;&#22987;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#20010;&#38543;&#25991;&#26723;&#38271;&#24230;&#32447;&#24615;&#25193;&#23637;&#30340;&#23567;&#27880;&#24847;&#21147;&#23618;&#23558;&#23427;&#20204;&#32452;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#26631;&#20934;&#30340;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#25253;&#21578;&#20102;&#36825;&#31181;&#31616;&#21333;&#26550;&#26500;&#30340;&#32467;&#26524;&#12290;&#19982;&#20351;&#29992;&#26631;&#20934;&#24494;&#35843;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#65288;&#21363;&#20351;&#22312;&#36825;&#31181;&#37197;&#32622;&#19979;&#27809;&#26377;&#26126;&#30830;&#30340;&#26368;&#20339;&#27169;&#22411;&#65289;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#35813;&#26041;&#27861;&#21487;&#23454;&#29616;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite being the current de-facto models in most NLP tasks, transformers are often limited to short sequences due to their quadratic attention complexity on the number of tokens. Several attempts to address this issue were studied, either by reducing the cost of the self-attention computation or by modeling smaller sequences and combining them through a recurrence mechanism or using a new transformer model. In this paper, we suggest to take advantage of pre-trained sentence transformers to start from semantically meaningful embeddings of the individual sentences, and then combine them through a small attention layer that scales linearly with the document length. We report the results obtained by this simple architecture on three standard document classification datasets. When compared with the current state-of-the-art models using standard fine-tuning, the studied method obtains competitive results (even if there is no clear best model in this configuration). We also showcase that the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.09059</link><description>&lt;p&gt;
&#25991;&#23383;&#24819;&#35937;&#30340;&#37322;&#25918;&#65306;&#36890;&#36807;&#25506;&#32034;&#25991;&#23383;&#30340;&#21147;&#37327;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words. (arXiv:2307.09059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#30340;&#30446;&#26631;&#26159;&#20174;&#22823;&#22411;&#22270;&#24211;&#20013;&#26816;&#32034;&#19982;&#32473;&#23450;&#25991;&#26412;&#25551;&#36848;&#30456;&#21305;&#37197;&#30340;&#20154;&#29289;&#22270;&#20687;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#20449;&#24687;&#34920;&#31034;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#25991;&#26412;&#27169;&#24577;&#36890;&#36807;&#35789;&#27719;&#21644;&#35821;&#27861;&#32467;&#26500;&#20256;&#36882;&#25277;&#35937;&#21644;&#31934;&#30830;&#30340;&#20449;&#24687;&#65292;&#32780;&#35270;&#35273;&#27169;&#24577;&#36890;&#36807;&#22270;&#20687;&#20256;&#36882;&#20855;&#20307;&#21644;&#30452;&#35266;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#25991;&#23383;&#34920;&#31034;&#30340;&#34920;&#36798;&#21147;&#65292;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#22270;&#20687;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#21477;&#23376;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#37322;&#25918;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#20154;&#29289;&#26816;&#32034;&#20013;&#30340;&#25991;&#23383;&#24819;&#35937;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#20840;&#38754;CLIP&#27169;&#22411;&#20316;&#20026;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21452;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#20808;&#21069;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Text-to-image person retrieval is to retrieve person images from a large gallery that match the given textual descriptions. The main challenge of this task lies in the significant differences in information representation between the visual and textual modalities. The textual modality conveys abstract and precise information through vocabulary and grammatical structures, while the visual modality conveys concrete and intuitive information through images. To fully leverage the expressive power of textual representations, it is essential to accurately map abstract textual descriptions to specific images.  To address this issue, we propose a novel framework to Unleash the Imagination of Text (UIT) in text-to-image person retrieval, aiming to fully explore the power of words in sentences. Specifically, the framework employs the pre-trained full CLIP model as a dual encoder for the images and texts , taking advantage of prior cross-modal alignment knowledge. The Text-guided Imag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#21327;&#20316;&#23545;&#35805;&#31995;&#32479;&#20013;&#23545;&#35805;&#31649;&#29702;&#30340;&#28436;&#21464;&#65292;&#24182;&#20998;&#26512;&#20102;&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#21327;&#20316;&#23545;&#35805;&#31649;&#29702;&#30340;&#26368;&#26032;&#24037;&#20316;&#65292;&#26088;&#22312;&#20026;&#26410;&#26469;&#22312;&#21327;&#20316;&#23545;&#35805;&#31649;&#29702;&#39046;&#22495;&#30340;&#36827;&#23637;&#25552;&#20379;&#22522;&#30784;&#32972;&#26223;&#12290;</title><link>http://arxiv.org/abs/2307.09021</link><description>&lt;p&gt;
&#26397;&#30528;&#21327;&#20316;&#23545;&#35805;&#31649;&#29702;&#30340;&#31070;&#32463;&#26102;&#20195;&#21457;&#23637;&#65306;&#19968;&#39033;&#25991;&#29486;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Towards a Neural Era in Dialogue Management for Collaboration: A Literature Survey. (arXiv:2307.09021v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#21327;&#20316;&#23545;&#35805;&#31995;&#32479;&#20013;&#23545;&#35805;&#31649;&#29702;&#30340;&#28436;&#21464;&#65292;&#24182;&#20998;&#26512;&#20102;&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#21327;&#20316;&#23545;&#35805;&#31649;&#29702;&#30340;&#26368;&#26032;&#24037;&#20316;&#65292;&#26088;&#22312;&#20026;&#26410;&#26469;&#22312;&#21327;&#20316;&#23545;&#35805;&#31649;&#29702;&#39046;&#22495;&#30340;&#36827;&#23637;&#25552;&#20379;&#22522;&#30784;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#35805;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21327;&#20316;&#21487;&#20197;&#24443;&#24213;&#25913;&#21464;&#21327;&#20316;&#38382;&#39064;&#35299;&#20915;&#12289;&#21019;&#36896;&#24615;&#25506;&#32034;&#21644;&#31038;&#20132;&#25903;&#25345;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24517;&#39035;&#24320;&#21457;&#29087;&#24713;&#25216;&#24039;&#22914;&#21327;&#21830;&#12289;&#36981;&#24490;&#25351;&#31034;&#12289;&#24314;&#31435;&#20849;&#21516;&#22522;&#30784;&#21644;&#25512;&#36827;&#20849;&#20139;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#20195;&#29702;&#12290;&#26412;&#25991;&#23545;&#21327;&#20316;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#23545;&#35805;&#31649;&#29702;&#33539;&#20363;&#30340;&#28436;&#21464;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#20174;&#20256;&#32479;&#30340;&#25163;&#24037;&#21046;&#20316;&#21644;&#22522;&#20110;&#20449;&#24687;&#29366;&#24577;&#30340;&#26041;&#27861;&#21040;&#21463;&#20154;&#24037;&#26234;&#33021;&#35268;&#21010;&#21551;&#21457;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#23427;&#36716;&#21521;&#24403;&#20195;&#30340;&#25968;&#25454;&#39537;&#21160;&#23545;&#35805;&#31649;&#29702;&#25216;&#26415;&#65292;&#36825;&#20123;&#25216;&#26415;&#35797;&#22270;&#23558;&#28145;&#24230;&#23398;&#20064;&#22312;&#22635;&#34920;&#21644;&#24320;&#25918;&#39046;&#22495;&#35774;&#32622;&#20013;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#21327;&#20316;&#29615;&#22659;&#12290;&#26412;&#25991;&#36824;&#20998;&#26512;&#20102;&#19968;&#32452;&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#21327;&#20316;&#23545;&#35805;&#31649;&#29702;&#30340;&#26368;&#36817;&#24037;&#20316;&#65292;&#24182;&#31361;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#27969;&#34892;&#36235;&#21183;&#12290;&#36825;&#39033;&#35843;&#26597;&#24076;&#26395;&#20026;&#21327;&#20316;&#23545;&#35805;&#31649;&#29702;&#39046;&#22495;&#30340;&#26410;&#26469;&#36827;&#23637;&#25552;&#20379;&#22522;&#30784;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue-based human-AI collaboration can revolutionize collaborative problem-solving, creative exploration, and social support. To realize this goal, the development of automated agents proficient in skills such as negotiating, following instructions, establishing common ground, and progressing shared tasks is essential. This survey begins by reviewing the evolution of dialogue management paradigms in collaborative dialogue systems, from traditional handcrafted and information-state based methods to AI planning-inspired approaches. It then shifts focus to contemporary data-driven dialogue management techniques, which seek to transfer deep learning successes from form-filling and open-domain settings to collaborative contexts. The paper proceeds to analyze a selected set of recent works that apply neural approaches to collaborative dialogue management, spotlighting prevailing trends in the field. This survey hopes to provide foundational background for future advancements in collaborat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;KeyBERT&#21644;SNA&#26041;&#27861;&#65292;&#25506;&#32034;&#24037;&#31243;&#23398;&#29983;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25919;&#31574;&#30340;&#25509;&#21463;&#24230;&#12290;&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#20998;&#26512;&#30740;&#31350;&#29983;&#30340;&#24847;&#35265;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#32456;&#31471;&#29992;&#25143;&#23545;&#36825;&#20123;&#25919;&#31574;&#25509;&#21463;&#24230;&#30340;&#20998;&#26512;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2307.09014</link><description>&lt;p&gt;
&#20351;&#29992;KeyBERT&#21644;SNA&#25506;&#32034;&#38024;&#23545;&#24037;&#31243;&#23398;&#29983;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25919;&#31574;&#25509;&#21463;&#24230;
&lt;/p&gt;
&lt;p&gt;
Exploring acceptance of autonomous vehicle policies using KeyBERT and SNA: Targeting engineering students. (arXiv:2307.09014v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;KeyBERT&#21644;SNA&#26041;&#27861;&#65292;&#25506;&#32034;&#24037;&#31243;&#23398;&#29983;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25919;&#31574;&#30340;&#25509;&#21463;&#24230;&#12290;&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#20998;&#26512;&#30740;&#31350;&#29983;&#30340;&#24847;&#35265;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#32456;&#31471;&#29992;&#25143;&#23545;&#36825;&#20123;&#25919;&#31574;&#25509;&#21463;&#24230;&#30340;&#20998;&#26512;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#25913;&#36827;&#30340;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#65292;&#25506;&#32034;&#29992;&#25143;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25919;&#31574;&#30340;&#25509;&#21463;&#24230;&#12290;&#36817;&#24180;&#26469;&#65292;&#38889;&#22269;&#25919;&#31574;&#21046;&#23450;&#32773;&#23558;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65288;ADC&#65289;&#21644;&#33258;&#21160;&#39550;&#39542;&#26426;&#22120;&#20154;&#65288;ADR&#65289;&#35270;&#20026;&#19979;&#19968;&#20195;&#20132;&#36890;&#24037;&#20855;&#65292;&#21487;&#20197;&#38477;&#20302;&#20056;&#23458;&#21644;&#36135;&#29289;&#36816;&#36755;&#25104;&#26412;&#12290;&#20182;&#20204;&#25903;&#25345;&#20026;ADC&#24314;&#35774;V2I&#21644;V2V&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#35748;&#35782;&#21040;ADR&#31561;&#21516;&#20110;&#34892;&#20154;&#65292;&#20197;&#20419;&#36827;&#20854;&#22312;&#20154;&#34892;&#36947;&#19978;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#22635;&#34917;&#32456;&#31471;&#29992;&#25143;&#23545;&#36825;&#20123;&#25919;&#31574;&#25509;&#21463;&#24230;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#30340;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#23558;&#20004;&#31181;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#24212;&#29992;&#20110;&#24037;&#19994;&#12289;&#26426;&#26800;&#21644;&#30005;&#23376;-&#30005;&#27668;-&#35745;&#31639;&#26426;&#39046;&#22495;&#30340;&#30740;&#31350;&#29983;&#24847;&#35265;&#12290;&#19968;&#31181;&#26159;&#22522;&#20110;TF-IWF&#21644;Dice&#31995;&#25968;&#30340;&#20849;&#29616;&#32593;&#32476;&#20998;&#26512;&#65288;CNA&#65289;&#65292;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#33021;&#22815;&#20197;&#19978;&#19979;&#25991;&#26041;&#24335;&#34920;&#31034;&#24847;&#35265;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#24037;&#20855;KeyBERT&#30340;&#19978;&#19979;&#25991;&#35821;&#20041;&#32593;&#32476;&#20998;&#26512;&#65288;C-SNA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to explore user acceptance of Autonomous Vehicle (AV) policies with improved text-mining methods. Recently, South Korean policymakers have viewed Autonomous Driving Car (ADC) and Autonomous Driving Robot (ADR) as next-generation means of transportation that will reduce the cost of transporting passengers and goods. They support the construction of V2I and V2V communication infrastructures for ADC and recognize that ADR is equivalent to pedestrians to promote its deployment into sidewalks. To fill the gap where end-user acceptance of these policies is not well considered, this study applied two text-mining methods to the comments of graduate students in the fields of Industrial, Mechanical, and Electronics-Electrical-Computer. One is the Co-occurrence Network Analysis (CNA) based on TF-IWF and Dice coefficient, and the other is the Contextual Semantic Network Analysis (C-SNA) based on both KeyBERT, which extracts keywords that contextually represent the comments, and dou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#19978;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#21464;&#21270;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09009</link><description>&lt;p&gt;
ChatGPT&#30340;&#34892;&#20026;&#38543;&#26102;&#38388;&#21464;&#21270;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#19978;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#21464;&#21270;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT-3.5&#21644;GPT-4&#26159;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#26356;&#26032;&#26159;&#19981;&#36879;&#26126;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;GPT-3.5&#21644;GPT-4&#30340;2023&#24180;3&#26376;&#21644;2023&#24180;6&#26376;&#29256;&#26412;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28041;&#21450;&#22235;&#39033;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;1&#65289;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;2&#65289;&#22238;&#31572;&#25935;&#24863;/&#21361;&#38505;&#38382;&#39064;&#65292;3&#65289;&#29983;&#25104;&#20195;&#30721;&#21644;4&#65289;&#35270;&#35273;&#25512;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-3.5&#21644;GPT-4&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#22312;&#26102;&#38388;&#19978;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;GPT-4&#65288;2023&#24180;3&#26376;&#65289;&#22312;&#35782;&#21035;&#36136;&#25968;&#26041;&#38754;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65288;&#20934;&#30830;&#29575;&#20026;97.6%&#65289;&#65292;&#20294;GPT-4&#65288;2023&#24180;6&#26376;&#65289;&#22312;&#30456;&#21516;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#38750;&#24120;&#24046;&#65288;&#20934;&#30830;&#29575;&#20026;2.4%&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;GPT-3.5&#65288;2023&#24180;6&#26376;&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#27604;GPT-3.5&#65288;2023&#24180;3&#26376;&#65289;&#35201;&#22909;&#24471;&#22810;&#12290;GPT-4&#22312;6&#26376;&#20221;&#23545;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#30340;&#24847;&#24895;&#36739;3&#26376;&#20221;&#35201;&#20302;&#65292;&#32780;&#26080;&#35770;&#26159;GPT-4&#36824;&#26159;GPT-3.5&#22312;6&#26376;&#20221;&#30340;&#20195;&#30721;&#29983;&#25104;&#20013;&#37117;&#26377;&#26356;&#22810;&#30340;&#26684;&#24335;&#38169;&#35823;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#30456;&#21516;LLM&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#36739;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#37325;&#22823;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;ChatGPT&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#21644;&#25340;&#20889;&#26816;&#26597;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#24182;&#19981;&#29702;&#24819;&#12290;</title><link>http://arxiv.org/abs/2307.09007</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#20013;&#30340;&#65288;&#26080;&#65289;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the (In)Effectiveness of Large Language Models for Chinese Text Correction. (arXiv:2307.09007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;ChatGPT&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#21644;&#25340;&#20889;&#26816;&#26597;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#24182;&#19981;&#29702;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21644;&#36827;&#27493;&#20196;&#25972;&#20010;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#24778;&#21497;&#19981;&#24050;&#12290;&#20316;&#20026;LLMs&#30340;&#26480;&#20986;&#20195;&#34920;&#21644;&#24341;&#21457;&#20102;&#23545;LLMs&#30740;&#31350;&#28909;&#28526;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;ChatGPT&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#30740;&#31350;&#20854;&#22312;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;&#22312;&#23545;ChatGPT&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24778;&#20154;&#34920;&#29616;&#24863;&#21040;&#24778;&#21497;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;ChatGPT&#36824;&#20855;&#26377;&#20986;&#33394;&#30340;&#22810;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#20363;&#22914;&#20013;&#25991;&#12290;&#20026;&#20102;&#25506;&#32034;ChatGPT&#30340;&#20013;&#25991;&#22788;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#36825;&#20010;&#22522;&#30784;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20013;&#25991;NLP&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#65288;CGEC&#65289;&#21644;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#65288;CSC&#65289;&#36825;&#20004;&#20010;&#20027;&#35201;&#30340;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#22330;&#26223;&#19978;&#35780;&#20272;ChatGPT&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20998;&#26512;&#21644;&#19982;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#24494;&#35843;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#21457;&#29616;ChatGPT&#30340;&#24615;&#33021;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the development and progress of Large Language Models (LLMs) have amazed the entire Artificial Intelligence community. As an outstanding representative of LLMs and the foundation model that set off this wave of research on LLMs, ChatGPT has attracted more and more researchers to study its capabilities and performance on various downstream Natural Language Processing (NLP) tasks. While marveling at ChatGPT's incredible performance on kinds of tasks, we notice that ChatGPT also has excellent multilingual processing capabilities, such as Chinese. To explore the Chinese processing ability of ChatGPT, we focus on Chinese Text Correction, a fundamental and challenging Chinese NLP task. Specifically, we evaluate ChatGPT on the Chinese Grammatical Error Correction (CGEC) and Chinese Spelling Check (CSC) tasks, which are two main Chinese Text Correction scenarios. From extensive analyses and comparisons with previous state-of-the-art fine-tuned models, we empirically find that the Cha
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeCoLe&#30340;&#20462;&#21098;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#26631;&#31614;&#20559;&#35265;&#38382;&#39064;&#12290;&#30740;&#31350;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.08945</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#32806;&#32622;&#20449;&#23398;&#20064;&#26469;&#20943;&#32531;&#26631;&#31614;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Label Bias via Decoupled Confident Learning. (arXiv:2307.08945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08945
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeCoLe&#30340;&#20462;&#21098;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#26631;&#31614;&#20559;&#35265;&#38382;&#39064;&#12290;&#30740;&#31350;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#25285;&#24551;&#22686;&#21152;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#20943;&#36731;&#31639;&#27861;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20551;&#35774;&#35757;&#32451;&#25968;&#25454;&#20013;&#35266;&#23519;&#21040;&#30340;&#26631;&#31614;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#22240;&#20026;&#26631;&#31614;&#20559;&#35265;&#22312;&#21253;&#25324;&#21307;&#30103;&#12289;&#25307;&#32856;&#21644;&#20869;&#23481;&#23457;&#26680;&#22312;&#20869;&#30340;&#37325;&#35201;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#29305;&#21035;&#26159;&#65292;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#23481;&#26131;&#24102;&#26377;&#31038;&#20250;&#20559;&#35265;&#12290;&#34429;&#28982;&#26631;&#31614;&#20559;&#35265;&#30340;&#23384;&#22312;&#24050;&#32463;&#22312;&#27010;&#24565;&#19978;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#20294;&#32570;&#20047;&#24212;&#23545;&#27492;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#21098;&#26041;&#27861;&#8212;&#8212;&#35299;&#32806;&#32622;&#20449;&#23398;&#20064;&#65288;DeCoLe&#65289;&#65292;&#19987;&#38376;&#35774;&#35745;&#26469;&#20943;&#36731;&#26631;&#31614;&#20559;&#35265;&#12290;&#22312;&#28436;&#31034;&#20102;&#20854;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#21518;&#65292;&#25105;&#20204;&#23558;DeCoLe&#24212;&#29992;&#20110;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#34987;&#35748;&#20026;&#26159;&#37325;&#35201;&#25361;&#25112;&#30340;&#39046;&#22495;&#65292;&#24182;&#23637;&#31034;&#20854;&#25104;&#21151;&#35782;&#21035;&#20986;&#20559;&#35265;&#26631;&#31614;&#24182;&#36229;&#36234;&#31454;&#20105;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Growing concerns regarding algorithmic fairness have led to a surge in methodologies to mitigate algorithmic bias. However, such methodologies largely assume that observed labels in training data are correct. This is problematic because bias in labels is pervasive across important domains, including healthcare, hiring, and content moderation. In particular, human-generated labels are prone to encoding societal biases. While the presence of labeling bias has been discussed conceptually, there is a lack of methodologies to address this problem. We propose a pruning method -- Decoupled Confident Learning (DeCoLe) -- specifically designed to mitigate label bias. After illustrating its performance on a synthetic dataset, we apply DeCoLe in the context of hate speech detection, where label bias has been recognized as an important challenge, and show that it successfully identifies biased labels and outperforms competing approaches.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#36817;&#20284;MLP&#34701;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#30340;&#21516;&#26102;&#20445;&#25345;&#36739;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08941</link><description>&lt;p&gt;
NTK-&#36817;&#20284;MLP&#34701;&#21512;&#29992;&#20110;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning. (arXiv:2307.08941v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08941
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#36817;&#20284;MLP&#34701;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#30340;&#21516;&#26102;&#20445;&#25345;&#36739;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#24050;&#25104;&#20026;&#20027;&#35201;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#24494;&#35843;PLM&#21644;&#36827;&#34892;&#25512;&#29702;&#20063;&#26159;&#26114;&#36149;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#33021;&#21147;&#36739;&#20302;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#19968;&#20123;&#36890;&#29992;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#37327;&#21270;&#21644;&#33976;&#39311;&#65289;&#26469;&#20943;&#23569;PLM&#24494;&#35843;&#30340;&#35745;&#31639;/&#23384;&#20648;&#24320;&#38144;&#65292;&#20294;&#24456;&#23569;&#26377;&#19968;&#27425;&#24615;&#21387;&#32553;&#25216;&#26415;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#27169;&#22359;&#20013;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;NTK&#36817;&#20284;MLP&#34701;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;PLM&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;MLP&#37325;&#26032;&#35270;&#20026;&#19968;&#26463;&#23376;MLP&#65292;&#24182;&#23558;&#23427;&#20204;&#32858;&#31867;&#20026;&#32473;&#23450;&#25968;&#37327;&#30340;&#36136;&#24515;&#65292;&#28982;&#21518;&#23558;&#20854;&#24674;&#22797;&#20026;&#21387;&#32553;&#30340;MLP&#65292;&#24182;&#24847;&#22806;&#22320;&#26174;&#31034;&#20986;&#23545;&#21407;&#22987;PLM&#30340;NTK&#36827;&#34892;&#33391;&#22909;&#36817;&#20284;&#30340;&#25928;&#26524;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#20197;&#39564;&#35777;PLM&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning a pre-trained language model (PLM) emerges as the predominant strategy in many natural language processing applications. However, even fine-tuning the PLMs and doing inference are expensive, especially on edge devices with low computing power. Some general approaches (e.g. quantization and distillation) have been widely studied to reduce the compute/memory of PLM fine-tuning, while very few one-shot compression techniques are explored. In this paper, we investigate the neural tangent kernel (NTK)--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight PLM through NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a bundle of sub-MLPs, and cluster them into a given number of centroids, which can then be restored as a compressed MLP and surprisingly shown to well approximate the NTK of the original PLM. Extensive experiments of PLM fine-tuning on both natural l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;MRC&#20219;&#21153;&#20998;&#20026;&#20004;&#20010;&#29420;&#31435;&#30340;&#38454;&#27573;&#26469;&#25945;&#23548;&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26723;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08931</link><description>&lt;p&gt;
&#25945;&#27169;&#22411;&#22312;&#29702;&#35299;&#25991;&#26723;&#21518;&#22238;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Teach model to answer questions after comprehending the document. (arXiv:2307.08931v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;MRC&#20219;&#21153;&#20998;&#20026;&#20004;&#20010;&#29420;&#31435;&#30340;&#38454;&#27573;&#26469;&#25945;&#23548;&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26723;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#36873;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;(MRC)&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25193;&#23637;&#65292;&#38656;&#35201;&#29702;&#35299;&#32473;&#23450;&#25991;&#26412;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#21644;&#36923;&#36753;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#20256;&#32479;&#19978;&#65292;MRC&#20219;&#21153;&#34987;&#35270;&#20026;&#26681;&#25454;&#32473;&#23450;&#25991;&#26412;&#22238;&#31572;&#38382;&#39064;&#30340;&#36807;&#31243;&#12290;&#36825;&#31181;&#21333;&#38454;&#27573;&#26041;&#27861;&#24448;&#24448;&#20351;&#32593;&#32476;&#19987;&#27880;&#20110;&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#21487;&#33021;&#24573;&#35270;&#20102;&#23545;&#25991;&#26412;&#26412;&#36523;&#30340;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#27969;&#34892;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#36739;&#38271;&#30340;&#25991;&#26412;&#26102;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;MRC&#20219;&#21153;&#20998;&#20026;&#20004;&#20010;&#29420;&#31435;&#30340;&#38454;&#27573;&#26469;&#25945;&#23548;&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26723;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#23398;&#29983;&#27169;&#22411;&#37197;&#22791;&#25105;&#20204;&#30340;&#26041;&#27861;&#26102;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-choice Machine Reading Comprehension (MRC) is a challenging extension of Natural Language Processing (NLP) that requires the ability to comprehend the semantics and logical relationships between entities in a given text. The MRC task has traditionally been viewed as a process of answering questions based on the given text. This single-stage approach has often led the network to concentrate on generating the correct answer, potentially neglecting the comprehension of the text itself. As a result, many prevalent models have faced challenges in performing well on this task when dealing with longer texts. In this paper, we propose a two-stage knowledge distillation method that teaches the model to better comprehend the document by dividing the MRC task into two separate stages. Our experimental results show that the student model, when equipped with our method, achieves significant improvements, demonstrating the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#24335;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#20998;&#25955;&#25968;&#25454;&#30340;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#38480;&#21046;&#21644;&#31169;&#26377;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#38656;&#27714;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#36825;&#19977;&#20010;&#32452;&#20214;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#26045;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FL&#21644;LLM&#38598;&#25104;&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#65292;&#24182;&#20998;&#26512;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21644;&#28508;&#22312;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2307.08925</link><description>&lt;p&gt;
&#32852;&#37030;&#24335;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#20010;&#31435;&#22330;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Federated Large Language Model: A Position Paper. (arXiv:2307.08925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08925
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#24335;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#20998;&#25955;&#25968;&#25454;&#30340;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#38480;&#21046;&#21644;&#31169;&#26377;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#38656;&#27714;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#36825;&#19977;&#20010;&#32452;&#20214;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#26045;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FL&#21644;LLM&#38598;&#25104;&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#65292;&#24182;&#20998;&#26512;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21644;&#28508;&#22312;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#33719;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#24182;&#25214;&#21040;&#20102;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#65292;&#20294;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#24320;&#21457;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#28304;&#20110;&#20844;&#20849;&#39046;&#22495;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#21294;&#20047;&#20197;&#21450;&#23545;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#39033;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#20986;&#29616;&#20102;&#65292;&#23427;&#33021;&#22815;&#22312;&#20445;&#25345;&#20998;&#25955;&#25968;&#25454;&#30340;&#21516;&#26102;&#23454;&#29616;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#24335;LLM&#30340;&#27010;&#24565;&#65292;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#32852;&#37030;&#24335;LLM&#39044;&#35757;&#32451;&#12289;&#32852;&#37030;&#24335;LLM&#24494;&#35843;&#21644;&#32852;&#37030;&#24335;LLM&#25552;&#31034;&#24037;&#31243;&#12290;&#23545;&#20110;&#27599;&#20010;&#32452;&#20214;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23427;&#30456;&#23545;&#20110;&#20256;&#32479;LLM&#35757;&#32451;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#20307;&#30340;&#24037;&#31243;&#31574;&#30053;&#26469;&#23454;&#26045;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FL&#21644;LLM&#38598;&#25104;&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#12290;&#25105;&#20204;&#20998;&#26512;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#30830;&#23450;&#21487;&#33021;&#30340;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Large scale language models (LLM) have received significant attention and found diverse applications across various domains, but their development encounters challenges in real-world scenarios. These challenges arise due to the scarcity of public domain data availability and the need to maintain privacy with respect to private domain data. To address these issues, federated learning (FL) has emerged as a promising technology that enables collaborative training of shared models while preserving decentralized data. We propose the concept of federated LLM, which comprises three key components, i.e., federated LLM pre-training, federated LLM fine-tuning, and federated LLM prompt engineering. For each component, we discuss its advantage over traditional LLM training methods and propose specific engineering strategies for implementation. Furthermore, we explore the novel challenges introduced by the integration of FL and LLM. We analyze existing solutions and identify potential obstacles fac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#35786;&#26029;&#20013;&#20351;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#25193;&#23637;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#35786;&#26029;&#25512;&#29702; CoT &#23454;&#20363;&#26469;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#19968;&#33324;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35786;&#26029;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;15%&#65292;&#22312;&#39046;&#22495;&#22806;&#30340;&#35774;&#32622;&#20013;&#65292;&#36825;&#19968;&#25552;&#21319;&#36798;&#21040;&#20102;18%&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24341;&#21457;&#19987;&#23478;&#30693;&#35782;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.08922</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35786;&#26029;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Perform Diagnostic Reasoning. (arXiv:2307.08922v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#35786;&#26029;&#20013;&#20351;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#25193;&#23637;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#35786;&#26029;&#25512;&#29702; CoT &#23454;&#20363;&#26469;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#19968;&#33324;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35786;&#26029;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;15%&#65292;&#22312;&#39046;&#22495;&#22806;&#30340;&#35774;&#32622;&#20013;&#65292;&#36825;&#19968;&#25552;&#21319;&#36798;&#21040;&#20102;18%&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24341;&#21457;&#19987;&#23478;&#30693;&#35782;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#24605;&#32500;&#38142; (CoT) &#25552;&#31034;&#25193;&#23637;&#21040;&#21307;&#30103;&#25512;&#29702;&#20197;&#36827;&#34892;&#33258;&#21160;&#35786;&#26029;&#30340;&#20219;&#21153;&#12290;&#21463;&#21307;&#29983;&#28508;&#22312;&#30340;&#25512;&#29702;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35786;&#26029;&#25512;&#29702; CoT (DR-CoT)&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#20165;&#36890;&#36807;&#29992;&#20004;&#20010;&#35786;&#26029;&#25512;&#29702; CoT &#23454;&#20363;&#25552;&#31034;&#20165;&#22312;&#19968;&#33324;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35786;&#26029;&#20934;&#30830;&#29575;&#27604;&#26631;&#20934;&#25552;&#31034;&#25552;&#39640;&#20102;15%&#12290;&#27492;&#22806;&#65292;&#22312;&#39046;&#22495;&#22806;&#30340;&#35774;&#32622;&#20013;&#65292;&#36825;&#19968;&#24046;&#36317;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;18%&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#21487;&#20197;&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19987;&#23478;&#30693;&#35782;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the extension of chain-of-thought (CoT) prompting to medical reasoning for the task of automatic diagnosis. Motivated by doctors' underlying reasoning process, we present Diagnostic-Reasoning CoT (DR-CoT). Empirical results demonstrate that by simply prompting large language models trained only on general text corpus with two DR-CoT exemplars, the diagnostic accuracy improves by 15% comparing to standard prompting. Moreover, the gap reaches a pronounced 18% in out-domain settings. Our findings suggest expert-knowledge reasoning in large language models can be elicited through proper promptings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#22797;&#26434;&#24615;&#24418;&#24335;&#21270;&#21644;&#27169;&#22411;&#33021;&#21147;&#20316;&#20026;&#22256;&#38590;&#24230;&#26631;&#20934;&#65292;&#20197;&#21450;&#32771;&#34385;&#26679;&#26412;&#22256;&#38590;&#24230;&#21644;&#27169;&#22411;&#33021;&#21147;&#30340;&#19981;&#21516;&#35270;&#35282;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#32454;&#31890;&#24230;&#22270;&#22256;&#38590;&#24230;&#26631;&#20934;&#30340;&#32435;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.08859</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35838;&#31243;&#23398;&#20064;&#65306;&#19968;&#31181;&#22522;&#20110;&#22810;&#35270;&#35282;&#21644;&#33021;&#21147;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach. (arXiv:2307.08859v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#22797;&#26434;&#24615;&#24418;&#24335;&#21270;&#21644;&#27169;&#22411;&#33021;&#21147;&#20316;&#20026;&#22256;&#38590;&#24230;&#26631;&#20934;&#65292;&#20197;&#21450;&#32771;&#34385;&#26679;&#26412;&#22256;&#38590;&#24230;&#21644;&#27169;&#22411;&#33021;&#21147;&#30340;&#19981;&#21516;&#35270;&#35282;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#32454;&#31890;&#24230;&#22270;&#22256;&#38590;&#24230;&#26631;&#20934;&#30340;&#32435;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35838;&#31243;&#23398;&#20064;&#26159;&#19968;&#31181;&#35745;&#21010;&#22909;&#30340;&#23398;&#20064;&#26448;&#26009;&#24207;&#21015;&#65292;&#26377;&#25928;&#30340;&#35838;&#31243;&#23398;&#20064;&#21487;&#20197;&#20351;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#23398;&#20064;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#35821;&#35328;&#24212;&#29992;&#20013;&#20026;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#20102;&#26377;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#33539;&#24335;&#20013;&#36890;&#24120;&#21482;&#20351;&#29992;&#21333;&#19968;&#30340;&#22256;&#38590;&#24230;&#26631;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35838;&#31243;&#23398;&#20064;&#35270;&#35282;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#24314;&#31435;&#22312;&#22270;&#22797;&#26434;&#24615;&#24418;&#24335;&#21270;&#65288;&#20316;&#20026;&#22256;&#38590;&#24230;&#26631;&#20934;&#65289;&#21644;&#27169;&#22411;&#33021;&#21147;&#20043;&#19978;&#30340;&#26032;&#26041;&#27861;&#26469;&#36827;&#34892;&#35838;&#31243;&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#19968;&#20010;&#35843;&#24230;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#26679;&#26412;&#22256;&#38590;&#24230;&#21644;&#27169;&#22411;&#33021;&#21147;&#30340;&#19981;&#21516;&#35270;&#35282;&#22312;&#35757;&#32451;&#26399;&#38388;&#25512;&#23548;&#20986;&#26377;&#25928;&#30340;&#35838;&#31243;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35838;&#31243;&#23398;&#20064;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#33539;&#24335;&#20013;&#32435;&#20837;&#32454;&#31890;&#24230;&#30340;&#22270;&#22256;&#38590;&#24230;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
A curriculum is a planned sequence of learning materials and an effective one can make learning efficient and effective for both humans and machines. Recent studies developed effective data-driven curriculum learning approaches for training graph neural networks in language applications. However, existing curriculum learning approaches often employ a single criterion of difficulty in their training paradigms. In this paper, we propose a new perspective on curriculum learning by introducing a novel approach that builds on graph complexity formalisms (as difficulty criteria) and model competence during training. The model consists of a scheduling scheme which derives effective curricula by accounting for different views of sample difficulty and model competence during training. The proposed solution advances existing research in curriculum learning for graph neural networks with the ability to incorporate a fine-grained spectrum of graph difficulty criteria in their training paradigms. E
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21462;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.08813</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21462;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#26041;&#38754;&#30340;&#27604;&#36739;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge. (arXiv:2307.08813v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21462;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#23545;&#20110;&#25581;&#31034;&#29983;&#29289;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21644;&#30740;&#31350;&#29983;&#29289;&#21151;&#33021;&#21644;&#22797;&#26434;&#30142;&#30149;&#30340;&#22522;&#26412;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#26469;&#33258;&#25991;&#29486;&#21644;&#20854;&#20182;&#28304;&#30340;&#31574;&#21010;&#29983;&#29289;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#19981;&#23436;&#25972;&#19988;&#32500;&#25252;&#24037;&#20316;&#32321;&#37325;&#65292;&#22240;&#27492;&#38656;&#35201;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#33258;&#21160;&#20174;&#30456;&#20851;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#36825;&#20123;&#30693;&#35782;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#12289;&#36890;&#36335;&#21644;&#22522;&#22240;&#35843;&#25511;&#20851;&#31995;&#31561;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#31361;&#20986;&#20102;&#37325;&#35201;&#30340;&#21457;&#29616;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#26410;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#38142;&#25509;&#21487;&#22312;&#35770;&#25991;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding protein interactions and pathway knowledge is crucial for unraveling the complexities of living systems and investigating the underlying mechanisms of biological functions and complex diseases. While existing databases provide curated biological data from literature and other sources, they are often incomplete and their maintenance is labor-intensive, necessitating alternative approaches. In this study, we propose to harness the capabilities of large language models to address these issues by automatically extracting such knowledge from the relevant scientific literature. Toward this goal, in this work, we investigate the effectiveness of different large language models in tasks that involve recognizing protein interactions, pathways, and gene regulatory relations. We thoroughly evaluate the performance of various models, highlight the significant findings, and discuss both the future opportunities and the remaining challenges associated with this approach. The code and d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31574;&#30053;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22312;&#25277;&#35937;&#23618;&#21644;&#31532;&#20108;&#23618;&#37319;&#29992;&#19981;&#21516;&#30340;&#25506;&#32034;&#26041;&#24335;&#65292;&#21462;&#24471;&#20102;&#36229;&#36807;2%&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.08767</link><description>&lt;p&gt;
&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#30340;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
A mixed policy to improve performance of language models on math problems. (arXiv:2307.08767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31574;&#30053;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22312;&#25277;&#35937;&#23618;&#21644;&#31532;&#20108;&#23618;&#37319;&#29992;&#19981;&#21516;&#30340;&#25506;&#32034;&#26041;&#24335;&#65292;&#21462;&#24471;&#20102;&#36229;&#36807;2%&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26102;&#65292;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#37319;&#29992;&#37319;&#26679;&#31574;&#30053;&#26681;&#25454;&#26465;&#20214;&#27010;&#29575;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#12290;&#22312;&#25968;&#23398;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#21487;&#33021;&#20250;&#29983;&#25104;&#38169;&#35823;&#30340;&#31572;&#26696;&#12290;&#32771;&#34385;&#21040;&#25968;&#23398;&#38382;&#39064;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31574;&#30053;&#30340;&#25506;&#32034;&#26041;&#27861;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#32423;&#26631;&#35760;&#25506;&#32034;&#31574;&#30053;&#65306;&#25277;&#35937;&#23618;&#20197;&#27010;&#29575;&#37319;&#26679;&#26469;&#20915;&#23450;&#19979;&#19968;&#20010;&#26631;&#35760;&#26159;&#36816;&#31639;&#31526;&#36824;&#26159;&#25805;&#20316;&#25968;&#65292;&#32780;&#31532;&#20108;&#23618;&#21017;&#20197;&#36138;&#23146;&#26041;&#24335;&#36873;&#25321;&#24471;&#20998;&#26368;&#39640;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-2&#27169;&#22411;&#22312;GSM8K&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36229;&#36807;2&#65285;&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#20195;&#30721;&#21487;&#22312;https://github.com/vividitytech/math_lm_rl&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
When to solve math problems, most language models take a sampling strategy to predict next word according conditional probabilities. In the math reasoning step, it may generate wrong answer. Considering math problems are deterministic, we propose a mixed policy exploration approach to solve math problems with reinforcement learning. In peculiar, we propose a two level token exploration policy: the abstract level explores next token with probability and the second level is deterministic. Specifically, the abstract level policy will decide whether the token is operator or operand with probability sampling, while the second level is deterministic to select next token with the highest score in a greedy way. We test our method on GSM8K dataset with GPT-2 model, and demonstrate more than $2\%$ performance gain. Our implementation is available at https://github.com/vividitytech/math_lm_rl.
&lt;/p&gt;</description></item><item><title>ivrit.ai&#26159;&#19968;&#20221;&#20840;&#38754;&#24076;&#20271;&#26469;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;3300&#23567;&#26102;&#30340;&#35821;&#38899;&#21644;&#19968;&#21315;&#22810;&#20010;&#35828;&#35805;&#32773;&#65292;&#21487;&#29992;&#20110;&#25512;&#21160;&#24076;&#20271;&#26469;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#30740;&#31350;&#12289;&#24320;&#21457;&#21644;&#21830;&#19994;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.08720</link><description>&lt;p&gt;
ivrit.ai&#65306;&#19968;&#20221;&#29992;&#20110;AI&#30740;&#31350;&#21644;&#24320;&#21457;&#30340;&#20840;&#38754;&#24076;&#20271;&#26469;&#35821;&#38899;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ivrit.ai: A Comprehensive Dataset of Hebrew Speech for AI Research and Development. (arXiv:2307.08720v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08720
&lt;/p&gt;
&lt;p&gt;
ivrit.ai&#26159;&#19968;&#20221;&#20840;&#38754;&#24076;&#20271;&#26469;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;3300&#23567;&#26102;&#30340;&#35821;&#38899;&#21644;&#19968;&#21315;&#22810;&#20010;&#35828;&#35805;&#32773;&#65292;&#21487;&#29992;&#20110;&#25512;&#21160;&#24076;&#20271;&#26469;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#30740;&#31350;&#12289;&#24320;&#21457;&#21644;&#21830;&#19994;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20221;&#21517;&#20026;"ivrit.ai"&#30340;&#20840;&#38754;&#24076;&#20271;&#26469;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#25512;&#21160;&#24076;&#20271;&#26469;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#25216;&#26415;&#30340;&#36164;&#28304;&#32570;&#20047;&#38382;&#39064;&#12290;ivrit.ai&#21253;&#21547;&#20102;&#36229;&#36807;3300&#23567;&#26102;&#30340;&#35821;&#38899;&#21644;&#19968;&#21315;&#22810;&#20010;&#19981;&#21516;&#30340;&#35828;&#35805;&#32773;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#35821;&#22659;&#20013;&#30340;&#24076;&#20271;&#26469;&#35821;&#38899;&#12290;&#23427;&#25552;&#20379;&#20102;&#19977;&#31181;&#24418;&#24335;&#30340;&#25968;&#25454;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#30740;&#31350;&#38656;&#27714;&#65306;&#21407;&#22987;&#26410;&#22788;&#29702;&#30340;&#38899;&#39057;&#12289;&#32463;&#36807;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#30340;&#25968;&#25454;&#21644;&#37096;&#20998;&#24102;&#26377;&#36716;&#24405;&#30340;&#25968;&#25454;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#31361;&#20986;&#29305;&#28857;&#26159;&#27861;&#24459;&#19978;&#21487;&#20197;&#26080;&#20607;&#33719;&#24471;&#65292;&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#24320;&#21457;&#32773;&#21644;&#21830;&#19994;&#23454;&#20307;&#30340;&#37325;&#35201;&#36164;&#28304;&#12290;ivrit.ai&#24320;&#36767;&#20102;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#65292;&#23545;&#22686;&#24378;&#24076;&#20271;&#26469;&#35821;&#30340;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26410;&#26469;&#30340;&#21162;&#21147;&#23558;&#36827;&#19968;&#27493;&#25193;&#23637;ivrit.ai&#65292;&#25512;&#21160;&#24076;&#20271;&#26469;&#35821;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#25216;&#26415;&#20013;&#30340;&#22320;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce "ivrit.ai", a comprehensive Hebrew speech dataset, addressing the distinct lack of extensive, high-quality resources for advancing Automated Speech Recognition (ASR) technology in Hebrew. With over 3,300 speech hours and a over a thousand diverse speakers, ivrit.ai offers a substantial compilation of Hebrew speech across various contexts. It is delivered in three forms to cater to varying research needs: raw unprocessed audio; data post-Voice Activity Detection, and partially transcribed data. The dataset stands out for its legal accessibility, permitting use at no cost, thereby serving as a crucial resource for researchers, developers, and commercial entities. ivrit.ai opens up numerous applications, offering vast potential to enhance AI capabilities in Hebrew. Future efforts aim to expand ivrit.ai further, thereby advancing Hebrew's standing in AI research and technology.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#19968;&#33268;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#36890;&#36807;&#20174;&#33521;&#35821;&#21040;&#38463;&#25289;&#20271;&#35821;&#30340;&#30693;&#35782;&#20256;&#36882;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#21830;&#23478;&#12289;&#37329;&#39069;&#21644;&#20854;&#20182;&#23383;&#27573;&#12290;</title><link>http://arxiv.org/abs/2307.08714</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#30340;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual NER for Financial Transaction Data in Low-Resource Languages. (arXiv:2307.08714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08714
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#19968;&#33268;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#36890;&#36807;&#20174;&#33521;&#35821;&#21040;&#38463;&#25289;&#20271;&#35821;&#30340;&#30693;&#35782;&#20256;&#36882;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#21830;&#23478;&#12289;&#37329;&#39069;&#21644;&#20854;&#20182;&#23383;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21322;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#30693;&#35782;&#33976;&#39311;&#21644;&#19968;&#33268;&#24615;&#35757;&#32451;&#12290;&#24314;&#27169;&#26694;&#26550;&#21033;&#29992;&#22312;&#28304;&#35821;&#35328;&#19978;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;XLMRoBERTa&#65289;&#30340;&#30693;&#35782;&#65292;&#24182;&#37319;&#29992;&#20102;&#23398;&#29983;-&#25945;&#24072;&#20851;&#31995;&#65288;&#30693;&#35782;&#33976;&#39311;&#65289;&#12290;&#23398;&#29983;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#30446;&#26631;&#35821;&#35328;&#19978;&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;&#19968;&#33268;&#24615;&#35757;&#32451;&#65288;&#20351;&#29992;KL&#25955;&#24230;&#25439;&#22833;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#29420;&#31435;&#30340;&#33521;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#30701;&#20449;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#21253;&#21547;&#21322;&#32467;&#26500;&#21270;&#30340;&#38134;&#34892;&#20132;&#26131;&#20449;&#24687;&#65292;&#24182;&#37325;&#28857;&#23637;&#31034;&#20174;&#33521;&#35821;&#21040;&#38463;&#25289;&#20271;&#35821;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;&#22312;&#20165;&#26377;30&#20010;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20174;&#33521;&#35821;&#27867;&#21270;&#21040;&#38463;&#25289;&#20271;&#35821;&#30340;&#35782;&#21035;&#21830;&#23478;&#12289;&#37329;&#39069;&#21644;&#20854;&#20182;&#23383;&#27573;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24314;&#27169;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#19982;DistilBERT&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an efficient modeling framework for cross-lingual named entity recognition in semi-structured text data. Our approach relies on both knowledge distillation and consistency training. The modeling framework leverages knowledge from a large language model (XLMRoBERTa) pre-trained on the source language, with a student-teacher relationship (knowledge distillation). The student model incorporates unsupervised consistency training (with KL divergence loss) on the low-resource target language.  We employ two independent datasets of SMSs in English and Arabic, each carrying semi-structured banking transaction information, and focus on exhibiting the transfer of knowledge from English to Arabic. With access to only 30 labeled samples, our model can generalize the recognition of merchants, amounts, and other fields from English to Arabic. We show that our modeling approach, while efficient, performs best overall when compared to state-of-the-art approaches like DistilBERT pre-trained 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36719;&#20214;&#24320;&#21457;&#33539;&#24335;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25972;&#20010;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#65292;&#28040;&#38500;&#20102;&#27599;&#20010;&#38454;&#27573;&#38656;&#35201;&#19987;&#38376;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#35813;&#33539;&#24335;&#20351;&#29992;ChatDev&#20316;&#20026;&#19968;&#20010;&#34394;&#25311;&#32842;&#22825;&#39537;&#21160;&#30340;&#36719;&#20214;&#24320;&#21457;&#20844;&#21496;&#65292;&#36890;&#36807;&#35774;&#35745;&#12289;&#32534;&#30721;&#12289;&#27979;&#35797;&#21644;&#25991;&#26723;&#21270;&#22235;&#20010;&#38454;&#27573;&#30340;&#20195;&#29702;&#20154;&#22242;&#38431;&#20419;&#36827;&#21327;&#20316;&#12290;</title><link>http://arxiv.org/abs/2307.07924</link><description>&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#20132;&#27969;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Communicative Agents for Software Development. (arXiv:2307.07924v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36719;&#20214;&#24320;&#21457;&#33539;&#24335;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25972;&#20010;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#65292;&#28040;&#38500;&#20102;&#27599;&#20010;&#38454;&#27573;&#38656;&#35201;&#19987;&#38376;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#35813;&#33539;&#24335;&#20351;&#29992;ChatDev&#20316;&#20026;&#19968;&#20010;&#34394;&#25311;&#32842;&#22825;&#39537;&#21160;&#30340;&#36719;&#20214;&#24320;&#21457;&#20844;&#21496;&#65292;&#36890;&#36807;&#35774;&#35745;&#12289;&#32534;&#30721;&#12289;&#27979;&#35797;&#21644;&#25991;&#26723;&#21270;&#22235;&#20010;&#38454;&#27573;&#30340;&#20195;&#29702;&#20154;&#22242;&#38431;&#20419;&#36827;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24037;&#31243;&#26159;&#19968;&#20010;&#20197;&#24494;&#22937;&#30340;&#30452;&#35273;&#21644;&#21672;&#35810;&#20026;&#29305;&#24449;&#30340;&#39046;&#22495;&#65292;&#20915;&#31574;&#36807;&#31243;&#22797;&#26434;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#24320;&#22987;&#36890;&#36807;&#22312;&#36719;&#20214;&#24320;&#21457;&#30340;&#21508;&#20010;&#38454;&#27573;&#23454;&#26045;&#31934;&#24515;&#35774;&#35745;&#26469;&#38761;&#26032;&#36719;&#20214;&#24037;&#31243;&#23454;&#36341;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#65292;&#22312;&#25972;&#20010;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#31616;&#21270;&#21644;&#32479;&#19968;&#20851;&#38190;&#27969;&#31243;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#22312;&#27599;&#20010;&#38454;&#27573;&#38656;&#35201;&#19987;&#38376;&#30340;&#27169;&#22411;&#30340;&#38656;&#35201;&#12290;&#36825;&#20010;&#33539;&#24335;&#30340;&#26680;&#24515;&#26159;ChatDev&#65292;&#19968;&#20010;&#34394;&#25311;&#30340;&#32842;&#22825;&#39537;&#21160;&#36719;&#20214;&#24320;&#21457;&#20844;&#21496;&#65292;&#23427;&#27169;&#20223;&#20102;&#24050;&#32463;&#24314;&#31435;&#30340;&#28689;&#24067;&#27169;&#22411;&#65292;&#23558;&#24320;&#21457;&#36807;&#31243;&#32454;&#20998;&#20026;&#22235;&#20010;&#19981;&#21516;&#30340;&#26102;&#38388;&#38454;&#27573;&#65306;&#35774;&#35745;&#12289;&#32534;&#30721;&#12289;&#27979;&#35797;&#21644;&#25991;&#26723;&#21270;&#12290;&#27599;&#20010;&#38454;&#27573;&#37117;&#28041;&#21450;&#19968;&#20010;&#22242;&#38431;&#30340;&#20195;&#29702;&#20154;&#65292;&#22914;&#31243;&#24207;&#21592;&#12289;&#20195;&#30721;&#23457;&#26597;&#20154;&#21592;&#21644;&#27979;&#35797;&#24037;&#31243;&#24072;&#65292;&#20419;&#36827;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software engineering is a domain characterized by intricate decision-making processes, often relying on nuanced intuition and consultation. Recent advancements in deep learning have started to revolutionize software engineering practices through elaborate designs implemented at various stages of software development. In this paper, we present an innovative paradigm that leverages large language models (LLMs) throughout the entire software development process, streamlining and unifying key processes through natural language communication, thereby eliminating the need for specialized models at each phase. At the core of this paradigm lies ChatDev, a virtual chat-powered software development company that mirrors the established waterfall model, meticulously dividing the development process into four distinct chronological stages: designing, coding, testing, and documenting. Each stage engages a team of agents, such as programmers, code reviewers, and test engineers, fostering collaborativ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;RLHF&#30340;&#31192;&#23494;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22870;&#21169;&#27169;&#22411;&#12289;PPO&#21644;&#36827;&#31243;&#30417;&#30563;&#31561;&#25216;&#26415;&#36335;&#24452;&#65292;&#25506;&#32034;&#22914;&#20309;&#35299;&#20915;RLHF&#30340;&#31283;&#23450;&#35757;&#32451;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04964</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;RLHF&#30340;&#31192;&#23494; &#31532;&#19968;&#37096;&#20998;&#65306;PPO
&lt;/p&gt;
&lt;p&gt;
Secrets of RLHF in Large Language Models Part I: PPO. (arXiv:2307.04964v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;RLHF&#30340;&#31192;&#23494;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22870;&#21169;&#27169;&#22411;&#12289;PPO&#21644;&#36827;&#31243;&#30417;&#30563;&#31561;&#25216;&#26415;&#36335;&#24452;&#65292;&#25506;&#32034;&#22914;&#20309;&#35299;&#20915;RLHF&#30340;&#31283;&#23450;&#35757;&#32451;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#25512;&#21160;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;&#34013;&#22270;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#25104;&#20026;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#65288;&#26377;&#30410;&#12289;&#35802;&#23454;&#21644;&#26080;&#23475;&#65289;&#21161;&#25163;&#12290;&#19982;&#20154;&#31867;&#30340;&#23545;&#40784;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#24847;&#20041;&#65292;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#25104;&#20026;&#25903;&#25745;&#36825;&#19968;&#36861;&#27714;&#30340;&#20851;&#38190;&#25216;&#26415;&#33539;&#24335;&#12290;&#24403;&#21069;&#30340;&#25216;&#26415;&#36335;&#32447;&#36890;&#24120;&#21253;&#25324;&#29992;&#20110;&#34913;&#37327;&#20154;&#31867;&#20559;&#22909;&#30340;&#22870;&#21169;&#27169;&#22411;&#12289;&#29992;&#20110;&#20248;&#21270;&#31574;&#30053;&#27169;&#22411;&#36755;&#20986;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#20197;&#21450;&#29992;&#20110;&#25913;&#21892;&#36880;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#36827;&#31243;&#30417;&#30563;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22870;&#21169;&#35774;&#35745;&#12289;&#29615;&#22659;&#20132;&#20114;&#21644;&#20195;&#29702;&#35757;&#32451;&#30340;&#25361;&#25112;&#65292;&#20877;&#21152;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35797;&#39564;&#25104;&#26412;&#24040;&#22823;&#65292;&#23545;&#20110;AI&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#65292;&#28608;&#21169;&#25216;&#26415;&#23545;&#40784;&#21644;LLMs&#30340;&#23433;&#20840;&#30528;&#38470;&#23384;&#22312;&#37325;&#22823;&#38556;&#30861;&#12290;RLHF&#30340;&#31283;&#23450;&#35757;&#32451;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \textbf{reward models} to measure human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.03109</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#32780;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#38543;&#30528;LLMs&#22312;&#30740;&#31350;&#21644;&#26085;&#24120;&#20351;&#29992;&#20013;&#32487;&#32493;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#19981;&#20165;&#22312;&#20219;&#21153;&#27700;&#24179;&#19978;&#65292;&#32780;&#19988;&#22312;&#31038;&#20250;&#23618;&#38754;&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;LLMs&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#36825;&#20123;&#35780;&#20272;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35780;&#20272;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#19968;&#33324;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#21644;&#20854;&#20182;&#39046;&#22495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20934;&#31572;&#26696;&#26469;&#22238;&#31572;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#20351;&#29992;LLM&#36827;&#34892;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#65292;&#36890;&#36807;&#36827;&#21270;&#35843;&#20248;&#20107;&#20214;&#27169;&#24335;&#32676;&#20307;&#65292;&#25552;&#39640;&#29983;&#25104;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02839</link><description>&lt;p&gt;
&#20351;&#29992;&#36827;&#21270;&#35843;&#20248;&#22686;&#24378;LLM&#36827;&#34892;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation. (arXiv:2307.02839v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#20351;&#29992;LLM&#36827;&#34892;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#65292;&#36890;&#36807;&#36827;&#21270;&#35843;&#20248;&#20107;&#20214;&#27169;&#24335;&#32676;&#20307;&#65292;&#25552;&#39640;&#29983;&#25104;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#26159;&#24773;&#25253;&#20998;&#26512;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#20840;&#38754;&#30340;&#20449;&#24687;&#65292;&#24110;&#21161;&#20154;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#24212;&#23545;&#22797;&#26434;&#30340;&#29616;&#23454;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#27169;&#22411;&#26412;&#36523;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#38480;&#21046;&#65292;&#20197;&#21450;&#25991;&#26412;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20351;&#24471;&#20934;&#30830;&#29983;&#25104;&#21487;&#38752;&#20449;&#24687;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#24378;&#22823;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#30340;LLM&#36827;&#34892;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#30340;&#26032;&#33539;&#24335;&#12290;&#25105;&#20204;&#21033;&#29992;LLM&#20174;&#26032;&#38395;&#27573;&#33853;&#20013;&#25552;&#21462;&#22810;&#20010;&#32467;&#26500;&#21270;&#20107;&#20214;&#27169;&#24335;&#65292;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#36827;&#21270;&#20107;&#20214;&#27169;&#24335;&#32676;&#20307;&#65292;&#24182;&#36873;&#25321;&#26368;&#36866;&#24212;&#30340;&#20107;&#20214;&#27169;&#24335;&#36755;&#20837;LLM&#29983;&#25104;&#26032;&#38395;&#25688;&#35201;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#22120;(NSG)&#26469;&#36873;&#25321;&#21644;&#36827;&#21270;&#20107;&#20214;&#27169;&#24335;&#32676;&#20307;&#65292;&#24182;&#29983;&#25104;&#26032;&#38395;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
News summary generation is an important task in the field of intelligence analysis, which can provide accurate and comprehensive information to help people better understand and respond to complex real-world events. However, traditional news summary generation methods face some challenges, which are limited by the model itself and the amount of training data, as well as the influence of text noise, making it difficult to generate reliable information accurately. In this paper, we propose a new paradigm for news summary generation using LLM with powerful natural language understanding and generative capabilities. We use LLM to extract multiple structured event patterns from the events contained in news paragraphs, evolve the event pattern population with genetic algorithm, and select the most adaptive event pattern to input into the LLM to generate news summaries. A News Summary Generator (NSG) is designed to select and evolve the event pattern populations and generate news summaries. T
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#22312;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;GPT-4&#30340;&#21484;&#22238;&#29575;&#36739;&#39640;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#36807;&#24230;&#20462;&#27491;&#12290;</title><link>http://arxiv.org/abs/2306.15788</link><description>&lt;p&gt;
&#22312;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#38754;&#35780;&#20272;GPT-3.5&#21644;GPT-4
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese. (arXiv:2306.15788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15788
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#22312;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;GPT-4&#30340;&#21484;&#22238;&#29575;&#36739;&#39640;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#36807;&#24230;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;GPT-3.5&#21644;GPT-4&#36825;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;GEC&#65289;&#24037;&#20855;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;Microsoft Word&#21644;Google Docs&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38024;&#23545;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;GEC&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22235;&#20010;&#31867;&#21035;&#65306;&#35821;&#27861;&#12289;&#25340;&#20889;&#12289;&#20114;&#32852;&#32593;&#21644;&#24555;&#36895;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;GPT-4&#30340;&#21484;&#22238;&#29575;&#27604;&#20854;&#20182;&#26041;&#27861;&#39640;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#20855;&#26377;&#36739;&#20302;&#30340;&#31934;&#30830;&#24230;&#65292;&#23548;&#33268;&#36807;&#24230;&#20462;&#27491;&#12290;&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#23454;&#38469;GEC&#24037;&#20855;&#30340;&#28508;&#21147;&#65292;&#24182;&#40723;&#21169;&#36827;&#19968;&#27493;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#21644;&#20854;&#20182;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the effectiveness of GPT-3.5 and GPT-4, two large language models, as Grammatical Error Correction (GEC) tools for Brazilian Portuguese and compare their performance against Microsoft Word and Google Docs. We introduce a GEC dataset for Brazilian Portuguese with four categories: Grammar, Spelling, Internet, and Fast typing. Our results show that while GPT-4 has higher recall than other methods, LLMs tend to have lower precision, leading to overcorrection. This study demonstrates the potential of LLMs as practical GEC tools for Brazilian Portuguese and encourages further exploration of LLMs for non-English languages and other educational settings.
&lt;/p&gt;</description></item><item><title>SparseOptimizer&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#23427;&#37319;&#29992;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#26080;&#38656;&#23545;&#20195;&#30721;&#36827;&#34892;&#20462;&#25913;&#21363;&#21487;&#36866;&#24212;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#23494;&#38598;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.15656</link><description>&lt;p&gt;
SparseOptimizer: &#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#26469;&#38477;&#20302;&#35821;&#35328;&#27169;&#22411;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#36890;&#36807;&#32534;&#35793;&#22120;&#20849;&#21516;&#35774;&#35745;&#26469;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design. (arXiv:2306.15656v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15656
&lt;/p&gt;
&lt;p&gt;
SparseOptimizer&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#23427;&#37319;&#29992;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#26080;&#38656;&#23545;&#20195;&#30721;&#36827;&#34892;&#20462;&#25913;&#21363;&#21487;&#36866;&#24212;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#23494;&#38598;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SparseOptimizer&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65292;ALBERT&#21644;GPT&#65289;&#20013;&#33258;&#28982;&#22320;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;SparseOptimizer&#35774;&#35745;&#30340;&#20851;&#38190;&#26159;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#23427;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#30452;&#25509;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#36825;&#20010;&#25805;&#20316;&#31526;&#36890;&#36807;&#22362;&#23454;&#30340;&#29702;&#35770;&#26694;&#26550;&#25903;&#25345;&#65292;&#24182;&#21253;&#21547;&#20102;&#19968;&#20010;&#20998;&#26512;&#35299;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20248;&#21270;&#22120;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;SparseOptimizer&#30340;&#21363;&#25554;&#21363;&#29992;&#21151;&#33021;&#28040;&#38500;&#20102;&#23545;&#20195;&#30721;&#20462;&#25913;&#30340;&#38656;&#27714;&#65292;&#20351;&#20854;&#25104;&#20026;&#36866;&#29992;&#20110;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#36866;&#24212;&#24037;&#20855;&#12290;&#22312;GLUE&#12289;RACE&#12289;SQuAD1&#21644;SQuAD2&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#36890;&#36807;SparseOptimizer&#31232;&#30095;&#21270;&#21518;&#30340;SparseBERT&#21644;SparseALBERT&#22312;&#24615;&#33021;&#19978;&#19982;&#23494;&#38598;&#22411;&#30340;BERT&#21644;ALBERT&#30456;&#24403;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces SparseOptimizer, a novel deep learning optimizer that exploits Moreau-Yosida regularization to naturally induce sparsity in large language models such as BERT, ALBERT and GPT. Key to the design of SparseOptimizer is an embedded shrinkage operator, which imparts sparsity directly within the optimization process. This operator, backed by a sound theoretical framework, includes an analytical solution, thereby reinforcing the optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play functionality eradicates the need for code modifications, making it a universally adaptable tool for a wide array of large language models. Empirical evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2 confirm that SparseBERT and SparseALBERT, when sparsified using SparseOptimizer, achieve performance comparable to their dense counterparts, BERT and ALBERT, while significantly reducing their parameter count. Further, this work proposes an innovati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#30340;&#33258;&#27965;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20551;&#35774;&#33258;&#27965;&#24615;&#21644;&#32452;&#21512;&#33258;&#27965;&#24615;&#20004;&#20010;&#37325;&#35201;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;GPT-3/-4&#27169;&#22411;&#22312;&#36825;&#20004;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#20102;&#36739;&#24046;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14279</link><description>&lt;p&gt;
LLM&#30340;&#22810;&#27493;&#25512;&#29702;&#20013;&#30340;&#20004;&#20010;&#33258;&#27965;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs. (arXiv:2305.14279v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#30340;&#33258;&#27965;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20551;&#35774;&#33258;&#27965;&#24615;&#21644;&#32452;&#21512;&#33258;&#27965;&#24615;&#20004;&#20010;&#37325;&#35201;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;GPT-3/-4&#27169;&#22411;&#22312;&#36825;&#20004;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#20102;&#36739;&#24046;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#19978;&#19979;&#25991;&#20026;&#22522;&#30784;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#20294;&#36825;&#31181;&#25104;&#21151;&#36890;&#24120;&#26159;&#36890;&#36807;&#27491;&#30830;&#24615;&#32780;&#19981;&#26159;&#19968;&#33268;&#24615;&#26469;&#35780;&#20272;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#22312;&#35299;&#20915;&#30001;&#22810;&#20010;&#23376;&#27493;&#39588;&#30340;&#31572;&#26696;&#32452;&#25104;&#30340;&#20219;&#21153;&#30340;&#22810;&#27493;&#25512;&#29702;&#20013;&#65292;&#33258;&#27965;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23545;&#20110;&#22810;&#27493;&#25512;&#29702;&#29305;&#21035;&#37325;&#35201;&#30340;&#33258;&#27965;&#24615;&#31867;&#22411;&#65306;&#20551;&#35774;&#33258;&#27965;&#24615;&#65288;&#27169;&#22411;&#22312;&#20551;&#35774;&#30340;&#20854;&#20182;&#19978;&#19979;&#25991;&#20013;&#30340;&#36755;&#20986;&#39044;&#27979;&#33021;&#21147;&#65289;&#21644;&#32452;&#21512;&#33258;&#27965;&#24615;&#65288;&#24403;&#23558;&#20013;&#38388;&#23376;&#27493;&#39588;&#26367;&#25442;&#20026;&#27169;&#22411;&#23545;&#36825;&#20123;&#27493;&#39588;&#30340;&#36755;&#20986;&#26102;&#65292;&#27169;&#22411;&#30340;&#26368;&#32456;&#36755;&#20986;&#30340;&#19968;&#33268;&#24615;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GPT-3/-4&#27169;&#22411;&#30340;&#22810;&#20010;&#21464;&#20307;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#20302;&#19968;&#33268;&#24615;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved widespread success on a variety of in-context few-shot tasks, but this success is typically evaluated via correctness rather than consistency. We argue that self-consistency is an important criteria for valid multi-step reasoning in tasks where the solution is composed of the answers to multiple sub-steps. We propose two types of self-consistency that are particularly important for multi-step reasoning -hypothetical consistency (a model's ability to predict what its output would be in a hypothetical other context) and compositional consistency (consistency of a model's final outputs when intermediate sub-steps are replaced with the model's outputs for those steps). We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#35780;&#20272;&#24320;&#25918;&#24335;&#38382;&#31572;&#65288;Open-QA&#65289;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;QA-Eval&#21644;&#25968;&#25454;&#38598;EVOUNA&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#19982;&#20154;&#24037;&#35780;&#20272;&#30456;&#20851;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#32570;&#38519;&#21644;&#25913;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#23545;&#20110;&#26410;&#26469;&#30340;&#33258;&#21160;&#35780;&#20272;&#24037;&#20855;&#21457;&#23637;&#21644;&#30740;&#31350;&#20855;&#26377;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.12421</link><description>&lt;p&gt;
&#35780;&#20272;&#24320;&#25918;&#24335;&#38382;&#31572;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating Open-QA Evaluation. (arXiv:2305.12421v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#35780;&#20272;&#24320;&#25918;&#24335;&#38382;&#31572;&#65288;Open-QA&#65289;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;QA-Eval&#21644;&#25968;&#25454;&#38598;EVOUNA&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#19982;&#20154;&#24037;&#35780;&#20272;&#30456;&#20851;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#32570;&#38519;&#21644;&#25913;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#23545;&#20110;&#26410;&#26469;&#30340;&#33258;&#21160;&#35780;&#20272;&#24037;&#20855;&#21457;&#23637;&#21644;&#30740;&#31350;&#20855;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#23545;&#24320;&#25918;&#24335;&#38382;&#31572;&#65288;Open-QA&#65289;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#35813;&#20219;&#21153;&#21487;&#20197;&#30452;&#25509;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20107;&#23454;&#24615;&#12290;&#30446;&#21069;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#24050;&#26174;&#31034;&#20986;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#65292;&#34920;&#26126;&#20154;&#24037;&#35780;&#20272;&#20173;&#28982;&#26159;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#35780;&#20272;QA&#35780;&#20272;&#65288;QA-Eval&#65289;&#20197;&#21450;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;EVOUNA&#65292;&#26088;&#22312;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;Open-QA&#20013;&#30340;&#26631;&#20934;&#31572;&#26696;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#20154;&#24037;&#26631;&#27880;&#30340;&#32467;&#26524;&#26469;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#37027;&#20123;&#19982;&#20154;&#24037;&#35780;&#20272;&#20855;&#26377;&#39640;&#24230;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#35748;&#20026;&#23427;&#20204;&#26356;&#21487;&#38752;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#32570;&#38519;&#20197;&#21450;&#25913;&#36827;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#20010;&#26032;&#30340;QA-Eval&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;EVOUNA&#23558;&#20419;&#36827;&#26356;&#26377;&#25928;&#30340;&#33258;&#21160;&#35780;&#20272;&#24037;&#20855;&#30340;&#24320;&#21457;&#65292;&#24182;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#20855;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on the evaluation of the Open Question Answering (Open-QA) task, which can directly estimate the factuality of large language models (LLMs). Current automatic evaluation methods have shown limitations, indicating that human evaluation still remains the most reliable approach. We introduce a new task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset EVOUNA, designed to assess the accuracy of AI-generated answers in relation to standard answers within Open-QA. Our evaluation of these methods utilizes human-annotated results to measure their performance. Specifically, the work investigates methods that show high correlation with human evaluations, deeming them more reliable. We also discuss the pitfalls of current methods and methods to improve LLM-based evaluators. We believe this new QA-Eval task and corresponding dataset EVOUNA will facilitate the development of more effective automatic evaluation tools and prove valuable for future research in this a
&lt;/p&gt;</description></item><item><title>&#8220;Life of PII&#8221;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#28102;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;PII&#36716;&#21270;&#20026;&#20154;&#36896;PII&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#28151;&#28102;&#25991;&#26723;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26723;&#30340;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09550</link><description>&lt;p&gt;
PII&#30340;&#29983;&#21629;--&#19968;&#31181;PII&#28151;&#28102;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Life of PII -- A PII Obfuscation Transformer. (arXiv:2305.09550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09550
&lt;/p&gt;
&lt;p&gt;
&#8220;Life of PII&#8221;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#28102;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;PII&#36716;&#21270;&#20026;&#20154;&#36896;PII&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#28151;&#28102;&#25991;&#26723;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26723;&#30340;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#26381;&#21153;&#30340;&#19990;&#30028;&#20013;&#65292;&#20445;&#25252;&#25935;&#24863;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#25200;&#21160;&#25216;&#26415;&#26469;&#20943;&#23569;(&#25935;&#24863;)&#20010;&#20154;&#36523;&#20221;&#35782;&#21035;&#20449;&#24687;(PII)&#25968;&#25454;&#30340;&#36807;&#24230;&#23454;&#29992;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;&#25968;&#25454;&#25200;&#21160;&#26041;&#27861;&#32463;&#24120;&#23548;&#33268;&#26174;&#30528;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;PII&#30340;&#29983;&#21629;&#8221;--&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#28102;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;PII&#36716;&#21270;&#20026;&#20154;&#36896;PII&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;API&#26469;&#19982;&#32473;&#23450;&#30340;&#25991;&#26723;&#36827;&#34892;&#25509;&#21475;&#65292;&#19968;&#20010;&#22522;&#20110;&#37197;&#32622;&#30340;&#28151;&#28102;&#22120;&#21644;&#19968;&#20010;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#21644;LLMs&#20013;&#34920;&#29616;&#20986;&#39640;&#30340;&#19978;&#19979;&#25991;&#20445;&#23384;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#21407;&#22987;PII&#21644;&#20854;&#36716;&#25442;&#21518;&#30340;&#20154;&#36896;PII&#23545;&#24212;&#30340;&#26144;&#23556;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#28151;&#28102;&#25991;&#26723;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26723;&#30340;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protecting sensitive information is crucial in today's world of Large Language Models (LLMs) and data-driven services. One common method used to preserve privacy is by using data perturbation techniques to reduce overreaching utility of (sensitive) Personal Identifiable Information (PII) data while maintaining its statistical and semantic properties. Data perturbation methods often result in significant information loss, making them impractical for use. In this paper, we propose 'Life of PII', a novel Obfuscation Transformer framework for transforming PII into faux-PII while preserving the original information, intent, and context as much as possible. Our approach includes an API to interface with the given document, a configuration-based obfuscator, and a model based on the Transformer architecture, which has shown high context preservation and performance in natural language processing tasks and LLMs.  Our Transformer-based approach learns mapping between the original PII and its tra
&lt;/p&gt;</description></item><item><title>GIFT&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#22810;&#26041;&#23545;&#35805;&#29702;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#22235;&#31181;&#31867;&#22411;&#30340;&#36793;&#32536;&#23558;&#22270;&#24863;&#30693;&#20449;&#24687;&#38598;&#25104;&#21040;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#25913;&#36827;&#20102;&#21407;&#22987;&#30340;&#39034;&#24207;&#25991;&#26412;&#22788;&#29702;&#30340;PLM&#12290;</title><link>http://arxiv.org/abs/2305.09360</link><description>&lt;p&gt;
GIFT: &#22522;&#20110;&#22270;&#24863;&#30693;&#24494;&#35843;&#30340;&#22810;&#26041;&#23545;&#35805;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding. (arXiv:2305.09360v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09360
&lt;/p&gt;
&lt;p&gt;
GIFT&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#22810;&#26041;&#23545;&#35805;&#29702;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#22235;&#31181;&#31867;&#22411;&#30340;&#36793;&#32536;&#23558;&#22270;&#24863;&#30693;&#20449;&#24687;&#38598;&#25104;&#21040;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#25913;&#36827;&#20102;&#21407;&#22987;&#30340;&#39034;&#24207;&#25991;&#26412;&#22788;&#29702;&#30340;PLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#35841;&#19982;&#35841;&#22312;&#22810;&#26041;&#23545;&#35805;&#20013;&#35828;&#20102;&#20160;&#20040;&#30340;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#24456;&#22810;&#30740;&#31350;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#26041;&#23545;&#35805;&#29702;&#35299;&#26041;&#27861;&#36890;&#24120;&#23558;&#35828;&#35805;&#32773;&#21644;&#35805;&#35821;&#23884;&#20837;&#21040;&#39034;&#24207;&#20449;&#24687;&#27969;&#20013;&#65292;&#25110;&#20165;&#21033;&#29992;&#22810;&#26041;&#23545;&#35805;&#20013;&#22266;&#26377;&#22270;&#32467;&#26500;&#30340;&#34920;&#23618;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24863;&#30693;&#24494;&#35843;&#65288;GIFT&#65289;&#30340;&#21363;&#25554;&#21363;&#29992;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#36890;&#29992;&#22810;&#26041;&#23545;&#35805;&#29702;&#35299;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#26222;&#36890;Transformer&#20013;&#65292;&#35805;&#35821;&#20043;&#38388;&#30340;&#20840;&#31561;&#36830;&#25509;&#20250;&#24573;&#30053;&#19968;&#20010;&#35805;&#35821;&#23545;&#21478;&#19968;&#20010;&#35805;&#35821;&#30340;&#31232;&#30095;&#20294;&#26377;&#21306;&#21035;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#21306;&#20998;&#35805;&#35821;&#20043;&#38388;&#30340;&#19981;&#21516;&#20851;&#31995;&#65292;&#35774;&#35745;&#20102;&#22235;&#31181;&#31867;&#22411;&#30340;&#36793;&#32536;&#20197;&#23558;&#22270;&#24863;&#30693;&#20449;&#21495;&#38598;&#25104;&#21040;&#27880;&#24847;&#26426;&#21046;&#20013;&#65292;&#20197;&#25913;&#36827;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#39034;&#24207;&#25991;&#26412;&#30340;PLMs&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;GIFT&#23454;&#29616;&#21040;&#19977;&#20010;PLMs&#24182;&#23545;&#20854;&#36827;&#34892;&#27979;&#35797;&#26469;&#35780;&#20272;GIFT&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing the issues of who saying what to whom in multi-party conversations (MPCs) has recently attracted a lot of research attention. However, existing methods on MPC understanding typically embed interlocutors and utterances into sequential information flows, or utilize only the superficial of inherent graph structures in MPCs. To this end, we present a plug-and-play and lightweight method named graph-induced fine-tuning (GIFT) which can adapt various Transformer-based pre-trained language models (PLMs) for universal MPC understanding. In detail, the full and equivalent connections among utterances in regular Transformer ignore the sparse but distinctive dependency of an utterance on another in MPCs. To distinguish different relationships between utterances, four types of edges are designed to integrate graph-induced signals into attention mechanisms to refine PLMs originally designed for processing sequential texts. We evaluate GIFT by implementing it into three PLMs, and test the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#35843;&#25972;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#27169;&#22411;&#65292;&#20849;&#21516;&#25552;&#21462;RCT&#25253;&#21578;&#20013;&#30340;&#24178;&#39044;&#12289;&#32467;&#26524;&#21644;&#21457;&#29616;&#20449;&#24687;&#65292;&#23454;&#29616;&#30456;&#24403;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.03642</link><description>&lt;p&gt;
LLM&#27169;&#22411;&#20849;&#21516;&#25552;&#21462;RCT&#25253;&#21578;&#20013;&#24178;&#39044;&#12289;&#32467;&#26524;&#21644;&#21457;&#29616;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs. (arXiv:2305.03642v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#35843;&#25972;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#27169;&#22411;&#65292;&#20849;&#21516;&#25552;&#21462;RCT&#25253;&#21578;&#20013;&#30340;&#24178;&#39044;&#12289;&#32467;&#26524;&#21644;&#21457;&#29616;&#20449;&#24687;&#65292;&#23454;&#29616;&#30456;&#24403;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65288;RCT&#65289;&#30340;&#32467;&#26524;&#30830;&#23450;&#24178;&#39044;&#25514;&#26045;&#30340;&#30456;&#23545;&#26377;&#25928;&#24615;&#65292;&#36827;&#32780;&#25104;&#20026;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#30103;&#20445;&#20581;&#30340;&#20851;&#38190;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;RCT&#32467;&#26524;&#20197;&#65288;&#36890;&#24120;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#65289;&#33258;&#28982;&#35821;&#35328;&#25991;&#31456;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#25551;&#36848;&#35797;&#39564;&#30340;&#35774;&#35745;&#12289;&#25191;&#34892;&#21644;&#32467;&#26524;&#65307;&#20020;&#24202;&#21307;&#29983;&#24517;&#39035;&#20174;&#36825;&#20123;&#25991;&#31456;&#20013;&#25163;&#21160;&#25552;&#21462;&#26377;&#20851;&#25152;&#20851;&#27880;&#30340;&#24178;&#39044;&#25514;&#26045;&#21644;&#32467;&#26524;&#30340;&#21457;&#29616;&#12290;&#36825;&#31181;&#32321;&#29712;&#30340;&#25163;&#21160;&#36807;&#31243;&#20419;&#20351;&#20154;&#20204;&#21033;&#29992;&#65288;&#21322;&#65289;&#33258;&#21160;&#21270;&#30340;&#26041;&#24335;&#20174;&#35797;&#39564;&#25253;&#21578;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#35777;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#22522;&#20110;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#20020;&#24202;&#25688;&#35201;&#20013;&#20849;&#21516;&#25552;&#21462;&#24178;&#39044;&#25514;&#26045;&#12289;&#32467;&#26524;&#21644;&#27604;&#36739;&#22240;&#32032;&#65288;ICO&#20803;&#32032;&#65289;&#65292;&#24182;&#25512;&#26029;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#20154;&#24037;&#65288;&#19987;&#23478;&#65289;&#21644;&#33258;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;&#23558;&#35777;&#25454;&#25552;&#21462;&#26694;&#26550;&#20316;&#20026;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#20026;&#27492;&#30446;&#30340;&#24494;&#35843;LLMs&#21487;&#20197;&#23454;&#29616;&#30456;&#24403;&#22823;&#30340;&#65288;&#32422;20&#20010;&#28857;&#65289;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Results from Randomized Controlled Trials (RCTs) establish the comparative effectiveness of interventions, and are in turn critical inputs for evidence-based care. However, results from RCTs are presented in (often unstructured) natural language articles describing the design, execution, and outcomes of trials; clinicians must manually extract findings pertaining to interventions and outcomes of interest from such articles. This onerous manual process has motivated work on (semi-)automating extraction of structured evidence from trial reports. In this work we propose and evaluate a text-to-text model built on instruction-tuned Large Language Models (LLMs) to jointly extract Interventions, Outcomes, and Comparators (ICO elements) from clinical abstracts, and infer the associated results reported. Manual (expert) and automated evaluations indicate that framing evidence extraction as a conditional generation task and fine-tuning LLMs for this purpose realizes considerable ($\sim$20 point 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#20851;&#32852;&#30340;&#27874;&#26031;&#35821;&#31038;&#20132;&#23186;&#20307;&#35805;&#39064;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#35789;&#27719;&#20851;&#32852;&#21644;&#20851;&#32852;&#24341;&#21147;&#21147;&#37327;&#29983;&#25104;&#22270;&#65292;&#24182;&#36890;&#36807;&#23884;&#20837;&#21644;&#32858;&#31867;&#26041;&#27861;&#25552;&#21462;&#35805;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.09775</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#20851;&#32852;&#21644;&#22270;&#23884;&#20837;&#30340;&#27874;&#26031;&#35821;&#35805;&#39064;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Persian topic detection based on Human Word association and graph embedding. (arXiv:2302.09775v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#20851;&#32852;&#30340;&#27874;&#26031;&#35821;&#31038;&#20132;&#23186;&#20307;&#35805;&#39064;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#35789;&#27719;&#20851;&#32852;&#21644;&#20851;&#32852;&#24341;&#21147;&#21147;&#37327;&#29983;&#25104;&#22270;&#65292;&#24182;&#36890;&#36807;&#23884;&#20837;&#21644;&#32858;&#31867;&#26041;&#27861;&#25552;&#21462;&#35805;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#20851;&#32852;&#30340;&#31038;&#20132;&#23186;&#20307;&#35805;&#39064;&#26816;&#27979;&#26694;&#26550;&#12290;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#35782;&#21035;&#35752;&#35770;&#30340;&#35805;&#39064;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#36825;&#20010;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#26159;&#22312;&#33521;&#35821;&#26041;&#38754;&#30340;&#65292;&#20294;&#26159;&#22312;&#27874;&#26031;&#35821;&#26041;&#38754;&#20063;&#20570;&#20102;&#24456;&#22810;&#24037;&#20316;&#65292;&#29305;&#21035;&#26159;&#27874;&#26031;&#35821;&#30340;&#24494;&#21338;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#26356;&#22810;&#22320;&#20851;&#27880;&#25506;&#32034;&#39057;&#32321;&#27169;&#24335;&#25110;&#35821;&#20041;&#20851;&#31995;&#65292;&#24573;&#35270;&#20102;&#35821;&#35328;&#30340;&#32467;&#26500;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#35789;&#27719;&#20851;&#32852;&#30340;&#35805;&#39064;&#26816;&#27979;&#26694;&#26550;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#20102;&#27169;&#20223;&#24515;&#29702;&#33021;&#21147;&#36827;&#34892;&#35789;&#27719;&#20851;&#32852;&#12290;&#35813;&#26041;&#27861;&#36824;&#35745;&#31639;&#20986;&#20102;&#20851;&#32852;&#24341;&#21147;&#21147;&#37327;&#26469;&#26174;&#31034;&#35789;&#35821;&#30340;&#20851;&#32852;&#31243;&#24230;&#12290;&#21033;&#29992;&#36825;&#20010;&#21442;&#25968;&#65292;&#21487;&#20197;&#29983;&#25104;&#19968;&#20010;&#22270;&#65292;&#36890;&#36807;&#23884;&#20837;&#36825;&#20010;&#22270;&#24182;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;&#20986;&#35805;&#39064;&#12290;&#36825;&#20010;&#26041;&#27861;&#24050;&#32463;&#24212;&#29992;&#20110;&#20174;Telegram&#25910;&#38598;&#30340;&#27874;&#26031;&#35821;&#25968;&#25454;&#38598;&#19978;&#12290;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a framework to detect topics in social media based on Human Word Association. Identifying topics discussed in these media has become a critical and significant challenge. Most of the work done in this area is in English, but much has been done in the Persian language, especially microblogs written in Persian. Also, the existing works focused more on exploring frequent patterns or semantic relationships and ignored the structural methods of language. In this paper, a topic detection framework using HWA, a method for Human Word Association, is proposed. This method uses the concept of imitation of mental ability for word association. This method also calculates the Associative Gravity Force that shows how words are related. Using this parameter, a graph can be generated. The topics can be extracted by embedding this graph and using clustering methods. This approach has been applied to a Persian language dataset collected from Telegram. Several experimental studi
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;PPOCoder&#26694;&#26550;&#23558;&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#21644;Proximal Policy Optimization&#25216;&#26415;&#32467;&#21512;&#65292;&#36890;&#36807;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#21644;&#32467;&#26500;&#23545;&#40784;&#30340;&#38750;&#21487;&#24494;&#21453;&#39304;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2301.13816</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Execution-based Code Generation using Deep Reinforcement Learning. (arXiv:2301.13816v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13816
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;PPOCoder&#26694;&#26550;&#23558;&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#21644;Proximal Policy Optimization&#25216;&#26415;&#32467;&#21512;&#65292;&#36890;&#36807;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#21644;&#32467;&#26500;&#23545;&#40784;&#30340;&#38750;&#21487;&#24494;&#21453;&#39304;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22312;&#22823;&#35268;&#27169;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#65288;PL&#65289;&#27169;&#22411;&#65292;&#20316;&#20026;&#33258;&#21160;&#21270;&#36719;&#20214;&#24037;&#31243;&#36807;&#31243;&#30340;&#25163;&#27573;&#65292;&#22312;&#20195;&#30721;&#23436;&#25104;&#12289;&#20195;&#30721;&#32763;&#35793;&#21644;&#31243;&#24207;&#21512;&#25104;&#31561;&#21508;&#31181;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#25991;&#26412;&#29983;&#25104;&#20013;&#20511;&#29992;&#30340;&#30417;&#30563;&#24494;&#35843;&#30446;&#26631;&#65292;&#24573;&#35270;&#20102;&#20195;&#30721;&#30340;&#29420;&#29305;&#24207;&#21015;&#32423;&#29305;&#24449;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#21487;&#32534;&#35793;&#24615;&#20197;&#21450;&#35821;&#27861;&#21644;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PPOCoder&#65292;&#19968;&#31181;&#26032;&#30340;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;PL&#27169;&#22411;&#19982;Proximal Policy Optimization&#65288;PPO&#65289;&#30456;&#32467;&#21512;&#65292;PPO&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#12290;&#36890;&#36807;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#21644;&#32467;&#26500;&#23545;&#40784;&#30340;&#38750;&#21487;&#24494;&#21453;&#39304;&#65292;PPOCoder&#23558;&#22806;&#37096;&#20195;&#30721;&#29305;&#23450;&#30693;&#35782;&#26080;&#32541;&#38598;&#25104;&#21040;&#27169;&#22411;&#20248;&#21270;&#36807;&#31243;&#20013;&#12290;&#36825;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#32852;&#24819;&#30340;&#31038;&#20132;&#32593;&#32476;&#20027;&#39064;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#35821;&#35328;&#32467;&#26500;&#24182;&#35774;&#35745;&#19987;&#38376;&#30340;&#25277;&#21462;&#31639;&#27861;&#65292;&#22312;FA-CUP&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13066</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#32852;&#24819;&#30340;&#31038;&#20132;&#32593;&#32476;&#20027;&#39064;&#26816;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Human Word Association based model for topic detection in social networks. (arXiv:2301.13066v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#32852;&#24819;&#30340;&#31038;&#20132;&#32593;&#32476;&#20027;&#39064;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#35821;&#35328;&#32467;&#26500;&#24182;&#35774;&#35745;&#19987;&#38376;&#30340;&#25277;&#21462;&#31639;&#27861;&#65292;&#22312;FA-CUP&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#32593;&#32476;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#26816;&#27979;&#36825;&#20123;&#32593;&#32476;&#20013;&#35752;&#35770;&#30340;&#20027;&#39064;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#22522;&#20110;&#39057;&#32321;&#27169;&#24335;&#25366;&#25496;&#25110;&#35821;&#20041;&#20851;&#31995;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#35821;&#35328;&#32467;&#26500;&#12290;&#35821;&#35328;&#32467;&#26500;&#26041;&#27861;&#30340;&#24847;&#20041;&#22312;&#20110;&#21457;&#29616;&#35789;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#20154;&#31867;&#22914;&#20309;&#29702;&#35299;&#23427;&#20204;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#21033;&#29992;&#35789;&#27719;&#32852;&#24819;&#30340;&#24515;&#29702;&#33021;&#21147;&#27169;&#25311;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#32852;&#24819;&#30340;&#31038;&#20132;&#32593;&#32476;&#20027;&#39064;&#26816;&#27979;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#32852;&#24819;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19987;&#38376;&#30340;&#25277;&#21462;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;FA-CUP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20027;&#39064;&#26816;&#27979;&#39046;&#22495;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20027;&#39064;&#21484;&#22238;&#29575;&#21644;&#20851;&#38190;&#35789;F1&#20540;&#19978;&#26377;&#36739;&#22909;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#20027;&#39064;&#26816;&#27979;&#39046;&#22495;&#20013;&#30340;&#22823;&#22810;&#25968;&#20808;&#21069;&#24037;&#20316;&#20027;&#35201;&#22522;&#20110;&#27169;&#24335;&#25366;&#25496;&#25110;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread use of social networks, detecting the topics discussed in these networks has become a significant challenge. The current works are mainly based on frequent pattern mining or semantic relations, and the language structure is not considered. The meaning of language structural methods is to discover the relationship between words and how humans understand them. Therefore, this paper uses the Concept of the Imitation of the Mental Ability of Word Association to propose a topic detection framework in social networks. This framework is based on the Human Word Association method. A special extraction algorithm has also been designed for this purpose. The performance of this method is evaluated on the FA-CUP dataset. It is a benchmark dataset in the field of topic detection. The results show that the proposed method is a good improvement compared to other methods, based on the Topic-recall and the keyword F1 measure. Also, most of the previous works in the field of topic de
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#23545;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#24378;&#38544;&#31169;&#20445;&#25252;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#25991;&#26412;&#65292;&#24182;&#19988;&#19982;&#38750;&#38544;&#31169;&#29256;&#26412;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2210.14348</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#21512;&#25104;&#25991;&#26412;&#29983;&#25104;&#65306;&#19968;&#20010;&#31616;&#21333;&#32780;&#23454;&#29992;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe. (arXiv:2210.14348v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14348
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#23545;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#24378;&#38544;&#31169;&#20445;&#25252;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#25991;&#26412;&#65292;&#24182;&#19988;&#19982;&#38750;&#38544;&#31169;&#29256;&#26412;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#25935;&#24863;&#25968;&#25454;&#30340;&#35760;&#24518;&#20542;&#21521;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#38544;&#31169;&#38382;&#39064;&#25104;&#20026;&#25968;&#25454;&#39537;&#21160;&#20135;&#21697;&#30340;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#29983;&#25104;&#20855;&#26377;&#24418;&#24335;&#38544;&#31169;&#20445;&#35777;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#22914;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#65292;&#20026;&#32531;&#35299;&#36825;&#20123;&#38544;&#31169;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#20294;&#26159;&#65292;&#20197;&#24448;&#30340;&#25351;&#21521;&#27492;&#26041;&#21521;&#30340;&#26041;&#27861;&#36890;&#24120;&#26410;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24046;&#20998;&#38544;&#31169;&#19979;&#25991;&#26412;&#39046;&#22495;&#30340;&#19968;&#20010;&#31616;&#21333;&#23454;&#29992;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#23545;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#24182;&#21152;&#20837;DP&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#24378;&#38544;&#31169;&#20445;&#25252;&#30340;&#26377;&#29992;&#30340;&#21512;&#25104;&#25991;&#26412;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#25968;&#25454;&#21644;&#31169;&#20154;&#23458;&#25143;&#25968;&#25454;&#30340;&#24191;&#27867;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#19982;&#38750;&#38544;&#31169;&#29256;&#26412;&#30456;&#20284;&#30340;&#23454;&#29992;&#24615;&#30340;&#21512;&#25104;&#25991;&#26412;&#65292;&#21516;&#26102;&#25552;&#20379;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#36991;&#20813;&#28508;&#22312;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy concerns have attracted increasing attention in data-driven products due to the tendency of machine learning models to memorize sensitive training data. Generating synthetic versions of such data with a formal privacy guarantee, such as differential privacy (DP), provides a promising path to mitigating these privacy concerns, but previous approaches in this direction have typically failed to produce synthetic data of high quality. In this work, we show that a simple and practical recipe in the text domain is effective: simply fine-tuning a pretrained generative language model with DP enables the model to generate useful synthetic text with strong privacy protection. Through extensive empirical analyses on both benchmark and private customer data, we demonstrate that our method produces synthetic text that is competitive in terms of utility with its non-private counterpart, meanwhile providing strong protection against potential privacy leakages.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InitialGAN&#30340;&#35821;&#35328;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#37319;&#29992;dropout&#37319;&#26679;&#21644;&#23436;&#20840;&#24402;&#19968;&#21270;&#30340;LSTM&#31561;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#35821;&#35328;GAN&#27169;&#22411;&#20381;&#36182;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#38543;&#26426;&#21021;&#22987;&#21270;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2208.02531</link><description>&lt;p&gt;
InitialGAN&#65306;&#19968;&#31181;&#23436;&#20840;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#35821;&#35328;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
InitialGAN: A Language GAN with Completely Random Initialization. (arXiv:2208.02531v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InitialGAN&#30340;&#35821;&#35328;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#37319;&#29992;dropout&#37319;&#26679;&#21644;&#23436;&#20840;&#24402;&#19968;&#21270;&#30340;LSTM&#31561;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#35821;&#35328;GAN&#27169;&#22411;&#20381;&#36182;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#38543;&#26426;&#21021;&#22987;&#21270;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#35757;&#32451;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#23384;&#22312;&#8220;&#26333;&#20809;&#20559;&#24046;&#8221;&#38382;&#39064;&#65292;&#32780;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#34987;&#35777;&#26126;&#21487;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#35821;&#35328;GAN&#37319;&#29992;REINFORCE&#25110;&#36830;&#32493;&#24347;&#35947;&#31561;&#20272;&#35745;&#22120;&#26469;&#24314;&#27169;&#35789;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20272;&#35745;&#22120;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#23548;&#33268;&#24403;&#21069;&#27169;&#22411;&#20381;&#36182;&#39044;&#35757;&#32451;&#25216;&#26415;&#65288;MLE&#39044;&#35757;&#32451;&#25110;&#39044;&#35757;&#32451;&#23884;&#20837;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20197;&#21069;&#30340;&#23581;&#35797;&#20013;&#36825;&#20123;&#34920;&#31034;&#24314;&#27169;&#26041;&#27861;&#30340;&#24615;&#33021;&#36739;&#24046;&#65292;&#22240;&#27492;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#26080;&#25928;&#30340;&#37319;&#26679;&#26041;&#27861;&#21644;&#19981;&#20581;&#24247;&#30340;&#26799;&#24230;&#26159;&#23548;&#33268;&#36825;&#31181;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#25216;&#24039;&#65306;dropout&#37319;&#26679;&#21644;&#23436;&#20840;&#24402;&#19968;&#21270;&#30340;LSTM&#12290;&#22522;&#20110;&#36825;&#20004;&#31181;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23436;&#20840;&#38543;&#26426;&#21021;&#22987;&#21270;&#21442;&#25968;&#30340;InitialGAN&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;
&lt;/p&gt;
&lt;p&gt;
Text generative models trained via Maximum Likelihood Estimation (MLE) suffer from the notorious exposure bias problem, and Generative Adversarial Networks (GANs) are shown to have potential to tackle this problem. Existing language GANs adopt estimators like REINFORCE or continuous relaxations to model word probabilities. The inherent limitations of such estimators lead current models to rely on pre-training techniques (MLE pre-training or pre-trained embeddings). Representation modeling methods which are free from those limitations, however, are seldomly explored because of their poor performance in previous attempts. Our analyses reveal that invalid sampling methods and unhealthy gradients are the main contributors to such unsatisfactory performance. In this work, we present two techniques to tackle these problems: dropout sampling and fully normalized LSTM. Based on these two techniques, we propose InitialGAN whose parameters are randomly initialized in full. Besides, we introduce 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;PMI&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#21644;&#37327;&#21270;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;&#35299;&#37322;&#21644;&#32479;&#35745;&#26174;&#33879;&#24615;&#65292;&#24182;&#22312;&#25429;&#25417;&#24615;&#21035;&#24046;&#36317;&#26041;&#38754;&#20135;&#29983;&#20102;&#31867;&#20284;&#20110;&#22522;&#20110;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2104.06474</link><description>&lt;p&gt;
&#23545;&#25991;&#26412;&#20013;&#20559;&#35265;&#24230;&#37327;&#30340;&#35299;&#37322;&#24615;&#21644;&#37325;&#35201;&#24615;&#65306;&#19968;&#31181;&#22522;&#20110;PMI&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Interpretability and Significance of Bias Metrics in Texts: a PMI-based Approach. (arXiv:2104.06474v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.06474
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;PMI&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#21644;&#37327;&#21270;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;&#35299;&#37322;&#21644;&#32479;&#35745;&#26174;&#33879;&#24615;&#65292;&#24182;&#22312;&#25429;&#25417;&#24615;&#21035;&#24046;&#36317;&#26041;&#38754;&#20135;&#29983;&#20102;&#31867;&#20284;&#20110;&#22522;&#20110;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35789;&#23884;&#20837;&#24191;&#27867;&#29992;&#20110;&#27979;&#37327;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#12290;&#23613;&#31649;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#22312;&#26816;&#27979;&#21508;&#31181;&#20559;&#35265;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#22522;&#20110;&#35789;&#23884;&#20837;&#30340;&#24230;&#37327;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;PMI&#30340;&#26367;&#20195;&#24230;&#37327;&#26041;&#27861;&#26469;&#37327;&#21270;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#12290;&#23427;&#21487;&#20197;&#34920;&#31034;&#20026;&#26465;&#20214;&#27010;&#29575;&#30340;&#20989;&#25968;&#65292;&#21487;&#20197;&#31616;&#21333;&#22320;&#35299;&#37322;&#20026;&#35789;&#35821;&#20849;&#29616;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#23427;&#21487;&#20197;&#36817;&#20284;&#20026;&#19968;&#31181;&#27604;&#20540;&#27604;&#65292;&#36825;&#20801;&#35768;&#20272;&#35745;&#25991;&#26412;&#20559;&#35265;&#30340;&#32622;&#20449;&#21306;&#38388;&#21644;&#32479;&#35745;&#26174;&#33879;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#25429;&#25417;&#23884;&#20837;&#22823;&#22411;&#35821;&#26009;&#24211;&#20013;&#30495;&#23454;&#19990;&#30028;&#30340;&#24615;&#21035;&#24046;&#36317;&#26102;&#20135;&#29983;&#31867;&#20284;&#20110;&#22522;&#20110;&#35789;&#23884;&#20837;&#30340;&#24230;&#37327;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, word embeddings have been widely used to measure biases in texts. Even if they have proven to be effective in detecting a wide variety of biases, metrics based on word embeddings lack transparency and interpretability. We analyze an alternative PMI-based metric to quantify biases in texts. It can be expressed as a function of conditional probabilities, which provides a simple interpretation in terms of word co-occurrences. We also prove that it can be approximated by an odds ratio, which allows estimating confidence intervals and statistical significance of textual biases. This approach produces similar results to metrics based on word embeddings when capturing gender gaps of the real world embedded in large corpora.
&lt;/p&gt;</description></item></channel></rss>