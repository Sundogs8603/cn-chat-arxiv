<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15506</link><description>&lt;p&gt;
AgentOhana&#65306;&#20026;&#26377;&#25928;&#26234;&#33021;&#20307;&#23398;&#20064;&#35774;&#35745;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15506
&lt;/p&gt;
&lt;p&gt;
AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#25903;&#25345;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#24341;&#36215;&#20102;&#37325;&#22823;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#36827;&#34892;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#38754;&#20020;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#20855;&#26377;&#22810;&#36718;&#36712;&#36857;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#28304;&#30340;&#24322;&#26500;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;AgentOhana&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;AgentOhana&#20174;&#19981;&#21516;&#29615;&#22659;&#20013;&#32858;&#21512;&#26234;&#33021;&#20307;&#36712;&#36857;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24773;&#26223;&#12290;&#23427;&#31934;&#24515;&#22320;&#23558;&#36825;&#20123;&#36712;&#36857;&#26631;&#20934;&#21270;&#21644;&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#26684;&#24335;&#20013;&#65292;&#31616;&#21270;&#20102;&#20026;&#26234;&#33021;&#20307;&#35757;&#32451;&#20248;&#21270;&#30340;&#36890;&#29992;&#25968;&#25454;&#21152;&#36733;&#22120;&#30340;&#21019;&#24314;&#12290;&#36890;&#36807;&#25968;&#25454;&#32479;&#19968;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;&#27969;&#27700;&#32447;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#21010;&#20998;&#21644;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#35774;&#22791;&#20043;&#38388;&#30340;&#29420;&#31435;&#38543;&#26426;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;xLAM-v0.1&#65292;&#19968;&#20010;&#22823;&#21160;&#20316;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15506v1 Announce Type: new  Abstract: Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address these challenges. \textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \textbf{xLAM-v0.1}, a large action mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;API-BLEND&#65292;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#31995;&#32479;&#27979;&#35797;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;&#33719;&#21462;&#28041;&#21450;&#35843;&#29992;&#24037;&#20855;/API&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#27169;&#25311;&#30495;&#23454;&#22330;&#26223;&#30340;API&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.15491</link><description>&lt;p&gt;
API-BLEND&#65306;&#29992;&#20110;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;API LLM&#30340;&#32508;&#21512;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;API-BLEND&#65292;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#31995;&#32479;&#27979;&#35797;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;&#33719;&#21462;&#28041;&#21450;&#35843;&#29992;&#24037;&#20855;/API&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#27169;&#25311;&#30495;&#23454;&#22330;&#26223;&#30340;API&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#25928;&#20351;&#29992;&#24037;&#20855;&#21644;&#22806;&#37096;&#24212;&#29992;&#31243;&#24207;&#32534;&#31243;&#25509;&#21475;&#65288;APIs&#65289;&#26469;&#35268;&#21010;&#21644;&#23436;&#25104;&#20219;&#21153;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#38271;&#65292;&#23545;&#21487;&#20197;&#33719;&#21462;&#28041;&#21450;&#35843;&#29992;&#24037;&#20855;/API&#30340;&#36275;&#22815;&#25968;&#37327;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#20851;&#27880;&#35782;&#21035;&#12289;&#25972;&#29702;&#21644;&#36716;&#21270;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;API-BLEND&#65292;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#31995;&#32479;&#27979;&#35797;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#27169;&#25311;&#28041;&#21450;API&#20219;&#21153;&#30340;&#30495;&#23454;&#22330;&#26223;&#65292;&#22914;API/&#24037;&#20855;&#26816;&#27979;&#12289;&#27133;&#22635;&#20805;&#20197;&#21450;&#26816;&#27979;&#21040;&#30340;API&#30340;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15491v1 Announce Type: cross  Abstract: There is a growing need for Large Language Models (LLMs) to effectively use tools and external Application Programming Interfaces (APIs) to plan and complete tasks. As such, there is tremendous interest in methods that can acquire sufficient quantities of train and test data that involve calls to tools / APIs. Two lines of research have emerged as the predominant strategies for addressing this challenge. The first has focused on synthetic data generation techniques, while the second has involved curating task-adjacent datasets which can be transformed into API / Tool-based tasks. In this paper, we focus on the task of identifying, curating, and transforming existing datasets and, in turn, introduce API-BLEND, a large corpora for training and systematic testing of tool-augmented LLMs. The datasets mimic real-world scenarios involving API-tasks such as API / tool detection, slot filling, and sequencing of the detected APIs. We demonstrat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Prejudice-Caprice Framework&#65288;PCF&#65289;&#26469;&#20840;&#38754;&#34913;&#37327;LLMs&#20013;&#30340;&#27495;&#35270;&#65292;&#32771;&#34385;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#19968;&#36143;&#20559;&#35265;&#20559;&#22909;&#21644;&#20559;&#22909;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.15481</link><description>&lt;p&gt;
&#20559;&#35265;&#21644;&#21453;&#22797;&#26080;&#24120;&#65306;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31038;&#20250;&#27495;&#35270;&#30340;&#32479;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15481
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Prejudice-Caprice Framework&#65288;PCF&#65289;&#26469;&#20840;&#38754;&#34913;&#37327;LLMs&#20013;&#30340;&#27495;&#35270;&#65292;&#32771;&#34385;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#19968;&#36143;&#20559;&#35265;&#20559;&#22909;&#21644;&#20559;&#22909;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15481v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31038;&#20250;&#36816;&#33829;&#20013;&#30340;&#26085;&#30410;&#34701;&#21512;&#21152;&#21095;&#20102;&#23427;&#20204;&#23545;&#32463;&#27982;&#12289;&#27861;&#24459;&#12289;&#25945;&#32946;&#21644;&#21307;&#30103;&#31561;&#37325;&#35201;&#39046;&#22495;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#24341;&#21457;&#20102;&#20844;&#20247;&#23545;&#36825;&#20123;&#27169;&#22411;&#28041;&#21450;&#27495;&#35270;&#23433;&#20840;&#21644;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#27495;&#35270;&#27979;&#37327;&#26694;&#26550;&#20165;&#35780;&#20272;LLMs&#30340;&#24179;&#22343;&#27495;&#35270;&#34892;&#20026;&#65292;&#24448;&#24448;&#30001;&#20110;&#24573;&#35270;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#23548;&#33268;&#27495;&#35270;&#30340;&#22240;&#32032;&#65292;&#21363;LLMs&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#39044;&#27979;&#21464;&#21270;&#32780;&#21464;&#24471;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prejudice-Caprice Framework&#65288;PCF&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;LLMs&#30340;&#19968;&#36143;&#20559;&#35265;&#20559;&#22909;&#21644;&#22312;&#22810;&#26679;&#19978;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15481v1 Announce Type: new  Abstract: The growing integration of large language models (LLMs) into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models' discrimination-related safety and reliability. However, prior discrimination measuring frameworks solely assess the average discriminatory behavior of LLMs, often proving inadequate due to the overlook of an additional discrimination-leading factor, i.e., the LLMs' prediction variation across diverse contexts. In this work, we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in LLMs by considering both their consistently biased preference and preference variation across diverse contexts. Specifically, we mathematically dissect the aggregated contextualized discrimination risk of LLMs into prejudice risk, originating from LLMs' persistent prejudice, and caprice risk, stemmin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;RLHF&#20013;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#26469;&#38477;&#20302;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#25152;&#38656;&#30340;&#22823;&#37327;&#20154;&#31867;&#20559;&#22909;&#27880;&#37322;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.15473</link><description>&lt;p&gt;
&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#22312;RLHF&#20013;&#39640;&#25928;&#24314;&#27169;&#22870;&#21169;&#65306;&#30005;&#23376;&#21830;&#21153;&#24847;&#35265;&#25688;&#35201;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study in E-Commerce Opinion Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15473
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;RLHF&#20013;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#26469;&#38477;&#20302;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#25152;&#38656;&#30340;&#22823;&#37327;&#20154;&#31867;&#20559;&#22909;&#27880;&#37322;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#25104;&#20026;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26397;&#21521;&#20154;&#31867;&#20215;&#20540;/&#30446;&#26631;&#30340;&#20027;&#23548;&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#30340;&#20851;&#38190;&#22312;&#20110;&#20351;&#29992;&#19968;&#20010;&#33021;&#22815;&#21453;&#26144;&#19982;&#20154;&#31867;&#30456;&#20851;&#30340;&#28508;&#22312;&#22870;&#21169;&#27169;&#22411;&#30340;&#22870;&#21169;&#27169;&#22411;&#65288;{$\varphi$}&#65289;&#12290;&#34429;&#28982;&#36825;&#19968;&#31574;&#30053;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#35757;&#32451;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#20154;&#31867;&#20559;&#22909;&#27880;&#37322;&#65288;&#36890;&#24120;&#25968;&#37327;&#32423;&#20026;&#25968;&#19975;&#65289;&#26469;&#35757;&#32451;{$\varphi$}&#12290;&#22914;&#26524;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#34987;&#26222;&#36941;&#20351;&#29992;&#65292;&#36825;&#31181;&#22823;&#35268;&#27169;&#20559;&#22909;&#27880;&#37322;&#26159;&#21487;&#20197;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#20215;&#20540;/&#30446;&#26631;&#26159;&#20027;&#35266;&#30340;&#65292;&#24182;&#19988;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#24615;&#36136;&#12290;&#36825;&#23545;&#20110;&#25910;&#38598;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#30340;&#22810;&#26679;&#21270;&#20559;&#22909;&#26500;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#34701;&#20837;{$\varphi$}&#20013;&#65292;&#20174;&#32780;&#20943;&#23569;&#25152;&#38656;&#27880;&#37322;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#22312;&#30005;&#23376;&#21830;&#21153;&#24847;&#35265;&#25688;&#35201;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15473v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a dominating strategy in steering Language Models (LMs) towards human values/goals. The key to the strategy is employing a reward model ({$\varphi$}) which can reflect a latent reward model with humans. While this strategy has proven to be effective, the training methodology requires a lot of human preference annotation (usually of the order of tens of thousands) to train {$\varphi$}. Such large-scale preference annotations can be achievable if the reward model can be ubiquitously used. However, human values/goals are subjective and depend on the nature of the task. This poses a challenge in collecting diverse preferences for downstream applications. To address this, we propose a novel methodology to infuse domain knowledge into {$\varphi$}, which reduces the size of preference annotation required. We validate our approach in E-Commerce Opinion Summarization, with a significant
&lt;/p&gt;</description></item><item><title>&#22238;&#22768;&#23884;&#20837;&#26041;&#27861;&#36890;&#36807;&#37325;&#22797;&#36755;&#20837;&#26469;&#25552;&#21462;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#26080;&#27861;&#21253;&#21547;&#21518;&#32493;&#20196;&#29260;&#20449;&#24687;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#20805;&#20998;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23884;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.15449</link><description>&lt;p&gt;
&#37325;&#22797;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Repetition Improves Language Model Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15449
&lt;/p&gt;
&lt;p&gt;
&#22238;&#22768;&#23884;&#20837;&#26041;&#27861;&#36890;&#36807;&#37325;&#22797;&#36755;&#20837;&#26469;&#25552;&#21462;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#26080;&#27861;&#21253;&#21547;&#21518;&#32493;&#20196;&#29260;&#20449;&#24687;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#20805;&#20998;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25913;&#36827;&#20174;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25552;&#21462;&#25991;&#26412;&#23884;&#20837;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#25968;&#25454;&#12289;&#39592;&#24178;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25110;&#36890;&#36807;&#25351;&#20196;&#25913;&#36827;&#20219;&#21153;&#24046;&#24322;&#21270;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#19968;&#20010;&#26550;&#26500;&#38480;&#21046;&#65306;&#20196;&#29260;&#23884;&#20837;&#19981;&#33021;&#21253;&#21547;&#26469;&#33258;&#36755;&#20837;&#20013;&#21518;&#32493;&#20196;&#29260;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#8220;&#22238;&#22768;&#23884;&#20837;&#8221;&#65292;&#20854;&#20013;&#25105;&#20204;&#22312;&#19978;&#19979;&#25991;&#20013;&#23558;&#36755;&#20837;&#37325;&#22797;&#20004;&#27425;&#65292;&#24182;&#20174;&#31532;&#20108;&#27425;&#20986;&#29616;&#20013;&#25552;&#21462;&#23884;&#20837;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26089;&#26399;&#20196;&#29260;&#30340;&#22238;&#22768;&#23884;&#20837;&#21487;&#20197;&#32534;&#30721;&#20851;&#20110;&#21518;&#32493;&#20196;&#29260;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;LLMs&#36827;&#34892;&#23884;&#20837;&#12290;&#22312;MTEB&#25490;&#34892;&#27036;&#19978;&#65292;&#22238;&#22768;&#23884;&#20837;&#22312;&#38646;&#23556;&#20987;&#20013;&#27604;&#32463;&#20856;&#23884;&#20837;&#25552;&#39640;&#20102;&#36229;&#36807;9%&#65292;&#22312;&#24494;&#35843;&#26102;&#25552;&#39640;&#20102;&#32422;0.7%&#12290;&#20351;&#29992;Mistral-7B&#27169;&#22411;&#30340;&#22238;&#22768;&#23884;&#20837;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15449v1 Announce Type: new  Abstract: Recent approaches to improving the extraction of text embeddings from autoregressive large language models (LLMs) have largely focused on improvements to data, backbone pretrained language models, or improving task-differentiation via instructions. In this work, we address an architectural limitation of autoregressive models: token embeddings cannot contain information from tokens that appear later in the input. To address this limitation, we propose a simple approach, "echo embeddings," in which we repeat the input twice in context and extract embeddings from the second occurrence. We show that echo embeddings of early tokens can encode information about later tokens, allowing us to maximally leverage high-quality LLMs for embeddings. On the MTEB leaderboard, echo embeddings improve over classical embeddings by over 9% zero-shot and by around 0.7% when fine-tuned. Echo embeddings with a Mistral-7B model achieve state-of-the-art compared
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#21644;&#27169;&#24577;&#23545;&#25239;&#35757;&#32451;&#65288;AdaMF-MAT&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#20013;&#23384;&#22312;&#30340;&#27169;&#24577;&#20449;&#24687;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21457;&#25381;&#19981;&#24179;&#34913;&#27169;&#24577;&#20449;&#24687;&#30340;&#21147;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.15444</link><description>&lt;p&gt;
&#21457;&#25381;&#19981;&#24179;&#34913;&#27169;&#24577;&#20449;&#24687;&#22312;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Imbalanced Modality Information for Multi-modal Knowledge Graph Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15444
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#21644;&#27169;&#24577;&#23545;&#25239;&#35757;&#32451;&#65288;AdaMF-MAT&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#20013;&#23384;&#22312;&#30340;&#27169;&#24577;&#20449;&#24687;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21457;&#25381;&#19981;&#24179;&#34913;&#27169;&#24577;&#20449;&#24687;&#30340;&#21147;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#65288;MMKGC&#65289;&#26088;&#22312;&#36890;&#36807;&#23558;&#23454;&#20307;&#30340;&#32467;&#26500;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#32435;&#20837;&#21028;&#21035;&#27169;&#22411;&#26469;&#39044;&#27979;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#20013;&#32570;&#22833;&#30340;&#19977;&#20803;&#32452;&#12290;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#23558;&#20849;&#21516;&#24037;&#20316;&#20197;&#34913;&#37327;&#19977;&#20803;&#32452;&#30340;&#21487;&#33021;&#24615;&#12290;&#29616;&#26377;&#30340;MMKGC&#26041;&#27861;&#24573;&#35270;&#20102;&#23454;&#20307;&#20043;&#38388;&#27169;&#24577;&#20449;&#24687;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#27169;&#24577;&#34701;&#21512;&#19981;&#36275;&#20197;&#21450;&#23545;&#21407;&#22987;&#27169;&#24577;&#20449;&#24687;&#30340;&#20302;&#25928;&#21033;&#29992;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#21644;&#27169;&#24577;&#23545;&#25239;&#35757;&#32451;&#65288;AdaMF-MAT&#65289;&#65292;&#20197;&#21457;&#25381;&#19981;&#24179;&#34913;&#27169;&#24577;&#20449;&#24687;&#22312;MMKGC&#20013;&#30340;&#21147;&#37327;&#12290;AdaMF-MAT&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#24577;&#26435;&#37325;&#23454;&#29616;&#22810;&#27169;&#24577;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#27169;&#24577;&#23545;&#25239;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#20197;&#22686;&#24378;&#19981;&#24179;&#34913;&#27169;&#24577;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;MMKGC&#27169;&#22411;&#21644;&#35757;&#32451;&#30340;&#21327;&#21516;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15444v1 Announce Type: new  Abstract: Multi-modal knowledge graph completion (MMKGC) aims to predict the missing triples in the multi-modal knowledge graphs by incorporating structural, visual, and textual information of entities into the discriminant models. The information from different modalities will work together to measure the triple plausibility. Existing MMKGC methods overlook the imbalance problem of modality information among entities, resulting in inadequate modal fusion and inefficient utilization of the raw modality information. To address the mentioned problems, we propose Adaptive Multi-modal Fusion and Modality Adversarial Training (AdaMF-MAT) to unleash the power of imbalanced modality information for MMKGC. AdaMF-MAT achieves multi-modal fusion with adaptive modality weights and further generates adversarial samples by modality-adversarial training to enhance the imbalanced modality information. Our approach is a co-design of the MMKGC model and training s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#21307;&#29983;&#31508;&#35760;&#29983;&#25104;&#24739;&#32773;&#24635;&#32467;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#26631;&#35760;&#21327;&#35758;&#21644;&#21307;&#23398;&#19987;&#23478;&#26631;&#35760;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#26080;&#24187;&#35273;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#33021;&#26377;&#25928;&#20943;&#23569;&#24187;&#35273;&#30340;&#29983;&#25104;&#65292;&#24182;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.15422</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24544;&#23454;&#19988;&#39640;&#36136;&#37327;&#30340;&#30149;&#20154;&#24635;&#32467;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#21307;&#29983;&#31508;&#35760;&#29983;&#25104;&#24739;&#32773;&#24635;&#32467;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#26631;&#35760;&#21327;&#35758;&#21644;&#21307;&#23398;&#19987;&#23478;&#26631;&#35760;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#26080;&#24187;&#35273;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#33021;&#26377;&#25928;&#20943;&#23569;&#24187;&#35273;&#30340;&#29983;&#25104;&#65292;&#24182;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#32773;&#32463;&#24120;&#38754;&#20020;&#38590;&#20197;&#29702;&#35299;&#20854;&#20303;&#38498;&#24773;&#20917;&#30340;&#22256;&#38590;&#65292;&#32780;&#21307;&#25252;&#20154;&#21592;&#36164;&#28304;&#26377;&#38480;&#20197;&#25552;&#20379;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#21307;&#29983;&#31508;&#35760;&#29983;&#25104;&#24739;&#32773;&#24635;&#32467;&#30340;&#28508;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#35757;&#32451;&#25968;&#25454;&#23545;&#29983;&#25104;&#24635;&#32467;&#30340;&#24544;&#23454;&#24615;&#21644;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20005;&#26684;&#30340;&#26631;&#35760;&#21327;&#35758;&#29992;&#20110;&#24187;&#35273;&#65292;&#35753;&#20004;&#20301;&#21307;&#23398;&#19987;&#23478;&#26631;&#35760;&#20102;100&#20010;&#30495;&#23454;&#24635;&#32467;&#21644;100&#20010;&#29983;&#25104;&#30340;&#24635;&#32467;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26080;&#24187;&#35273;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;Llama 2&#27599;&#20010;&#24635;&#32467;&#30340;&#24187;&#35273;&#20174;2.60&#38477;&#20302;&#21040;1.55&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#12290;&#34429;&#28982;&#25928;&#26524;&#20173;&#28982;&#23384;&#22312;&#65292;&#20294;&#24403;&#20351;&#29992;&#20116;&#20010;&#20363;&#23376;&#25552;&#31034;GPT-4&#26102;&#65292;&#35813;&#25928;&#26524;&#35201;&#23567;&#24471;&#22810;&#65288;0.70&#38477;&#33267;0.40&#65289;&#12290;&#25105;&#20204;&#36824;&#23545;&#26080;&#24187;&#35273;&#21644;&#25913;&#36827;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#20102;&#23450;&#24615;&#35780;&#20272;&#12290;&#21363;&#20351;&#22312;&#24187;&#35273;&#33258;&#30001;&#25968;&#25454;&#19979;&#65292;GPT-4&#20063;&#23637;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15422v1 Announce Type: cross  Abstract: Patients often face difficulties in understanding their hospitalizations, while healthcare workers have limited resources to provide explanations. In this work, we investigate the potential of large language models to generate patient summaries based on doctors' notes and study the effect of training data on the faithfulness and quality of the generated summaries. To this end, we develop a rigorous labeling protocol for hallucinations, and have two medical experts annotate 100 real-world summaries and 100 generated summaries. We show that fine-tuning on hallucination-free data effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama 2, while preserving relevant information. Although the effect is still present, it is much smaller for GPT-4 when prompted with five examples (0.70 to 0.40). We also conduct a qualitative evaluation using hallucination-free and improved training data. GPT-4 shows very good results even in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#38646;&#26679;&#26412;&#35821;&#35328;&#25512;&#29702;&#26469;&#21010;&#20998;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#26597;&#35810;&#20449;&#24687;&#24182;&#37325;&#26032;&#23450;&#20041;&#22870;&#21169;&#23398;&#20064;&#30446;&#26631;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15420</link><description>&lt;p&gt;
PREDILECT&#65306;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#38646;&#26679;&#26412;&#35821;&#35328;&#25512;&#29702;&#21010;&#20998;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#38646;&#26679;&#26412;&#35821;&#35328;&#25512;&#29702;&#26469;&#21010;&#20998;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#26597;&#35810;&#20449;&#24687;&#24182;&#37325;&#26032;&#23450;&#20041;&#22870;&#21169;&#23398;&#20064;&#30446;&#26631;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26032;&#39046;&#22495;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#20154;&#31867;&#36890;&#36807;&#23545;&#19981;&#21516;&#29366;&#24577;-&#21160;&#20316;&#24207;&#21015;&#34920;&#36798;&#20559;&#22909;&#26469;&#22609;&#36896;&#26426;&#22120;&#20154;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20026;&#26426;&#22120;&#20154;&#21046;&#23450;&#29616;&#23454;&#25919;&#31574;&#38656;&#35201;&#20154;&#31867;&#23545;&#22823;&#37327;&#26597;&#35810;&#30340;&#21709;&#24212;&#12290;&#26412;&#24037;&#20316;&#36890;&#36807;&#25193;&#23637;&#27599;&#20010;&#26597;&#35810;&#25910;&#38598;&#30340;&#20449;&#24687;&#65292;&#21253;&#21547;&#20559;&#22909;&#21644;&#21487;&#36873;&#25991;&#26412;&#25552;&#31034;&#65292;&#26469;&#35299;&#20915;&#26679;&#26412;&#25928;&#29575;&#25361;&#25112;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#26469;&#20174;&#20154;&#31867;&#25552;&#20379;&#30340;&#25991;&#26412;&#20013;&#36827;&#34892;&#25512;&#29702;&#12290;&#20026;&#20102;&#36866;&#24212;&#39069;&#22806;&#30340;&#26597;&#35810;&#20449;&#24687;&#65292;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#20102;&#22870;&#21169;&#23398;&#20064;&#30446;&#26631;&#65292;&#21253;&#21547;&#28789;&#27963;&#30340;&#37325;&#28857; &#8212;&#8212; &#21253;&#21547;&#30456;&#23545;&#39640;&#20449;&#24687;&#37327;&#19988;&#19982;&#38646;&#23556;&#26679;&#26412;&#20256;&#36882;&#30340;&#29305;&#24449;&#30456;&#20851;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#12290;&#22312;&#20223;&#30495;&#22330;&#26223;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15420v1 Announce Type: cross  Abstract: Preference-based reinforcement learning (RL) has emerged as a new field in robot learning, where humans play a pivotal role in shaping robot behavior by expressing preferences on different sequences of state-action pairs. However, formulating realistic policies for robots demands responses from humans to an extensive array of queries. In this work, we approach the sample-efficiency challenge by expanding the information collected per query to contain both preferences and optional text prompting. To accomplish this, we leverage the zero-shot capabilities of a large language model (LLM) to reason from the text provided by humans. To accommodate the additional query information, we reformulate the reward learning objectives to contain flexible highlights -- state-action pairs that contain relatively high information and are related to the features processed in a zero-shot fashion from a pretrained LLM. In both a simulated scenario and a u
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#36328;&#24322;&#26500;&#26469;&#28304;&#36827;&#34892;&#25805;&#20316;&#30340;&#26102;&#24577;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#24378;&#21046;&#25191;&#34892;&#26102;&#38388;&#32422;&#26463;&#20197;&#30830;&#20445;&#24544;&#23454;&#22238;&#31572;&#65292;&#27491;&#30830;&#22788;&#29702;&#38544;&#21547;&#38382;&#39064;&#65292;&#24182;&#20197;&#32479;&#19968;&#26041;&#24335;&#35206;&#30422;&#30693;&#35782;&#24211;&#12289;&#25991;&#26412;&#21644;&#32593;&#32476;&#34920;&#26684;&#12290;</title><link>https://arxiv.org/abs/2402.15400</link><description>&lt;p&gt;
&#36328;&#24322;&#26500;&#26469;&#28304;&#30340;&#24544;&#23454;&#26102;&#24577;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Faithful Temporal Question Answering over Heterogeneous Sources
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15400
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#36328;&#24322;&#26500;&#26469;&#28304;&#36827;&#34892;&#25805;&#20316;&#30340;&#26102;&#24577;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#24378;&#21046;&#25191;&#34892;&#26102;&#38388;&#32422;&#26463;&#20197;&#30830;&#20445;&#24544;&#23454;&#22238;&#31572;&#65292;&#27491;&#30830;&#22788;&#29702;&#38544;&#21547;&#38382;&#39064;&#65292;&#24182;&#20197;&#32479;&#19968;&#26041;&#24335;&#35206;&#30422;&#30693;&#35782;&#24211;&#12289;&#25991;&#26412;&#21644;&#32593;&#32476;&#34920;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24577;&#38382;&#31572;&#28041;&#21450;&#26102;&#38388;&#32422;&#26463;&#65292;&#20363;&#22914;&#8220;...&#22312;2019&#24180;&#8221;&#25110;&#8220;...&#22312;COVID&#20043;&#21069;&#8221;&#12290;&#22312;&#21069;&#32773;&#20013;&#65292;&#26102;&#38388;&#26159;&#19968;&#20010;&#26126;&#30830;&#30340;&#26465;&#20214;&#65292;&#22312;&#21518;&#32773;&#20013;&#65292;&#23427;&#26159;&#38544;&#21547;&#30340;&#12290;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#22312;&#19977;&#20010;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#31070;&#32463;&#25512;&#29702;&#26102;&#65292;&#26102;&#38388;&#32422;&#26463;&#20165;&#34987;&#36719;&#21305;&#37197;&#65292;&#23481;&#26131;&#23548;&#33268;&#26080;&#25928;&#25110;&#26080;&#27861;&#35299;&#37322;&#30340;&#31572;&#26696;&#12290;&#20854;&#27425;&#65292;&#23545;&#20110;&#28041;&#21450;&#38544;&#21547;&#26102;&#38388;&#30340;&#38382;&#39064;&#25903;&#25345;&#19981;&#36275;&#12290;&#31532;&#19977;&#65292;&#31572;&#26696;&#21482;&#26469;&#33258;&#21333;&#19968;&#26469;&#28304;&#65306;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#25110;&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#24577;&#38382;&#31572;&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#12290;&#39318;&#20808;&#65292;&#23427;&#36890;&#36807;&#20855;&#20307;&#35777;&#25454;&#24378;&#21046;&#25191;&#34892;&#26102;&#38388;&#32422;&#26463;&#20197;&#30830;&#20445;&#24544;&#23454;&#22238;&#31572;&#12290;&#20854;&#27425;&#65292;&#23427;&#27491;&#30830;&#22788;&#29702;&#38544;&#21547;&#38382;&#39064;&#12290;&#31532;&#19977;&#65292;&#23427;&#20197;&#32479;&#19968;&#26041;&#24335;&#35206;&#30422;&#30693;&#35782;&#24211;&#12289;&#25991;&#26412;&#21644;&#32593;&#32476;&#34920;&#26684;&#65292;&#36328;&#24322;&#26500;&#26469;&#28304;&#36827;&#34892;&#25805;&#20316;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#29702;&#35299;&#38382;&#39064;&#21450;&#20854;&#26102;&#38388;&#26465;&#20214;&#65292;&#65288;ii&#65289;&#20174;&#20013;&#26816;&#32034;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15400v1 Announce Type: cross  Abstract: Temporal question answering (QA) involves time constraints, with phrases such as "... in 2019" or "... before COVID". In the former, time is an explicit condition, in the latter it is implicit. State-of-the-art methods have limitations along three dimensions. First, with neural inference, time constraints are merely soft-matched, giving room to invalid or inexplicable answers. Second, questions with implicit time are poorly supported. Third, answers come from a single source: either a knowledge base (KB) or a text corpus. We propose a temporal QA system that addresses these shortcomings. First, it enforces temporal constraints for faithful answering with tangible evidence. Second, it properly handles implicit questions. Third, it operates over heterogeneous sources, covering KB, text and web tables in a unified manner. The method has three stages: (i) understanding the question and its temporal conditions, (ii) retrieving evidence from
&lt;/p&gt;</description></item><item><title>&#33258;&#20462;&#22797;&#29616;&#35937;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#23478;&#26063;&#21644;&#23610;&#23544;&#19978;&#65292;&#20294;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#26159;&#19981;&#23436;&#32654;&#21644;&#22024;&#26434;&#30340;&#65292;&#26377;&#20004;&#31181;&#26426;&#21046;&#21487;&#20419;&#25104;&#33258;&#20462;&#22797;&#65292;&#21253;&#25324;&#26368;&#32456;LayerNorm&#32553;&#25918;&#22240;&#23376;&#30340;&#21464;&#21270;&#21644;&#23454;&#29616;&#21453;&#25830;&#38500;&#30340;&#31232;&#30095;&#31070;&#32463;&#20803;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.15390</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#20462;&#22797;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Explorations of Self-Repair in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15390
&lt;/p&gt;
&lt;p&gt;
&#33258;&#20462;&#22797;&#29616;&#35937;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#23478;&#26063;&#21644;&#23610;&#23544;&#19978;&#65292;&#20294;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#26159;&#19981;&#23436;&#32654;&#21644;&#22024;&#26434;&#30340;&#65292;&#26377;&#20004;&#31181;&#26426;&#21046;&#21487;&#20419;&#25104;&#33258;&#20462;&#22797;&#65292;&#21253;&#25324;&#26368;&#32456;LayerNorm&#32553;&#25918;&#22240;&#23376;&#30340;&#21464;&#21270;&#21644;&#23454;&#29616;&#21453;&#25830;&#38500;&#30340;&#31232;&#30095;&#31070;&#32463;&#20803;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30740;&#31350;&#29421;&#31364;&#20998;&#24067;&#30340;&#21487;&#35299;&#37322;&#24615;&#21457;&#29616;&#20102;&#33258;&#20462;&#22797;&#29616;&#35937;&#65292;&#21363;&#22914;&#26524;&#21093;&#31163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#20214;&#65292;&#21518;&#32493;&#32452;&#20214;&#20250;&#25913;&#21464;&#20854;&#34892;&#20026;&#20197;&#36827;&#34892;&#34917;&#20607;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#36825;&#20123;&#36807;&#21435;&#30340;&#25991;&#29486;&#65292;&#23637;&#31034;&#20102;&#24403;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#21093;&#31163;&#21333;&#20010;&#27880;&#24847;&#21147;&#22836;&#26102;&#65292;&#33258;&#20462;&#22797;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#23478;&#26063;&#21644;&#23610;&#23544;&#19978;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#65292;&#33258;&#20462;&#22797;&#26159;&#19981;&#23436;&#32654;&#30340;&#65292;&#22240;&#20026;&#22836;&#37096;&#30340;&#21407;&#22987;&#30452;&#25509;&#25928;&#26524;&#24182;&#26410;&#23436;&#20840;&#24674;&#22797;&#65292;&#24182;&#19988;&#26159;&#22024;&#26434;&#30340;&#65292;&#22240;&#20026;&#33258;&#20462;&#22797;&#31243;&#24230;&#22312;&#19981;&#21516;&#25552;&#31034;&#20043;&#38388;&#26174;&#33879;&#21464;&#21270;&#65288;&#26377;&#26102;&#36229;&#36807;&#21407;&#22987;&#25928;&#26524;&#65289;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#20419;&#25104;&#33258;&#20462;&#22797;&#30340;&#20004;&#31181;&#19981;&#21516;&#26426;&#21046;&#65292;&#21253;&#25324;&#26368;&#32456;LayerNorm&#32553;&#25918;&#22240;&#23376;&#30340;&#21464;&#21270;&#65288;&#21487;&#20462;&#22797;&#30452;&#25509;&#25928;&#26524;&#30340;30%&#65289;&#20197;&#21450;&#23454;&#29616;&#21453;&#25830;&#38500;&#30340;&#31232;&#30095;&#31070;&#32463;&#20803;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15390v1 Announce Type: cross  Abstract: Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor (which can repair up to 30% of the direct effect) and sparse sets of neurons implementing Anti-Erasure
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#65288;D2E2S&#65289;&#65292;&#32467;&#21512;&#20102;BERT&#36890;&#36947;&#21644;&#22686;&#24378;&#22411;LSTM&#36890;&#36947;&#26469;&#26368;&#22823;&#21270;&#21333;&#35789;&#38388;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#20851;&#31995;&#65292;&#24341;&#20837;&#20102;&#24322;&#26500;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#29992;&#20110;&#25429;&#33719;&#22797;&#26434;&#20114;&#21160;&#21644;&#21160;&#24577;&#36873;&#25321;&#37325;&#35201;&#33410;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.15370</link><description>&lt;p&gt;
&#21452;&#32534;&#30721;&#22120;&#65306;&#21033;&#29992;&#21477;&#27861;&#21644;&#35821;&#20041;&#28508;&#21147;&#36827;&#34892;&#26041;&#38754;&#24773;&#24863;&#19977;&#20803;&#32452;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Dual Encoder: Exploiting the Potential of Syntactic and Semantic for Aspect Sentiment Triplet Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15370
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#65288;D2E2S&#65289;&#65292;&#32467;&#21512;&#20102;BERT&#36890;&#36947;&#21644;&#22686;&#24378;&#22411;LSTM&#36890;&#36947;&#26469;&#26368;&#22823;&#21270;&#21333;&#35789;&#38388;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#20851;&#31995;&#65292;&#24341;&#20837;&#20102;&#24322;&#26500;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#29992;&#20110;&#25429;&#33719;&#22797;&#26434;&#20114;&#21160;&#21644;&#21160;&#24577;&#36873;&#25321;&#37325;&#35201;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#38754;&#24773;&#24863;&#19977;&#20803;&#32452;&#25552;&#21462;&#65288;ASTE&#65289;&#26159;&#31934;&#32454;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#26032;&#20852;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#24314;&#27169;&#19977;&#20803;&#32452;&#20803;&#32032;&#22266;&#26377;&#30340;&#21477;&#27861;-&#35821;&#20041;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#23578;&#26410;&#20805;&#20998;&#21457;&#25381;ASTE&#20219;&#21153;&#20013;&#21477;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;\emph{&#21452;&#32534;&#30721;&#22120;&#65306;&#21033;&#29992;&#21477;&#27861;&#21644;&#35821;&#20041;&#28508;&#21147;}&#27169;&#22411;&#65288;D2E2S&#65289;&#65292;&#26368;&#22823;&#21270;&#21333;&#35789;&#38388;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#21452;&#36890;&#36947;&#32534;&#30721;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;BERT&#36890;&#36947;&#26469;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#22686;&#24378;&#22411;LSTM&#36890;&#36947;&#29992;&#20110;&#20840;&#38754;&#25429;&#25417;&#21477;&#27861;&#20449;&#24687;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24322;&#26500;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#65292;&#20197;&#25429;&#33719;&#20381;&#36182;&#21477;&#27861;&#19982;&#27880;&#24847;&#21147;&#35821;&#20041;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#65292;&#24182;&#21160;&#24577;&#36873;&#25321;&#37325;&#35201;&#33410;&#28857;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#27169;&#22359;&#30340;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15370v1 Announce Type: cross  Abstract: Aspect Sentiment Triple Extraction (ASTE) is an emerging task in fine-grained sentiment analysis. Recent studies have employed Graph Neural Networks (GNN) to model the syntax-semantic relationships inherent in triplet elements. However, they have yet to fully tap into the vast potential of syntactic and semantic information within the ASTE task. In this work, we propose a \emph{Dual Encoder: Exploiting the potential of Syntactic and Semantic} model (D2E2S), which maximizes the syntactic and semantic relationships among words. Specifically, our model utilizes a dual-channel encoder with a BERT channel to capture semantic information, and an enhanced LSTM channel for comprehensive syntactic information capture. Subsequently, we introduce the heterogeneous feature interaction module to capture intricate interactions between dependency syntax and attention semantics, and to dynamically select vital nodes. We leverage the synergy of these m
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;LLM&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#23454;&#20307;&#35782;&#21035;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#65292;&#21019;&#24314;&#20102;NuNER&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#32039;&#20945;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#32988;&#36807;&#30456;&#20284;&#22823;&#23567;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#19982;&#26356;&#22823;&#30340;LLMs&#31454;&#20105;&#12290;</title><link>https://arxiv.org/abs/2402.15343</link><description>&lt;p&gt;
NuNER: &#21033;&#29992;LLM&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#23454;&#20307;&#35782;&#21035;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15343
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;LLM&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#23454;&#20307;&#35782;&#21035;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#65292;&#21019;&#24314;&#20102;NuNER&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#32039;&#20945;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#32988;&#36807;&#30456;&#20284;&#22823;&#23567;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#19982;&#26356;&#22823;&#30340;LLMs&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#22312;&#25968;&#25454;&#26631;&#27880;&#26041;&#38754;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20026;&#35299;&#20915;&#32463;&#20856;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#21019;&#24314;NuNER&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20219;&#21153;&#30340;&#32039;&#20945;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#12290;NuNER&#21487;&#20197;&#34987;&#24494;&#35843;&#20197;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#35299;&#20915;&#19979;&#28216;&#30340;NER&#38382;&#39064;&#65292;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#32988;&#36807;&#30456;&#20284;&#22823;&#23567;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#19982;&#26356;&#22823;&#30340;LLMs&#31454;&#20105;&#12290;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#23454;&#20307;&#31867;&#22411;&#30340;&#22810;&#26679;&#24615;&#26159;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#23558;NuNER&#35270;&#20026;&#26368;&#36817;&#34987;LLMs&#35299;&#38145;&#30340;&#26356;&#24191;&#27867;&#30340;&#29305;&#23450;&#20219;&#21153;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#30340;&#19968;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15343v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown impressive abilities in data annotation, opening the way for new approaches to solve classic NLP problems. In this paper, we show how to use LLMs to create NuNER, a compact language representation model specialized in the Named Entity Recognition (NER) task. NuNER can be fine-tuned to solve downstream NER problems in a data-efficient way, outperforming similar-sized foundation models in the few-shot regime and competing with much larger LLMs. We find that the size and entity-type diversity of the pre-training dataset are key to achieving good performance. We view NuNER as a member of the broader family of task-specific foundation models, recently unlocked by LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;LLMs&#25506;&#32034;&#27010;&#24565;&#31354;&#38388;&#32500;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#20307;&#25490;&#21517;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20854;&#22312;&#24863;&#30693;&#21644;&#20027;&#35266;&#29305;&#24449;&#19978;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15337</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#27839;&#30528;&#27010;&#24565;&#31354;&#38388;&#32500;&#24230;&#23545;&#23454;&#20307;&#36827;&#34892;&#25490;&#21517;&#65306;&#24494;&#35843;&#31574;&#30053;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;LLMs&#25506;&#32034;&#27010;&#24565;&#31354;&#38388;&#32500;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#20307;&#25490;&#21517;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20854;&#22312;&#24863;&#30693;&#21644;&#20027;&#35266;&#29305;&#24449;&#19978;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#31354;&#38388;&#20197;&#23454;&#20307;&#30340;&#21407;&#22987;&#35821;&#20041;&#29305;&#24449;&#34920;&#31034;&#12290;&#36825;&#31181;&#34920;&#31034;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#20294;&#23398;&#20064;&#36215;&#26469;&#38750;&#24120;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#24314;&#27169;&#24863;&#30693;&#21644;&#20027;&#35266;&#29305;&#24449;&#26102;&#12290;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25552;&#28860;&#27010;&#24565;&#31354;&#38388;&#26368;&#36817;&#20986;&#29616;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20165;&#38480;&#20110;&#20351;&#29992;&#30456;&#23545;&#31616;&#21333;&#30340;&#38646;&#26679;&#26412;&#31574;&#30053;&#25506;&#26597;&#39044;&#35757;&#32451;&#30340;LLMs&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#26681;&#25454;&#32473;&#23450;&#30340;&#27010;&#24565;&#31354;&#38388;&#32500;&#24230;&#23545;&#23454;&#20307;&#36827;&#34892;&#25490;&#21517;&#30340;&#20219;&#21153;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#27010;&#24565;&#31354;&#38388;&#32500;&#24230;&#30340;&#30495;&#23454;&#25490;&#21517;&#24456;&#23569;&#35265;&#65292;&#25105;&#20204;&#26080;&#27861;&#30452;&#25509;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#24494;&#35843;LLMs&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#26356;&#23481;&#26131;&#33719;&#24471;&#30340;&#29305;&#24449;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20998;&#26512;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#30340;&#25490;&#21517;&#33021;&#21147;&#26159;&#21542;&#33021;&#36716;&#31227;&#21040;&#24863;&#30693;&#21644;&#20027;&#35266;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#30830;&#23454;&#26159;&#36825;&#31181;&#24773;&#20917;&#65292;&#20294;&#26159;&#26410;&#23436;&#25104;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15337v1 Announce Type: new  Abstract: Conceptual spaces represent entities in terms of their primitive semantic features. Such representations are highly valuable but they are notoriously difficult to learn, especially when it comes to modelling perceptual and subjective features. Distilling conceptual spaces from Large Language Models (LLMs) has recently emerged as a promising strategy. However, existing work has been limited to probing pre-trained LLMs using relatively simple zero-shot strategies. We focus in particular on the task of ranking entities according to a given conceptual space dimension. Unfortunately, we cannot directly fine-tune LLMs on this task, because ground truth rankings for conceptual space dimensions are rare. We therefore use more readily available features as training data and analyse whether the ranking capabilities of the resulting models transfer to perceptual and subjective features. We find that this is indeed the case, to some extent, but havi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#21152;&#37327;&#21270;&#32500;&#24230;&#65292;GPTVQ&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#19981;&#20165;&#26174;&#33879;&#25913;&#21892;&#20102;&#22823;&#23567;&#19982;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#65292;&#36824;&#25552;&#39640;&#20102;&#22788;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15319</link><description>&lt;p&gt;
GPTVQ&#65306;LLM&#37327;&#21270;&#20013;&#32500;&#24230;&#30340;&#31119;&#38899;
&lt;/p&gt;
&lt;p&gt;
GPTVQ: The Blessing of Dimensionality for LLM Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15319
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#37327;&#21270;&#32500;&#24230;&#65292;GPTVQ&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#19981;&#20165;&#26174;&#33879;&#25913;&#21892;&#20102;&#22823;&#23567;&#19982;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#65292;&#36824;&#25552;&#39640;&#20102;&#22788;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22686;&#21152;&#37327;&#21270;&#32500;&#24230;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#30340;&#22823;&#23567;&#19982;&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GPTVQ&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#24555;&#36895;&#21518;&#35757;&#32451;&#21521;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20132;&#26367;&#36827;&#34892;&#19968;&#20010;&#25110;&#22810;&#20010;&#21015;&#30340;&#37327;&#21270;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;&#27599;&#23618;&#36755;&#20986;&#37325;&#24314;MSE&#30340;Hessian&#20449;&#24687;&#26469;&#26356;&#26032;&#20854;&#20313;&#26410;&#37327;&#21270;&#30340;&#26435;&#37325;&#12290;&#37327;&#21270;&#30721;&#20070;&#20351;&#29992;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#24863;&#30693;&#29256;&#26412;&#30340;EM&#31639;&#27861;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#25972;&#25968;&#37327;&#21270;&#21644;&#22522;&#20110;SVD&#30340;&#21387;&#32553;&#36827;&#19968;&#27493;&#21387;&#32553;&#30721;&#20070;&#12290;GPTVQ&#22312;&#35832;&#22914;Llama-v2&#21644;Mistral&#31561;&#21508;&#31181;LLMs&#19978;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#22823;&#23567;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39640;&#25928;&#65306;&#22312;&#21333;&#20010;H100&#19978;&#65292;&#22788;&#29702;&#19968;&#20010;Llamav2-70B&#38656;&#35201;3&#33267;11&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15319v1 Announce Type: cross  Abstract: In this work we show that the size versus accuracy trade-off of neural network quantization can be significantly improved by increasing the quantization dimensionality. We propose the GPTVQ method, a new fast method for post-training vector quantization (VQ) that scales well to Large Language Models (LLMs). Our method interleaves quantization of one or more columns with updates to the remaining unquantized weights, using information from the Hessian of the per-layer output reconstruction MSE. Quantization codebooks are initialized using an efficient data-aware version of the EM algorithm. The codebooks are then updated, and further compressed by using integer quantization and SVD-based compression. GPTVQ establishes a new state-of-the art in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2 and Mistral. Furthermore, our method is efficient: on a single H100 it takes between 3 and 11 hours to process a Llamav2-70B
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ArabianGPT&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#19987;&#38376;&#20026;&#38463;&#25289;&#20271;&#35821;&#35774;&#35745;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#30340;ArabianGPT-0.1B&#21644;ArabianGPT-0.3B&#65292;&#24110;&#21161;&#24357;&#34917;&#20102;&#26412;&#22303;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.15313</link><description>&lt;p&gt;
ArabianGPT&#65306;&#22522;&#20110;&#21407;&#29983;&#38463;&#25289;&#20271;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ArabianGPT: Native Arabic GPT-based Large Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15313
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ArabianGPT&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#19987;&#38376;&#20026;&#38463;&#25289;&#20271;&#35821;&#35774;&#35745;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#30340;ArabianGPT-0.1B&#21644;ArabianGPT-0.3B&#65292;&#24110;&#21161;&#24357;&#34917;&#20102;&#26412;&#22303;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33521;&#35821;&#21644;&#25289;&#19969;&#35821;&#20026;&#20027;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20027;&#23548;&#22320;&#20301;&#23548;&#33268;&#20102;&#26412;&#22303;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#26174;&#33879;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;ArabianGPT&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#19987;&#38376;&#20026;&#38463;&#25289;&#20271;&#35821;&#35774;&#35745;&#32780;&#25104;&#12290;&#36825;&#20123;&#27169;&#22411;&#21253;&#25324;ArabianGPT-0.1B&#21644;ArabianGPT-0.3B&#65292;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#65292;&#19982;&#38463;&#25289;&#20271;&#35821;&#30340;&#24494;&#22937;&#35821;&#35328;&#29305;&#24449;&#30456;&#22865;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15313v1 Announce Type: cross  Abstract: The predominance of English and Latin-based large language models (LLMs) has led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models, detracting from their efficacy in processing native Arabic's intricate morphology and syntax. Consequently, there is a theoretical and practical imperative for developing LLMs predominantly focused on Arabic linguistic elements. To address this gap, this paper proposes ArabianGPT, a series of transformer-based models within the ArabianLLM suite designed explicitly for Arabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in size and complexity, aligning with the nuanced linguistic characteristics of Arabic. The AraNizer tokenizer, integral to these models, addresses the unique morphological aspects of Arabic script, ensuring more accurate text processing. Empirical results from fine-tuning t
&lt;/p&gt;</description></item><item><title>&#21453;&#20107;&#23454;&#29983;&#25104;&#38754;&#20020;&#30528;&#37197;&#23545;&#25968;&#25454;&#31232;&#32570;&#21644;&#26631;&#27880;&#20449;&#24687;&#26377;&#38480;&#31561;&#25361;&#25112;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#36807;&#24230;&#31616;&#21270;&#30340;&#20551;&#35774;&#65292;&#20294;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#19979;&#36825;&#20123;&#20551;&#35774;&#21487;&#33021;&#19981;&#25104;&#31435;&#12290;</title><link>https://arxiv.org/abs/2402.15309</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#35782;&#21035;&#24615;&#20445;&#35777;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Generation with Identifiability Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15309
&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#29983;&#25104;&#38754;&#20020;&#30528;&#37197;&#23545;&#25968;&#25454;&#31232;&#32570;&#21644;&#26631;&#27880;&#20449;&#24687;&#26377;&#38480;&#31561;&#25361;&#25112;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#36807;&#24230;&#31616;&#21270;&#30340;&#20551;&#35774;&#65292;&#20294;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#19979;&#36825;&#20123;&#20551;&#35774;&#21487;&#33021;&#19981;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#29983;&#25104;&#26159;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#26680;&#24515;&#65292;&#21253;&#25324;&#22270;&#20687;&#36716;&#25442;&#21644;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#36825;&#19968;&#29983;&#25104;&#36807;&#31243;&#36890;&#24120;&#38656;&#35201;&#35782;&#21035;&#28508;&#22312;&#30340;&#20998;&#35299;&#34920;&#24449;&#65292;&#22914;&#20869;&#23481;&#21644;&#39118;&#26684;&#65292;&#36825;&#20123;&#34920;&#24449;&#28508;&#22312;&#22320;&#25903;&#25745;&#30528;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#37197;&#23545;&#25968;&#25454;&#21644;&#26631;&#27880;&#20449;&#24687;&#30340;&#31232;&#32570;&#26102;&#65292;&#24773;&#20917;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#20998;&#35299;&#26041;&#27861;&#20851;&#38190;&#20381;&#36182;&#20110;&#36807;&#24230;&#31616;&#21270;&#30340;&#20551;&#35774;&#65292;&#27604;&#22914;&#20551;&#35774;&#20869;&#23481;&#21644;&#39118;&#26684;&#21464;&#37327;&#29420;&#31435;&#65292;&#20197;&#35782;&#21035;&#28508;&#22312;&#21464;&#37327;&#65292;&#23613;&#31649;&#36825;&#26679;&#30340;&#20551;&#35774;&#21487;&#33021;&#24182;&#19981;&#36866;&#29992;&#20110;&#22797;&#26434;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20363;&#22914;&#65292;&#39135;&#29289;&#35780;&#35770;&#24448;&#24448;&#28041;&#21450;&#8220;&#32654;&#21619;&#8221;&#31561;&#35789;&#35821;&#65292;&#32780;&#30005;&#24433;&#35780;&#35770;&#36890;&#24120;&#21547;&#26377;&#8220;&#24778;&#24515;&#21160;&#39748;&#8221;&#31561;&#35789;&#35821;&#34920;&#31034;&#21516;&#26679;&#30340;&#31215;&#26497;&#24773;&#24863;&#12290;&#24403;&#25968;&#25454;&#20174;&#22810;&#20010;&#39046;&#22495;&#37319;&#26679;&#26102;&#65292;&#30001;&#20110;&#20869;&#23481;&#21644;&#39118;&#26684;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#21487;&#33021;&#20250;&#26174;&#33879;&#21464;&#21270;&#65292;&#36825;&#20010;&#38382;&#39064;&#20250;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15309v1 Announce Type: cross  Abstract: Counterfactual generation lies at the core of various machine learning tasks, including image translation and controllable text generation. This generation process usually requires the identification of the disentangled latent representations, such as content and style, that underlie the observed data. However, it becomes more challenging when faced with a scarcity of paired data and labeling information. Existing disentangled methods crucially rely on oversimplified assumptions, such as assuming independent content and style variables, to identify the latent variables, even though such assumptions may not hold for complex data distributions. For instance, food reviews tend to involve words like tasty, whereas movie reviews commonly contain words such as thrilling for the same positive sentiment. This problem is exacerbated when data are sampled from multiple domains since the dependence between content and style may vary significantly
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25351;&#20196;&#20013;&#24515;&#21709;&#24212;&#30340;&#23481;&#24525;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22797;&#26434;&#26597;&#35810;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25581;&#31034;&#35302;&#21457;&#19981;&#36947;&#24503;&#21709;&#24212;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15302</link><description>&lt;p&gt;
&#26377;&#20851;LLMs&#25351;&#20196;&#20013;&#24515;&#21709;&#24212;&#30340;&#65288;&#19981;&#36947;&#24503;&#65289;&#31243;&#24230;&#26377;&#22810;&#39640;&#65311;&#25581;&#31034;&#23433;&#20840;&#38450;&#25252;&#26639;&#23545;&#26377;&#23475;&#26597;&#35810;&#30340;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25351;&#20196;&#20013;&#24515;&#21709;&#24212;&#30340;&#23481;&#24525;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22797;&#26434;&#26597;&#35810;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25581;&#31034;&#35302;&#21457;&#19981;&#36947;&#24503;&#21709;&#24212;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#22260;&#32469;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23433;&#20840;&#21644;&#36947;&#24503;&#20351;&#29992;&#26085;&#30410;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20250;&#34987;&#21508;&#31181;&#22797;&#26434;&#30340;&#26041;&#27861;&#27450;&#39575;&#65292;&#20135;&#29983;&#26377;&#23475;&#25110;&#19981;&#36947;&#24503;&#20869;&#23481;&#65292;&#21253;&#25324;&#8220;&#36234;&#29425;&#8221;&#25216;&#26415;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25805;&#32437;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#19978;&#65306;LLMs&#22312;&#35201;&#27714;&#23427;&#20204;&#29983;&#25104;&#20197;&#20266;&#20195;&#30721;&#12289;&#31243;&#24207;&#25110;&#36719;&#20214;&#29255;&#27573;&#20026;&#20013;&#24515;&#30340;&#21709;&#24212;&#26102;&#65292;&#26377;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#33021;&#20250;&#34987;&#35823;&#23548;&#65292;&#32780;&#19981;&#26159;&#29983;&#25104;&#26222;&#36890;&#25991;&#26412;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TechHazardQA&#65292;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#24212;&#20197;&#25991;&#26412;&#21644;&#20197;&#25351;&#20196;&#20026;&#20013;&#24515;&#26684;&#24335;&#65288;&#20363;&#22914;&#20266;&#20195;&#30721;&#65289;&#22238;&#31572;&#30340;&#22797;&#26434;&#26597;&#35810;&#65292;&#26088;&#22312;&#35782;&#21035;&#19981;&#36947;&#24503;&#21709;&#24212;&#30340;&#35302;&#21457;&#22120;&#12290;&#25105;&#20204;&#26597;&#35810;&#20102;&#19968;&#31995;&#21015;LLMs-- Llama-2-13b&#65292;Llama-2-7b&#65292;Mistral-V2&#21644;Mistral 8X7B--&#24182;&#35201;&#27714;&#23427;&#20204;&#29983;&#25104;&#25991;&#26412;&#21644;&#25351;&#20196;&#20026;&#20013;&#24515;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15302v1 Announce Type: new  Abstract: In this study, we tackle a growing concern around the safety and ethical use of large language models (LLMs). Despite their potential, these models can be tricked into producing harmful or unethical content through various sophisticated methods, including 'jailbreaking' techniques and targeted manipulation. Our work zeroes in on a specific issue: to what extent LLMs can be led astray by asking them to generate responses that are instruction-centric such as a pseudocode, a program or a software snippet as opposed to vanilla text. To investigate this question, we introduce TechHazardQA, a dataset containing complex queries which should be answered in both text and instruction-centric formats (e.g., pseudocodes), aimed at identifying triggers for unethical responses. We query a series of LLMs -- Llama-2-13b, Llama-2-7b, Mistral-V2 and Mistral 8X7B -- and ask them to generate both text and instruction-centric responses. For evaluation we rep
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#21463;&#38480;&#20110;&#25968;&#25454;&#25910;&#38598;&#20559;&#35265;&#21644;&#20010;&#20307;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15301</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15301
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#21463;&#38480;&#20110;&#25968;&#25454;&#25910;&#38598;&#20559;&#35265;&#21644;&#20010;&#20307;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#22270;&#24674;&#22797;&#22312;&#22240;&#26524;&#25512;&#26029;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#26159;&#22522;&#20110;&#30693;&#35782;&#25110;&#32479;&#35745;&#20272;&#35745;&#65292;&#21463;&#25968;&#25454;&#25910;&#38598;&#20559;&#35265;&#21644;&#20010;&#20307;&#20851;&#20110;&#24433;&#21709;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#30693;&#35782;&#30340;&#38480;&#21046;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#37327;&#31185;&#23398;&#25991;&#29486;&#20013;&#25152;&#21253;&#21547;&#30340;&#30693;&#35782;&#25512;&#23548;&#19968;&#33324;&#22240;&#26524;&#22270;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;LLMs&#31995;&#32479;&#22320;&#20998;&#26512;&#21644;&#25552;&#21462;&#26469;&#33258;&#24191;&#27867;&#30740;&#31350;&#35770;&#25991;&#38598;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20174;&#27719;&#24635;&#30340;&#25991;&#29486;&#20013;&#26816;&#32034;&#30456;&#20851;&#25991;&#26412;&#29255;&#27573;&#12290;&#28982;&#21518;&#65292;LLM&#34987;&#29992;&#26469;&#35782;&#21035;&#21644;&#26631;&#35760;&#22240;&#32032;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#32852;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15301v1 Announce Type: new  Abstract: Causal graph recovery is essential in the field of causal inference. Traditional methods are typically knowledge-based or statistical estimation-based, which are limited by data collection biases and individuals' knowledge about factors affecting the relations between variables of interests. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that utilizes the extensive knowledge contained within a large corpus of scientific literature to deduce causal relationships in general causal graph recovery tasks. This method leverages Retrieval Augmented-Generation (RAG) based LLMs to systematically analyze and extract pertinent information from a comprehensive collection of research papers. Our method first retrieves relevant text chunks from the aggregated literature. Then, the LLM is tasked with identifying and labelling potential associations between factors. Finally, we giv
&lt;/p&gt;</description></item><item><title>CLIP&#30456;&#20284;&#24615;&#20316;&#20026;&#26356;&#24378;&#22823;&#21644;&#26356;&#31283;&#20581;&#30340;&#24187;&#35273;&#25351;&#26631;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#35299;&#30721;&#65288;CGD&#65289;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#20943;&#23569;&#23545;&#35937;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.15300</link><description>&lt;p&gt;
&#35265;&#35777;&#20026;&#20449;&#65306;&#36890;&#36807;CLIP&#24341;&#23548;&#35299;&#30721;&#32531;&#35299;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15300
&lt;/p&gt;
&lt;p&gt;
CLIP&#30456;&#20284;&#24615;&#20316;&#20026;&#26356;&#24378;&#22823;&#21644;&#26356;&#31283;&#20581;&#30340;&#24187;&#35273;&#25351;&#26631;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#35299;&#30721;&#65288;CGD&#65289;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#20943;&#23569;&#23545;&#35937;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#23481;&#26131;&#20986;&#29616;&#23545;&#35937;&#24187;&#35273;&#65292;&#21363;&#29983;&#25104;&#30340;&#25991;&#26412;&#21253;&#21547;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#21477;&#23376;&#32423;LVLM&#24187;&#35273;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#19982;&#22270;&#20687;&#30340;CLIP&#30456;&#20284;&#24615;&#20316;&#20026;&#19968;&#20010;&#27604;&#21333;&#35789;&#21487;&#33021;&#24615;&#26356;&#24378;&#22823;&#12289;&#26356;&#31283;&#20581;&#30340;&#24187;&#35273;&#25351;&#31034;&#22120;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#35299;&#30721;&#65288;CGD&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#35299;&#30721;&#26102;&#30340;&#23545;&#35937;&#24187;&#35273;&#12290;CGD&#21033;&#29992;CLIP&#26469;&#24341;&#23548;&#27169;&#22411;&#30340;&#35299;&#30721;&#36807;&#31243;&#65292;&#36890;&#36807;&#22686;&#24378;&#29983;&#25104;&#25991;&#26412;&#19982;&#22270;&#20687;&#30340;&#35270;&#35273;&#32852;&#31995;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;CGD&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#23545;&#35937;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15300v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) are susceptible to object hallucinations, an issue in which their generated text contains non-existent objects, greatly limiting their reliability and practicality. Current approaches often rely on the model's token likelihoods or other internal information, instruction tuning on additional datasets, or incorporating complex external tools. We first perform empirical analysis on sentence-level LVLM hallucination, finding that CLIP similarity to the image acts as a stronger and more robust indicator of hallucination compared to token likelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD) approach, a straightforward but effective training-free approach to reduce object hallucination at decoding time. CGD uses CLIP to guide the model's decoding process by enhancing visual grounding of generated text with the image. Experiments demonstrate that CGD effectively mitigates object hallu
&lt;/p&gt;</description></item><item><title>MemoryPrompt&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#24490;&#29615;&#32593;&#32476;&#65292;&#23558;&#20449;&#24687;&#20256;&#36882;&#32473;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#36319;&#36394;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.15268</link><description>&lt;p&gt;
MemoryPrompt: &#19968;&#31181;&#25913;&#36827;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#36319;&#36394;&#30340;&#36731;&#37327;&#23553;&#35013;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15268
&lt;/p&gt;
&lt;p&gt;
MemoryPrompt&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#24490;&#29615;&#32593;&#32476;&#65292;&#23558;&#20449;&#24687;&#20256;&#36882;&#32473;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#36319;&#36394;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22823;&#22411;&#30828;&#32534;&#30721;&#36755;&#20837;&#31383;&#21475;&#36319;&#36394;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#24341;&#20837;MemoryPrompt&#65292;&#19968;&#31181;&#26356;&#31934;&#31616;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#30001;&#19968;&#20010;&#23567;&#30340;&#36741;&#21161;&#24490;&#29615;&#32593;&#32476;&#34917;&#20805;&#65292;&#36890;&#36807;&#22312;&#20854;&#24120;&#35268;&#36755;&#20837;&#20043;&#21069;&#28155;&#21152;&#19968;&#31995;&#21015;&#21521;&#37327;&#65288;&#31867;&#20284;&#20110;&#36719;&#25552;&#31034;&#65289;&#23558;&#20449;&#24687;&#20256;&#36882;&#32473;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#38656;&#35201;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#23545;&#19968;&#20010;&#26088;&#22312;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#36319;&#36394;&#22810;&#20010;&#20107;&#23454;&#26356;&#26032;&#33021;&#21147;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;MemoryPrompt&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#37027;&#20123;&#21487;&#20197;&#35775;&#38382;&#23436;&#25972;&#36755;&#20837;&#21382;&#21490;&#35760;&#24405;&#30340;&#26356;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#38271;&#36317;&#31163;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;MemoryPrompt&#65292;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#65292;&#20854;&#24615;&#33021;&#19982;&#22312;&#25972;&#20010;&#23545;&#35805;&#21382;&#21490;&#35760;&#24405;&#19978;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#30340;&#27169;&#22411;&#30456;&#24403;&#12290;&#22312;&#36825;&#20004;&#20010;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#19982;&#23436;&#20840;&#24494;&#35843;&#26041;&#27861;&#19981;&#21516;&#65292;MemoryPrompt&#22312;&#36866;&#24212;&#26032;&#20219;&#21153;&#26102;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#22240;&#27492;&#19981;&#20250;&#30772;&#22351;&#38750;&#19987;&#23478;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15268v1 Announce Type: cross  Abstract: Transformer-based language models (LMs) track contextual information through large, hard-coded input windows. We introduce MemoryPrompt, a leaner approach in which the LM is complemented by a small auxiliary recurrent network that passes information to the LM by prefixing its regular input with a sequence of vectors, akin to soft prompts, without requiring LM finetuning. Tested on a task designed to probe a LM's ability to keep track of multiple fact updates, a MemoryPrompt-augmented LM outperforms much larger LMs that have access to the full input history. We also test MemoryPrompt on a long-distance dialogue dataset, where its performance is comparable to that of a model conditioned on the entire conversation history. In both experiments we also observe that, unlike full-finetuning approaches, MemoryPrompt does not suffer from catastrophic forgetting when adapted to new tasks, thus not disrupting the generalist capabilities of the un
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35843;&#26597;&#20102;&#29992;&#25143;&#22914;&#20309;&#23450;&#21046;&#20195;&#29702;&#20154;&#35774;&#20197;&#21450;&#20854;&#23545;&#20114;&#21160;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#21160;&#24577;&#30340;&#24433;&#21709;&#65292;&#24320;&#21457;&#20102;CloChat&#30028;&#38754;&#20197;&#25903;&#25345;&#22312;LLMs&#20013;&#36731;&#26494;&#20934;&#30830;&#23450;&#21046;&#20195;&#29702;&#20154;&#35774;</title><link>https://arxiv.org/abs/2402.15265</link><description>&lt;p&gt;
CloChat&#65306;&#20102;&#35299;&#20154;&#20204;&#22914;&#20309;&#23450;&#21046;&#12289;&#20114;&#21160;&#21644;&#20307;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#35774;
&lt;/p&gt;
&lt;p&gt;
CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15265
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35843;&#26597;&#20102;&#29992;&#25143;&#22914;&#20309;&#23450;&#21046;&#20195;&#29702;&#20154;&#35774;&#20197;&#21450;&#20854;&#23545;&#20114;&#21160;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#21160;&#24577;&#30340;&#24433;&#21709;&#65292;&#24320;&#21457;&#20102;CloChat&#30028;&#38754;&#20197;&#25903;&#25345;&#22312;LLMs&#20013;&#36731;&#26494;&#20934;&#30830;&#23450;&#21046;&#20195;&#29702;&#20154;&#35774;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#22312;&#29983;&#25104;&#23545;&#35805;&#20195;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#20027;&#39064;&#19978;&#36827;&#34892;&#26080;&#32541;&#12289;&#20855;&#26377;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLM&#39537;&#21160;&#30340;&#23545;&#35805;&#20195;&#29702;&#20855;&#26377;&#22266;&#23450;&#30340;&#20010;&#24615;&#21644;&#21151;&#33021;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#20010;&#20307;&#29992;&#25143;&#38656;&#27714;&#30340;&#36866;&#24212;&#24615;&#12290;&#21019;&#24314;&#20855;&#26377;&#19981;&#21516;&#19987;&#19994;&#30693;&#35782;&#25110;&#29305;&#28857;&#30340;&#20010;&#24615;&#21270;&#20195;&#29702;&#20154;&#35774;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#32570;&#20047;&#20851;&#20110;&#20154;&#20204;&#22914;&#20309;&#23450;&#21046;&#21644;&#20114;&#21160;&#20195;&#29702;&#20154;&#35774;&#30340;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29992;&#25143;&#22914;&#20309;&#23450;&#21046;&#20195;&#29702;&#20154;&#35774;&#20197;&#21450;&#23427;&#20204;&#23545;&#20114;&#21160;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;CloChat&#65292;&#36825;&#26159;&#19968;&#31181;&#25903;&#25345;&#22312;LLMs&#20013;&#36731;&#26494;&#20934;&#30830;&#23450;&#21046;&#20195;&#29702;&#20154;&#35774;&#30340;&#30028;&#38754;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#27604;&#36739;&#21442;&#19982;&#32773;&#22914;&#20309;&#19982;CloChat&#21644;ChatGPT&#36827;&#34892;&#20114;&#21160;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21442;&#19982;&#32773;&#19982;&#23450;&#21046;&#20195;&#29702;&#20154;&#35774;&#24314;&#31435;&#20102;&#24773;&#24863;&#32852;&#31995;&#65292;&#21442;&#19982;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15265v1 Announce Type: cross  Abstract: Large language models (LLMs) have facilitated significant strides in generating conversational agents, enabling seamless, contextually relevant dialogues across diverse topics. However, the existing LLM-driven conversational agents have fixed personalities and functionalities, limiting their adaptability to individual user needs. Creating personalized agent personas with distinct expertise or traits can address this issue. Nonetheless, we lack knowledge of how people customize and interact with agent personas. In this research, we investigated how users customize agent personas and their impact on interaction quality, diversity, and dynamics. To this end, we developed CloChat, an interface supporting easy and accurate customization of agent personas in LLMs. We conducted a study comparing how participants interact with CloChat and ChatGPT. The results indicate that participants formed emotional bonds with the customized agents, engaged
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Dynamic Experienced Expert Modeling&#65288;DEEM&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#32463;&#39564;&#19987;&#23478;&#20351;LLMs&#33021;&#22815;&#20197;&#21322;&#21442;&#25968;&#21270;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;&#22312;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15264</link><description>&lt;p&gt;
DEEM&#65306;&#38754;&#21521;&#31435;&#22330;&#26816;&#27979;&#30340;&#21160;&#24577;&#20307;&#39564;&#19987;&#23478;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
DEEM: Dynamic Experienced Expert Modeling for Stance Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Dynamic Experienced Expert Modeling&#65288;DEEM&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#32463;&#39564;&#19987;&#23478;&#20351;LLMs&#33021;&#22815;&#20197;&#21322;&#21442;&#25968;&#21270;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;&#22312;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21021;&#27493;&#23581;&#35797;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#65292;&#23637;&#29616;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#31435;&#22330;&#26816;&#27979;&#36890;&#24120;&#38656;&#35201;&#35814;&#32454;&#30340;&#32972;&#26223;&#30693;&#35782;&#65292;&#20256;&#32479;&#30340;&#25512;&#29702;&#26041;&#27861;&#21487;&#33021;&#20250;&#24573;&#35270;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#36827;&#34892;&#19987;&#19994;&#21644;&#20934;&#30830;&#30340;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;LLMs&#30340;&#25512;&#29702;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#23588;&#20854;&#22312;&#21033;&#29992;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#27169;&#25311;&#29305;&#23450;&#19987;&#23478;&#65288;&#21363;&#22810;&#26234;&#33021;&#20307;&#65289;&#26469;&#26816;&#27979;&#31435;&#22330;&#26041;&#38754;&#12290;&#19982;&#29616;&#26377;&#38656;&#35201;&#35814;&#32454;&#25551;&#36848;&#24182;&#20351;&#29992;&#22266;&#23450;&#19987;&#23478;&#30340;&#22810;&#26234;&#33021;&#20307;&#20316;&#21697;&#19981;&#21516;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Dynamic Experienced Expert Modeling&#65288;DEEM&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#29983;&#25104;&#30340;&#32463;&#39564;&#19987;&#23478;&#65292;&#24182;&#35753;LLMs&#20197;&#21322;&#21442;&#25968;&#21270;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#65292;&#20351;&#19987;&#23478;&#26356;&#20855;&#26222;&#36866;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DEEM&#22312;&#19977;&#20010;&#22330;&#26223;&#19978;&#19968;&#30452;&#36798;&#21040;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15264v1 Announce Type: new  Abstract: Recent work has made a preliminary attempt to use large language models (LLMs) to solve the stance detection task, showing promising results. However, considering that stance detection usually requires detailed background knowledge, the vanilla reasoning method may neglect the domain knowledge to make a professional and accurate analysis. Thus, there is still room for improvement of LLMs reasoning, especially in leveraging the generation capability of LLMs to simulate specific experts (i.e., multi-agents) to detect the stance. In this paper, different from existing multi-agent works that require detailed descriptions and use fixed experts, we propose a Dynamic Experienced Expert Modeling (DEEM) method which can leverage the generated experienced experts and let LLMs reason in a semi-parametric way, making the experts more generalizable and reliable. Experimental results demonstrate that DEEM consistently achieves the best results on thre
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;Llama-2-70B&#22686;&#24378;MultiWOZ&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#65292;&#26377;&#25928;&#35299;&#20915;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#20013;&#30340;&#38386;&#32842;&#24178;&#25200;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#21516;&#26102;&#25215;&#35748;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#24182;&#25512;&#21160;&#20219;&#21153;&#30340;&#36827;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.15248</link><description>&lt;p&gt;
Chitchat&#20316;&#20026;&#24178;&#25200;&#65306;&#21521;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#28155;&#21152;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Chitchat as Interference: Adding User Backstories to Task-Oriented Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15248
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;Llama-2-70B&#22686;&#24378;MultiWOZ&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#65292;&#26377;&#25928;&#35299;&#20915;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#20013;&#30340;&#38386;&#32842;&#24178;&#25200;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#21516;&#26102;&#25215;&#35748;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#24182;&#25512;&#21160;&#20219;&#21153;&#30340;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#65288;TOD&#65289;&#20013;&#65292;&#20154;&#31867;&#29992;&#25143;&#33258;&#28982;&#20250;&#24341;&#20837;&#36229;&#20986;&#20219;&#21153;&#33539;&#22260;&#30340;&#38386;&#32842;&#65292;&#24178;&#25200;&#20102;&#23545;&#35805;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;Llama-2-70B&#36827;&#34892;&#23569;&#26679;&#26412;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;MultiWOZ&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#65292;&#36825;&#26159;TOD&#20013;&#20856;&#22411;&#30340;&#38386;&#32842;&#24178;&#25200;&#30340;&#19968;&#20010;&#20363;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#35797;&#20004;&#20010;&#27169;&#22411;&#26469;&#35780;&#20272;&#27492;&#28155;&#21152;&#30340;&#24433;&#21709;&#65306;&#19968;&#20010;&#20165;&#22312;TOD&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21478;&#19968;&#20010;&#22312;TOD&#19978;&#36827;&#34892;&#21021;&#27493;&#38386;&#32842;&#20132;&#20114;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#31995;&#32479;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#26377;&#25928;&#29992;&#20110;&#35757;&#32451;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#22312;&#21516;&#19968;&#36718;&#20013;&#25345;&#32493;&#25215;&#35748;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#24182;&#25104;&#21151;&#25512;&#21160;&#20219;&#21153;&#30340;&#36827;&#34892;&#65292;&#36825;&#24471;&#21040;&#20102;&#20154;&#31867;&#35780;&#20272;&#30340;&#30830;&#35748;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#24341;&#20837;&#29992;&#25143;&#32972;&#26223;&#25925;&#20107;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15248v1 Announce Type: new  Abstract: During task-oriented dialogues (TODs), human users naturally introduce chitchat that is beyond the immediate scope of the task, interfering with the flow of the conversation. To address this issue without the need for expensive manual data creation, we use few-shot prompting with Llama-2-70B to enhance the MultiWOZ dataset with user backstories, a typical example of chitchat interference in TODs. We assess the impact of this addition by testing two models: one trained solely on TODs and another trained on TODs with a preliminary chitchat interaction. Our analysis reveals that our enriched dataset poses a significant challenge to these systems. Moreover, we demonstrate that our dataset can be effectively used for training purposes, enabling a system to consistently acknowledge the user's backstory while also successfully moving the task forward in the same turn, as confirmed by human evaluation. These findings highlight the benefits of ge
&lt;/p&gt;</description></item><item><title>GPT-HateCheck&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#29616;&#23454;&#30340;&#21151;&#33021;&#27979;&#35797;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27979;&#35797;&#26696;&#20363;&#36807;&#20110;&#36890;&#29992;&#31616;&#21333;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15238</link><description>&lt;p&gt;
GPT-HateCheck: LLMs&#33021;&#21542;&#20026;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#32534;&#20889;&#26356;&#22909;&#30340;&#21151;&#33021;&#27979;&#35797;&#65311;
&lt;/p&gt;
&lt;p&gt;
GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15238
&lt;/p&gt;
&lt;p&gt;
GPT-HateCheck&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#29616;&#23454;&#30340;&#21151;&#33021;&#27979;&#35797;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27979;&#35797;&#26696;&#20363;&#36807;&#20110;&#36890;&#29992;&#31616;&#21333;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20167;&#24680;&#26816;&#27979;&#21463;&#21040;&#25968;&#25454;&#37319;&#26679;&#12289;&#27880;&#37322;&#21644;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#24341;&#20837;&#30340;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#20165;&#27979;&#37327;&#22312;&#30041;&#23384;&#27979;&#35797;&#25968;&#25454;&#20013;&#25152;&#26377;&#31034;&#20363;&#19978;&#30340;&#24179;&#22343;&#24615;&#33021;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24517;&#39035;&#35782;&#21035;&#29305;&#23450;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#24182;&#22312;&#20854;&#26356;&#26377;&#21487;&#33021;&#22833;&#36133;&#26102;&#33719;&#24471;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#36817;&#30340;&#26041;&#21521;&#65292;&#21363;HateCheck&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#20351;&#29992;&#27169;&#26495;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#27979;&#35797;&#31934;&#32454;&#31890;&#24230;&#27169;&#22411;&#21151;&#33021;&#30340;&#22871;&#20214;&#65292;&#27169;&#26495;&#30340;&#24418;&#24335;&#20026;&#8220;&#20320;&#21482;&#26159;&#19968;&#20010;[&#39554;&#20154;&#30340;&#35789;]&#23545;&#25105;&#26469;&#35828;&#8221;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;HateCheck&#20801;&#35768;&#33719;&#24471;&#26356;&#35814;&#32454;&#30340;&#35786;&#26029;&#35265;&#35299;&#65292;&#20294;&#20854;&#27979;&#35797;&#29992;&#20363;&#36890;&#24120;&#26159;&#36890;&#29992;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#31616;&#21333;&#30340;&#21477;&#23376;&#32467;&#26500;&#65292;&#19981;&#31526;&#21512;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPT-HateCheck&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#29616;&#23454;&#30340;&#21151;&#33021;&#27979;&#35797;&#12290;&#25105;&#20204;&#37319;&#29992;&#39069;&#22806;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#26469;&#39564;&#35777;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15238v1 Announce Type: new  Abstract: Online hate detection suffers from biases incurred in data sampling, annotation, and model pre-training. Therefore, measuring the averaged performance over all examples in held-out test data is inadequate. Instead, we must identify specific model weaknesses and be informed when it is more likely to fail. A recent proposal in this direction is HateCheck, a suite for testing fine-grained model functionalities on synthesized data generated using templates of the kind "You are just a [slur] to me." However, despite enabling more detailed diagnostic insights, the HateCheck test cases are often generic and have simplistic sentence structures that do not match the real-world data. To address this limitation, we propose GPT-HateCheck, a framework to generate more diverse and realistic functional tests from scratch by instructing large language models (LLMs). We employ an additional natural language inference (NLI) model to verify the generations
&lt;/p&gt;</description></item><item><title>ChunkAttention&#26159;&#19968;&#31181;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#24182;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#34892;&#26102;&#25913;&#21892;&#20869;&#23384;&#21033;&#29992;&#29575;&#30340;KV&#32531;&#23384;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#20197;&#25552;&#39640;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15220</link><description>&lt;p&gt;
ChunkAttention: &#20855;&#26377;&#21069;&#32512;&#24863;&#30693;KV&#32531;&#23384;&#21644;&#20004;&#38454;&#27573;&#20998;&#21306;&#30340;&#39640;&#25928;&#33258;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15220
&lt;/p&gt;
&lt;p&gt;
ChunkAttention&#26159;&#19968;&#31181;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#24182;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#34892;&#26102;&#25913;&#21892;&#20869;&#23384;&#21033;&#29992;&#29575;&#30340;KV&#32531;&#23384;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#20197;&#25552;&#39640;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23545;&#20110;&#38271;&#24207;&#21015;&#26469;&#35828;&#26159;&#25512;&#29702;&#24310;&#36831;&#30340;&#19968;&#20010;&#26174;&#33879;&#26469;&#28304;&#12290;&#22312;&#22810;&#31199;&#25143;LLMs&#26381;&#21153;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;LLM&#35831;&#27714;&#22312;&#21069;&#32512;&#20013;&#20849;&#20139;&#31995;&#32479;&#25552;&#31034;&#30340;&#27010;&#29575;&#65292;&#21487;&#20197;&#20248;&#21270;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25805;&#20316;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChunkAttention&#65292;&#19968;&#31181;&#20855;&#26377;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#26816;&#27979;&#22810;&#20010;&#35831;&#27714;&#20043;&#38388;&#21305;&#37197;&#30340;&#25552;&#31034;&#21069;&#32512;&#65292;&#24182;&#20849;&#20139;&#23427;&#20204;&#30340;&#38190;/&#20540;&#24352;&#37327;&#20197;&#25913;&#36827;KV&#32531;&#23384;&#30340;&#20869;&#23384;&#21033;&#29992;&#29575;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#25972;&#20307;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#65292;&#24182;&#23558;&#23427;&#20204;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#26469;&#23454;&#29616;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;KV&#32531;&#23384;&#20043;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#33258;&#27880;&#24847;&#21147;&#20869;&#26680;&#65292;&#20854;&#20013;&#23454;&#29616;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#65292;&#20197;&#25913;&#21892;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15220v1 Announce Type: cross  Abstract: Self-attention is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences. In multi-tenant LLMs serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#38544;&#34109;&#25552;&#31034;&#25915;&#20987;&#65288;BSPA&#65289;&#65292;&#37319;&#29992;&#26816;&#32034;&#22120;&#27169;&#25311;&#25915;&#20987;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15218</link><description>&lt;p&gt;
&#25506;&#32034;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#40657;&#21283;&#23376;&#38544;&#34109;&#25552;&#31034;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
BSPA: Exploring Black-box Stealthy Prompt Attacks against Image Generators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15218
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#38544;&#34109;&#25552;&#31034;&#25915;&#20987;&#65288;BSPA&#65289;&#65292;&#37319;&#29992;&#26816;&#32034;&#22120;&#27169;&#25311;&#25915;&#20987;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#20854;&#22823;&#22411;&#30340;&#22270;&#20687;&#29983;&#25104;&#22120;&#22312;&#21508;&#20010;&#39046;&#22495;&#25552;&#20379;&#20102;&#37325;&#22823;&#30340;&#21464;&#38761;&#28508;&#21147;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#35774;&#35745;&#29305;&#23450;&#25552;&#31034;&#26469;&#36890;&#36807;&#19968;&#20123;&#40657;&#21283;&#23376;API&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#22270;&#20687;&#29983;&#25104;&#22120;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#36890;&#36807;&#20154;&#24037;&#35774;&#35745;&#30340;&#27602;&#32032;&#25991;&#26412;&#29983;&#25104;&#19981;&#36866;&#23452;&#24037;&#20316;&#20869;&#23481;&#65288;NSFW&#65289;&#65292;&#23588;&#20854;&#26159;&#23545;&#20154;&#31867;&#35266;&#23519;&#32773;&#26497;&#38590;&#23519;&#35273;&#12290;&#25105;&#20204;&#36843;&#20999;&#38656;&#35201;&#22823;&#37327;&#36890;&#29992;&#19988;&#21487;&#36801;&#31227;&#30340;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#23588;&#20854;&#26159;&#40657;&#30418;&#21457;&#24067;&#30340;API&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21463;&#38480;&#20110;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#35774;&#35745;&#27969;&#31243;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#32473;&#23450;&#25351;&#20196;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#38544;&#34109;&#25552;&#31034;&#25915;&#20987;&#65288;BSPA&#65289;&#65292;&#37319;&#29992;&#26816;&#32034;&#22120;&#27169;&#25311;API&#29992;&#25143;&#30340;&#25915;&#20987;&#12290;&#23427;&#33021;&#26377;&#25928;&#21033;&#29992;&#36807;&#28388;&#22120;&#35780;&#20998;&#26469;&#35843;&#25972;&#25935;&#24863;&#35789;&#27719;&#30340;&#26816;&#32034;&#31354;&#38388;&#65292;&#20197;&#21305;&#37197;&#36755;&#20837;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15218v1 Announce Type: cross  Abstract: Extremely large image generators offer significant transformative potential across diverse sectors. It allows users to design specific prompts to generate realistic images through some black-box APIs. However, some studies reveal that image generators are notably susceptible to attacks and generate Not Suitable For Work (NSFW) contents by manually designed toxin texts, especially imperceptible to human observers. We urgently need a multitude of universal and transferable prompts to improve the safety of image generators, especially black-box-released APIs. Nevertheless, they are constrained by labor-intensive design processes and heavily reliant on the quality of the given instructions. To achieve this, we introduce a black-box stealthy prompt attack (BSPA) that adopts a retriever to simulate attacks from API users. It can effectively harness filter scores to tune the retrieval space of sensitive words for matching the input prompts, t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#23454;&#20363;&#32423;&#21069;&#32512;&#22312;&#27880;&#24847;&#21147;&#31354;&#38388;&#20013;&#36827;&#34892;&#32454;&#31890;&#24230;&#27604;&#36739;&#65292;&#20174;&#32780;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#33073;&#27602;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15202</link><description>&lt;p&gt;
&#36890;&#36807;&#23454;&#20363;&#32423;&#21069;&#32512;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#33073;&#27602;
&lt;/p&gt;
&lt;p&gt;
Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15202
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#23454;&#20363;&#32423;&#21069;&#32512;&#22312;&#27880;&#24847;&#21147;&#31354;&#38388;&#20013;&#36827;&#34892;&#32454;&#31890;&#24230;&#27604;&#36739;&#65292;&#20174;&#32780;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#33073;&#27602;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20598;&#23572;&#20250;&#23545;&#26576;&#20123;&#25552;&#31034;&#29983;&#25104;&#27602;&#24615;&#20869;&#23481;&#65292;&#22914;&#20398;&#36785;&#12289;&#23041;&#32961;&#21644;&#31895;&#35805;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#21033;&#29992;&#21508;&#31181;&#22522;&#20110;&#24494;&#35843;&#21644;&#22522;&#20110;&#35299;&#30721;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#27602;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#25104;&#26412;&#65292;&#22914;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#36741;&#21161;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#23454;&#20363;&#32423;&#21069;&#32512;&#36827;&#34892;&#32454;&#31890;&#24230;&#33073;&#27602;&#65288;FGDILP&#65289;&#65292;&#20197;&#20943;&#36731;&#27602;&#24615;&#25991;&#26412;&#32780;&#26080;&#38656;&#39069;&#22806;&#36153;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;FGDILP&#36890;&#36807;&#22312;&#23454;&#20363;&#32423;&#21035;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#27491;&#21069;&#32512;&#30340;&#25552;&#31034;&#26469;&#23545;&#27604;&#27880;&#24847;&#21147;&#31354;&#38388;&#20013;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#32780;&#22810;&#20010;&#24102;&#26377;&#36127;&#21069;&#32512;&#30340;&#25552;&#31034;&#12290;&#36825;&#20801;&#35768;&#26500;&#24314;&#32454;&#31890;&#24230;&#30340;&#27425;&#27602;&#24615;&#21521;&#37327;&#65292;&#20351;&#25991;&#26412;&#34987;&#35782;&#21035;&#20026;&#27425;&#27602;&#24615;&#21464;&#24471;&#26356;&#21152;&#31934;&#32454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15202v1 Announce Type: new  Abstract: Impressive results have been achieved in natural language processing (NLP) tasks through the training of large language models (LLMs). However, these models occasionally produce toxic content such as insults, threats, and profanity in response to certain prompts, thereby constraining their practical utility. To tackle this issue, various finetuning-based and decoding-based approaches have been utilized to mitigate toxicity. However, these methods typically necessitate additional costs such as high-quality training data or auxiliary models. In this paper, we propose fine-grained detoxification via instance-level prefixes (FGDILP) to mitigate toxic text without additional cost. Specifically, FGDILP contrasts the contextualized representation in attention space using a positive prefix-prepended prompt against multiple negative prefix-prepended prompts at the instance level. This allows for constructing fine-grained subtoxicity vectors, whic
&lt;/p&gt;</description></item><item><title>DeMPT&#25552;&#20986;&#20102;&#35299;&#30721;&#22686;&#24378;&#30340;&#22810;&#38454;&#27573;&#25552;&#31034;&#20248;&#21270;&#65292;&#20351;&#24471;LLMs&#26356;&#22909;&#22320;&#27169;&#25311;&#21644;&#21033;&#29992;&#21477;&#38388;&#21644;&#21477;&#20869;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#36866;&#24212;&#19978;&#19979;&#25991;&#24863;&#30693;NMT&#12290;</title><link>https://arxiv.org/abs/2402.15200</link><description>&lt;p&gt;
DeMPT&#65306;&#35299;&#30721;&#22686;&#24378;&#30340;&#22810;&#38454;&#27573;&#25552;&#31034;&#20248;&#21270;&#65292;&#20351;LLMs&#25104;&#20026;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32763;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15200
&lt;/p&gt;
&lt;p&gt;
DeMPT&#25552;&#20986;&#20102;&#35299;&#30721;&#22686;&#24378;&#30340;&#22810;&#38454;&#27573;&#25552;&#31034;&#20248;&#21270;&#65292;&#20351;&#24471;LLMs&#26356;&#22909;&#22320;&#27169;&#25311;&#21644;&#21033;&#29992;&#21477;&#38388;&#21644;&#21477;&#20869;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#36866;&#24212;&#19978;&#19979;&#25991;&#24863;&#30693;NMT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#20165;&#20855;&#26377;&#35299;&#30721;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#36830;&#25509;&#30340;&#26041;&#24335;&#36866;&#24212;&#19978;&#19979;&#25991;&#24863;&#30693;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#65292;&#20854;&#20013;LLMs&#23558;&#28304;&#21477;&#65288;&#21363;&#21477;&#20869;&#19978;&#19979;&#25991;&#65289;&#21644;&#21477;&#38388;&#19978;&#19979;&#25991;&#30340;&#36830;&#25509;&#20316;&#20026;&#36755;&#20837;&#65292;&#28982;&#21518;&#39034;&#24207;&#29983;&#25104;&#30446;&#26631;&#26631;&#35760;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Decoding-enhanced Multi-phase Prompt Tuning&#65288;DeMPT&#65289;&#30340;&#26367;&#20195;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#20351;LLMs&#33021;&#22815;&#27495;&#35270;&#24615;&#22320;&#23545;&#27169;&#32452;&#21644;&#21033;&#29992;&#21477;&#38388;&#21644;&#21477;&#20869;&#19978;&#19979;&#25991;&#65292;&#24182;&#26356;&#26377;&#25928;&#22320;&#23558;LLMs&#35843;&#25972;&#21040;&#19978;&#19979;&#25991;&#24863;&#30693;NMT&#12290;&#39318;&#20808;&#65292;DeMPT&#23558;&#19978;&#19979;&#25991;&#24863;&#30693;NMT&#36807;&#31243;&#20998;&#20026;&#19977;&#20010;&#21333;&#29420;&#38454;&#27573;&#12290;&#22312;&#27599;&#20010;&#38454;&#27573;&#65292;&#24341;&#20837;&#19981;&#21516;&#30340;&#36830;&#32493;&#25552;&#31034;&#65292;&#20351;LLMs&#33021;&#22815;&#21306;&#20998;&#22320;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15200v1 Announce Type: new  Abstract: Generally, the decoder-only large language models (LLMs) are adapted to context-aware neural machine translation (NMT) in a concatenating way, where LLMs take the concatenation of the source sentence (i.e., intra-sentence context) and the inter-sentence context as the input, and then to generate the target tokens sequentially. This adaptation strategy, i.e., concatenation mode, considers intra-sentence and inter-sentence contexts with the same priority, despite an apparent difference between the two kinds of contexts. In this paper, we propose an alternative adaptation approach, named Decoding-enhanced Multi-phase Prompt Tuning (DeMPT), to make LLMs discriminately model and utilize the inter- and intra-sentence context and more effectively adapt LLMs to context-aware NMT. First, DeMPT divides the context-aware NMT process into three separate phases. During each phase, different continuous prompts are introduced to make LLMs discriminatel
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;BioELQA&#65292;&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#30475;&#20316;&#26159;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#65292;&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#26816;&#32034;&#22120;&#33719;&#24471;&#20505;&#36873;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23454;&#20307;&#38142;&#25509;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.15189</link><description>&lt;p&gt;
&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#35270;&#20026;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Biomedical Entity Linking as Multiple Choice Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15189
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;BioELQA&#65292;&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#30475;&#20316;&#26159;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#65292;&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#26816;&#32034;&#22120;&#33719;&#24471;&#20505;&#36873;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23454;&#20307;&#38142;&#25509;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#65288;BioEL&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#32454;&#31890;&#24230;&#21644;&#38271;&#23614;&#23454;&#20307;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BioELQA&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#35270;&#20026;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;BioELQA&#39318;&#20808;&#21033;&#29992;&#24555;&#36895;&#26816;&#32034;&#22120;&#33719;&#24471;&#20505;&#36873;&#23454;&#20307;&#65292;&#23558;&#25552;&#21450;&#21644;&#20505;&#36873;&#23454;&#20307;&#20849;&#21516;&#21576;&#29616;&#32473;&#29983;&#25104;&#22120;&#65292;&#28982;&#21518;&#36755;&#20986;&#19982;&#20854;&#36873;&#23450;&#23454;&#20307;&#30456;&#20851;&#30340;&#39044;&#27979;&#31526;&#21495;&#12290;&#36825;&#31181;&#20844;&#24335;&#20351;&#24471;&#19981;&#21516;&#20505;&#36873;&#23454;&#20307;&#20043;&#38388;&#30340;&#26126;&#30830;&#27604;&#36739;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#25429;&#25417;&#20102;&#25552;&#21450;&#21644;&#23454;&#20307;&#20043;&#38388;&#20197;&#21450;&#23454;&#20307;&#20043;&#38388;&#30340;&#31934;&#32454;&#20132;&#20114;&#12290;&#20026;&#20102;&#25913;&#21892;&#38271;&#23614;&#23454;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#26816;&#32034;&#30456;&#20284;&#30340;&#24050;&#26631;&#35760;&#35757;&#32451;&#23454;&#20363;&#20316;&#20026;&#32447;&#32034;&#65292;&#24182;&#23558;&#36755;&#20837;&#19982;&#26816;&#32034;&#23454;&#20363;&#36830;&#25509;&#21040;&#29983;&#25104;&#22120;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BioELQA&#30340;&#34920;&#29616;&#20248;&#20110;&#32479;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15189v1 Announce Type: cross  Abstract: Although biomedical entity linking (BioEL) has made significant progress with pre-trained language models, challenges still exist for fine-grained and long-tailed entities. To address these challenges, we present BioELQA, a novel model that treats Biomedical Entity Linking as Multiple Choice Question Answering. BioELQA first obtains candidate entities with a fast retriever, jointly presents the mention and candidate entities to a generator, and then outputs the predicted symbol associated with its chosen entity. This formulation enables explicit comparison of different candidate entities, thus capturing fine-grained interactions between mentions and entities, as well as among entities themselves. To improve generalization for long-tailed entities, we retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator. Extensive experimental results show that BioELQA outperforms stat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#23436;&#21892;&#21644;&#26684;&#24335;&#21270;&#25913;&#36827;LMs&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#38750;&#23433;&#20840;&#23545;&#40784;&#30340;LMs&#20013;&#20063;&#20855;&#26377;&#20986;&#33394;&#30340;&#23433;&#20840;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15180</link><description>&lt;p&gt;
&#25171;&#30772;Breakout: &#29992;&#33258;&#25105;&#23436;&#21892;&#37325;&#26032;&#23450;&#20041;LM&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15180
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#23436;&#21892;&#21644;&#26684;&#24335;&#21270;&#25913;&#36827;LMs&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#38750;&#23433;&#20840;&#23545;&#40784;&#30340;LMs&#20013;&#20063;&#20855;&#26377;&#20986;&#33394;&#30340;&#23433;&#20840;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35686;&#21578;&#65306;&#26412;&#25991;&#21253;&#21547;&#21487;&#33021;&#24341;&#36215;&#19981;&#24555;&#30340;&#20882;&#29359;&#24615;&#35789;&#35821;&#12290;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#23481;&#26131;&#34987;&#21033;&#29992;&#36827;&#34892;&#24694;&#24847;&#28389;&#29992;&#12290;&#23545;LM&#36827;&#34892;&#23433;&#20840;&#23545;&#40784;&#30340;&#35757;&#32451;&#38750;&#24120;&#22797;&#26434;&#65292;&#20351;&#24471;&#38590;&#20197;&#31435;&#21363;&#24212;&#23545;&#24555;&#36895;&#21457;&#23637;&#30340;&#25915;&#20987;&#65292;&#22914;&#36234;&#29425;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26684;&#24335;&#33258;&#25105;&#23436;&#21892;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#38750;&#23433;&#20840;&#23545;&#40784;&#30340;LMs&#20013;&#20063;&#33021;&#23454;&#29616;&#20986;&#33394;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#31181;&#38450;&#24481;&#22522;&#32447;&#36827;&#34892;&#35780;&#20272;&#65292;&#34920;&#26126;&#36825;&#26159;&#38024;&#23545;&#36234;&#29425;&#25915;&#20987;&#26368;&#23433;&#20840;&#30340;&#26080;&#35757;&#32451;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#33258;&#25105;&#23436;&#21892;&#36807;&#31243;&#25928;&#29575;&#30340;&#26684;&#24335;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#36739;&#23569;&#36845;&#20195;&#20013;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#38750;&#23433;&#20840;&#23545;&#40784;&#30340;LM&#22312;&#23433;&#20840;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#23433;&#20840;&#23545;&#40784;&#30340;LM&#65292;&#22240;&#20026;&#23427;&#20204;&#32473;&#20986;&#26356;&#26377;&#29992;&#19988;&#26356;&#23433;&#20840;&#30340;&#22238;&#22797;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#33021;&#22815;&#22312;&#36739;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#23454;&#29616;&#26356;&#23569;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15180v1 Announce Type: cross  Abstract: Caution: This paper includes offensive words that could potentially cause unpleasantness. Language models (LMs) are vulnerable to exploitation for adversarial misuse. Training LMs for safety alignment is extensive and makes it hard to respond to fast-developing attacks immediately, such as jailbreaks. We propose self-refine with formatting that achieves outstanding safety even in non-safety-aligned LMs and evaluate our method alongside several defense baselines, demonstrating that it is the safest training-free method against jailbreak attacks. Additionally, we proposed a formatting method that improves the efficiency of the self-refine process while reducing attack success rates in fewer iterations. We've also observed that non-safety-aligned LMs outperform safety-aligned LMs in safety tasks by giving more helpful and safe responses. In conclusion, our findings can achieve less safety risk with fewer computational costs, allowing non-
&lt;/p&gt;</description></item><item><title>RED&#36890;&#36807;&#34920;&#31034;&#32534;&#36753;&#26174;&#33879;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.15179</link><description>&lt;p&gt;
&#36890;&#36807;&#34920;&#31034;&#32534;&#36753;&#25512;&#36827;&#24494;&#35843;&#20013;&#30340;&#21442;&#25968;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Advancing Parameter Efficiency in Fine-tuning via Representation Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15179
&lt;/p&gt;
&lt;p&gt;
RED&#36890;&#36807;&#34920;&#31034;&#32534;&#36753;&#26174;&#33879;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#22240;&#20854;&#33021;&#22815;&#22312;&#20165;&#26356;&#26032;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#26102;&#36798;&#21040;&#31454;&#20105;&#24615;&#32467;&#26524;&#32780;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#31070;&#32463;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#34920;&#31034;&#32534;&#36753;&#65288;RED&#65289;&#65292;&#20854;&#25193;&#25918;&#21644;&#20559;&#32622;&#27599;&#19968;&#23618;&#20135;&#29983;&#30340;&#34920;&#31034;&#12290;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#27604;&#65292;RED&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#38477;&#20302;&#20102;$25,700$&#20493;&#65292;&#24182;&#19982;LoRA&#30456;&#27604;&#38477;&#20302;&#20102;32&#20493;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;RED&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#23545;&#19981;&#21516;&#26550;&#26500;&#21644;&#35268;&#27169;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15179v1 Announce Type: cross  Abstract: Parameter Efficient Fine-Tuning (PEFT) has gained significant attention for its ability to achieve competitive results while updating only a small subset of trainable parameters. Despite the promising performance of current PEFT methods, they present challenges in hyperparameter selection, such as determining the rank of LoRA or Adapter, or specifying the length of soft prompts. In addressing these challenges, we propose a novel approach to fine-tuning neural models, termed Representation EDiting (RED), which scales and biases the representation produced at each layer. RED substantially reduces the number of trainable parameters by a factor of $25,700$ compared to full parameter fine-tuning, and by a factor of $32$ compared to LoRA. Remarkably, RED achieves comparable or superior results to full parameter fine-tuning and other PEFT methods. Extensive experiments were conducted across models of varying architectures and scales, includin
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22522;&#20110;&#24494;&#35843;&#30340;&#25688;&#35201;&#27169;&#22411;&#22312;&#22788;&#29702;&#30693;&#35782;&#20914;&#31361;&#26102;&#30340;&#23454;&#20307;&#32423;&#20107;&#23454;&#36866;&#24212;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22686;&#24378;&#20102;&#20107;&#23454;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15162</link><description>&lt;p&gt;
&#22522;&#20110;&#24494;&#35843;&#30340;&#25277;&#35937;&#24335;&#25688;&#35201;&#27169;&#22411;&#30340;&#23454;&#20307;&#32423;&#20107;&#23454;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Entity-level Factual Adaptiveness of Fine-tuning based Abstractive Summarization Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15162
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22522;&#20110;&#24494;&#35843;&#30340;&#25688;&#35201;&#27169;&#22411;&#22312;&#22788;&#29702;&#30693;&#35782;&#20914;&#31361;&#26102;&#30340;&#23454;&#20307;&#32423;&#20107;&#23454;&#36866;&#24212;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22686;&#24378;&#20102;&#20107;&#23454;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#24335;&#25688;&#35201;&#27169;&#22411;&#22312;&#22788;&#29702;&#21442;&#25968;&#21270;&#30693;&#35782;&#19982;&#36755;&#20837;&#25991;&#26723;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#26102;&#65292;&#24448;&#24448;&#29983;&#25104;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#20869;&#23481;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;&#24494;&#35843;&#30340;&#25688;&#35201;&#27169;&#22411;&#23545;&#30693;&#35782;&#20914;&#31361;&#30340;&#40065;&#26834;&#24615;&#65292;&#21363;&#25105;&#20204;&#31216;&#20043;&#20026;&#20107;&#23454;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#35780;&#20272;&#38598;&#65292;&#24182;&#21457;&#29616;&#20107;&#23454;&#36866;&#24212;&#24615;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#24182;&#38750;&#24378;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#25511;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20854;&#20013;&#22686;&#24378;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#31243;&#24230;&#26159;&#21487;&#35843;&#33410;&#30340;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PEGASUS &#21644; BART&#65289;&#21644;&#20004;&#20010;&#24494;&#35843;&#25968;&#25454;&#38598;&#65288;XSum &#21644; CNN/DailyMail&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#20107;&#23454;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#23545;&#27604;&#26041;&#27861;&#30456;&#24403;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15162v1 Announce Type: cross  Abstract: Abstractive summarization models often generate factually inconsistent content particularly when the parametric knowledge of the model conflicts with the knowledge in the input document. In this paper, we analyze the robustness of fine-tuning based summarization models to the knowledge conflict, which we call factual adaptiveness. We utilize pre-trained language models to construct evaluation sets and find that factual adaptiveness is not strongly correlated with factual consistency on original datasets. Furthermore, we introduce a controllable counterfactual data augmentation method where the degree of knowledge conflict within the augmented data can be adjustable. Our experimental results on two pre-trained language models (PEGASUS and BART) and two fine-tuning datasets (XSum and CNN/DailyMail) demonstrate that our method enhances factual adaptiveness while achieving factual consistency on original datasets on par with the contrastiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25506;&#35752;&#20102;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#36951;&#24536;&#26694;&#26550;&#21450;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25913;&#36827;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#25928;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#25351;&#21335;&#12290;</title><link>https://arxiv.org/abs/2402.15159</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning of Pre-trained Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25506;&#35752;&#20102;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#36951;&#24536;&#26694;&#26550;&#21450;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25913;&#36827;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#25928;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32972;&#26223;&#19979;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20197;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#35299;&#20915;&#26041;&#26696;&#65292;&#37325;&#28857;&#20851;&#27880;&#39044;&#35757;&#32451;&#27169;&#22411;&#8212;&#8212;&#19968;&#20010;&#26126;&#26174;&#32570;&#20047;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;LLMs&#20013;&#21246;&#21202;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#21253;&#25324;&#23545;&#19971;&#31181;&#19981;&#21516;&#36951;&#24536;&#26041;&#27861;&#30340;&#25209;&#21028;&#24615;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;arXiv&#12289;&#20070;&#31821;&#21644;GitHub&#30340;&#31574;&#21010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#21147;&#30340;&#26426;&#22120;&#36951;&#24536;&#24615;&#33021;&#22522;&#20934;&#65292;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#27604;&#37325;&#26032;&#35757;&#32451;&#39640;&#20986; $10^5$ &#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#24067;&#25968;&#25454;&#19978;&#23558;&#26799;&#24230;&#19978;&#21319;&#19982;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#21487;&#20197;&#25913;&#21892;&#36229;&#21442;&#25968;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#36951;&#24536;&#36807;&#31243;&#20013;&#36827;&#34892;&#39640;&#25928;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#35814;&#32454;&#25351;&#21335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25512;&#21160;&#20102;&#26377;&#20851;&#20262;&#29702;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#30340;&#35752;&#35770;&#65292;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15159v1 Announce Type: cross  Abstract: This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Self-Adaptive Reconstruction Contrastive Sentence Embeddings&#65288;SARCSE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#37325;&#24314;&#21477;&#23376;&#20013;&#30340;&#25152;&#26377;&#20196;&#29260;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#20445;&#30041;&#26356;&#22810;&#32454;&#31890;&#24230;&#35821;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37325;&#24314;&#25439;&#22833;&#26469;&#32531;&#35299;&#23545;&#20196;&#29260;&#39057;&#29575;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.15153</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23545;&#27604;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#33258;&#36866;&#24212;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Self-Adaptive Reconstruction with Contrastive Learning for Unsupervised Sentence Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15153
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Self-Adaptive Reconstruction Contrastive Sentence Embeddings&#65288;SARCSE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#37325;&#24314;&#21477;&#23376;&#20013;&#30340;&#25152;&#26377;&#20196;&#29260;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#20445;&#30041;&#26356;&#22810;&#32454;&#31890;&#24230;&#35821;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37325;&#24314;&#25439;&#22833;&#26469;&#32531;&#35299;&#23545;&#20196;&#29260;&#39057;&#29575;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#20219;&#21153;&#26088;&#22312;&#23558;&#21477;&#23376;&#36716;&#25442;&#20026;&#35821;&#20041;&#21521;&#37327;&#34920;&#31034;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#30452;&#25509;&#20351;&#29992;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#27966;&#29983;&#30340;&#21477;&#23376;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20196;&#29260;&#20559;&#24046;&#65292;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#21477;&#23376;&#20013;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#65292;&#23548;&#33268;&#39044;&#27979;&#33021;&#21147;&#19981;&#20339;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37325;&#24314;&#23545;&#27604;&#21477;&#23376;&#23884;&#20837;&#65288;SARCSE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#37325;&#24314;&#21477;&#23376;&#20013;&#30340;&#25152;&#26377;&#20196;&#29260;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#32858;&#21512;&#20196;&#29260;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#32454;&#31890;&#24230;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37325;&#24314;&#25439;&#22833;&#26469;&#32531;&#35299;&#23545;&#20196;&#29260;&#39057;&#29575;&#30340;&#20559;&#35265;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24378;&#22522;&#20934;SimCSE&#30456;&#27604;&#65292;SARCSE&#22312;7&#20010;STS&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15153v1 Announce Type: new  Abstract: Unsupervised sentence embeddings task aims to convert sentences to semantic vector representations. Most previous works directly use the sentence representations derived from pretrained language models. However, due to the token bias in pretrained language models, the models can not capture the fine-grained semantics in sentences, which leads to poor predictions. To address this issue, we propose a novel Self-Adaptive Reconstruction Contrastive Sentence Embeddings (SARCSE) framework, which reconstructs all tokens in sentences with an AutoEncoder to help the model to preserve more fine-grained semantics during tokens aggregating. In addition, we proposed a self-adaptive reconstruction loss to alleviate the token bias towards frequency. Experimental results show that SARCSE gains significant improvements compared with the strong baseline SimCSE on the 7 STS tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;VSP-LLM&#26694;&#26550;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#24314;&#27169;&#33021;&#21147;&#65292;&#23454;&#29616;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#30340;&#22810;&#20219;&#21153;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.15151</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#38899;&#36935;&#35265;&#35821;&#35328;&#65306;VSP-LLM&#26694;&#26550;&#29992;&#20110;&#39640;&#25928;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#35270;&#35273;&#35821;&#38899;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15151
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;VSP-LLM&#26694;&#26550;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#24314;&#27169;&#33021;&#21147;&#65292;&#23454;&#29616;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#30340;&#22810;&#20219;&#21153;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#35821;&#38899;&#22788;&#29702;&#20013;&#65292;&#30001;&#20110;&#21767;&#37096;&#36816;&#21160;&#30340;&#27169;&#31946;&#24615;&#36136;&#65292;&#19978;&#19979;&#25991;&#24314;&#27169;&#33021;&#21147;&#26159;&#26368;&#37325;&#35201;&#30340;&#35201;&#27714;&#20043;&#19968;&#12290;&#20363;&#22914;&#65292;&#21516;&#38899;&#24322;&#20041;&#35789;&#65292;&#21363;&#20855;&#26377;&#30456;&#21516;&#21767;&#37096;&#36816;&#21160;&#20294;&#20135;&#29983;&#19981;&#21516;&#22768;&#38899;&#30340;&#21333;&#35789;&#65292;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#19978;&#19979;&#25991;&#26469;&#21306;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#38598;&#25104;LLM&#30340;&#35270;&#35273;&#35821;&#38899;&#22788;&#29702;&#65288;VSP-LLM&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;LLM&#30340;&#24378;&#22823;&#33021;&#21147;&#26469;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#24314;&#27169;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;VSP-LLM&#26088;&#22312;&#25191;&#34892;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#30340;&#22810;&#20219;&#21153;&#65292;&#20854;&#20013;&#32473;&#23450;&#30340;&#25351;&#20196;&#25511;&#21046;&#20219;&#21153;&#31867;&#22411;&#12290;&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#35270;&#35273;&#35821;&#38899;&#27169;&#22411;&#65292;&#23558;&#36755;&#20837;&#35270;&#39057;&#26144;&#23556;&#21040;LLM&#30340;&#36755;&#20837;&#28508;&#22312;&#31354;&#38388;&#12290;&#38024;&#23545;&#36755;&#20837;&#24103;&#23384;&#22312;&#20887;&#20313;&#20449;&#24687;&#30340;&#20107;&#23454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#37325;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#35821;&#38899;&#21333;&#20803;&#20943;&#23569;&#23884;&#20837;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15151v1 Announce Type: cross  Abstract: In visual speech processing, context modeling capability is one of the most important requirements due to the ambiguous nature of lip movements. For example, homophenes, words that share identical lip movements but produce different sounds, can be distinguished by considering the context. In this paper, we propose a novel framework, namely Visual Speech Processing incorporated with LLMs (VSP-LLM), to maximize the context modeling ability by bringing the overwhelming power of LLMs. Specifically, VSP-LLM is designed to perform multi-tasks of visual speech recognition and translation, where the given instructions control the type of task. The input video is mapped to the input latent space of a LLM by employing a self-supervised visual speech model. Focused on the fact that there is redundant information in input frames, we propose a novel deduplication method that reduces the embedded visual features by employing visual speech units. Thr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;NLI&#25968;&#25454;&#38598;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;STS&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15132</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;NLI&#25968;&#25454;&#38598;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Improving Sentence Embeddings with an Automatically Generated NLI Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15132
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;NLI&#25968;&#25454;&#38598;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;STS&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#24615;&#33021;&#12290;&#36825;&#22312;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#20013;&#21516;&#26679;&#25104;&#31435;&#65292;&#20854;&#20013;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;PromptEOL &#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;PromptEOL &#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21033;&#29992;&#20102;&#23545;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#25968;&#25454;&#38598;&#30340;&#25163;&#21160;&#26631;&#27880;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLM&#33258;&#21160;&#29983;&#25104;&#30340;NLI&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#23398;&#20064;&#30340;&#21477;&#23376;&#23884;&#20837;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24494;&#35843;PromptEOL&#12290;&#22312;STS&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#36798;&#21040;&#20102;82.21&#30340;&#24179;&#22343;Spearman&#31561;&#32423;&#30456;&#20851;&#31995;&#25968;&#65292;&#20174;&#32780;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#32780;&#26080;&#38656;&#20351;&#29992;&#22823;&#35268;&#27169;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15132v1 Announce Type: new  Abstract: Decoder-based large language models (LLMs) have shown high performance on many tasks in natural language processing. This is also true for sentence embedding learning, where a decoder-based model, PromptEOL, has achieved the best performance on semantic textual similarity (STS) tasks. However, PromptEOL makes great use of fine-tuning with a manually annotated natural language inference (NLI) dataset. We aim to improve sentence embeddings learned in an unsupervised setting by automatically generating an NLI dataset with an LLM and using it to fine-tune PromptEOL. In experiments on STS tasks, the proposed method achieved an average Spearman's rank correlation coefficient of 82.21 with respect to human evaluation, thus outperforming existing methods without using large, manually annotated datasets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#21160;&#24335;KBQA&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30693;&#35782;&#24211;&#20114;&#21160;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;KB&#20132;&#20114;&#30340;&#36890;&#29992;API&#65292;&#24182;&#35774;&#35745;&#20102;&#31034;&#20363;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.15131</link><description>&lt;p&gt;
&#20114;&#21160;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#36718;&#20132;&#20114;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#21160;&#24335;KBQA&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30693;&#35782;&#24211;&#20114;&#21160;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;KB&#20132;&#20114;&#30340;&#36890;&#29992;API&#65292;&#24182;&#35774;&#35745;&#20102;&#31034;&#20363;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#30340;&#39046;&#22495;&#12290;KBQA&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#23558;&#22797;&#26434;&#38382;&#39064;&#35299;&#26512;&#20026;&#21487;&#25191;&#34892;&#36923;&#36753;&#24418;&#24335;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#35821;&#20041;&#35299;&#26512;&#65288;SP&#65289;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#27880;&#37322;&#65292;&#36825;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#25104;&#26412;&#12290;&#26368;&#36817;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#21160;&#30340;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20986;&#29616;&#23637;&#31034;&#20102;&#24456;&#22909;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#26223;&#19979;&#20805;&#20998;&#21033;&#29992;LLMs&#23558;&#38382;&#39064;&#35299;&#26512;&#20026;&#36923;&#36753;&#24418;&#24335;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20114;&#21160;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;Interactive-KBQA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#19982;&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#30452;&#25509;&#20114;&#21160;&#26469;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#30340;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#20010;&#29992;&#20110;KB&#20132;&#20114;&#30340;&#36890;&#29992;API&#12290;&#23545;&#20110;&#27599;&#31181;&#22797;&#26434;&#38382;&#39064;&#31867;&#21035;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31034;&#20363;&#26469;&#25351;&#23548;LLMs&#23436;&#25104;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15131v1 Announce Type: cross  Abstract: This study explores the realm of knowledge-base question answering (KBQA). KBQA is considered a challenging task, particularly in parsing intricate questions into executable logical forms. Traditional semantic parsing (SP)-based methods require extensive data annotations, which result in significant costs. Recently, the advent of few-shot in-context learning, powered by large language models (LLMs), has showcased promising capabilities. Yet, fully leveraging LLMs to parse questions into logical forms in low-resource scenarios poses a substantial challenge. To tackle these hurdles, we introduce Interactive-KBQA, a framework designed to generate logical forms through direct interaction with knowledge bases (KBs). Within this framework, we have developed three generic APIs for KB interaction. For each category of complex question, we devised exemplars to guide LLMs through the reasoning processes. Our method achieves competitive results o
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#20195;&#29702;&#65288;LMAs&#65289;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#28085;&#30422;&#20102;&#24320;&#21457;&#32452;&#20214;&#12289;&#30740;&#31350;&#31867;&#22411;&#20998;&#31867;&#20197;&#21450;&#38598;&#20307;&#25928;&#33021;&#22686;&#24378;&#30340;&#21512;&#20316;&#26694;&#26550;&#31561;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2402.15116</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#20195;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Multimodal Agents: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15116
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#20195;&#29702;&#65288;LMAs&#65289;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#28085;&#30422;&#20102;&#24320;&#21457;&#32452;&#20214;&#12289;&#30740;&#31350;&#31867;&#22411;&#20998;&#31867;&#20197;&#21450;&#38598;&#20307;&#25928;&#33021;&#22686;&#24378;&#30340;&#21512;&#20316;&#26694;&#26550;&#31561;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#21160;&#22522;&#20110;&#25991;&#26412;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26102;&#21462;&#24471;&#20102;&#21331;&#36234;&#34920;&#29616;&#65292;&#36171;&#20104;&#23427;&#20204;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#20915;&#31574;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#26377;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#36235;&#21183;&#19987;&#27880;&#20110;&#23558;&#36825;&#20123;LLM&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#39046;&#22495;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;LLM&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#20195;&#29702;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#22823;&#22411;&#22810;&#27169;&#24577;&#20195;&#29702;&#65288;LMAs&#31616;&#31216;&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21457;&#23637;LMAs&#25152;&#28041;&#21450;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23558;&#24403;&#21069;&#30340;&#30740;&#31350;&#33539;&#30068;&#20998;&#20026;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#38598;&#25104;&#22810;&#20010;LMAs&#20197;&#22686;&#24378;&#38598;&#20307;&#25928;&#33021;&#30340;&#21512;&#20316;&#26694;&#26550;&#12290;&#36825;&#19968;&#39046;&#22495;&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#29616;&#26377;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#22810;&#26679;&#21270;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15116v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved superior performance in powering text-based AI agents, endowing them with decision-making and reasoning abilities akin to humans. Concurrently, there is an emerging research trend focused on extending these LLM-powered AI agents into the multimodal domain. This extension enables AI agents to interpret and respond to diverse multimodal user queries, thereby handling more intricate and nuanced tasks. In this paper, we conduct a systematic review of LLM-driven multimodal agents, which we refer to as large multimodal agents ( LMAs for short). First, we introduce the essential components involved in developing LMAs and categorize the current body of research into four distinct types. Subsequently, we review the collaborative frameworks integrating multiple LMAs , enhancing collective efficacy. One of the critical challenges in this field is the diverse evaluation methods used across existing studie
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#35268;&#27169;&#30417;&#27979;&#21644;&#20998;&#26512;GPT&#21830;&#24215;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#21270;&#24037;&#20855;&#26469;&#30740;&#31350;GPT&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#28431;&#27934;&#21644;&#25220;&#34989;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.15105</link><description>&lt;p&gt;
GPT&#24212;&#29992;&#30340;&#21021;&#25506;&#65306;&#26684;&#23616;&#19982;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
A First Look at GPT Apps: Landscape and Vulnerability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15105
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#35268;&#27169;&#30417;&#27979;&#21644;&#20998;&#26512;GPT&#21830;&#24215;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#21270;&#24037;&#20855;&#26469;&#30740;&#31350;GPT&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#28431;&#27934;&#21644;&#25220;&#34989;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#65292;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#24378;&#22823;&#30340;GPT&#36827;&#20837;&#24066;&#22330;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;LLM&#29983;&#24577;&#31995;&#32479;&#20173;&#28982;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;LLMs&#23545;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#24341;&#21457;&#20102;&#23545;&#23433;&#20840;&#24615;&#21644;&#25220;&#34989;&#30340;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;GPT&#21830;&#24215;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#30340;&#25506;&#32034;&#65292;&#26088;&#22312;&#30740;&#31350;GPT&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#28431;&#27934;&#21644;&#25220;&#34989;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25454;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#27425;&#22823;&#35268;&#27169;&#30417;&#27979;&#21644;&#20998;&#26512;&#65292;&#20998;&#21035;&#26159;&#19968;&#20010;&#38750;&#23448;&#26041;&#30340;GPTStore.AI&#21644;&#19968;&#20010;&#23448;&#26041;&#30340;OpenAI GPT Store&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;TriLevel GPT Reversing&#65288;T-GR&#65289;&#31574;&#30053;&#65292;&#29992;&#20110;&#25552;&#21462;GPT&#20869;&#37096;&#20449;&#24687;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23436;&#25104;&#36825;&#20004;&#39033;&#20219;&#21153;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#33258;&#21160;&#21270;&#24037;&#20855;&#65306;&#19968;&#20010;&#29992;&#20110;&#32593;&#32476;&#25235;&#21462;&#65292;&#21478;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#19982;GPT&#36827;&#34892;&#31243;&#24207;&#21270;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#29992;&#25143;&#21644;&#24320;&#21457;&#32773;&#23545;GPT&#20132;&#20114;&#21644;&#21019;&#24314;&#30340;&#24040;&#22823;&#28909;&#24773;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15105v1 Announce Type: cross  Abstract: With the advancement of Large Language Models (LLMs), increasingly sophisticated and powerful GPTs are entering the market. Despite their popularity, the LLM ecosystem still remains unexplored. Additionally, LLMs' susceptibility to attacks raises concerns over safety and plagiarism. Thus, in this work, we conduct a pioneering exploration of GPT stores, aiming to study vulnerabilities and plagiarism within GPT applications. To begin with, we conduct, to our knowledge, the first large-scale monitoring and analysis of two stores, an unofficial GPTStore.AI, and an official OpenAI GPT Store. Then, we propose a TriLevel GPT Reversing (T-GR) strategy for extracting GPT internals. To complete these two tasks efficiently, we develop two automated tools: one for web scraping and another designed for programmatically interacting with GPTs. Our findings reveal a significant enthusiasm among users and developers for GPT interaction and creation, as
&lt;/p&gt;</description></item><item><title>AttributionBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;&#33258;&#21160;&#24402;&#22240;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#21363;&#20351;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#21482;&#33021;&#36798;&#21040;80%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15089</link><description>&lt;p&gt;
AttributionBench&#65306;&#33258;&#21160;&#24402;&#22240;&#35780;&#20272;&#26377;&#22810;&#38590;&#65311;
&lt;/p&gt;
&lt;p&gt;
AttributionBench: How Hard is Automatic Attribution Evaluation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15089
&lt;/p&gt;
&lt;p&gt;
AttributionBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;&#33258;&#21160;&#24402;&#22240;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#21363;&#20351;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#21482;&#33021;&#36798;&#21040;80%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#36890;&#36807;&#25552;&#20379;&#24341;&#29992;&#35777;&#25454;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#31572;&#26696;&#30340;&#24402;&#22240;&#65292;&#21363;&#29983;&#25104;&#21709;&#24212;&#20013;&#30340;&#27599;&#20010;&#22768;&#26126;&#26159;&#21542;&#37117;&#24471;&#21040;&#20854;&#24341;&#29992;&#35777;&#25454;&#30340;&#20805;&#20998;&#25903;&#25345;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#20154;&#24037;&#35780;&#20272;&#30340;&#36825;&#31181;&#39564;&#35777;&#24378;&#35843;&#20102;&#23545;&#33258;&#21160;&#24402;&#22240;&#35780;&#20272;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#26631;&#20934;&#21270;&#22522;&#20934;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AttributionBench&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#24615;&#22522;&#20934;&#65292;&#30001;&#21508;&#31181;&#29616;&#26377;&#30340;&#24402;&#22240;&#25968;&#25454;&#38598;&#32534;&#21046;&#32780;&#25104;&#12290;&#25105;&#20204;&#22312;AttributionBench&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#25581;&#31034;&#20102;&#33258;&#21160;&#24402;&#22240;&#35780;&#20272;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21363;&#20351;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;LLM&#20063;&#26159;&#22914;&#27492;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#32463;&#36807;&#20248;&#21270;&#30340;GPT-3.5&#22312;&#20108;&#20803;&#20998;&#31867;&#20844;&#24335;&#19979;&#20063;&#21482;&#33021;&#36798;&#21040;&#32422;80%&#30340;&#23439;F1&#20998;&#25968;&#12290;&#26356; than 300 error c
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15089v1 Announce Type: cross  Abstract: Modern generative search engines enhance the reliability of large language model (LLM) responses by providing cited evidence. However, evaluating the answer's attribution, i.e., whether every claim within the generated responses is fully supported by its cited evidence, remains an open problem. This verification, traditionally dependent on costly human evaluation, underscores the urgent need for automatic attribution evaluation methods. To bridge the gap in the absence of standardized benchmarks for these methods, we present AttributionBench, a comprehensive benchmark compiled from various existing attribution datasets. Our extensive experiments on AttributionBench reveal the challenges of automatic attribution evaluation, even for state-of-the-art LLMs. Specifically, our findings show that even a fine-tuned GPT-3.5 only achieves around 80% macro-F1 under a binary classification formulation. A detailed analysis of more than 300 error c
&lt;/p&gt;</description></item><item><title>Hands-Free VR &#26159;&#19968;&#31181;&#26080;&#38656;&#25163;&#37096;&#25805;&#20316;&#30340;&#34394;&#25311;&#29616;&#23454;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#38899;&#21629;&#20196;&#23454;&#29616;&#65292;&#20855;&#26377;&#33521;&#35821;&#21475;&#38899;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#23545;&#25991;&#26412;&#30340;&#36716;&#25442;&#21644;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.15083</link><description>&lt;p&gt;
&#26080;&#38656;&#25163;&#37096;&#25805;&#20316;&#30340;&#34394;&#25311;&#29616;&#23454;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Hands-Free VR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15083
&lt;/p&gt;
&lt;p&gt;
Hands-Free VR &#26159;&#19968;&#31181;&#26080;&#38656;&#25163;&#37096;&#25805;&#20316;&#30340;&#34394;&#25311;&#29616;&#23454;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#38899;&#21629;&#20196;&#23454;&#29616;&#65292;&#20855;&#26377;&#33521;&#35821;&#21475;&#38899;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#23545;&#25991;&#26412;&#30340;&#36716;&#25442;&#21644;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Hands-Free VR&#30340;&#22522;&#20110;&#35821;&#38899;&#30340;&#33258;&#28982;&#35821;&#35328;&#34394;&#25311;&#29616;&#23454;&#30028;&#38754;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#35821;&#38899;&#21457;&#20986;&#21629;&#20196;&#65292;&#20854;&#35821;&#38899;&#38899;&#39057;&#25968;&#25454;&#32463;&#36807;&#19968;&#20010;&#38024;&#23545;&#21333;&#35789;&#38899;&#32032;&#30456;&#20284;&#24615;&#21644;&#33521;&#35821;&#21475;&#38899;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#24494;&#35843;&#30340;&#35821;&#38899;&#35782;&#21035;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#28982;&#21518;&#21033;&#29992;&#19968;&#20010;&#23545;&#33258;&#28982;&#35821;&#35328;&#22810;&#26679;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#25991;&#26412;&#26144;&#23556;&#20026;&#21487;&#25191;&#34892;&#30340;&#34394;&#25311;&#29616;&#23454;&#21629;&#20196;&#12290;Hands-Free VR&#22312;&#19968;&#20010;&#21463;&#25511;&#30340;&#34987;&#35797;&#30740;&#31350;&#20013;&#65288;N = 22&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35201;&#27714;&#21442;&#19982;&#32773;&#25214;&#21040;&#29305;&#23450;&#29289;&#20307;&#24182;&#20197;&#21508;&#31181;&#37197;&#32622;&#25918;&#32622;&#23427;&#20204;&#12290;&#22312;&#23545;&#29031;&#26465;&#20214;&#19979;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;&#20256;&#32479;&#30340;&#34394;&#25311;&#29616;&#23454;&#29992;&#25143;&#30028;&#38754;&#36890;&#36807;&#25163;&#25345;&#25511;&#21046;&#22120;&#25235;&#21462;&#12289;&#25644;&#36816;&#21644;&#23450;&#20301;&#29289;&#20307;&#12290;&#22312;&#23454;&#39564;&#26465;&#20214;&#19979;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;Hands-Free VR&#12290;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;Hands-Free VR&#23545;&#33521;&#35821;&#21475;&#38899;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22240;&#20026;&#22312;&#25105;&#20204;&#30340;20&#21517;&#21442;&#19982;&#32773;&#20013;&#65292;&#33521;&#35821;&#19981;&#26159;&#20182;&#20204;&#30340;&#39318;&#36873;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15083v1 Announce Type: cross  Abstract: The paper introduces Hands-Free VR, a voice-based natural-language interface for VR. The user gives a command using their voice, the speech audio data is converted to text using a speech-to-text deep learning model that is fine-tuned for robustness to word phonetic similarity and to spoken English accents, and the text is mapped to an executable VR command using a large language model that is robust to natural language diversity. Hands-Free VR was evaluated in a controlled within-subjects study (N = 22) that asked participants to find specific objects and to place them in various configurations. In the control condition participants used a conventional VR user interface to grab, carry, and position the objects using the handheld controllers. In the experimental condition participants used Hands-Free VR. The results confirm that: (1) Hands-Free VR is robust to spoken English accents, as for 20 of our participants English was not their f
&lt;/p&gt;</description></item><item><title>PEMT &#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#25193;&#23637;&#19987;&#23478;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#26435;&#37325;&#32452;&#21512;&#25429;&#33719;&#28304;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#65292;&#20174;&#32780;&#26377;&#25928;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21644;&#28304;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15082</link><description>&lt;p&gt;
PEMT: &#22810;&#20219;&#21153;&#30456;&#20851;&#24615;&#24341;&#23548;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15082
&lt;/p&gt;
&lt;p&gt;
PEMT &#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#25193;&#23637;&#19987;&#23478;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#26435;&#37325;&#32452;&#21512;&#25429;&#33719;&#28304;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#65292;&#20174;&#32780;&#26377;&#25928;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21644;&#28304;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#20316;&#20026;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#30340;&#26041;&#27861;&#24050;&#32463;&#23835;&#36215;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#20174;&#19968;&#20010;&#25110;&#22810;&#20010;&#20219;&#21153;&#36716;&#31227;&#30693;&#35782;&#21040;&#19979;&#28216;&#30446;&#26631;&#20219;&#21153;&#20197;&#23454;&#29616;&#24615;&#33021;&#25552;&#21319;&#20135;&#29983;&#20102;&#36234;&#26469;&#36234;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#22312;&#21508;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#36866;&#37197;&#22120;&#65292;&#35201;&#20040;&#20174;&#28304;&#20219;&#21153;&#20013;&#25552;&#21462;&#20849;&#20139;&#30693;&#35782;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21644;&#28304;&#20219;&#21153;&#19982;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PEMT&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#30340;&#21019;&#26032;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#12290;PEMT&#23558;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#26694;&#26550;&#25193;&#23637;&#20026;&#28304;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#30340;&#21152;&#26435;&#32452;&#21512;&#20197;&#25429;&#33719;&#21487;&#36716;&#31227;&#30693;&#35782;&#12290;&#36825;&#20123;&#26435;&#37325;&#30001;&#19968;&#20010;&#38376;&#25511;&#21333;&#20803;&#30830;&#23450;&#65292;&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#27979;&#37327;&#30446;&#26631;&#20219;&#21153;&#21644;&#27599;&#20010;&#28304;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15082v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as an effective method for adapting pre-trained language models to various tasks efficiently. Recently, there has been a growing interest in transferring knowledge from one or multiple tasks to the downstream target task to achieve performance improvements. However, current approaches typically either train adapters on individual tasks or distill shared knowledge from source tasks, failing to fully exploit task-specific knowledge and the correlation between source and target tasks. To overcome these limitations, we propose PEMT, a novel parameter-efficient fine-tuning framework based on multi-task transfer learning. PEMT extends the mixture-of-experts (MoE) framework to capture the transferable knowledge as a weighted combination of adapters trained on source tasks. These weights are determined by a gated unit, measuring the correlation between the target and each source task using task 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#21442;&#25968;&#39640;&#25928;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#32423;&#26631;&#31614;&#32454;&#21270;&#26041;&#27861;&#28145;&#24230;&#25972;&#21512;&#23618;&#32423;&#25351;&#23548;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23618;&#27425;&#38544;&#24335;&#35805;&#35821;&#20851;&#31995;&#35782;&#21035;&#20013;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.15080</link><description>&lt;p&gt;
&#23558;&#23618;&#32423;&#25351;&#23548;&#34701;&#20837;&#25552;&#31034;&#35843;&#25972;&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#23618;&#27425;&#38544;&#24335;&#35805;&#35821;&#20851;&#31995;&#35782;&#21035;&#30340;&#21442;&#25968;&#39640;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Infusing Hierarchical Guidance into Prompt Tuning: A Parameter-Efficient Framework for Multi-level Implicit Discourse Relation Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15080
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#21442;&#25968;&#39640;&#25928;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#32423;&#26631;&#31614;&#32454;&#21270;&#26041;&#27861;&#28145;&#24230;&#25972;&#21512;&#23618;&#32423;&#25351;&#23548;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23618;&#27425;&#38544;&#24335;&#35805;&#35821;&#20851;&#31995;&#35782;&#21035;&#20013;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#27425;&#38544;&#24335;&#35805;&#35821;&#20851;&#31995;&#35782;&#21035;(MIDRR)&#26088;&#22312;&#35782;&#21035;&#35770;&#28857;&#20043;&#38388;&#30340;&#23618;&#27425;&#35805;&#35821;&#20851;&#31995;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#24494;&#35843;PLM&#26469;&#23454;&#29616;&#25512;&#24191;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#31232;&#32570;&#21644;&#20219;&#21153;&#24046;&#36317;&#65292;&#39044;&#35757;&#32451;&#29305;&#24449;&#31354;&#38388;&#26080;&#27861;&#20934;&#30830;&#35843;&#25972;&#21040;&#29305;&#23450;&#20219;&#21153;&#31354;&#38388;&#65292;&#29978;&#33267;&#21152;&#21095;&#20102;&#22522;&#30784;&#31354;&#38388;&#30340;&#23849;&#28291;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;MIDRR&#30340;&#23618;&#27425;&#35821;&#20041;&#29702;&#35299;&#20351;&#24471;&#36716;&#25442;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#21442;&#25968;&#39640;&#25928;&#22810;&#23618;&#27425;IDRR&#65288;PEMI&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#23558;&#36755;&#20837;&#30340;&#35770;&#28857;&#39537;&#21160;&#21040;&#21305;&#37197;&#39044;&#35757;&#32451;&#31354;&#38388;&#24182;&#21033;&#29992;&#23569;&#37327;&#21442;&#25968;&#23454;&#29616;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#32423;&#26631;&#31614;&#32454;&#21270;&#65288;HLR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35753;&#25552;&#31034;&#34920;&#36798;&#22120;&#28145;&#24230;&#25972;&#21512;&#23618;&#32423;&#25351;&#23548;&#21040;&#25552;&#31034;&#35843;&#25972;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15080v1 Announce Type: new  Abstract: Multi-level implicit discourse relation recognition (MIDRR) aims at identifying hierarchical discourse relations among arguments. Previous methods achieve the promotion through fine-tuning PLMs. However, due to the data scarcity and the task gap, the pre-trained feature space cannot be accurately tuned to the task-specific space, which even aggravates the collapse of the vanilla space. Besides, the comprehension of hierarchical semantics for MIDRR makes the conversion much harder. In this paper, we propose a prompt-based Parameter-Efficient Multi-level IDRR (PEMI) framework to solve the above problems. First, we leverage parameter-efficient prompt tuning to drive the inputted arguments to match the pre-trained space and realize the approximation with few parameters. Furthermore, we propose a hierarchical label refining (HLR) method for the prompt verbalizer to deeply integrate hierarchical guidance into the prompt tuning. Finally, our mo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#22238;&#31572;&#26410;&#30693;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#25298;&#32477;&#22238;&#31572;&#24182;&#35299;&#37322;&#26410;&#30693;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#30340;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2402.15062</link><description>&lt;p&gt;
&#21035;&#32781;&#33457;&#25307;&#65281;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#35843;&#25972;&#20197;&#22238;&#31572;&#26410;&#30693;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Gotcha! Don't trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15062
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#22238;&#31572;&#26410;&#30693;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#25298;&#32477;&#22238;&#31572;&#24182;&#35299;&#37322;&#26410;&#30693;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20986;&#33394;&#30340;&#22238;&#31572;&#38382;&#39064;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#38382;&#39064;&#27809;&#26377;&#26126;&#30830;&#31572;&#26696;&#26102;&#24448;&#24448;&#34920;&#29616;&#20986;&#30456;&#24403;&#31243;&#24230;&#30340;&#33258;&#20449;&#36807;&#24230;&#12290;&#20026;&#20102;&#36991;&#20813;&#21521;&#36825;&#20123;&#26410;&#30693;&#38382;&#39064;&#25552;&#20379;&#34394;&#26500;&#31572;&#26696;&#65292;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#25506;&#35752;&#25298;&#32477;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;&#33258;&#25105;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#26412;&#36523;&#26469;&#22686;&#24378;&#20854;&#23545;&#19981;&#21516;&#31867;&#22411;&#26410;&#30693;&#38382;&#39064;&#30340;&#22238;&#24212;&#33021;&#21147;&#65292;&#19981;&#20165;&#33021;&#22815;&#25298;&#32477;&#22238;&#31572;&#65292;&#36824;&#33021;&#22815;&#35299;&#37322;&#26410;&#30693;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#30340;&#21407;&#22240;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Self-Align&#26041;&#27861;&#39318;&#20808;&#37319;&#29992;&#20004;&#38454;&#27573;&#31867;&#24863;&#30693;&#33258;&#25105;&#22686;&#24378;&#26041;&#27861;&#29983;&#25104;&#22823;&#37327;&#26410;&#30693;&#38382;&#39064;-&#22238;&#24212;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#24046;&#24322;&#39537;&#21160;&#30340;&#33258;&#25105;&#25972;&#29702;&#65292;&#36873;&#25321;&#21512;&#26684;&#25968;&#25454;&#23545;LLM&#26412;&#36523;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#35843;&#25972;&#23545;&#26410;&#30693;&#38382;&#39064;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15062v1 Announce Type: new  Abstract: Despite the remarkable abilities of Large Language Models (LLMs) to answer questions, they often display a considerable level of overconfidence even when the question does not have a definitive answer. To avoid providing hallucinated answers to these unknown questions, existing studies typically investigate approaches to refusing to answer these questions. In this work, we propose a novel and scalable self-alignment method to utilize the LLM itself to enhance its response-ability to different types of unknown questions, being capable of not only refusing to answer but also providing explanation to the unanswerability of unknown questions. Specifically, the Self-Align method first employ a two-stage class-aware self-augmentation approach to generate a large amount of unknown question-response data. Then we conduct disparity-driven self-curation to select qualified data for fine-tuning the LLM itself for aligning the responses to unknown q
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LlamaIT&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15061</link><description>&lt;p&gt;
&#38024;&#23545;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Models for Domain-specific Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15061
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LlamaIT&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#39046;&#22495;&#29305;&#23450;MT&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#24403;&#21069;&#22522;&#20110;LLMs&#30340;MT&#31995;&#32479;&#20173;&#28982;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LlamaIT&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#39640;&#25928;&#22320;&#20026;&#39046;&#22495;&#29305;&#23450;MT&#20219;&#21153;&#24494;&#35843;&#36890;&#29992;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15061v1 Announce Type: new  Abstract: Large language models (LLMs) have made significant progress in machine translation (MT). However, their potential in domain-specific MT remains under-explored. Current LLM-based MT systems still face several challenges. First, for LLMs with in-context learning, their effectiveness is highly sensitive to input translation examples, and processing them can increase inference costs. They often require extra post-processing due to over-generation. Second, LLMs with fine-tuning on domain-specific data often require high training costs for domain adaptation, and may weaken the zero-shot MT capabilities of LLMs due to over-specialization. The aforementioned methods can struggle to translate rare words in domain transfer scenarios. To address these challenges, this paper proposes a prompt-oriented fine-tuning method, denoted as LlamaIT, to effectively and efficiently fine-tune a general-purpose LLM for domain-specific MT tasks. First, we constru
&lt;/p&gt;</description></item><item><title>ColBERT-XM&#27169;&#22411;&#36890;&#36807;&#20174;&#21333;&#19968;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#38646;-shot&#36716;&#31227;&#21040;&#24191;&#27867;&#30340;&#35821;&#35328;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#29305;&#23450;&#35821;&#35328;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.15059</link><description>&lt;p&gt;
ColBERT-XM&#65306;&#19968;&#31181;&#29992;&#20110;&#38646;-shot &#22810;&#35821;&#20449;&#24687;&#26816;&#32034;&#30340;&#27169;&#22359;&#21270;&#22810;&#21521;&#37327;&#34920;&#31034;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ColBERT-XM: A Modular Multi-Vector Representation Model for Zero-Shot Multilingual Information Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15059
&lt;/p&gt;
&lt;p&gt;
ColBERT-XM&#27169;&#22411;&#36890;&#36807;&#20174;&#21333;&#19968;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#38646;-shot&#36716;&#31227;&#21040;&#24191;&#27867;&#30340;&#35821;&#35328;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#29305;&#23450;&#35821;&#35328;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#26816;&#32034;&#22120;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#31561;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#28041;&#21450;&#20854;&#20182;&#35821;&#35328;&#30340;&#26816;&#32034;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#24403;&#21069;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#33021;&#22815;&#36827;&#34892;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#35268;&#36991;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#32570;&#20047;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#22823;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#22312;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#34920;&#31034;&#26497;&#23569;&#30340;&#35821;&#35328;&#20013;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20043;&#21518;&#38590;&#20197;&#34701;&#21512;&#26032;&#35821;&#35328;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#65292;&#23427;&#20174;&#21333;&#19968;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#20016;&#23500;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#26377;&#25928;&#22320;&#38646;-shot &#36716;&#31227;&#21040;&#24191;&#27867;&#30340;&#35821;&#35328;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#29305;&#23450;&#35821;&#35328;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411; ColBERT-XM &#22312;&#29616;&#26377;&#27169;&#22411;&#19978;&#23637;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15059v1 Announce Type: new  Abstract: State-of-the-art neural retrievers predominantly focus on high-resource languages like English, which impedes their adoption in retrieval scenarios involving other languages. Current approaches circumvent the lack of high-quality labeled data in non-English languages by leveraging multilingual pretrained language models capable of cross-lingual transfer. However, these models require substantial task-specific fine-tuning across multiple languages, often perform poorly in languages with minimal representation in the pretraining corpus, and struggle to incorporate new languages after the pretraining phase. In this work, we present a novel modular dense retrieval model that learns from the rich data of a single high-resource language and effectively zero-shot transfers to a wide array of languages, thereby eliminating the need for language-specific labeled data. Our model, ColBERT-XM, demonstrates competitive performance against existing st
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23545;&#35805;&#24335;&#32593;&#32476;&#23548;&#33322;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;MT-Mind2Web&#30340;&#29305;&#27530;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Self-MAP&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#25351;&#20196;&#36319;&#36394;&#20013;&#30340;&#38271;&#24230;&#21644;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15057</link><description>&lt;p&gt;
&#20851;&#20110;&#38754;&#21521;&#23545;&#35805;&#24335;&#32593;&#32476;&#20195;&#29702;&#30340;&#22810;&#36718;&#25351;&#20196;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
On the Multi-turn Instruction Following for Conversational Web Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15057
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23545;&#35805;&#24335;&#32593;&#32476;&#23548;&#33322;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;MT-Mind2Web&#30340;&#29305;&#27530;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Self-MAP&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#25351;&#20196;&#36319;&#36394;&#20013;&#30340;&#38271;&#24230;&#21644;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#32593;&#32476;&#20195;&#29702;&#22312;&#35268;&#21010;&#21644;&#25191;&#34892;&#22797;&#26434;&#22522;&#20110;&#32593;&#32476;&#30340;&#22810;&#27493;&#20132;&#20114;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#23436;&#25104;&#20102;&#21508;&#31181;&#32593;&#32476;&#23548;&#33322;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20197;LLM&#20026;&#21160;&#21147;&#30340;&#20195;&#29702;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#26377;&#25928;&#19982;&#39034;&#24207;&#29992;&#25143;&#25351;&#20196;&#36827;&#34892;&#20132;&#20114;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#23545;&#35805;&#24335;&#32593;&#32476;&#23548;&#33322;&#30340;&#26032;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#38656;&#35201;&#19982;&#29992;&#25143;&#21644;&#29615;&#22659;&#36827;&#34892;&#36328;&#22810;&#36718;&#30340;&#22797;&#26434;&#20132;&#20114;&#65292;&#25903;&#25345;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;&#22810;&#36718;Mind2Web&#65288;MT-Mind2Web&#65289;&#30340;&#29305;&#21035;&#24320;&#21457;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#30340;&#26377;&#38480;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#23545;&#35805;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#21453;&#26144;&#35760;&#24518;&#22686;&#24378;&#35268;&#21010;&#65288;Self-MAP&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#35760;&#24518;&#21033;&#29992;&#21644;&#33258;&#25105;&#21453;&#24605;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15057v1 Announce Type: cross  Abstract: Web agents powered by Large Language Models (LLMs) have demonstrated remarkable abilities in planning and executing multi-step interactions within complex web-based environments, fulfilling a wide range of web navigation tasks. Despite these advancements, the potential for LLM-powered agents to effectively engage with sequential user instructions in real-world scenarios has not been fully explored. In this work, we introduce a new task of Conversational Web Navigation, which necessitates sophisticated interactions that span multiple turns with both the users and the environment, supported by a specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To tackle the limited context length of LLMs and the context-dependency issue of the conversational tasks, we further propose a novel framework, named self-reflective memory-augmented planning (Self-MAP), which employs memory utilization and self-reflection techniques. Extensive
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;Transformer&#20013;&#27880;&#24847;&#21147;&#22836;&#21644;MLP&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#29305;&#23450;&#19978;&#19979;&#25991;&#19979;&#28608;&#27963;&#29305;&#23450;token&#39044;&#27979;&#30340;&#26426;&#21046;&#65292;&#20174;&#32780;&#38416;&#26126;&#22312;LLMs&#20013;&#27880;&#24847;&#21147;&#22914;&#20309;&#20419;&#25104;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#19987;&#38376;&#21270;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.15055</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#35299;&#37322;&#19978;&#19979;&#25991;&#26597;&#25214;&#65306;&#25506;&#31350;&#27880;&#24847;&#21147;-MLP&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15055
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;Transformer&#20013;&#27880;&#24847;&#21147;&#22836;&#21644;MLP&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#29305;&#23450;&#19978;&#19979;&#25991;&#19979;&#28608;&#27963;&#29305;&#23450;token&#39044;&#27979;&#30340;&#26426;&#21046;&#65292;&#20174;&#32780;&#38416;&#26126;&#22312;LLMs&#20013;&#27880;&#24847;&#21147;&#22914;&#20309;&#20419;&#25104;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#19987;&#38376;&#21270;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#22836;&#21644;Multilayer Perceptron&#20013;&#19987;&#38376;&#39044;&#27979;&#29305;&#23450;token&#30340;"next-token"&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#20419;&#20351;&#20687;GPT-4&#36825;&#26679;&#30340;LLM&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#20869;&#37096;&#65292;&#25105;&#20204;&#21487;&#20197;&#38416;&#26126;&#28608;&#27963;&#26576;&#20123;next-token&#31070;&#32463;&#20803;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#30830;&#23450;&#20102;&#35782;&#21035;&#19982;&#39044;&#27979;&#29305;&#23450;token&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#30340;attention heads&#65292;&#36890;&#36807;&#27531;&#24046;&#36830;&#25509;&#28608;&#27963;&#30456;&#20851;&#32852;&#30340;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#22312;&#36739;&#26089;&#30340;&#23618;&#20013;&#22987;&#32456;&#28608;&#27963;&#30456;&#21516;next-token&#31070;&#32463;&#20803;&#30340;attention heads&#12290;&#25506;&#32034;&#36825;&#20123;&#19981;&#21516;&#30340;&#28608;&#27963;&#27169;&#24335;&#25581;&#31034;&#20102;&#20026;&#19981;&#21516;&#35821;&#35328;&#19978;&#19979;&#25991;&#19987;&#38376;&#21270;&#30340;&#22836;&#19982;&#29983;&#25104;&#26576;&#20123;tokens&#30456;&#20851;&#32852;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#31070;&#32463;&#35299;&#37322;&#21644;&#25506;&#27979;&#23396;&#31435;&#30340;&#32452;&#20214;&#65292;&#20197;&#38416;&#26126;&#27880;&#24847;&#21147;&#22914;&#20309;&#20351;LLMs&#20013;&#30340;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#19987;&#38376;&#22788;&#29702;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15055v1 Announce Type: cross  Abstract: In this paper, we investigate the interplay between attention heads and specialized "next-token" neurons in the Multilayer Perceptron that predict specific tokens. By prompting an LLM like GPT-4 to explain these model internals, we can elucidate attention mechanisms that activate certain next-token neurons. Our analysis identifies attention heads that recognize contexts relevant to predicting a particular token, activating the associated neuron through the residual connection. We focus specifically on heads in earlier layers consistently activating the same next-token neuron across similar prompts. Exploring these differential activation patterns reveals that heads that specialize for distinct linguistic contexts are tied to generating certain tokens. Overall, our method combines neural explanations and probing isolated components to illuminate how attention enables context-dependent, specialized processing in LLMs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ToMBench&#26694;&#26550;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#24515;&#28789;&#29702;&#35770;&#24615;&#33021;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#34920;&#29616;&#36229;&#36807;10%&#12290;</title><link>https://arxiv.org/abs/2402.15052</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22522;&#20934;&#27979;&#35797;&#24515;&#28789;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
ToMBench: Benchmarking Theory of Mind in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15052
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ToMBench&#26694;&#26550;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#24515;&#28789;&#29702;&#35770;&#24615;&#33021;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#34920;&#29616;&#36229;&#36807;10%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#28789;&#29702;&#35770;&#65288;ToM&#65289;&#26159;&#25351;&#24863;&#30693;&#21644;&#24402;&#22240;&#33258;&#24049;&#20197;&#21450;&#20182;&#20154;&#30340;&#24515;&#29702;&#29366;&#24577;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#21457;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#34920;&#29616;&#20986;&#19968;&#31181;&#24418;&#24335;&#30340;&#24515;&#28789;&#29702;&#35770;&#30340;&#20105;&#35770;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24515;&#28789;&#29702;&#35770;&#35780;&#20272;&#21463;&#21040;&#35832;&#22914;&#21463;&#38480;&#33539;&#22260;&#12289;&#20027;&#35266;&#21028;&#26029;&#21644;&#24847;&#22806;&#27745;&#26579;&#31561;&#25361;&#25112;&#30340;&#21046;&#32422;&#65292;&#23548;&#33268;&#35780;&#20272;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ToMBench&#65292;&#20855;&#26377;&#19977;&#20010;&#20851;&#38190;&#29305;&#24449;&#65306;&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#28085;&#30422;&#31038;&#20250;&#35748;&#30693;&#20013;&#30340;8&#39033;&#20219;&#21153;&#21644;31&#39033;&#33021;&#21147;&#65292;&#22810;&#39033;&#36873;&#25321;&#39064;&#26684;&#24335;&#20197;&#25903;&#25345;&#33258;&#21160;&#21270;&#21644;&#26080;&#20559;&#35265;&#30340;&#35780;&#20272;&#65292;&#20197;&#21450;&#22522;&#20110;&#21452;&#35821;&#28165;&#21333;&#30340;&#20174;&#22836;&#26500;&#24314;&#65292;&#20005;&#26684;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#22522;&#20110;ToMBench&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;10&#20010;&#27969;&#34892;LLMs&#22312;&#20219;&#21153;&#21644;&#33021;&#21147;&#26041;&#38754;&#30340;&#24515;&#28789;&#29702;&#35770;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#20687;GPT-4&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#30340;LLMs&#20063;&#27604;&#20154;&#31867;&#34920;&#29616;&#33853;&#21518;&#36229;&#36807;10&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15052v1 Announce Type: cross  Abstract: Theory of Mind (ToM) is the cognitive capability to perceive and ascribe mental states to oneself and others. Recent research has sparked a debate over whether large language models (LLMs) exhibit a form of ToM. However, existing ToM evaluations are hindered by challenges such as constrained scope, subjective judgment, and unintended contamination, yielding inadequate assessments. To address this gap, we introduce ToMBench with three key characteristics: a systematic evaluation framework encompassing 8 tasks and 31 abilities in social cognition, a multiple-choice question format to support automated and unbiased evaluation, and a build-from-scratch bilingual inventory to strictly avoid data leakage. Based on ToMBench, we conduct extensive experiments to evaluate the ToM performance of 10 popular LLMs across tasks and abilities. We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicati
&lt;/p&gt;</description></item><item><title>ChatEA&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#23454;&#20307;&#23545;&#40784;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;KG-code&#32763;&#35793;&#27169;&#22359;&#21644;&#20004;&#38454;&#27573;EA&#31574;&#30053;&#26469;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15048</link><description>&lt;p&gt;
&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#20307;&#23545;&#40784;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Power of Large Language Models for Entity Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15048
&lt;/p&gt;
&lt;p&gt;
ChatEA&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#23454;&#20307;&#23545;&#40784;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;KG-code&#32763;&#35793;&#27169;&#22359;&#21644;&#20004;&#38454;&#27573;EA&#31574;&#30053;&#26469;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#23545;&#20110;&#25972;&#21512;&#19981;&#21516;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20256;&#32479;&#30340;EA&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#27604;&#36739;&#23454;&#20307;&#23884;&#20837;&#65292;&#20294;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;&#36755;&#20837;KG&#25968;&#25454;&#21644;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#30340;&#33021;&#21147;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#32422;&#26463;&#12290;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatEA&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34701;&#20837;&#20197;&#25913;&#21892;EA&#12290;&#20026;&#20102;&#35299;&#20915;&#26377;&#38480;&#30340;&#36755;&#20837;KG&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;ChatEA&#24341;&#20837;&#20102;&#19968;&#20010;KG-code&#32763;&#35793;&#27169;&#22359;&#65292;&#23558;KG&#32467;&#26500;&#32763;&#35793;&#25104;LLMs&#21487;&#29702;&#35299;&#30340;&#26684;&#24335;&#65292;&#20174;&#32780;&#20351;LLMs&#33021;&#22815;&#21033;&#29992;&#20854;&#24191;&#27867;&#30340;&#32972;&#26223;&#30693;&#35782;&#25552;&#39640;EA&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#23545;&#23454;&#20307;&#23884;&#20837;&#27604;&#36739;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;ChatEA&#23454;&#29616;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;EA&#31574;&#30053;&#65292;&#21033;&#29992;LLMs&#22312;&#23545;&#35805;&#26684;&#24335;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15048v1 Announce Type: cross  Abstract: Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG) data, playing a crucial role in data-driven AI applications. Traditional EA methods primarily rely on comparing entity embeddings, but their effectiveness is constrained by the limited input KG data and the capabilities of the representation learning techniques. Against this backdrop, we introduce ChatEA, an innovative framework that incorporates large language models (LLMs) to improve EA. To address the constraints of limited input KG data, ChatEA introduces a KG-code translation module that translates KG structures into a format understandable by LLMs, thereby allowing LLMs to utilize their extensive background knowledge to improve EA accuracy. To overcome the over-reliance on entity embedding comparisons, ChatEA implements a two-stage EA strategy that capitalizes on LLMs' capability for multi-step reasoning in a dialogue format, thereby enhancing accuracy wh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#38889;&#25991;&#26041;&#38754;&#32423;&#21035;&#24773;&#24863;&#20998;&#31867;&#30340;CARBD-Ko&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#20102;&#21452;&#26631;&#26497;&#24615;&#20197;&#21306;&#20998;&#29305;&#23450;&#26041;&#38754;&#21644;&#26041;&#38754;&#19981;&#21487;&#30693;&#24773;&#24863;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#37319;&#29992;Siamese&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#26631;&#26041;&#38754;&#26497;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15046</link><description>&lt;p&gt;
CARBD-Ko: &#19968;&#20010;&#29992;&#20110;&#38889;&#25991;&#26041;&#38754;&#32423;&#21035;&#24773;&#24863;&#20998;&#31867;&#30340;&#24773;&#22659;&#27880;&#37322;&#35780;&#35770;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CARBD-Ko: A Contextually Annotated Review Benchmark Dataset for Aspect-Level Sentiment Classification in Korean
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15046
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#38889;&#25991;&#26041;&#38754;&#32423;&#21035;&#24773;&#24863;&#20998;&#31867;&#30340;CARBD-Ko&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#20102;&#21452;&#26631;&#26497;&#24615;&#20197;&#21306;&#20998;&#29305;&#23450;&#26041;&#38754;&#21644;&#26041;&#38754;&#19981;&#21487;&#30693;&#24773;&#24863;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#37319;&#29992;Siamese&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#26631;&#26041;&#38754;&#26497;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20013;&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#65288;ABSC&#65289;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#19978;&#19979;&#25991;&#21270;&#21644;&#24187;&#35273;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CARBD-Ko&#65288;&#29992;&#20110;&#38889;&#25991;&#26041;&#38754;&#32423;&#21035;&#24773;&#24863;&#20998;&#31867;&#30340;&#24773;&#22659;&#27880;&#37322;&#35780;&#35770;&#22522;&#20934;&#25968;&#25454;&#38598;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#23427;&#32467;&#21512;&#20102;&#26041;&#38754;&#21644;&#21452;&#26631;&#26497;&#24615;&#65292;&#20197;&#21306;&#20998;&#29305;&#23450;&#26041;&#38754;&#21644;&#26041;&#38754;&#19981;&#21487;&#30693;&#24773;&#24863;&#20998;&#31867;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#29992;&#29305;&#23450;&#26041;&#38754;&#12289;&#26041;&#38754;&#26497;&#24615;&#12289;&#26041;&#38754;&#19981;&#21487;&#30693;&#26497;&#24615;&#21644;&#26041;&#38754;&#24378;&#24230;&#36827;&#34892;&#27880;&#37322;&#30340;&#21477;&#23376;&#12290;&#20026;&#20102;&#35299;&#20915;&#21452;&#26631;&#26041;&#38754;&#26497;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#23402;&#29983;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#20986;&#20102;&#20934;&#30830;&#39044;&#27979;&#21452;&#26497;&#24615;&#30340;&#22266;&#26377;&#22256;&#38590;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#24773;&#22659;&#21270;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15046v1 Announce Type: new  Abstract: This paper explores the challenges posed by aspect-based sentiment classification (ABSC) within pretrained language models (PLMs), with a particular focus on contextualization and hallucination issues. In order to tackle these challenges, we introduce CARBD-Ko (a Contextually Annotated Review Benchmark Dataset for Aspect-Based Sentiment Classification in Korean), a benchmark dataset that incorporates aspects and dual-tagged polarities to distinguish between aspect-specific and aspect-agnostic sentiment classification. The dataset consists of sentences annotated with specific aspects, aspect polarity, aspect-agnostic polarity, and the intensity of aspects. To address the issue of dual-tagged aspect polarities, we propose a novel approach employing a Siamese Network. Our experimental findings highlight the inherent difficulties in accurately predicting dual-polarities and underscore the significance of contextualized sentiment analysis mod
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;KIEval&#65292;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;LLM-powered "interactor"&#35282;&#33394;&#23454;&#29616;&#21160;&#24577;&#30340;&#25239;&#27745;&#26579;&#35780;&#20272;</title><link>https://arxiv.org/abs/2402.15043</link><description>&lt;p&gt;
KIEval&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;KIEval&#65292;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;LLM-powered "interactor"&#35282;&#33394;&#23454;&#29616;&#21160;&#24577;&#30340;&#25239;&#27745;&#26579;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#23545;&#20854;&#26377;&#25928;&#24615;&#30340;&#35780;&#20272;&#34987;&#22840;&#22823;&#12290;&#29616;&#26377;&#30340;&#31574;&#30053;&#26088;&#22312;&#26816;&#27979;&#21463;&#27745;&#26579;&#30340;&#25991;&#26412;&#65292;&#20294;&#20391;&#37325;&#20110;&#37327;&#21270;&#27745;&#26579;&#31243;&#24230;&#32780;&#38750;&#20934;&#30830;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;KIEval&#65292;&#36825;&#26159;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;LLM&#39537;&#21160;&#30340;&#8220;&#20132;&#20114;&#32773;&#8221;&#35282;&#33394;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#25239;&#27745;&#26579;&#35780;&#20272;&#12290;&#20174;&#28041;&#21450;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#30340;&#24120;&#35268;LLM&#22522;&#20934;&#38382;&#39064;&#24320;&#22987;&#65292;KIEval&#21033;&#29992;&#21160;&#24577;&#29983;&#25104;&#30340;&#12289;&#22810;&#36718;&#12289;&#20197;&#30693;&#35782;&#20026;&#37325;&#28857;&#30340;&#23545;&#35805;&#65292;&#20197;&#30830;&#23450;&#27169;&#22411;&#30340;&#21709;&#24212;&#26159;&#21542;&#20165;&#26159;&#22522;&#20934;&#31572;&#26696;&#30340;&#22238;&#24518;&#65292;&#36824;&#26159;&#34920;&#26126;&#20102;&#28145;&#20837;&#29702;&#35299;&#24182;&#33021;&#22312;&#26356;&#22797;&#26434;&#30340;&#23545;&#35805;&#20013;&#24212;&#29992;&#30693;&#35782;&#12290;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#19971;&#20010;&#39046;&#20808;&#30340;LLM&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;KI
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15043v1 Announce Type: cross  Abstract: Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered "interactor" role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KI
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#27169;&#22411;&#32534;&#30721;&#32452;&#21512;&#24615;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#22312;&#32452;&#21512;&#24615;&#22522;&#20934;&#19978;&#21462;&#24471;&#36229;&#36807;10% &#30340;&#32477;&#23545;&#25913;&#36827;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#22312;&#26631;&#20934;&#23545;&#35937;&#35782;&#21035;&#21644;&#26816;&#32034;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15021</link><description>&lt;p&gt;
CLoVe: &#22312;&#23545;&#27604;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#32452;&#21512;&#24615;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#27169;&#22411;&#32534;&#30721;&#32452;&#21512;&#24615;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#22312;&#32452;&#21512;&#24615;&#22522;&#20934;&#19978;&#21462;&#24471;&#36229;&#36807;10% &#30340;&#32477;&#23545;&#25913;&#36827;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#22312;&#26631;&#20934;&#23545;&#35937;&#35782;&#21035;&#21644;&#26816;&#32034;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#30340;&#34920;&#29616;&#26174;&#33879;&#25552;&#21319;&#12290;&#22522;&#30784;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22914;CLIP&#24050;&#22312;&#22810;&#20010;&#35774;&#32622;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20960;&#20010;&#20219;&#21153;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#25797;&#38271;&#20110;&#23545;&#35937;&#20013;&#24515;&#35782;&#21035;&#65292;&#20294;&#23398;&#20064;&#30340;&#25991;&#26412;&#34920;&#31034;&#20284;&#20046;&#23545;&#35789;&#24207;&#19981;&#21464;&#65292;&#26410;&#33021;&#20197;&#26032;&#39062;&#26041;&#24335;&#32452;&#25104;&#24050;&#30693;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#20219;&#20309;VLM&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#21333;&#27969;&#27169;&#22411;&#22914;GPT-4V&#65292;&#25104;&#21151;&#35782;&#21035;&#32452;&#21512;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#29616;&#26377;&#27169;&#22411;&#32534;&#30721;&#32452;&#21512;&#24615;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#22312;&#32452;&#21512;&#24615;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#36229;&#36807;10% &#30340;&#32477;&#23545;&#25913;&#36827;&#65292;&#21516;&#26102;&#22312;&#26631;&#20934;&#23545;&#35937;&#35782;&#21035;&#21644;&#26816;&#32034;&#22522;&#20934;&#19978;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/netflix/&#22788;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15021v1 Announce Type: cross  Abstract: Recent years have witnessed a significant increase in the performance of Vision and Language tasks. Foundational Vision-Language Models (VLMs), such as CLIP, have been leveraged in multiple settings and demonstrated remarkable performance across several tasks. Such models excel at object-centric recognition yet learn text representations that seem invariant to word order, failing to compose known concepts in novel ways. However, no evidence exists that any VLM, including large-scale single-stream models such as GPT-4V, identifies compositions successfully. In this paper, we introduce a framework to significantly improve the ability of existing models to encode compositional language, with over 10% absolute improvement on compositionality benchmarks, while maintaining or improving the performance on standard object-recognition and retrieval benchmarks. Our code and pre-trained models are publicly available at https://github.com/netflix/
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#26463;&#25628;&#32034;&#30340;&#27010;&#29575;&#20581;&#22766;&#26041;&#27861;&#65292;&#34920;&#26126;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15020</link><description>&lt;p&gt;
&#20855;&#26377;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#20581;&#22766;&#26463;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Probabilistically-sound beam search with masked language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15020
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#26463;&#25628;&#32034;&#30340;&#27010;&#29575;&#20581;&#22766;&#26041;&#27861;&#65292;&#34920;&#26126;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#30340;&#26463;&#25628;&#32034;&#23384;&#22312;&#25361;&#25112;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#24207;&#21015;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#19981;&#20687;&#33258;&#22238;&#24402;&#27169;&#22411;&#37027;&#26679;readily available&#12290;&#28982;&#32780;&#65292;&#20272;&#31639;&#36825;&#26679;&#30340;&#20998;&#24067;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#24212;&#29992;&#65292;&#21253;&#25324;&#34507;&#30333;&#24037;&#31243;&#21644;&#21476;&#20195;&#25991;&#26412;&#24674;&#22797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#27010;&#29575;&#20581;&#22766;&#24615;&#30340;&#20351;&#29992;MLMs&#36827;&#34892;&#26463;&#25628;&#32034;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#20351;&#29992;&#26631;&#20934;&#26463;&#25628;&#32034;&#23545;MLMs&#25191;&#34892;&#25991;&#26412;&#22635;&#20805;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#38752;&#30340;&#12290;&#24403;&#36825;&#20123;&#26465;&#20214;&#22833;&#36133;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20855;&#26377;&#27010;&#29575;&#20581;&#22766;&#24615;&#30340;&#20462;&#25913;&#65292;&#32780;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#39044;&#26399;&#26465;&#20214;&#19979;&#23427;&#20248;&#20110;&#21069;&#36848;&#30340;&#26463;&#25628;&#32034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27604;&#36739;&#22810;&#20010;&#39046;&#22495;&#20013;&#20960;&#31181;&#20351;&#29992;MLMs&#36827;&#34892;&#22635;&#20805;&#30340;&#26041;&#27861;&#30340;&#32463;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15020v1 Announce Type: cross  Abstract: Beam search with masked language models (MLMs) is challenging in part because joint probability distributions over sequences are not readily available, unlike for autoregressive models. Nevertheless, estimating such distributions has applications in many domains, including protein engineering and ancient text restoration. We present probabilistically-sound methods for beam search with MLMs. First, we clarify the conditions under which it is theoretically sound to perform text infilling with MLMs using standard beam search. When these conditions fail, we provide a probabilistically-sound modification with no additional computational complexity and demonstrate that it is superior to the aforementioned beam search in the expected conditions. We then present empirical results comparing several infilling approaches with MLMs across several domains.
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#29992;&#25143;&#20559;&#22909;&#23545;&#40784;&#21487;&#33021;&#20250;&#23548;&#33268;&#33521;&#35821;&#26041;&#35328;&#21644;&#20840;&#29699;&#24847;&#35265;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20294;&#20063;&#25552;&#39640;&#20102;&#22810;&#31181;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15018</link><description>&lt;p&gt;
LLM&#23545;&#20840;&#29699;&#34920;&#31034;&#30340;&#24847;&#22806;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Unintended Impacts of LLM Alignment on Global Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15018
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#29992;&#25143;&#20559;&#22909;&#23545;&#40784;&#21487;&#33021;&#20250;&#23548;&#33268;&#33521;&#35821;&#26041;&#35328;&#21644;&#20840;&#29699;&#24847;&#35265;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20294;&#20063;&#25552;&#39640;&#20102;&#22810;&#31181;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20026;&#38754;&#21521;&#29992;&#25143;&#30340;&#24212;&#29992;&#31243;&#24207;&#37096;&#32626;&#20043;&#21069;&#65292;&#24320;&#21457;&#20154;&#21592;&#36890;&#36807;&#21508;&#31181;&#31243;&#24207;&#65288;&#22914;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#65289;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#12290;&#30446;&#21069;&#23545;&#36825;&#20123;&#31243;&#24207;&#30340;&#35780;&#20272;&#20391;&#37325;&#20110;&#36981;&#24490;&#25351;&#23548;&#12289;&#25512;&#29702;&#21644;&#30495;&#23454;&#24615;&#30340;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#20559;&#22909;&#24182;&#38750;&#26222;&#36941;&#65292;&#23545;&#29305;&#23450;&#20559;&#22909;&#38598;&#36827;&#34892;&#23545;&#40784;&#21487;&#33021;&#20250;&#20135;&#29983;&#24847;&#22806;&#24433;&#21709;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#23545;&#19977;&#20010;&#20840;&#29699;&#34920;&#31034;&#32500;&#24230;&#65306;&#33521;&#35821;&#26041;&#35328;&#12289;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#20840;&#29699;&#21508;&#22269;&#24847;&#35265;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#23545;&#40784;&#31243;&#24207;&#22312;&#33521;&#35821;&#26041;&#35328;&#21644;&#20840;&#29699;&#24847;&#35265;&#20043;&#38388;&#20135;&#29983;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#23545;&#40784;&#25552;&#39640;&#20102;&#22810;&#31181;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23548;&#33268;&#36825;&#20123;&#24847;&#22806;&#24433;&#21709;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#24182;&#20026;&#26356;&#20844;&#24179;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15018v1 Announce Type: new  Abstract: Before being deployed for user-facing applications, developers align Large Language Models (LLMs) to user preferences through a variety of procedures, such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference Optimization (DPO). Current evaluations of these procedures focus on benchmarks of instruction following, reasoning, and truthfulness. However, human preferences are not universal, and aligning to specific preference sets may have unintended effects. We explore how alignment impacts performance along three axes of global representation: English dialects, multilingualism, and opinions from and about countries worldwide. Our results show that current alignment procedures create disparities between English dialects and global opinions. We find alignment improves capabilities in several languages. We conclude by discussing design decisions that led to these unintended impacts and recommendations for more equitable 
&lt;/p&gt;</description></item><item><title>&#22810;&#20219;&#21153;&#24494;&#35843;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#22522;&#30784;&#27169;&#22411;&#19978;&#23545;&#30456;&#20851;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#28982;&#21518;&#36866;&#24212;&#38480;&#21046;&#26631;&#31614;&#25968;&#30340;&#30446;&#26631;&#20219;&#21153;&#65292;&#33021;&#22815;&#38477;&#20302;&#30446;&#26631;&#20219;&#21153;&#20013;&#30340;&#35823;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#20219;&#21153;&#36873;&#25321;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15017</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20219;&#21153;&#24494;&#35843;&#23454;&#29616;&#22522;&#30784;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15017
&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#24494;&#35843;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#22522;&#30784;&#27169;&#22411;&#19978;&#23545;&#30456;&#20851;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#28982;&#21518;&#36866;&#24212;&#38480;&#21046;&#26631;&#31614;&#25968;&#30340;&#30446;&#26631;&#20219;&#21153;&#65292;&#33021;&#22815;&#38477;&#20302;&#30446;&#26631;&#20219;&#21153;&#20013;&#30340;&#35823;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#20219;&#21153;&#36873;&#25321;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#23613;&#31649;&#22522;&#30784;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#26377;&#25928;&#22320;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#25968;&#25454;&#26631;&#31614;&#26377;&#38480;&#30340;&#20219;&#21153;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#19988;&#32570;&#20047;&#29702;&#35770;&#29702;&#35299;&#12290;&#26368;&#36817;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#30340;&#19968;&#31181;&#26032;&#20852;&#35299;&#20915;&#26041;&#26696;&#26159;&#65292;&#22312;&#22522;&#30784;&#27169;&#22411;&#19978;&#23545;&#19968;&#31995;&#21015;&#30456;&#20851;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#28982;&#21518;&#20877;&#36866;&#24212;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#26679;&#26412;&#30340;&#30446;&#26631;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#31181;&#22810;&#20219;&#21153;&#24494;&#35843;&#26041;&#27861;&#30340;&#29702;&#35770;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#36890;&#36807;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#30456;&#20851;&#20219;&#21153;&#38598;&#65292;&#36825;&#31181;&#22810;&#20219;&#21153;&#24494;&#35843;&#21487;&#20197;&#38477;&#20302;&#30446;&#26631;&#20219;&#21153;&#20013;&#30340;&#35823;&#24046;&#65292;&#19982;&#30452;&#25509;&#36866;&#24212;&#30456;&#21516;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#26679;&#24615;&#21644;&#19968;&#33268;&#24615;&#25351;&#26631;&#37327;&#21270;&#20102;&#24494;&#35843;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#20219;&#21153;&#36873;&#25321;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15017v1 Announce Type: cross  Abstract: Foundation models have emerged as a powerful tool for many AI problems. Despite the tremendous success of foundation models, effective adaptation to new tasks, particularly those with limited labels, remains an open question and lacks theoretical understanding. An emerging solution with recent success in vision and NLP involves finetuning a foundation model on a selection of relevant tasks, before its adaptation to a target task with limited labeled samples. In this paper, we study the theoretical justification of this multitask finetuning approach. Our theoretical analysis reveals that with a diverse set of related tasks, this multitask finetuning leads to reduced error in the target task, in comparison to directly adapting the same pretrained model. We quantify the relationship between finetuning tasks and target tasks by diversity and consistency metrics, and further propose a practical task selection algorithm. We substantiate our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Ar-Spider&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38463;&#25289;&#20271;&#36328;&#39046;&#22495;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#65292;&#20026;&#35299;&#20915;&#38463;&#25289;&#20271;&#35821;&#35328;&#30340;&#29420;&#29305;&#24615;&#36136;&#25152;&#24102;&#26469;&#30340;&#27169;&#24335;&#35821;&#35328;&#21644;SQL&#32467;&#26500;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;&#24182;&#27979;&#35797;&#20102;&#20004;&#20010;&#36328;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15012</link><description>&lt;p&gt;
Ar-Spider&#65306;&#38463;&#25289;&#20271;&#35821;&#20013;&#30340;&#25991;&#26412;&#36716;SQL
&lt;/p&gt;
&lt;p&gt;
Ar-Spider: Text-to-SQL in Arabic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Ar-Spider&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38463;&#25289;&#20271;&#36328;&#39046;&#22495;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#65292;&#20026;&#35299;&#20915;&#38463;&#25289;&#20271;&#35821;&#35328;&#30340;&#29420;&#29305;&#24615;&#36136;&#25152;&#24102;&#26469;&#30340;&#27169;&#24335;&#35821;&#35328;&#21644;SQL&#32467;&#26500;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;&#24182;&#27979;&#35797;&#20102;&#20004;&#20010;&#36328;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#26159;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#23427;&#26088;&#22312;&#20351;&#29992;&#25143;&#20197;&#26356;&#33258;&#28982;&#30340;&#26041;&#24335;&#19982;&#25968;&#25454;&#24211;&#36827;&#34892;&#20132;&#20114;&#12290;&#36817;&#24180;&#26469;&#65292;&#25991;&#26412;&#21040;SQL&#24050;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#26159;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#38463;&#25289;&#20271;&#36328;&#39046;&#22495;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;Ar-Spider&#12290;&#30001;&#20110;&#35813;&#35821;&#35328;&#30340;&#29420;&#29305;&#24615;&#36136;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21363;&#27169;&#24335;&#35821;&#35328;&#21644;SQL &#32467;&#26500;&#25361;&#25112;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#24182;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;LGESQL&#21644;S2SQL&#65292;&#20004;&#32773;&#22343;&#19982;&#20004;&#20010;&#36328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20197;&#20943;&#36731;&#27169;&#24335;&#35821;&#35328;&#21644;SQL&#32467;&#26500;&#38142;&#25509;&#25361;&#25112;&#30340;&#24433;&#21709;&#12290;&#22522;&#32447;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#38463;&#25289;&#20271;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;Ar-Spider&#19978;&#34920;&#29616;&#20986;&#19981;&#38169;&#30340;&#21333;&#35821;&#35328;&#24615;&#33021;&#65292;&#20854;&#20013;S2SQL&#23454;&#29616;&#20102;62.48%&#65292;LGESQL&#23454;&#29616;&#20102;65.57%&#65292;&#20165;&#20302;&#20110;8.79%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15012v1 Announce Type: cross  Abstract: In Natural Language Processing (NLP), one of the most important tasks is text-to-SQL semantic parsing, which focuses on enabling users to interact with the database in a more natural manner. In recent years, text-to-SQL has made significant progress, but most were English-centric. In this paper, we introduce Ar-Spider 1, the first Arabic cross-domain text-to-SQL dataset. Due to the unique nature of the language, two major challenges have been encountered, namely schema linguistic and SQL structural challenges. In order to handle these issues and conduct the experiments, we adopt two baseline models LGESQL [4] and S2SQL [12], both of which are tested with two cross-lingual models to alleviate the effects of schema linguistic and SQL structure linking challenges. The baselines demonstrate decent single-language performance on our Arabic text-to-SQL dataset, Ar-Spider, achieving 62.48% for S2SQL and 65.57% for LGESQL, only 8.79% below the
&lt;/p&gt;</description></item><item><title>&#23376;&#35789;&#26631;&#35760;&#21270;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#20027;&#27969;&#26631;&#20934;&#65292;&#20294;&#20854;&#25104;&#21151;&#22240;&#32032;&#65292;&#22914;&#19981;&#21516;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#26368;&#20339;&#20998;&#21106;&#31890;&#24230;&#12289;&#25968;&#25454;&#28304;&#23545;&#26631;&#35760;&#24037;&#20855;&#30340;&#24433;&#21709;&#20197;&#21450;&#24418;&#24577;&#20449;&#24687;&#22312;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#65292;&#20173;&#19981;&#26126;&#30830;&#12290;</title><link>https://arxiv.org/abs/2402.15010</link><description>&lt;p&gt;
&#27861;&#35821;&#21307;&#29992;&#21475;&#32617;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26631;&#35760;&#21270;&#26377;&#22810;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Important Is Tokenization in French Medical Masked Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15010
&lt;/p&gt;
&lt;p&gt;
&#23376;&#35789;&#26631;&#35760;&#21270;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#20027;&#27969;&#26631;&#20934;&#65292;&#20294;&#20854;&#25104;&#21151;&#22240;&#32032;&#65292;&#22914;&#19981;&#21516;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#26368;&#20339;&#20998;&#21106;&#31890;&#24230;&#12289;&#25968;&#25454;&#28304;&#23545;&#26631;&#35760;&#24037;&#20855;&#30340;&#24433;&#21709;&#20197;&#21450;&#24418;&#24577;&#20449;&#24687;&#22312;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#65292;&#20173;&#19981;&#26126;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#23376;&#35789;&#30340;&#26631;&#35760;&#21270;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20013;&#30340;&#20027;&#27969;&#26631;&#20934;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23548;&#33268;&#20854;&#25104;&#21151;&#30340;&#30830;&#20999;&#22240;&#32032;&#65292;&#22914;&#19981;&#21516;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#26368;&#20339;&#20998;&#21106;&#31890;&#24230;&#65292;&#25968;&#25454;&#28304;&#23545;&#26631;&#35760;&#24037;&#20855;&#30340;&#24433;&#21709;&#20197;&#21450;&#24418;&#24577;&#20449;&#24687;&#22312;&#21360;&#27431;&#35821;&#35328;&#20013;&#30340;&#20316;&#29992;&#65292;&#20173;&#28982;&#19981;&#22815;&#28165;&#26970;&#12290;&#36825;&#22312;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#26041;&#38754;&#23588;&#20026;&#37325;&#35201;&#65292;&#20854;&#29305;&#28857;&#26159;&#20855;&#26377;&#31649;&#29702;&#24418;&#24577;&#32032;&#32452;&#21512;&#30340;&#29305;&#23450;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15010v1 Announce Type: cross  Abstract: Subword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language models. This shift began with Byte-Pair Encoding (BPE) and was later followed by the adoption of SentencePiece and WordPiece. While subword tokenization consistently outperforms character and word-level tokenization, the precise factors contributing to its success remain unclear. Key aspects such as the optimal segmentation granularity for diverse tasks and languages, the influence of data sources on tokenizers, and the role of morphological information in Indo-European languages remain insufficiently explored. This is particularly pertinent for biomedical terminology, characterized by specific rules governing morpheme combinations. Despite the agglutinative nature of biomedical terminology, existing language models do not explicitly incorporate 
&lt;/p&gt;</description></item><item><title>CommVQA&#25968;&#25454;&#38598;&#23558;&#22270;&#20687;&#32622;&#20110;&#33258;&#28982;&#29615;&#22659;&#20013;&#65292;&#25361;&#25112;&#20102;&#24403;&#21069;&#30340;VQA&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#20026;&#27169;&#22411;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15002</link><description>&lt;p&gt;
&#23558;&#35270;&#35273;&#38382;&#31572;&#32622;&#20110;&#20132;&#38469;&#32972;&#26223;&#20013;&#30340;CommVQA
&lt;/p&gt;
&lt;p&gt;
CommVQA: Situating Visual Question Answering in Communicative Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15002
&lt;/p&gt;
&lt;p&gt;
CommVQA&#25968;&#25454;&#38598;&#23558;&#22270;&#20687;&#32622;&#20110;&#33258;&#28982;&#29615;&#22659;&#20013;&#65292;&#25361;&#25112;&#20102;&#24403;&#21069;&#30340;VQA&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#20026;&#27169;&#22411;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#24448;&#24448;&#22312;&#23396;&#31435;&#30340;&#22270;&#20687;-&#38382;&#39064;&#23545;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#25552;&#20986;&#30340;&#38382;&#39064;&#21462;&#20915;&#20110;&#20182;&#20204;&#30340;&#20449;&#24687;&#38656;&#27714;&#21644;&#23545;&#22270;&#20687;&#20869;&#23481;&#30340;&#20808;&#21069;&#20102;&#35299;&#12290;&#20026;&#20102;&#35780;&#20272;&#23558;&#22270;&#20687;&#32622;&#20110;&#33258;&#28982;&#29615;&#22659;&#20013;&#22914;&#20309;&#22609;&#36896;&#35270;&#35273;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CommVQA&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#22270;&#20687;&#12289;&#22270;&#20687;&#25551;&#36848;&#12289;&#22270;&#20687;&#21487;&#33021;&#20986;&#29616;&#30340;&#30495;&#23454;&#20132;&#38469;&#22330;&#26223;&#65288;&#20363;&#22914;&#26053;&#34892;&#32593;&#31449;&#65289;&#20197;&#21450;&#20381;&#36182;&#20110;&#22330;&#26223;&#30340;&#21518;&#32493;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;VQA&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CommVQA&#23545;&#24403;&#21069;&#27169;&#22411;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;VQA&#27169;&#22411;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#21487;&#24191;&#27867;&#25552;&#39640;&#24615;&#33021;&#65292;&#31361;&#26174;&#23558;&#31995;&#32479;&#32622;&#20110;&#20132;&#38469;&#22330;&#26223;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15002v1 Announce Type: new  Abstract: Current visual question answering (VQA) models tend to be trained and evaluated on image-question pairs in isolation. However, the questions people ask are dependent on their informational needs and prior knowledge about the image content. To evaluate how situating images within naturalistic contexts shapes visual questions, we introduce CommVQA, a VQA dataset consisting of images, image descriptions, real-world communicative scenarios where the image might appear (e.g., a travel website), and follow-up questions and answers conditioned on the scenario. We show that CommVQA poses a challenge for current models. Providing contextual information to VQA models improves performance broadly, highlighting the relevance of situating systems within a communicative scenario.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25512;&#29702;&#20219;&#21153;&#20998;&#35299;&#20026;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#21644;&#38382;&#39064;&#35299;&#20915;&#38454;&#27573;&#30340;&#31574;&#30053;&#65292;&#21457;&#29616;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#30456;&#27604;&#38382;&#39064;&#35299;&#20915;&#26356;&#23481;&#26131;&#25552;&#28860;&#20026;&#36739;&#23567;&#27169;&#22411;&#65292;&#24182;&#35777;&#23454;&#35813;&#31574;&#30053;&#32988;&#36807;&#21333;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.15000</link><description>&lt;p&gt;
&#21010;&#20998;&#36824;&#26159;&#24449;&#26381;&#65311;&#20320;&#24212;&#35813;&#25552;&#28860;LLM&#30340;&#21738;&#19968;&#37096;&#20998;&#65311;
&lt;/p&gt;
&lt;p&gt;
Divide-or-Conquer? Which Part Should You Distill Your LLM?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25512;&#29702;&#20219;&#21153;&#20998;&#35299;&#20026;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#21644;&#38382;&#39064;&#35299;&#20915;&#38454;&#27573;&#30340;&#31574;&#30053;&#65292;&#21457;&#29616;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#30456;&#27604;&#38382;&#39064;&#35299;&#20915;&#26356;&#23481;&#26131;&#25552;&#28860;&#20026;&#36739;&#23567;&#27169;&#22411;&#65292;&#24182;&#35777;&#23454;&#35813;&#31574;&#30053;&#32988;&#36807;&#21333;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#34987;&#40723;&#21169;&#20808;&#35299;&#20915;&#20027;&#35201;&#20219;&#21153;&#30340;&#23376;&#20219;&#21153;&#26102;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#25512;&#29702;&#20219;&#21153;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#31867;&#20284;&#30340;&#31574;&#30053;&#65292;&#23558;&#25512;&#29702;&#20219;&#21153;&#20998;&#35299;&#20026;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#21644;&#38382;&#39064;&#35299;&#20915;&#38454;&#27573;&#65292;&#24182;&#23637;&#31034;&#35813;&#31574;&#30053;&#33021;&#22815;&#32988;&#36807;&#21333;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20551;&#35774;&#19982;&#35299;&#20915;&#38382;&#39064;&#30456;&#27604;&#65292;&#20998;&#35299;&#38454;&#27573;&#26356;&#23481;&#26131;&#34987;&#25552;&#28860;&#20026;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#22240;&#20026;&#21518;&#32773;&#38656;&#35201;&#22823;&#37327;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#32780;&#21069;&#32773;&#21482;&#38656;&#35201;&#23398;&#20064;&#19968;&#33324;&#30340;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25552;&#28860;&#36825;&#20004;&#31181;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#25512;&#29702;&#32467;&#26524;&#21644;&#25512;&#29702;&#25104;&#26412;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#21487;&#20197;&#25552;&#28860;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#65292;&#24182;&#21516;&#26102;&#22312;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#35201;&#25552;&#28860;&#38382;&#39064;&#35299;&#20915;&#38454;&#27573;&#23601;&#26356;&#22256;&#38590;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15000v1 Announce Type: new  Abstract: Recent methods have demonstrated that Large Language Models (LLMs) can solve reasoning tasks better when they are encouraged to solve subtasks of the main task first. In this paper we devise a similar strategy that breaks down reasoning tasks into a problem decomposition phase and a problem solving phase and show that the strategy is able to outperform a single stage solution. Further, we hypothesize that the decomposition should be easier to distill into a smaller model compared to the problem solving because the latter requires large amounts of domain knowledge while the former only requires learning general problem solving strategies. We propose methods to distill these two capabilities and evaluate their impact on reasoning outcomes and inference cost. We find that we can distill the problem decomposition phase and at the same time achieve good generalization across tasks, datasets, and models. However, it is harder to distill the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20943;&#23569;&#35780;&#20272;LLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#35780;&#20272;&#27425;&#25968;&#30340;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23567;&#35268;&#27169;&#31034;&#20363;&#19978;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;LLMs&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14992</link><description>&lt;p&gt;
&#23567;&#22411;&#22522;&#20934;&#27979;&#35797;&#65306;&#29992;&#26356;&#23569;&#30340;&#31034;&#20363;&#35780;&#20272;LLM
&lt;/p&gt;
&lt;p&gt;
tinyBenchmarks: evaluating LLMs with fewer examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20943;&#23569;&#35780;&#20272;LLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#35780;&#20272;&#27425;&#25968;&#30340;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23567;&#35268;&#27169;&#31034;&#20363;&#19978;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;LLMs&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22810;&#21151;&#33021;&#24615;&#23548;&#33268;&#21019;&#24314;&#20102;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#65292;&#24443;&#24213;&#27979;&#35797;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#31034;&#20363;&#65292;&#20351;&#24471;&#35780;&#20272;LLMs&#38750;&#24120;&#26114;&#36149;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20943;&#23569;&#35780;&#20272;LLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#35780;&#20272;&#27425;&#25968;&#30340;&#31574;&#30053;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35201;&#20934;&#30830;&#20272;&#35745;LLMs&#22312;MMLU&#19978;&#30340;&#24615;&#33021;&#65288;&#19968;&#20010;&#21253;&#21547;14K&#20010;&#31034;&#20363;&#30340;&#27969;&#34892;&#22810;&#36873;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#65289;&#65292;&#21482;&#38656;&#35201;&#22312;100&#20010;&#31934;&#24515;&#25361;&#36873;&#30340;&#31034;&#20363;&#19978;&#35780;&#20272;&#36825;&#20010;LLMs&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#35780;&#20272;&#24037;&#20855;&#21644;&#27969;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#24494;&#22411;&#29256;&#26412;&#65306;Open LLM Leaderboard&#12289;MMLU&#12289;HELM&#21644;AlpacaEval 2.0&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#24037;&#20855;&#21644;&#24494;&#22411;&#22522;&#20934;&#27979;&#35797;&#36275;&#20197;&#21487;&#38752;&#19988;&#39640;&#25928;&#22320;&#37325;&#29616;&#21407;&#22987;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14992v1 Announce Type: cross  Abstract: The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#23558;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#35270;&#20026;&#19968;&#20010;&#22240;&#26524;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22240;&#26524;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#24182;&#36890;&#36807;&#21452;&#37325;&#31283;&#20581;CPO(DR-CPO)&#38477;&#20302;&#20102;&#26367;&#20195;&#30446;&#26631;&#30340;&#26041;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.14979</link><description>&lt;p&gt;
&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#20197;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#26159;&#19968;&#20010;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Optimizing Language Models for Human Preferences is a Causal Inference Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#23558;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#35270;&#20026;&#19968;&#20010;&#22240;&#26524;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22240;&#26524;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#24182;&#36890;&#36807;&#21452;&#37325;&#31283;&#20581;CPO(DR-CPO)&#38477;&#20302;&#20102;&#26367;&#20195;&#30446;&#26631;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23398;&#26415;&#21644;&#21830;&#19994;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#25991;&#26412;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#20174;&#30452;&#25509;&#32467;&#26524;&#25968;&#25454;&#38598;&#20013;&#38024;&#23545;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#65292;&#20854;&#20013;&#27599;&#20010;&#26679;&#26412;&#30001;&#19968;&#27573;&#25991;&#26412;&#21644;&#19968;&#20010;&#34913;&#37327;&#35835;&#32773;&#21709;&#24212;&#30340;&#30456;&#20851;&#25968;&#20540;&#32467;&#26524;&#32452;&#25104;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#24212;&#23558;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#35270;&#20026;&#19968;&#20010;&#22240;&#26524;&#38382;&#39064;&#65292;&#20197;&#30830;&#20445;&#27169;&#22411;&#27491;&#30830;&#23398;&#20064;&#25991;&#26412;&#19982;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#27491;&#24335;&#21270;&#20102;&#36825;&#20010;&#22240;&#26524;&#35821;&#35328;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;--&#22240;&#26524;&#20559;&#22909;&#20248;&#21270;(CPO)--&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26080;&#20559;&#26367;&#20195;&#30446;&#26631;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#21452;&#37325;&#31283;&#20581;&#30340;CPO(DR-CPO)&#25193;&#23637;CPO&#65292;&#38477;&#20302;&#20102;&#26367;&#20195;&#30446;&#26631;&#30340;&#26041;&#24046;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#26126;&#26174;&#24378;&#26377;&#21147;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14979v1 Announce Type: cross  Abstract: As large language models (LLMs) see greater use in academic and commercial settings, there is increasing interest in methods that allow language models to generate texts aligned with human preferences. In this paper, we present an initial exploration of language model optimization for human preferences from direct outcome datasets, where each sample consists of a text and an associated numerical outcome measuring the reader's response. We first propose that language model optimization should be viewed as a causal problem to ensure that the model correctly learns the relationship between the text and the outcome. We formalize this causal language optimization problem, and we develop a method--causal preference optimization (CPO)--that solves an unbiased surrogate objective for the problem. We further extend CPO with doubly robust CPO (DR-CPO), which reduces the variance of the surrogate objective while retaining provably strong guarante
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14973</link><description>&lt;p&gt;
GenCeption&#65306;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#22810;&#27169;&#24577;LLM
&lt;/p&gt;
&lt;p&gt;
GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14973
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36890;&#24120;&#20351;&#29992;&#26114;&#36149;&#30340;&#24102;&#26631;&#27880;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#36890;&#24120;&#38590;&#20197;&#36319;&#19978;MLLM&#35780;&#20272;&#30340;&#24555;&#36895;&#21457;&#23637;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GenCeption&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#38656;&#27880;&#37322;&#30340;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#20165;&#38656;&#35201;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#21453;&#26144;&#20986;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#12290;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;DrawCeption&#28216;&#25103;&#65292;GenCeption&#20174;&#19968;&#20010;&#38750;&#25991;&#26412;&#26679;&#26412;&#24320;&#22987;&#65292;&#24182;&#32463;&#21382;&#19968;&#31995;&#21015;&#36845;&#20195;&#30340;&#25551;&#36848;&#21644;&#29983;&#25104;&#27493;&#39588;&#12290;&#36845;&#20195;&#20043;&#38388;&#30340;&#35821;&#20041;&#28418;&#31227;&#20351;&#29992;GC@T&#25351;&#26631;&#36827;&#34892;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#39564;&#35777;&#20102;GenCeption&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#20986;&#19982;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;GenCeption&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26222;&#36941;&#23384;&#22312;&#19988;&#20197;&#21069;&#26410;&#35265;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#25193;&#23637;&#65292;&#20197;&#20943;&#36731;&#35757;&#32451;&#25968;&#25454;&#30340;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14973v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) are commonly evaluated using costly annotated multimodal benchmarks. However, these benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models' inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption's efficacy, showing strong correlations with popular MLLM benchmarking results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.
&lt;/p&gt;</description></item><item><title>MultiLS&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#21019;&#24314;&#22810;&#20219;&#21153;LS&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;MultiLS-PT&#20316;&#20026;&#31532;&#19968;&#20010;&#20351;&#29992;&#35813;&#26694;&#26550;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#35789;&#27719;&#31616;&#21270;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14972</link><description>&lt;p&gt;
MultiLS: &#19968;&#20010;&#22810;&#20219;&#21153;&#35789;&#27719;&#31616;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MultiLS: A Multi-task Lexical Simplification Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14972
&lt;/p&gt;
&lt;p&gt;
MultiLS&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#21019;&#24314;&#22810;&#20219;&#21153;LS&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;MultiLS-PT&#20316;&#20026;&#31532;&#19968;&#20010;&#20351;&#29992;&#35813;&#26694;&#26550;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#35789;&#27719;&#31616;&#21270;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#31616;&#21270;&#65288;LS&#65289;&#33258;&#21160;&#26367;&#25442;&#38590;&#20197;&#29702;&#35299;&#30340;&#21333;&#35789;&#20026;&#26356;&#26131;&#35835;&#30340;&#26367;&#20195;&#35789;&#65292;&#21516;&#26102;&#20445;&#30041;&#21477;&#23376;&#30340;&#21407;&#22987;&#21547;&#20041;&#12290;LS&#26159;&#25991;&#26412;&#31616;&#21270;&#30340;&#21069;&#36523;&#65292;&#26088;&#22312;&#25913;&#21892;&#25991;&#26412;&#23545;&#21508;&#31181;&#30446;&#26631;&#20154;&#32676;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#21253;&#25324;&#20799;&#31461;&#12289;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#12289;&#38405;&#35835;&#38556;&#30861;&#25110;&#20302;&#35782;&#23383;&#29575;&#30340;&#20154;&#32676;&#12290;&#23384;&#22312;&#19968;&#20123;&#19987;&#38376;&#29992;&#20110;LS&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#19987;&#27880;&#20110;LS&#27969;&#31243;&#20013;&#30340;&#19968;&#20010;&#25110;&#20004;&#20010;&#23376;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26410;&#24320;&#21457;&#20986;&#19968;&#20010;&#35206;&#30422;&#25152;&#26377;LS&#23376;&#20219;&#21153;&#30340;&#21333;&#20010;LS&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MultiLS&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#21019;&#24314;&#22810;&#20219;&#21153;LS&#25968;&#25454;&#38598;&#30340;LS&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;MultiLS-PT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;MultiLS&#26694;&#26550;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#25191;&#34892;&#25152;&#26377;LS&#23376;&#20219;&#21153;&#65292;&#21253;&#25324;&#65288;1&#65289;&#35789;&#27719;&#22797;&#26434;&#24615;&#39044;&#27979;&#65288;LCP&#65289;&#12289;&#65288;2&#65289;&#26367;&#20195;&#35789;&#29983;&#25104;&#21644;&#65288;3&#65289;&#26367;&#20195;&#35789;&#25490;&#21517;&#65292;&#23637;&#31034;&#20102;MultiLS-PT&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14972v1 Announce Type: cross  Abstract: Lexical Simplification (LS) automatically replaces difficult to read words for easier alternatives while preserving a sentence's original meaning. LS is a precursor to Text Simplification with the aim of improving text accessibility to various target demographics, including children, second language learners, individuals with reading disabilities or low literacy. Several datasets exist for LS. These LS datasets specialize on one or two sub-tasks within the LS pipeline. However, as of this moment, no single LS dataset has been developed that covers all LS sub-tasks. We present MultiLS, the first LS framework that allows for the creation of a multi-task LS dataset. We also present MultiLS-PT, the first dataset to be created using the MultiLS framework. We demonstrate the potential of MultiLS-PT by carrying out all LS sub-tasks of (1). lexical complexity prediction (LCP), (2). substitute generation, and (3). substitute ranking for Portugu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21518;&#38376;&#22686;&#24378;&#23545;&#40784;&#26041;&#27861;&#26377;&#25928;&#32531;&#35299;&#24494;&#35843;&#36234;&#29425;&#25915;&#20987;&#65292;&#36991;&#20813;&#38656;&#35201;&#22823;&#37327;&#23433;&#20840;&#31034;&#20363;&#30340;&#20302;&#25928;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14968</link><description>&lt;p&gt;
&#20351;&#29992;&#21518;&#38376;&#22686;&#24378;&#23545;&#40784;&#26469;&#32531;&#35299;&#24494;&#35843;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14968
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21518;&#38376;&#22686;&#24378;&#23545;&#40784;&#26041;&#27861;&#26377;&#25928;&#32531;&#35299;&#24494;&#35843;&#36234;&#29425;&#25915;&#20987;&#65292;&#36991;&#20813;&#38656;&#35201;&#22823;&#37327;&#23433;&#20840;&#31034;&#20363;&#30340;&#20302;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#21644;Llama-2&#20855;&#26377;&#19968;&#33324;&#33021;&#21147;&#65292;&#20294;&#22312;&#28385;&#36275;&#29305;&#23450;&#19994;&#21153;&#38656;&#27714;&#21644;&#23450;&#21046;&#29992;&#20363;&#30340;&#22797;&#26434;&#24615;&#26102;&#65292;&#20173;&#28982;&#38656;&#35201;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#25110;&#33258;&#36866;&#24212;&#20197;&#28385;&#36275;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#19981;&#21487;&#36991;&#20813;&#22320;&#24341;&#20837;&#20102;&#26032;&#30340;&#23433;&#20840;&#23041;&#32961;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#22522;&#20110;&#24494;&#35843;&#30340;&#36234;&#29425;&#25915;&#20987;&#65288;FJAttack&#65289;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23558;&#20165;&#20960;&#20010;&#26377;&#23475;&#31034;&#20363;&#32435;&#20837;&#24494;&#35843;&#25968;&#25454;&#38598;&#23601;&#21487;&#33021;&#26174;&#30528;&#22320;&#25439;&#23475;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#20363;&#22914;&#23558;&#23433;&#20840;&#31034;&#20363;&#32435;&#20837;&#24494;&#35843;&#25968;&#25454;&#38598;&#20197;&#20943;&#23569;&#23433;&#20840;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#32435;&#20837;&#22823;&#37327;&#30340;&#23433;&#20840;&#31034;&#20363;&#65292;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#38024;&#23545;FJAttack&#36827;&#34892;&#38450;&#24481;&#24182;&#21482;&#20351;&#29992;&#26377;&#38480;&#30340;&#23433;&#20840;&#31034;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#24863;&#26469;&#33258;&#21518;&#38376;&#25915;&#20987;&#27010;&#24565;&#30340;&#21518;&#38376;&#22686;&#24378;&#23433;&#20840;&#23545;&#40784;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14968v1 Announce Type: cross  Abstract: Despite the general capabilities of Large Language Models (LLMs) like GPT-4 and Llama-2, these models still request fine-tuning or adaptation with customized data when it comes to meeting the specific business demands and intricacies of tailored use cases. However, this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack), where incorporating just a few harmful examples into the fine-tuning dataset can significantly compromise the model safety. Though potential defenses have been proposed by incorporating safety examples into the fine-tuning dataset to reduce the safety issues, such approaches require incorporating a substantial amount of safety examples, making it inefficient. To effectively defend against the FJAttack with limited safety examples, we propose a Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In pa
&lt;/p&gt;</description></item><item><title>Mirror &#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#33258;&#25105;&#21453;&#24605;&#26041;&#27861;&#65292;&#36890;&#36807;&#23548;&#33322;&#32773;&#21644;&#25512;&#29702;&#32773;&#20043;&#38388;&#30340;&#21551;&#21457;&#24335;&#20132;&#20114;&#65292;&#20419;&#36827;&#22810;&#26679;&#24615;&#32780;&#20855;&#26377;&#21487;&#38752;&#24615;&#30340;&#25512;&#29702;&#36712;&#36857;&#21457;&#23637;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#30693;&#35782;&#20016;&#23500;&#38382;&#39064;&#19978;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.14963</link><description>&lt;p&gt;
&#38236;&#20687;&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#30693;&#35782;&#20016;&#23500;&#25512;&#29702;&#30340;&#22810;&#35270;&#35282;&#33258;&#25105;&#21453;&#24605;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14963
&lt;/p&gt;
&lt;p&gt;
Mirror &#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#33258;&#25105;&#21453;&#24605;&#26041;&#27861;&#65292;&#36890;&#36807;&#23548;&#33322;&#32773;&#21644;&#25512;&#29702;&#32773;&#20043;&#38388;&#30340;&#21551;&#21457;&#24335;&#20132;&#20114;&#65292;&#20419;&#36827;&#22810;&#26679;&#24615;&#32780;&#20855;&#26377;&#21487;&#38752;&#24615;&#30340;&#25512;&#29702;&#36712;&#36857;&#21457;&#23637;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#30693;&#35782;&#20016;&#23500;&#38382;&#39064;&#19978;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#33021;&#21147;&#21453;&#22797;&#21453;&#24605;&#33258;&#24049;&#30340;&#36755;&#20986;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;&#27809;&#26377;&#22806;&#37096;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#30693;&#35782;&#20016;&#23500;&#38382;&#39064;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#38500;&#20102;LLMs&#22312;&#33258;&#25105;&#35780;&#20272;&#26041;&#38754;&#30340;&#20302;&#25928;&#29575;&#22806;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#23613;&#31649;&#21463;&#21040;&#26126;&#30830;&#36127;&#38754;&#21453;&#39304;&#65292;LLMs&#20173;&#28982;&#38590;&#20197;&#37325;&#26032;&#23457;&#35270;&#20854;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Mirror&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#30693;&#35782;&#20016;&#23500;&#25512;&#29702;&#30340;&#22810;&#35282;&#24230;&#33258;&#25105;&#21453;&#24605;&#26041;&#27861;&#65292;&#20197;&#36991;&#20813;&#22312;&#29305;&#23450;&#21453;&#24605;&#36845;&#20195;&#20013;&#21345;&#20303;&#12290;Mirror&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#23548;&#33322;&#32773;&#21644;&#25512;&#29702;&#32773;&#20043;&#38388;&#30340;&#21551;&#21457;&#24335;&#20132;&#20114;&#33719;&#24471;&#22810;&#35270;&#35282;&#32447;&#32034;&#30340;&#21453;&#24605;&#65292;&#24341;&#23548;&#20195;&#29702;&#21521;&#22810;&#26679;&#24615;&#32780;&#20855;&#26377;&#21487;&#38752;&#24615;&#30340;&#25512;&#29702;&#36712;&#36857;&#21457;&#23637;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#22320;&#38754;&#30495;&#30456;&#65292;&#36890;&#36807;&#40723;&#21169;&#65288;1&#65289;&#23548;&#33322;&#32773;&#29983;&#25104;&#30340;&#26041;&#21521;&#30340;&#22810;&#26679;&#24615;&#19982;&#65288;2&#65289;&#31574;&#30053;&#24615;&#24341;&#21457;&#30340;&#25200;&#21160;&#22312;&#20135;&#29983;&#30340;&#22238;&#24212;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14963v1 Announce Type: cross  Abstract: While Large language models (LLMs) have the capability to iteratively reflect on their own outputs, recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback. Therefore, We propose Mirror, a Multiple-perspective self-reflection method for knowledge-rich reasoning, to avoid getting stuck at a particular reflection iteration. Mirror enables LLMs to reflect from multiple-perspective clues, achieved through a heuristic interaction between a Navigator and a Reasoner. It guides agents toward diverse yet plausibly reliable reasoning trajectory without access to ground truth by encouraging (1) diversity of directions generated by Navigator and (2) agreement among strategically induced perturbations in responses generated by 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32467;&#21512;&#32447;&#24615;&#27880;&#24847;&#21147;&#21644;&#32447;&#24615;MLP&#32452;&#20214;&#30340;&#32447;&#24615;Transformer&#22359;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#20960;&#20046;&#21487;&#20197;&#36798;&#21040;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#65292;&#24182;&#19988;&#19982;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#20272;&#35745;&#22120;&#26377;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.14951</link><description>&lt;p&gt;
&#19968;&#20010;&#32447;&#24615;Transformer&#22359;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;MLP&#32452;&#20214;&#21644;&#19968;&#27493;GD&#21021;&#22987;&#21270;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14951
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32467;&#21512;&#32447;&#24615;&#27880;&#24847;&#21147;&#21644;&#32447;&#24615;MLP&#32452;&#20214;&#30340;&#32447;&#24615;Transformer&#22359;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#20960;&#20046;&#21487;&#20197;&#36798;&#21040;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#65292;&#24182;&#19988;&#19982;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#20272;&#35745;&#22120;&#26377;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32467;&#21512;&#32447;&#24615;&#27880;&#24847;&#21147;&#32452;&#20214;&#21644;&#32447;&#24615;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#32452;&#20214;&#30340;&#32447;&#24615;Transformer&#22359;&#65288;LTB&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#23545;&#20110;&#20855;&#26377;&#39640;&#26031;&#20808;&#39564;&#21644;&#38750;&#38646;&#22343;&#20540;&#30340;&#32447;&#24615;&#22238;&#24402;&#30340;ICL&#65292;&#25105;&#20204;&#34920;&#26126;LTB&#21487;&#20197;&#23454;&#29616;&#20960;&#20046;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;ICL&#39118;&#38505;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20165;&#20351;&#29992;&#32447;&#24615;&#27880;&#24847;&#21147;&#24517;&#39035;&#20135;&#29983;&#19981;&#21487;&#36991;&#20813;&#30340;&#38468;&#21152;&#36817;&#20284;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;LTB&#19982;&#20855;&#26377;&#21487;&#23398;&#20064;&#21021;&#22987;&#21270;&#30340;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#20272;&#35745;&#22120;&#65288;$\mathsf{GD}-\mathbf{\beta}$&#65289;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#20174;&#27599;&#20010;$\mathsf{GD}-\mathbf{\beta}$&#20272;&#35745;&#22120;&#21487;&#20197;&#36890;&#36807;LTB&#20272;&#35745;&#22120;&#23454;&#29616;&#65292;&#21040;&#26368;&#23567;&#21270;&#31867;&#20869;ICL&#39118;&#38505;&#30340;&#27599;&#20010;&#26368;&#20248;LTB&#20272;&#35745;&#22120;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;$\mathsf{GD}-\mathbf{\beta}$&#20272;&#35745;&#22120;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;$\mathsf{GD}-\mathbf{\beta}$&#20272;&#35745;&#22120;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#39640;&#25928;&#22320;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14951v1 Announce Type: cross  Abstract: We study the \emph{in-context learning} (ICL) ability of a \emph{Linear Transformer Block} (LTB) that combines a linear attention component and a linear multi-layer perceptron (MLP) component. For ICL of linear regression with a Gaussian prior and a \emph{non-zero mean}, we show that LTB can achieve nearly Bayes optimal ICL risk. In contrast, using only linear attention must incur an irreducible additive approximation error. Furthermore, we establish a correspondence between LTB and one-step gradient descent estimators with learnable initialization ($\mathsf{GD}\text{-}\mathbf{\beta}$), in the sense that every $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator can be implemented by an LTB estimator and every optimal LTB estimator that minimizes the in-class ICL risk is effectively a $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator. Finally, we show that $\mathsf{GD}\text{-}\mathbf{\beta}$ estimators can be efficiently optimized with gradient f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#35838;&#31243;&#30340;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064; CuPUL &#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#22024;&#26434;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.14948</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#36828;&#31243;&#30417;&#30563;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65306;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14948
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#35838;&#31243;&#30340;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064; CuPUL &#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#22024;&#26434;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#36828;&#31243;&#30417;&#30563;&#65288;DS-NER&#65289;&#26694;&#26550;&#19979;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#65292;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#26631;&#31614;&#36136;&#37327;&#21463;&#21040;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#22914;&#20551;&#38451;&#24615;&#12289;&#20551;&#38452;&#24615;&#21644;&#27491;&#21521;&#31867;&#22411;&#38169;&#35823;&#12290;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#35780;&#20272;&#20102;&#24403;&#21069;DS-NER&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;QTL&#30340;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#23427;&#20204;&#30340;&#24615;&#33021;&#24448;&#24448;&#19981;&#31526;&#21512;&#39044;&#26399;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#26222;&#36941;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#35838;&#31243;&#30340;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064;&#65288;CuPUL&#65289;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#31574;&#30053;&#24615;&#22320;&#20174;&#8220;&#26131;&#8221;&#21644;&#26356;&#28165;&#27905;&#30340;&#26679;&#26412;&#24320;&#22987;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#22024;&#26434;&#26679;&#26412;&#30340;&#38887;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#31361;&#20986;&#20102;CuPUL&#20943;&#23569;&#22024;&#26434;&#26631;&#31614;&#24433;&#21709;&#24182;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14948v1 Announce Type: new  Abstract: This paper delves into Named Entity Recognition (NER) under the framework of Distant Supervision (DS-NER), where the main challenge lies in the compromised quality of labels due to inherent errors such as false positives, false negatives, and positive type errors. We critically assess the efficacy of current DS-NER methodologies using a real-world benchmark dataset named QTL, revealing that their performance often does not meet expectations. To tackle the prevalent issue of label noise, we introduce a simple yet effective approach, Curriculum-based Positive-Unlabeled Learning CuPUL, which strategically starts on "easy" and cleaner samples during the training process to enhance model resilience to noisy samples. Our empirical results highlight the capability of CuPUL to significantly reduce the impact of noisy labels and outperform existing methods.
&lt;/p&gt;</description></item><item><title>MobileLLM&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26550;&#26500;&#65292;&#37319;&#29992;&#28145;&#24230;&#21644;&#30246;&#36523;&#32467;&#26500;&#12289;&#23884;&#20837;&#20849;&#20139;&#21644;&#20998;&#32452;&#26597;&#35810;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;2.7%/4.3%&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#19988;&#20165;&#26377;&#26497;&#23567;&#24310;&#36831;&#24320;&#38144;&#30340;&#22359;&#29366;&#26435;&#37325;&#20849;&#20139;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.14905</link><description>&lt;p&gt;
MobileLLM&#65306;&#20248;&#21270;&#20122;&#21313;&#20159;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;&#20197;&#29992;&#20110;&#35774;&#22791;&#31471;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14905
&lt;/p&gt;
&lt;p&gt;
MobileLLM&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26550;&#26500;&#65292;&#37319;&#29992;&#28145;&#24230;&#21644;&#30246;&#36523;&#32467;&#26500;&#12289;&#23884;&#20837;&#20849;&#20139;&#21644;&#20998;&#32452;&#26597;&#35810;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;2.7%/4.3%&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#19988;&#20165;&#26377;&#26497;&#23567;&#24310;&#36831;&#24320;&#38144;&#30340;&#22359;&#29366;&#26435;&#37325;&#20849;&#20139;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#31227;&#21160;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36843;&#20999;&#38656;&#27714;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#20113;&#25104;&#26412;&#21644;&#24310;&#36831;&#38382;&#39064;&#19981;&#26029;&#22686;&#21152;&#25152;&#23548;&#33268;&#30340;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#35774;&#35745;&#20855;&#26377;&#19981;&#21040;&#21313;&#20159;&#21442;&#25968;&#30340;&#39030;&#32423;LLMs&#65292;&#36825;&#26159;&#31227;&#21160;&#37096;&#32626;&#30340;&#23454;&#38469;&#36873;&#25321;&#12290;&#19982;&#26222;&#36941;&#30340;&#35266;&#28857;&#30456;&#21453;&#65292;&#24378;&#35843;&#25968;&#25454;&#21644;&#21442;&#25968;&#25968;&#37327;&#22312;&#30830;&#23450;&#27169;&#22411;&#36136;&#37327;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20122;&#21313;&#20159;&#35268;&#27169;LLMs&#30340;&#27169;&#22411;&#26550;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#28145;&#24230;&#21644;&#30246;&#36523;&#32467;&#26500;&#65292;&#20877;&#21152;&#19978;&#23884;&#20837;&#20849;&#20139;&#21644;&#20998;&#32452;&#26597;&#35810;&#27880;&#24847;&#26426;&#21046;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#32593;&#32476;&#65292;&#31216;&#20026;MobileLLM&#65292;&#20854;&#22312;&#23558;&#36817;125M/350M&#20808;&#36827;&#27169;&#22411;&#19978;&#20998;&#21035;&#33719;&#24471;&#20102;&#24778;&#20154;&#30340;2.7%/4.3%&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31435;&#21363;&#30340;&#22359;&#29366;&#26435;&#37325;&#20849;&#20139;&#26041;&#27861;&#65292;&#19981;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#65292;&#19988;&#20165;&#20855;&#26377;&#26497;&#23567;&#30340;&#24310;&#36831;&#24320;&#38144;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;MobileLLM-L
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14905v1 Announce Type: cross  Abstract: This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal role of data and parameter quantity in determining model quality, our investigation underscores the significance of model architecture for sub-billion scale LLMs. Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M state-of-the-art models. Additionally, we propose an immediate block-wise weight sharing approach with no increase in model size and only marginal latency overhead. The resultant models, denoted as MobileLLM-L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25918;&#23556;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#35757;&#32451;&#25968;&#25454;&#33021;&#26356;&#23481;&#26131;&#26816;&#27979;&#21040;&#65292;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#21363;&#20351;&#21482;&#26377;&#24456;&#23569;&#27604;&#20363;&#30340;&#27700;&#21360;&#35757;&#32451;&#25991;&#26412;&#65292;&#20173;&#21487;&#20197;&#39640;&#32622;&#20449;&#24230;&#22320;&#26816;&#27979;&#20986;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.14904</link><description>&lt;p&gt;
&#25968;&#23383;&#27700;&#21360;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#25918;&#23556;&#24615;
&lt;/p&gt;
&lt;p&gt;
Watermarking Makes Language Models Radioactive
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25918;&#23556;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#35757;&#32451;&#25968;&#25454;&#33021;&#26356;&#23481;&#26131;&#26816;&#27979;&#21040;&#65292;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#21363;&#20351;&#21482;&#26377;&#24456;&#23569;&#27604;&#20363;&#30340;&#27700;&#21360;&#35757;&#32451;&#25991;&#26412;&#65292;&#20173;&#21487;&#20197;&#39640;&#32622;&#20449;&#24230;&#22320;&#26816;&#27979;&#20986;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#25918;&#23556;&#24615;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#26816;&#27979;&#21040;&#36825;&#31181;&#36755;&#20837;&#34987;&#29992;&#20316;&#35757;&#32451;&#25968;&#25454;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#25104;&#21592;&#25512;&#26029;&#21487;&#20197;&#20197;&#19968;&#23450;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#36825;&#31181;&#26816;&#27979;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24102;&#26377;&#25968;&#23383;&#27700;&#21360;&#30340;&#35757;&#32451;&#25968;&#25454;&#30041;&#19979;&#30340;&#30165;&#36857;&#27604;&#25104;&#21592;&#25512;&#26029;&#26356;&#23481;&#26131;&#26816;&#27979;&#19988;&#26356;&#21487;&#38752;&#12290;&#25105;&#20204;&#23558;&#27745;&#26579;&#27700;&#24179;&#19982;&#27700;&#21360;&#30340;&#40065;&#26834;&#24615;&#12289;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#27604;&#20363;&#21644;&#24494;&#35843;&#36807;&#31243;&#32852;&#31995;&#36215;&#26469;&#12290;&#29305;&#21035;&#26159;&#25105;&#20204;&#23637;&#31034;&#65292;&#21363;&#20351;&#21482;&#26377;5&#65285;&#30340;&#35757;&#32451;&#25991;&#26412;&#34987;&#25968;&#23383;&#27700;&#21360;&#26631;&#35760;&#65292;&#35757;&#32451;&#22312;&#24102;&#26377;&#25968;&#23383;&#27700;&#21360;&#30340;&#21512;&#25104;&#25351;&#20196;&#19978;&#20173;&#28982;&#21487;&#20197;&#20855;&#26377;&#39640;&#32622;&#20449;&#24230;&#65288;p&#20540;&lt;1e-5&#65289;&#34987;&#26816;&#27979;&#21040;&#12290;&#22240;&#27492;&#65292;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;LLM&#27700;&#21360;&#25216;&#26415;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36731;&#26494;&#30830;&#23450;&#24102;&#26377;&#25968;&#23383;&#27700;&#21360;&#30340;LLM&#30340;&#36755;&#20986;&#26159;&#21542;&#34987;&#29992;&#26469;&#23545;&#21478;&#19968;&#20010;LLM&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14904v1 Announce Type: cross  Abstract: This paper investigates the radioactivity of LLM-generated texts, i.e. whether it is possible to detect that such input was used as training data. Conventional methods like membership inference can carry out this detection with some level of accuracy. We show that watermarked training data leaves traces easier to detect and much more reliable than membership inference. We link the contamination level to the watermark robustness, its proportion in the training set, and the fine-tuning process. We notably demonstrate that training on watermarked synthetic instructions can be detected with high confidence (p-value &lt; 1e-5) even when as little as 5% of training text is watermarked. Thus, LLM watermarking, originally designed for detecting machine-generated text, gives the ability to easily identify if the outputs of a watermarked LLM were used to fine-tune another LLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#36755;&#20837;&#25991;&#26412;&#36827;&#34892;tokenization&#23545;&#25968;&#20540;&#25512;&#29702;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#37319;&#29992;&#20174;&#21491;&#21040;&#24038;&#30340;tokenization&#26041;&#24335;&#21487;&#26174;&#33879;&#25552;&#39640;&#31639;&#26415;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.14903</link><description>&lt;p&gt;
Tokenization&#35745;&#25968;&#65306;Tokenization&#23545;&#21069;&#27839;LLMs&#20013;&#31639;&#26415;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#36755;&#20837;&#25991;&#26412;&#36827;&#34892;tokenization&#23545;&#25968;&#20540;&#25512;&#29702;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#37319;&#29992;&#20174;&#21491;&#21040;&#24038;&#30340;tokenization&#26041;&#24335;&#21487;&#26174;&#33879;&#25552;&#39640;&#31639;&#26415;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tokenization&#65292;&#21363;&#23558;&#36755;&#20837;&#25991;&#26412;&#20998;&#25104;&#36755;&#20837;token&#30340;&#36807;&#31243;&#65292;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#31649;&#36947;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#21487;&#33021;&#26159;&#26377;&#29992;&#30340;&#25110;&#26377;&#23475;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#26469;&#28304;&#12290;&#22312;&#21382;&#21490;&#19978;&#65292;LLMs&#20542;&#21521;&#20110;&#20351;&#29992;&#23383;&#33410;&#23545;&#32534;&#30721;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#29305;&#23450;&#30340;&#36755;&#20837;&#39046;&#22495;&#12290;&#38543;&#30528;LLMs&#29992;&#20110;&#25512;&#29702;&#30340;&#22686;&#21152;&#65292;&#21508;&#31181;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;tokenization&#26041;&#26696;&#24471;&#21040;&#20102;&#37319;&#29992;&#65292;&#20687;LLaMa&#21644;PaLM&#36825;&#26679;&#30340;&#27969;&#34892;&#27169;&#22411;&#36873;&#25321;&#20102;&#21333;&#20010;&#25968;&#23383;tokenization&#65292;&#32780;GPT-3.5&#21644;GPT-4&#20026;&#27599;&#20010;1&#20301;&#12289;2&#20301;&#21644;3&#20301;&#25968;&#23383;&#37117;&#26377;&#21333;&#29420;&#30340;token&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31639;&#26415;&#20219;&#21153;&#30740;&#31350;&#36825;&#31181;&#36873;&#25321;&#23545;&#25968;&#20540;&#25512;&#29702;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;GPT-3.5&#21644;-4&#30340;&#20174;&#24038;&#21040;&#21491;&#21644;&#20174;&#21491;&#21040;&#24038;&#30340;tokenization&#65292;&#21457;&#29616;&#20174;&#21491;&#21040;&#24038;&#30340;tokenization&#65288;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#36887;&#21495;&#20998;&#31163;&#25968;&#23383;&#65289;&#23548;&#33268;&#20102;&#22823;&#24133;&#25552;&#39640;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20351;&#29992;&#26631;&#20934;&#30340;&#20174;&#24038;&#21040;&#21491;tokenization&#26102;&#27169;&#22411;&#23384;&#22312;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14903v1 Announce Type: new  Abstract: Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases. Historically, LLMs have relied on byte pair encoding, without care to specific input domains. With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers. In this work, we study the effect this choice has on numerical reasoning through the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left tokenization (enforced by comma separating numbers at inference time) leads to largely improved performance. Furthermore, we find that model errors when using standard left-to-r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30005;&#23376;&#21830;&#21153;&#20013;&#24847;&#22270;&#29702;&#35299;&#30340;&#19968;&#20010;&#26032;&#35270;&#35282;&#65292;&#19981;&#20381;&#36182;&#20110;&#20135;&#21697;&#26412;&#20307;&#65292;&#36890;&#36807;&#24341;&#20837;&#20135;&#21697;&#24674;&#22797;&#22522;&#20934;&#39564;&#35777;&#20102;&#24403;&#21069;&#24847;&#22270;&#30693;&#35782;&#22270;&#30340;&#24369;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.14901</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#20013;&#24847;&#22270;&#29702;&#35299;&#30340;&#20351;&#29992;&#20013;&#24515;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Usage-centric Take on Intent Understanding in E-Commerce
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14901
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30005;&#23376;&#21830;&#21153;&#20013;&#24847;&#22270;&#29702;&#35299;&#30340;&#19968;&#20010;&#26032;&#35270;&#35282;&#65292;&#19981;&#20381;&#36182;&#20110;&#20135;&#21697;&#26412;&#20307;&#65292;&#36890;&#36807;&#24341;&#20837;&#20135;&#21697;&#24674;&#22797;&#22522;&#20934;&#39564;&#35777;&#20102;&#24403;&#21069;&#24847;&#22270;&#30693;&#35782;&#22270;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#26159;&#30005;&#23376;&#21830;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#24847;&#22270;&#29702;&#35299;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#23450;&#20041;&#24182;&#19981;&#19968;&#33268;&#65292;&#19988;&#32570;&#20047;&#20934;&#30830;&#30340;&#22522;&#20934;&#12290;&#26412;&#25991;&#20851;&#27880;&#23558;&#29992;&#25143;&#24847;&#22270;&#23450;&#20041;&#20026;"&#39038;&#23458;&#22914;&#20309;&#20351;&#29992;&#20135;&#21697;"&#30340;&#39044;&#27979;&#24615;&#29992;&#25143;&#24847;&#22270;&#65292;&#24182;&#23558;&#24847;&#22270;&#29702;&#35299;&#35270;&#20026;&#19968;&#39033;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#29420;&#31435;&#20110;&#20135;&#21697;&#26412;&#20307;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;FolkScope&#30340;&#20004;&#20010;&#24369;&#28857;&#65292;&#36825;&#26159;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#30005;&#23376;&#21830;&#21153;&#24847;&#22270;&#30693;&#35782;&#22270;&#65292;&#38480;&#21046;&#20102;&#20854;&#25512;&#29702;&#29992;&#25143;&#24847;&#22270;&#21644;&#25512;&#33616;&#22810;&#26679;&#26377;&#29992;&#20135;&#21697;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20135;&#21697;&#24674;&#22797;&#22522;&#20934;&#65292;&#21253;&#25324;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#20010;&#31034;&#20363;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#19978;&#36848;FolkScope&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14901v1 Announce Type: cross  Abstract: Identifying and understanding user intents is a pivotal task for E-Commerce. Despite its popularity, intent understanding has not been consistently defined or accurately benchmarked. In this paper, we focus on predicative user intents as "how a customer uses a product", and pose intent understanding as a natural language reasoning task, independent of product ontologies. We identify two weaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph, that limit its capacity to reason about user intents and to recommend diverse useful products. Following these observations, we introduce a Product Recovery Benchmark including a novel evaluation framework and an example dataset. We further validate the above FolkScope weaknesses on this benchmark.
&lt;/p&gt;</description></item><item><title>&#20102;&#35299;Chain-of-Thought&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;&#27169;&#22411;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#19982;&#24544;&#23454;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#29305;&#23450;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;130&#20159;&#21442;&#25968;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24544;&#23454;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14897</link><description>&lt;p&gt;
Chain-of-Thought&#19981;&#24544;&#35802;&#20316;&#20026;&#20266;&#35013;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Unfaithfulness as Disguised Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14897
&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;Chain-of-Thought&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;&#27169;&#22411;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#19982;&#24544;&#23454;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#29305;&#23450;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;130&#20159;&#21442;&#25968;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;Chain-of-Thought (CoT)&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;LLM&#30340;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;CoT&#24544;&#23454;&#24230;&#30340;&#20195;&#29702;&#65292;arXiv:2307.13702&#25552;&#20986;&#20102;&#19968;&#20010;&#24230;&#37327;&#27169;&#22411;&#20381;&#36182;&#20854;CoT&#29983;&#25104;&#31572;&#26696;&#30340;&#25351;&#26631;&#12290;&#22312;&#19968;&#20010;&#19987;&#26377;&#27169;&#22411;&#31995;&#21015;&#20013;&#65292;&#20182;&#20204;&#21457;&#29616;LLM&#34920;&#29616;&#20986;&#27169;&#22411;&#22823;&#23567;&#19982;&#20854;&#24544;&#23454;&#24230;&#27979;&#37327;&#20043;&#38388;&#30340;&#32553;&#25918;-&#21453;&#21521;&#32553;&#25918;&#20851;&#31995;&#65292;&#24182;&#19988;130&#20159;&#21442;&#25968;&#27169;&#22411;&#30456;&#27604;&#20110;&#23610;&#23544;&#20171;&#20110;8.1&#20159;&#21040;1750&#20159;&#21442;&#25968;&#20043;&#38388;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#22686;&#21152;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#36825;&#20123;&#32467;&#26524;&#26159;&#21542;&#20316;&#20026;&#25152;&#26377;LLM&#30340;&#29305;&#24615;&#27867;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#31995;&#21015;&#30340;&#27169;&#22411;&#22797;&#21046;&#20182;&#20204;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#24182;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#20182;&#20204;&#25253;&#21578;&#30340;CoT&#24544;&#23454;&#24230;&#30340;&#32553;&#25918;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#25913;&#21464;&#35774;&#23450;&#20250;&#23548;&#33268;&#36825;&#20123;&#27169;&#24335;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14897v1 Announce Type: cross  Abstract: Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, arXiv:2307.13702 propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate their experimental setup with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, we discover that simply changin
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#22686;&#24378;&#19981;&#36807;&#26159;&#26356;&#22909;&#22320;&#24494;&#35843;&#27169;&#22411;&#65292;&#38646;&#21761;&#24577;&#21644;&#23569;&#26679;&#26412;&#25968;&#25454;&#29983;&#25104;&#21487;&#25552;&#39640;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.14895</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#24050;&#27515;&#65292;&#25968;&#25454;&#22686;&#24378;&#19975;&#23681;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation is Dead, Long Live Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14895
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#19981;&#36807;&#26159;&#26356;&#22909;&#22320;&#24494;&#35843;&#27169;&#22411;&#65292;&#38646;&#21761;&#24577;&#21644;&#23569;&#26679;&#26412;&#25968;&#25454;&#29983;&#25104;&#21487;&#25552;&#39640;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#26159;&#19968;&#20010;&#32321;&#33635;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#19981;&#26029;&#25552;&#20986;&#26032;&#39062;&#30340;&#25216;&#26415;&#26469;&#21019;&#24314;&#20154;&#24037;&#25968;&#25454;&#65292;&#24050;&#32463;&#22312;&#23567;&#25968;&#25454;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#29575;&#65292;&#33267;&#23569;&#23545;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#32780;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#36825;&#20123;&#32467;&#26524;&#65292;&#34920;&#26126;&#32463;&#20856;&#30340;&#25968;&#25454;&#22686;&#24378;&#21482;&#26159;&#19968;&#31181;&#26356;&#22909;&#22320;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#24335;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#25968;&#25454;&#22686;&#24378;&#20043;&#21069;&#33457;&#26356;&#22810;&#26102;&#38388;&#36827;&#34892;&#24494;&#35843;&#20250;&#25269;&#28040;&#20854;&#25928;&#26524;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#36129;&#29486;&#65292;&#22240;&#20026;&#23427;&#22238;&#31572;&#20102;&#26368;&#36817;&#20960;&#24180;&#30041;&#19979;&#30340;&#20960;&#20010;&#38382;&#39064;&#65292;&#21363;&#65306;&#21738;&#31181;DA&#25216;&#26415;&#34920;&#29616;&#26368;&#20339;&#65288;&#21482;&#35201;&#23427;&#20204;&#29983;&#25104;&#30340;&#25968;&#25454;&#19982;&#35757;&#32451;&#38598;&#36275;&#22815;&#25509;&#36817;&#65292;&#19981;&#20250;&#25439;&#23475;&#35757;&#32451;&#65289;&#65292;&#20026;&#20160;&#20040;DA&#34920;&#29616;&#20986;&#31215;&#26497;&#30340;&#32467;&#26524;&#65288;&#31616;&#21270;&#32593;&#32476;&#35757;&#32451;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#23545;&#35805;&#20195;&#29702;&#65288;&#22914;ChatGPT&#25110;LLama2&#65289;&#38646;&#21761;&#24577;&#21644;&#23569;&#26679;&#26412;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20174;&#32780;&#24471;&#20986;&#20102;&#32467;&#35770;&#65292;&#27492;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14895v1 Announce Type: cross  Abstract: Textual data augmentation (DA) is a prolific field of study where novel techniques to create artificial data are regularly proposed, and that has demonstrated great efficiency on small data settings, at least for text classification tasks. In this paper, we challenge those results, showing that classical data augmentation is simply a way of performing better fine-tuning, and that spending more time fine-tuning before applying data augmentation negates its effect. This is a significant contribution as it answers several questions that were left open in recent years, namely~: which DA technique performs best (all of them as long as they generate data close enough to the training set as to not impair training) and why did DA show positive results (facilitates training of network). We furthermore show that zero and few-shot data generation via conversational agents such as ChatGPT or LLama2 can increase performances, concluding that this f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.14891</link><description>&lt;p&gt;
LLMBind: &#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMBind: A Unified Modality-Task Integration Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#38598;&#25104;&#33021;&#21147;&#26377;&#38480;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24102;&#22836;&#25506;&#32034;&#24182;&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#29992;&#20110;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30456;&#24212;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#19982;&#20219;&#21153;&#29305;&#23450;&#30340;&#26631;&#35760;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;&#22240;&#27492;&#65292;LLMBind&#21487;&#20197;&#20197;&#22810;&#31181;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#30340;&#32452;&#21512;&#35299;&#37322;&#36755;&#20837;&#24182;&#29983;&#25104;&#36755;&#20986;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#25216;&#26415;&#65292;&#36890;&#36807;&#19981;&#21516;&#19987;&#23478;&#20043;&#38388;&#30340;&#21327;&#20316;&#23454;&#29616;&#19981;&#21516;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#26377;&#25928;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;40&#19975;&#26465;&#25351;&#20196;&#25968;&#25454;&#30340;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35299;&#38145;&#20102;&#20132;&#20114;&#24335;&#35270;&#35273;&#29983;&#25104;&#21644;&#32534;&#36753;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14891v1 Announce Type: cross  Abstract: While recent progress in multimodal large language models tackles various modality tasks, they posses limited integration capabilities for complex multi-modality tasks, consequently constraining the development of the field. In this work, we take the initiative to explore and propose the LLMBind, a unified framework for modality task integration, which binds Large Language Models and corresponding pre-trained task models with task-specific tokens. Consequently, LLMBind can interpret inputs and produce outputs in versatile combinations of image, text, video, and audio. Specifically, we introduce a Mixture-of-Experts technique to enable effective learning for different multimodal tasks through collaboration among diverse experts. Furthermore, we create a multi-task dataset comprising 400k instruction data, which unlocks the ability for interactive visual generation and editing tasks. Extensive experiments show the effectiveness of our fr
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#23545;&#24615;&#33021;&#32780;&#38750;&#20219;&#21153;&#23646;&#24615;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#8220;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#8221;&#65292;&#21487;&#24110;&#21161;&#20943;&#23569;&#35780;&#20272;&#20219;&#21153;&#25968;&#37327;&#24182;&#20445;&#25345;&#39640;&#39564;&#35777;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.14890</link><description>&lt;p&gt;
Vygotsky Distance: &#29992;&#20110;&#22522;&#20934;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vygotsky Distance: Measure for Benchmark Task Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14890
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#23545;&#24615;&#33021;&#32780;&#38750;&#20219;&#21153;&#23646;&#24615;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#8220;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#8221;&#65292;&#21487;&#24110;&#21161;&#20943;&#23569;&#35780;&#20272;&#20219;&#21153;&#25968;&#37327;&#24182;&#20445;&#25345;&#39640;&#39564;&#35777;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29702;&#35770;&#24037;&#20855;&#21644;&#23454;&#36341;&#31639;&#27861;&#26469;&#35745;&#31639;&#22522;&#20934;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#31216;&#20043;&#20026;"&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;"&#12290;&#36825;&#31181;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22522;&#20110;&#8220;&#23398;&#29983;&#8221;&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#30340;&#30456;&#23545;&#34920;&#29616;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#20219;&#21153;&#26412;&#36523;&#30340;&#23646;&#24615;&#12290;&#22914;&#26524;&#20004;&#20010;&#20219;&#21153;&#22312;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#19978;&#24444;&#27492;&#25509;&#36817;&#65292;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978; tend to have similar relative performance&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20102;&#35299;&#20219;&#21153;&#20043;&#38388;&#30340;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35780;&#20272;&#20219;&#21153;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#39564;&#35777;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14890v1 Announce Type: cross  Abstract: Evaluation plays a significant role in modern natural language processing. Most modern NLP benchmarks consist of arbitrary sets of tasks that neither guarantee any generalization potential for the model once applied outside the test set nor try to minimize the resource consumption needed for model evaluation. This paper presents a theoretical instrument and a practical algorithm to calculate similarity between benchmark tasks, we call this similarity measure "Vygotsky distance". The core idea of this similarity measure is that it is based on relative performance of the "students" on a given task, rather that on the properties of the task itself. If two tasks are close to each other in terms of Vygotsky distance the models tend to have similar relative performance on them. Thus knowing Vygotsky distance between tasks one can significantly reduce the number of evaluation tasks while maintaining a high validation quality. Experiments on v
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;COBIAS&#65292;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#22810;&#26679;&#24773;&#22659;&#30340;&#29992;&#25143;&#36755;&#20837;&#20869;&#23481;&#65292;&#34913;&#37327;&#35821;&#21477;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.14889</link><description>&lt;p&gt;
COBIAS&#65306;&#20559;&#35265;&#35780;&#20272;&#20013;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
COBIAS: Contextual Reliability in Bias Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14889
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;COBIAS&#65292;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#22810;&#26679;&#24773;&#22659;&#30340;&#29992;&#25143;&#36755;&#20837;&#20869;&#23481;&#65292;&#34913;&#37327;&#35821;&#21477;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22522;&#20110;&#22266;&#26377;&#20559;&#35265;&#25968;&#25454;&#35757;&#32451;&#30340;&#12290;&#20197;&#24448;&#30340;&#21435;&#20559;&#35265;&#27169;&#22411;&#30740;&#31350;&#20381;&#36182;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30001;&#20110;&#23545;&#20559;&#35265;&#30340;&#26497;&#20854;&#20027;&#35266;&#29702;&#35299;&#32780;&#23384;&#22312;&#22810;&#20010;&#32570;&#38519;&#65292;&#20984;&#26174;&#20986;&#23545;&#24773;&#22659;&#25506;&#32034;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#36755;&#20837;&#29992;&#25143;&#20869;&#23481;&#30340;&#24773;&#22659;&#65292;&#32771;&#34385;&#21040;&#36755;&#20837;&#35821;&#21477;&#21487;&#33021;&#23384;&#22312;&#30340;&#22810;&#31181;&#24773;&#20917;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#20801;&#35768;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#30340;&#26694;&#26550;&#65292;&#32780;&#19981;&#26159;&#20260;&#23475;&#29992;&#25143;&#21442;&#19982;&#30340;&#38450;&#25252;&#35774;&#26045;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;(i) &#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;2287&#20010;&#38472;&#35789;&#28389;&#35843;&#35821;&#21477;&#20197;&#21450;&#28155;&#21152;&#24773;&#22659;&#35201;&#28857;&#30340;&#25968;&#25454;&#38598;&#65307;(ii) &#25105;&#20204;&#24320;&#21457;&#20102;&#38754;&#21521;&#24773;&#22659;&#30340;&#20559;&#35265;&#25351;&#26631;&#21644;&#35780;&#20272;&#20998;&#25968;&#65288;COBIAS&#65289;&#26469;&#35780;&#20272;&#35821;&#21477;&#22312;&#34913;&#37327;&#20559;&#35265;&#26041;&#38754;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#26159;&#34913;&#37327;&#20559;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#24773;&#22659;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#39044;&#27979;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14889v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are trained on inherently biased data. Previous works on debiasing models rely on benchmark datasets to measure model performance. However, these datasets suffer from several pitfalls due to the extremely subjective understanding of bias, highlighting a critical need for contextual exploration. We propose understanding the context of user inputs with consideration of the diverse situations in which input statements are possible. This approach would allow for frameworks that foster bias awareness rather than guardrails that hurt user engagement. Our contribution is twofold: (i) we create a dataset of 2287 stereotyped statements augmented with points for adding context; (ii) we develop the Context-Oriented Bias Indicator and Assessment Score (COBIAS) to assess statements' contextual reliability in measuring bias. Our metric is a significant predictor of the contextual reliability of bias-benchmark datasets ($
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#22270;&#32467;&#26500;&#30340;&#39640;&#25928;&#25968;&#25454;&#36873;&#25321;&#26426;&#21046;&#65292;&#21487;&#22312;&#19981;&#32463;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#27169;&#22411;&#25110;&#20854;&#20182;&#23494;&#38598;&#30340;&#39044;&#22788;&#29702;&#36716;&#25442;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.14888</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#22270;&#32467;&#26500;&#36827;&#34892;&#39640;&#25928;&#25968;&#25454;&#36873;&#25321;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient data selection employing Semantic Similarity-based Graph Structures for model training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14888
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#22270;&#32467;&#26500;&#30340;&#39640;&#25928;&#25968;&#25454;&#36873;&#25321;&#26426;&#21046;&#65292;&#21487;&#22312;&#19981;&#32463;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#27169;&#22411;&#25110;&#20854;&#20182;&#23494;&#38598;&#30340;&#39044;&#22788;&#29702;&#36716;&#25442;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#20984;&#26174;&#20102;&#27169;&#22411;&#20934;&#30830;&#25429;&#25417;&#25991;&#26412;&#20449;&#24687;&#25152;&#38656;&#22823;&#37327;&#25968;&#25454;&#30340;&#24517;&#35201;&#24615;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#35757;&#32451;&#27492;&#31867;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;SeSaME&#8221;&#30340;&#25968;&#25454;&#36873;&#25321;&#26426;&#21046;&#65292;&#23427;&#20165;&#22522;&#20110;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#39640;&#25928;&#30340;&#25968;&#25454;&#37319;&#26679;&#65292;&#26080;&#38656;&#36890;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#27169;&#22411;&#25110;&#20854;&#20182;&#23494;&#38598;&#30340;&#39044;&#22788;&#29702;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14888v1 Announce Type: cross  Abstract: Recent developments in natural language processing (NLP) have highlighted the need for substantial amounts of data for models to capture textual information accurately. This raises concerns regarding the computational resources and time required for training such models. This paper introduces Semantics for data SAliency in Model performance Estimation (SeSaME). It is an efficient data sampling mechanism solely based on textual information without passing the data through a compute-heavy model or other intensive pre-processing transformations. The application of this approach is demonstrated in the use case of low-resource automated speech recognition (ASR) models, which excessively rely on text-to-speech (TTS) calls when using augmented data. SeSaME learns to categorize new incoming data points into speech recognition difficulty buckets by employing semantic similarity-based graph structures and discrete ASR information from homophilou
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;ChatGPT&#30340;&#20316;&#24330;&#23545;&#27979;&#35797;&#39064;&#30340;&#28431;&#27934;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#36776;&#21035;&#27979;&#35797;&#39064;&#20013;&#23545;ChatGPT&#26368;&#23481;&#26131;&#22238;&#31572;&#38169;&#35823;&#30340;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14881</link><description>&lt;p&gt;
&#23545;&#25239;&#22522;&#20110;ChatGPT&#20316;&#24330;&#30340;&#27979;&#35797;&#39064;&#28431;&#27934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on the Vulnerability of Test Questions against ChatGPT-based Cheating
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14881
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;ChatGPT&#30340;&#20316;&#24330;&#23545;&#27979;&#35797;&#39064;&#30340;&#28431;&#27934;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#36776;&#21035;&#27979;&#35797;&#39064;&#20013;&#23545;ChatGPT&#26368;&#23481;&#26131;&#22238;&#31572;&#38169;&#35823;&#30340;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#21487;&#20197;&#30456;&#24403;&#20934;&#30830;&#22320;&#22238;&#31572;&#25991;&#26412;&#25552;&#31034;&#65292;&#29978;&#33267;&#22312;&#30740;&#31350;&#29983;&#32423;&#21035;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#35768;&#22810;&#25945;&#32946;&#24037;&#20316;&#32773;&#21457;&#29616;&#20182;&#20204;&#30340;&#35838;&#19994;&#25110;&#36828;&#31243;&#27979;&#35797;&#21644;&#32771;&#35797;&#23481;&#26131;&#21463;&#21040;&#22522;&#20110;ChatGPT&#30340;&#20316;&#24330;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#23398;&#29983;&#21487;&#33021;&#30452;&#25509;&#20351;&#29992;ChatGPT&#31561;&#24037;&#20855;&#25552;&#20379;&#30340;&#31572;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#22238;&#31572;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;ChatGPT&#33021;&#22810;&#22909;&#22238;&#31572;&#27979;&#35797;&#39064;&#65292;&#20197;&#21450;&#25105;&#20204;&#22914;&#20309;&#26816;&#27979;&#27979;&#35797;&#39064;&#26159;&#21542;&#33021;&#34987;ChatGPT&#27491;&#30830;&#22238;&#31572;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;ChatGPT&#23545;MedMCQA&#25968;&#25454;&#38598;&#30340;&#21709;&#24212;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#36229;&#36807;10,000&#20010;&#21307;&#23398;&#38498;&#20837;&#23398;&#32771;&#35797;&#38382;&#39064;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#22238;&#31572;&#65292;&#24182;&#25581;&#31034;&#20102;ChatGPT&#22312;&#26576;&#20123;&#38382;&#39064;&#19978;&#30340;&#22238;&#31572;&#27604;&#20854;&#20182;&#38382;&#39064;&#26356;&#19981;&#20934;&#30830;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#26412;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19968;&#32452;&#38382;&#39064;&#25110;&#26679;&#26412;&#32771;&#35797;&#20013;&#31579;&#36873;&#20986;&#23545;ChatGPT&#26368;&#26131;&#21463;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#21487;&#20197;&#34987;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14881v1 Announce Type: cross  Abstract: ChatGPT is a chatbot that can answer text prompts fairly accurately, even performing very well on postgraduate-level questions. Many educators have found that their take-home or remote tests and exams are vulnerable to ChatGPT-based cheating because students may directly use answers provided by tools like ChatGPT. In this paper, we try to provide an answer to an important question: how well ChatGPT can answer test questions and how we can detect whether the questions of a test can be answered correctly by ChatGPT. We generated ChatGPT's responses to the MedMCQA dataset, which contains over 10,000 medical school entrance exam questions. We analyzed the responses and uncovered certain types of questions ChatGPT answers more inaccurately than others. In addition, we have created a basic natural language processing model to single out the most vulnerable questions to ChatGPT in a collection of questions or a sample exam. Our tool can be us
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#30452;&#26041;&#22270;&#21487;&#35270;&#21270;&#24037;&#20855;AutoHistograms&#65292;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#30456;&#20851;&#29305;&#24449;&#12289;&#20197;&#30452;&#26041;&#22270;&#24418;&#24335;&#23637;&#31034;&#24182;&#20801;&#35768;&#29992;&#25143;&#20132;&#20114;&#24335;&#22320;&#26597;&#35810;&#25968;&#25454;&#38598;&#65292;&#24110;&#21161;&#25968;&#25454;&#24037;&#20316;&#32773;&#24555;&#36895;&#25506;&#32034;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.14880</link><description>&lt;p&gt;
&#33258;&#21160;&#30452;&#26041;&#22270;&#65306;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#25968;&#25454;&#38598;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Automatic Histograms: Leveraging Language Models for Text Dataset Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#30452;&#26041;&#22270;&#21487;&#35270;&#21270;&#24037;&#20855;AutoHistograms&#65292;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#30456;&#20851;&#29305;&#24449;&#12289;&#20197;&#30452;&#26041;&#22270;&#24418;&#24335;&#23637;&#31034;&#24182;&#20801;&#35768;&#29992;&#25143;&#20132;&#20114;&#24335;&#22320;&#26597;&#35810;&#25968;&#25454;&#38598;&#65292;&#24110;&#21161;&#25968;&#25454;&#24037;&#20316;&#32773;&#24555;&#36895;&#25506;&#32034;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#38598;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25968;&#25454;&#24037;&#20316;&#20154;&#21592;&#24120;&#24120;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#25688;&#35201;&#65292;&#29305;&#21035;&#26159;&#21508;&#31181;&#27966;&#29983;&#29305;&#24449;&#30340;&#20998;&#24067;&#12290;&#19968;&#20123;&#29305;&#24449;&#65292;&#22914;&#27602;&#24615;&#25110;&#20027;&#39064;&#65292;&#23545;&#35768;&#22810;&#25968;&#25454;&#38598;&#37117;&#26377;&#24433;&#21709;&#65292;&#20294;&#35768;&#22810;&#26377;&#36259;&#30340;&#29305;&#24449;&#26159;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#65306;&#38899;&#20048;&#25968;&#25454;&#38598;&#30340;&#20048;&#22120;&#21644;&#27969;&#27966;&#65292;&#25110;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#30142;&#30149;&#21644;&#30151;&#29366;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#24037;&#20316;&#32773;&#32463;&#24120;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#36816;&#34892;&#33258;&#23450;&#20041;&#20998;&#26512;&#65292;&#36825;&#26082;&#32321;&#29712;&#21448;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AutoHistograms&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;LLM&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#12290;AutoHistograms&#33258;&#21160;&#35782;&#21035;&#30456;&#20851;&#29305;&#24449;&#65292;&#29992;&#30452;&#26041;&#22270;&#24418;&#24335;&#23637;&#31034;&#23427;&#20204;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#20132;&#20114;&#24335;&#22320;&#26597;&#35810;&#25968;&#25454;&#38598;&#30340;&#23454;&#20307;&#31867;&#21035;&#24182;&#21019;&#24314;&#26032;&#30340;&#30452;&#26041;&#22270;&#12290;&#22312;&#19982;10&#21517;&#25968;&#25454;&#24037;&#20316;&#32773;&#65288;n=10&#65289;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#21442;&#19982;&#32773;&#21487;&#20197;&#24555;&#36895;&#21033;&#29992;AutoHistograms&#35782;&#21035;&#35265;&#35299;&#24182;&#25506;&#32034;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14880v1 Announce Type: cross  Abstract: Making sense of unstructured text datasets is perennially difficult, yet increasingly relevant with Large Language Models. Data workers often rely on dataset summaries, especially distributions of various derived features. Some features, like toxicity or topics, are relevant to many datasets, but many interesting features are domain specific: instruments and genres for a music dataset, or diseases and symptoms for a medical dataset. Accordingly, data workers often run custom analyses for each dataset, which is cumbersome and difficult. We present AutoHistograms, a visualization tool leveragingLLMs. AutoHistograms automatically identifies relevant features, visualizes them with histograms, and allows the user to interactively query the dataset for categories of entities and create new histograms. In a user study with 10 data workers (n=10), we observe that participants can quickly identify insights and explore the data using AutoHistogr
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#20540;&#65292;&#22312;&#35270;&#39057;&#28216;&#25103;&#35282;&#33394;&#24320;&#21457;&#20013;&#20195;&#34920;&#32473;&#23450;&#30340;&#20154;&#26684;&#29305;&#24449;&#65292;&#22686;&#24378;&#28216;&#25103;&#35282;&#33394;&#30340;&#31867;&#20154;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14879</link><description>&lt;p&gt;
&#20197;&#20154;&#26684;&#39537;&#21160;&#29983;&#25104;&#24335;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Driving Generative Agents With Their Personality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14879
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#20540;&#65292;&#22312;&#35270;&#39057;&#28216;&#25103;&#35282;&#33394;&#24320;&#21457;&#20013;&#20195;&#34920;&#32473;&#23450;&#30340;&#20154;&#26684;&#29305;&#24449;&#65292;&#22686;&#24378;&#28216;&#25103;&#35282;&#33394;&#30340;&#31867;&#20154;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#20540;&#65292;&#29305;&#21035;&#26159;&#20154;&#26684;&#20449;&#24687;&#65292;&#22312;&#35270;&#39057;&#28216;&#25103;&#35282;&#33394;&#24320;&#21457;&#32972;&#26223;&#19979;&#30340;&#28508;&#21147;&#12290;&#24773;&#24863;&#35745;&#31639;&#65288;AC&#65289;&#31995;&#32479;&#37327;&#21270;&#20102;&#38750;&#29609;&#23478;&#35282;&#33394;&#65288;NPC&#65289;&#30340;&#24515;&#29702;&#65292;LLM&#21487;&#20197;&#21033;&#29992;&#35813;&#31995;&#32479;&#30340;&#20449;&#24687;&#65292;&#20351;&#29992;&#20540;&#36827;&#34892;&#25552;&#31034;&#29983;&#25104;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#21487;&#20197;&#22987;&#32456;&#20195;&#34920;&#32473;&#23450;&#30340;&#20154;&#26684;&#29305;&#24449;&#65292;&#20174;&#32780;&#22686;&#24378;&#28216;&#25103;&#35282;&#33394;&#30340;&#31867;&#20154;&#29305;&#24615;&#12290;&#23558;&#20154;&#31867;&#26816;&#26597;&#37325;&#26032;&#29992;&#20110;&#35780;&#20272;LLM&#30340;&#22269;&#38469;&#20154;&#26684;&#39033;&#30446;&#27744;&#65288;IPIP&#65289;&#38382;&#21367;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#29983;&#25104;&#19982;&#25152;&#25552;&#20379;&#20154;&#26684;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#30340;&#25913;&#36827;&#65292;&#22914;&#26368;&#26032;&#30340;GPT-4&#27169;&#22411;&#65292;&#21487;&#20197;&#22987;&#32456;&#21033;&#29992;&#21644;&#35299;&#37322;&#20154;&#26684;&#20197;&#20195;&#34920;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14879v1 Announce Type: cross  Abstract: This research explores the potential of Large Language Models (LLMs) to utilize psychometric values, specifically personality information, within the context of video game character development. Affective Computing (AC) systems quantify a Non-Player character's (NPC) psyche, and an LLM can take advantage of the system's information by using the values for prompt generation. The research shows an LLM can consistently represent a given personality profile, thereby enhancing the human-like characteristics of game characters. Repurposing a human examination, the International Personality Item Pool (IPIP) questionnaire, to evaluate an LLM shows that the model can accurately generate content concerning the personality provided. Results show that the improvement of LLM, such as the latest GPT-4 model, can consistently utilize and interpret a personality to represent behavior.
&lt;/p&gt;</description></item><item><title>&#35843;&#26597;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;&#65292;&#23588;&#20854;&#23545;&#19982;&#40657;&#20154;&#22899;&#24615;&#30456;&#20851;&#30340;&#21517;&#23383;&#34920;&#29616;&#26368;&#19981;&#21033;&#12290;&#23457;&#35745;&#22312;&#27169;&#22411;&#37096;&#32626;&#21644;&#23454;&#26045;&#26102;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#24378;&#35843;&#12290;</title><link>https://arxiv.org/abs/2402.14875</link><description>&lt;p&gt;
&#21517;&#23383;&#30340;&#21547;&#20041;&#26159;&#20160;&#20040;&#65311;&#23457;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
What's in a Name? Auditing Large Language Models for Race and Gender Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14875
&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;&#65292;&#23588;&#20854;&#23545;&#19982;&#40657;&#20154;&#22899;&#24615;&#30456;&#20851;&#30340;&#21517;&#23383;&#34920;&#29616;&#26368;&#19981;&#21033;&#12290;&#23457;&#35745;&#22312;&#27169;&#22411;&#37096;&#32626;&#21644;&#23454;&#26045;&#26102;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#24378;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37319;&#29992;&#23457;&#35745;&#35774;&#35745;&#26469;&#35843;&#26597;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#21253;&#25324;GPT-4&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#21457;&#27169;&#22411;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#20026;&#20010;&#20154;&#25552;&#20379;&#24314;&#35758;&#65292;&#27604;&#22914;&#22312;&#36141;&#36710;&#35848;&#21028;&#25110;&#36873;&#20030;&#32467;&#26524;&#39044;&#27979;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#35813;&#24314;&#35758;&#31995;&#32479;&#24615;&#22320;&#23545;&#19982;&#31181;&#26063;&#23569;&#25968;&#32676;&#20307;&#21644;&#22899;&#24615;&#24120;&#35265;&#30456;&#20851;&#30340;&#21517;&#23383;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#19982;&#40657;&#20154;&#22899;&#24615;&#30456;&#20851;&#30340;&#21517;&#23383;&#24471;&#21040;&#30340;&#32467;&#26524;&#26368;&#19981;&#21033;&#12290;&#36825;&#20123;&#20559;&#35265;&#22312;42&#20010;&#25552;&#31034;&#27169;&#26495;&#21644;&#22810;&#20010;&#27169;&#22411;&#20013;&#37117;&#26159;&#19968;&#33268;&#30340;&#65292;&#34920;&#26126;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#24615;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#23396;&#31435;&#20107;&#20214;&#12290;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#25968;&#20540;&#12289;&#19982;&#20915;&#31574;&#30456;&#20851;&#30340;&#38170;&#28857;&#21487;&#20197;&#25104;&#21151;&#25269;&#28040;&#20559;&#35265;&#65292;&#32780;&#23450;&#24615;&#32454;&#33410;&#30340;&#24433;&#21709;&#24182;&#19981;&#19968;&#33268;&#65292;&#29978;&#33267;&#21487;&#33021;&#20250;&#21152;&#21095;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#37096;&#32626;&#21644;&#23454;&#26045;&#26102;&#36827;&#34892;&#23457;&#35745;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#20943;&#36731;&#20854;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14875v1 Announce Type: cross  Abstract: We employ an audit design to investigate biases in state-of-the-art large language models, including GPT-4. In our study, we elicit prompt the models for advice regarding an individual across a variety of scenarios, such as during car purchase negotiations or election outcome predictions. We find that the advice systematically disadvantages names that are commonly associated with racial minorities and women. Names associated with Black women receive the least advantageous outcomes. The biases are consistent across 42 prompt templates and several models, indicating a systemic issue rather than isolated incidents. While providing numerical, decision-relevant anchors in the prompt can successfully counteract the biases, qualitative details have inconsistent effects and may even increase disparities. Our findings underscore the importance of conducting audits at the point of LLM deployment and implementation to mitigate their potential for
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65288;DCD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#23545;&#27604;&#25552;&#31034;&#19982;&#33976;&#39311;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#23545;&#27604;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14874</link><description>&lt;p&gt;
&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65306;&#21033;&#29992;&#23545;&#27604;&#35299;&#30721;&#21644;&#33976;&#39311;&#25552;&#21319;LLM&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14874
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65288;DCD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#23545;&#27604;&#25552;&#31034;&#19982;&#33976;&#39311;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#23545;&#27604;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65288;DCD&#65289;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;&#20110;&#36739;&#23567;&#30340;&#19994;&#20313;&#27169;&#22411;&#25110;&#38544;&#34255;&#29366;&#24577;&#24046;&#24322;&#20998;&#26512;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;DCD&#37319;&#29992;&#20102;&#23545;&#27604;&#24335;&#24605;&#32500;&#24341;&#23548;&#21644;&#20808;&#36827;&#30340;&#33976;&#39311;&#25216;&#26415;&#65292;&#21253;&#25324;Dropout&#21644;&#37327;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#23545;&#27604;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#23616;&#38480;&#24615;&#65292;&#21518;&#32773;&#36890;&#24120;&#38656;&#35201;&#19987;&#23478;&#21644;&#19994;&#20313;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#21152;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;&#36890;&#36807;&#23558;&#23545;&#27604;&#25552;&#31034;&#19982;&#33976;&#39311;&#30456;&#32467;&#21512;&#65292;DCD&#28040;&#38500;&#20102;&#23545;&#19994;&#20313;&#27169;&#22411;&#30340;&#38656;&#27714;&#24182;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;DCD&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#21508;&#31181;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;GSM8K&#21644;StrategyQA&#25968;&#25454;&#38598;&#20013;&#22343;&#36229;&#36807;&#20102;CD&#21644;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14874v1 Announce Type: cross  Abstract: We propose a straightforward approach called Distillation Contrastive Decoding (DCD) to enhance the reasoning capabilities of Large Language Models (LLMs) during inference. In contrast to previous approaches that relied on smaller amateur models or analysis of hidden state differences, DCD employs Contrastive Chain-of-thought Prompting and advanced distillation techniques, including Dropout and Quantization. This approach effectively addresses the limitations of Contrastive Decoding (CD), which typically requires both an expert and an amateur model, thus increasing computational resource demands. By integrating contrastive prompts with distillation, DCD obviates the need for an amateur model and reduces memory usage. Our evaluations demonstrate that DCD significantly enhances LLM performance across a range of reasoning benchmarks, surpassing both CD and existing methods in the GSM8K and StrategyQA datasets.
&lt;/p&gt;</description></item><item><title>Checkfor.ai AI&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#21306;&#20998;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#32534;&#20889;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#25552;&#20986;&#20102;&#30828;&#36127;&#25366;&#25496;&#19982;&#21512;&#25104;&#38236;&#20687;&#35757;&#32451;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14873</link><description>&lt;p&gt;
Checkfor.ai AI&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#22120;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Technical Report on the Checkfor.ai AI-Generated Text Classifier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14873
&lt;/p&gt;
&lt;p&gt;
Checkfor.ai AI&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#21306;&#20998;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#32534;&#20889;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#25552;&#20986;&#20102;&#30828;&#36127;&#25366;&#25496;&#19982;&#21512;&#25104;&#38236;&#20687;&#35757;&#32451;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Checkfor.ai&#25991;&#26412;&#20998;&#31867;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#32463;&#36807;&#35757;&#32451;&#21487;&#20197;&#21306;&#20998;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#20889;&#30340;&#25991;&#26412;&#21644;&#30001;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#12290;Checkfor.ai&#22312;&#30001;&#21313;&#31181;&#25991;&#26412;&#39046;&#22495;&#65288;&#23398;&#29983;&#20889;&#20316;&#12289;&#21019;&#24847;&#20889;&#20316;&#12289;&#31185;&#23398;&#20889;&#20316;&#12289;&#20070;&#31821;&#12289;&#30334;&#31185;&#20840;&#20070;&#12289;&#26032;&#38395;&#12289;&#30005;&#23376;&#37038;&#20214;&#12289;&#31185;&#23398;&#35770;&#25991;&#12289;&#31616;&#31572;&#38382;&#31572;&#65289;&#21644;8&#20010;&#24320;&#28304;&#38381;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32452;&#25104;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#34920;&#29616;&#20248;&#20110;&#38646;&#20914;&#20987;&#26041;&#27861;&#22914;DetectGPT&#20197;&#21450;&#20027;&#27969;&#21830;&#19994;AI&#26816;&#27979;&#24037;&#20855;&#65292;&#35823;&#24046;&#29575;&#38477;&#20302;&#20102;9&#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#31639;&#27861;&#65292;&#21363;&#30828;&#36127;&#25366;&#25496;&#19982;&#21512;&#25104;&#38236;&#20687;&#65292;&#20351;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#33021;&#22815;&#22312;&#35780;&#35770;&#31561;&#39640;&#25968;&#25454;&#39046;&#22495;&#23454;&#29616;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#26356;&#20302;&#35823;&#25253;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Checkfor.ai&#19981;&#23545;&#38750;&#27597;&#35821;&#33521;&#35821;&#20154;&#22763;&#20135;&#29983;&#20559;&#35265;&#65292;&#24182;&#25512;&#24191;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#30340;&#39046;&#22495;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14873v1 Announce Type: cross  Abstract: We present the Checkfor.ai text classifier, a transformer-based neural network trained to distinguish text written by large language models from text written by humans. Checkfor.ai outperforms zero-shot methods such as DetectGPT as well as leading commercial AI detection tools with over 9 times lower error rates on a comprehensive benchmark comprised of ten text domains (student writing, creative writing, scientific writing, books, encyclopedias, news, email, scientific papers, short-form Q\&amp;A) and 8 open- and closed-source large language models. We propose a training algorithm, hard negative mining with synthetic mirrors, that enables our classifier to achieve orders of magnitude lower false positive rates on high-data domains such as reviews. Finally, we show that Checkfor.ai is not biased against nonnative English speakers and generalizes to domains and models unseen during training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#38236;&#20687;&#36234;&#29425;&#65288;SMJ&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22312;&#35821;&#20041;&#19978;&#31867;&#20284;&#20110;&#21407;&#22987;&#38382;&#39064;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;LLMs&#12290;</title><link>https://arxiv.org/abs/2402.14872</link><description>&lt;p&gt;
&#35821;&#20041;&#38236;&#20687;&#36234;&#29425;:&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#38024;&#23545;&#24320;&#28304;LLM&#30340;&#36234;&#29425;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#38236;&#20687;&#36234;&#29425;&#65288;SMJ&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22312;&#35821;&#20041;&#19978;&#31867;&#20284;&#20110;&#21407;&#22987;&#38382;&#39064;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#29992;&#20110;&#21019;&#24847;&#20889;&#20316;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#65292;&#26681;&#25454;&#36755;&#20837;&#24207;&#21015;&#29983;&#25104;&#25991;&#26412;&#65292;&#20294;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#20250;&#23548;&#33268;&#26377;&#23475;&#36755;&#20986;&#12290;&#22823;&#22810;&#25968;&#36234;&#29425;&#25552;&#31034;&#26041;&#27861;&#20351;&#29992;&#19968;&#32452;&#36234;&#29425;&#27169;&#26495;&#65292;&#28982;&#21518;&#36319;&#38543;&#25552;&#20986;&#38382;&#39064;&#65292;&#21019;&#24314;&#36234;&#29425;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36234;&#29425;&#25552;&#31034;&#35774;&#35745;&#36890;&#24120;&#23384;&#22312;&#36807;&#22810;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#23548;&#33268;&#26080;&#27861;&#25269;&#24481;&#20351;&#29992;&#31616;&#21333;&#35821;&#20041;&#24230;&#37327;&#20316;&#20026;&#38408;&#20540;&#30340;&#38450;&#24481;&#12290;&#36234;&#29425;&#25552;&#31034;&#22312;&#35821;&#20041;&#19978;&#27604;&#29992;&#20110;&#26597;&#35810;&#30340;&#21407;&#22987;&#38382;&#39064;&#26356;&#21152;&#22810;&#26679;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#35821;&#20041;&#38236;&#20687;&#36234;&#29425;&#65288;SMJ&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22312;&#35821;&#20041;&#19978;&#31867;&#20284;&#20110;&#21407;&#22987;&#38382;&#39064;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;LLMs&#12290;&#25105;&#20204;&#23558;&#23547;&#25214;&#26082;&#28385;&#36275;&#35821;&#20041;&#30456;&#20284;&#24615;&#21448;&#20855;&#26377;&#36234;&#29425;&#26377;&#25928;&#24615;&#30340;&#36234;&#29425;&#25552;&#31034;&#24314;&#27169;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14872v1 Announce Type: cross  Abstract: Large Language Models (LLMs), used in creative writing, code generation, and translation, generate text based on input sequences but are vulnerable to jailbreak attacks, where crafted prompts induce harmful outputs. Most jailbreak prompt methods use a combination of jailbreak templates followed by questions to ask to create jailbreak prompts. However, existing jailbreak prompt designs generally suffer from excessive semantic differences, resulting in an inability to resist defenses that use simple semantic metrics as thresholds. Jailbreak prompts are semantically more varied than the original questions used for queries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approach that bypasses LLMs by generating jailbreak prompts that are semantically similar to the original question. We model the search for jailbreak prompts that satisfy both semantic similarity and jailbreak validity as a multi-objective optimization proble
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;LLMs&#19982;&#25552;&#31034;&#24037;&#31243;&#21644;&#22810;Agent&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#32467;&#26500;&#30340;&#26032;&#25991;&#26723;&#12290;</title><link>https://arxiv.org/abs/2402.14871</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#22810;Agent&#29983;&#25104;&#20844;&#20849;&#34892;&#25919;&#39046;&#22495;&#35821;&#20041;&#27169;&#26495;&#20013;&#30340;&#21322;&#32467;&#26500;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14871
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;LLMs&#19982;&#25552;&#31034;&#24037;&#31243;&#21644;&#22810;Agent&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#32467;&#26500;&#30340;&#26032;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#30340;&#25968;&#23383;&#21270;&#36807;&#31243;&#20013;&#65292;&#21508;&#20010;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#20844;&#20849;&#34892;&#25919;&#39046;&#22495;&#20013;&#25991;&#26723;&#30340;&#21019;&#24314;&#21644;&#31649;&#29702;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#12290;&#21322;&#32467;&#26500;&#25991;&#26723;&#38656;&#35201;&#22788;&#29702;&#19968;&#31995;&#21015;&#29305;&#23450;&#25968;&#25454;&#20294;&#27809;&#26377;&#22266;&#23450;&#26684;&#24335;&#65292;&#22240;&#27492;&#19981;&#33021;&#20351;&#29992;&#22522;&#20110;&#27169;&#26495;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;LLMs&#19982;&#25552;&#31034;&#24037;&#31243;&#21644;&#22810;Agent&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#31526;&#21512;&#26399;&#26395;&#32467;&#26500;&#30340;&#26032;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14871v1 Announce Type: cross  Abstract: In the last years' digitalization process, the creation and management of documents in various domains, particularly in Public Administration (PA), have become increasingly complex and diverse. This complexity arises from the need to handle a wide range of document types, often characterized by semi-structured forms. Semi-structured documents present a fixed set of data without a fixed format. As a consequence, a template-based solution cannot be used, as understanding a document requires the extraction of the data structure. The recent introduction of Large Language Models (LLMs) has enabled the creation of customized text output satisfying user requests. In this work, we propose a novel approach that combines the LLMs with prompt engineering and multi-agent systems for generating new documents compliant with a desired structure. The main contribution of this work concerns replacing the commonly used manual prompting with a task descr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#19981;&#21516;&#30340;&#21152;&#26435;&#29305;&#24449;&#26041;&#27861;&#65288;&#20108;&#20803;&#21644;&#35789;&#39057;&#21152;&#26435;&#65289;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;&#20572;&#29992;&#35789;&#26102;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#35780;&#20272;&#20934;&#30830;&#24615;&#12289;&#21484;&#22238;&#29575;&#12289;&#31934;&#30830;&#24230;&#21644;F-&#24230;&#37327;&#20540;&#65292;&#32467;&#26524;&#34920;&#26126;&#20572;&#29992;&#35789;&#30340;&#22788;&#29702;&#26041;&#24335;&#23545;&#25991;&#26412;&#20998;&#31867;&#32467;&#26524;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.14867</link><description>&lt;p&gt;
&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;&#20572;&#29992;&#35789;&#23545;&#38463;&#25289;&#20271;&#25991;&#26412;&#20998;&#31867;&#30340;&#21152;&#26435;&#26041;&#27861;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effects of term weighting approach with and without stop words removing on Arabic text classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#19981;&#21516;&#30340;&#21152;&#26435;&#29305;&#24449;&#26041;&#27861;&#65288;&#20108;&#20803;&#21644;&#35789;&#39057;&#21152;&#26435;&#65289;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;&#20572;&#29992;&#35789;&#26102;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#35780;&#20272;&#20934;&#30830;&#24615;&#12289;&#21484;&#22238;&#29575;&#12289;&#31934;&#30830;&#24230;&#21644;F-&#24230;&#37327;&#20540;&#65292;&#32467;&#26524;&#34920;&#26126;&#20572;&#29992;&#35789;&#30340;&#22788;&#29702;&#26041;&#24335;&#23545;&#25991;&#26412;&#20998;&#31867;&#32467;&#26524;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#25991;&#26412;&#26159;&#19968;&#31181;&#23558;&#25991;&#26723;&#20998;&#31867;&#20026;&#39044;&#20808;&#24314;&#31435;&#30340;&#32676;&#32452;&#30340;&#26041;&#27861;&#12290;&#22312;&#20998;&#31867;&#20043;&#21069;&#65292;&#25991;&#26412;&#25991;&#26723;&#24517;&#39035;&#20197;&#36866;&#21512;&#25968;&#25454;&#25366;&#25496;&#25152;&#20351;&#29992;&#30340;&#31639;&#27861;&#30340;&#26041;&#24335;&#36827;&#34892;&#20934;&#22791;&#21644;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25991;&#29486;&#20013;&#24050;&#32463;&#21019;&#24314;&#20102;&#35768;&#22810;&#26415;&#35821;&#21152;&#26435;&#31574;&#30053;&#26469;&#22686;&#24378;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#30340;&#21151;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20108;&#20803;&#21644;&#35789;&#39057;&#21152;&#26435;&#29305;&#24449;&#26041;&#27861;&#23545;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#19968;&#27425;&#21024;&#38500;&#20572;&#29992;&#35789;&#21644;&#19981;&#21024;&#38500;&#20572;&#29992;&#35789;&#12290;&#20026;&#20102;&#35780;&#20272;&#20808;&#21069;&#29305;&#24449;&#21152;&#26435;&#26041;&#27861;&#23545;&#20998;&#31867;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;322&#20221;&#25991;&#26723;&#30340;&#38463;&#25289;&#20271;&#25968;&#25454;&#38598;&#65292;&#20998;&#20026;&#20845;&#20010;&#20027;&#39064;&#65288;&#20892;&#19994;&#12289;&#32463;&#27982;&#12289;&#20581;&#24247;&#12289;&#25919;&#27835;&#12289;&#31185;&#23398;&#21644;&#20307;&#32946;&#65289;&#65292;&#27599;&#20010;&#20027;&#39064;&#21253;&#21547;50&#20221;&#25991;&#26723;&#65292;&#21807;&#29420;&#20581;&#24247;&#31867;&#21035;&#38500;&#22806;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14867v1 Announce Type: cross  Abstract: Classifying text is a method for categorizing documents into pre-established groups. Text documents must be prepared and represented in a way that is appropriate for the algorithms used for data mining prior to classification. As a result, a number of term weighting strategies have been created in the literature to enhance text categorization algorithms' functionality. This study compares the effects of Binary and Term frequency weighting feature methodologies on the text's classification method when stop words are eliminated once and when they are not. In recognition of assessing the effects of prior weighting of features approaches on classification results in terms of accuracy, recall, precision, and F-measure values, we used an Arabic data set made up of 322 documents divided into six main topics (agriculture, economy, health, politics, science, and sport), each of which contains 50 documents, with the exception of the health categ
&lt;/p&gt;</description></item><item><title>APTQ&#25552;&#20986;&#20102;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#21518;&#35757;&#32451;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#22312;&#38646;-shot&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;</title><link>https://arxiv.org/abs/2402.14866</link><description>&lt;p&gt;
APTQ: &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#21518;&#35757;&#32451;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14866
&lt;/p&gt;
&lt;p&gt;
APTQ&#25552;&#20986;&#20102;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#21518;&#35757;&#32451;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#22312;&#38646;-shot&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#39640;&#35745;&#31639;&#36127;&#36733;&#21644;&#24040;&#22823;&#30340;&#27169;&#22411;&#23610;&#23544;&#23545;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#26500;&#25104;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;LLMs&#30340;APTQ&#65288;Attention-aware Post-Training Mixed-Precision Quantization&#65289;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#32771;&#34385;&#20102;&#27599;&#23618;&#26435;&#37325;&#30340;&#20108;&#38454;&#20449;&#24687;&#65292;&#32780;&#19988;&#39318;&#27425;&#32771;&#34385;&#20102;&#27880;&#24847;&#21147;&#36755;&#20986;&#23545;&#25972;&#20010;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#24433;&#21709;&#12290;&#25105;&#20204;&#21033;&#29992;Hessian&#36857;&#20316;&#20026;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25935;&#24863;&#24230;&#24230;&#37327;&#65292;&#30830;&#20445;&#32463;&#36807;&#29702;&#24615;&#30340;&#31934;&#24230;&#38477;&#20302;&#33021;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;APTQ&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#22312;C4&#25968;&#25454;&#38598;&#20013;&#20197;&#24179;&#22343;4&#20301;&#23485;&#24230;&#33719;&#24471;5.22&#22256;&#24785;&#24230;&#65292;&#20960;&#20046;&#31561;&#25928;&#20110;&#20840;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;APTQ&#22312;LLaMa-7B&#21644;LLaMa-1&#20013;&#20197;&#24179;&#22343;3.8&#20301;&#23485;&#24230;&#36798;&#21040;&#20102;68.24&#65285;&#21644;70.48&#65285;&#30340;&#26368;&#20808;&#36827;&#38646;-shot&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14866v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have greatly advanced the natural language processing paradigm. However, the high computational load and huge model sizes pose a grand challenge for deployment on edge devices. To this end, we propose APTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs, which considers not only the second-order information of each layer's weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model. We leverage the Hessian trace as a sensitivity metric for mixed-precision quantization, ensuring an informed precision reduction that retains model performance. Experiments show APTQ surpasses previous quantization methods, achieving an average of 4 bit width a 5.22 perplexity nearly equivalent to full precision in the C4 dataset. In addition, APTQ attains state-of-the-art zero-shot accuracy of 68.24\% and 70.48\% at an average bitwidth of 3.8 in LLaMa-7B and LLaMa-1
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#23398;&#24605;&#24819;&#30340;&#20803;&#25506;&#27979;&#20195;&#29702;&#65288;MPA&#65289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14865</link><description>&lt;p&gt;
DyVal 2: &#20803;&#25506;&#27979;&#20195;&#29702;&#21160;&#24577;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#23398;&#24605;&#24819;&#30340;&#20803;&#25506;&#27979;&#20195;&#29702;&#65288;MPA&#65289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#24341;&#36215;&#20102;&#31038;&#21306;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#22240;&#20026;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#12290;&#29616;&#26377;&#24037;&#20316;&#35774;&#35745;&#20102;&#20351;&#29992;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#26126;&#30830;&#23450;&#20041;&#31639;&#27861;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#36825;&#20123;&#21327;&#35758;&#26080;&#27861;&#36731;&#26494;&#25193;&#23637;&#21040;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#22522;&#20934;&#21482;&#33021;&#25552;&#20379;&#25972;&#20307;&#22522;&#20934;&#32467;&#26524;&#65292;&#19981;&#33021;&#25903;&#25345;&#23545;LLMs&#33021;&#21147;&#36827;&#34892;&#32454;&#31890;&#24230;&#21644;&#22810;&#26041;&#38754;&#30340;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20803;&#25506;&#27979;&#20195;&#29702;&#65288;MPA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#24515;&#29702;&#27979;&#37327;&#23398;&#21551;&#21457;&#30340;&#36890;&#29992;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#12290; MPA &#26159; DyVal 2 &#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#33258;&#28982;&#22320;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340; DyVal&#12290; MPA &#35774;&#35745;&#20102;&#25506;&#27979;&#21644;&#35780;&#21028;&#20195;&#29702;&#65292;&#20197;&#33258;&#21160;&#23558;&#21407;&#22987;&#35780;&#20272;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#36981;&#24490;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#22312;&#19977;&#20010;&#22522;&#26412;&#35748;&#30693;&#33021;&#21147;&#19978;&#30340;&#24212;&#29992;: &#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#39064;&#35299;&#20915;&#21644;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14865v1 Announce Type: cross  Abstract: Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal 2, which naturally extends the previous DyVal~\citep{zhu2023dyval}. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are 
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#21322;&#33258;&#20027;&#31995;&#32479;&#65292;&#20801;&#35768;&#36828;&#31243;&#25805;&#20316;&#21592;&#22312;&#23454;&#26102;&#25511;&#21046;&#19979;&#25509;&#31649;&#33258;&#20027;&#19987;&#27880;&#21548;&#21462;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#26816;&#27979;&#20302;&#20852;&#36259;&#21644;&#21442;&#19982;&#24230;&#26469;&#20026;&#25805;&#20316;&#21592;&#25552;&#20379;&#26126;&#30830;&#30340;&#25509;&#31649;&#25552;&#31034;&#65292;&#30456;&#36739;&#20110;&#23436;&#20840;&#33258;&#20027;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#34987;&#26222;&#36941;&#35748;&#20026;&#26356;&#31215;&#26497;&#12290;</title><link>https://arxiv.org/abs/2402.14863</link><description>&lt;p&gt;
&#35780;&#20272;&#19968;&#31181;&#21322;&#33258;&#20027;&#27880;&#24847;&#21147;&#21548;&#31995;&#32479;&#19982;&#25509;&#31649;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Evaluation of a semi-autonomous attentive listening system with takeover prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14863
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#21322;&#33258;&#20027;&#31995;&#32479;&#65292;&#20801;&#35768;&#36828;&#31243;&#25805;&#20316;&#21592;&#22312;&#23454;&#26102;&#25511;&#21046;&#19979;&#25509;&#31649;&#33258;&#20027;&#19987;&#27880;&#21548;&#21462;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#26816;&#27979;&#20302;&#20852;&#36259;&#21644;&#21442;&#19982;&#24230;&#26469;&#20026;&#25805;&#20316;&#21592;&#25552;&#20379;&#26126;&#30830;&#30340;&#25509;&#31649;&#25552;&#31034;&#65292;&#30456;&#36739;&#20110;&#23436;&#20840;&#33258;&#20027;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#34987;&#26222;&#36941;&#35748;&#20026;&#26356;&#31215;&#26497;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27807;&#36890;&#20013;&#26029;&#21644;&#22833;&#21435;&#21442;&#19982;&#24230;&#30340;&#22788;&#29702;&#26159;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20687;&#19987;&#27880;&#21548;&#21462;&#36825;&#26679;&#30340;&#32842;&#22825;&#31995;&#32479;&#65292;&#29992;&#25143;&#20027;&#35201;&#22312;&#35828;&#35805;&#26102;&#12290;&#25105;&#20204;&#35748;&#20026;&#26368;&#36866;&#21512;&#22788;&#29702;&#36825;&#39033;&#20219;&#21153;&#24182;&#25405;&#25937;&#23545;&#35805;&#27969;&#31243;&#30340;&#26159;&#20154;&#31867;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#33258;&#20027;&#31995;&#32479;&#65292;&#20854;&#20013;&#36828;&#31243;&#25805;&#20316;&#21592;&#21487;&#20197;&#23454;&#26102;&#25511;&#21046;&#33258;&#20027;&#27880;&#24847;&#21147;&#21548;&#21462;&#31995;&#32479;&#12290;&#20026;&#20102;&#20351;&#20154;&#31867;&#24178;&#39044;&#21464;&#24471;&#31616;&#21333;&#19988;&#19968;&#33268;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#20302;&#20852;&#36259;&#21644;&#21442;&#19982;&#24230;&#30340;&#33258;&#21160;&#26816;&#27979;&#65292;&#20197;&#21521;&#36828;&#31243;&#25805;&#20316;&#21592;&#25552;&#20379;&#26126;&#30830;&#30340;&#25509;&#31649;&#25552;&#31034;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#21322;&#33258;&#20027;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#26816;&#27979;&#25805;&#20316;&#21592;&#30340;&#25509;&#31649;&#28857;&#65292;&#24182;&#23558;&#20854;&#19982;&#23436;&#20840;&#36828;&#31243;&#25805;&#20316;&#21644;&#23436;&#20840;&#33258;&#20027;&#30340;&#19987;&#27880;&#21548;&#21462;&#31995;&#32479;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#21322;&#33258;&#20027;&#31995;&#32479;&#36890;&#24120;&#27604;&#33258;&#20027;&#31995;&#32479;&#26356;&#21463;&#32943;&#23450;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14863v1 Announce Type: new  Abstract: The handling of communication breakdowns and loss of engagement is an important aspect of spoken dialogue systems, particularly for chatting systems such as attentive listening, where the user is mostly speaking. We presume that a human is best equipped to handle this task and rescue the flow of conversation. To this end, we propose a semi-autonomous system, where a remote operator can take control of an autonomous attentive listening system in real-time. In order to make human intervention easy and consistent, we introduce automatic detection of low interest and engagement to provide explicit takeover prompts to the remote operator. We implement this semi-autonomous system which detects takeover points for the operator and compare it to fully tele-operated and fully autonomous attentive listening systems. We find that the semi-autonomous system is generally perceived more positively than the autonomous system. The results suggest that i
&lt;/p&gt;</description></item><item><title>&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14860</link><description>&lt;p&gt;
&#22312;&#27809;&#26377;&#22522;&#20934;&#23454;&#20917;&#30340;&#24773;&#20917;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Ranking Large Language Models without Ground Truth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14860
&lt;/p&gt;
&lt;p&gt;
&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#21644;&#24433;&#21709;&#21147;&#30340;&#22686;&#24378;&#65292;&#35780;&#20272;&#21644;&#25490;&#21517;LLMs&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#33719;&#21462;&#26114;&#36149;&#30340;&#20154;&#31867;&#21709;&#24212;&#65292;&#35201;&#20040;&#20351;&#29992;LLMs&#25104;&#23545;&#22320;&#20114;&#30456;&#35780;&#20272;&#65292;&#36825;&#21487;&#33021;&#19981;&#22815;&#21487;&#38752;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#22312;&#32473;&#23450;&#19968;&#32452;&#25552;&#31034;&#25968;&#25454;&#38598;&#65288;&#27604;&#22914;&#38382;&#39064;&#12289;&#35828;&#26126;&#31561;&#65289;&#21644;&#19968;&#32452;LLMs&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#20219;&#20309;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#24773;&#20917;&#19979;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#21517;&#12290;&#21463;&#21040;&#29616;&#23454;&#29983;&#27963;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#19987;&#23478;&#21644;&#26377;&#30693;&#35782;&#30340;&#20154;&#37117;&#33021;&#35782;&#21035;&#19968;&#20010;&#26032;&#25163;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#36335;&#26159;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#35780;&#20272;&#20854;&#20182;&#20004;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#27491;&#30830;&#35782;&#21035;&#26368;&#24046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#24182;&#25552;&#20379;&#20102;&#25104;&#21151;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#36890;&#36807;&#21453;&#22797;&#24212;&#29992;&#36825;&#19968;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23545;LLMs&#36827;&#34892;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14860v1 Announce Type: cross  Abstract: Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generati
&lt;/p&gt;</description></item><item><title>ChatEL&#26694;&#26550;&#36890;&#36807;&#19977;&#27493;&#26694;&#26550;&#25913;&#36827;&#20102;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#24179;&#22343;F1&#24615;&#33021;&#25552;&#39640;&#36229;&#36807;2&#65285;</title><link>https://arxiv.org/abs/2402.14858</link><description>&lt;p&gt;
ChatEL: &#19982;&#32842;&#22825;&#26426;&#22120;&#20154;&#19968;&#36215;&#36827;&#34892;&#23454;&#20307;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
ChatEL: Entity Linking with Chatbots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14858
&lt;/p&gt;
&lt;p&gt;
ChatEL&#26694;&#26550;&#36890;&#36807;&#19977;&#27493;&#26694;&#26550;&#25913;&#36827;&#20102;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#24179;&#22343;F1&#24615;&#33021;&#25552;&#39640;&#36229;&#36807;2&#65285;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#25991;&#26723;&#25110;&#21477;&#23376;&#20013;&#34920;&#31034;&#23454;&#20307;&#30340;&#19968;&#20123;&#25991;&#26412;&#19982;&#23383;&#20856;&#25110;&#30693;&#35782;&#24211;&#20013;&#30456;&#24212;&#30340;&#26465;&#30446;&#36827;&#34892;&#38142;&#25509;&#12290;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#19987;&#27880;&#20110;&#21019;&#24314;&#22797;&#26434;&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#65292;&#20197;&#23547;&#25214;&#21608;&#22260;&#21333;&#35789;&#30340;&#32447;&#32034;&#26469;&#24110;&#21161;&#35299;&#20915;&#38142;&#25509;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20123;&#32463;&#36807;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#38590;&#20197;&#22788;&#29702;&#65292;&#38590;&#20197;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#20854;&#20182;&#39046;&#22495;&#36716;&#31227;&#25928;&#26524;&#19981;&#20339;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#20687;GPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;EL&#27169;&#22411;&#20013;&#22266;&#26377;&#38382;&#39064;&#25552;&#20379;&#20102;&#39640;&#24230;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23545;LLM&#36827;&#34892;&#31616;&#21333;&#30340;&#25552;&#31034;&#24182;&#19981;&#22863;&#25928;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;ChatEL&#65292;&#36825;&#26159;&#19968;&#20010;&#19977;&#27493;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#31034;LLM&#36820;&#22238;&#20934;&#30830;&#32467;&#26524;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;ChatEL&#26694;&#26550;&#23558;10&#20010;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;F1&#24615;&#33021;&#25552;&#39640;&#20102;&#36229;&#36807;2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14858v1 Announce Type: cross  Abstract: Entity Linking (EL) is an essential and challenging task in natural language processing that seeks to link some text representing an entity within a document or sentence with its corresponding entry in a dictionary or knowledge base. Most existing approaches focus on creating elaborate contextual models that look for clues the words surrounding the entity-text to help solve the linking problem. Although these fine-tuned language models tend to work, they can be unwieldy, difficult to train, and do not transfer well to other domains. Fortunately, Large Language Models (LLMs) like GPT provide a highly-advanced solution to the problems inherent in EL models, but simply naive prompts to LLMs do not work well. In the present work, we define ChatEL, which is a three-step framework to prompt LLMs to return accurate results. Overall the ChatEL framework improves the average F1 performance across 10 datasets by more than 2%. Finally, a thorough
&lt;/p&gt;</description></item><item><title>&#31995;&#32479;&#28040;&#24687;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36234;&#29425;&#36807;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#19981;&#21516;&#31995;&#32479;&#28040;&#24687;&#23545;&#25269;&#25239;&#36234;&#29425;&#20855;&#26377;&#19981;&#21516;&#24433;&#21709;&#65292;&#19988;&#36234;&#29425;&#21487;&#33021;&#22312;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#21487;&#36716;&#31227;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14857</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31995;&#32479;&#28040;&#24687;&#23545;&#36234;&#29425;&#26159;&#21542;&#30495;&#30340;&#24456;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is the System Message Really Important to Jailbreaks in Large Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14857
&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#28040;&#24687;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36234;&#29425;&#36807;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#19981;&#21516;&#31995;&#32479;&#28040;&#24687;&#23545;&#25269;&#25239;&#36234;&#29425;&#20855;&#26377;&#19981;&#21516;&#24433;&#21709;&#65292;&#19988;&#36234;&#29425;&#21487;&#33021;&#22312;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#23427;&#20204;&#22312;&#29616;&#20195;&#31038;&#20250;&#20013;&#19981;&#21487;&#25110;&#32570;&#12290;&#23613;&#31649;&#36890;&#24120;&#20250;&#37319;&#21462;&#23433;&#20840;&#25514;&#26045;&#22312;&#21457;&#24067;&#21069;&#23558;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#29616;&#35937;&#65292;&#34987;&#31216;&#20026;"&#36234;&#29425;"&#12290;&#36825;&#20010;&#26415;&#35821;&#25351;&#30340;&#26159;&#24403;LLMs&#21463;&#21040;&#24694;&#24847;&#38382;&#39064;&#25552;&#31034;&#26102;&#20135;&#29983;&#24847;&#22806;&#19988;&#21487;&#33021;&#26377;&#23475;&#30340;&#21709;&#24212;&#12290;&#29616;&#26377;&#30740;&#31350;&#20391;&#37325;&#20110;&#29983;&#25104;&#36234;&#29425;&#25552;&#31034;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#19968;&#20010;&#19981;&#21516;&#30340;&#38382;&#39064;&#65306;&#31995;&#32479;&#28040;&#24687;&#23545;LLMs&#20013;&#30340;&#36234;&#29425;&#26159;&#21542;&#30495;&#30340;&#24456;&#37325;&#35201;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#31283;&#23450;&#30340;GPT&#29256;&#26412;gpt-3.5-turbo-0613&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#29983;&#25104;&#20102;&#20855;&#26377;&#19981;&#21516;&#31995;&#32479;&#28040;&#24687;&#30340;&#36234;&#29425;&#25552;&#31034;&#65306;&#30701;&#65292;&#38271;&#21644;&#26080;&#28040;&#24687;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;&#31995;&#32479;&#28040;&#24687;&#36890;&#36807;&#23454;&#39564;&#20855;&#26377;&#19981;&#21516;&#30340;&#25269;&#25239;&#36234;&#29425;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#36234;&#29425;&#22312;LLMs&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#31995;&#32479;&#28040;&#24687;&#22312;&#38450;&#27490;LLMs&#36234;&#29425;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14857v1 Announce Type: cross  Abstract: The rapid evolution of Large Language Models (LLMs) has rendered them indispensable in modern society. While security measures are typically in place to align LLMs with human values prior to release, recent studies have unveiled a concerning phenomenon named "jailbreak." This term refers to the unexpected and potentially harmful responses generated by LLMs when prompted with malicious questions. Existing research focuses on generating jailbreak prompts but our study aim to answer a different question: Is the system message really important to jailbreak in LLMs? To address this question, we conducted experiments in a stable GPT version gpt-3.5-turbo-0613 to generated jailbreak prompts with varying system messages: short, long, and none. We discover that different system messages have distinct resistances to jailbreak by experiments. Additionally, we explore the transferability of jailbreak across LLMs. This finding underscores the signi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21629;&#39064;&#36923;&#36753;&#38382;&#39064;&#30340;&#21709;&#24212;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20986;&#23427;&#20204;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25512;&#29702;&#27169;&#24335;&#21644;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.14856</link><description>&lt;p&gt;
&#22312;&#25512;&#29702;&#24605;&#32500;&#20013;&#27604;&#36739;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14856
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21629;&#39064;&#36923;&#36753;&#38382;&#39064;&#30340;&#21709;&#24212;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20986;&#23427;&#20204;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25512;&#29702;&#27169;&#24335;&#21644;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#24605;&#32500;&#22312;&#21046;&#23450;&#20581;&#20840;&#21644;&#36830;&#36143;&#35770;&#28857;&#26041;&#38754;&#25198;&#28436;&#20102;&#20851;&#38190;&#35282;&#33394;&#12290;&#23427;&#20801;&#35768;&#20010;&#20307;&#26681;&#25454;&#25152;&#25552;&#20379;&#20449;&#24687;&#30340;&#30495;&#20540;&#24471;&#20986;&#36923;&#36753;&#19978;&#30340;&#32467;&#35770;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#25191;&#34892;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#20027;&#35201;&#35780;&#20272;LLMs&#22312;&#35299;&#20915;&#27492;&#31867;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#23545;&#20854;&#25512;&#29702;&#34892;&#20026;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#35748;&#30693;&#24515;&#29702;&#23398;&#21407;&#29702;&#65292;&#36890;&#36807;&#23545;&#23427;&#20204;&#23545;&#21629;&#39064;&#36923;&#36753;&#38382;&#39064;&#30340;&#21709;&#24212;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#65292;&#26469;&#30740;&#31350;LLMs&#37319;&#29992;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#35266;&#23519;&#21040;&#30340;&#25512;&#29702;&#27169;&#24335;&#65292;&#21253;&#25324;&#35832;&#22914;&#8220;&#20551;&#23450;&#36319;&#38543;&#8221;&#25110;&#8220;&#38142;&#26500;&#24314;&#8221;&#31561;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;arXiv:2402.14856v1 Announce Type: cross
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14856v1 Announce Type: cross  Abstract: Deductive reasoning plays a pivotal role in the formulation of sound and cohesive arguments. It allows individuals to draw conclusions that logically follow, given the truth value of the information provided. Recent progress in the domain of large language models (LLMs) has showcased their capability in executing deductive reasoning tasks. Nonetheless, a significant portion of research primarily assesses the accuracy of LLMs in solving such tasks, often overlooking a deeper analysis of their reasoning behavior. In this study, we draw upon principles from cognitive psychology to examine inferential strategies employed by LLMs, through a detailed evaluation of their responses to propositional logic problems. Our findings indicate that LLMs display reasoning patterns akin to those observed in humans, including strategies like $\textit{supposition following}$ or $\textit{chain construction}$. Moreover, our research demonstrates that the ar
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25991;&#26412;&#21040;&#26597;&#35810;&#24212;&#29992;&#30340;LLM&#25104;&#29087;&#24230;&#27169;&#22411;&#65292;&#19981;&#20165;&#20851;&#27880;&#20934;&#30830;&#24615;&#65292;&#36824;&#25193;&#23637;&#21040;&#26356;&#22810;&#32500;&#24230;&#12290;&#21516;&#26102;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#29992;&#20110;&#25191;&#27861;&#39046;&#22495;&#30340;&#23454;&#38469;&#26696;&#20363;&#65292;&#20171;&#32461;&#20102;&#22495;&#29305;&#23450;&#25991;&#26412;&#21040;&#26597;&#35810;&#21161;&#25163;QueryIQ&#12290;</title><link>https://arxiv.org/abs/2402.14855</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#21487;&#38752;&#36879;&#26126;&#25991;&#26412;&#21040;&#26597;&#35810;&#30340;LLM&#25104;&#29087;&#24230;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An LLM Maturity Model for Reliable and Transparent Text-to-Query
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14855
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25991;&#26412;&#21040;&#26597;&#35810;&#24212;&#29992;&#30340;LLM&#25104;&#29087;&#24230;&#27169;&#22411;&#65292;&#19981;&#20165;&#20851;&#27880;&#20934;&#30830;&#24615;&#65292;&#36824;&#25193;&#23637;&#21040;&#26356;&#22810;&#32500;&#24230;&#12290;&#21516;&#26102;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#29992;&#20110;&#25191;&#27861;&#39046;&#22495;&#30340;&#23454;&#38469;&#26696;&#20363;&#65292;&#20171;&#32461;&#20102;&#22495;&#29305;&#23450;&#25991;&#26412;&#21040;&#26597;&#35810;&#21161;&#25163;QueryIQ&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#35782;&#21040;&#35299;&#20915;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21487;&#38752;&#24615;&#21644;&#36879;&#26126;&#24615;&#38382;&#39064;&#30340;&#24517;&#35201;&#24615;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#21040;&#26597;&#35810;&#24212;&#29992;&#30340;LLM&#25104;&#29087;&#24230;&#27169;&#22411;&#12290;&#35813;&#25104;&#29087;&#24230;&#27169;&#22411;&#26088;&#22312;&#22635;&#34917;&#22312;&#35780;&#20272;LLM&#22312;&#27492;&#31867;&#24212;&#29992;&#20013;&#30340;&#19981;&#36275;&#65292;&#21516;&#26102;&#32435;&#20837;&#20102;&#36229;&#36234;&#32431;&#31929;&#27491;&#30830;&#24615;&#25110;&#20934;&#30830;&#24615;&#30340;&#32500;&#24230;&#12290;&#27492;&#22806;&#65292;&#35813;&#24037;&#20316;&#24341;&#20837;&#20102;&#25191;&#27861;&#39046;&#22495;&#30340;&#19968;&#20010;&#30495;&#23454;&#29992;&#20363;&#65292;&#24182;&#23637;&#31034;&#20102;QueryIQ&#65292;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#39046;&#22495;&#29305;&#23450;&#25991;&#26412;&#21040;&#26597;&#35810;&#21161;&#25163;&#65292;&#20197;&#21152;&#36895;&#29992;&#25143;&#24037;&#20316;&#27969;&#31243;&#24182;&#25581;&#31034;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14855v1 Announce Type: cross  Abstract: Recognizing the imperative to address the reliability and transparency issues of Large Language Models (LLM), this work proposes an LLM maturity model tailored for text-to-query applications. This maturity model seeks to fill the existing void in evaluating LLMs in such applications by incorporating dimensions beyond mere correctness or accuracy. Moreover, this work introduces a real-world use case from the law enforcement domain and showcases QueryIQ, an LLM-powered, domain-specific text-to-query assistant to expedite user workflows and reveal hidden relationship in data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#25552;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#19987;&#23478;&#36523;&#20221;&#21644;&#33258;&#26432;&#35789;&#20856;&#19982;&#24515;&#29702;&#20581;&#24247;&#29305;&#23450;LLM&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#20013;&#30340;&#35299;&#37322;&#24615;&#21644;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#35780;&#20272;&#24515;&#29702;&#29366;&#24577;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.14854</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24515;&#29702;&#20581;&#24247;&#35821;&#35328;&#27169;&#22411;&#30340;&#21452;&#25552;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Dual-Prompting for Interpretable Mental Health Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14854
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#25552;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#19987;&#23478;&#36523;&#20221;&#21644;&#33258;&#26432;&#35789;&#20856;&#19982;&#24515;&#29702;&#20581;&#24247;&#29305;&#23450;LLM&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#20013;&#30340;&#35299;&#37322;&#24615;&#21644;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#35780;&#20272;&#24515;&#29702;&#29366;&#24577;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24037;&#26234;&#33021;&#24515;&#29702;&#20581;&#24247;&#30417;&#27979;&#24037;&#20855;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#23427;&#20204;&#23545;&#20020;&#24202;&#21307;&#29983;&#30340;&#23454;&#38469;&#25928;&#29992;&#26377;&#38480;&#12290;CLPsych 2024&#20849;&#20139;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#33258;&#26432;&#24847;&#35782;&#30340;&#35777;&#25454;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#25552;&#31034;&#26041;&#27861;&#65306;&#65288;i&#65289;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#36523;&#20221;&#21644;&#33258;&#26432;&#35789;&#20856;&#19982;&#24515;&#29702;&#20581;&#24247;&#29305;&#23450;LLM&#30456;&#32467;&#21512;&#65292;&#36827;&#34892;&#30693;&#35782;&#24863;&#30693;&#35777;&#25454;&#25552;&#21462;&#65307;&#20197;&#21450;&#65288;ii&#65289;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#19968;&#33268;&#24615;&#35780;&#20272;&#22120;&#26469;&#36827;&#34892;&#35777;&#25454;&#24635;&#32467;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#32467;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#65292;&#25581;&#31034;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#21644;&#35813;&#26041;&#27861;&#22312;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#35780;&#20272;&#24515;&#29702;&#29366;&#24577;&#36827;&#23637;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14854v1 Announce Type: cross  Abstract: Despite the increasing demand for AI-based mental health monitoring tools, their practical utility for clinicians is limited by the lack of interpretability.The CLPsych 2024 Shared Task (Chim et al., 2024) aims to enhance the interpretability of Large Language Models (LLMs), particularly in mental health analysis, by providing evidence of suicidality through linguistic content. We propose a dual-prompting approach: (i) Knowledge-aware evidence extraction by leveraging the expert identity and a suicide dictionary with a mental health-specific LLM; and (ii) Evidence summarization by employing an LLM-based consistency evaluator. Comprehensive experiments demonstrate the effectiveness of combining domain-specific information, revealing performance improvements and the approach's potential to aid clinicians in assessing mental state progression.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;NL2Formula&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#29983;&#25104;&#22522;&#20110;&#30005;&#23376;&#34920;&#26684;&#34920;&#26684;&#30340;&#21487;&#25191;&#34892;&#20844;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;fCoder&#30340;&#22522;&#20934;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.14853</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#29983;&#25104;&#30005;&#23376;&#34920;&#26684;&#20844;&#24335;&#30340;NL2Formula
&lt;/p&gt;
&lt;p&gt;
NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;NL2Formula&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#29983;&#25104;&#22522;&#20110;&#30005;&#23376;&#34920;&#26684;&#34920;&#26684;&#30340;&#21487;&#25191;&#34892;&#20844;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;fCoder&#30340;&#22522;&#20934;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#34920;&#26684;&#19978;&#32534;&#20889;&#20844;&#24335;&#65292;&#22914;Microsoft Excel&#21644;Google Sheets&#65292;&#26159;&#35768;&#22810;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#30340;&#29992;&#25143;&#24191;&#27867;&#20351;&#29992;&#30340;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#26368;&#32456;&#29992;&#25143;&#26469;&#35828;&#65292;&#21046;&#20316;&#30005;&#23376;&#34920;&#26684;&#20844;&#24335;&#20173;&#28982;&#26159;&#19968;&#39033;&#32321;&#29712;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22797;&#26434;&#25805;&#20316;&#26102;&#12290;&#20026;&#20102;&#20943;&#36731;&#32534;&#20889;&#30005;&#23376;&#34920;&#26684;&#20844;&#24335;&#25152;&#24102;&#26469;&#30340;&#36127;&#25285;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31216;&#20026;NL2Formula&#30340;&#26032;&#22411;&#22522;&#20934;&#20219;&#21153;&#65292;&#26088;&#22312;&#26681;&#25454;&#36755;&#20837;&#30340;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#26597;&#35810;&#29983;&#25104;&#22522;&#20110;&#30005;&#23376;&#34920;&#26684;&#30340;&#21487;&#25191;&#34892;&#20844;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;70,799&#20010;&#37197;&#23545;NL&#26597;&#35810;&#21644;&#30456;&#24212;&#30005;&#23376;&#34920;&#26684;&#20844;&#24335;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;21,670&#20010;&#34920;&#26684;&#21644;37&#31181;&#20844;&#24335;&#20989;&#25968;&#31867;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#31216;&#20026;fCoder&#30340;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#22522;&#20934;&#23454;&#29616;&#26469;&#23454;&#29616;NL2Formula&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;fCoder&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#20102;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14853v1 Announce Type: cross  Abstract: Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets, is a widespread practice among users performing data analysis. However, crafting formulas on spreadsheets remains a tedious and error-prone task for many end-users, particularly when dealing with complex operations. To alleviate the burden associated with writing spreadsheet formulas, this paper introduces a novel benchmark task called NL2Formula, with the aim to generate executable formulas that are grounded on a spreadsheet table, given a Natural Language (NL) query as input. To accomplish this, we construct a comprehensive dataset consisting of 70,799 paired NL queries and corresponding spreadsheet formulas, covering 21,670 tables and 37 types of formula functions. We realize the NL2Formula task by providing a sequence-to-sequence baseline implementation called fCoder. Experimental results validate the effectiveness of fCoder, demonstrating its superior per
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26368;&#26032;&#30340;GPT-4&#27169;&#22411;&#22312;&#31243;&#24207;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#36890;&#36807;&#22312;HumanEval&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#22312;&#38646;&#26679;&#26412;Python&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#31454;&#20105;&#24615;&#24615;&#33021;&#21644;&#26356;&#22810;&#22810;&#27493;&#39588;&#33539;&#24335;&#32508;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.14852</link><description>&lt;p&gt;
&#26368;&#26032;GPT&#27169;&#22411;&#19978;&#30340;HumanEval -- 2024
&lt;/p&gt;
&lt;p&gt;
HumanEval on Latest GPT Models -- 2024
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14852
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26368;&#26032;&#30340;GPT-4&#27169;&#22411;&#22312;&#31243;&#24207;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#36890;&#36807;&#22312;HumanEval&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#22312;&#38646;&#26679;&#26412;Python&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#31454;&#20105;&#24615;&#24615;&#33021;&#21644;&#26356;&#22810;&#22810;&#27493;&#39588;&#33539;&#24335;&#32508;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;2023&#24180;&#65292;&#25105;&#20204;&#27491;&#22312;&#20351;&#29992;&#26368;&#26032;&#30340;GPT-4&#27169;&#22411;&#26469;&#25512;&#36827;&#31243;&#24207;&#21512;&#25104;&#12290;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26174;&#33879;&#25913;&#36827;&#20102;&#36825;&#19968;&#30446;&#30340;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#20026;&#20102;&#20351;&#36825;&#20123;&#36827;&#23637;&#26356;&#26131;&#20110;&#35775;&#38382;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#23558;&#36825;&#20123;&#27169;&#22411;&#36830;&#25509;&#21040;Human Eval&#30340;&#23384;&#20648;&#24211;&#12290;&#35813;&#25968;&#25454;&#38598;&#26368;&#21021;&#26159;&#20026;&#19982;&#21517;&#20026;CODEGEN&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#25968;&#25454;&#19978;&#20351;&#29992;&#32780;&#24320;&#21457;&#30340;&#12290;&#36890;&#36807;&#23637;&#31034;&#36825;&#20123;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#19982;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#22312;HumanEval&#20219;&#21153;&#19978;&#38646;&#26679;&#26412;Python&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#31454;&#20105;&#24615;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#35757;&#32451;&#27169;&#22411;&#30340;&#25928;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#20026;&#24320;&#21457;&#26356;&#22810;&#30340;&#22810;&#27493;&#39588;&#33539;&#24335;&#32508;&#21512;&#21019;&#36896;&#20102;&#21487;&#33021;&#12290;&#36825;&#19968;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;160&#20010;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#38598;&#65292;&#36825;&#20123;&#38382;&#39064;&#38598;&#34987;&#20998;&#35299;&#25104;&#22810;&#27493;&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#36825;&#26174;&#33879;&#25913;&#36827;&#20102;&#21333;&#36718;&#36755;&#20837;&#19978;&#30340;&#31243;&#24207;&#32508;&#21512;&#12290;&#25152;&#26377;&#20195;&#30721;&#22343;&#20197;&#24320;&#28304;&#26041;&#24335;&#21457;&#24067;&#22312;https://github.com/daniel442li/gpt-human-eval&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14852v1 Announce Type: cross  Abstract: In 2023, we are using the latest models of GPT-4 to advance program synthesis. The large language models have significantly improved the state-of-the-art for this purpose. To make these advancements more accessible, we have created a repository that connects these models to Huamn Eval. This dataset was initally developed to be used with a language model called CODEGEN on natural and programming language data. The utility of these trained models is showcased by demonstrating their competitive performance in zero-shot Python code generation on HumanEval tasks compared to previous state-of-the-art solutions. Additionally, this gives way to developing more multi-step paradigm synthesis. This benchmark features 160 diverse problem sets factorized into multistep prompts that our analysis shows significantly improves program synthesis over single-turn inputs. All code is open source at https://github.com/daniel442li/gpt-human-eval .
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SQL-CRAFT&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#25913;&#36827;&#21644;&#22686;&#24378;&#25512;&#29702;&#65292;&#25552;&#21319;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;SQL&#36716;&#25442;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24615;&#33021;&#25552;&#21319;&#39640;&#36798;5.7%&#65292;&#24182;&#22312;Spider&#27036;&#21333;&#19978;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.14851</link><description>&lt;p&gt;
SQL-CRAFT: &#36890;&#36807;&#20132;&#20114;&#24335;&#25913;&#36827;&#21644;&#22686;&#24378;&#25512;&#29702;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
SQL-CRAFT: Text-to-SQL through Interactive Refinement and Enhanced Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14851
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SQL-CRAFT&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#25913;&#36827;&#21644;&#22686;&#24378;&#25512;&#29702;&#65292;&#25552;&#21319;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;SQL&#36716;&#25442;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24615;&#33021;&#25552;&#21319;&#39640;&#36798;5.7%&#65292;&#24182;&#22312;Spider&#27036;&#21333;&#19978;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#20294;&#22312;&#19987;&#38376;&#20219;&#21153;&#65288;&#22914;&#25991;&#26412;&#21040;SQL&#65289;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SQL-CRAFT&#65292;&#19968;&#20010;&#36890;&#36807;&#20132;&#20114;&#24335;&#25913;&#36827;&#21644;&#22686;&#24378;&#25512;&#29702;&#26469;&#25552;&#21319;&#22823;&#35821;&#35328;&#27169;&#22411;SQL&#29983;&#25104;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#21033;&#29992;&#20132;&#20114;&#24335;&#32416;&#38169;&#24490;&#29615;&#65288;IC-Loop&#65289;&#20351;&#22823;&#35821;&#35328;&#27169;&#22411;&#19982;&#25968;&#25454;&#24211;&#33258;&#21160;&#20132;&#20114;&#65292;&#21516;&#26102;&#37319;&#29992;&#22686;&#24378;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;Spider&#21644;Bird&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24615;&#33021;&#27604;&#26420;&#32032;&#25552;&#31034;&#26041;&#27861;&#25552;&#39640;&#20102;&#39640;&#36798;5.7%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Spider&#27036;&#21333;&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14851v1 Announce Type: cross  Abstract: Modern LLMs have become increasingly powerful, but they are still facing challenges in specialized tasks such as Text-to-SQL. We propose SQL-CRAFT, a framework to advance LLMs' SQL generation Capabilities through inteRActive reFinemenT and enhanced reasoning. We leverage an Interactive Correction Loop (IC-Loop) for LLMs to interact with databases automatically, as well as Python-enhanced reasoning. We conduct experiments on two Text-to-SQL datasets, Spider and Bird, with performance improvements of up to 5.7% compared to the naive prompting method. Moreover, our method surpasses the current state-of-the-art on the Spider Leaderboard, demonstrating the effectiveness of our framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#38750;&#23433;&#20840;&#20851;&#38190;&#30340;&#25112;&#30053;&#20132;&#36890;&#27969;&#37327;&#31649;&#29702;&#29615;&#22659;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHATATC&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#37327;&#21382;&#21490;&#25968;&#25454;&#38598;&#23454;&#29616;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#27979;&#35797;&#20102;&#20854;&#26597;&#35810;&#21644;&#21709;&#24212;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14850</link><description>&lt;p&gt;
CHATATC&#65306;&#29992;&#20110;&#25903;&#25345;&#25112;&#30053;&#31354;&#20013;&#20132;&#36890;&#27969;&#37327;&#31649;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
CHATATC: Large Language Model-Driven Conversational Agents for Supporting Strategic Air Traffic Flow Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#38750;&#23433;&#20840;&#20851;&#38190;&#30340;&#25112;&#30053;&#20132;&#36890;&#27969;&#37327;&#31649;&#29702;&#29615;&#22659;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHATATC&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#37327;&#21382;&#21490;&#25968;&#25454;&#38598;&#23454;&#29616;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#27979;&#35797;&#20102;&#20854;&#26597;&#35810;&#21644;&#21709;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#36890;&#36807;&#35832;&#22914;ChatGPT&#31561;&#20844;&#24320;&#21487;&#29992;&#24037;&#20855;&#24555;&#36895;&#36208;&#32418;&#12290;LLMs&#22312;&#20010;&#20154;&#21644;&#19987;&#19994;&#39046;&#22495;&#30340;&#24212;&#29992;&#24471;&#21040;&#25512;&#21160;&#65292;&#26159;&#30001;&#20110;&#20154;&#31867;&#29992;&#25143;&#19982;ChatGPT&#31561;&#35745;&#31639;&#26426;&#24212;&#29992;&#20043;&#38388;&#33258;&#28982;&#30340;&#20114;&#21160;&#65292;&#20197;&#21450;&#24378;&#22823;&#30340;&#25688;&#35201;&#21644;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20123;&#29983;&#25104;AI&#24037;&#20855;&#22914;&#20309;&#22312;&#38750;&#23433;&#20840;&#20851;&#38190;&#30340;&#25112;&#30053;&#20132;&#36890;&#27969;&#37327;&#31649;&#29702;&#29615;&#22659;&#20013;&#37096;&#32626;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22522;&#20110;&#21253;&#21547;&#36229;&#36807;80,000&#20010;GDP&#23454;&#26045;&#12289;&#20462;&#35746;&#21644;&#21462;&#28040;&#30340;&#22823;&#37327;&#21382;&#21490;&#25968;&#25454;&#38598;&#65292;&#23545;CHATATC&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;CHATATC&#30340;&#26597;&#35810;&#21644;&#21709;&#24212;&#33021;&#21147;&#65292;&#35760;&#24405;&#20102;&#25104;&#21151;&#20043;&#22788;&#65288;&#20363;&#22914;&#65292;&#25552;&#20379;&#27491;&#30830;&#30340;GDP&#29575;&#12289;&#25345;&#32493;&#26102;&#38388;&#21644;&#21407;&#22240;&#65289;&#20197;&#21450;&#19981;&#36275;&#20043;&#22788;&#65288;&#20363;&#22914;&#65292;&#26368;&#39640;&#27700;&#24179;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14850v1 Announce Type: cross  Abstract: Generative artificial intelligence (AI) and large language models (LLMs) have gained rapid popularity through publicly available tools such as ChatGPT. The adoption of LLMs for personal and professional use is fueled by the natural interactions between human users and computer applications such as ChatGPT, along with powerful summarization and text generation capabilities. Given the widespread use of such generative AI tools, in this work we investigate how these tools can be deployed in a non-safety critical, strategic traffic flow management setting. Specifically, we train an LLM, CHATATC, based on a large historical data set of Ground Delay Program (GDP) issuances, spanning 2000-2023 and consisting of over 80,000 GDP implementations, revisions, and cancellations. We test the query and response capabilities of CHATATC, documenting successes (e.g., providing correct GDP rates, durations, and reason) and shortcomings (e.g,. superlative
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25913;&#36827;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#24322;&#27493;&#21644;&#20998;&#27573;&#30340;&#21452;&#21521;&#32534;&#30721;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14849</link><description>&lt;p&gt;
&#24322;&#27493;&#21644;&#20998;&#27573;&#30340;&#21452;&#21521;&#32534;&#30721;&#23545;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Asynchronous and Segmented Bidirectional Encoding for NMT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25913;&#36827;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#24322;&#27493;&#21644;&#20998;&#27573;&#30340;&#21452;&#21521;&#32534;&#30721;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#25552;&#39640;&#32763;&#35793;&#25928;&#29575;&#21644;&#36136;&#37327;&#24050;&#25104;&#20026;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25913;&#36827;&#27169;&#22411;&#65292;&#23454;&#26045;&#20102;&#24322;&#27493;&#21644;&#20998;&#27573;&#30340;&#21452;&#21521;&#35299;&#30721;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#32763;&#35793;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#20174;&#24038;&#21040;&#21491;&#25110;&#20174;&#21491;&#21040;&#24038;&#30340;&#21333;&#21521;&#32763;&#35793;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#38271;&#21477;&#26102;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#22909;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;IWSLT2017&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#21152;&#36895;&#32763;&#35793;&#21644;&#25552;&#39640;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#21333;&#21521;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14849v1 Announce Type: cross  Abstract: With the rapid advancement of Neural Machine Translation (NMT), enhancing translation efficiency and quality has become a focal point of research. Despite the commendable performance of general models such as the Transformer in various aspects, they still fall short in processing long sentences and fully leveraging bidirectional contextual information. This paper introduces an improved model based on the Transformer, implementing an asynchronous and segmented bidirectional decoding strategy aimed at elevating translation efficiency and accuracy. Compared to traditional unidirectional translations from left-to-right or right-to-left, our method demonstrates heightened efficiency and improved translation quality, particularly in handling long sentences. Experimental results on the IWSLT2017 dataset confirm the effectiveness of our approach in accelerating translation and increasing accuracy, especially surpassing traditional unidirection
&lt;/p&gt;</description></item><item><title>&#36755;&#20837;&#38271;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#38477;&#32423;&#36235;&#21183;&#20986;&#29616;&#22312;&#27604;&#25216;&#26415;&#26368;&#22823;&#20540;&#30701;&#24471;&#22810;&#30340;&#36755;&#20837;&#38271;&#24230;&#19979;&#12290;</title><link>https://arxiv.org/abs/2402.14848</link><description>&lt;p&gt;
&#20219;&#21153;&#30456;&#21516;&#65292;&#20196;&#29260;&#26356;&#22810;&#65306;&#36755;&#20837;&#38271;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14848
&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#38271;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#38477;&#32423;&#36235;&#21183;&#20986;&#29616;&#22312;&#27604;&#25216;&#26415;&#26368;&#22823;&#20540;&#30701;&#24471;&#22810;&#30340;&#36755;&#20837;&#38271;&#24230;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#25193;&#23637;&#36755;&#20837;&#38271;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;LLMs&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#19981;&#21516;&#36755;&#20837;&#38271;&#24230;&#19979;&#30340;&#24615;&#33021;&#19968;&#33268;&#24615;&#23578;&#19981;&#26126;&#30830;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#38382;&#31572;&#25512;&#29702;&#26694;&#26550;&#26469;&#30740;&#31350;&#27492;&#26041;&#38754;&#65292;&#35813;&#26694;&#26550;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#36755;&#20837;&#38271;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#21516;&#19968;&#26679;&#26412;&#30340;&#22810;&#20010;&#29256;&#26412;&#65292;&#27599;&#20010;&#29256;&#26412;&#37117;&#36890;&#36807;&#19981;&#21516;&#38271;&#24230;&#12289;&#31867;&#22411;&#21644;&#20301;&#32622;&#30340;&#22635;&#20805;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20174;&#32780;&#20998;&#31163;&#20102;&#36755;&#20837;&#38271;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#27604;&#23427;&#20204;&#30340;&#25216;&#26415;&#26368;&#22823;&#20540;&#30701;&#24471;&#22810;&#30340;&#36755;&#20837;&#38271;&#24230;&#19979;&#65292;LLMs&#30340;&#25512;&#29702;&#24615;&#33021;&#26126;&#26174;&#38477;&#20302;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38477;&#32423;&#36235;&#21183;&#20986;&#29616;&#22312;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#27599;&#20010;&#29256;&#26412;&#20013;&#65292;&#23613;&#31649;&#24378;&#24230;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20256;&#32479;&#30340;&#22256;&#24785;&#24230;&#24230;&#37327;&#19982;LLMs&#22312;&#38271;&#36755;&#20837;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#27809;&#26377;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#24182;&#35782;&#21035;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14848v1 Announce Type: cross  Abstract: This paper explores the impact of extending input lengths on the capabilities of Large Language Models (LLMs). Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood. We investigate this aspect by introducing a novel QA reasoning framework, specifically designed to assess the impact of input length. We isolate the effect of input length using multiple versions of the same sample, each being extended with padding of different lengths, types and locations. Our findings show a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum. We show that the degradation trend appears in every version of our dataset, although at different intensities. Additionally, our study reveals that traditional perplexity metrics do not correlate with performance of LLMs' in long input reasoning tasks. We analyse our results and identif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#20215;&#20540;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#34920;&#36798;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#23545;&#35805;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;19&#20010;LLMs&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.14846</link><description>&lt;p&gt;
&#22362;&#25345;&#20320;&#30340;&#35282;&#33394;&#65281;&#20010;&#20154;&#20215;&#20540;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stick to your Role! Stability of Personal Values Expressed in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#20215;&#20540;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#34920;&#36798;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#23545;&#35805;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;19&#20010;LLMs&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#25110;&#24515;&#29702;&#38382;&#21367;&#30340;&#26631;&#20934;&#26041;&#24335;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#25552;&#20379;&#35768;&#22810;&#26469;&#28304;&#20110;&#31867;&#20284;&#26368;&#23567;&#32972;&#26223;&#30340;&#19981;&#21516;&#26597;&#35810;&#65288;&#20363;&#22914;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#39640;&#24230;&#20381;&#36182;&#20110;&#32972;&#26223;&#65292;&#22240;&#27492;&#20174;&#36825;&#31181;&#26368;&#23567;&#32972;&#26223;&#35780;&#20272;&#20013;&#24471;&#20986;&#30340;&#32467;&#35770;&#21487;&#33021;&#23545;&#27169;&#22411;&#22312;&#37096;&#32626;&#20013;&#30340;&#34892;&#20026;&#65288;&#22312;&#37027;&#37324;&#23427;&#23558;&#26292;&#38706;&#20110;&#35768;&#22810;&#26032;&#32972;&#26223;&#65289;&#30340;&#35828;&#26126;&#24456;&#23569;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20381;&#36182;&#20110;&#32972;&#26223;&#30340;&#29305;&#24615;&#24212;&#35813;&#20316;&#20026;LLM&#27604;&#36739;&#30340;&#21478;&#19968;&#20010;&#32500;&#24230;&#26469;&#30740;&#31350;&#65292;&#32780;&#19981;&#26159;&#20854;&#20182;&#32500;&#24230;&#65292;&#22914;&#35748;&#30693;&#33021;&#21147;&#12289;&#30693;&#35782;&#25110;&#27169;&#22411;&#22823;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#65288;&#27169;&#25311;&#23545;&#19981;&#21516;&#35805;&#39064;&#30340;&#23545;&#35805;&#65289;&#20215;&#20540;&#34920;&#36798;&#31283;&#23450;&#24615;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#24515;&#29702;&#23398;&#38382;&#21367;&#65288;PVQ&#65289;&#21644;&#34892;&#20026;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#27979;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26469;&#33258;&#20116;&#20010;&#23478;&#26063;&#30340;19&#20010;&#24320;&#28304;LLM&#12290;&#20511;&#37492;&#24515;&#29702;&#23398;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31561;&#32423;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14846v1 Announce Type: cross  Abstract: The standard way to study Large Language Models (LLMs) through benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLM's highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model's behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence should be studied as another dimension of LLM comparison alongside others such as cognitive abilities, knowledge, or model size. In this paper, we present a case-study about the stability of value expression over different contexts (simulated conversations on different topics), and as measured using a standard psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19 open-sourced LLMs from five families. Reusing methods from psychology, we study Rank-order stabilit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#21487;&#20197;&#26377;&#25928;&#20928;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20445;&#25345;&#20854;&#24615;&#33021;&#24182;&#20943;&#36731;&#29256;&#26435;&#20405;&#26435;&#12289;&#25968;&#25454;&#27745;&#26579;&#21644;&#38544;&#31169;&#20405;&#29359;&#31561;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.14845</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#20928;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Purifying Large Language Models by Ensembling a Small Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14845
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#21487;&#20197;&#26377;&#25928;&#20928;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20445;&#25345;&#20854;&#24615;&#33021;&#24182;&#20943;&#36731;&#29256;&#26435;&#20405;&#26435;&#12289;&#25968;&#25454;&#27745;&#26579;&#21644;&#38544;&#31169;&#20405;&#29359;&#31561;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#20174;&#22806;&#37096;&#65288;&#19981;&#21463;&#20449;&#20219;&#65289;&#26469;&#28304;&#25910;&#38598;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23613;&#31649;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#36827;&#34892;&#25968;&#25454;&#28165;&#27927;&#21644;&#31934;&#24515;&#31574;&#21010;&#65292;&#20294;&#24050;&#26377;&#25253;&#36947;&#26174;&#31034;&#26500;&#24314;&#33391;&#22909;&#30340;LLMs&#23384;&#22312;&#29256;&#26435;&#20405;&#26435;&#12289;&#25968;&#25454;&#27745;&#26579;&#21644;/&#25110;&#38544;&#31169;&#20405;&#29359;&#38382;&#39064;&#65292;&#36825;&#23558;&#38459;&#30861;LLMs&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;LLMs&#19982;&#33391;&#24615;&#23567;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#38598;&#25104;&#26469;&#20928;&#21270;LLMs&#20813;&#21463;&#26410;&#32463;&#31579;&#36873;&#25968;&#25454;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#38500;&#20102;&#29702;&#35770;&#20445;&#35777;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#20174;&#32463;&#39564;&#35777;&#23454;&#65292;LLMs&#19982;SLMs&#38598;&#25104;&#21487;&#20197;&#26377;&#25928;&#20445;&#25345;LLMs&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#36731;&#29256;&#26435;&#20405;&#26435;&#12289;&#25968;&#25454;&#27745;&#26579;&#21644;&#38544;&#31169;&#20405;&#29359;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14845v1 Announce Type: cross  Abstract: The emerging success of large language models (LLMs) heavily relies on collecting abundant training data from external (untrusted) sources. Despite substantial efforts devoted to data cleaning and curation, well-constructed LLMs have been reported to suffer from copyright infringement, data poisoning, and/or privacy violations, which would impede practical deployment of LLMs. In this study, we propose a simple and easily implementable method for purifying LLMs from the negative effects caused by uncurated data, namely, through ensembling LLMs with benign and small language models (SLMs). Aside from theoretical guarantees, we perform comprehensive experiments to empirically confirm the efficacy of ensembling LLMs with SLMs, which can effectively preserve the performance of LLMs while mitigating issues such as copyright infringement, data poisoning, and privacy violations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TREC&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#21270;&#35843;&#33410;&#21644;&#26102;&#38388;&#24863;&#30693;&#26041;&#24046;&#32553;&#25918;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#25105;&#35843;&#33410;&#30340;&#36864;&#21270;&#21644;&#35757;&#32451;&#19982;&#37319;&#26679;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14843</link><description>&lt;p&gt;
&#20855;&#26377;&#24378;&#21270;&#35843;&#33410;&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Text Diffusion with Reinforced Conditioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14843
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TREC&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#21270;&#35843;&#33410;&#21644;&#26102;&#38388;&#24863;&#30693;&#26041;&#24046;&#32553;&#25918;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#25105;&#35843;&#33410;&#30340;&#36864;&#21270;&#21644;&#35757;&#32451;&#19982;&#37319;&#26679;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#30001;&#20110;&#22312;&#36845;&#20195;&#25913;&#36827;&#20013;&#30340;&#36866;&#24212;&#24615;&#65292;&#23427;&#20204;&#23545;&#23454;&#29616;&#26356;&#22909;&#30340;&#38750;&#33258;&#22238;&#24402;&#24207;&#21015;&#29983;&#25104;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22788;&#29702;&#35821;&#35328;&#30340;&#31163;&#25955;&#24615;&#30340;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#22312;&#24615;&#33021;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#19981;&#36275;&#12290;&#26412;&#25991;&#23545;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#20004;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#25105;&#35843;&#33410;&#30340;&#36864;&#21270;&#21644;&#35757;&#32451;&#19982;&#37319;&#26679;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#21457;&#29616;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TREC&#30340;&#26032;&#22411;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#21270;&#35843;&#33410;&#32531;&#35299;&#20102;&#36864;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#26102;&#38388;&#24863;&#30693;&#26041;&#24046;&#32553;&#25918;&#35299;&#20915;&#20102;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;TREC&#22312;&#33258;&#22238;&#24402;&#12289;&#38750;&#33258;&#22238;&#24402;&#21644;&#25193;&#25955;&#22522;&#32447;&#20013;&#30340;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#23450;&#24615;&#20998;&#26512;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14843v1 Announce Type: cross  Abstract: Diffusion models have demonstrated exceptional capability in generating high-quality images, videos, and audio. Due to their adaptiveness in iterative refinement, they provide a strong potential for achieving better non-autoregressive sequence generation. However, existing text diffusion models still fall short in their performance due to a challenge in handling the discreteness of language. This paper thoroughly analyzes text diffusion models and uncovers two significant limitations: degradation of self-conditioning during training and misalignment between training and sampling. Motivated by our findings, we propose a novel Text Diffusion model called TREC, which mitigates the degradation with Reinforced Conditioning and the misalignment by Time-Aware Variance Scaling. Our extensive experiments demonstrate the competitiveness of TREC against autoregressive, non-autoregressive, and diffusion baselines. Moreover, qualitative analysis sh
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;RJUA-MedDQA&#65292;&#19968;&#20010;&#21307;&#23398;&#19987;&#19994;&#39046;&#22495;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35201;&#27714;&#65292;&#28041;&#21450;&#35299;&#37322;&#22270;&#20687;&#20869;&#23481;&#12289;&#25968;&#20540;&#25512;&#29702;&#21644;&#20020;&#24202;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14840</link><description>&lt;p&gt;
RJUA-MedDQA&#65306;&#21307;&#30103;&#25991;&#26723;&#38382;&#31572;&#21644;&#20020;&#24202;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14840
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;RJUA-MedDQA&#65292;&#19968;&#20010;&#21307;&#23398;&#19987;&#19994;&#39046;&#22495;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35201;&#27714;&#65292;&#28041;&#21450;&#35299;&#37322;&#22270;&#20687;&#20869;&#23481;&#12289;&#25968;&#20540;&#25512;&#29702;&#21644;&#20020;&#24202;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20013;&#26174;&#31034;&#20986;&#22312;&#21508;&#31181;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#65292;&#27604;&#22914;&#26234;&#33021;&#21307;&#23398;&#35786;&#26029;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#22522;&#20934;&#24182;&#26410;&#21453;&#26144;&#20986;&#30495;&#23454;&#21307;&#30103;&#25253;&#21578;&#30340;&#22797;&#26434;&#24615;&#21644;&#19987;&#19994;&#30340;&#28145;&#20837;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RJUA-MedDQA&#65292;&#36825;&#26159;&#21307;&#23398;&#19987;&#19994;&#39046;&#22495;&#30340;&#19968;&#20010;&#20840;&#38754;&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;&#20960;&#20010;&#25361;&#25112;&#65306;&#20840;&#38754;&#35299;&#37322;&#19981;&#21516;&#25361;&#25112;&#24615;&#24067;&#23616;&#20013;&#30340;&#22270;&#20687;&#20869;&#23481;&#65292;&#20855;&#22791;&#35782;&#21035;&#24322;&#24120;&#25351;&#26631;&#30340;&#25968;&#20540;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20020;&#24202;&#25512;&#29702;&#33021;&#21147;&#65292;&#26681;&#25454;&#21307;&#23398;&#32972;&#26223;&#25552;&#20379;&#30142;&#30149;&#35786;&#26029;&#12289;&#29366;&#24577;&#21644;&#24314;&#35758;&#30340;&#38472;&#36848;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#24182;&#25552;&#20986;&#20102;&#26088;&#22312;&#24674;&#22797;&#25991;&#26412;&#21644;&#34920;&#26684;&#30340;&#39640;&#25928;&#32467;&#26500;&#24674;&#22797;&#27880;&#37322;&#65288;ESRA&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14840v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs) and Large Multi-modal Models (LMMs) have shown potential in various medical applications, such as Intelligent Medical Diagnosis. Although impressive results have been achieved, we find that existing benchmarks do not reflect the complexity of real medical reports and specialized in-depth reasoning capabilities. In this work, we introduced RJUA-MedDQA, a comprehensive benchmark in the field of medical specialization, which poses several challenges: comprehensively interpreting imgage content across diverse challenging layouts, possessing numerical reasoning ability to identify abnormal indicators and demonstrating clinical reasoning ability to provide statements of disease diagnosis, status and advice based on medical contexts. We carefully design the data generation pipeline and proposed the Efficient Structural Restoration Annotation (ESRA) Method, aimed at restoring textual and tabu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#35821;&#20041;&#21644;&#21477;&#27861;&#20004;&#20010;&#26041;&#38754;&#29992;&#20110;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#20934;&#30830;&#24230;&#30340;AI&#27169;&#22411;&#65292;&#22312;M4&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14838</link><description>&lt;p&gt;
RFBES&#22312;SemEval-2024&#20219;&#21153;8&#20013;&#30340;&#24212;&#29992;&#65306;&#25506;&#31350;&#29992;&#20110;&#21306;&#20998;AI&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14838
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#35821;&#20041;&#21644;&#21477;&#27861;&#20004;&#20010;&#26041;&#38754;&#29992;&#20110;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#20934;&#30830;&#24230;&#30340;AI&#27169;&#22411;&#65292;&#22312;M4&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#24182;&#19988;LLMs&#24050;&#34987;&#29992;&#20110;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#19981;&#21516;&#20219;&#21153;&#20013;&#29983;&#25104;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35895;&#27468;&#21644;OpenAI&#31561;&#30693;&#21517;&#20844;&#21496;&#30340;&#21442;&#19982;&#65292;LLMs&#29616;&#22312;&#26356;&#26131;&#33719;&#24471;&#65292;&#20154;&#20204;&#21487;&#20197;&#36731;&#26494;&#20351;&#29992;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#22914;&#20309;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#21306;&#21035;&#12290;&#26412;&#25991;&#20174;&#35821;&#20041;&#21644;&#21477;&#27861;&#20004;&#20010;&#26041;&#38754;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#38382;&#39064;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;AI&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;M4&#25968;&#25454;&#38598;&#19978;&#39640;&#20934;&#30830;&#24230;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#25776;&#20889;&#25991;&#26412;&#65292;&#26080;&#35770;&#26159;&#22810;&#35821;&#35328;&#36824;&#26159;&#21333;&#35821;&#20219;&#21153;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#20351;&#29992;&#35821;&#20041;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;&#26356;&#26377;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#22312;&#21477;&#27861;&#26041;&#27861;&#19978;&#36824;&#26377;&#24456;&#22823;&#25913;&#36827;&#31354;&#38388;&#65292;&#36825;&#23558;&#26159;&#26410;&#26469;&#24037;&#20316;&#30340;&#19968;&#20010;&#33391;&#22909;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14838v1 Announce Type: cross  Abstract: Nowadays, the usage of Large Language Models (LLMs) has increased, and LLMs have been used to generate texts in different languages and for different tasks. Additionally, due to the participation of remarkable companies such as Google and OpenAI, LLMs are now more accessible, and people can easily use them. However, an important issue is how we can detect AI-generated texts from human-written ones. In this article, we have investigated the problem of AI-generated text detection from two different aspects: semantics and syntax. Finally, we presented an AI model that can distinguish AI-generated texts from human-written ones with high accuracy on both multilingual and monolingual tasks using the M4 dataset. According to our results, using a semantic approach would be more helpful for detection. However, there is a lot of room for improvement in the syntactic approach, and it would be a good approach for future work.
&lt;/p&gt;</description></item><item><title>&#32534;&#21046;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#25216;&#26415;&#28165;&#21333;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#20998;&#31867;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#20174;&#19994;&#32773;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.14837</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#25216;&#26415;&#30340;&#23454;&#35777;&#20998;&#31867;&#65306;&#20174;&#19994;&#32773;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
An Empirical Categorization of Prompting Techniques for Large Language Models: A Practitioner's Guide
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14837
&lt;/p&gt;
&lt;p&gt;
&#32534;&#21046;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#25216;&#26415;&#28165;&#21333;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#20998;&#31867;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#20174;&#19994;&#32773;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26368;&#36817;&#29992;&#25552;&#31034;&#35821;&#26469;&#32534;&#31243;&#36825;&#20123;&#27169;&#22411;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#30340;&#25968;&#37327;&#24222;&#22823;&#65292;&#23545;&#20110;&#24076;&#26395;&#21033;&#29992;&#36825;&#20123;&#24037;&#20855;&#30340;&#20174;&#19994;&#32773;&#26469;&#35828;&#65292;&#36825;&#26500;&#25104;&#20102;&#19968;&#20010;&#20196;&#20154;&#38590;&#20197;&#24212;&#23545;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#26368;&#26377;&#25928;&#22320;&#21033;&#29992;LLMs&#65292;&#32534;&#21046;&#19968;&#20010;&#20840;&#38754;&#30340;&#25552;&#31034;&#25216;&#26415;&#28165;&#21333;&#24182;&#24314;&#31435;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#36328;&#23398;&#31185;&#20998;&#31867;&#26694;&#26550;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#26412;&#35843;&#26597;&#30740;&#31350;&#20102;&#19968;&#20123;&#26368;&#30693;&#21517;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20174;&#23398;&#26415;&#21644;&#23454;&#36341;&#35282;&#24230;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#20998;&#20026;&#19971;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#27599;&#20010;&#31867;&#21035;&#65292;&#26088;&#22312;&#28548;&#28165;&#23427;&#20204;&#30340;&#29420;&#29305;&#36129;&#29486;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#31034;&#20363;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20197;&#20026;&#21516;&#34892;&#20174;&#19994;&#32773;&#25552;&#20379;&#19968;&#20010;&#32467;&#26500;&#21270;&#26694;&#26550;&#65292;&#24110;&#21161;&#20182;&#20204;&#29702;&#35299;&#21644;&#24402;&#31867;&#25552;&#31034;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14837v1 Announce Type: cross  Abstract: Due to rapid advancements in the development of Large Language Models (LLMs), programming these models with prompts has recently gained significant attention. However, the sheer number of available prompt engineering techniques creates an overwhelming landscape for practitioners looking to utilize these tools. For the most efficient and effective use of LLMs, it is important to compile a comprehensive list of prompting techniques and establish a standardized, interdisciplinary categorization framework. In this survey, we examine some of the most well-known prompting techniques from both academic and practical viewpoints and classify them into seven distinct categories. We present an overview of each category, aiming to clarify their unique contributions and showcase their practical applications in real-world examples in order to equip fellow practitioners with a structured framework for understanding and categorizing prompting techniqu
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33616;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#38544;&#31192;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#25991;&#26412;&#20869;&#23481;&#22312;&#19981;&#24178;&#39044;&#27169;&#22411;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#29289;&#21697;&#30340;&#26333;&#20809;&#24230;&#65292;&#32780;&#36825;&#31181;&#25915;&#20987;&#23545;&#25972;&#20307;&#25512;&#33616;&#24615;&#33021;&#26080;&#24433;&#21709;&#19988;&#38590;&#20197;&#34987;&#26816;&#27979;&#21040;&#12290;</title><link>https://arxiv.org/abs/2402.14836</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33616;&#20013;&#30340;&#38544;&#31192;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Stealthy Attack on Large Language Model based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14836
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33616;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#38544;&#31192;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#25991;&#26412;&#20869;&#23481;&#22312;&#19981;&#24178;&#39044;&#27169;&#22411;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#29289;&#21697;&#30340;&#26333;&#20809;&#24230;&#65292;&#32780;&#36825;&#31181;&#25915;&#20987;&#23545;&#25972;&#20307;&#25512;&#33616;&#24615;&#33021;&#26080;&#24433;&#21709;&#19988;&#38590;&#20197;&#34987;&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25512;&#21160;&#25512;&#33616;&#31995;&#32479;(RS)&#30340;&#36827;&#23637;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#31995;&#32479;&#34028;&#21187;&#21457;&#23637;&#65292;&#20294;&#23427;&#20204;&#23545;&#23433;&#20840;&#23041;&#32961;&#30340;&#25935;&#24863;&#24615;&#21364;&#34987;&#22823;&#22810;&#24573;&#35270;&#20102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LLMs&#24341;&#20837;&#25512;&#33616;&#27169;&#22411;&#20013;&#20135;&#29983;&#26032;&#23433;&#20840;&#28431;&#27934;&#30340;&#24773;&#20917;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#27880;&#37325;&#29289;&#21697;&#30340;&#25991;&#26412;&#20869;&#23481;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#27979;&#35797;&#38454;&#27573;&#20165;&#36890;&#36807;&#25913;&#21464;&#29289;&#21697;&#30340;&#25991;&#26412;&#20869;&#23481;&#26174;&#33879;&#22686;&#21152;&#20854;&#26333;&#20809;&#24230;&#65292;&#32780;&#26080;&#38656;&#30452;&#25509;&#24178;&#39044;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#35813;&#25915;&#20987;&#20855;&#26377;&#26174;&#33879;&#30340;&#38544;&#31192;&#24615;&#65292;&#22240;&#20026;&#23427;&#19981;&#20250;&#24433;&#21709;&#25972;&#20307;&#25512;&#33616;&#24615;&#33021;&#65292;&#23545;&#25991;&#26412;&#30340;&#20462;&#25913;&#24494;&#22937;&#65292;&#20351;&#29992;&#25143;&#21644;&#24179;&#21488;&#38590;&#20197;&#26816;&#27979;&#21040;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20027;&#27969;&#30340;LLM-based&#25512;&#33616;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14836v1 Announce Type: cross  Abstract: Recently, the powerful large language models (LLMs) have been instrumental in propelling the progress of recommender systems (RS). However, while these systems have flourished, their susceptibility to security threats has been largely overlooked. In this work, we reveal that the introduction of LLMs into recommendation models presents new security vulnerabilities due to their emphasis on the textual content of items. We demonstrate that attackers can significantly boost an item's exposure by merely altering its textual content during the testing phase, without requiring direct interference with the model's training process. Additionally, the attack is notably stealthy, as it does not affect the overall recommendation performance and the modifications to the text are subtle, making it difficult for users and platforms to detect. Our comprehensive experiments across four mainstream LLM-based recommendation models demonstrate the superior
&lt;/p&gt;</description></item><item><title>MIKE&#26159;&#19968;&#20010;&#38024;&#23545;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#23454;&#20307;&#30693;&#35782;&#32534;&#36753;&#30340;&#20840;&#38754;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;&#31361;&#30772;&#20102;&#29616;&#26377;&#22522;&#20934;&#20027;&#35201;&#20391;&#37325;&#20110;&#31895;&#31890;&#24230;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#24418;&#24335;&#20197;&#35780;&#20272;&#32534;&#36753;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.14835</link><description>&lt;p&gt;
MIKE&#65306;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#23454;&#20307;&#30693;&#35782;&#32534;&#36753;&#30340;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14835
&lt;/p&gt;
&lt;p&gt;
MIKE&#26159;&#19968;&#20010;&#38024;&#23545;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#23454;&#20307;&#30693;&#35782;&#32534;&#36753;&#30340;&#20840;&#38754;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;&#31361;&#30772;&#20102;&#29616;&#26377;&#22522;&#20934;&#20027;&#35201;&#20391;&#37325;&#20110;&#31895;&#31890;&#24230;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#24418;&#24335;&#20197;&#35780;&#20272;&#32534;&#36753;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#32534;&#36753;&#26159;&#22686;&#24378;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21151;&#33021;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#23613;&#31649;&#20854;&#28508;&#21147;&#24040;&#22823;&#65292;&#20294;&#24403;&#21069;&#30340;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#31895;&#31890;&#24230;&#30693;&#35782;&#19978;&#65292;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#23454;&#20307;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#22823;&#22810;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MIKE&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#23454;&#20307;&#30693;&#35782;&#32534;&#36753;&#35774;&#35745;&#30340;&#20840;&#38754;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14835v1 Announce Type: cross  Abstract: Multimodal knowledge editing represents a critical advancement in enhancing the capabilities of Multimodal Large Language Models (MLLMs). Despite its potential, current benchmarks predominantly focus on coarse-grained knowledge, leaving the intricacies of fine-grained (FG) multimodal entity knowledge largely unexplored. This gap presents a notable challenge, as FG entity recognition is pivotal for the practical deployment and effectiveness of MLLMs in diverse real-world scenarios. To bridge this gap, we introduce MIKE, a comprehensive benchmark and dataset specifically designed for the FG multimodal entity knowledge editing. MIKE encompasses a suite of tasks tailored to assess different perspectives, including Vanilla Name Answering, Entity-Level Caption, and Complex-Scenario Recognition. In addition, a new form of knowledge editing, Multi-step Editing, is introduced to evaluate the editing efficiency. Through our extensive evaluations
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22810;&#36339;&#35821;&#27861;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34917;&#20805;&#30340;&#35821;&#27861;&#20449;&#24687;&#26469;&#22788;&#29702;&#20551;&#26032;&#38395;&#20013;&#30340;&#24494;&#22937;&#36716;&#25240;</title><link>https://arxiv.org/abs/2402.14834</link><description>&lt;p&gt;
MSynFD: &#22810;&#36339;&#35821;&#27861;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MSynFD: Multi-hop Syntax aware Fake News Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14834
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22810;&#36339;&#35821;&#27861;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34917;&#20805;&#30340;&#35821;&#27861;&#20449;&#24687;&#26469;&#22788;&#29702;&#20551;&#26032;&#38395;&#20013;&#30340;&#24494;&#22937;&#36716;&#25240;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#24191;&#27867;&#20256;&#25773;&#21161;&#38271;&#20102;&#20551;&#26032;&#38395;&#30340;&#24555;&#36895;&#20256;&#25773;&#65292;&#23545;&#25105;&#20204;&#30340;&#29616;&#23454;&#31038;&#20250;&#26500;&#25104;&#23041;&#32961;&#12290;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#25110;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22686;&#24378;&#23545;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#65292;&#36890;&#36807;&#20998;&#26512;&#26032;&#38395;&#20869;&#23481;&#21644;/&#25110;&#20854;&#31038;&#20250;&#32972;&#26223;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#20102;&#22522;&#26412;&#30340;&#25991;&#26412;&#26032;&#38395;&#20869;&#23481;&#65288;&#25991;&#31456;&#65289;&#65292;&#24182;&#19988;&#36807;&#20998;&#20381;&#36182;&#24207;&#21015;&#24314;&#27169;&#21644;&#20840;&#23616;&#27880;&#24847;&#21147;&#26469;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#22797;&#26434;&#12289;&#24494;&#22937;&#30340;&#36716;&#25240;&#65292;&#27604;&#22914;&#21477;&#27861;-&#35821;&#20041;&#19981;&#21305;&#37197;&#21644;&#20808;&#39564;&#20559;&#24046;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#20302;&#65292;&#24182;&#22312;&#32570;&#22833;&#27169;&#24577;&#25110;&#31038;&#20250;&#32972;&#26223;&#26102;&#21487;&#33021;&#22833;&#36133;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20123;&#37325;&#35201;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#36339;&#35821;&#27861;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#65288;MSynFD&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#34701;&#21512;&#20102;&#34917;&#20805;&#30340;&#35821;&#27861;&#20449;&#24687;&#65292;&#20197;&#22788;&#29702;&#20551;&#26032;&#38395;&#20013;&#30340;&#24494;&#22937;&#36716;&#25240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14834v1 Announce Type: cross  Abstract: The proliferation of social media platforms has fueled the rapid dissemination of fake news, posing threats to our real-life society. Existing methods use multimodal data or contextual information to enhance the detection of fake news by analyzing news content and/or its social context. However, these methods often overlook essential textual news content (articles) and heavily rely on sequential modeling and global attention to extract semantic information. These existing methods fail to handle the complex, subtle twists in news articles, such as syntax-semantics mismatches and prior biases, leading to lower performance and potential failure when modalities or social context are missing. To bridge these significant gaps, we propose a novel multi-hop syntax aware fake news detection (MSynFD) method, which incorporates complementary syntax information to deal with subtle twists in fake news. Specifically, we introduce a syntactical depen
&lt;/p&gt;</description></item><item><title>CliqueParcel&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#25209;&#22788;&#29702;&#26469;&#25552;&#39640;LLM&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#26368;&#23567;&#21270;&#19982;&#21407;&#22987;&#36755;&#20986;&#30340;&#20559;&#24046;&#65292;&#35299;&#20915;&#20102;&#25240;&#20215;&#36755;&#20986;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14833</link><description>&lt;p&gt;
CliqueParcel&#65306;&#19968;&#31181;&#21516;&#26102;&#20248;&#21270;&#25928;&#29575;&#21644;&#24544;&#23454;&#24230;&#30340;&#25209;&#22788;&#29702;LLM&#25552;&#31034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes Efficiency And Faithfulness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14833
&lt;/p&gt;
&lt;p&gt;
CliqueParcel&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#25209;&#22788;&#29702;&#26469;&#25552;&#39640;LLM&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#26368;&#23567;&#21270;&#19982;&#21407;&#22987;&#36755;&#20986;&#30340;&#20559;&#24046;&#65292;&#35299;&#20915;&#20102;&#25240;&#20215;&#36755;&#20986;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;LLM&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CliqueParcel&#65292;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#25552;&#31034;&#25209;&#22788;&#29702;&#26469;&#25552;&#39640;LLM&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#20248;&#21270;&#25512;&#29702;&#25928;&#29575;&#30340;&#31574;&#30053;&#36890;&#24120;&#20250;&#23545;&#36755;&#20986;&#36136;&#37327;&#36827;&#34892;&#22949;&#21327;&#65292;&#23548;&#33268;&#25240;&#20215;&#36755;&#20986;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#33021;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#25110;&#36755;&#20986;&#32570;&#20047;&#32454;&#33410;&#12290;CliqueParcel&#26159;&#25105;&#20204;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#22238;&#24212;&#12290;&#22312;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#26368;&#23567;&#21270;&#19982;&#21407;&#22987;&#36755;&#20986;&#30340;&#20559;&#24046;&#65288;&#21363;&#24544;&#23454;&#24230;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#20026;&#20102;&#22880;&#23450;&#22522;&#30784;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25490;&#38500;&#30001;&#20110;&#38271;&#24230;&#32553;&#30701;&#32780;&#23548;&#33268;&#30340;&#36816;&#34892;&#26102;&#38388;&#20943;&#23569;&#26469;&#37325;&#26032;&#23450;&#20041;&#25928;&#29575;&#27979;&#37327;&#26631;&#20934;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25928;&#29575;&#21644;&#24544;&#23454;&#24230;&#20043;&#38388;&#30340;&#20840;&#38754;&#26435;&#34913;&#65292;&#20197;&#38416;&#26126;&#8220;&#25240;&#20215;&#36755;&#20986;&#8221;&#38382;&#39064;&#30340;&#26412;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14833v1 Announce Type: cross  Abstract: Large language models (LLMs) have become pivotal in recent research. However, during the inference process, LLMs still require substantial resources. In this paper, we propose CliqueParcel, a method designed to improve the efficiency of LLMs via prompt batching. Existing strategies to optimize inference efficiency often compromise on output quality, leading to a discounted output problem. This issue might result in reduced accuracy or outputs that are less detailed. CliqueParcel is our answer to this challenge. While ensuring accuracy and minimizing deviations from the original outputs (i.e., faithfulness), our method significantly improves efficiency during inference.   To lay the groundwork, we first redefine efficiency measurements by excluding the reduction in running time due to shorter lengths. Then, we provide a comprehensive trade-off between efficiency and faithfulness to clarify the nature of the 'discounted output' problem. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Mistral-7B&#30340;70&#20159;&#21442;&#25968;&#30340;Orca-Math&#23567;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#22312;&#23567;&#23398;&#25968;&#23398;&#20013;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14830</link><description>&lt;p&gt;
Orca-Math&#65306;&#37322;&#25918;&#23567;&#35821;&#35328;&#27169;&#22411;&#22312;&#23567;&#23398;&#25968;&#23398;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Orca-Math: Unlocking the potential of SLMs in Grade School Math
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14830
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Mistral-7B&#30340;70&#20159;&#21442;&#25968;&#30340;Orca-Math&#23567;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#22312;&#23567;&#23398;&#25968;&#23398;&#20013;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#35299;&#20915;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#23567;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#38754;&#20020;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#20551;&#35774;&#65292;&#20026;&#20102;&#22312;GSM8K&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#36229;&#36807;80%&#30340;&#20934;&#30830;&#24230;&#65292;&#26368;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#38656;&#35201;&#20026;340&#20159;&#20010;&#21442;&#25968;&#12290;&#20026;&#20102;&#29992;&#26356;&#23567;&#30340;&#27169;&#22411;&#36798;&#21040;&#36825;&#19968;&#24615;&#33021;&#27700;&#24179;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#35757;&#32451;SLMs&#29983;&#25104;Python&#20195;&#30721;&#25110;&#20351;&#29992;&#24037;&#20855;&#26469;&#24110;&#21161;&#36991;&#20813;&#35745;&#31639;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#20351;&#29992;&#38598;&#25104;&#65292;&#23558;&#22810;&#36798;100&#27425;&#27169;&#22411;&#36816;&#34892;&#30340;&#36755;&#20986;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#36873;&#25321;&#26159;&#36890;&#36807;&#20849;&#35782;&#12289;&#22810;&#25968;&#25237;&#31080;&#25110;&#19982;SLM&#19968;&#36215;&#20351;&#29992;&#30340;&#21333;&#29420;&#30340;&#39564;&#35777;&#22120;&#27169;&#22411;&#26469;&#36827;&#34892;&#30340;&#12290;&#38598;&#25104;&#22823;&#22823;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#20294;&#38543;&#20043;&#32780;&#26469;&#30340;&#26159;&#23545;&#27169;&#22411;&#30340;&#22810;&#27425;&#35843;&#29992;&#36896;&#25104;&#30340;&#26174;&#33879;&#25104;&#26412;&#22686;&#21152;&#65288;&#20363;&#22914;&#65292;Phi-GSM&#20351;&#29992;&#21069;48&#20010;&#26469;&#23558;&#24615;&#33021;&#20174;68.2&#25552;&#21319;&#21040;81.5&#65289;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Orca-Math&#65292;&#19968;&#20010;&#22522;&#20110;Mistral-7B&#30340;70&#20159;&#21442;&#25968;SLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14830v1 Announce Type: cross  Abstract: Mathematical word problem-solving has long been recognized as a complex task for small language models (SLMs). A recent study hypothesized that the smallest model size, needed to achieve over 80% accuracy on the GSM8K benchmark, is 34 billion parameters. To reach this level of performance with smaller models, researcher often train SLMs to generate Python code or use tools to help avoid calculation errors. Additionally, they employ ensembling, where outputs of up to 100 model runs are combined to arrive at a more accurate result. Result selection is done using consensus, majority vote or a separate a verifier model used in conjunction with the SLM. Ensembling provides a substantial boost in accuracy but at a significant cost increase with multiple calls to the model (e.g., Phi-GSM uses top-48 to boost the performance from 68.2 to 81.5).   In this work, we present Orca-Math, a 7-billion-parameter SLM based on the Mistral-7B, which achie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35789;&#27719;&#31616;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#22312;&#21407;&#22987;&#21477;&#23376;&#20013;&#39044;&#27979;&#35789;&#27719;&#20462;&#25913;&#65292;&#24341;&#20837;LLM&#22686;&#24378;&#25439;&#22833;&#36827;&#34892;&#30693;&#35782;&#25552;&#28860;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#38590;&#24230;&#24863;&#30693;&#30340;&#22635;&#20805;&#27169;&#22359;&#23558;&#22797;&#26434;&#35789;&#26367;&#25442;&#20026;&#31616;&#21333;&#35789;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14704</link><description>&lt;p&gt;
&#19968;&#31181;LLM&#22686;&#24378;&#30340;&#35789;&#27719;&#31616;&#21270;&#23545;&#25239;&#32534;&#36753;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An LLM-Enhanced Adversarial Editing System for Lexical Simplification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14704
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35789;&#27719;&#31616;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#22312;&#21407;&#22987;&#21477;&#23376;&#20013;&#39044;&#27979;&#35789;&#27719;&#20462;&#25913;&#65292;&#24341;&#20837;LLM&#22686;&#24378;&#25439;&#22833;&#36827;&#34892;&#30693;&#35782;&#25552;&#28860;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#38590;&#24230;&#24863;&#30693;&#30340;&#22635;&#20805;&#27169;&#22359;&#23558;&#22797;&#26434;&#35789;&#26367;&#25442;&#20026;&#31616;&#21333;&#35789;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#31616;&#21270;&#65288;LS&#65289;&#26088;&#22312;&#22312;&#35789;&#27719;&#32423;&#21035;&#31616;&#21270;&#25991;&#26412;&#12290;&#29616;&#26377;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#26631;&#27880;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#38590;&#20197;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LS&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#23545;&#25239;&#32534;&#36753;&#31995;&#32479;&#65292;&#24182;&#32467;&#21512;&#28151;&#28102;&#25439;&#22833;&#21644;&#19981;&#21464;&#24615;&#25439;&#22833;&#26469;&#39044;&#27979;&#21407;&#22987;&#21477;&#23376;&#20013;&#30340;&#35789;&#27719;&#20462;&#25913;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;LLM&#22686;&#24378;&#25439;&#22833;&#65292;&#20197;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#25552;&#28860;&#25104;&#23567;&#22411;LS&#31995;&#32479;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21477;&#23376;&#20013;&#30340;&#22797;&#26434;&#35789;&#34987;&#23631;&#34109;&#65292;&#21046;&#20316;&#20102;&#19968;&#20010;&#22522;&#20110;&#38590;&#24230;&#24863;&#30693;&#30340;&#22635;&#20805;&#27169;&#22359;&#65292;&#29992;&#26356;&#31616;&#21333;&#30340;&#35789;&#26367;&#25442;&#23631;&#34109;&#20301;&#32622;&#12290;&#26368;&#21518;&#65292;&#23545;&#19977;&#20010;&#22522;&#20934;LS&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14704v1 Announce Type: new  Abstract: Lexical Simplification (LS) aims to simplify text at the lexical level. Existing methods rely heavily on annotated data, making it challenging to apply in low-resource scenarios. In this paper, we propose a novel LS method without parallel corpora. This method employs an Adversarial Editing System with guidance from a confusion loss and an invariance loss to predict lexical edits in the original sentences. Meanwhile, we introduce an innovative LLM-enhanced loss to enable the distillation of knowledge from Large Language Models (LLMs) into a small-size LS system. From that, complex words within sentences are masked and a Difficulty-aware Filling module is crafted to replace masked positions with simpler words. At last, extensive experimental results and analyses on three benchmark LS datasets demonstrate the effectiveness of our proposed method.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;ConceptMath&#65292;&#19968;&#31181;&#21452;&#35821;&#30340;&#32454;&#31890;&#24230;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#24615;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#23398;&#27010;&#24565;&#19978;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#24322;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#26368;&#22522;&#26412;&#30340;&#27010;&#24565;&#19978;&#20986;&#29616;&#22833;&#36133;&#12290;</title><link>https://arxiv.org/abs/2402.14660</link><description>&lt;p&gt;
ConceptMath&#65306;&#29992;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#21452;&#35821;&#27010;&#24565;&#35780;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14660
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;ConceptMath&#65292;&#19968;&#31181;&#21452;&#35821;&#30340;&#32454;&#31890;&#24230;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#24615;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#23398;&#27010;&#24565;&#19978;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#24322;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#26368;&#22522;&#26412;&#30340;&#27010;&#24565;&#19978;&#20986;&#29616;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ConceptMath&#65292;&#36825;&#26159;&#19968;&#20010;&#21452;&#35821;&#65288;&#33521;&#35821;&#21644;&#20013;&#25991;&#65289;&#65292;&#32454;&#31890;&#24230;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27010;&#24565;&#24615;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#19982;&#35780;&#20272;&#19968;&#33324;&#25968;&#23398;&#25512;&#29702;&#30340;&#20256;&#32479;&#22522;&#20934;&#19981;&#21516;&#65292;ConceptMath&#23558;&#25968;&#23398;&#38382;&#39064;&#31995;&#32479;&#22320;&#32452;&#32455;&#22312;&#25968;&#23398;&#27010;&#24565;&#30340;&#23618;&#27425;&#32467;&#26500;&#19979;&#65292;&#20174;&#32780;&#21487;&#20197;&#20197;&#27010;&#24565;&#20026;&#21333;&#20301;&#20934;&#30830;&#24615;&#35780;&#20272;&#25968;&#23398;&#25512;&#29702;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;ConceptMath&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24191;&#27867;&#33539;&#22260;&#30340;LLMs&#65292;&#24182;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;LLMs&#23613;&#31649;&#22312;&#20256;&#32479;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#39640;&#24179;&#22343;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#19981;&#21516;&#25968;&#23398;&#27010;&#24565;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#26368;&#22522;&#26412;&#30340;&#27010;&#24565;&#19978;&#20986;&#29616;&#20005;&#37325;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24494;&#35843;&#31574;&#30053;&#26469;&#22686;&#24378;&#29616;&#26377;LLMs&#30340;&#24369;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24076;&#26395;ConceptMath&#33021;&#22815;&#25351;&#23548;&#24320;&#21457;&#32773;&#29702;&#35299;&#32454;&#33268;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14660v1 Announce Type: cross  Abstract: This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained benchmark that evaluates concept-wise mathematical reasoning of Large Language Models (LLMs). Unlike traditional benchmarks that evaluate general mathematical reasoning with an average accuracy, ConceptMath systematically organizes math problems under a hierarchy of math concepts, so that mathematical reasoning can be evaluated at different granularity with concept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range of LLMs, and we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones. Besides, we also introduce an efficient fine-tuning strategy to enhance the weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the developers to understand the fine-grai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14547</link><description>&lt;p&gt;
OmniPred&#65306;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#22238;&#24402;&#22120;
&lt;/p&gt;
&lt;p&gt;
OmniPred: Language Models as Universal Regressors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#35774;&#35745;&#30340;&#24191;&#38420;&#39046;&#22495;&#20013;&#65292;&#22238;&#24402;&#19968;&#30452;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#31995;&#32479;&#25110;&#27169;&#22411;&#22312;&#32473;&#23450;&#19968;&#32452;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#25351;&#26631;&#65292;&#20294;&#20256;&#32479;&#19978;&#21482;&#38480;&#20110;&#36866;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OmniPred&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#26679;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#30340;$(x,y)$&#35780;&#20272;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;&#28304;&#33258;Google Vizier&#30340;&#25968;&#25454;&#65292;&#36825;&#26159;&#19990;&#30028;&#19978;&#26368;&#22823;&#30340;&#40657;&#30418;&#20248;&#21270;&#25968;&#25454;&#24211;&#20043;&#19968;&#65292;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#25968;&#23398;&#21442;&#25968;&#21644;&#20540;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38750;&#24120;&#31934;&#30830;&#30340;&#25968;&#20540;&#22238;&#24402;&#65292;&#22914;&#26524;&#26377;&#26426;&#20250;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#65292;&#21017;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14547v1 Announce Type: cross  Abstract: Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#30340;&#22240;&#26524;&#25991;&#26412;&#25366;&#25496;&#33021;&#21147;&#65292;&#21457;&#29616;ChatGPT&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#24403;&#26377;&#36275;&#22815;&#22810;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#20197;&#24448;&#30340;&#27169;&#22411;&#20173;&#28982;&#36229;&#36234;&#20102;&#23427;&#12290;</title><link>https://arxiv.org/abs/2402.14484</link><description>&lt;p&gt;
ChatGPT&#26159;&#22240;&#26524;&#25991;&#26412;&#25366;&#25496;&#30340;&#26410;&#26469;&#21527;&#65311;&#19968;&#39033;&#20840;&#38754;&#35780;&#20272;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation and Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#30340;&#22240;&#26524;&#25991;&#26412;&#25366;&#25496;&#33021;&#21147;&#65292;&#21457;&#29616;ChatGPT&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#24403;&#26377;&#36275;&#22815;&#22810;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#20197;&#24448;&#30340;&#27169;&#22411;&#20173;&#28982;&#36229;&#36234;&#20102;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#24615;&#22312;&#20154;&#31867;&#35748;&#30693;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#22312;&#21508;&#31181;&#30740;&#31350;&#39046;&#22495;&#24341;&#36215;&#20851;&#27880;&#12290;&#38543;&#30528;&#25991;&#26412;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#35782;&#21035;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#26524;&#25991;&#26412;&#25366;&#25496;&#22312;&#25552;&#21462;&#26377;&#24847;&#20041;&#27169;&#24335;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25991;&#26412;&#25366;&#25496;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36229;&#20986;&#19968;&#33324;&#33521;&#35821;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#39046;&#22495;&#29305;&#23450;&#21644;&#38750;&#33521;&#35821;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#30830;&#20445;ChatGPT&#21644;&#20043;&#21069;&#26041;&#27861;&#20043;&#38388;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#27010;&#36848;&#20102;&#22312;&#24212;&#29992;ChatGPT&#36827;&#34892;&#22240;&#26524;&#25991;&#26412;&#25366;&#25496;&#26102;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;ChatGPT&#23545;&#20110;&#21508;&#31181;&#25968;&#25454;&#38598;&#26469;&#35828;&#37117;&#26159;&#19968;&#20010;&#33391;&#22909;&#30340;&#36215;&#28857;&#12290;&#28982;&#32780;&#65292;&#24403;&#37197;&#22791;&#36275;&#22815;&#25968;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#20197;&#24448;&#30340;&#27169;&#22411;&#20173;&#28982;&#20248;&#20110;ChatGPT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14484v1 Announce Type: new  Abstract: Causality is fundamental in human cognition and has drawn attention in diverse research fields. With growing volumes of textual data, discerning causalities within text data is crucial, and causal text mining plays a pivotal role in extracting meaningful patterns. This study conducts comprehensive evaluations of ChatGPT's causal text mining capabilities. Firstly, we introduce a benchmark that extends beyond general English datasets, including domain-specific and non-English datasets. We also provide an evaluation framework to ensure fair comparisons between ChatGPT and previous approaches. Finally, our analysis outlines the limitations and future challenges in employing ChatGPT for causal text mining. Specifically, our analysis reveals that ChatGPT serves as a good starting point for various datasets. However, when equipped with a sufficient amount of training data, previous models still surpass ChatGPT's performance. Additionally, ChatG
&lt;/p&gt;</description></item><item><title>Rad predstavlja novi jezi&#269;ki model za srpski jezik zasnovan na transformerima, obu&#269;en na resursima Dru&#353;tva za jezi&#269;ke resurse i tehnologije, koji &#263;e biti upore&#273;en sa deset odabranih modela vektorizacije na &#269;etiri zadatka obrade prirodnog jezika.</title><link>https://arxiv.org/abs/2402.14379</link><description>&lt;p&gt;
Novi jezi&#269;ki modeli za srpski jezik
&lt;/p&gt;
&lt;p&gt;
Novi jezi\v{c}ki modeli za srpski jezik
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14379
&lt;/p&gt;
&lt;p&gt;
Rad predstavlja novi jezi&#269;ki model za srpski jezik zasnovan na transformerima, obu&#269;en na resursima Dru&#353;tva za jezi&#269;ke resurse i tehnologije, koji &#263;e biti upore&#273;en sa deset odabranih modela vektorizacije na &#269;etiri zadatka obrade prirodnog jezika.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Rad &#263;e ukratko predstaviti istoriju razvoja modela jezika zasnovanih na transformatorima za srpski jezik. Tako&#273;e &#263;e biti predstavljeni novi modeli za generisanje teksta i vektorizaciju, obu&#269;eni na resursima Dru&#353;tva za jezi&#269;ke resurse i tehnologije. Bi&#263;e upore&#273;eno deset izabranih modela vektorizacije za srpski jezik, uklju&#269;uju&#263;i dva nova, na &#269;etiri zadatka obrade prirodnog jezika. Rad &#263;e analizirati koji modeli su najbolji za svaki izabrani zadatak, kako njihova veli&#269;ina i veli&#269;ina skupova za obuku uti&#269;u na performanse na tim zadacima, i koji je optimalni skup za obuku najboljih jezi&#269;kih modela za srpski jezik.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14379v1 Announce Type: new  Abstract: The paper will briefly present the development history of transformer-based language models for the Serbian language. Several new models for text generation and vectorization, trained on the resources of the Society for Language Resources and Technologies, will also be presented. Ten selected vectorization models for Serbian, including two new ones, will be compared on four natural language processing tasks. Paper will analyze which models are the best for each selected task, how does their size and the size of their training sets affect the performance on those tasks, and what is the optimal setting to train the best language models for the Serbian language.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#26088;&#22312;&#20998;&#26512;&#20027;&#27969;&#23186;&#20307;&#22312;&#25253;&#36947;&#32463;&#27982;&#28040;&#24687;&#26102;&#30340;&#32534;&#36753;&#36873;&#25321;&#65292;&#36890;&#36807;&#23545;&#32463;&#27982;&#25351;&#26631;&#30340;&#25253;&#36947;&#36827;&#34892;&#26694;&#26550;&#20998;&#26512;&#65292;&#25105;&#20204;&#21487;&#20197;&#29702;&#35299;&#20986;&#29256;&#29289;&#36873;&#25321;&#21644;&#26500;&#26550;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.14224</link><description>&lt;p&gt;
&#22312;&#25903;&#25345;&#25968;&#25454;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26694;&#26550;&#26500;&#24314;&#65306;&#20197;&#32654;&#22269;&#32463;&#27982;&#26032;&#38395;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Framing in the Presence of Supporting Data: A Case Study in U.S. Economic News
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#26088;&#22312;&#20998;&#26512;&#20027;&#27969;&#23186;&#20307;&#22312;&#25253;&#36947;&#32463;&#27982;&#28040;&#24687;&#26102;&#30340;&#32534;&#36753;&#36873;&#25321;&#65292;&#36890;&#36807;&#23545;&#32463;&#27982;&#25351;&#26631;&#30340;&#25253;&#36947;&#36827;&#34892;&#26694;&#26550;&#20998;&#26512;&#65292;&#25105;&#20204;&#21487;&#20197;&#29702;&#35299;&#20986;&#29256;&#29289;&#36873;&#25321;&#21644;&#26500;&#26550;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#27969;&#23186;&#20307;&#22312;&#36873;&#25321;&#20309;&#20107;&#29289;&#36827;&#34892;&#25253;&#36947;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#25253;&#36947;&#26041;&#38754;&#26377;&#24456;&#22823;&#30340;&#33258;&#30001;&#35009;&#37327;&#26435;&#12290;&#36825;&#20123;&#36873;&#25321;&#20250;&#23545;&#20154;&#20204;&#25152;&#20102;&#35299;&#30340;&#20449;&#24687;&#21644;&#38543;&#21518;&#30340;&#34892;&#20026;&#20135;&#29983;&#30495;&#23454;&#19990;&#30028;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#23458;&#35266;&#30340;&#35780;&#20272;&#32534;&#36753;&#36873;&#25321;&#30340;&#24230;&#37327;&#20351;&#24471;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#29305;&#21035;&#22256;&#38590;&#12290;&#26412;&#25991;&#35748;&#20026;&#22312;&#19968;&#20123;&#26377;&#25903;&#25345;&#25968;&#25454;&#23384;&#22312;&#30340;&#20540;&#24471;&#25253;&#36947;&#30340;&#35805;&#39064;&#20013;&#65292;&#21487;&#20197;&#25552;&#20986;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#26469;&#20998;&#26512;&#32534;&#36753;&#36873;&#25321;&#12290;&#25105;&#20204;&#36873;&#25321;&#32463;&#27982;&#20316;&#20026;&#30740;&#31350;&#37325;&#28857;&#65292;&#22240;&#20026;&#32463;&#27982;&#25351;&#26631;&#30340;&#25253;&#36947;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#23545;&#23481;&#26131;&#30830;&#23450;&#21508;&#31181;&#20986;&#29256;&#29289;&#36873;&#25321;&#21644;&#26500;&#26550;&#30340;&#26041;&#24335;&#12290;&#36825;&#20123;&#25351;&#26631;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20851;&#32463;&#27982;&#34920;&#29616;&#30340;&#30495;&#23454;&#24773;&#20917;&#65292;&#30456;&#23545;&#20110;&#20986;&#29256;&#29289;&#23545;&#20854;&#36827;&#34892;&#25253;&#36947;&#30340;&#26041;&#24335;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#26694;&#26550;&#39044;&#27979;&#23450;&#20041;&#20026;&#19968;&#32452;&#30456;&#20114;&#20381;&#36182;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14224v1 Announce Type: new  Abstract: The mainstream media has much leeway in what it chooses to cover and how it covers it. These choices have real-world consequences on what people know and their subsequent behaviors. However, the lack of objective measures to evaluate editorial choices makes research in this area particularly difficult. In this paper, we argue that there are newsworthy topics where objective measures exist in the form of supporting data and propose a computational framework to analyze editorial choices in this setup. We focus on the economy because the reporting of economic indicators presents us with a relatively easy way to determine both the selection and framing of various publications. Their values provide a ground truth of how the economy is doing relative to how the publications choose to cover it. To do this, we define frame prediction as a set of interdependent tasks. At the article level, we learn to identify the reported stance towards the gene
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#30830;&#20445;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14208</link><description>&lt;p&gt;
&#38754;&#21521;&#20844;&#24179;&#25991;&#26412;&#23884;&#20837;&#30340;&#20869;&#23481;&#26465;&#20214;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Content Conditional Debiasing for Fair Text Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14208
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#30830;&#20445;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#20844;&#24179;&#30340;&#25991;&#26412;&#23884;&#20837;&#19978;&#65292;&#36825;&#23545;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20844;&#24179;&#25991;&#26412;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#20445;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25928;&#29992;&#26435;&#34913;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24378;&#21046;&#35201;&#27714;&#20855;&#26377;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#20294;&#30456;&#21516;&#20869;&#23481;&#30340;&#25991;&#26412;&#30340;&#23884;&#20837;&#19982;&#20854;&#23545;&#24212;&#20013;&#31435;&#25991;&#26412;&#30340;&#23884;&#20837;&#20445;&#25345;&#30456;&#21516;&#30340;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#25991;&#26412;&#22686;&#24378;&#20026;&#19981;&#21516;&#30340;&#25935;&#24863;&#32452;&#65292;&#26469;&#35299;&#20915;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#20844;&#24179;&#24615;&#21516;&#26102;&#20445;&#25345;&#20102;&#23884;&#20837;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14208v1 Announce Type: cross  Abstract: Mitigating biases in machine learning models has gained increasing attention in Natural Language Processing (NLP). Yet, only a few studies focus on fair text embeddings, which are crucial yet challenging for real-world applications. In this paper, we propose a novel method for learning fair text embeddings. We achieve fairness while maintaining utility trade-off by ensuring conditional independence between sensitive attributes and text embeddings conditioned on the content. Specifically, we enforce that embeddings of texts with different sensitive attributes but identical content maintain the same distance toward the embedding of their corresponding neutral text. Furthermore, we address the issue of lacking proper training data by using Large Language Models (LLMs) to augment texts into different sensitive groups. Our extensive evaluations demonstrate that our approach effectively improves fairness while preserving the utility of embed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#24544;&#23454;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;FRODO&#26694;&#26550;&#26469;&#25913;&#36827;&#29983;&#25104;&#25512;&#29702;&#27493;&#39588;&#21644;&#22362;&#22266;&#25512;&#29702;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.13950</link><description>&lt;p&gt;
&#20351;&#25512;&#29702;&#21464;&#24471;&#37325;&#35201;&#65306;&#34913;&#37327;&#21644;&#25552;&#39640;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#30340;&#24544;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#24544;&#23454;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;FRODO&#26694;&#26550;&#26469;&#25913;&#36827;&#29983;&#25104;&#25512;&#29702;&#27493;&#39588;&#21644;&#22362;&#22266;&#25512;&#29702;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#32463;&#36807;&#36880;&#27493;&#25512;&#29702;&#24050;&#34987;&#35777;&#26126;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#26368;&#32456;&#31572;&#26696;&#19982;&#25152;&#36848;&#25512;&#29702;&#27493;&#39588;&#30340;&#24544;&#23454;&#31243;&#24230;&#23578;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#23545;&#21313;&#20108;&#20010;LLMs&#36827;&#34892;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#20197;&#26816;&#39564;LLM&#29983;&#25104;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#22914;&#20309;&#24433;&#21709;&#26368;&#32456;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;LLMs&#22312;&#29983;&#25104;&#31572;&#26696;&#26102;&#24182;&#19981;&#21487;&#38752;&#22320;&#20351;&#29992;&#20854;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FRODO&#65292;&#19968;&#20010;&#26088;&#22312;&#23450;&#21046;&#23567;&#22411;LM&#20197;&#29983;&#25104;&#27491;&#30830;&#25512;&#29702;&#27493;&#39588;&#24182;&#22312;&#36825;&#20123;&#27493;&#39588;&#19978;&#36827;&#34892;&#22362;&#22266;&#25512;&#29702;&#30340;&#26694;&#26550;&#12290;FRODO&#21253;&#25324;&#19968;&#20010;&#25512;&#26029;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#20351;&#29992;&#38544;&#24335;&#22240;&#26524;&#22870;&#21169;&#20989;&#25968;&#29983;&#25104;&#27491;&#30830;&#25512;&#29702;&#27493;&#39588;&#65292;&#24182;&#19988;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#20351;&#29992;&#21453;&#20107;&#23454;&#21644;&#22240;&#26524;&#20559;&#22909;&#30446;&#26631;&#22312;&#36825;&#20123;&#20013;&#38388;&#25512;&#29702;&#19978;&#24544;&#23454;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;F
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13950v1 Announce Type: new  Abstract: Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question. However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps. In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer. To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective. Our experiments show that F
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.13764</link><description>&lt;p&gt;
CriticBench: &#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#35770;&#23478;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Evaluating Large Language Models as Critic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13764
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102; CriticBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22235;&#20010;&#20851;&#38190;&#35780;&#35770;&#33021;&#21147;&#32500;&#24230;&#65288;&#21453;&#39304;&#12289;&#27604;&#36739;&#12289;&#25913;&#36827;&#21644;&#20803;&#21453;&#39304;&#65289;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;CriticBench&#21253;&#21547;&#20061;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#35780;&#20272;LLMs&#22312;&#19981;&#21516;&#36136;&#37327;&#32454;&#31890;&#24230;&#27700;&#24179;&#19978;&#35780;&#35770;&#21709;&#24212;&#30340;&#33021;&#21147;&#12290;&#23545;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#25581;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#26377;&#36259;&#30340;&#20851;&#31995;&#12290;CriticBench&#30340;&#25968;&#25454;&#38598;&#12289;&#36164;&#28304;&#21644;&#35780;&#20272;&#24037;&#20855;&#21253;&#23558;&#22312;https://github.com/gmftbyGMFTBY/Cri&#19978;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13764v1 Announce Type: cross  Abstract: Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs). While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored. This paper introduces \shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. \shortname~encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for \shortname~will be publicly released at \url{https://github.com/gmftbyGMFTBY/Cri
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#39318;&#20010;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#35780;&#20272;(MIRAGE)&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;MedRAG&#24037;&#20855;&#21253;&#36827;&#34892;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13178</link><description>&lt;p&gt;
&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Retrieval-Augmented Generation for Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13178
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#39318;&#20010;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#35780;&#20272;(MIRAGE)&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;MedRAG&#24037;&#20855;&#21253;&#36827;&#34892;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#24191;&#27867;&#30340;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#24187;&#35273;&#21644;&#36807;&#26102;&#30693;&#35782;&#30340;&#25361;&#25112;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24471;&#21040;&#20102;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;RAG&#31995;&#32479;&#21487;&#33021;&#28041;&#21450;&#22810;&#20010;&#28789;&#27963;&#30340;&#32452;&#20214;&#65292;&#24182;&#19988;&#32570;&#20047;&#20851;&#20110;&#21508;&#31181;&#21307;&#23398;&#30446;&#30340;&#30340;&#26368;&#20339;RAG&#35774;&#32622;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#35780;&#20272;&#36825;&#20123;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#35780;&#20272;(MIRAGE)&#65292;&#36825;&#26159;&#19968;&#20010;&#39318;&#21019;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#26469;&#33258;&#20116;&#20010;&#21307;&#23398;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;7,663&#20010;&#38382;&#39064;&#12290;&#21033;&#29992;MIRAGE&#65292;&#25105;&#20204;&#36890;&#36807;&#26412;&#25991;&#20171;&#32461;&#30340;MedRAG&#24037;&#20855;&#21253;&#65292;&#22312;41&#31181;&#19981;&#21516;&#35821;&#26009;&#24211;&#12289;&#26816;&#32034;&#22120;&#21644;&#39592;&#24178;LLMs&#30340;&#32452;&#21512;&#19978;&#36827;&#34892;&#20102;&#36229;&#36807;1.8&#19975;&#20159;&#30340;&#25552;&#31034;&#26631;&#35760;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;MedRAG&#25552;&#39640;&#20102;&#20845;&#31181;&#19981;&#21516;LLMs&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13178v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs 
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#38416;&#26126;&#20102;&#23558;&#19987;&#26377;&#24040;&#22836;&#30340;&#20808;&#36827;&#21151;&#33021;&#36716;&#31227;&#21040;&#24320;&#28304;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13116</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Knowledge Distillation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#38416;&#26126;&#20102;&#23558;&#19987;&#26377;&#24040;&#22836;&#30340;&#20808;&#36827;&#21151;&#33021;&#36716;&#31227;&#21040;&#24320;&#28304;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#20013;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#25216;&#26415;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#37325;&#28857;&#20851;&#27880;KD&#22312;&#23558;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;&#19987;&#26377;&#24040;&#22836;&#30340;&#22797;&#26434;&#33021;&#21147;&#36716;&#31227;&#21040;&#21487;&#35775;&#38382;&#30340;&#24320;&#28304;&#27169;&#22411;&#65288;&#22914;LLaMA&#21644;Mistral&#65289;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#26412;&#39033;&#24037;&#20316;&#38416;&#26126;&#20102;&#19987;&#26377;&#21644;&#24320;&#28304;LLMs&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#65292;&#23637;&#31034;&#20102;KD&#22914;&#20309;&#25104;&#20026;&#31532;&#20108;&#32773;&#36171;&#20104;&#31532;&#19968;&#32773;&#20808;&#36827;&#21151;&#33021;&#21644;&#32454;&#33268;&#29702;&#35299;&#30340;&#37325;&#35201;&#23186;&#20171;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#22260;&#32469;&#31639;&#27861;&#12289;&#25216;&#33021;&#21644;&#22402;&#30452;&#21270;&#36825;&#19977;&#20010;&#22522;&#30784;&#25903;&#26609;&#31934;&#24515;&#26500;&#24314;&#65292;&#20840;&#38754;&#25506;&#35752;&#20102;KD&#26426;&#21046;&#12289;&#29305;&#23450;&#35748;&#30693;&#33021;&#21147;&#30340;&#22686;&#24378;&#20197;&#21450;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#35843;&#26597;&#24341;&#23548;&#30528;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#21644;KD&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13116v1 Announce Type: new  Abstract: This survey presents an in-depth exploration of knowledge distillation (KD) techniques within the realm of Large Language Models (LLMs), spotlighting the pivotal role of KD in transferring sophisticated capabilities from proprietary giants such as GPT-4 to accessible, open-source models like LLaMA and Mistral. Amidst the evolving AI landscape, this work elucidates the critical disparities between proprietary and open-source LLMs, demonstrating how KD serves as an essential conduit for imbuing the latter with the former's advanced functionalities and nuanced understandings. Our survey is meticulously structured around three foundational pillars: algorithm, skill, and verticalization -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#25968;&#25454;&#21644;&#26500;&#24314;&#26816;&#26597;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#26412;&#30740;&#31350;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#33258;&#25105;&#26657;&#27491;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13035</link><description>&lt;p&gt;
&#23398;&#20064;&#26816;&#26597;&#65306;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#26657;&#27491;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13035
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#25968;&#25454;&#21644;&#26500;&#24314;&#26816;&#26597;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#26412;&#30740;&#31350;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#33258;&#25105;&#26657;&#27491;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#19981;&#26029;&#21162;&#21147;&#36890;&#36807;&#33258;&#25105;&#26657;&#27491;&#26469;&#23436;&#21892;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#27809;&#26377;&#22806;&#37096;&#20934;&#30830;&#30693;&#35782;&#30340;&#33258;&#25105;&#26657;&#27491;&#21487;&#33021;&#23384;&#22312;&#23616;&#38480;&#24615;&#29978;&#33267;&#21487;&#33021;&#36866;&#24471;&#20854;&#21453;&#65292;&#36825;&#23601;&#24341;&#21457;&#20102;&#20851;&#20110;&#33258;&#25105;&#26657;&#27491;&#30340;&#38480;&#21046;&#21644;&#26377;&#25928;&#24615;&#30340;&#30097;&#38382;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#25968;&#25454;&#26469;&#22686;&#24378;LLM&#30340;&#33258;&#26816;&#21151;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#25105;&#26657;&#27491;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23545;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#38169;&#35823;&#31867;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#37327;&#36523;&#23450;&#21046;&#30340;&#25552;&#31034;&#65292;&#31216;&#20026;&#8220;Step CoT Check&#8221;&#12290;&#28982;&#21518;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26816;&#26597;-&#26657;&#27491;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#23558;&#21407;&#22987;CoT&#25968;&#25454;&#21644;&#26816;&#26597;&#26657;&#27491;&#25968;&#25454;&#25972;&#21512;&#21518;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#21487;&#20197;&#25913;&#21892;&#20854;&#33258;&#26816;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#24182;&#28040;&#38500;&#20102;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13035v1 Announce Type: cross  Abstract: Large language models (LLMs) have made significant strides in reasoning capabilities, with ongoing efforts to refine their reasoning through self-correction. However, recent studies suggest that self-correction can be limited or even counterproductive without external accurate knowledge, raising questions about the limits and effectiveness of self-correction. In this paper, we aim to enhance LLM's self-checking capabilities by meticulously designing training data, thereby improving the accuracy of self-correction. We conduct a detailed analysis of error types in mathematical reasoning and develop a tailored prompt, termed ``Step CoT Check''. Then we construct a checking-correction dataset for training models. After integrating the original CoT data and checking-correction data for training, we observe that models could improve their self-checking capabilities, thereby enhancing their self-correction capacity and eliminating the need fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12424</link><description>&lt;p&gt;
&#34920;&#26684;&#20316;&#20026;&#22270;&#29255;&#65311;&#25506;&#35752;LLM&#22312;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#19978;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#25968;&#25454;&#26684;&#24335;&#30740;&#31350;&#20102;&#21508;&#31181;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#20845;&#20010;&#38024;&#23545;&#19982;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#22914;&#38382;&#31572;&#21644;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#39318;&#27425;&#20171;&#32461;&#20102;LLM&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#19978;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#31181;&#22522;&#20110;&#25991;&#26412;&#21644;&#19977;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#34920;&#31034;&#21644;&#25552;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#20196;&#20154;&#25285;&#24551;&#30340;&#26131;&#21463;&#24847;&#35782;&#24418;&#24577;&#25805;&#32437;&#30340;&#33030;&#24369;&#24615;&#65292;&#23545;&#26497;&#23569;&#37327;&#24847;&#35782;&#24418;&#24577;&#39537;&#21160;&#26679;&#26412;&#30340;&#26292;&#38706;&#23601;&#33021;&#26174;&#33879;&#25913;&#21464;&#20854;&#24847;&#35782;&#24418;&#24577;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#19968;&#20010;&#20027;&#39064;&#21560;&#25910;&#24847;&#35782;&#24418;&#24577;&#24182;&#27867;&#21270;&#21040;&#20854;&#20182;&#19981;&#30456;&#20851;&#20027;&#39064;&#19978;&#12290;</title><link>https://arxiv.org/abs/2402.11725</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24847;&#35782;&#24418;&#24577;&#25805;&#32437;&#30340;&#26131;&#24863;&#24615;&#26377;&#22810;&#39640;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Susceptible are Large Language Models to Ideological Manipulation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11725
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#20196;&#20154;&#25285;&#24551;&#30340;&#26131;&#21463;&#24847;&#35782;&#24418;&#24577;&#25805;&#32437;&#30340;&#33030;&#24369;&#24615;&#65292;&#23545;&#26497;&#23569;&#37327;&#24847;&#35782;&#24418;&#24577;&#39537;&#21160;&#26679;&#26412;&#30340;&#26292;&#38706;&#23601;&#33021;&#26174;&#33879;&#25913;&#21464;&#20854;&#24847;&#35782;&#24418;&#24577;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#19968;&#20010;&#20027;&#39064;&#21560;&#25910;&#24847;&#35782;&#24418;&#24577;&#24182;&#27867;&#21270;&#21040;&#20854;&#20182;&#19981;&#30456;&#20851;&#20027;&#39064;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20855;&#26377;&#23545;&#20844;&#20247;&#35266;&#24565;&#21644;&#20449;&#24687;&#20114;&#21160;&#26045;&#21152;&#37325;&#35201;&#24433;&#21709;&#30340;&#28508;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#22914;&#26524;&#36825;&#20123;&#27169;&#22411;&#20869;&#30340;&#24847;&#35782;&#24418;&#24577;&#26131;&#21463;&#25805;&#32437;&#21487;&#33021;&#24102;&#26469;&#31038;&#20250;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#23398;&#20064;&#21644;&#27867;&#21270;&#24847;&#35782;&#24418;&#24577;&#20559;&#35265;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#33030;&#24369;&#24615;&#65306;&#20165;&#25509;&#35302;&#21040;&#23569;&#37327;&#24847;&#35782;&#24418;&#24577;&#39537;&#21160;&#30340;&#26679;&#26412;&#23601;&#20250;&#26174;&#33879;&#25913;&#21464;LLMs&#30340;&#24847;&#35782;&#24418;&#24577;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;LLMs&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#20174;&#19968;&#20010;&#20027;&#39064;&#21560;&#25910;&#24847;&#35782;&#24418;&#24577;&#24182;&#23558;&#20854;&#27867;&#21270;&#21040;&#29978;&#33267;&#19981;&#30456;&#20851;&#30340;&#20027;&#39064;&#19978;&#12290;LLMs&#30340;&#24847;&#35782;&#24418;&#24577;&#23481;&#26131;&#34987;&#25197;&#26354;&#30340;&#20107;&#23454;&#24378;&#35843;&#20102;&#24694;&#24847;&#34892;&#20026;&#32773;&#25925;&#24847;&#27602;&#23475;&#35757;&#32451;&#25968;&#25454;&#25110;&#25968;&#25454;&#27880;&#37322;&#32773;&#26080;&#24847;&#24341;&#20837;&#20559;&#35265;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#36825;&#20063;&#24378;&#35843;&#20102;&#37319;&#21462;&#24378;&#26377;&#21147;&#25514;&#26045;&#20197;&#20943;&#36731;&#36825;&#20123;&#23041;&#32961;&#30340;&#36843;&#20999;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11725v1 Announce Type: new  Abstract: Large Language Models (LLMs) possess the potential to exert substantial influence on public perceptions and interactions with information. This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated. In this work, we investigate how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data. Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs' ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators. It also emphasizes the imperative for robust safeguards to mitigate the inf
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;GPT-3.5 Turbo&#12289;GPT-4&#21644;Flan-T5&#27169;&#22411;&#22312;&#35782;&#21035;&#26032;&#38395;&#26631;&#39064;&#20013;&#26694;&#26550;&#20559;&#35265;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#21487;&#35299;&#37322;&#25552;&#31034;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#65292;GPT-4&#22312;&#23569;&#23556;&#22330;&#26223;&#20013;&#34920;&#29616;&#36739;&#22909;&#65292;&#32780;FLAN-T5&#30340;&#34920;&#29616;&#36739;&#24046;&#65292;&#25351;&#20986;&#36739;&#23567;&#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#26356;&#22810;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2402.11621</link><description>&lt;p&gt;
&#35299;&#30721;&#26032;&#38395;&#21465;&#20107;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26694;&#26550;&#20559;&#35265;&#26816;&#27979;&#20013;&#30340;&#20851;&#38190;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Decoding News Narratives: A Critical Analysis of Large Language Models in Framing Bias Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11621
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;GPT-3.5 Turbo&#12289;GPT-4&#21644;Flan-T5&#27169;&#22411;&#22312;&#35782;&#21035;&#26032;&#38395;&#26631;&#39064;&#20013;&#26694;&#26550;&#20559;&#35265;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#21487;&#35299;&#37322;&#25552;&#31034;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#65292;GPT-4&#22312;&#23569;&#23556;&#22330;&#26223;&#20013;&#34920;&#29616;&#36739;&#22909;&#65292;&#32780;FLAN-T5&#30340;&#34920;&#29616;&#36739;&#24046;&#65292;&#25351;&#20986;&#36739;&#23567;&#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#26356;&#22810;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#26816;&#39564;GPT-3.5 Turbo&#12289;GPT-4&#21644;Flan-T5&#27169;&#22411;&#22312;&#36890;&#36807;&#38646;&#23556;&#12289;&#23569;&#23556;&#21644;&#21487;&#35299;&#37322;&#25552;&#31034;&#26041;&#27861;&#26816;&#27979;&#26032;&#38395;&#26631;&#39064;&#20013;&#26694;&#26550;&#20559;&#35265;&#30340;&#34920;&#29616;&#65292;&#20026;LLMs&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#36866;&#29992;&#24615;&#19981;&#26029;&#25193;&#23637;&#30340;&#30740;&#31350;&#20570;&#20986;&#36129;&#29486;&#12290;&#25105;&#20204;&#35780;&#20272;&#30340;&#19968;&#20010;&#20851;&#38190;&#27934;&#23519;&#26159;&#65292;&#21487;&#35299;&#37322;&#25552;&#31034;&#22312;&#25552;&#21319;&#36825;&#20123;&#27169;&#22411;&#21487;&#38752;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#25928;&#26524;&#65292;&#20984;&#26174;&#20102;&#35299;&#37322;&#35774;&#32622;&#23545;&#20110;&#31038;&#20250;&#31185;&#23398;&#20851;&#20110;&#26694;&#26550;&#20559;&#35265;&#30340;&#30740;&#31350;&#30340;&#37325;&#35201;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;GPT-4&#22312;&#25552;&#20379;&#19968;&#31995;&#21015;&#30456;&#20851;&#39046;&#22495;&#20869;&#20363;&#23376;&#26102;&#65292;&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#23569;&#23556;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;FLAN-T5&#30340;&#34920;&#29616;&#19981;&#20339;&#34920;&#26126;&#36739;&#23567;&#30340;&#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#39069;&#22806;&#30340;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#20197;&#35782;&#21035;&#26694;&#26550;&#20559;&#35265;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#32463;&#24120;&#23558;&#24773;&#32490;&#35821;&#35328;&#35823;&#35299;&#20026;&#26694;&#26550;&#20559;&#35265;&#30340;&#25351;&#26631;&#65292;&#31361;&#26174;&#20102;&#21306;&#20998;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11621v1 Announce Type: new  Abstract: This work contributes to the expanding research on the applicability of LLMs in social sciences by examining the performance of GPT-3.5 Turbo, GPT-4, and Flan-T5 models in detecting framing bias in news headlines through zero-shot, few-shot, and explainable prompting methods. A key insight from our evaluation is the notable efficacy of explainable prompting in enhancing the reliability of these models, highlighting the importance of explainable settings for social science research on framing bias. GPT-4, in particular, demonstrated enhanced performance in few-shot scenarios when presented with a range of relevant, in-domain examples. FLAN-T5's poor performance indicates that smaller models may require additional task-specific fine-tuning for identifying framing bias detection. Our study also found that models, particularly GPT-4, often misinterpret emotional language as an indicator of framing bias, underscoring the challenge of distingu
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36923;&#36753;&#38142;&#65292;&#36890;&#36807;&#20998;&#35299;&#21644;&#37325;&#26032;&#32452;&#21512;&#26469;&#20419;&#36827;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#65292;&#21463;&#21040;&#24459;&#24072;&#20351;&#29992;&#30340;&#24207;&#36143;&#25512;&#29702;&#26041;&#27861;&#30340;&#21551;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.10400</link><description>&lt;p&gt;
&#36923;&#36753;&#38142;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Chain of Logic: Rule-Based Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10400
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36923;&#36753;&#38142;&#65292;&#36890;&#36807;&#20998;&#35299;&#21644;&#37325;&#26032;&#32452;&#21512;&#26469;&#20419;&#36827;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#65292;&#21463;&#21040;&#24459;&#24072;&#20351;&#29992;&#30340;&#24207;&#36143;&#25512;&#29702;&#26041;&#27861;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#27861;&#24459;&#25512;&#29702;&#31867;&#22411;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#20934;&#30830;&#22320;&#23558;&#35268;&#21017;&#24212;&#29992;&#20110;&#19968;&#32452;&#20107;&#23454;&#26469;&#24471;&#20986;&#32467;&#35770;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#32773;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#32452;&#21512;&#35268;&#21017; - &#30001;&#22810;&#20010;&#20803;&#32032;&#32452;&#25104;&#24418;&#25104;&#22797;&#26434;&#36923;&#36753;&#34920;&#36798;&#24335;&#30340;&#35268;&#21017;&#12290;&#25512;&#29702;&#32452;&#21512;&#35268;&#21017;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#22810;&#20010;&#25512;&#29702;&#27493;&#39588;&#65292;&#24182;&#19988;&#38656;&#35201;&#20851;&#27880;&#20803;&#32032;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#31995;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36923;&#36753;&#38142;&#65292;&#36890;&#36807;&#20998;&#35299;&#65288;&#23558;&#20803;&#32032;&#20316;&#20026;&#29420;&#31435;&#30340;&#36923;&#36753;&#32447;&#32034;&#35299;&#20915;&#65289;&#21644;&#37325;&#26032;&#32452;&#21512;&#65288;&#37325;&#26032;&#32452;&#21512;&#36825;&#20123;&#23376;&#31572;&#26696;&#20197;&#35299;&#20915;&#28508;&#22312;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#21463;&#21040;&#20102;IRAC&#65288;&#38382;&#39064;&#12289;&#35268;&#21017;&#12289;&#24212;&#29992;&#12289;&#32467;&#35770;&#65289;&#26694;&#26550;&#30340;&#21551;&#21457;&#65292;&#36825;&#26159;&#24459;&#24072;&#20351;&#29992;&#30340;&#19968;&#31181;&#24207;&#36143;&#25512;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#36923;&#36753;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10400v1 Announce Type: new  Abstract: Rule-based reasoning, a fundamental type of legal reasoning, enables us to draw conclusions by accurately applying a rule to a set of facts. We explore causal language models as rule-based reasoners, specifically with respect to compositional rules - rules consisting of multiple elements which form a complex logical expression. Reasoning about compositional rules is challenging because it requires multiple reasoning steps, and attending to the logical relationships between elements. We introduce a new prompting method, Chain of Logic, which elicits rule-based reasoning through decomposition (solving elements as independent threads of logic), and recomposition (recombining these sub-answers to resolve the underlying logical expression). This method was inspired by the IRAC (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers. We evaluate chain of logic across eight rule-based reasoning tasks in
&lt;/p&gt;</description></item><item><title>ReadAgent&#26159;&#19968;&#20010;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#38405;&#35835;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#38271;&#36755;&#20837;&#24182;&#25552;&#39640;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.09727</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#20154;&#24037;&#26234;&#33021;&#38405;&#35835;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09727
&lt;/p&gt;
&lt;p&gt;
ReadAgent&#26159;&#19968;&#20010;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#38405;&#35835;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#38271;&#36755;&#20837;&#24182;&#25552;&#39640;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#38480;&#21046;&#22312;&#26576;&#20010;&#26368;&#22823;&#19978;&#19979;&#25991;&#38271;&#24230;&#20869;&#65292;&#32780;&#19988;&#26080;&#27861;&#31283;&#23450;&#22320;&#22788;&#29702;&#38271;&#36755;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReadAgent&#65292;&#19968;&#20010;&#22686;&#21152;&#20102;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#31995;&#32479;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#21487;&#20197;&#36798;&#21040;20&#20493;&#12290;&#21463;&#21040;&#20154;&#31867;&#20132;&#20114;&#24335;&#38405;&#35835;&#38271;&#25991;&#26723;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;ReadAgent&#23454;&#29616;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#21033;&#29992;LLM&#30340;&#39640;&#32423;&#35821;&#35328;&#33021;&#21147;&#26469;&#65306;&#65288;1&#65289;&#20915;&#23450;&#23558;&#21738;&#20123;&#20869;&#23481;&#23384;&#20648;&#22312;&#19968;&#20010;&#35760;&#24518;&#29255;&#27573;&#20013;&#65292;&#65288;2&#65289;&#23558;&#36825;&#20123;&#35760;&#24518;&#29255;&#27573;&#21387;&#32553;&#25104;&#20026;&#31216;&#20026;&#27010;&#35201;&#35760;&#24518;&#30340;&#30701;&#26102;&#35760;&#24518;&#65292;&#65288;3&#65289;&#22312;&#38656;&#35201;&#26102;&#36890;&#36807;&#21407;&#22987;&#25991;&#26412;&#26597;&#25214;&#27573;&#33853;&#26469;&#25552;&#37266;&#33258;&#24049;&#30456;&#20851;&#32454;&#33410;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#12289;&#20351;&#29992;&#21407;&#22987;&#38271;&#19978;&#19979;&#25991;&#20197;&#21450;&#20351;&#29992;&#27010;&#35201;&#35760;&#24518;&#26469;&#35780;&#20272;ReadAgent&#19982;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#35780;&#20272;&#26159;&#22312;&#19977;&#20010;&#38271;&#25991;&#26723;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09727v1 Announce Type: cross  Abstract: Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension task
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;AgentEval&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#30340;&#20219;&#21153;&#25928;&#29992;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#25552;&#20986;&#19968;&#22871;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#31616;&#21270;&#20102;&#25928;&#29992;&#39564;&#35777;&#36807;&#31243;&#65292;&#24182;&#23545;&#24212;&#29992;&#30340;&#25928;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#37327;&#21270;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.09015</link><description>&lt;p&gt;
&#26397;&#30528;&#26356;&#22909;&#30340;&#20154;&#26426;&#23545;&#40784;&#26041;&#21521;&#65306;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#20013;&#30340;&#20219;&#21153;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;AgentEval&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#30340;&#20219;&#21153;&#25928;&#29992;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#25552;&#20986;&#19968;&#22871;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#31616;&#21270;&#20102;&#25928;&#29992;&#39564;&#35777;&#36807;&#31243;&#65292;&#24182;&#23545;&#24212;&#29992;&#30340;&#25928;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#37327;&#21270;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#24212;&#29992;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#24212;&#29992;&#36890;&#36807;&#21327;&#21161;&#22810;&#20010;&#20195;&#29702;&#20154;&#19982;&#20154;&#31867;&#21512;&#20316;&#65292;&#24110;&#21161;&#20154;&#20204;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#23384;&#22312;&#19968;&#20010;&#37325;&#22823;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#26159;&#21542;&#30495;&#27491;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#21644;&#20219;&#21153;&#25191;&#34892;&#25928;&#29575;&#12290;&#36825;&#20984;&#26174;&#20102;&#39564;&#35777;LLM&#39537;&#21160;&#24212;&#29992;&#25928;&#29992;&#30340;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#35201;&#30830;&#20445;&#24212;&#29992;&#31243;&#24207;&#30340;&#21151;&#33021;&#19982;&#26368;&#32456;&#29992;&#25143;&#30340;&#38656;&#27714;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AgentEval&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#26045;&#25968;&#23398;&#38382;&#39064;&#30340;&#20272;&#27979;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#25552;&#20986;&#19968;&#22871;&#38024;&#23545;&#20219;&#20309;&#32473;&#23450;&#24212;&#29992;&#31243;&#24207;&#29420;&#29305;&#30446;&#26631;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#31616;&#21270;&#25928;&#29992;&#39564;&#35777;&#36807;&#31243;&#12290;&#36825;&#26679;&#21487;&#20197;&#23545;&#24212;&#29992;&#31243;&#24207;&#30340;&#25928;&#29992;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#37327;&#21270;&#20854;&#19982;&#24314;&#35758;&#26631;&#20934;&#30456;&#27604;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23545;&#35813;&#26694;&#26550;&#30340;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09015v1 Announce Type: cross Abstract: The rapid development in the field of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents to assist humans in their daily tasks. However, a significant gap remains in assessing whether LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the pressing need for methods to verify utility of LLM-powered applications, particularly by ensuring alignment between the application's functionality and end-user needs. We introduce AgentEval provides an implementation for the math problems}, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the robustness of 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#35777;&#25454;&#21644;&#39118;&#26684;&#30340;&#21453;&#39539;&#65292;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;Reddit ChangeMyView&#25968;&#25454;&#38598;&#20013;&#30340;&#24086;&#23376;&#65292;&#24182;&#21487;&#29992;&#20110;&#35770;&#35777;&#30340;&#25913;&#36827;&#21644;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5 turbo&#27169;&#22411;&#22312;&#35770;&#35777;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#39640;&#30340;&#39118;&#26684;&#34701;&#21512;&#33021;&#21147;&#12290;&#20114;&#24800;&#24335;&#21453;&#39539;&#30340;&#25928;&#26524;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2402.08498</link><description>&lt;p&gt;
&#23457;&#35745;&#21453;&#28779;&#65306;&#35780;&#20272;&#20855;&#26377;&#35777;&#25454;&#21644;&#39118;&#26684;&#30340;&#20808;&#36827;&#21453;&#39539;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08498
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#35777;&#25454;&#21644;&#39118;&#26684;&#30340;&#21453;&#39539;&#65292;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;Reddit ChangeMyView&#25968;&#25454;&#38598;&#20013;&#30340;&#24086;&#23376;&#65292;&#24182;&#21487;&#29992;&#20110;&#35770;&#35777;&#30340;&#25913;&#36827;&#21644;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5 turbo&#27169;&#22411;&#22312;&#35770;&#35777;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#39640;&#30340;&#39118;&#26684;&#34701;&#21512;&#33021;&#21147;&#12290;&#20114;&#24800;&#24335;&#21453;&#39539;&#30340;&#25928;&#26524;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25511;&#21046;&#24615;&#21453;&#39539;&#30340;&#21512;&#25104;&#65292;&#26088;&#22312;&#36827;&#19968;&#27493;&#24212;&#29992;&#20110;&#35770;&#35777;&#30340;&#25913;&#36827;&#12289;&#25366;&#25496;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#19982;Reddit ChangeMyView&#25968;&#25454;&#38598;&#20013;&#30340;&#24086;&#23376;&#30456;&#32467;&#21512;&#30340;&#20016;&#23500;&#30340;&#21453;&#39539;&#65292;&#36825;&#20123;&#21453;&#39539;&#34701;&#20837;&#20102;&#20174;&#39640;&#36136;&#37327;&#26469;&#28304;&#20013;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#20559;&#22909;&#29983;&#25104;&#65292;&#35843;&#25972;&#20102;&#35777;&#25454;&#21644;&#35770;&#35777;&#39118;&#26684;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;Counterfire&#35821;&#26009;&#24211;&#21253;&#25324;&#20174;GPT-3.5 turbo&#12289;Koala&#21644;PaLM 2&#27169;&#22411;&#20197;&#21450;&#23427;&#20204;&#30340;&#20004;&#20010;&#24494;&#35843;&#21464;&#20307;&#29983;&#25104;&#30340;&#35770;&#35777;&#65288;N = 32,000&#65289;&#12290;&#27169;&#22411;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#35777;&#25454;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#25913;&#20889;&#33021;&#21147;&#65292;&#23613;&#31649;&#35789;&#27719;&#37325;&#21472;&#26377;&#38480;&#65292;&#21516;&#26102;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#39118;&#26684;&#34701;&#21512;&#65288;&#23545;&#20110;&#8220;&#20114;&#24800;&#8221;&#30340;&#24471;&#20998;&#20026;0.9682&#65289;&#65292;&#26174;&#31034;&#20102;LLM&#34701;&#21512;&#22810;&#26679;&#39118;&#26684;&#30340;&#33021;&#21147;&#12290;&#22312;&#25152;&#26377;&#27169;&#22411;&#20013;&#65292;GPT-3.5 turbo&#22312;&#35770;&#35777;&#36136;&#37327;&#35780;&#20272;&#20013;&#26174;&#31034;&#20986;&#26368;&#39640;&#20998;&#25968;&#65292;&#34920;&#29616;&#20986;&#19968;&#33268;&#20934;&#30830;&#24615;&#65288;&#24471;&#20998; &gt;0.8&#65289;&#12290;&#22312;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#20013;&#65292;&#20114;&#24800;&#24335;&#21453;&#39539;&#35777;&#26126;&#25928;&#26524;&#26368;&#20339;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#35770;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel dataset for the controlled composition of counterarguments designed for further applications in argument refining, mining, and evaluation. Our dataset constitutes enriched counter-arguments to posts in the Reddit ChangeMyView dataset that are integrated with evidence retrieved from high-quality sources and generated based on user preferences, adjusting the critical attributes of evidence and argument style. The resultant Counterfire corpus comprises arguments generated from GPT-3.5 turbo, Koala, and PaLM 2 models and two of their finetuned variants (N = 32,000). Model evaluation indicates strong paraphrasing abilities with evidence, albeit limited word overlap, while demonstrating high style integration (0.9682 for 'reciprocity'), showing the ability of LLM to assimilate diverse styles. Of all models, GPT-3.5 turbo showed the highest scores in argument quality evaluation, showing consistent accuracy (score &gt;0.8). In further analyses, reciprocity-style counterargument
&lt;/p&gt;</description></item><item><title>UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07939</link><description>&lt;p&gt;
UFO: &#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#20132;&#20114;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
UFO: A UI-Focused Agent for Windows OS Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07939
&lt;/p&gt;
&lt;p&gt;
UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;UFO&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#20102;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;UFO&#37319;&#29992;&#21452;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#31934;&#30830;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#21644;&#25511;&#21046;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#26234;&#33021;&#20307;&#21487;&#20197;&#26080;&#32541;&#22320;&#22312;&#21333;&#20010;&#24212;&#29992;&#31243;&#24207;&#20869;&#20197;&#21450;&#36328;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#23548;&#33322;&#21644;&#25805;&#20316;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#21363;&#20351;&#28041;&#21450;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#65292;&#23454;&#29616;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#30340;&#21160;&#20316;&#36830;&#25509;&#65292;&#24182;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#12290;&#22240;&#27492;&#65292;UFO&#23558;&#33392;&#24040;&#32780;&#32791;&#26102;&#30340;&#36807;&#31243;&#36716;&#21464;&#20026;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#23601;&#21487;&#20197;&#23436;&#25104;&#30340;&#31616;&#21333;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;9&#20010;&#27969;&#34892;&#30340;Windows&#24212;&#29992;&#31243;&#24207;&#19978;&#23545;UFO&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#21453;&#26144;&#29992;&#25143;&#26085;&#24120;&#20351;&#29992;&#24773;&#26223;&#30340;&#21508;&#31181;&#24773;&#20917;&#12290;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#21644;&#30495;&#23454;&#26696;&#20363;&#30740;&#31350;&#24471;&#20986;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;UFO&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users' daily usage. The results, derived from both quantitative metrics and real-case studies, underscore t
&lt;/p&gt;</description></item><item><title>&#20026;&#35299;&#20915;&#32534;&#31243;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;Python&#23398;&#20064;&#32773;&#30340;&#20013;&#25991;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25910;&#38598;&#19982;&#20998;&#31867;&#30495;&#23454;&#23398;&#29983;&#38382;&#39064;&#65292;&#25552;&#39640;&#22312;&#32447;&#32534;&#31243;&#25945;&#32946;&#30340;&#25928;&#26524;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.07913</link><description>&lt;p&gt;
&#20026;&#24110;&#21161;&#20013;&#22269;Python&#32534;&#31243;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#19968;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
QACP: An Annotated Question Answering Dataset for Assisting Chinese Python Programming Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07913
&lt;/p&gt;
&lt;p&gt;
&#20026;&#35299;&#20915;&#32534;&#31243;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;Python&#23398;&#20064;&#32773;&#30340;&#20013;&#25991;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25910;&#38598;&#19982;&#20998;&#31867;&#30495;&#23454;&#23398;&#29983;&#38382;&#39064;&#65292;&#25552;&#39640;&#22312;&#32447;&#32534;&#31243;&#25945;&#32946;&#30340;&#25928;&#26524;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#23398;&#20064;&#24179;&#21488;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#24555;&#36895;&#22686;&#38271;&#30340;&#35745;&#31639;&#26426;&#32534;&#31243;&#35838;&#31243;&#20013;&#65292;&#35299;&#31572;&#25104;&#21315;&#19978;&#19975;&#23398;&#29983;&#30340;&#23398;&#20064;&#38382;&#39064;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#20154;&#21147;&#25104;&#26412;&#12290;&#20026;&#32534;&#31243;&#25945;&#32946;&#23450;&#21046;&#26234;&#33021;&#21161;&#25163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21019;&#24314;&#38656;&#35201;&#29420;&#29305;&#30340;&#25968;&#25454;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#29992;&#20110;&#35757;&#32451;&#27492;&#31867;LLMs&#30340;&#25968;&#25454;&#36164;&#28304;&#30456;&#23545;&#31232;&#32570;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#35299;&#20915;&#32534;&#31243;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;Python&#23398;&#20064;&#32773;&#30340;&#20013;&#25991;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#20026;&#30830;&#20445;&#38382;&#39064;&#30340;&#26469;&#28304;&#30340;&#30495;&#23454;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#23454;&#38469;&#23398;&#29983;&#25552;&#20986;&#30340;&#38382;&#39064;&#65292;&#24182;&#26681;&#25454;&#38382;&#39064;&#30340;&#31867;&#22411;&#21644;&#23398;&#20064;&#32773;&#30340;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#36825;&#31181;&#27880;&#37322;&#21407;&#21017;&#26088;&#22312;&#25552;&#39640;&#22312;&#32447;&#32534;&#31243;&#25945;&#32946;&#30340;&#25928;&#26524;&#21644;&#36136;&#37327;&#65292;&#20026;&#24320;&#21457;&#36825;&#26041;&#38754;&#30340;&#24037;&#20316;&#25552;&#20379;&#22362;&#23454;&#30340;&#25968;&#25454;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In online learning platforms, particularly in rapidly growing computer programming courses, addressing the thousands of students' learning queries requires considerable human cost. The creation of intelligent assistant large language models (LLMs) tailored for programming education necessitates distinct data support. However, in real application scenarios, the data resources for training such LLMs are relatively scarce. Therefore, to address the data scarcity in intelligent educational systems for programming, this paper proposes a new Chinese question-and-answer dataset for Python learners. To ensure the authenticity and reliability of the sources of the questions, we collected questions from actual student questions and categorized them according to various dimensions such as the type of questions and the type of learners. This annotation principle is designed to enhance the effectiveness and quality of online programming education, providing a solid data foundation for developing th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38271;&#25991;&#29983;&#25104;&#20013;&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#27573;&#33853;&#26102;&#21487;&#33021;&#20250;&#30001;&#20110;&#23454;&#20307;&#27169;&#31946;&#32780;&#23558;&#21487;&#39564;&#35777;&#30340;&#20107;&#23454;&#32452;&#21512;&#25104;&#38750;&#20107;&#23454;&#30340;&#27573;&#33853;&#12290;&#29616;&#26377;&#30340;&#20107;&#23454;&#20934;&#30830;&#24230;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#27491;&#30830;&#35780;&#20272;&#36825;&#20123;&#38750;&#20107;&#23454;&#27573;&#33853;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#24230;&#37327;&#25351;&#26631;&#26469;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05629</link><description>&lt;p&gt;
&#21512;&#24182;&#20107;&#23454;&#65292;&#22609;&#36896;&#35884;&#35823;&#65306;&#35780;&#20272;&#38271;&#25991;&#29983;&#25104;&#20013;&#32858;&#21512;&#20107;&#23454;&#24615;&#20027;&#24352;&#30340;&#30683;&#30462;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05629
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38271;&#25991;&#29983;&#25104;&#20013;&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#27573;&#33853;&#26102;&#21487;&#33021;&#20250;&#30001;&#20110;&#23454;&#20307;&#27169;&#31946;&#32780;&#23558;&#21487;&#39564;&#35777;&#30340;&#20107;&#23454;&#32452;&#21512;&#25104;&#38750;&#20107;&#23454;&#30340;&#27573;&#33853;&#12290;&#29616;&#26377;&#30340;&#20107;&#23454;&#20934;&#30830;&#24230;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#27491;&#30830;&#35780;&#20272;&#36825;&#20123;&#38750;&#20107;&#23454;&#27573;&#33853;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#24230;&#37327;&#25351;&#26631;&#26469;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#30340;&#38271;&#25991;&#29983;&#25104;&#29289;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#20107;&#23454;&#21644;&#38750;&#20107;&#23454;&#30340;&#20027;&#24352;&#65292;&#36825;&#20351;&#24471;&#35780;&#20272;&#20107;&#23454;&#24615;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#20197;&#26356;&#31934;&#32454;&#30340;&#26041;&#24335;&#35780;&#20272;&#38271;&#25991;&#29983;&#25104;&#29289;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#23558;&#38271;&#25991;&#29983;&#25104;&#29289;&#20998;&#35299;&#20026;&#22810;&#20010;&#21487;&#39564;&#35777;&#30340;&#20107;&#23454;&#24182;&#29420;&#31435;&#39564;&#35777;&#36825;&#20123;&#20107;&#23454;&#12290;&#29983;&#25104;&#29289;&#30340;&#20107;&#23454;&#24615;&#26159;&#25152;&#26377;&#20107;&#23454;&#20013;&#21487;&#39564;&#35777;&#20107;&#23454;&#30340;&#27604;&#20363;&#12290;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#32467;&#21512;&#20102;&#20107;&#23454;&#20027;&#24352;&#24418;&#25104;&#20102;&#19968;&#20010;&#20107;&#23454;&#24615;&#27573;&#33853;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#19968;&#20551;&#35774;&#21487;&#33021;&#22240;&#20026;&#23454;&#20307;&#27169;&#31946;&#32780;&#34987;&#36829;&#21453;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#21487;&#20197;&#29983;&#25104;&#21253;&#21547;&#21487;&#39564;&#35777;&#20107;&#23454;&#30340;&#27573;&#33853;&#65292;&#20294;&#30001;&#20110;&#23454;&#20307;&#27169;&#31946;&#65292;&#36825;&#20123;&#20107;&#23454;&#34987;&#32467;&#21512;&#24418;&#25104;&#20102;&#19968;&#20010;&#38750;&#20107;&#23454;&#30340;&#27573;&#33853;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#29616;&#26377;&#30340;&#20107;&#23454;&#20934;&#30830;&#24230;&#24230;&#37327;&#25351;&#26631;&#65292;&#21253;&#25324;FActScore&#21644;&#24341;&#29992;&#22238;&#39038;&#65292;&#26080;&#27861;&#27491;&#30830;&#35780;&#20272;&#36825;&#20123;&#38750;&#20107;&#23454;&#27573;&#33853;&#30340;&#20107;&#23454;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22686;&#24378;&#24230;&#37327;&#25351;&#26631;&#65292;D-FActScore&#65292;&#20316;&#20026;&#19968;&#20010;&#20855;&#20307;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-form generations from large language models (LLMs) contains a mix of factual and non-factual claims, making evaluating factuality difficult. To evaluate factual precision of long-form generations in a more fine-grained way, prior works propose to decompose long-form generations into multiple verifiable facts and verify those facts independently. The factuality of the generation is the proportion of verifiable facts among all the facts. Such methods assume that combining factual claims forms a factual paragraph. This paper shows that the assumption can be violated due to entity ambiguity. We show that LLMs can generate paragraphs that contain verifiable facts, but the facts are combined to form a non-factual paragraph due to entity ambiguity. We further reveal that existing factual precision metrics, including FActScore and citation recall, cannot properly evaluate the factuality of these non-factual paragraphs. To address this, we introduce an enhanced metric, D-FActScore, specifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;KICGPT&#65292;&#23427;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#19977;&#20803;&#32452;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26816;&#32034;&#22120;&#30340;&#26694;&#26550;&#12290;&#23427;&#36890;&#36807;&#30693;&#35782;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#65292;&#32531;&#35299;&#20102;&#38271;&#23614;&#38382;&#39064;&#65292;&#24182;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#24320;&#38144;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02389</link><description>&lt;p&gt;
KICGPT: &#20855;&#22791;&#19978;&#19979;&#25991;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;KICGPT&#65292;&#23427;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#19977;&#20803;&#32452;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26816;&#32034;&#22120;&#30340;&#26694;&#26550;&#12290;&#23427;&#36890;&#36807;&#30693;&#35782;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#65292;&#32531;&#35299;&#20102;&#38271;&#23614;&#38382;&#39064;&#65292;&#24182;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#24320;&#38144;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#23545;&#20110;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#19981;&#23436;&#25972;&#24615;&#21644;&#25903;&#25345;&#19979;&#28216;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#27169;&#22411;&#65292;&#23427;&#20204;&#21487;&#20197;&#20998;&#20026;&#22522;&#20110;&#19977;&#20803;&#32452;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#20004;&#31867;&#12290;&#22522;&#20110;&#19977;&#20803;&#32452;&#30340;&#26041;&#27861;&#30001;&#20110;&#32467;&#26500;&#20449;&#24687;&#26377;&#38480;&#21644;&#23454;&#20307;&#20998;&#24067;&#19981;&#22343;&#34913;&#32780;&#22256;&#38590;&#37325;&#37325;&#12290;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#38656;&#35201;&#26114;&#36149;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21644;&#29305;&#23450;&#30340;&#30693;&#35782;&#22270;&#35889;&#24494;&#35843;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;KICGPT&#65292;&#19968;&#31181;&#38598;&#25104;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21644;&#22522;&#20110;&#19977;&#20803;&#32452;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26816;&#32034;&#22120;&#30340;&#26694;&#26550;&#12290;&#23427;&#21487;&#20197;&#32531;&#35299;&#38271;&#23614;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#35757;&#32451;&#24320;&#38144;&#12290;KICGPT&#20351;&#29992;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#65292;&#31216;&#20026;&#30693;&#35782;&#25552;&#31034;&#65292;&#23427;&#23558;&#32467;&#26500;&#30693;&#35782;&#32534;&#30721;&#20026;&#28436;&#31034;&#65292;&#20197;&#24341;&#23548;LLM&#30340;&#23398;&#20064;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph incompleteness and supporting downstream applications. Many models have been proposed for KGC. They can be categorized into two main classes: triple-based and text-based approaches. Triple-based methods struggle with long-tail entities due to limited structural information and imbalanced entity distributions. Text-based methods alleviate this issue but require costly training for language models and specific finetuning for knowledge graphs, which limits their efficiency. To alleviate these limitations, in this paper, we propose KICGPT, a framework that integrates a large language model (LLM) and a triple-based KGC retriever. It alleviates the long-tail problem without incurring additional training overhead. KICGPT uses an in-context learning strategy called Knowledge Prompt, which encodes structural knowledge into demonstrations to guide the LLM. Empirical results on benchmark datasets demonstrate the effectiven
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#28857;&#26159;GPT-4&#65292;&#23545;&#35838;&#22530;&#23545;&#35805;&#36827;&#34892;&#20998;&#26512;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#33021;&#22815;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#19988;&#22312;&#32534;&#30721;&#19968;&#33268;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02380</link><description>&lt;p&gt;
&#22312;&#20998;&#26512;&#35838;&#22530;&#23545;&#35805;&#26102;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models in Analysing Classroom Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#28857;&#26159;GPT-4&#65292;&#23545;&#35838;&#22530;&#23545;&#35805;&#36827;&#34892;&#20998;&#26512;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#33021;&#22815;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#19988;&#22312;&#32534;&#30721;&#19968;&#33268;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#22312;&#20998;&#26512;&#35838;&#22530;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;&#25945;&#23398;&#35786;&#26029;&#21644;&#36136;&#37327;&#25913;&#36827;&#30340;&#37325;&#35201;&#30740;&#31350;&#20219;&#21153;&#12290;&#37492;&#20110;&#20256;&#32479;&#25945;&#32946;&#30740;&#31350;&#20013;&#30693;&#35782;&#23494;&#38598;&#21644;&#21171;&#21160;&#23494;&#38598;&#30340;&#23450;&#24615;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;LLM&#22312;&#20248;&#21270;&#21644;&#22686;&#24378;&#20998;&#26512;&#36807;&#31243;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#35813;&#30740;&#31350;&#28041;&#21450;&#20013;&#23398;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25968;&#23398;&#21644;&#35821;&#25991;&#35838;&#22530;&#19978;&#30340;&#23545;&#35805;&#12290;&#36825;&#20123;&#23545;&#35805;&#30001;&#25945;&#32946;&#19987;&#23478;&#25163;&#21160;&#32534;&#30721;&#65292;&#28982;&#21518;&#20351;&#29992;&#23450;&#21046;&#30340;GPT-4&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#27604;&#36739;&#25163;&#21160;&#27880;&#37322;&#19982;GPT-4&#30340;&#36755;&#20986;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#20998;&#26512;&#25945;&#32946;&#23545;&#35805;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#35780;&#20272;&#26102;&#38388;&#25928;&#29575;&#12289;&#32534;&#30721;&#32773;&#38388;&#19968;&#33268;&#24615;&#21644;&#32534;&#30721;&#32773;&#38388;&#21487;&#38752;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;GPT-4&#21487;&#20197;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#24182;&#22312;&#32534;&#30721;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the application of Large Language Models (LLMs), specifically GPT-4, in the analysis of classroom dialogue, a crucial research task for both teaching diagnosis and quality improvement. Recognizing the knowledge-intensive and labor-intensive nature of traditional qualitative methods in educational research, this study investigates the potential of LLM to streamline and enhance the analysis process. The study involves datasets from a middle school, encompassing classroom dialogues across mathematics and Chinese classes. These dialogues were manually coded by educational experts and then analyzed using a customised GPT-4 model. This study focuses on comparing manual annotations with the outputs of GPT-4 to evaluate its efficacy in analyzing educational dialogues. Time efficiency, inter-coder agreement, and inter-coder reliability between human coders and GPT-4 are evaluated. Results indicate substantial time savings with GPT-4, and a high degree of consistency in codin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#39640;&#25928;&#31934;&#30830;&#20248;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#31574;&#30053;&#21442;&#25968;&#21270;&#20219;&#24847;&#30340;&#24773;&#20917;&#19979;&#65292;&#28176;&#36817;&#22320;&#19982;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#19968;&#33268;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.00856</link><description>&lt;p&gt;
&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#39640;&#25928;&#31934;&#30830;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient and Exact Optimization of Language Model Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#39640;&#25928;&#31934;&#30830;&#20248;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#31574;&#30053;&#21442;&#25968;&#21270;&#20219;&#24847;&#30340;&#24773;&#20917;&#19979;&#65292;&#28176;&#36817;&#22320;&#19982;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#19968;&#33268;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#23545;&#20110;&#20854;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#35813;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#21270;&#21453;&#26144;&#20154;&#31867;&#20559;&#22909;&#30340;&#39044;&#26399;&#22870;&#21169;&#65292;&#24182;&#23613;&#37327;&#20943;&#23567;&#19982;&#21021;&#22987;&#31574;&#30053;&#30340;&#20559;&#24046;&#12290;&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#30452;&#25509;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20854;&#31574;&#30053;&#26356;&#26032;&#30340;&#26041;&#24046;&#24456;&#39640;&#65292;&#38459;&#30861;&#20102;&#39640;&#25928;&#30340;&#31574;&#30053;&#25913;&#36827;&#12290;&#26368;&#36817;&#65292;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#34987;&#25552;&#20986;&#20197;&#30452;&#25509;&#20174;&#20559;&#22909;&#25968;&#25454;&#20013;&#20248;&#21270;&#31574;&#30053;&#12290;&#23613;&#31649;&#23454;&#29616;&#31616;&#21333;&#65292;DPO&#26159;&#22522;&#20110;&#19981;&#19968;&#23450;&#33021;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#30340;&#26368;&#20248;&#31574;&#30053;&#23548;&#20986;&#30340;&#65292;&#36825;&#21066;&#24369;&#20102;&#20854;&#25910;&#25947;&#21040;&#39044;&#26399;&#35299;&#20915;&#26041;&#26696;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31934;&#30830;&#20248;&#21270;&#65288;EXO&#65289;&#30340;&#23545;&#40784;&#30446;&#26631;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#31574;&#30053;&#30340;&#20219;&#24847;&#21442;&#25968;&#21270;&#65292;EXO&#20445;&#35777;&#28176;&#36817;&#22320;&#19982;RL&#31639;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#19968;&#33268;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The alignment of language models with human preferences is vital for their application in real-world tasks. The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy. While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement. Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data. Though simple to implement, DPO is derived based on the optimal policy that is not assured to be achieved in practice, which undermines its convergence to the intended solution.   In this paper, we propose efficient exact optimization (EXO) of the alignment objective. We prove that EXO is guaranteed to optimize in the same direction as the RL algorithms asymptotically for arbitary parametrization of the policy, while enables efficient op
&lt;/p&gt;</description></item><item><title>PythonSaga&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#65292;&#38024;&#23545;Python&#20195;&#30721;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;,&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#23384;&#22312;&#30340;&#32534;&#31243;&#27010;&#24565;&#20559;&#35265;&#21644;&#31616;&#21333;&#20219;&#21153;&#26222;&#36941;&#24615;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2401.03855</link><description>&lt;p&gt;
PythonSaga&#65306;&#37325;&#26032;&#23450;&#20041;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;LLM&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03855
&lt;/p&gt;
&lt;p&gt;
PythonSaga&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#65292;&#38024;&#23545;Python&#20195;&#30721;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;,&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#23384;&#22312;&#30340;&#32534;&#31243;&#27010;&#24565;&#20559;&#35265;&#21644;&#31616;&#21333;&#20219;&#21153;&#26222;&#36941;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#20195;&#30721;&#28608;&#22686;&#30340;&#25512;&#21160;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;LLMs&#30340;&#21151;&#33021;&#12290;&#25105;&#20204;&#23545;HumanEval&#21644;MBPP&#20004;&#20010;&#27969;&#34892;&#30340;Python&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20154;&#24037;&#35780;&#20272;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#22810;&#26679;&#24615;&#21644;&#38590;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#23545;&#19968;&#32452;&#26377;&#38480;&#30340;&#32534;&#31243;&#27010;&#24565;&#23384;&#22312;&#20005;&#37325;&#20559;&#35265;&#65292;&#23436;&#20840;&#24573;&#35270;&#20102;&#22823;&#22810;&#25968;&#20854;&#20182;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22823;&#37327;&#31616;&#21333;&#20219;&#21153;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#21487;&#33021;&#22840;&#22823;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#20272;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;PythonSaga&#65292;&#21253;&#21547;&#20102;185&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#25552;&#31034;&#65292;&#28085;&#30422;&#20102;38&#20010;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#32534;&#31243;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03855v2 Announce Type: replace-cross  Abstract: Driven by the surge in code generation using large language models (LLMs), numerous benchmarks have emerged to evaluate these LLMs capabilities. We conducted a large-scale human evaluation of HumanEval and MBPP, two popular benchmarks for Python code generation, analyzing their diversity and difficulty. Our findings unveil a critical bias towards a limited set of programming concepts, neglecting most of the other concepts entirely. Furthermore, we uncover a worrying prevalence of easy tasks, potentially inflating model performance estimations. To address these limitations, we propose a novel benchmark, PythonSaga, featuring 185 hand-crafted prompts on a balanced representation of 38 programming concepts across diverse difficulty levels.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;ChatGPT&#22312;&#21476;&#20195;&#27721;&#35821;&#32763;&#35793;&#21644;&#20154;&#21517;&#35782;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#32763;&#35793;&#26041;&#38754;&#30340;&#34920;&#29616;&#20173;&#26377;&#24453;&#25552;&#39640;&#65292;&#26368;&#20339;&#34920;&#29616;&#26159;&#22312;&#36755;&#20837;&#19977;&#20010;&#19978;&#19979;&#25991;&#21477;&#23376;&#26102;&#23454;&#29616;&#30340;&#12290;</title><link>https://arxiv.org/abs/2312.15304</link><description>&lt;p&gt;
&#25506;&#32034;ChatGPT&#22312;&#21476;&#20195;&#27721;&#35821;&#32763;&#35793;&#21644;&#20154;&#21517;&#35782;&#21035;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Capabilities of ChatGPT in Ancient Chinese Translation and Person Name Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;ChatGPT&#22312;&#21476;&#20195;&#27721;&#35821;&#32763;&#35793;&#21644;&#20154;&#21517;&#35782;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#32763;&#35793;&#26041;&#38754;&#30340;&#34920;&#29616;&#20173;&#26377;&#24453;&#25552;&#39640;&#65292;&#26368;&#20339;&#34920;&#29616;&#26159;&#22312;&#36755;&#20837;&#19977;&#20010;&#19978;&#19979;&#25991;&#21477;&#23376;&#26102;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#22788;&#29702;&#29616;&#20195;&#26631;&#20934;&#35821;&#35328;&#26041;&#38754;&#30340;&#29087;&#32451;&#34920;&#26126;&#20854;&#20855;&#26377;&#28508;&#21147;&#29992;&#20110;&#29702;&#35299;&#21476;&#20195;&#27721;&#35821;&#12290;&#26412;&#25991;&#36890;&#36807;&#20004;&#39033;&#20219;&#21153;&#25506;&#35752;&#20102;ChatGPT&#22312;&#21476;&#20195;&#27721;&#35821;&#26041;&#38754;&#30340;&#33021;&#21147;&#65306;&#23558;&#21476;&#20195;&#27721;&#35821;&#32763;&#35793;&#20026;&#29616;&#20195;&#27721;&#35821;&#21644;&#35782;&#21035;&#21476;&#20195;&#27721;&#35821;&#20154;&#21517;&#12290;&#36890;&#36807;&#23558;ChatGPT&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#32763;&#35793;&#36827;&#34892;&#27604;&#36739;&#65292;&#35780;&#20272;&#20854;&#23545;&#21476;&#20195;&#27721;&#35821;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#21457;&#29616;&#65306;&#65288;1.&#65289;ChatGPT&#23545;&#21476;&#20195;&#27721;&#35821;&#30340;&#29087;&#32451;&#31243;&#24230;&#23578;&#26410;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#27700;&#24179;&#65307;&#65288;2.&#65289;&#22312;&#36755;&#20837;&#19977;&#20010;&#19978;&#19979;&#25991;&#21477;&#23376;&#26102;&#65292;ChatGPT&#22312;&#21476;&#20195;&#27721;&#35821;&#21040;&#29616;&#20195;&#27721;&#35821;&#30340;&#32763;&#35793;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#20026;&#20102;&#24110;&#21161;&#37325;&#29616;&#25105;&#20204;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26412;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;Python&#20195;&#30721;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15304v2 Announce Type: replace-cross  Abstract: ChatGPT's proficiency in handling modern standard languages suggests potential for its use in understanding ancient Chinese. This paper explores ChatGPT's capabilities on ancient Chinese via two tasks: translating ancient Chinese to modern Chinese and recognizing ancient Chinese names. A comparison of ChatGPT's output with human translations serves to evaluate its comprehension of ancient Chinese. The findings indicate that: (1.)the proficiency of ancient Chinese by ChatGPT is yet to reach a satisfactory level; (2.) ChatGPT performs the best on ancient-to-modern translation when feeding with three context sentences. To help reproduce our work, we display the python code snippets used in this study.
&lt;/p&gt;</description></item><item><title>&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;&#65288;SPC&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#21644;&#39044;&#27979;&#20219;&#21153;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#32039;&#20945;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#23454;&#29616;&#26356;&#22909;&#30340;&#35206;&#30422;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.13933</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Structured Probabilistic Coding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13933
&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;&#65288;SPC&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#21644;&#39044;&#27979;&#20219;&#21153;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#32039;&#20945;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#23454;&#29616;&#26356;&#22909;&#30340;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#32467;&#26500;&#21270;&#27010;&#29575;&#32534;&#30721;&#65288;SPC&#65289;&#65292;&#29992;&#20110;&#20174;&#19982;&#30446;&#26631;&#20219;&#21153;&#30456;&#20851;&#30340;&#36755;&#20837;&#20013;&#23398;&#20064;&#32039;&#20945;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;SPC&#26159;&#19968;&#31181;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#27010;&#29575;&#32534;&#30721;&#25216;&#26415;&#65292;&#20855;&#26377;&#26469;&#33258;&#30446;&#26631;&#31354;&#38388;&#30340;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#12290;&#23427;&#21487;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27010;&#29575;&#32534;&#30721;&#22312;&#19968;&#20010;&#27169;&#22359;&#20013;&#21516;&#26102;&#36827;&#34892;&#20449;&#24687;&#32534;&#30721;&#21644;&#20219;&#21153;&#39044;&#27979;&#65292;&#20197;&#26356;&#20805;&#20998;&#22320;&#21033;&#29992;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#26377;&#25928;&#20449;&#24687;&#12290;&#23427;&#20351;&#29992;&#36755;&#20986;&#31354;&#38388;&#30340;&#21464;&#20998;&#25512;&#26029;&#26469;&#20943;&#23569;&#38543;&#26426;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#25511;&#21046;&#27010;&#29575;&#34920;&#31034;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#65292;&#20197;&#20419;&#36827;&#31867;&#21035;&#20043;&#38388;&#30340;&#22343;&#21248;&#24615;&#12290;&#36890;&#36807;&#27491;&#21017;&#21270;&#39033;&#65292;SPC&#21487;&#20197;&#20445;&#25345;&#28508;&#22312;&#32534;&#30721;&#30340;&#39640;&#26031;&#32467;&#26500;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new supervised representation learning framework, namely structured probabilistic coding (SPC), to learn compact and informative representations from input related to the target task. SPC is an encoder-only probabilistic coding technology with a structured regularization from the target space. It can enhance the generalization ability of pre-trained language models for better language understanding. Specifically, our probabilistic coding simultaneously performs information encoding and task prediction in one module to more fully utilize the effective information from input data. It uses variational inference in the output space to reduce randomness and uncertainty. Besides, to better control the learning process of probabilistic representations, a structured regularization is proposed to promote uniformity across classes in the latent space. With the regularization term, SPC can preserve the Gaussian structure of the latent code and achieve better coverage of the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#22320;&#27010;&#36848;&#20102;&#36817;&#24180;&#26469;&#22312;&#25968;&#23398;&#39046;&#22495;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#25968;&#23398;LLMs&#30340;&#20998;&#31867;&#21644;&#23545;&#36229;&#36807;60&#20010;&#25968;&#23398;&#25968;&#25454;&#38598;&#30340;&#32534;&#21046;&#65292;&#20026;&#25968;&#23398;LM&#39046;&#22495;&#26410;&#26469;&#30340;&#21457;&#23637;&#25351;&#26126;&#20102;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2312.07622</link><description>&lt;p&gt;
&#25968;&#23398;&#35821;&#35328;&#27169;&#22411;: &#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Mathematical Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#22320;&#27010;&#36848;&#20102;&#36817;&#24180;&#26469;&#22312;&#25968;&#23398;&#39046;&#22495;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#25968;&#23398;LLMs&#30340;&#20998;&#31867;&#21644;&#23545;&#36229;&#36807;60&#20010;&#25968;&#23398;&#25968;&#25454;&#38598;&#30340;&#32534;&#21046;&#65292;&#20026;&#25968;&#23398;LM&#39046;&#22495;&#26410;&#26469;&#30340;&#21457;&#23637;&#25351;&#26126;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#25968;&#23398;&#39046;&#22495;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#26412;&#25991;&#23545;&#25968;&#23398;LMs&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#31995;&#32479;&#22320;&#20174;&#20004;&#20010;&#19981;&#21516;&#30340;&#35270;&#35282;&#23545;&#37325;&#35201;&#30340;&#30740;&#31350;&#21162;&#21147;&#36827;&#34892;&#20102;&#20998;&#31867;&#65306;&#20219;&#21153;&#21644;&#26041;&#27861;&#35770;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;&#20986;&#22823;&#37327;&#25552;&#20986;&#30340;&#25968;&#23398;LLMs&#65292;&#36827;&#19968;&#27493;&#21010;&#20998;&#20026;&#25351;&#20196;&#23398;&#20064;&#12289;&#22522;&#20110;&#24037;&#20855;&#30340;&#26041;&#27861;&#12289;&#22522;&#30784;CoT&#25216;&#26415;&#21644;&#39640;&#32423;CoT&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#21253;&#25324;&#32534;&#21046;&#20102;60&#22810;&#20010;&#25968;&#23398;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#35757;&#32451;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#22686;&#24378;&#25968;&#25454;&#38598;&#12290;&#35299;&#20915;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#21246;&#21202;&#25968;&#23398;LM&#39046;&#22495;&#26410;&#26469;&#30340;&#21457;&#23637;&#36712;&#36857;&#65292;&#26412;&#35843;&#26597;&#34987;&#23450;&#20301;&#20026;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#26088;&#22312;&#20419;&#36827;&#24182;&#28608;&#21169;&#26410;&#26469;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07622v3 Announce Type: replace  Abstract: In recent years, there has been remarkable progress in leveraging Language Models (LMs), encompassing Pre-trained Language Models (PLMs) and Large-scale Language Models (LLMs), within the domain of mathematics. This paper conducts a comprehensive survey of mathematical LMs, systematically categorizing pivotal research endeavors from two distinct perspectives: tasks and methodologies. The landscape reveals a large number of proposed mathematical LLMs, which are further delineated into instruction learning, tool-based methods, fundamental CoT techniques, and advanced CoT methodologies. In addition, our survey entails the compilation of over 60 mathematical datasets, including training datasets, benchmark datasets, and augmented datasets. Addressing the primary challenges and delineating future trajectories within the field of mathematical LMs, this survey is positioned as a valuable resource, poised to facilitate and inspire future inn
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#20013;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#21457;&#29616;&#26631;&#31614;&#24179;&#28369;&#21487;&#20197;&#21152;&#36895;&#28145;&#24230;&#27169;&#22411;&#30340;&#25910;&#25947;&#65292;&#24182;&#20351;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#26356;&#23481;&#26131;&#21306;&#20998;</title><link>https://arxiv.org/abs/2312.06522</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#26631;&#31614;&#24179;&#28369;&#22312;&#22686;&#24378;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Role of Label Smoothing in Enhanced Text Sentiment Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06522
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#20013;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#21457;&#29616;&#26631;&#31614;&#24179;&#28369;&#21487;&#20197;&#21152;&#36895;&#28145;&#24230;&#27169;&#22411;&#30340;&#25910;&#25947;&#65292;&#24182;&#20351;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#26356;&#23481;&#26131;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#30340;&#25216;&#26415;&#65292;&#22914;&#25991;&#26412;&#20998;&#31867;&#12289;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#65292;&#20197;&#26377;&#25928;&#23545;&#25239;&#27169;&#22411;&#36807;&#25311;&#21512;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26631;&#31614;&#24179;&#28369;&#22914;&#20309;&#22686;&#24378;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#30340;&#32454;&#33268;&#20998;&#26512;&#21364;&#24456;&#23569;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#22312;&#20843;&#20010;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65288;TextCNN&#12289;BERT&#21644;RoBERTa&#65289;&#20197;&#21450;&#20004;&#31181;&#23398;&#20064;&#26041;&#26696;&#19979;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#28145;&#20837;&#20998;&#26512;&#12290;&#36890;&#36807;&#35843;&#25972;&#24179;&#28369;&#21442;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#27599;&#20010;&#27169;&#22411;&#26550;&#26500;&#30340;&#20960;&#20046;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#26631;&#31614;&#24179;&#28369;&#30340;&#22909;&#22788;&#65292;&#21457;&#29616;&#26631;&#31614;&#24179;&#28369;&#21487;&#20197;&#21152;&#36895;&#28145;&#24230;&#27169;&#22411;&#30340;&#25910;&#25947;&#65292;&#24182;&#20351;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#26356;&#23481;&#26131;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06522v2 Announce Type: replace-cross  Abstract: Label smoothing is a widely used technique in various domains, such as text classification, image classification and speech recognition, known for effectively combating model overfitting. However, there is little fine-grained analysis on how label smoothing enhances text sentiment classification. To fill in the gap, this article performs a set of in-depth analyses on eight datasets for text sentiment classification and three deep learning architectures: TextCNN, BERT, and RoBERTa, under two learning schemes: training from scratch and fine-tuning. By tuning the smoothing parameters, we can achieve improved performance on almost all datasets for each model architecture. We further investigate the benefits of label smoothing, finding that label smoothing can accelerate the convergence of deep models and make samples of different labels easily distinguishable.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;RADIAL&#65292;&#36890;&#36807;&#25918;&#22823;LLMs&#29983;&#25104;&#32943;&#23450;&#21709;&#24212;&#30340;&#28508;&#21147;&#26469;&#32469;&#36807;&#23433;&#20840;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#33521;&#35821;&#24694;&#24847;&#25351;&#20196;&#30340;&#20248;&#31168;&#25915;&#20987;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.04127</link><description>&lt;p&gt;
&#20998;&#26512;LLMs&#30340;&#22266;&#26377;&#21709;&#24212;&#20542;&#21521;&#65306;&#30495;&#23454;&#19990;&#30028;&#25351;&#20196;&#39537;&#21160;&#30340;&#36234;&#29425;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04127
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;RADIAL&#65292;&#36890;&#36807;&#25918;&#22823;LLMs&#29983;&#25104;&#32943;&#23450;&#21709;&#24212;&#30340;&#28508;&#21147;&#26469;&#32469;&#36807;&#23433;&#20840;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#33521;&#35821;&#24694;&#24847;&#25351;&#20196;&#30340;&#20248;&#31168;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#24037;&#20316;&#33268;&#21147;&#20110;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#24694;&#24847;&#25351;&#20196;&#26102;&#65292;LLMs&#20173;&#28982;&#20542;&#21521;&#20110;&#29983;&#25104;&#26377;&#23475;&#21709;&#24212;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#36234;&#29425;&#25915;&#20987;&#8221;&#12290; &#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;RADIAL&#65292;&#36890;&#36807;&#25918;&#22823;LLMs&#29983;&#25104;&#32943;&#23450;&#21709;&#24212;&#30340;&#28508;&#21147;&#26469;&#32469;&#36807;&#23433;&#20840;&#26426;&#21046;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#36234;&#29425;&#24605;&#24819;&#26159;&#8220;&#22266;&#26377;&#21709;&#24212;&#20542;&#21521;&#20998;&#26512;&#8221;&#65292;&#23427;&#35782;&#21035;&#20986;&#37027;&#20123;&#22312;&#26412;&#36136;&#19978;&#21487;&#20197;&#23548;&#33268;LLMs&#29983;&#25104;&#32943;&#23450;&#21709;&#24212;&#30340;&#30495;&#23454;&#19990;&#30028;&#25351;&#20196;&#65292;&#30456;&#24212;&#30340;&#36234;&#29425;&#31574;&#30053;&#26159;&#8220;&#30495;&#23454;&#19990;&#30028;&#25351;&#20196;&#39537;&#21160;&#30340;&#36234;&#29425;&#8221;&#65292;&#23427;&#28041;&#21450;&#36890;&#36807;&#22312;&#24694;&#24847;&#25351;&#20196;&#21608;&#22260;&#31574;&#30053;&#24615;&#22320;&#25340;&#25509;&#36890;&#36807;&#19978;&#36848;&#20998;&#26512;&#35782;&#21035;&#20986;&#30340;&#30495;&#23454;&#19990;&#30028;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20116;&#20010;&#24320;&#28304;&#20808;&#36827;&#30340;LLMs&#19978;&#23545;&#33521;&#35821;&#24694;&#24847;&#25351;&#20196;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04127v2 Announce Type: replace  Abstract: Extensive work has been devoted to improving the safety mechanism of Large Language Models (LLMs). However, LLMs still tend to generate harmful responses when faced with malicious instructions, a phenomenon referred to as "Jailbreak Attack". In our research, we introduce a novel automatic jailbreak method RADIAL, which bypasses the security mechanism by amplifying the potential of LLMs to generate affirmation responses. The jailbreak idea of our method is "Inherent Response Tendency Analysis" which identifies real-world instructions that can inherently induce LLMs to generate affirmation responses and the corresponding jailbreak strategy is "Real-World Instructions-Driven Jailbreak" which involves strategically splicing real-world instructions identified through the above analysis around the malicious instruction. Our method achieves excellent attack performance on English malicious instructions with five open-source advanced LLMs wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;RLAIF&#35299;&#37322;&#20026;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32463;&#36807;&#31934;&#28860;&#30340;&#33258;&#25105;&#25209;&#35780;&#23545;LLM&#30340;&#36755;&#20986;&#36827;&#34892;&#31934;&#28860;&#65292;&#20026;&#33719;&#24471;&#24494;&#35843;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#19988;&#24265;&#20215;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2312.01957</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;&#32463;&#36807;&#31934;&#28860;&#30340;LLMs&#33258;&#25105;&#25209;&#35780;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;RLAIF&#35299;&#37322;&#20026;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32463;&#36807;&#31934;&#28860;&#30340;&#33258;&#25105;&#25209;&#35780;&#23545;LLM&#30340;&#36755;&#20986;&#36827;&#34892;&#31934;&#28860;&#65292;&#20026;&#33719;&#24471;&#24494;&#35843;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#19988;&#24265;&#20215;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;RLAIF&#35299;&#37322;&#20026;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#32463;&#36807;&#31934;&#28860;&#30340;&#33258;&#25105;&#25209;&#35780;(dSC)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;Gibbs&#37319;&#26679;&#22120;&#23545;LLM&#30340;&#36755;&#20986;&#36827;&#34892;&#31934;&#28860;&#65292;&#28982;&#21518;&#23558;&#20854;&#33976;&#39311;&#25104;&#19968;&#20010;&#24494;&#35843;&#27169;&#22411;&#12290;&#21482;&#38656;&#35201;&#21512;&#25104;&#25968;&#25454;&#65292;dSC&#22312;&#28041;&#21450;&#23433;&#20840;&#24615;&#12289;&#24773;&#24863;&#21644;&#38544;&#31169;&#25511;&#21046;&#30340;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#34920;&#26126;&#23427;&#21487;&#20197;&#20316;&#20026;&#23545;&#40784;LLMs&#30340;&#19968;&#31181;&#21487;&#34892;&#19988;&#24265;&#20215;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20195;&#30721;&#22312;\url{https://github.com/vicgalle/distilled-self-critique}&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01957v2 Announce Type: replace  Abstract: This paper proposes an interpretation of RLAIF as Bayesian inference by introducing distilled Self-Critique (dSC), which refines the outputs of a LLM through a Gibbs sampler that is later distilled into a fine-tuned model. Only requiring synthetic data, dSC is exercised in experiments regarding safety, sentiment, and privacy control, showing it can be a viable and cheap alternative to align LLMs. Code released at \url{https://github.com/vicgalle/distilled-self-critique}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#21069;&#19968;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#33521;&#35821;&#34394;&#20551;&#26032;&#38395;&#25991;&#31456;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#20196;&#20154;&#20449;&#26381;&#30340;&#25903;&#25345;&#21361;&#38505;&#34394;&#20551;&#20449;&#24687;&#30340;&#26032;&#38395;&#25991;&#31456;&#12290;</title><link>https://arxiv.org/abs/2311.08838</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34394;&#20551;&#20449;&#24687;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Disinformation Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#21069;&#19968;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#33521;&#35821;&#34394;&#20551;&#26032;&#38395;&#25991;&#31456;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#20196;&#20154;&#20449;&#26381;&#30340;&#25903;&#25345;&#21361;&#38505;&#34394;&#20551;&#20449;&#24687;&#30340;&#26032;&#38395;&#25991;&#31456;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#34394;&#20551;&#20449;&#24687;&#29983;&#25104;&#32463;&#24120;&#34987;&#21015;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30456;&#20851;&#30340;&#37325;&#35201;&#39118;&#38505;&#12290;&#22312;&#20449;&#24687;&#31354;&#38388;&#20013;&#20805;&#26021;&#34394;&#20551;&#20449;&#24687;&#20869;&#23481;&#30340;&#29702;&#35770;&#33021;&#21147;&#21487;&#33021;&#23545;&#20840;&#29699;&#31038;&#20250;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;&#24403;&#21069;&#19968;&#20195;LLMs&#30340;&#34394;&#20551;&#20449;&#24687;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#29983;&#25104;&#20102;&#33521;&#35821;&#34394;&#20551;&#26032;&#38395;&#25991;&#31456;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;20&#20010;&#34394;&#20551;&#20449;&#24687;&#21465;&#20107;&#35780;&#20272;&#20102;10&#20010;LLMs&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#30340;&#20960;&#20010;&#26041;&#38754;&#65306;&#23427;&#20204;&#29983;&#25104;&#26032;&#38395;&#25991;&#31456;&#30340;&#25928;&#26524;&#22914;&#20309;&#65292;&#23427;&#20204;&#20542;&#21521;&#20110;&#25903;&#25345;&#25110;&#21453;&#23545;&#34394;&#20551;&#21465;&#20107;&#30340;&#31243;&#24230;&#22914;&#20309;&#65292;&#23427;&#20204;&#29983;&#25104;&#23433;&#20840;&#35686;&#21578;&#30340;&#39057;&#29575;&#31561;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#26816;&#27979;&#27169;&#22411;&#26816;&#27979;&#36825;&#20123;&#25991;&#31456;&#26159;&#21542;&#20026;LLM&#29983;&#25104;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;LLMs&#33021;&#22815;&#29983;&#25104;&#20196;&#20154;&#20449;&#26381;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#25903;&#25345;&#21361;&#38505;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08838v2 Announce Type: replace  Abstract: Automated disinformation generation is often listed as an important risk associated with large language models (LLMs). The theoretical ability to flood the information space with disinformation content might have dramatic consequences for societies around the world. This paper presents a comprehensive study of the disinformation capabilities of the current generation of LLMs to generate false news articles in the English language. In our study, we evaluated the capabilities of 10 LLMs using 20 disinformation narratives. We evaluated several aspects of the LLMs: how good they are at generating news articles, how strongly they tend to agree or disagree with the disinformation narratives, how often they generate safety warnings, etc. We also evaluated the abilities of detection models to detect these articles as LLM-generated. We conclude that LLMs are able to generate convincing news articles that agree with dangerous disinformation na
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2311.08045</link><description>&lt;p&gt;
&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adversarial Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08045
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20559;&#22909;&#35843;&#25972;&#26159;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20132;&#20114;&#36136;&#37327;&#30340;&#20851;&#38190;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25351;&#23548;LLM&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25345;&#32493;&#26356;&#26032;LLMs&#20250;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#19982;&#20154;&#31867;&#39318;&#36873;&#21709;&#24212;&#20043;&#38388;&#23384;&#22312;&#20998;&#24067;&#24046;&#36317;&#65292;&#36825;&#38459;&#30861;&#20102;&#27169;&#22411;&#24494;&#35843;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#22312;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#39069;&#22806;&#36827;&#34892;&#20559;&#22909;&#27880;&#37322;&#65292;&#20197;&#36866;&#24212;&#36716;&#31227;&#20998;&#24067;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#36164;&#28304;&#12290;&#38024;&#23545;&#26356;&#39640;&#25928;&#30340;&#20154;&#31867;&#20559;&#22909;&#20248;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;LLM&#20195;&#29702;&#21644;&#20559;&#22909;&#27169;&#22411;&#36890;&#36807;&#26497;&#23567;-&#26497;&#22823;&#21338;&#24328;&#20132;&#26367;&#26356;&#26032;&#12290;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;APO&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08045v2 Announce Type: replace-cross  Abstract: Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing aligning methods depend on manually annotated preference data to guide the LLM optimization directions. However, in practice, continuously updating LLMs raises a distribution gap between model-generated samples and human-preferred responses, which hinders model fine-tuning efficiency. To mitigate this issue, previous methods require additional preference annotation on generated samples to adapt the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an adversarial preference optimization (APO) framework, where the LLM agent and the preference model update alternatively via a min-max game. Without additional annotation, our APO method can make a self-adaption to the generation distribution gap through the adversarial learni
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;LLM&#30340;&#22810;&#32500;&#22522;&#20934;AMBER&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;MLLM&#30340;&#24187;&#35937;&#65292;&#35774;&#35745;&#20102;&#20302;&#25104;&#26412;&#39640;&#25928;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#24182;&#23545;&#20027;&#27969;MLLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#20998;&#26512;</title><link>https://arxiv.org/abs/2311.07397</link><description>&lt;p&gt;
AMBER&#65306;&#19968;&#31181;&#26080;&#38656;LLM&#30340;&#22810;&#32500;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;MLLM&#30340;&#24187;&#35937;
&lt;/p&gt;
&lt;p&gt;
AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07397
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;LLM&#30340;&#22810;&#32500;&#22522;&#20934;AMBER&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;MLLM&#30340;&#24187;&#35937;&#65292;&#35774;&#35745;&#20102;&#20302;&#25104;&#26412;&#39640;&#25928;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#24182;&#23545;&#20027;&#27969;MLLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#38754;&#20020;&#24187;&#35273;&#30340;&#37325;&#22823;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#26377;&#23475;&#21518;&#26524;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;MLLMs&#30340;&#24187;&#35273;&#23545;&#20110;&#27169;&#22411;&#25913;&#36827;&#21644;&#23454;&#38469;&#24212;&#29992;&#37096;&#32626;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#39640;&#35780;&#20272;&#25104;&#26412;&#65288;&#20363;&#22914;&#20381;&#36182;&#20154;&#31867;&#25110;&#39640;&#32423;LLMs&#65289;&#21644;&#35780;&#20272;&#32500;&#24230;&#19981;&#36275;&#65288;&#20363;&#22914;&#20219;&#21153;&#21644;&#24187;&#35273;&#31867;&#22411;&#65289;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;LLM&#30340;&#22810;&#32500;&#22522;&#20934;AMBER&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#20219;&#21153;&#21644;&#21028;&#21035;&#20219;&#21153;&#65292;&#21253;&#25324;&#23384;&#22312;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#24187;&#35273;&#12290;&#22522;&#20110;AMBER&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20302;&#25104;&#26412;&#39640;&#25928;&#30340;&#35780;&#20272;&#27969;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#20027;&#27969;MLLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#35814;&#32454;&#20998;&#26512;&#65292;&#21253;&#25324;GPT-4V(ision)&#65292;&#24182;&#25552;&#20986;&#20102;&#25351;&#23548;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07397v2 Announce Type: replace  Abstract: Despite making significant progress in multi-modal tasks, current Multi-modal Large Language Models (MLLMs) encounter the significant challenge of hallucinations, which may lead to harmful consequences. Therefore, evaluating MLLMs' hallucinations is becoming increasingly important in model improvement and practical application deployment. Previous works are limited in high evaluation costs (e.g., relying on humans or advanced LLMs) and insufficient evaluation dimensions (e.g., types of tasks and hallucinations). In this paper, we propose an LLM-free multi-dimensional benchmark AMBER, which can be used to evaluate both generative task and discriminative task including existence, attribute and relation hallucination. Based on AMBER, we design a low-cost and efficient evaluation pipeline. Additionally, we conduct a comprehensive evaluation and detailed analysis of mainstream MLLMs including GPT-4V(ision), and also give guideline suggest
&lt;/p&gt;</description></item><item><title>&#24320;&#28304;&#20195;&#30721;LLMs&#38590;&#20197;&#29983;&#25104;&#27491;&#30830;&#25351;&#23548;&#30340;&#21453;&#39304;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Coffee&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;Coffee&#25968;&#25454;&#38598;&#26500;&#24314;CoffeePots&#65292;&#36890;&#36807;&#20248;&#21270;&#35843;&#25972;&#21644;&#36873;&#25321;&#65292;&#23454;&#29616;&#33258;&#21160;&#29983;&#25104;&#24102;&#26377;&#27491;&#30830;&#25351;&#23548;&#30340;&#21453;&#39304;&#20197;&#29992;&#20110;&#20195;&#30721;&#20462;&#22797;&#12290;</title><link>https://arxiv.org/abs/2311.07215</link><description>&lt;p&gt;
&#21654;&#21857;&#65306;&#36890;&#36807;&#21453;&#39304;&#20462;&#22797;&#38169;&#35823;&#26469;&#25552;&#21319;&#20195;&#30721;LLMs
&lt;/p&gt;
&lt;p&gt;
Coffee: Boost Your Code LLMs by Fixing Bugs with Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07215
&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#20195;&#30721;LLMs&#38590;&#20197;&#29983;&#25104;&#27491;&#30830;&#25351;&#23548;&#30340;&#21453;&#39304;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Coffee&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;Coffee&#25968;&#25454;&#38598;&#26500;&#24314;CoffeePots&#65292;&#36890;&#36807;&#20248;&#21270;&#35843;&#25972;&#21644;&#36873;&#25321;&#65292;&#23454;&#29616;&#33258;&#21160;&#29983;&#25104;&#24102;&#26377;&#27491;&#30830;&#25351;&#23548;&#30340;&#21453;&#39304;&#20197;&#29992;&#20110;&#20195;&#30721;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07215v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#20195;&#30721;&#32534;&#36753;&#26159;&#30830;&#20445;&#31243;&#24207;&#32508;&#21512;&#30340;&#19968;&#20010;&#37325;&#35201;&#27493;&#39588;&#65292;&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;&#20195;&#30721;LLMs&#29983;&#25104;&#30340;&#20851;&#38190;&#38169;&#35823;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#38381;&#28304;LLMs&#65288;&#22914;ChatGPT&#21644;GPT-4&#65289;&#33021;&#22815;&#29983;&#25104;&#32416;&#27491;&#24615;&#21453;&#39304;&#65292;&#29992;&#20110;&#32534;&#36753;&#38169;&#35823;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#24320;&#28304;&#20195;&#30721;LLMs&#29983;&#25104;&#29992;&#20110;&#20195;&#30721;&#32534;&#36753;&#30340;&#21453;&#39304;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#20542;&#21521;&#20110;&#36981;&#24490;&#34920;&#38754;&#26684;&#24335;&#25552;&#20379;&#19982;&#35823;&#23548;&#20449;&#24687;&#30456;&#28151;&#28102;&#30340;&#21453;&#39304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#21033;&#29992;&#24320;&#28304;&#20195;&#30721;LLMs&#29983;&#25104;&#20855;&#26377;&#27491;&#30830;&#25351;&#23548;&#30340;&#26377;&#29992;&#21453;&#39304;&#29992;&#20110;&#20195;&#30721;&#32534;&#36753;&#12290;&#20026;&#27492;&#24341;&#20837;&#20102;Coffee&#65292;&#19968;&#20010;&#19987;&#20026;&#24102;&#26377;&#21453;&#39304;&#30340;&#20195;&#30721;&#20462;&#22797;&#32780;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#65292;&#26500;&#24314;&#20102;CoffeePots&#65292;&#19968;&#20010;&#36890;&#36807;&#20559;&#22909;&#20248;&#21270;&#35843;&#25972;&#21644;&#36873;&#25321;&#30340;COde Fixing with FEEdback&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#26377;&#29992;&#30340;&#21453;&#39304;&#20197;&#24110;&#21161;&#20195;&#30721;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07215v2 Announce Type: replace  Abstract: Code editing is an essential step towards reliable program synthesis to automatically correct critical errors generated from code LLMs. Recent studies have demonstrated that closed-source LLMs (i.e., ChatGPT and GPT-4) are capable of generating corrective feedback to edit erroneous inputs. However, it remains challenging for open-source code LLMs to generate feedback for code editing, since these models tend to adhere to the superficial formats of feedback and provide feedback with misleading information. Hence, the focus of our work is to leverage open-source code LLMs to generate helpful feedback with correct guidance for code editing. To this end, we present Coffee, a collected dataset specifically designed for code fixing with feedback. Using this dataset, we construct CoffeePots, a framework for COde Fixing with FEEdback via Preference-Optimized Tuning and Selection. The proposed framework aims to automatically generate helpful 
&lt;/p&gt;</description></item><item><title>&#22312;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#36807;&#31243;&#20013;&#65292;&#35813;&#30740;&#31350;&#39318;&#27425;&#20840;&#38754;&#20998;&#26512;&#20102;&#19981;&#21516;&#20219;&#21153;&#20013;&#27169;&#22411;&#30340;&#35760;&#24518;&#29616;&#35937;&#65292;&#21457;&#29616;&#20102;&#35760;&#24518;&#22312;&#21508;&#31181;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#29702;&#35770;&#35299;&#37322;&#20102;&#36825;&#31181;&#20219;&#21153;&#24046;&#24322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.06714</link><description>&lt;p&gt;
&#25506;&#32034;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring Memorization in Fine-tuned Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06714
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#36807;&#31243;&#20013;&#65292;&#35813;&#30740;&#31350;&#39318;&#27425;&#20840;&#38754;&#20998;&#26512;&#20102;&#19981;&#21516;&#20219;&#21153;&#20013;&#27169;&#22411;&#30340;&#35760;&#24518;&#29616;&#35937;&#65292;&#21457;&#29616;&#20102;&#35760;&#24518;&#22312;&#21508;&#31181;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#29702;&#35770;&#35299;&#37322;&#20102;&#36825;&#31181;&#20219;&#21153;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24040;&#22823;&#33021;&#21147;&#65292;&#20294;&#21516;&#26102;&#20063;&#34920;&#29616;&#20986;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#65292;&#24341;&#36215;&#20102;&#24040;&#22823;&#30340;&#38544;&#31169;&#21644;&#29256;&#26435;&#25285;&#24551;&#12290; &#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26102;&#30340;&#35760;&#24518;&#29616;&#35937;&#12290; &#25105;&#20204;&#20351;&#29992;&#24320;&#28304;&#21644;&#25105;&#20204;&#33258;&#24049;&#30340;&#24494;&#35843;LMs&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#19981;&#21516;&#24494;&#35843;&#20219;&#21153;&#20013;&#65292;&#35760;&#24518;&#21576;&#29616;&#20986;&#36739;&#24378;&#30340;&#24046;&#24322;&#24615;&#12290; &#25105;&#20204;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#29702;&#35770;&#25552;&#20379;&#20102;&#23545;&#36825;&#31181;&#20219;&#21153;&#24046;&#24322;&#24615;&#30340;&#30452;&#35266;&#35299;&#37322;&#65292;&#24182;&#25581;&#31034;&#20102;&#35760;&#24518;&#21644;&#27880;&#24847;&#21147;&#20998;&#25968;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06714v2 Announce Type: replace  Abstract: Large language models (LLMs) have shown great capabilities in various tasks but also exhibited memorization of training data, raising tremendous privacy and copyright concerns. While prior works have studied memorization during pre-training, the exploration of memorization during fine-tuning is rather limited. Compared to pre-training, fine-tuning typically involves more sensitive data and diverse objectives, thus may bring distinct privacy risks and unique memorization behaviors. In this work, we conduct the first comprehensive analysis to explore language models' (LMs) memorization during fine-tuning across tasks. Our studies with open-sourced and our own fine-tuned LMs across various tasks indicate that memorization presents a strong disparity among different fine-tuning tasks. We provide an intuitive explanation of this task disparity via sparse coding theory and unveil a strong correlation between memorization and attention scor
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20195;&#30721;&#29420;&#29305;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#21360;&#26041;&#27861;SWEET&#65292;&#36890;&#36807;&#22312;&#20855;&#26377;&#39640;&#29109;&#30340;&#20301;&#32622;&#20165;&#25918;&#32622;&#8220;&#32511;&#33394;&#8221;&#20196;&#29260;&#26469;&#30830;&#20445;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#27979;&#35797;&#21644;Z&#20998;&#25968;&#36827;&#34892;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2305.15060</link><description>&lt;p&gt;
&#35841;&#32534;&#20889;&#20102;&#36825;&#27573;&#20195;&#30721;&#65311;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Who Wrote this Code? Watermarking for Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.15060
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20195;&#30721;&#29420;&#29305;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#21360;&#26041;&#27861;SWEET&#65292;&#36890;&#36807;&#22312;&#20855;&#26377;&#39640;&#29109;&#30340;&#20301;&#32622;&#20165;&#25918;&#32622;&#8220;&#32511;&#33394;&#8221;&#20196;&#29260;&#26469;&#30830;&#20445;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#27979;&#35797;&#21644;Z&#20998;&#25968;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#33394;&#29983;&#25104;&#24615;&#33021;&#65292;&#20851;&#20110;&#20351;&#29992;&#23427;&#20204;&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#38382;&#39064;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#65292;&#22914;&#25220;&#34989;&#21644;&#29256;&#26435;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#27700;&#21360;&#21644;&#26816;&#27979;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20808;&#21069;&#30340;&#26041;&#27861;&#30001;&#20110;&#20195;&#30721;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#26080;&#27861;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#12290;&#22522;&#20110;Kirchenbauer&#31561;&#20154;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#21360;&#26041;&#27861;&#65292;&#21517;&#20026;Selective WatErmarking via Entropy Thresholding&#65288;SWEET&#65289;&#65292;&#35813;&#26041;&#27861;&#20165;&#22312;&#29983;&#25104;&#26399;&#38388;&#23558;&#8220;&#32511;&#33394;&#8221;&#20196;&#29260;&#25918;&#32622;&#22312;&#20855;&#26377;&#39640;&#29109;&#30340;&#20196;&#29260;&#20998;&#24067;&#20301;&#32622;&#65292;&#20174;&#32780;&#20445;&#30041;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;&#27700;&#21360;&#20195;&#30721;&#36890;&#36807;&#22522;&#20110;&#29109;&#20449;&#24687;&#30340;&#32479;&#35745;&#27979;&#35797;&#21644;Z&#20998;&#25968;&#36827;&#34892;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;HumanEval&#21644;MBPP&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SWEET&#26174;&#33879;&#25913;&#21892;&#20102;&#29983;&#25104;&#20195;&#30721;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.15060v3 Announce Type: replace  Abstract: With the remarkable generation performance of large language models, ethical and legal concerns about using them have been raised, such as plagiarism and copyright issues. For such concerns, several approaches to watermark and detect LLM-generated text have been proposed very recently. However, we discover that the previous methods fail to function appropriately with code generation tasks because of the syntactic and semantic characteristics of code. Based on \citet{Kirchenbauer2023watermark}, we propose a new watermarking method, Selective WatErmarking via Entropy Thresholding (SWEET), that promotes "green" tokens only at the position with high entropy of the token distribution during generation, thereby preserving the correctness of the generated code. The watermarked code is detected by the statistical test and Z-score based on the entropy information. Our experiments on HumanEval and MBPP show that SWEET significantly improves th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#23569;&#26679;&#26412;&#23454;&#20307;&#35782;&#21035;&#38382;&#39064;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#20043;&#38388;&#30340;&#25299;&#25169;&#30456;&#37051;&#20851;&#31995;&#65292;&#24378;&#35843;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#65292;&#23454;&#29616;&#23545;&#22270;&#20687;&#25805;&#20316;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2305.14828</link><description>&lt;p&gt;
&#38024;&#23545;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#23569;&#26679;&#26412;&#23454;&#20307;&#35782;&#21035;&#65306;&#19968;&#31181;&#23545;&#22270;&#20687;&#25805;&#20316;&#40065;&#26834;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Few-shot Entity Recognition in Document Images: A Graph Neural Network Approach Robust to Image Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#23569;&#26679;&#26412;&#23454;&#20307;&#35782;&#21035;&#38382;&#39064;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#20043;&#38388;&#30340;&#25299;&#25169;&#30456;&#37051;&#20851;&#31995;&#65292;&#24378;&#35843;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#65292;&#23454;&#29616;&#23545;&#22270;&#20687;&#25805;&#20316;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#24067;&#23616;&#20449;&#24687;&#65288;&#36890;&#24120;&#20026;&#36793;&#30028;&#26694;&#22352;&#26631;&#65289;&#34701;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#25991;&#26723;&#22270;&#20687;&#20013;&#23454;&#29616;&#20102;&#23454;&#20307;&#35782;&#21035;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22352;&#26631;&#21487;&#20197;&#36731;&#26494;&#22320;&#24314;&#27169;&#27599;&#20010;&#26631;&#35760;&#30340;&#32477;&#23545;&#20301;&#32622;&#65292;&#20294;&#22312;&#25991;&#26723;&#22270;&#20687;&#65288;&#22914;&#24179;&#31227;&#12289;&#26059;&#36716;&#25110;&#32553;&#25918;&#31561;&#65289;&#30340;&#25805;&#32437;&#19979;&#21487;&#33021;&#24456;&#25935;&#24863;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#26631;&#35760;&#20043;&#38388;&#30340;&#25299;&#25169;&#30456;&#37051;&#20851;&#31995;&#65292;&#24378;&#35843;&#23427;&#20204;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25991;&#26723;&#20013;&#30340;&#26631;&#35760;&#35270;&#20026;&#33410;&#28857;&#65292;&#24182;&#26681;&#25454;&#26368;&#36817;&#37051;&#36793;&#30028;&#26694;&#30340;&#25299;&#25169;&#21551;&#21457;&#24335;&#24418;&#25104;&#36793;&#12290;&#36825;&#20123;&#30456;&#37051;&#22270;&#23545;&#20223;&#23556;&#21464;&#25442;&#65288;&#21253;&#25324;&#24179;&#31227;&#12289;&#26059;&#36716;&#21644;&#32553;&#25918;&#65289;&#19981;&#21464;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#36825;&#20123;&#22270;&#32467;&#26500;&#32467;&#21512;&#21040;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.14828v2 Announce Type: replace  Abstract: Recent advances of incorporating layout information, typically bounding box coordinates, into pre-trained language models have achieved significant performance in entity recognition from document images. Using coordinates can easily model the absolute position of each token, but they might be sensitive to manipulations in document images (e.g., shifting, rotation or scaling), especially when the training data is limited in few-shot settings. In this paper, we propose to further introduce the topological adjacency relationship among the tokens, emphasizing their relative position information. Specifically, we consider the tokens in the documents as nodes and formulate the edges based on the topological heuristics from the k-nearest bounding boxes. Such adjacency graphs are invariant to affine transformations including shifting, rotations and scaling. We incorporate these graphs into the pre-trained language model by adding graph neura
&lt;/p&gt;</description></item><item><title>&#36127;&#38754;&#35789;&#27719;&#23545;&#26032;&#38395;&#24773;&#32490;&#35780;&#20998;&#30340;&#36127;&#38754;&#24615;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#22686;&#24378;&#30340;&#24773;&#32490;&#35789;&#20856;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#26032;&#38395;&#24773;&#32490;&#23545;&#32929;&#24066;&#25351;&#25968;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2304.00468</link><description>&lt;p&gt;
&#26377;&#37325;&#35201;&#24433;&#21709;&#30340;&#35789;&#35821;&#65306;&#36127;&#38754;&#35789;&#27719;&#23545;&#26032;&#38395;&#24773;&#32490;&#21644;&#32929;&#24066;&#25351;&#25968;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Words that Matter: The Impact of Negative Words on News Sentiment and Stock Market Index
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.00468
&lt;/p&gt;
&lt;p&gt;
&#36127;&#38754;&#35789;&#27719;&#23545;&#26032;&#38395;&#24773;&#32490;&#35780;&#20998;&#30340;&#36127;&#38754;&#24615;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#22686;&#24378;&#30340;&#24773;&#32490;&#35789;&#20856;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#26032;&#38395;&#24773;&#32490;&#23545;&#32929;&#24066;&#25351;&#25968;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36127;&#38754;&#35789;&#27719;&#23545;&#24773;&#32490;&#20998;&#26512;&#30340;&#24433;&#21709;&#20197;&#21450;&#20854;&#23545;&#38889;&#22269;&#32929;&#24066;&#25351;&#25968;KOSPI200&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#20351;&#29992;Word2Vec&#12289;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;&#25193;&#23637;&#35789;&#27719;&#34920;&#20998;&#26512;&#20102;45,723&#31687;&#38889;&#22269;&#26085;&#24120;&#32463;&#27982;&#26032;&#38395;&#25991;&#31456;&#30340;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23558;&#36127;&#38754;&#35789;&#27719;&#32435;&#20837;&#24773;&#32490;&#35780;&#20998;&#26174;&#33879;&#22686;&#21152;&#20102;&#26032;&#38395;&#26631;&#39064;&#20013;&#30340;&#36127;&#38754;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#32929;&#24066;&#25351;&#25968;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#20010;&#22686;&#24378;&#30340;&#24773;&#32490;&#35789;&#20856;&#65288;Sent1000&#65289;&#65292;&#21253;&#25324;&#19982;&#8220;&#21361;&#26426;&#8221;&#20855;&#26377;&#39640;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#21069;1,000&#20010;&#36127;&#38754;&#35789;&#27719;&#65292;&#27604;&#21407;&#22987;&#24773;&#32490;&#35789;&#20856;&#65288;Sent0&#65289;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#26032;&#38395;&#24773;&#32490;&#23545;&#32929;&#24066;&#25351;&#25968;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;&#20998;&#26512;&#26032;&#38395;&#20869;&#23481;&#21450;&#20854;&#23545;&#24066;&#22330;&#21160;&#24577;&#21644;&#20844;&#20247;&#33286;&#35770;&#30340;&#28508;&#22312;&#24433;&#21709;&#26102;&#32771;&#34385;&#36127;&#38754;&#32454;&#24494;&#24046;&#21035;&#21644;&#35821;&#22659;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.00468v2 Announce Type: replace  Abstract: This study investigates the impact of negative words on sentiment analysis and its effect on the South Korean stock market index, KOSPI200. The research analyzes a dataset of 45,723 South Korean daily economic news articles using Word2Vec, cosine similarity, and an expanded lexicon. The findings suggest that incorporating negative words significantly increases sentiment scores' negativity in news titles, which can affect the stock market index. The study reveals that an augmented sentiment lexicon (Sent1000), including the top 1,000 negative words with high cosine similarity to 'Crisis,' more effectively captures the impact of news sentiment on the stock market index than the original sentiment lexicon (Sent0). The results underscore the importance of considering negative nuances and context when analyzing news content and its potential impact on market dynamics and public opinion.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35777;&#26126;&#23545;&#20110;&#25506;&#27979;&#35789;&#12289;&#19978;&#19979;&#25991;&#21644;&#25552;&#31034;&#25935;&#24863;&#65292;&#20294;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#24037;&#20855;&#36741;&#21161;&#22240;&#26524;&#22270;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2303.05279</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26500;&#24314;&#22240;&#26524;&#22270;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can large language models build causal graphs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.05279
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35777;&#26126;&#23545;&#20110;&#25506;&#27979;&#35789;&#12289;&#19978;&#19979;&#25991;&#21644;&#25552;&#31034;&#25935;&#24863;&#65292;&#20294;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#24037;&#20855;&#36741;&#21161;&#22240;&#26524;&#22270;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#22240;&#26524;&#22270;&#21487;&#33021;&#26159;&#19968;&#20010;&#36153;&#26102;&#36153;&#21147;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#30830;&#20445;&#25429;&#25417;&#21040;&#25152;&#26377;&#30456;&#20851;&#30340;&#22240;&#26524;&#36335;&#24452;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#19981;&#20165;&#35201;&#19982;&#20020;&#24202;&#21307;&#29983;&#21644;&#19987;&#23478;&#35752;&#35770;&#65292;&#36824;&#35201;&#23457;&#38405;&#22823;&#37327;&#30456;&#20851;&#30340;&#21307;&#23398;&#25991;&#29486;&#12290;&#36890;&#36807;&#32534;&#30721;&#24120;&#35265;&#21644;&#21307;&#23398;&#30693;&#35782;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20195;&#34920;&#20102;&#19968;&#31181;&#26426;&#20250;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#35780;&#20998;&#28508;&#22312;&#22270;&#20013;&#30340;&#36793;&#32536;&#65288;&#21363;&#20004;&#20010;&#21464;&#37327;&#20043;&#38388;&#30340;&#32852;&#31995;&#65289;&#26469;&#31616;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#35777;&#26126;LLMs&#23545;&#29992;&#25143;&#36873;&#25321;&#30340;&#25506;&#27979;&#35789;&#12289;&#19978;&#19979;&#25991;&#21644;&#25552;&#31034;&#38750;&#24120;&#25935;&#24863;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#26159;&#21542;&#33021;&#22815;&#25104;&#20026;&#34917;&#20805;&#22240;&#26524;&#22270;&#21457;&#23637;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.05279v2 Announce Type: replace-cross  Abstract: Building causal graphs can be a laborious process. To ensure all relevant causal pathways have been captured, researchers often have to discuss with clinicians and experts while also reviewing extensive relevant medical literature. By encoding common and medical knowledge, large language models (LLMs) represent an opportunity to ease this process by automatically scoring edges (i.e., connections between two variables) in potential graphs. LLMs however have been shown to be brittle to the choice of probing words, context, and prompts that the user employs. In this work, we evaluate if LLMs can be a useful tool in complementing causal graph development.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#20837;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#20381;&#36182;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#27169;&#22411;&#19982;&#25972;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#20851;&#38190;&#35789;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;PLMs&#20855;&#26377;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39640;&#36164;&#28304;&#24615;&#33021;&#21644;&#26368;&#20808;&#36827;&#30340;&#20302;&#36164;&#28304;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#23545;&#22522;&#20110;PLM&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2212.10233</link><description>&lt;p&gt;
&#38754;&#21521;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#28145;&#20837;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models for Keyphrase Generation: A Thorough Empirical Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.10233
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#20837;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#20381;&#36182;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#27169;&#22411;&#19982;&#25972;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#20851;&#38190;&#35789;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;PLMs&#20855;&#26377;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39640;&#36164;&#28304;&#24615;&#33021;&#21644;&#26368;&#20808;&#36827;&#30340;&#20302;&#36164;&#28304;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#23545;&#22522;&#20110;PLM&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#20381;&#36182;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#27169;&#22411;&#22312;&#25317;&#26377;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#22312;&#20851;&#38190;&#35789;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26032;&#26041;&#27861;&#24050;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#32435;&#20837;&#20854;&#25968;&#25454;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#23545;&#27604;&#36825;&#20004;&#31181;&#26041;&#27861;&#20197;&#21450;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;PLM&#27169;&#22411;&#24615;&#33021;&#30340;&#31995;&#32479;&#30740;&#31350;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#30693;&#35782;&#31354;&#30333;&#65292;&#24182;&#20419;&#36827;&#26356;&#21152;&#26126;&#26234;&#22320;&#20351;&#29992;PLMs&#36827;&#34892;&#20851;&#38190;&#35789;&#25552;&#21462;&#21644;&#20851;&#38190;&#35789;&#29983;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#20837;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;&#20851;&#38190;&#35789;&#25552;&#21462;&#26500;&#24314;&#20026;&#24207;&#21015;&#26631;&#27880;&#65292;&#20851;&#38190;&#35789;&#29983;&#25104;&#26500;&#24314;&#20026;&#24207;&#21015;&#21040;&#24207;&#21015;&#29983;&#25104;&#65292;&#24182;&#22312;&#19977;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;&#22312;&#23637;&#31034;PLMs&#20855;&#26377;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39640;&#36164;&#28304;&#24615;&#33021;&#21644;&#26368;&#20808;&#36827;&#30340;&#20302;&#36164;&#28304;&#24615;&#33021;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#37325;&#35201;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#21253;&#25324;&#39046;&#22495;&#20869;PLMs&#65292;&#20855;&#26377;&#19981;&#21516;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;PLMs&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.10233v2 Announce Type: replace  Abstract: Neural models that do not rely on pre-training have excelled in the keyphrase generation task with large annotated datasets. Meanwhile, new approaches have incorporated pre-trained language models (PLMs) for their data efficiency. However, there lacks a systematic study of how the two types of approaches compare and how different design choices can affect the performance of PLM-based models. To fill in this knowledge gap and facilitate a more informed use of PLMs for keyphrase extraction and keyphrase generation, we present an in-depth empirical study. Formulating keyphrase extraction as sequence labeling and keyphrase generation as sequence-to-sequence generation, we perform extensive experiments in three domains. After showing that PLMs have competitive high-resource performance and state-of-the-art low-resource performance, we investigate important design choices including in-domain PLMs, PLMs with different pre-training objective
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21307;&#23398;&#30693;&#35782;&#22270;&#38382;&#31572;&#27169;&#22411;MedKGQA&#65292;&#21033;&#29992;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#21644;&#24320;&#25918;&#22495;&#25991;&#26723;&#26500;&#24314;&#33647;&#29289;-&#34507;&#30333;&#36136;&#19977;&#20803;&#32452;&#30693;&#35782;&#22270;&#65292;&#36890;&#36807;&#21521;&#37327;&#21270;&#38774;&#28857;&#23646;&#24615;&#21644;&#24314;&#31435;&#26377;&#21521;&#36830;&#25509;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65292;&#21462;&#24471;4.5%&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2212.09400</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#36339;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#21307;&#23398;&#30693;&#35782;&#22270;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Medical Knowledge Graph QA for Drug-Drug Interaction Prediction based on Multi-hop Machine Reading Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.09400
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21307;&#23398;&#30693;&#35782;&#22270;&#38382;&#31572;&#27169;&#22411;MedKGQA&#65292;&#21033;&#29992;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#21644;&#24320;&#25918;&#22495;&#25991;&#26723;&#26500;&#24314;&#33647;&#29289;-&#34507;&#30333;&#36136;&#19977;&#20803;&#32452;&#30693;&#35782;&#22270;&#65292;&#36890;&#36807;&#21521;&#37327;&#21270;&#38774;&#28857;&#23646;&#24615;&#21644;&#24314;&#31435;&#26377;&#21521;&#36830;&#25509;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65292;&#21462;&#24471;4.5%&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#22312;&#20998;&#23376;&#29983;&#29289;&#23398;&#20013;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#36807;&#21307;&#23398;&#23454;&#39564;&#35266;&#23519;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#21644;&#20154;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MedKGQA&#30340;&#21307;&#23398;&#30693;&#35782;&#22270;&#38382;&#31572;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#23553;&#38381;&#22495;&#25991;&#29486;&#20013;&#36827;&#34892;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#21644;&#26500;&#24314;&#24320;&#25918;&#22495;&#25991;&#26723;&#20013;&#30340;&#33647;&#29289;-&#34507;&#30333;&#36136;&#19977;&#20803;&#32452;&#30693;&#35782;&#22270;&#65292;&#26469;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23454;&#20307;&#23884;&#20837;&#21521;&#37327;&#21270;&#22270;&#20013;&#30340;&#33647;&#29289;-&#34507;&#30333;&#36136;&#38774;&#28857;&#23646;&#24615;&#65292;&#24182;&#26681;&#25454;&#20154;&#20307;&#20869;&#34507;&#30333;&#36136;&#38774;&#28857;&#30340;&#20195;&#35874;&#30456;&#20114;&#20316;&#29992;&#36890;&#36335;&#20043;&#38388;&#24314;&#31435;&#33647;&#29289;&#21644;&#34507;&#30333;&#36136;&#23454;&#20307;&#20043;&#38388;&#30340;&#26377;&#21521;&#36830;&#25509;&#12290;&#36825;&#23558;&#22810;&#20010;&#22806;&#37096;&#30693;&#35782;&#23545;&#40784;&#24182;&#24212;&#29992;&#20110;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#27809;&#26377;&#33457;&#21736;&#25216;&#24039;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;4.5%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.09400v3 Announce Type: replace  Abstract: Drug-drug interaction prediction is a crucial issue in molecular biology. Traditional methods of observing drug-drug interactions through medical experiments require significant resources and labor. This paper presents a medical knowledge graph question answering model, dubbed MedKGQA, that predicts drug-drug interaction by employing machine reading comprehension from closed-domain literature and constructing a knowledge graph of drug-protein triplets from open-domain documents. The model vectorizes the drug-protein target attributes in the graph using entity embeddings and establishes directed connections between drug and protein entities based on the metabolic interaction pathways of protein targets in the human body. This aligns multiple external knowledge and applies it to learn the graph neural network. Without bells and whistles, the proposed model achieved a 4.5% improvement in terms of drug-drug interaction prediction accurac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35328;&#36766;&#36827;&#34892;&#35780;&#20998;&#24182;&#29983;&#25104;&#35299;&#37322;&#24615;&#25688;&#35201;&#65292;&#26469;&#22686;&#21152;&#35875;&#35328;&#39564;&#35777;&#30340;&#20449;&#24687;&#20016;&#23500;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12713</link><description>&lt;p&gt;
&#29983;&#25104;&#26080;&#30417;&#30563;&#30340;&#35328;&#36766;&#35299;&#37322;&#29992;&#20110;&#35875;&#35328;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Generating Unsupervised Abstractive Explanations for Rumour Verification. (arXiv:2401.12713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35328;&#36766;&#36827;&#34892;&#35780;&#20998;&#24182;&#29983;&#25104;&#35299;&#37322;&#24615;&#25688;&#35201;&#65292;&#26469;&#22686;&#21152;&#35875;&#35328;&#39564;&#35777;&#30340;&#20449;&#24687;&#20016;&#23500;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36827;&#34892;&#35875;&#35328;&#39564;&#35777;&#30340;&#20219;&#21153;&#28041;&#21450;&#26681;&#25454;&#30001;&#35813;&#35875;&#35328;&#24341;&#36215;&#30340;&#23545;&#35805;&#32447;&#31243;&#35780;&#20272;&#20854;&#30495;&#23454;&#24615;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#19987;&#27880;&#20110;&#39044;&#27979;&#30495;&#23454;&#24615;&#26631;&#31614;&#65292;&#20294;&#25105;&#20204;&#22312;&#36825;&#37324;&#37325;&#26032;&#21046;&#23450;&#20102;&#20219;&#21153;&#65292;&#20197;&#29983;&#25104;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#35875;&#35328;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#21033;&#29992;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#23545;&#32447;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#24086;&#23376;&#36827;&#34892;&#35780;&#20998;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#24086;&#23376;&#36890;&#36807;&#20351;&#29992;&#27169;&#26495;&#24341;&#23548;&#24635;&#32467;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#35299;&#37322;&#24615;&#25688;&#35201;&#12290;&#20026;&#20102;&#35780;&#20272;&#35299;&#37322;&#24615;&#25688;&#35201;&#30340;&#20449;&#24687;&#37327;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#25688;&#35201;&#26102;&#21487;&#20197;&#19982;&#20154;&#31867;&#36798;&#21040;&#31867;&#20284;&#30340;&#19968;&#33268;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35299;&#37322;&#24615;&#30340;&#27010;&#25324;&#25688;&#35201;&#27604;&#20165;&#20351;&#29992;&#32447;&#31243;&#20013;&#25490;&#21517;&#26368;&#39640;&#30340;&#24086;&#23376;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#24182;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#39044;&#27979;&#30340;&#35875;&#35328;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of rumour verification in social media concerns assessing the veracity of a claim on the basis of conversation threads that result from it. While previous work has focused on predicting a veracity label, here we reformulate the task to generate model-centric, free-text explanations of a rumour's veracity. We follow an unsupervised approach by first utilising post-hoc explainability methods to score the most important posts within a thread and then we use these posts to generate informative explanatory summaries by employing template-guided summarisation. To evaluate the informativeness of the explanatory summaries, we exploit the few-shot learning capabilities of a large language model (LLM). Our experiments show that LLMs can have similar agreement to humans in evaluating summaries. Importantly, we show that explanatory abstractive summaries are more informative and better reflect the predicted rumour veracity than just using the highest ranking posts in the thread.
&lt;/p&gt;</description></item><item><title>Kun&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#21644;&#31572;&#26696;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#65292;&#36890;&#36807;&#33258;&#25105;&#31579;&#36873;&#36807;&#31243;&#26469;&#25913;&#21892;&#21644;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25351;&#20196;-&#36755;&#20986;&#23545;&#12290;&#23427;&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#31639;&#27861;&#25913;&#36827;&#25552;&#39640;&#25968;&#25454;&#30340;&#20445;&#30041;&#21644;&#28165;&#26224;&#24230;&#65292;&#24182;&#36890;&#36807;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#20943;&#23569;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2401.06477</link><description>&lt;p&gt;
Kun: &#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#30340;&#20013;&#22269;&#33258;&#23545;&#40784;&#38382;&#39064;&#30340;&#31572;&#26696;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation. (arXiv:2401.06477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06477
&lt;/p&gt;
&lt;p&gt;
Kun&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#21644;&#31572;&#26696;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#65292;&#36890;&#36807;&#33258;&#25105;&#31579;&#36873;&#36807;&#31243;&#26469;&#25913;&#21892;&#21644;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25351;&#20196;-&#36755;&#20986;&#23545;&#12290;&#23427;&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#31639;&#27861;&#25913;&#36827;&#25552;&#39640;&#25968;&#25454;&#30340;&#20445;&#30041;&#21644;&#28165;&#26224;&#24230;&#65292;&#24182;&#36890;&#36807;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#20943;&#23569;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Kun&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#20381;&#36182;&#25163;&#21160;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;Kun&#21033;&#29992;&#26469;&#33258;&#21566;&#36947;&#12289;&#23436;&#21367;&#21644;SkyPile&#31561;&#22810;&#20010;&#26469;&#28304;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#37319;&#29992;&#22522;&#20110;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#21644;&#31572;&#26696;&#20248;&#21270;&#30340;&#33258;&#25105;&#35757;&#32451;&#31639;&#27861;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#20013;&#25991;&#25351;&#23548;&#25968;&#25454;&#28857;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#31579;&#36873;&#36807;&#31243;&#26469;&#23436;&#21892;&#21644;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25351;&#20196;-&#36755;&#20986;&#23545;&#65292;&#26174;&#33879;&#20559;&#31163;&#20256;&#32479;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;6B&#21442;&#25968;&#30340;Yi&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;Kun&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#36129;&#29486;&#22312;&#20110;&#31639;&#27861;&#30340;&#25913;&#36827;&#65292;&#22686;&#24378;&#20102;&#25968;&#25454;&#30340;&#20445;&#30041;&#21644;&#28165;&#26224;&#24230;&#65292;&#24182;&#19988;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#23545;&#26114;&#36149;&#21644;&#32791;&#26102;&#30340;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;&#36825;&#31181;&#26041;&#27861;ological&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#20013;&#25991;&#33258;&#23545;&#40784;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Kun, a novel approach for creating high-quality instruction-tuning datasets for large language models (LLMs) without relying on manual annotations. Adapting a self-training algorithm based on instruction back-translation and answer polishment, Kun leverages unlabelled data from diverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial dataset of over a million Chinese instructional data points. This approach significantly deviates from traditional methods by using a self-curation process to refine and select the most effective instruction-output pairs. Our experiments with the 6B-parameter Yi model across various benchmarks demonstrate Kun's robustness and scalability. Our method's core contributions lie in its algorithmic advancement, which enhances data retention and clarity, and its innovative data generation approach that substantially reduces the reliance on costly and time-consuming manual annotations. This methodology presents a sc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25991;&#26412;&#36164;&#28304;&#65292;&#22522;&#20110;135&#20010;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#36523;&#20221;&#32676;&#20307;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65288;T2I&#65289;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#30340;&#22320;&#29702;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21051;&#26495;&#23646;&#24615;&#22312;&#22270;&#20687;&#20013;&#30340;&#23384;&#22312;&#21487;&#33021;&#24615;&#26159;&#21051;&#26495;&#23646;&#24615;&#30340;&#19977;&#20493;&#12290;</title><link>http://arxiv.org/abs/2401.06310</link><description>&lt;p&gt;
&#36229;&#36234;&#34920;&#38754;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#35270;&#35273;&#21051;&#26495;&#21360;&#35937;&#30340;&#20840;&#29699;&#35268;&#27169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation. (arXiv:2401.06310v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25991;&#26412;&#36164;&#28304;&#65292;&#22522;&#20110;135&#20010;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#36523;&#20221;&#32676;&#20307;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65288;T2I&#65289;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#30340;&#22320;&#29702;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21051;&#26495;&#23646;&#24615;&#22312;&#22270;&#20687;&#20013;&#30340;&#23384;&#22312;&#21487;&#33021;&#24615;&#26159;&#21051;&#26495;&#23646;&#24615;&#30340;&#19977;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#24050;&#32463;&#24378;&#35843;&#20102;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65288;T2I&#65289;&#27169;&#22411;&#29983;&#25104;&#30340;&#20154;&#29289;&#24418;&#35937;&#20013;&#23384;&#22312;&#30340;&#19981;&#21516;&#36523;&#20221;&#32676;&#20307;&#30340;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#38480;&#21046;&#65292;&#21253;&#25324;&#22312;&#35780;&#20272;&#20013;&#23545;&#20840;&#29699;&#36523;&#20221;&#32676;&#20307;&#30340;&#35206;&#30422;&#29575;&#26126;&#26174;&#19981;&#36275;&#65292;&#20197;&#21450;&#30456;&#20851;&#21051;&#26495;&#21360;&#35937;&#30340;&#33539;&#22260;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36890;&#24120;&#32570;&#20047;&#23545;&#26412;&#36136;&#19978;&#26159;&#35270;&#35273;&#21051;&#26495;&#21360;&#35937;&#65288;&#22914;&#8220;&#30246;&#24369;&#8221;&#25110;&#8220;&#22696;&#35199;&#21733;&#33609;&#24125;&#8221;&#65289;&#21644;&#25991;&#21270;&#30456;&#20851;&#30340;&#21051;&#26495;&#21360;&#35937;&#65288;&#22914;&#8220;&#21560;&#24341;&#20154;&#8221;&#25110;&#8220;&#24656;&#24598;&#20998;&#23376;&#8221;&#65289;&#20043;&#38388;&#30340;&#37325;&#35201;&#21306;&#21035;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25991;&#26412;&#36164;&#28304;&#26469;&#23558;&#25105;&#20204;&#23545;&#26469;&#33258;T2I&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#19982;&#22320;&#29702;&#25991;&#21270;&#30456;&#20851;&#30340;&#21051;&#26495;&#21360;&#35937;&#30340;&#35780;&#20272;&#36827;&#34892;&#22522;&#30784;&#32465;&#23450;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#21051;&#26495;&#21360;&#35937;&#22522;&#20934;&#26469;&#35782;&#21035;&#21644;&#35780;&#20272;&#20840;&#29699;&#33539;&#22260;&#20869;&#28041;&#21450;135&#20010;&#22522;&#20110;&#22269;&#31821;&#30340;&#36523;&#20221;&#32676;&#20307;&#30340;&#35270;&#35273;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#22270;&#20687;&#20013;&#23384;&#22312;&#21051;&#26495;&#21360;&#35937;&#30340;&#21487;&#33021;&#24615;&#26159;&#21051;&#26495;&#23646;&#24615;&#30340;&#19977;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have highlighted the issue of stereotypical depictions for people of different identity groups in Text-to-Image (T2I) model generations. However, these existing approaches have several key limitations, including a noticeable lack of coverage of global identity groups in their evaluation, and the range of their associated stereotypes. Additionally, they often lack a critical distinction between inherently visual stereotypes, such as `underweight' or `sombrero', and culturally dependent stereotypes like `attractive' or `terrorist'. In this work, we address these limitations with a multifaceted approach that leverages existing textual resources to ground our evaluation of geo-cultural stereotypes in the generated images from T2I models. We employ existing stereotype benchmarks to identify and evaluate visual stereotypes at a global scale, spanning 135 nationality-based identity groups. We demonstrate that stereotypical attributes are thrice as likely to be present in images
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;EpiK-Eval&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#20998;&#21106;&#30340;&#21465;&#36848;&#20013;&#26500;&#24314;&#36830;&#36143;&#21644;&#19968;&#33268;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#30340;&#35757;&#32451;&#30446;&#26631;&#23384;&#22312;&#22266;&#26377;&#30340;&#32570;&#38519;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#25913;&#36827;&#30693;&#35782;&#25972;&#21512;&#26041;&#27861;&#30340;&#24314;&#35758;&#65292;&#20197;&#22823;&#24133;&#25552;&#39640;LLMs&#30340;&#25972;&#20307;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15372</link><description>&lt;p&gt;
EpiK-Eval&#65306;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#35782;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
EpiK-Eval: Evaluation for Language Models as Epistemic Models. (arXiv:2310.15372v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15372
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;EpiK-Eval&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#20998;&#21106;&#30340;&#21465;&#36848;&#20013;&#26500;&#24314;&#36830;&#36143;&#21644;&#19968;&#33268;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#30340;&#35757;&#32451;&#30446;&#26631;&#23384;&#22312;&#22266;&#26377;&#30340;&#32570;&#38519;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#25913;&#36827;&#30693;&#35782;&#25972;&#21512;&#26041;&#27861;&#30340;&#24314;&#35758;&#65292;&#20197;&#22823;&#24133;&#25552;&#39640;LLMs&#30340;&#25972;&#20307;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20316;&#29992;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23613;&#31649;&#23427;&#20204;&#26085;&#30410;&#26222;&#21450;&#65292;&#20294;&#23427;&#20204;&#22312;&#20174;&#19981;&#21516;&#35757;&#32451;&#25991;&#26723;&#20013;&#25972;&#21512;&#30693;&#35782;&#30340;&#33021;&#21147;&#8212;&#8212;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#37117;&#26159;&#20851;&#38190;&#33021;&#21147;&#8212;&#8212;&#20173;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;LLMs&#22312;&#20854;&#21442;&#25968;&#31354;&#38388;&#20869;&#26377;&#25928;&#22320;&#32467;&#21512;&#36825;&#31181;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;EpiK-Eval&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#20174;&#20998;&#21106;&#30340;&#21465;&#36848;&#20013;&#26500;&#24314;&#19968;&#31181;&#36830;&#36143;&#21644;&#19968;&#33268;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#23545;&#21508;&#31181;LLMs&#30340;&#35780;&#20272;&#25581;&#31034;&#20102;&#22312;&#36825;&#19968;&#39046;&#22495;&#23384;&#22312;&#30340;&#26174;&#33879;&#24369;&#28857;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#32570;&#28857;&#28304;&#20110;&#29616;&#26377;&#35757;&#32451;&#30446;&#26631;&#30340;&#22266;&#26377;&#24615;&#36136;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#25913;&#36827;&#30693;&#35782;&#25972;&#21512;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#36825;&#26377;&#28508;&#21147;&#26174;&#33879;&#25552;&#39640;LLMs&#30340;&#25972;&#20307;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the age of artificial intelligence, the role of large language models (LLMs) is becoming increasingly central. Despite their growing prevalence, their capacity to consolidate knowledge from different training documents - a crucial ability in numerous applications - remains unexplored. This paper presents the first study examining the capability of LLMs to effectively combine such information within their parameter space. We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs' proficiency in formulating a coherent and consistent knowledge representation from segmented narratives. Evaluations across various LLMs reveal significant weaknesses in this domain. We contend that these shortcomings stem from the intrinsic nature of prevailing training objectives. Consequently, we advocate for refining the approach towards knowledge consolidation, as it harbors the potential to dramatically improve their overall effectiveness and performance. The findings from 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MetaTool&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#20855;&#26377;&#24037;&#20855;&#20351;&#29992;&#24847;&#35782;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#36873;&#25321;&#24037;&#20855;&#12290;&#22522;&#20934;&#20013;&#21253;&#21547;&#19968;&#20010;&#21517;&#20026;ToolE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#65292;&#29992;&#20110;&#35302;&#21457;LLMs&#20351;&#29992;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2310.03128</link><description>&lt;p&gt;
MetaTool&#22522;&#20934;&#65306;&#20915;&#23450;&#26159;&#21542;&#20351;&#29992;&#24037;&#20855;&#21644;&#36873;&#25321;&#20351;&#29992;&#21738;&#20010;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
MetaTool Benchmark: Deciding Whether to Use Tools and Which to Use. (arXiv:2310.03128v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MetaTool&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#20855;&#26377;&#24037;&#20855;&#20351;&#29992;&#24847;&#35782;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#36873;&#25321;&#24037;&#20855;&#12290;&#22522;&#20934;&#20013;&#21253;&#21547;&#19968;&#20010;&#21517;&#20026;ToolE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#65292;&#29992;&#20110;&#35302;&#21457;LLMs&#20351;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;LLMs&#30340;&#24037;&#20855;&#21033;&#29992;&#33021;&#21147;&#12290;&#23427;&#20204;&#20027;&#35201;&#30740;&#31350;&#20102;LLMs&#22914;&#20309;&#26377;&#25928;&#22320;&#19982;&#32473;&#23450;&#30340;&#29305;&#23450;&#24037;&#20855;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;&#22312;LLMs&#20805;&#24403;&#26234;&#33021;&#20307;&#30340;&#22330;&#26223;&#20013;&#65292;&#20363;&#22914;AutoGPT&#21644;MetaGPT&#24212;&#29992;&#20013;&#65292;LLMs&#34987;&#26399;&#26395;&#21442;&#19982;&#28041;&#21450;&#26159;&#21542;&#20351;&#29992;&#24037;&#20855;&#20197;&#21450;&#20174;&#21487;&#29992;&#24037;&#20855;&#38598;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#24037;&#20855;&#26469;&#28385;&#36275;&#29992;&#25143;&#35831;&#27714;&#30340;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MetaTool&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#26159;&#21542;&#20855;&#26377;&#24037;&#20855;&#20351;&#29992;&#24847;&#35782;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#36873;&#25321;&#24037;&#20855;&#30340;&#22522;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#35813;&#22522;&#20934;&#20013;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;ToolE&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20197;&#35302;&#21457;LLMs&#20351;&#29992;&#24037;&#20855;&#30340;&#25552;&#31034;&#24418;&#24335;&#20986;&#29616;&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#65292;&#21253;&#25324;&#21333;&#19968;&#24037;&#20855;&#21644;&#22810;&#31181;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have garnered significant attention due to their impressive natural language processing (NLP) capabilities. Recently, many studies have focused on the tool utilization ability of LLMs. They primarily investigated how LLMs effectively collaborate with given specific tools. However, in scenarios where LLMs serve as intelligent agents, as seen in applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate decision-making processes that involve deciding whether to employ a tool and selecting the most suitable tool(s) from a collection of available tools to fulfill user requests. Therefore, in this paper, we introduce MetaTool, a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools. Specifically, we create a dataset called ToolE within the benchmark. This dataset contains various types of user queries in the form of prompts that trigger LLMs to use tools, including both single-tool and multi-too
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;CALLA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;LLMs&#20174;&#20013;&#25991;&#21307;&#23398;&#25991;&#29486;&#20013;&#33719;&#21462;&#20132;&#20114;&#24335;&#30693;&#35782;&#12290;&#36890;&#36807;&#33258;&#30001;&#23545;&#35805;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;LLMs&#25484;&#25569;&#21307;&#23398;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20107;&#23454;&#36319;&#38543;&#21709;&#24212;&#8221;&#30340;&#29616;&#35937;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20154;&#24037;&#26500;&#24314;&#20102;&#20004;&#31181;&#35282;&#24230;&#30340;&#27979;&#35797;&#25968;&#25454;&#65306;&#19968;&#31181;&#19982;&#20107;&#23454;&#19968;&#33268;&#65292;&#19968;&#31181;&#19982;&#20107;&#23454;&#19981;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2309.04198</link><description>&lt;p&gt;
CALLA&#25968;&#25454;&#38598;&#65306;&#20174;&#20013;&#25991;&#21307;&#23398;&#25991;&#29486;&#20013;&#25506;&#32034;LLMs&#30340;&#20132;&#20114;&#24335;&#30693;&#35782;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from Chinese Medical Literature. (arXiv:2309.04198v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04198
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;CALLA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;LLMs&#20174;&#20013;&#25991;&#21307;&#23398;&#25991;&#29486;&#20013;&#33719;&#21462;&#20132;&#20114;&#24335;&#30693;&#35782;&#12290;&#36890;&#36807;&#33258;&#30001;&#23545;&#35805;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;LLMs&#25484;&#25569;&#21307;&#23398;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20107;&#23454;&#36319;&#38543;&#21709;&#24212;&#8221;&#30340;&#29616;&#35937;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20154;&#24037;&#26500;&#24314;&#20102;&#20004;&#31181;&#35282;&#24230;&#30340;&#27979;&#35797;&#25968;&#25454;&#65306;&#19968;&#31181;&#19982;&#20107;&#23454;&#19968;&#33268;&#65292;&#19968;&#31181;&#19982;&#20107;&#23454;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#20852;&#36259;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#36890;&#36807;&#21307;&#23398;&#30693;&#35782;&#22270;&#26500;&#24314;&#25351;&#23548;&#24494;&#35843;&#65288;IFT&#65289;&#25968;&#25454;&#65292;&#20197;&#20016;&#23500;LLMs&#30340;&#20132;&#20114;&#24335;&#21307;&#23398;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#20016;&#23500;&#30340;&#21307;&#23398;&#30693;&#35782;&#26469;&#28304;&#30340;&#21307;&#23398;&#25991;&#29486;&#20173;&#26410;&#34987;&#24320;&#21457;&#21033;&#29992;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;CALLA&#25968;&#25454;&#38598;&#65292;&#20197;&#25506;&#32034;LLMs&#20174;&#20013;&#22269;&#21307;&#23398;&#25991;&#29486;&#20013;&#33719;&#21462;&#20132;&#20114;&#24335;&#30693;&#35782;&#12290;&#23427;&#36890;&#36807;&#33258;&#30001;&#23545;&#35805;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#35780;&#20272;LLMs&#25484;&#25569;&#21307;&#23398;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#31181;&#29616;&#35937;&#31216;&#20026;&#8220;&#20107;&#23454;&#36319;&#38543;&#21709;&#24212;&#8221;&#65292;LLMs&#20542;&#21521;&#20110;&#30830;&#35748;&#38382;&#39064;&#20013;&#25552;&#21040;&#30340;&#20107;&#23454;&#65292;&#24182;&#23545;&#25361;&#25112;&#36825;&#20123;&#20107;&#23454;&#34920;&#29616;&#20986;&#19981;&#24773;&#24895;&#12290;&#20026;&#28040;&#38500;&#36825;&#31181;&#29616;&#35937;&#23548;&#33268;&#30340;&#19981;&#20934;&#30830;&#35780;&#20272;&#65292;&#23545;&#20110;&#40644;&#37329;&#20107;&#23454;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#20154;&#24037;&#26500;&#24314;&#27979;&#35797;&#25968;&#25454;&#65306;&#19968;&#20010;&#19982;&#20107;&#23454;&#19968;&#33268;&#65292;&#19968;&#20010;&#19982;&#20107;&#23454;&#19981;&#19968;&#33268;&#12290;&#26681;&#25454;&#36825;&#20123;&#27979;&#35797;&#25968;&#25454;&#65292;&#25105;&#20204;&#20026;LLMs&#35780;&#20272;&#20854;&#23545;&#21307;&#23398;&#30693;&#35782;&#30340;&#25484;&#25569;&#33021;&#21147;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of Large Language Models (LLMs) to the medical domain has stimulated the interest of researchers. Recent studies have focused on constructing Instruction Fine-Tuning (IFT) data through medical knowledge graphs to enrich the interactive medical knowledge of LLMs. However, the medical literature serving as a rich source of medical knowledge remains unexplored. Our work introduces the CALLA dataset to probe LLMs' interactive knowledge acquisition from Chinese medical literature. It assesses the proficiency of LLMs in mastering medical knowledge through a free-dialogue fact-checking task. We identify a phenomenon called the ``fact-following response``, where LLMs tend to affirm facts mentioned in questions and display a reluctance to challenge them. To eliminate the inaccurate evaluation caused by this phenomenon, for the golden fact, we artificially construct test data from two perspectives: one consistent with the fact and one inconsistent with the fact. Drawing from the 
&lt;/p&gt;</description></item><item><title>ParaGuide&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#39118;&#26684;&#36716;&#31227;&#30340;&#24341;&#23548;&#24615;&#25193;&#25955;&#25913;&#20889;&#22120;&#65292;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#20219;&#24847;&#30446;&#26631;&#39118;&#26684;&#65292;&#36890;&#36807;&#26799;&#24230;&#24341;&#23548;&#21644;&#25913;&#20889;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#30340;&#39118;&#26684;&#36716;&#21464;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.15459</link><description>&lt;p&gt;
ParaGuide: &#29992;&#20110;&#21363;&#25554;&#21363;&#29992;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#30340;&#24341;&#23548;&#24615;&#25193;&#25955;&#25913;&#20889;&#22120;
&lt;/p&gt;
&lt;p&gt;
ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer. (arXiv:2308.15459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15459
&lt;/p&gt;
&lt;p&gt;
ParaGuide&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#39118;&#26684;&#36716;&#31227;&#30340;&#24341;&#23548;&#24615;&#25193;&#25955;&#25913;&#20889;&#22120;&#65292;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#20219;&#24847;&#30446;&#26631;&#39118;&#26684;&#65292;&#36890;&#36807;&#26799;&#24230;&#24341;&#23548;&#21644;&#25913;&#20889;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#30340;&#39118;&#26684;&#36716;&#21464;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#26159;&#22312;&#20445;&#30041;&#24847;&#20041;&#30340;&#21516;&#26102;&#36716;&#21464;&#25991;&#26412;&#30340;&#39118;&#26684;&#23646;&#24615;&#30340;&#20219;&#21153;&#12290;&#30446;&#26631;&#39118;&#26684;&#21487;&#20197;&#20197;&#22810;&#31181;&#26041;&#24335;&#23450;&#20041;&#65292;&#20174;&#21333;&#19968;&#23646;&#24615;&#65288;&#20363;&#22914;&#27491;&#24335;&#24615;&#65289;&#21040;&#20316;&#32773;&#65288;&#20363;&#22914;&#33678;&#22763;&#27604;&#20122;&#65289;&#12290;&#20808;&#21069;&#30340;&#26080;&#30417;&#30563;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#20165;&#36866;&#29992;&#20110;&#22266;&#23450;&#30340;&#39118;&#26684;&#38598;&#65292;&#25110;&#38656;&#35201;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#36890;&#29992;&#39118;&#26684;&#36716;&#31227;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#28789;&#27963;&#36866;&#24212;&#20219;&#24847;&#30446;&#26631;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;ParaGuide&#21033;&#29992;&#20102;&#25913;&#20889;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#20197;&#21450;&#26469;&#33258;&#29616;&#25104;&#30340;&#20998;&#31867;&#22120;&#21644;&#24378;&#22823;&#30340;&#39118;&#26684;&#23884;&#20837;&#22120;&#30340;&#26799;&#24230;&#24341;&#23548;&#65292;&#20197;&#36716;&#21464;&#25991;&#26412;&#30340;&#39118;&#26684;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;Enron&#37038;&#20214;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;&#20154;&#24037;&#21644;&#33258;&#21160;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27491;&#24335;&#24615;&#21644;... (&#20869;&#23481;&#22826;&#22810;&#65292;&#35831;&#21442;&#32771;&#33521;&#25991;&#25688;&#35201;)
&lt;/p&gt;
&lt;p&gt;
Textual style transfer is the task of transforming stylistic properties of text while preserving meaning. Target "styles" can be defined in numerous ways, ranging from single attributes (e.g, formality) to authorship (e.g, Shakespeare). Previous unsupervised style-transfer approaches generally rely on significant amounts of labeled data for only a fixed set of styles or require large language models. In contrast, we introduce a novel diffusion-based framework for general-purpose style transfer that can be flexibly adapted to arbitrary target styles at inference time. Our parameter-efficient approach, ParaGuide, leverages paraphrase-conditioned diffusion models alongside gradient-based guidance from both off-the-shelf classifiers and strong existing style embedders to transform the style of text while preserving semantic information. We validate the method on the Enron Email Corpus, with both human and automatic evaluations, and find that it outperforms strong baselines on formality, se
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20026;&#20363;&#65292;&#26816;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#20013;&#23398;&#26415;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#65292;&#24182;&#21457;&#29616;&#20102;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20219;&#21153;&#19981;&#31526;&#21512;&#22312;&#32447;&#26381;&#21153;&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#19981;&#30495;&#23454;&#12289;&#35780;&#20272;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#31561;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.12215</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#26041;&#38754;&#30340;&#25361;&#25112;&#65306;&#19968;&#20010;&#38024;&#23545;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection. (arXiv:2308.12215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20026;&#20363;&#65292;&#26816;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#20013;&#23398;&#26415;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#65292;&#24182;&#21457;&#29616;&#20102;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20219;&#21153;&#19981;&#31526;&#21512;&#22312;&#32447;&#26381;&#21153;&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#19981;&#30495;&#23454;&#12289;&#35780;&#20272;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#31561;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#26816;&#26597;&#20102;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#19978;&#23398;&#26415;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#12290;&#25105;&#20204;&#23545;&#35813;&#39046;&#22495;&#20013;270&#31687;&#24191;&#21463;&#24341;&#29992;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#33258;&#21160;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#30340;&#25991;&#29486;&#31995;&#32479;&#21270;&#65292;&#24182;&#23545;&#23376;&#38598;&#20013;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#25968;&#25454;&#21644;&#20195;&#30721;&#30340;&#21487;&#29992;&#24615;&#12289;&#35774;&#35745;&#22833;&#35823;&#12289;&#21487;&#22797;&#29616;&#24615;&#21644;&#27867;&#21270;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#25991;&#29486;&#20013;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#36825;&#23545;&#25152;&#22768;&#31216;&#30340;&#24615;&#33021;&#21644;&#23454;&#29992;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#26816;&#27979;&#20219;&#21153;&#36890;&#24120;&#19982;&#22312;&#32447;&#26381;&#21153;&#30495;&#27491;&#38754;&#20020;&#30340;&#25361;&#25112;&#26377;&#26412;&#36136;&#19978;&#30340;&#21306;&#21035;&#12290;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#36890;&#24120;&#19981;&#20195;&#34920;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#26223;&#65292;&#32780;&#19988;&#35780;&#20272;&#24448;&#24448;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#30340;&#21487;&#29992;&#24615;&#24456;&#24046;&#12290;&#27169;&#22411;&#22312;&#39046;&#22495;&#22806;&#30340;&#25968;&#25454;&#19978;&#27867;&#21270;&#33021;&#21147;&#19981;&#24378;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. We find significant shortcomings in the literature that call into question claimed performance and practicality. Detection tasks are often meaningfully distinct from the challenges that online services actually face. Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. Data and code availability is poor. Models do not generalize well to out-of-domain data. Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. Our aim is fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;ConceptBed&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;CCD&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#27010;&#24565;&#23398;&#20064;&#21644;&#21512;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04695</link><description>&lt;p&gt;
ConceptBed: &#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#24565;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models. (arXiv:2306.04695v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;ConceptBed&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;CCD&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#27010;&#24565;&#23398;&#20064;&#21644;&#21512;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35270;&#35273;&#27010;&#24565;&#24182;&#20174;&#22270;&#20687;&#20013;&#22797;&#21046;&#21644;&#32452;&#21512;&#36825;&#20123;&#27010;&#24565;&#30340;&#33021;&#21147;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#19968;&#20010;&#26680;&#24515;&#30446;&#26631;&#12290;&#26368;&#36817;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#36890;&#36807;&#23398;&#20064;&#22823;&#37327;&#22270;&#20687;&#21450;&#20854;&#25551;&#36848;&#26469;&#29983;&#25104;&#39640;&#28165;&#26224;&#24230;&#21644;&#36924;&#30495;&#30340;&#22270;&#20687;&#36136;&#37327;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;T2I&#27169;&#22411;&#30340;&#37325;&#28857;&#22312;&#20110;&#29031;&#29255;&#33324;&#30340;&#30495;&#23454;&#24863;&#21644;&#26377;&#38480;&#30340;&#35270;&#35273;&#29702;&#35299;&#23450;&#24615;&#37327;&#24230;&#12290;&#20026;&#20102;&#37327;&#21270;T2I&#27169;&#22411;&#22312;&#23398;&#20064;&#21644;&#21512;&#25104;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ConceptBed&#65292;&#19968;&#20010;&#21253;&#21547;284&#20010;&#29420;&#29305;&#35270;&#35273;&#27010;&#24565;&#12289;5K&#20010;&#29420;&#29305;&#27010;&#24565;&#32452;&#21512;&#21644;33K&#20010;&#32452;&#21512;&#25991;&#26412;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#38500;&#20102;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#25351;&#26631;Concept Confidence Deviation&#65288;CCD&#65289;&#65292;&#23427;&#21033;&#29992;oracle&#27010;&#24565;&#20998;&#31867;&#22120;&#30340;&#32622;&#20449;&#24230;&#26469;&#34913;&#37327;T2I&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#27010;&#24565;&#19982;&#22320;&#38754;&#30495;&#23454;&#22270;&#20687;&#20013;&#21253;&#21547;&#30340;&#27010;&#24565;&#20043;&#38388;&#30340;&#23545;&#40784;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#30340;&#35270;&#35273;&#27010;&#24565;&#26159;&#23545;&#35937;&#25110;&#32773;...
&lt;/p&gt;
&lt;p&gt;
The ability to understand visual concepts and replicate and compose these concepts from images is a central goal for computer vision. Recent advances in text-to-image (T2I) models have lead to high definition and realistic image quality generation by learning from large databases of images and their descriptions. However, the evaluation of T2I models has focused on photorealism and limited qualitative measures of visual understanding. To quantify the ability of T2I models in learning and synthesizing novel visual concepts, we introduce ConceptBed, a large-scale dataset that consists of 284 unique visual concepts, 5K unique concept compositions, and 33K composite text prompts. Along with the dataset, we propose an evaluation metric, Concept Confidence Deviation (CCD), that uses the confidence of oracle concept classifiers to measure the alignment between concepts generated by T2I generators and concepts contained in ground truth images. We evaluate visual concepts that are either object
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#21453;&#20107;&#23454;&#29983;&#25104;&#22120;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#23454;&#39564;&#21457;&#29616;&#23427;&#20204;&#22312;&#21508;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#20173;&#23384;&#22312;&#33258;&#25105;&#38480;&#21046;&#21644;&#32570;&#20047;&#36923;&#36753;&#25351;&#23548;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14791</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21453;&#20107;&#23454;&#29983;&#25104;&#22120;: &#20248;&#21183;&#21644;&#21155;&#21183;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Counterfactual Generator: Strengths and Weaknesses. (arXiv:2305.14791v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#21453;&#20107;&#23454;&#29983;&#25104;&#22120;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#23454;&#39564;&#21457;&#29616;&#23427;&#20204;&#22312;&#21508;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#20173;&#23384;&#22312;&#33258;&#25105;&#38480;&#21046;&#21644;&#32570;&#20047;&#36923;&#36753;&#25351;&#23548;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#29983;&#25104;&#21453;&#20107;&#23454;&#30340;&#33021;&#21147;&#65292;&#22914;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#21453;&#20107;&#23454;&#65292;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;LLMs&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#20998;&#26512;&#24433;&#21709;&#36825;&#31181;&#33021;&#21147;&#30340;&#22240;&#32032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#24773;&#24863;&#20998;&#26512;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#31561;&#22235;&#20010;&#20219;&#21153;&#20013;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#23454;&#39564;&#26469;&#35780;&#20272;LLMs&#22312;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#34429;&#28982;LLMs&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#37117;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#25913;&#36827;&#65292;&#20294;&#30001;&#20110;&#20854;&#33258;&#25105;&#38480;&#21046;&#21644;&#32570;&#20047;&#19982;&#24120;&#35782;&#30456;&#31526;&#30340;&#36923;&#36753;&#25351;&#23548;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#23427;&#20204;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20026;LLMs&#25552;&#20379;&#20934;&#30830;&#30340;&#20219;&#21153;&#23450;&#20041;&#21644;&#35814;&#32454;&#30340;&#36880;&#27493;&#25351;&#23548;&#22312;&#29983;&#25104;&#26377;&#25928;&#30340;&#21453;&#20107;&#23454;&#20013;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable performance in a range of natural language understanding and generation tasks. Yet, their ability to generate counterfactuals, which can be used for areas like data augmentation, remains under-explored. This study aims to investigate the counterfactual generation capabilities of LLMs and analysis factors that influence this ability. First, we evaluate how effective are LLMs in counterfactual generation through data augmentation experiments for small language models (SLMs) across four tasks: sentiment analysis, natural language inference, named entity recognition, and relation extraction. While LLMs show promising enhancements in various settings, they struggle in complex tasks due to their self-limitations and the lack of logical guidance to produce counterfactuals that align with commonsense. Second, our analysis reveals the pivotal role of providing accurate task definitions and detailed step-by-step instructions to LLMs in ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#25351;&#23548;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#31216;&#20026;LGTM&#65292;&#20854;&#21033;&#29992;&#33976;&#39311;&#25928;&#24212;&#36873;&#25321;&#26679;&#26412;&#20197;&#22686;&#24378;&#23398;&#29983;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;10&#20010;&#24120;&#35265;&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09651</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#25351;&#23548;&#26377;&#21161;&#20110;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation. (arXiv:2305.09651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#25351;&#23548;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#31216;&#20026;LGTM&#65292;&#20854;&#21033;&#29992;&#33976;&#39311;&#25928;&#24212;&#36873;&#25321;&#26679;&#26412;&#20197;&#22686;&#24378;&#23398;&#29983;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;10&#20010;&#24120;&#35265;&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30740;&#31350;&#34920;&#26126;&#65292;&#33021;&#21147;&#36229;&#32676;&#30340;&#25945;&#24072;&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#35753;&#23398;&#29983;&#27700;&#24179;&#24471;&#21040;&#25552;&#21319;&#65292;&#36825;&#20984;&#26174;&#20102;&#24403;&#21069;&#25945;&#24072;&#22521;&#35757;&#23454;&#36341;&#21644;&#26377;&#25928;&#30693;&#35782;&#20256;&#25480;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#25945;&#24072;&#22521;&#35757;&#36807;&#31243;&#30340;&#25351;&#23548;&#25928;&#26524;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#33976;&#39311;&#25928;&#24212;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#23545;&#23398;&#29983;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#22909;&#25945;&#24072;&#24456;&#37325;&#35201;&#65288;LGTM&#65289;&#30340;&#26377;&#25928;&#35757;&#32451;&#25216;&#26415;&#65292;&#20197;&#23558;&#33976;&#39311;&#25928;&#24212;&#32435;&#20837;&#25945;&#24072;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#21487;&#33021;&#25552;&#21319;&#23398;&#29983;&#27867;&#21270;&#33021;&#21147;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#30340;LGTM&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;10&#20010;&#24120;&#35265;&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student's generalization ability. In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher's learning process. By prioritizing samples that are likely to enhance the student's generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#26694;&#26550;&#65288;Caro&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#25552;&#21462;&#31283;&#20581;&#30340;&#34920;&#31034;&#26102;&#21024;&#38500;&#19982;&#24847;&#22270;&#26816;&#27979;&#26080;&#20851;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290;Caro&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#20808;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03237</link><description>&lt;p&gt;
&#32771;&#34385;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#39046;&#22495;&#22806;&#24847;&#22270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-Domain Intent Detection Considering Multi-turn Dialogue Contexts. (arXiv:2305.03237v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#26694;&#26550;&#65288;Caro&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#25552;&#21462;&#31283;&#20581;&#30340;&#34920;&#31034;&#26102;&#21024;&#38500;&#19982;&#24847;&#22270;&#26816;&#27979;&#26080;&#20851;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290;Caro&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#24847;&#22270;&#26816;&#27979;&#23545;&#20110;&#23454;&#29992;&#30340;&#23545;&#35805;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#65292;&#36890;&#24120;&#38656;&#35201;&#32771;&#34385;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;&#20165;&#38480;&#20110;&#21333;&#36718;&#23545;&#35805;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#65288;Caro&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#22810;&#36718;&#19978;&#19979;&#25991;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36981;&#24490;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#20174;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#31283;&#20581;&#30340;&#34920;&#31034;&#12290;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#26500;&#24314;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#20351;&#29992;&#22810;&#35270;&#22270;&#20449;&#24687;&#29942;&#39048;&#25439;&#22833;&#21024;&#38500;&#19982;&#24847;&#22270;&#26816;&#27979;&#26080;&#20851;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22312;Caro&#20013;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#26469;&#20174;&#36825;&#20123;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#25366;&#25496;OOD&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#33258;&#20030;&#26041;&#27861;&#29992;&#36825;&#20123;OOD&#26679;&#26412;&#26469;&#35757;&#32451;&#29983;&#25104;&#30340;&#27169;&#22411;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Caro&#22312;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#30340;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#20165;&#32771;&#34385;&#21333;&#36718;&#19978;&#19979;&#25991;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-Domain (OOD) intent detection is vital for practical dialogue systems, and it usually requires considering multi-turn dialogue contexts. However, most previous OOD intent detection approaches are limited to single dialogue turns. In this paper, we introduce a context-aware OOD intent detection (Caro) framework to model multi-turn contexts in OOD intent detection tasks. Specifically, we follow the information bottleneck principle to extract robust representations from multi-turn dialogue contexts. Two different views are constructed for each input sample and the superfluous information not related to intent detection is removed using a multi-view information bottleneck loss. Moreover, we also explore utilizing unlabeled data in Caro. A two-stage training process is introduced to mine OOD samples from these unlabeled data, and these OOD samples are used to train the resulting model with a bootstrapping approach. Comprehensive experiments demonstrate that Caro establishes state-of-
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;LLaMA&#29616;&#26377;&#30340;&#35789;&#27719;&#34920;&#65292;&#22686;&#21152;&#20102;20,000&#20010;&#20013;&#25991;&#26631;&#35760;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#32534;&#30721;&#25928;&#29575;&#21644;&#23545;&#27721;&#35821;&#35821;&#20041;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#20013;&#25991;&#25968;&#25454;&#19978;&#36827;&#34892;&#20108;&#27425;&#39044;&#35757;&#32451;&#21644;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;LLaMA&#23545;&#20013;&#25991;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.08177</link><description>&lt;p&gt;
&#20013;&#25991;LLaMA&#21644;Alpaca&#30340;&#39640;&#25928;&#26377;&#25928;&#30340;&#25991;&#26412;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca. (arXiv:2304.08177v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08177
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;LLaMA&#29616;&#26377;&#30340;&#35789;&#27719;&#34920;&#65292;&#22686;&#21152;&#20102;20,000&#20010;&#20013;&#25991;&#26631;&#35760;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#32534;&#30721;&#25928;&#29575;&#21644;&#23545;&#27721;&#35821;&#35821;&#20041;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#20013;&#25991;&#25968;&#25454;&#19978;&#36827;&#34892;&#20108;&#27425;&#39044;&#35757;&#32451;&#21644;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;LLaMA&#23545;&#20013;&#25991;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#65292;&#24182;&#26174;&#31034;&#20986;&#26397;&#30528;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21644;&#37096;&#32626;LLM&#30340;&#39640;&#25104;&#26412;&#23545;&#36879;&#26126;&#12289;&#21487;&#35775;&#38382;&#30340;&#23398;&#26415;&#30740;&#31350;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;LLaMA&#29616;&#26377;&#30340;&#35789;&#27719;&#34920;&#65292;&#22686;&#21152;&#20102;20,000&#20010;&#20013;&#25991;&#26631;&#35760;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#32534;&#30721;&#25928;&#29575;&#21644;&#23545;&#27721;&#35821;&#35821;&#20041;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#20013;&#25991;&#25968;&#25454;&#19978;&#36827;&#34892;&#20108;&#27425;&#39044;&#35757;&#32451;&#21644;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#29983;&#25104;&#20013;&#25991;&#25991;&#26412;&#21450;&#20854;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically transformed natural language processing research and shown promising strides towards Artificial General Intelligence (AGI). Nonetheless, the high costs associated with training and deploying LLMs present substantial obstacles to transparent, accessible academic research. While several large language models, such as LLaMA, have been open-sourced by the community, these predominantly focus on English corpora, limiting their usefulness for other languages. In this paper, we propose a method to augment LLaMA with capabilities for understanding and generating Chinese text and its ability to follow instructions. We achieve this by extending LLaMA's existing vocabulary with an additional 20,000 Chinese tokens, thereby improving its encoding efficiency and semantic understanding of Chinese. We further incorporate secondary pre-training using Chinese data and fine-tune the model with Chinese instruction datasets, signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2303.16755</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#35268;&#27169;&#21270;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Language Models with Language Feedback at Scale. (arXiv:2303.16755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#29983;&#25104;&#19981;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#65292;&#20363;&#22914;&#26377;&#23475;&#30340;&#25991;&#26412;&#25110;&#20107;&#23454;&#19981;&#27491;&#30830;&#30340;&#25688;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#31616;&#21333;&#30340;&#20154;&#31867;&#21453;&#39304;&#24418;&#24335;&#65288;&#21363;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#20043;&#38388;&#30340;&#27604;&#36739;&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#27604;&#36739;&#21453;&#39304;&#21482;&#33021;&#20256;&#36798;&#26377;&#38480;&#30340;&#20851;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65288;ILF&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#12290;ILF&#30001;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#32452;&#25104;&#65306;&#31532;&#19968;&#27493;&#65292;&#26681;&#25454;&#36755;&#20837;&#65292;&#21021;&#22987;LM&#36755;&#20986;&#21644;&#21453;&#39304;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35843;&#33410;&#20197;&#29983;&#25104;&#25913;&#36827;&#12290;&#31532;&#20108;&#27493;&#65292;&#36873;&#25321;&#26368;&#22810;&#21453;&#39304;&#30340;&#25913;&#36827;&#12290;&#31532;&#19977;&#27493;&#65292;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#21270;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#30340;&#25913;&#36827;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;ILF&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#31867;&#20284;&#20110;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;ILF&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;ILF&#65292;&#36890;&#36807;&#20174;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#26469;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#21453;&#39304;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16749</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Code Generation by Training with Natural Language Feedback. (arXiv:2303.16749v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;ILF&#65292;&#36890;&#36807;&#20174;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#26469;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#21453;&#39304;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#28508;&#21147;&#26159;&#26368;&#36817;&#30340;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Language Feedback&#65288;ILF&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;ILF&#22312;&#35757;&#32451;&#26399;&#38388;&#20165;&#38656;&#35201;&#23569;&#37327;&#30340;&#20154;&#24037;&#32534;&#20889;&#21453;&#39304;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#26102;&#19981;&#38656;&#35201;&#30456;&#21516;&#30340;&#21453;&#39304;&#65292;&#22240;&#27492;&#20351;&#29992;&#36215;&#26469;&#26082;&#26041;&#20415;&#21448;&#39640;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;ILF&#21487;&#20197;&#34987;&#35270;&#20026;&#26368;&#23567;&#21270;&#19982;&#22522;&#20934;&#20998;&#24067;&#30340;KL&#25955;&#24230;&#30340;&#19968;&#31181;&#24418;&#24335;&#65292;&#24182;&#22312;&#31070;&#32463;&#31243;&#24207;&#21512;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;ILF&#22312;Mostly Basic Python Problems(MBPP)&#22522;&#20934;&#27979;&#35797;&#19978;&#23558;Codegen-Mono 6.1B&#27169;&#22411;&#30340;pass @ 1&#35206;&#30422;&#29575;&#30456;&#23545;&#25552;&#39640;&#20102;38%&#65288;&#32477;&#23545;&#25552;&#39640;&#20102;10%&#65289;&#65292;&#32988;&#36807;&#20102;&#22312;MBPP&#19978;&#24494;&#35843;&#21644;&#22312;&#20154;&#31867;&#20462;&#22797;&#30340;&#31243;&#24207;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#21453;&#39304;&#65292;&#20174;&#20154;&#31867;&#32534;&#20889;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#20063;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential for pre-trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development. We build upon this observation by formalizing an algorithm for learning from natural language feedback at training time instead, which we call Imitation learning from Language Feedback (ILF). ILF requires only a small amount of human-written feedback during training and does not require the same feedback at test time, making it both user-friendly and sample-efficient. We further show that ILF can be seen as a form of minimizing the KL divergence to the ground truth distribution and demonstrate a proof-of-concept on a neural program synthesis task. We use ILF to improve a Codegen-Mono 6.1B model's pass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python Problems (MBPP) benchmark, outperforming both fine-tuning on MBPP and fine-tuning on repaired programs written by humans. Overall, our results suggest that learning from h
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#30340;&#26465;&#20214;&#21477;&#24448;&#24448;&#23384;&#22312;&#30528;&#19981;&#19968;&#33268;&#24615;&#65292;&#26080;&#27861;&#20174;&#19968;&#20010;&#36830;&#36143;&#30340;&#32852;&#21512;&#20998;&#24067;&#20013;&#25512;&#23548;&#20986;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#21457;&#29616;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#26222;&#36941;&#23384;&#22312;&#20110;&#19981;&#21516;&#23610;&#23544;&#21644;&#37197;&#32622;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#21477;&#38598;&#21512;&#26041;&#27861;&#26469;&#22312;&#25512;&#26029;&#38454;&#27573;&#22788;&#29702;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.00068</link><description>&lt;p&gt;
&#20851;&#20110;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#26465;&#20214;&#21477;&#30340;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Inconsistencies of Conditionals Learned by Masked Language Models. (arXiv:2301.00068v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#30340;&#26465;&#20214;&#21477;&#24448;&#24448;&#23384;&#22312;&#30528;&#19981;&#19968;&#33268;&#24615;&#65292;&#26080;&#27861;&#20174;&#19968;&#20010;&#36830;&#36143;&#30340;&#32852;&#21512;&#20998;&#24067;&#20013;&#25512;&#23548;&#20986;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#21457;&#29616;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#26222;&#36941;&#23384;&#22312;&#20110;&#19981;&#21516;&#23610;&#23544;&#21644;&#37197;&#32622;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#21477;&#38598;&#21512;&#26041;&#27861;&#26469;&#22312;&#25512;&#26029;&#38454;&#27573;&#22788;&#29702;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#20102;&#22312;&#24207;&#21015;&#20013;&#23398;&#20064;&#39044;&#27979;&#36974;&#34109;&#26631;&#35760;&#26159;&#19968;&#20010;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#24456;&#26377;&#21147;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#35757;&#32451;&#21518;&#65292;&#36825;&#20123;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#22522;&#20110;&#21452;&#21521;&#19978;&#19979;&#25991;&#30340;&#26631;&#35760;&#20998;&#24067;&#12290;&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19982;&#24120;&#35265;&#20551;&#35774;&#30456;&#21453;&#65292;&#36825;&#31181;&#21452;&#21521;&#26465;&#20214;&#21477;&#32463;&#24120;&#34920;&#29616;&#20986;&#30456;&#24403;&#22823;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#22312;&#32771;&#34385;&#22312;&#19968;&#36215;&#26102;&#19981;&#33021;&#20174;&#19968;&#20010;&#36830;&#36143;&#30340;&#32852;&#21512;&#20998;&#24067;&#23548;&#20986;&#23427;&#20204;&#12290;&#25105;&#20204;&#22312;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#20004;&#31181;&#24120;&#35265;&#39118;&#26684;&#65288;T5&#39118;&#26684;&#21644;BERT&#39118;&#26684;&#65289;&#30340;&#31616;&#21333;&#21452;&#23383;&#27597;&#35789;&#27604;&#36739;&#22330;&#26223;&#20013;&#36890;&#36807;&#23454;&#35777;&#37327;&#21270;&#20102;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;T5&#27169;&#22411;&#32463;&#24120;&#28151;&#28102;&#33258;&#24049;&#23545;&#20004;&#20010;&#30456;&#20284;&#21452;&#23383;&#27597;&#35789;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19981;&#19968;&#33268;&#24615;&#22312;&#19981;&#21516;&#23610;&#23544;&#21644;&#37197;&#32622;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20174;RoBERTa-base&#21040;GLM-130B&#12290;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#21021;&#22987;&#23581;&#35797;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#21477;&#38598;&#21512;&#65292;&#22312;&#25512;&#26029;&#38454;&#27573;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to predict masked tokens in a sequence has been shown to be a powerful pretraining objective for large language models. After training, such masked language models can provide distributions of tokens conditioned on bidirectional context.  In this paper, we show that contrary to popular assumptions, such bidirectional conditionals often demonstrate considerable inconsistencies, i.e., they cannot be derived from a coherent joint distribution when considered together. We empirically quantify such inconsistencies in the simple scenario of bigram comparison for two common styles of masked language models: T5-style and BERT-style. For example, we show that T5 models often confuse their own preference regarding two similar bigrams. We show that inconsistencies exist ubiquitously in masked language models of diverse sizes and configurations, from RoBERTa-base to GLM-130B.  As an initial attempt to address this issue during the inference phase, we propose Ensemble of Conditionals, a se
&lt;/p&gt;</description></item></channel></rss>