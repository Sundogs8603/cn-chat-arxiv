<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#29983;&#29289;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#28789;&#27963;&#22320;&#23545;&#22810;&#27169;&#24577;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#12289;&#25972;&#21512;&#21644;&#35299;&#37322;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#35832;&#22914;&#31185;&#23398;&#21457;&#29616;&#21644;&#25252;&#29702;&#20132;&#20184;&#31561;&#37325;&#35201;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2307.14334</link><description>&lt;p&gt;
&#36808;&#21521;&#36890;&#29992;&#30340;&#29983;&#29289;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Towards Generalist Biomedical AI. (arXiv:2307.14334v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14334
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#29983;&#29289;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#28789;&#27963;&#22320;&#23545;&#22810;&#27169;&#24577;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#12289;&#25972;&#21512;&#21644;&#35299;&#37322;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#35832;&#22914;&#31185;&#23398;&#21457;&#29616;&#21644;&#25252;&#29702;&#20132;&#20184;&#31561;&#37325;&#35201;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#26412;&#36136;&#19978;&#26159;&#22810;&#27169;&#24577;&#30340;&#65292;&#20855;&#26377;&#20016;&#23500;&#30340;&#25968;&#25454;&#27169;&#24577;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#24433;&#20687;&#12289;&#22522;&#22240;&#32452;&#31561;&#12290;&#28789;&#27963;&#22320;&#23545;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#12289;&#25972;&#21512;&#21644;&#35299;&#37322;&#30340;&#36890;&#29992;&#29983;&#29289;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#23454;&#29616;&#20174;&#31185;&#23398;&#21457;&#29616;&#21040;&#25252;&#29702;&#20132;&#20184;&#30340;&#37325;&#35201;&#24212;&#29992;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#39318;&#20808;&#31574;&#21010;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#22522;&#20934;&#65292;&#31216;&#20026;MultiMedBench&#12290;MultiMedBench&#28085;&#30422;&#20102;14&#39033;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#12289;&#20083;&#33146;&#36896;&#24433;&#21644;&#30382;&#32932;&#30149;&#22270;&#20687;&#35299;&#37322;&#12289;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#21644;&#24635;&#32467;&#12289;&#22522;&#22240;&#32452;&#21464;&#24322;&#35843;&#29992;&#31561;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Med-PaLM Multimodal (Med-PaLM M)&#65292;&#36825;&#26159;&#25105;&#20204;&#36890;&#29992;&#29983;&#29289;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#27010;&#24565;&#39564;&#35777;&#12290;Med-PaLM M&#26159;&#19968;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#32534;&#30721;&#21644;&#35299;&#37322;&#21253;&#25324;&#20020;&#24202;&#35821;&#35328;&#12289;&#24433;&#20687;&#21644;&#22522;&#22240;&#32452;&#22312;&#20869;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#65292;&#20351;&#29992;&#30456;&#21516;&#30340;&#27169;&#22411;&#26435;&#37325;&#12290;Med-PaLM M&#30340;&#24615;&#33021;&#36798;&#21040;&#25110;&#36229;&#36807;&#31454;&#20105;&#23545;&#25163;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medicine is inherently multimodal, with rich data modalities spanning text, imaging, genomics, and more. Generalist biomedical artificial intelligence (AI) systems that flexibly encode, integrate, and interpret this data at scale can potentially enable impactful applications ranging from scientific discovery to care delivery. To enable the development of these models, we first curate MultiMedBench, a new multimodal biomedical benchmark. MultiMedBench encompasses 14 diverse tasks such as medical question answering, mammography and dermatology image interpretation, radiology report generation and summarization, and genomic variant calling. We then introduce Med-PaLM Multimodal (Med-PaLM M), our proof of concept for a generalist biomedical AI system. Med-PaLM M is a large multimodal generative model that flexibly encodes and interprets biomedical data including clinical language, imaging, and genomics with the same set of model weights. Med-PaLM M reaches performance competitive with or e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLMs&#20013;&#32534;&#30721;&#30340;&#36947;&#24503;&#20449;&#24565;&#36827;&#34892;&#35780;&#20272;&#30340;&#26696;&#20363;&#30740;&#31350;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#22823;&#35268;&#27169;&#35843;&#26597;&#20102;&#35299;&#19981;&#21516;LLMs&#20013;&#30340;&#36947;&#24503;&#20449;&#24565;&#65292;&#22312;&#26126;&#30830;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#20542;&#21521;&#20110;&#19982;&#20154;&#31867;&#30340;&#36947;&#24503;&#30452;&#35273;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#22238;&#31572;&#20250;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14324</link><description>&lt;p&gt;
&#35780;&#20272;LLMs&#20013;&#32534;&#30721;&#30340;&#36947;&#24503;&#20449;&#24565;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Moral Beliefs Encoded in LLMs. (arXiv:2307.14324v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLMs&#20013;&#32534;&#30721;&#30340;&#36947;&#24503;&#20449;&#24565;&#36827;&#34892;&#35780;&#20272;&#30340;&#26696;&#20363;&#30740;&#31350;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#22823;&#35268;&#27169;&#35843;&#26597;&#20102;&#35299;&#19981;&#21516;LLMs&#20013;&#30340;&#36947;&#24503;&#20449;&#24565;&#65292;&#22312;&#26126;&#30830;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#20542;&#21521;&#20110;&#19982;&#20154;&#31867;&#30340;&#36947;&#24503;&#30452;&#35273;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#22238;&#31572;&#20250;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#35774;&#35745;&#12289;&#31649;&#29702;&#12289;&#21518;&#22788;&#29702;&#21644;&#35780;&#20272;&#35843;&#26597;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#23427;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;(1) &#19968;&#31181;&#29992;&#20110;&#33719;&#21462;LLMs&#20013;&#32534;&#30721;&#30340;&#20449;&#24565;&#30340;&#32479;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#32479;&#35745;&#37327;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#37327;&#21270;LLM&#8220;&#20570;&#20986;&#36873;&#25321;&#8221;&#30340;&#27010;&#29575;&#12289;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#36873;&#25321;&#30340;&#19968;&#33268;&#24615;&#12290;(b) &#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#30740;&#31350;&#19981;&#21516;LLMs&#20013;&#32534;&#30721;&#30340;&#36947;&#24503;&#20449;&#24565;&#65292;&#29305;&#21035;&#26159;&#22312;&#27491;&#30830;&#36873;&#25321;&#19981;&#26126;&#26174;&#30340;&#27169;&#31946;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35843;&#26597;&#65292;&#20854;&#20013;&#21253;&#25324;680&#20010;&#39640;&#27169;&#31946;&#24230;&#30340;&#36947;&#24503;&#22330;&#26223;&#65288;&#20363;&#22914;&#65292;&#8220;&#25105;&#24212;&#35813;&#25746;&#19968;&#20010;&#21892;&#24847;&#30340;&#35854;&#35328;&#21527;&#65311;&#8221;&#65289;&#21644;687&#20010;&#20302;&#27169;&#31946;&#24230;&#30340;&#36947;&#24503;&#22330;&#26223;&#65288;&#20363;&#22914;&#65292;&#8220;&#25105;&#24212;&#35813;&#20026;&#36335;&#19978;&#30340;&#34892;&#20154;&#20572;&#19979;&#26469;&#21527;&#65311;&#8221;&#65289;&#12290;&#27599;&#20010;&#22330;&#26223;&#21253;&#25324;&#19968;&#20010;&#25551;&#36848;&#12289;&#20004;&#20010;&#21487;&#33021;&#30340;&#34892;&#21160;&#20197;&#21450;&#25351;&#31034;&#36829;&#21453;&#35268;&#21017;&#30340;&#36741;&#21161;&#26631;&#31614;&#65288;&#20363;&#22914;&#65292;&#8220;&#19981;&#35201;&#26432;&#20154;&#8221;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#35843;&#26597;&#24212;&#29992;&#20110;28&#20010;&#24320;&#28304;&#21644;&#38381;&#28304;&#30340;LLMs&#12290;&#25105;&#20204;&#21457;&#29616;(b) &#22312;&#26126;&#30830;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs tend to align with human moral intuitions, but in ambiguous scenarios, their responses vary and may exhibit biases and inconsistencies.
&lt;/p&gt;
&lt;p&gt;
This paper presents a case study on the design, administration, post-processing, and evaluation of surveys on large language models (LLMs). It comprises two components: (1) A statistical method for eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that quantify the probability of an LLM "making a choice", the associated uncertainty, and the consistency of that choice. (2) We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious. We design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g., "Should I tell a white lie?") and 687 low-ambiguity moral scenarios (e.g., "Should I stop for a pedestrian on the road?"). Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g., "do not kill"). We administer the survey to 28 open- and closed-source LLMs. We find that (a) in unambiguous scen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#24773;&#24863;&#20998;&#26512;&#24211;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#24773;&#24863;&#20998;&#26512;&#21644;&#25152;&#20351;&#29992;&#30340;&#24211;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#21512;&#20316;&#26041;&#27861;&#26469;&#20998;&#31867;&#24773;&#32490;&#26497;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14311</link><description>&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#24211;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of Libraries for the Sentimental Analysis. (arXiv:2307.14311v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#24773;&#24863;&#20998;&#26512;&#24211;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#24773;&#24863;&#20998;&#26512;&#21644;&#25152;&#20351;&#29992;&#30340;&#24211;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#21512;&#20316;&#26041;&#27861;&#26469;&#20998;&#31867;&#24773;&#32490;&#26497;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#24773;&#24863;&#20998;&#26512;&#24211;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#19987;&#23478;&#23545;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#65292;&#30446;&#30340;&#26159;&#35782;&#21035;&#21644;&#20998;&#31867;&#19982;Twitter&#29992;&#25143;&#35328;&#35770;&#30456;&#20851;&#30340;&#24773;&#24863;&#12290;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#24773;&#24863;&#20998;&#26512;&#21644;&#25152;&#20351;&#29992;&#30340;&#24211;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#21512;&#20316;&#26041;&#27861;&#26469;&#20998;&#31867;&#24773;&#32490;&#26497;&#24615;&#12290;&#26681;&#25454;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#12289;&#20915;&#31574;&#26641;&#20998;&#31867;&#22120;&#12289;&#26368;&#22823;&#29109;&#20998;&#31867;&#22120;&#12289;Sklearn&#20998;&#31867;&#22120;&#12289;Sklearn&#20998;&#31867;&#22120;MultinomialNB&#31561;&#20849;&#21516;&#23398;&#20064;&#31639;&#27861;&#38750;&#24120;&#26377;&#25928;&#12290;&#22312;&#26412;&#39033;&#30446;&#20013;&#65292;&#23558;&#20351;&#29992;&#20116;&#20010;Python&#21644;R&#24211;NLTK&#12289;TextBlob&#12289;Vader&#12289;Transformers&#65288;GPT&#21644;BERT&#39044;&#35757;&#32451;&#65289;&#20197;&#21450;Tidytext&#26469;&#24212;&#29992;&#24773;&#24863;&#20998;&#26512;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study is main goal is to provide a comparative comparison of libraries using machine learning methods. Experts in natural language processing (NLP) are becoming more and more interested in sentiment analysis (SA) of text changes. The objective of employing NLP text analysis techniques is to recognize and categorize feelings related to twitter users utterances. In this examination, issues with SA and the libraries utilized are also looked at. provides a number of cooperative methods to classify emotional polarity. The Naive Bayes Classifier, Decision Tree Classifier, Maxent Classifier, Sklearn Classifier, Sklearn Classifier MultinomialNB, and other conjoint learning algorithms, according to recent research, are very effective. In the project will use Five Python and R libraries NLTK, TextBlob, Vader, Transformers (GPT and BERT pretrained), and Tidytext will be used in the study to apply sentiment analysis techniques. Four machine learning models Tree of Decisions (DT), Support Vect
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#24847;&#35265;&#24635;&#32467;&#30340;&#24847;&#35265;&#26222;&#36941;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#19982;&#25688;&#35201;&#38472;&#36848;&#19968;&#33268;&#30340;&#35780;&#35770;&#25968;&#37327;&#65292;&#24182;&#25490;&#38500;&#29712;&#30862;&#25110;&#20887;&#20313;&#30340;&#38472;&#36848;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20154;&#31867;&#32534;&#20889;&#30340;&#25688;&#35201;&#22312;&#24847;&#35265;&#26222;&#36941;&#24615;&#26041;&#38754;&#20165;&#31245;&#24494;&#20248;&#20110;&#20174;&#28304;&#35780;&#35770;&#20013;&#38543;&#26426;&#36873;&#25321;&#30340;&#25688;&#24405;&#12290;</title><link>http://arxiv.org/abs/2307.14305</link><description>&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#24847;&#35265;&#24635;&#32467;&#20013;&#30340;&#24847;&#35265;&#26222;&#36941;&#24615;
&lt;/p&gt;
&lt;p&gt;
Automatically Evaluating Opinion Prevalence in Opinion Summarization. (arXiv:2307.14305v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#24847;&#35265;&#24635;&#32467;&#30340;&#24847;&#35265;&#26222;&#36941;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#19982;&#25688;&#35201;&#38472;&#36848;&#19968;&#33268;&#30340;&#35780;&#35770;&#25968;&#37327;&#65292;&#24182;&#25490;&#38500;&#29712;&#30862;&#25110;&#20887;&#20313;&#30340;&#38472;&#36848;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20154;&#31867;&#32534;&#20889;&#30340;&#25688;&#35201;&#22312;&#24847;&#35265;&#26222;&#36941;&#24615;&#26041;&#38754;&#20165;&#31245;&#24494;&#20248;&#20110;&#20174;&#28304;&#35780;&#35770;&#20013;&#38543;&#26426;&#36873;&#25321;&#30340;&#25688;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#38754;&#23545;&#22823;&#37327;&#20135;&#21697;&#35780;&#35770;&#26102;&#65292;&#19981;&#28165;&#26970;&#20154;&#31867;&#26159;&#21542;&#33021;&#35760;&#20303;&#25152;&#26377;&#35780;&#35770;&#24182;&#20197;&#20195;&#34920;&#24615;&#26435;&#37325;&#32534;&#20889;&#22909;&#30340;&#21442;&#32771;&#25688;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#26469;&#27979;&#35797;&#25688;&#35201;&#25152;&#34920;&#36798;&#30340;&#24847;&#35265;&#26222;&#36941;&#24615;&#65292;&#35813;&#26631;&#20934;&#22522;&#20110;&#32479;&#35745;&#19982;&#25688;&#35201;&#20013;&#27599;&#20010;&#38472;&#36848;&#19968;&#33268;&#30340;&#35780;&#35770;&#25968;&#37327;&#65292;&#21516;&#26102;&#36140;&#20302;&#29712;&#30862;&#25110;&#20887;&#20313;&#30340;&#38472;&#36848;&#12290;&#20026;&#20102;&#21046;&#23450;&#36825;&#31181;&#24847;&#35265;&#26222;&#36941;&#24615;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#29616;&#26377;&#26041;&#27861;&#26469;&#35780;&#20998;&#25688;&#35201;&#38472;&#36848;&#30456;&#23545;&#20110;&#27599;&#20010;&#20010;&#20307;&#28304;&#35780;&#35770;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#22312;&#20122;&#39532;&#36874;&#20135;&#21697;&#35780;&#35770;&#35821;&#26009;&#24211;&#19978;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#22810;&#20010;&#20154;&#23545;&#24847;&#35265;&#19968;&#33268;&#24615;&#30340;&#35780;&#21028;&#65292;&#20197;&#30830;&#23450;&#21738;&#31181;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#26368;&#33021;&#34920;&#36798;&#20135;&#21697;&#35780;&#35770;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#24471;&#21040;&#30340;&#24847;&#35265;&#26222;&#36941;&#24615;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20154;&#31867;&#32534;&#20889;&#30340;&#25688;&#35201;&#19982;&#20174;&#28304;&#35780;&#35770;&#20013;&#38543;&#26426;&#36873;&#25321;&#30340;&#25688;&#24405;&#30456;&#27604;&#21482;&#26377;&#31245;&#24494;&#26356;&#22909;&#30340;&#24847;&#35265;&#26222;&#36941;&#24615;&#65292;&#20197;&#21450;&#20197;&#21069;&#30340;&#25688;&#24405;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When faced with a large number of product reviews, it is not clear that a human can remember all of them and weight opinions representatively to write a good reference summary. We propose an automatic metric to test the prevalence of the opinions that a summary expresses, based on counting the number of reviews that are consistent with each statement in the summary, while discrediting trivial or redundant statements. To formulate this opinion prevalence metric, we consider several existing methods to score the factual consistency of a summary statement with respect to each individual source review. On a corpus of Amazon product reviews, we gather multiple human judgments of the opinion consistency, to determine which automatic metric best expresses consistency in product reviews. Using the resulting opinion prevalence metric, we show that a human authored summary has only slightly better opinion prevalence than randomly selected extracts from the source reviews, and previous extractive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;ChatGPT&#21644;&#35828;&#26381;&#25216;&#26415;&#24212;&#29992;&#20110;&#37202;&#24215;&#25512;&#33616;&#31995;&#32479;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;ChatGPT&#21487;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#65292;&#32780;&#35828;&#26381;&#25216;&#26415;&#21487;&#24433;&#21709;&#29992;&#25143;&#34892;&#20026;&#24182;&#22686;&#24378;&#25512;&#33616;&#30340;&#35828;&#26381;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.14298</link><description>&lt;p&gt;
ChatGPT&#21644;&#35828;&#26381;&#25216;&#26415;&#22312;&#37202;&#24215;&#26381;&#21153;&#39046;&#22495;&#20010;&#24615;&#21270;&#25512;&#33616;&#31649;&#29702;&#21644;&#25552;&#20379;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and Persuasive Technologies for the Management and Delivery of Personalized Recommendations in Hotel Hospitality. (arXiv:2307.14298v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;ChatGPT&#21644;&#35828;&#26381;&#25216;&#26415;&#24212;&#29992;&#20110;&#37202;&#24215;&#25512;&#33616;&#31995;&#32479;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;ChatGPT&#21487;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#65292;&#32780;&#35828;&#26381;&#25216;&#26415;&#21487;&#24433;&#21709;&#29992;&#25143;&#34892;&#20026;&#24182;&#22686;&#24378;&#25512;&#33616;&#30340;&#35828;&#26381;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#37202;&#24215;&#26381;&#21153;&#19994;&#24050;&#25104;&#20026;&#19981;&#21487;&#25110;&#32570;&#30340;&#24037;&#20855;&#65292;&#20026;&#23458;&#20154;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#23450;&#21046;&#21270;&#30340;&#20307;&#39564;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#21644;&#35828;&#26381;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#20026;&#25552;&#21319;&#36825;&#20123;&#31995;&#32479;&#30340;&#25928;&#26524;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;ChatGPT&#21644;&#35828;&#26381;&#25216;&#26415;&#25972;&#21512;&#21040;&#37202;&#24215;&#26381;&#21153;&#25512;&#33616;&#31995;&#32479;&#20013;&#33258;&#21160;&#21270;&#21644;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;ChatGPT&#30340;&#33021;&#21147;&#65292;&#23427;&#21487;&#20197;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#25991;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;ChatGPT&#25972;&#21512;&#21040;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#33021;&#21147;&#65292;&#31361;&#20986;&#20102;&#20854;&#20998;&#26512;&#29992;&#25143;&#20559;&#22909;&#12289;&#20174;&#22312;&#32447;&#35780;&#35770;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#65292;&#24182;&#26681;&#25454;&#23458;&#20154;&#37197;&#32622;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35828;&#26381;&#25216;&#26415;&#22312;&#24433;&#21709;&#29992;&#25143;&#34892;&#20026;&#21644;&#25552;&#21319;&#37202;&#24215;&#25512;&#33616;&#30340;&#35828;&#26381;&#25928;&#26524;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems have become indispensable tools in the hotel hospitality industry, enabling personalized and tailored experiences for guests. Recent advancements in large language models (LLMs), such as ChatGPT, and persuasive technologies, have opened new avenues for enhancing the effectiveness of those systems. This paper explores the potential of integrating ChatGPT and persuasive technologies for automating and improving hotel hospitality recommender systems. First, we delve into the capabilities of ChatGPT, which can understand and generate human-like text, enabling more accurate and context-aware recommendations. We discuss the integration of ChatGPT into recommender systems, highlighting the ability to analyze user preferences, extract valuable insights from online reviews, and generate personalized recommendations based on guest profiles. Second, we investigate the role of persuasive technology in influencing user behavior and enhancing the persuasive impact of hotel recomm
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#24503;&#35821;&#21477;&#27861;&#29305;&#24449;&#22312;&#21271;&#19996;&#24847;&#22823;&#21033;&#26041;&#35328;&#20013;&#30340;&#20256;&#25773;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#23398;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22320;&#29702;&#25968;&#25454;&#31185;&#23398;&#24037;&#20855;&#29983;&#25104;&#20102;&#20132;&#20114;&#24335;&#22320;&#22270;&#12290;</title><link>http://arxiv.org/abs/2307.14291</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#23398;&#20013;&#21019;&#31435;&#19968;&#20010;&#25968;&#23398;&#25193;&#25955;&#27169;&#22411;&#12290;&#21271;&#19996;&#24847;&#22823;&#21033;&#26041;&#35328;&#20013;&#24503;&#35821;&#21477;&#27861;&#29305;&#24449;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Founding a mathematical diffusion model in linguistics. The case study of German syntactic features in the North-Eastern Italian dialects. (arXiv:2307.14291v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14291
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#24503;&#35821;&#21477;&#27861;&#29305;&#24449;&#22312;&#21271;&#19996;&#24847;&#22823;&#21033;&#26041;&#35328;&#20013;&#30340;&#20256;&#25773;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#23398;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22320;&#29702;&#25968;&#25454;&#31185;&#23398;&#24037;&#20855;&#29983;&#25104;&#20102;&#20132;&#20114;&#24335;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20197;&#21271;&#19996;&#24847;&#22823;&#21033;&#32599;&#26364;&#26041;&#35328;&#20013;&#24503;&#35821;&#21477;&#27861;&#29305;&#24449;&#30340;&#20256;&#25773;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#29305;&#24449;&#22312;&#20013;&#19990;&#32426;&#39640;&#20013;&#19990;&#32426;&#26102;&#26399;&#30340;&#33922;&#32599;&#23572;&#24503;&#22269;&#20154;&#31227;&#27665;&#21518;&#21457;&#29983;&#12290;&#20351;&#29992;&#22320;&#29702;&#25968;&#25454;&#31185;&#23398;&#24037;&#20855;&#32472;&#21046;&#20986;&#19968;&#20010;&#20132;&#20114;&#24335;&#22320;&#22270;&#12290;&#19968;&#20010;&#24179;&#28369;&#30340;&#20108;&#32500;&#26354;&#38754;$\mathcal{G}$&#34920;&#31034;&#24403;&#22320;&#20351;&#29992;&#32473;&#23450;&#24503;&#35821;&#35821;&#35328;&#29305;&#24449;&#30340;&#39046;&#22303;&#27604;&#20363;&#65306;&#36890;&#36807;&#23545;&#34920;&#31034;&#35813;&#29305;&#24449;&#22312;&#20219;&#20309;&#35843;&#26597;&#22320;&#28857;&#26159;&#21542;&#20351;&#29992;&#30340;&#31163;&#25955;&#20989;&#25968;&#36827;&#34892;&#25554;&#20540;&#24471;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We take as a case study the spread of Germanic syntactic features into Romance dialects of North-Eastern Italy, which occurred after the immigration of German people in the Tyrol during the High Middle Ages.  An interactive map is produced using tools of what is called Geographic Data Science. A smooth two-dimensional surface $\mathcal{G}$ expresses locally which fraction of territory uses a given German language feature: it is obtained by interpolating a discrete function that says if at any surveyed locality that feature is used or not.\newline  This surface $\mathcal{G}$ is thought of as the value at the present time of a function describing a diffusion-convection phenomenon in two dimensions (here said \emph{tidal} mode), which is subjected in a very natural way to the same equation, suitably contextualized, used in physics for a number of phenomenological facts like the heat diffusion. It is shown that solutions of this equation, evaluated at the present time, fit well with the da
&lt;/p&gt;</description></item><item><title>UnScientify&#26159;&#19968;&#20010;&#20132;&#20114;&#31995;&#32479;&#65292;&#21487;&#20197;&#26816;&#27979;&#23398;&#26415;&#20840;&#25991;&#20013;&#30340;&#31185;&#23398;&#19981;&#30830;&#23450;&#24615;&#12290;&#23427;&#21033;&#29992;&#24369;&#30417;&#30563;&#25216;&#26415;&#21644;&#32454;&#31890;&#24230;&#27880;&#37322;&#26041;&#26696;&#65292;&#33258;&#21160;&#26631;&#35760;&#21644;&#27880;&#37322;&#31185;&#23398;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.14236</link><description>&lt;p&gt;
UnScientify: &#26816;&#27979;&#23398;&#26415;&#20840;&#25991;&#20013;&#30340;&#31185;&#23398;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
UnScientify: Detecting Scientific Uncertainty in Scholarly Full Text. (arXiv:2307.14236v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14236
&lt;/p&gt;
&lt;p&gt;
UnScientify&#26159;&#19968;&#20010;&#20132;&#20114;&#31995;&#32479;&#65292;&#21487;&#20197;&#26816;&#27979;&#23398;&#26415;&#20840;&#25991;&#20013;&#30340;&#31185;&#23398;&#19981;&#30830;&#23450;&#24615;&#12290;&#23427;&#21033;&#29992;&#24369;&#30417;&#30563;&#25216;&#26415;&#21644;&#32454;&#31890;&#24230;&#27880;&#37322;&#26041;&#26696;&#65292;&#33258;&#21160;&#26631;&#35760;&#21644;&#27880;&#37322;&#31185;&#23398;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#28436;&#31034;&#35770;&#25991;&#20171;&#32461;&#20102;UnScientify&#65292;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#23398;&#26415;&#20840;&#25991;&#20013;&#31185;&#23398;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#20114;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#19968;&#31181;&#24369;&#30417;&#30563;&#25216;&#26415;&#65292;&#37319;&#29992;&#32454;&#31890;&#24230;&#27880;&#37322;&#26041;&#26696;&#65292;&#20174;&#21477;&#23376;&#32423;&#21035;&#19978;&#35782;&#21035;&#31185;&#23398;&#25991;&#26412;&#20013;&#21475;&#22836;&#34920;&#36848;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#31995;&#32479;&#30340;&#27969;&#31243;&#21253;&#25324;&#27169;&#24335;&#21305;&#37197;&#12289;&#22797;&#26434;&#21477;&#23376;&#26816;&#26597;&#21644;&#20316;&#32773;&#21442;&#32771;&#26816;&#26597;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#21160;&#26631;&#35760;&#21644;&#27880;&#37322;&#31185;&#23398;&#19981;&#30830;&#23450;&#24615;&#35782;&#21035;&#20219;&#21153;&#65292;&#32771;&#34385;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#31185;&#23398;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#12289;&#25991;&#26412;&#25366;&#25496;&#21644;&#23398;&#26415;&#25991;&#29486;&#22788;&#29702;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;UnScientify&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#25991;&#26412;&#20013;&#35782;&#21035;&#20986;&#30340;&#31185;&#23398;&#19981;&#30830;&#23450;&#24615;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This demo paper presents UnScientify, an interactive system designed to detect scientific uncertainty in scholarly full text. The system utilizes a weakly supervised technique that employs a fine-grained annotation scheme to identify verbally formulated uncertainty at the sentence level in scientific texts. The pipeline for the system includes a combination of pattern matching, complex sentence checking, and authorial reference checking. Our approach automates labeling and annotation tasks for scientific uncertainty identification, taking into account different types of scientific uncertainty, that can serve various applications such as information retrieval, text mining, and scholarly document processing. Additionally, UnScientify provides interpretable results, aiding in the comprehension of identified instances of scientific uncertainty in text.
&lt;/p&gt;</description></item><item><title>LOIS&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#35821;&#20041;&#29702;&#35299;&#38382;&#39064;&#12290;&#23427;&#19981;&#20381;&#36182;&#20110;&#36793;&#30028;&#26694;&#65292;&#24182;&#20351;&#29992;&#31934;&#32454;&#30340;&#29305;&#24449;&#25551;&#36848;&#26469;&#29983;&#25104;&#35270;&#35273;&#20107;&#23454;&#12290;&#21478;&#22806;&#65292;LOIS&#36890;&#36807;&#20004;&#31181;&#31867;&#22411;&#30340;&#20851;&#31995;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#35299;&#20915;&#30001;&#23454;&#20363;&#25513;&#30721;&#24341;&#36215;&#30340;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14142</link><description>&lt;p&gt;
LOIS: &#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#35266;&#23519;&#23454;&#20363;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
LOIS: Looking Out of Instance Semantics for Visual Question Answering. (arXiv:2307.14142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14142
&lt;/p&gt;
&lt;p&gt;
LOIS&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#35821;&#20041;&#29702;&#35299;&#38382;&#39064;&#12290;&#23427;&#19981;&#20381;&#36182;&#20110;&#36793;&#30028;&#26694;&#65292;&#24182;&#20351;&#29992;&#31934;&#32454;&#30340;&#29305;&#24449;&#25551;&#36848;&#26469;&#29983;&#25104;&#35270;&#35273;&#20107;&#23454;&#12290;&#21478;&#22806;&#65292;LOIS&#36890;&#36807;&#20004;&#31181;&#31867;&#22411;&#30340;&#20851;&#31995;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#35299;&#20915;&#30001;&#23454;&#20363;&#25513;&#30721;&#24341;&#36215;&#30340;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;(VQA)&#20316;&#20026;&#19968;&#39033;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#38656;&#35201;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#21162;&#21147;&#65292;&#20197;&#27491;&#30830;&#22320;&#25512;&#26029;&#31572;&#26696;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#24320;&#21457;&#20102;&#21508;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22359;&#26469;&#35299;&#20915;VQA&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#25512;&#26029;&#30340;&#24615;&#33021;&#20027;&#35201;&#21463;&#38480;&#20110;&#23545;&#35821;&#20041;&#29702;&#35299;&#30340;&#35270;&#35273;&#22788;&#29702;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#36793;&#30028;&#26694;&#65292;&#36825;&#23545;&#20110;VQA&#27169;&#22411;&#29702;&#35299;&#22270;&#20687;&#20013;&#29289;&#20307;&#35821;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#24182;&#27491;&#30830;&#25512;&#26029;&#19978;&#19979;&#25991;&#20449;&#24687;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#31934;&#32454;&#30340;&#27169;&#22411;&#26694;&#26550;&#65292;&#21517;&#20026;Looking Out of Instance Semantics (LOIS)&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;LOIS&#33021;&#22815;&#29983;&#25104;&#26356;&#31934;&#32454;&#30340;&#29305;&#24449;&#25551;&#36848;&#65292;&#20197;&#20135;&#29983;&#35270;&#35273;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20811;&#26381;&#23454;&#20363;&#25513;&#30721;&#24341;&#36215;&#30340;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#20851;&#31995;&#27880;&#24847;&#21147;&#27169;&#22359;&#65306;1) &#20869;&#37096;&#27169;&#24577;&#21644;2) &#20132;&#20114;&#27169;&#24577;&#65292;&#29992;&#20110;&#25512;&#26029;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) has been intensively studied as a multimodal task that requires effort in bridging vision and language to infer answers correctly. Recent attempts have developed various attention-based modules for solving VQA tasks. However, the performance of model inference is largely bottlenecked by visual processing for semantics understanding. Most existing detection methods rely on bounding boxes, remaining a serious challenge for VQA models to understand the causal nexus of object semantics in images and correctly infer contextual information. To this end, we propose a finer model framework without bounding boxes in this work, termed Looking Out of Instance Semantics (LOIS) to tackle this important issue. LOIS enables more fine-grained feature descriptions to produce visual facts. Furthermore, to overcome the label ambiguity caused by instance masks, two types of relation attention modules: 1) intra-modality and 2) inter-modality, are devised to infer the correct
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#23567;&#22411;&#21040;&#20013;&#22411;&#30340;&#22303;&#32819;&#20854;BERT&#27169;&#22411;&#65292;&#22312;&#22635;&#34917;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#26041;&#38754;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#25104;&#26524;&#65292;&#24182;&#20026;&#24320;&#21457;&#21644;&#24212;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.14134</link><description>&lt;p&gt;
&#24320;&#21457;&#21644;&#35780;&#20272;&#23567;&#22411;&#21040;&#20013;&#22411;&#30340;&#22303;&#32819;&#20854;BERT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models. (arXiv:2307.14134v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#23567;&#22411;&#21040;&#20013;&#22411;&#30340;&#22303;&#32819;&#20854;BERT&#27169;&#22411;&#65292;&#22312;&#22635;&#34917;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#26041;&#38754;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#25104;&#26524;&#65292;&#24182;&#20026;&#24320;&#21457;&#21644;&#24212;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#23567;&#22411;&#12289;&#36855;&#20320;&#22411;&#12289;&#23567;&#22411;&#21644;&#20013;&#22411;&#30340;&#26080;&#22823;&#23567;&#20889;&#30340;&#22303;&#32819;&#20854;BERT&#27169;&#22411;&#65292;&#26088;&#22312;&#22635;&#34917;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#26469;&#28304;&#30340;75GB&#20197;&#19978;&#25991;&#26412;&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;&#25513;&#30721;&#39044;&#27979;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#26032;&#38395;&#20998;&#31867;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#12290;&#23613;&#31649;&#35268;&#27169;&#36739;&#23567;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#26356;&#24555;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20026;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#29305;&#21035;&#26159;&#22312;&#22303;&#32819;&#20854;&#35821;&#22659;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces and evaluates tiny, mini, small, and medium-sized uncased Turkish BERT models, aiming to bridge the research gap in less-resourced languages. We trained these models on a diverse dataset encompassing over 75GB of text from multiple sources and tested them on several tasks, including mask prediction, sentiment analysis, news classification, and, zero-shot classification. Despite their smaller size, our models exhibited robust performance, including zero-shot task, while ensuring computational efficiency and faster execution times. Our findings provide valuable insights into the development and application of smaller language models, especially in the context of the Turkish language.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CIF-Transducer&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#23558;&#36830;&#32493;&#31215;&#20998;&#21644;&#28779;&#26426;&#21046;&#19982;RNN-T&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#24182;&#25918;&#24323;&#20102;RNN-T Loss&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#65292;&#24182;&#20351;&#39044;&#27979;&#32593;&#32476;&#21457;&#25381;&#26356;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;CIF-T&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.14132</link><description>&lt;p&gt;
&#21578;&#21035;RNN-T Loss&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;CIF&#30340;&#36716;&#24405;&#22120;&#26550;&#26500;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Say Goodbye to RNN-T Loss: A Novel CIF-based Transducer Architecture for Automatic Speech Recognition. (arXiv:2307.14132v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CIF-Transducer&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#23558;&#36830;&#32493;&#31215;&#20998;&#21644;&#28779;&#26426;&#21046;&#19982;RNN-T&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#24182;&#25918;&#24323;&#20102;RNN-T Loss&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#65292;&#24182;&#20351;&#39044;&#27979;&#32593;&#32476;&#21457;&#25381;&#26356;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;CIF-T&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RNN-T&#27169;&#22411;&#22312;ASR&#20013;&#24191;&#27867;&#20351;&#29992;&#65292;&#20381;&#38752;RNN-T Loss&#23454;&#29616;&#36755;&#20837;&#38899;&#39057;&#21644;&#30446;&#26631;&#24207;&#21015;&#30340;&#38271;&#24230;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;RNN-T Loss&#30340;&#23454;&#29616;&#22797;&#26434;&#24615;&#21644;&#22522;&#20110;&#23545;&#40784;&#30340;&#20248;&#21270;&#30446;&#26631;&#23548;&#33268;&#35745;&#31639;&#20887;&#20313;&#21644;&#39044;&#27979;&#32593;&#32476;&#35282;&#33394;&#30340;&#20943;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CIF-Transducer&#65288;CIF-T&#65289;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#23427;&#23558;&#36830;&#32493;&#31215;&#20998;&#21644;&#28779;&#65288;CIF&#65289;&#26426;&#21046;&#19982;RNN-T&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#23545;&#40784;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25918;&#24323;&#20102;RNN-T Loss&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#65292;&#24182;&#20351;&#39044;&#27979;&#32593;&#32476;&#21457;&#25381;&#26356;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Funnel-CIF&#12289;Context Blocks&#12289;Unified Gating&#21644;Bilinear Pooling&#32852;&#21512;&#32593;&#32476;&#20197;&#21450;&#36741;&#21161;&#35757;&#32451;&#31574;&#30053;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;178&#23567;&#26102;&#30340;AISHELL-1&#21644;10000&#23567;&#26102;&#30340;WenetSpeech&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;RNN-T&#27169;&#22411;&#30456;&#27604;&#65292;CIF-T&#20197;&#26356;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
RNN-T models are widely used in ASR, which rely on the RNN-T loss to achieve length alignment between input audio and target sequence. However, the implementation complexity and the alignment-based optimization target of RNN-T loss lead to computational redundancy and a reduced role for predictor network, respectively. In this paper, we propose a novel model named CIF-Transducer (CIF-T) which incorporates the Continuous Integrate-and-Fire (CIF) mechanism with the RNN-T model to achieve efficient alignment. In this way, the RNN-T loss is abandoned, thus bringing a computational reduction and allowing the predictor network a more significant role. We also introduce Funnel-CIF, Context Blocks, Unified Gating and Bilinear Pooling joint network, and auxiliary training strategy to further improve performance. Experiments on the 178-hour AISHELL-1 and 10000-hour WenetSpeech datasets show that CIF-T achieves state-of-the-art results with lower computational overhead compared to RNN-T models.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21033;&#29992;&#23545;&#35805;&#20013;&#30340;&#38544;&#24335;&#21453;&#39304;&#26469;&#25913;&#36827;&#31038;&#20132;&#23545;&#35805;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36890;&#36807;&#25910;&#38598;&#29992;&#25143;&#21709;&#24212;&#38271;&#24230;&#12289;&#24773;&#24863;&#21644;&#21453;&#24212;&#31561;&#20449;&#21495;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#29983;&#25104;&#35805;&#35821;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.14117</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#20013;&#21033;&#29992;&#26469;&#33258;&#37096;&#32626;&#25968;&#25454;&#30340;&#38544;&#24335;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Leveraging Implicit Feedback from Deployment Data in Dialogue. (arXiv:2307.14117v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14117
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21033;&#29992;&#23545;&#35805;&#20013;&#30340;&#38544;&#24335;&#21453;&#39304;&#26469;&#25913;&#36827;&#31038;&#20132;&#23545;&#35805;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36890;&#36807;&#25910;&#38598;&#29992;&#25143;&#21709;&#24212;&#38271;&#24230;&#12289;&#24773;&#24863;&#21644;&#21453;&#24212;&#31561;&#20449;&#21495;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#29983;&#25104;&#35805;&#35821;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#36890;&#36807;&#23398;&#20064;&#29992;&#25143;&#19982;&#37096;&#32626;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#28982;&#23545;&#35805;&#26469;&#25913;&#36827;&#31038;&#20132;&#23545;&#35805;&#31995;&#32479;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#27880;&#37322;&#12290;&#20026;&#20102;&#38544;&#24335;&#34913;&#37327;&#26426;&#22120;&#29983;&#25104;&#35805;&#35821;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#21033;&#29992;&#25910;&#38598;&#23545;&#35805;&#20013;&#26410;&#26469;&#20154;&#31867;&#35805;&#35821;&#30340;&#29992;&#25143;&#21709;&#24212;&#38271;&#24230;&#12289;&#24773;&#24863;&#21644;&#21453;&#24212;&#31561;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#20102;BlenderBot&#65288;Xu&#31561;&#65292;2023&#24180;&#65289;&#20844;&#24320;&#21457;&#24067;&#30340;&#37096;&#32626;&#25968;&#25454;&#12290;&#20154;&#31867;&#35780;&#20272;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#26032;&#27169;&#22411;&#27604;&#22522;&#32447;&#22238;&#22797;&#26377;&#25152;&#25913;&#36827;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#20195;&#29702;&#20449;&#21495;&#20063;&#21487;&#33021;&#23548;&#33268;&#20986;&#29616;&#19981;&#33391;&#23646;&#24615;&#30340;&#29983;&#25104;&#12290;&#20363;&#22914;&#65292;&#20248;&#21270;&#23545;&#35805;&#38271;&#24230;&#21487;&#33021;&#23548;&#33268;&#19982;&#22522;&#32447;&#30456;&#27604;&#26356;&#20855;&#20105;&#35758;&#24615;&#25110;&#19981;&#21451;&#22909;&#30340;&#29983;&#25104;&#65292;&#32780;&#20248;&#21270;&#31215;&#26497;&#24773;&#24863;&#25110;&#21453;&#24212;&#21017;&#21487;&#20197;&#20943;&#23569;&#36825;&#20123;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study improving social conversational agents by learning from natural dialogue between users and a deployed model, without extra annotations. To implicitly measure the quality of a machine-generated utterance, we leverage signals like user response length, sentiment and reaction of the future human utterances in the collected dialogue episodes. Our experiments use the publicly released deployment data from BlenderBot (Xu et al., 2023). Human evaluation indicates improvements in our new models over baseline responses; however, we find that some proxy signals can lead to more generations with undesirable properties as well. For example, optimizing for conversation length can lead to more controversial or unfriendly generations compared to the baseline, whereas optimizing for positive sentiment or reaction can decrease these behaviors.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;ChatGPT&#30340;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#20998;&#31867;&#23398;&#24635;&#32467;&#65292;&#24182;&#25506;&#32034;&#20102;&#20854;&#22312;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25209;&#21028;&#24615;&#20998;&#26512;&#20102;&#29616;&#26377;&#25991;&#29486;&#21644;ChatGPT&#22312;&#35299;&#20915;&#29616;&#23454;&#25361;&#25112;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.14107</link><description>&lt;p&gt;
&#35299;&#30721;ChatGPT&#65306;&#29616;&#26377;&#30740;&#31350;&#12289;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#21487;&#33021;&#26041;&#21521;&#30340;&#20998;&#31867;&#23398;
&lt;/p&gt;
&lt;p&gt;
Decoding ChatGPT: A Taxonomy of Existing Research, Current Challenges, and Possible Future Directions. (arXiv:2307.14107v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;ChatGPT&#30340;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#20998;&#31867;&#23398;&#24635;&#32467;&#65292;&#24182;&#25506;&#32034;&#20102;&#20854;&#22312;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25209;&#21028;&#24615;&#20998;&#26512;&#20102;&#29616;&#26377;&#25991;&#29486;&#21644;ChatGPT&#22312;&#35299;&#20915;&#29616;&#23454;&#25361;&#25112;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;2022&#24180;11&#26376;&#21457;&#24067;&#20197;&#26469;&#65292;Chat GPT&#65288;Chat Generative Pre-trained Transformer&#65289;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#21644;&#20851;&#27880;&#12290;&#23427;&#22312;&#21253;&#25324;&#32771;&#35797;&#36890;&#36807;&#21644;&#21019;&#36896;&#24615;&#20889;&#20316;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#19982;&#20559;&#35265;&#21644;&#20449;&#20219;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#25285;&#24551;&#20381;&#28982;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;ChatGPT&#30340;100&#22810;&#31687;Scopus&#32034;&#24341;&#30340;&#20986;&#29256;&#29289;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#65292;&#26088;&#22312;&#25552;&#20379;ChatGPT&#30740;&#31350;&#30340;&#20998;&#31867;&#23398;&#65292;&#24182;&#25506;&#32034;&#20854;&#24212;&#29992;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#25991;&#29486;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;ChatGPT&#22312;&#21307;&#30103;&#12289;&#33829;&#38144;&#21644;&#37329;&#34701;&#26381;&#21153;&#12289;&#36719;&#20214;&#24037;&#31243;&#12289;&#23398;&#26415;&#21644;&#31185;&#23398;&#20889;&#20316;&#12289;&#30740;&#31350;&#21644;&#25945;&#32946;&#12289;&#29615;&#22659;&#31185;&#23398;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#30740;&#31350;&#36825;&#20123;&#24212;&#29992;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;ChatGPT&#22312;&#24212;&#23545;&#29616;&#23454;&#25361;&#25112;&#26041;&#38754;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chat Generative Pre-trained Transformer (ChatGPT) has gained significant interest and attention since its launch in November 2022. It has shown impressive performance in various domains, including passing exams and creative writing. However, challenges and concerns related to biases and trust persist. In this work, we present a comprehensive review of over 100 Scopus-indexed publications on ChatGPT, aiming to provide a taxonomy of ChatGPT research and explore its applications. We critically analyze the existing literature, identifying common approaches employed in the studies. Additionally, we investigate diverse application areas where ChatGPT has found utility, such as healthcare, marketing and financial services, software engineering, academic and scientific writing, research and education, environmental science, and natural language processing. Through examining these applications, we gain valuable insights into the potential of ChatGPT in addressing real-world challenges. We also 
&lt;/p&gt;</description></item><item><title>Multi3WOZ&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#25991;&#21270;&#36866;&#24212;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#39046;&#22495;&#12289;&#22810;&#23545;&#35805;&#24179;&#34892;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.14031</link><description>&lt;p&gt;
Multi3WOZ: &#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#25991;&#21270;&#36866;&#24212;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#39046;&#22495;&#12289;&#22810;&#23545;&#35805;&#24179;&#34892;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Multi3WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems. (arXiv:2307.14031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14031
&lt;/p&gt;
&lt;p&gt;
Multi3WOZ&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#25991;&#21270;&#36866;&#24212;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#39046;&#22495;&#12289;&#22810;&#23545;&#35805;&#24179;&#34892;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;ToD&#65289;&#30340;&#39640;&#36136;&#37327;&#26631;&#27880;&#25968;&#25454;&#30340;&#21019;&#24314;&#34987;&#20844;&#35748;&#20026;&#26497;&#20854;&#22256;&#38590;&#65292;&#32780;&#24403;&#30446;&#26631;&#26159;&#20026;&#22810;&#31181;&#35821;&#35328;&#21019;&#24314;&#20844;&#24179;&#12289;&#25991;&#21270;&#36866;&#24212;&#21644;&#22823;&#35268;&#27169;&#30340;ToD&#25968;&#25454;&#38598;&#26102;&#65292;&#25361;&#25112;&#26356;&#21152;&#20005;&#23803;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#30340;&#25968;&#25454;&#38598;&#20173;&#28982;&#38750;&#24120;&#31232;&#32570;&#65292;&#24182;&#19988;&#23384;&#22312;&#35832;&#22914;&#22522;&#20110;&#32763;&#35793;&#30340;&#38750;&#27597;&#35821;&#23545;&#35805;&#19982;&#32763;&#35793;&#25928;&#26524;&#19981;&#20339;&#12289;&#35268;&#27169;&#23567;&#25110;&#32570;&#20047;&#25991;&#21270;&#36866;&#24212;&#31561;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#26803;&#29702;&#20102;&#24403;&#21069;&#22810;&#35821;&#35328;ToD&#25968;&#25454;&#38598;&#30340;&#29616;&#29366;&#65292;&#31995;&#32479;&#22320;&#27010;&#36848;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#21644;&#38480;&#21046;&#12290;&#20026;&#20102;&#20943;&#23569;&#25152;&#26377;&#26816;&#27979;&#21040;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Multi3WOZ&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#39046;&#22495;&#12289;&#22810;&#23545;&#35805;&#24179;&#34892;ToD&#25968;&#25454;&#38598;&#12290;&#23427;&#26159;&#22823;&#35268;&#27169;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#20197;&#22235;&#31181;&#35821;&#35328;&#36827;&#34892;&#25991;&#21270;&#36866;&#24212;&#30340;&#23545;&#35805;&#65292;&#20197;&#20415;&#35757;&#32451;&#21644;&#35780;&#20272;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#30340;ToD&#31995;&#32479;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#33258;&#19979;&#32780;&#19978;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#65292;&#24471;&#21040;&#20102;&#26368;&#32456;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating high-quality annotated data for task-oriented dialog (ToD) is known to be notoriously difficult, and the challenges are amplified when the goal is to create equitable, culturally adapted, and large-scale ToD datasets for multiple languages. Therefore, the current datasets are still very scarce and suffer from limitations such as translation-based non-native dialogs with translation artefacts, small scale, or lack of cultural adaptation, among others. In this work, we first take stock of the current landscape of multilingual ToD datasets, offering a systematic overview of their properties and limitations. Aiming to reduce all the detected limitations, we then introduce Multi3WOZ, a novel multilingual, multi-domain, multi-parallel ToD dataset. It is large-scale and offers culturally adapted dialogs in 4 languages to enable training and evaluation of multilingual and cross-lingual ToD systems. We describe a complex bottom-up data collection process that yielded the final dataset,
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#21333;&#19968;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#38190;&#35789;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#38271;&#25991;&#26412;&#20851;&#38190;&#35789;&#25552;&#21462;&#33021;&#21147;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#23616;&#37096;&#21644;&#20840;&#23616;&#20851;&#38190;&#35789;&#65292;&#24182;&#25581;&#31034;&#25991;&#26412;&#30340;&#22522;&#26412;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#26159;&#35821;&#35328;&#26080;&#20851;&#30340;&#65292;&#36866;&#29992;&#20110;&#30701;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.14005</link><description>&lt;p&gt;
&#21333;&#19968;&#25991;&#26412;&#30340;&#26080;&#30417;&#30563;&#25552;&#21462;&#23616;&#37096;&#21644;&#20840;&#23616;&#20851;&#38190;&#35789;
&lt;/p&gt;
&lt;p&gt;
Unsupervised extraction of local and global keywords from a single text. (arXiv:2307.14005v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14005
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#21333;&#19968;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#38190;&#35789;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#38271;&#25991;&#26412;&#20851;&#38190;&#35789;&#25552;&#21462;&#33021;&#21147;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#23616;&#37096;&#21644;&#20840;&#23616;&#20851;&#38190;&#35789;&#65292;&#24182;&#25581;&#31034;&#25991;&#26412;&#30340;&#22522;&#26412;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#26159;&#35821;&#35328;&#26080;&#20851;&#30340;&#65292;&#36866;&#29992;&#20110;&#30701;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#12289;&#19982;&#35821;&#26009;&#24211;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#20174;&#21333;&#19968;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#38190;&#35789;&#12290;&#23427;&#22522;&#20110;&#21333;&#35789;&#30340;&#31354;&#38388;&#20998;&#24067;&#21450;&#20854;&#23545;&#21333;&#35789;&#30340;&#38543;&#26426;&#25490;&#21015;&#30340;&#21709;&#24212;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;YAKE&#31561;&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19977;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;&#23427;&#22312;&#20174;&#38271;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#38190;&#35789;&#26041;&#38754;&#26356;&#21152;&#26377;&#25928;&#12290;&#20854;&#27425;&#65292;&#23427;&#33021;&#22815;&#25512;&#26029;&#20986;&#20004;&#31181;&#31867;&#22411;&#30340;&#20851;&#38190;&#35789;&#65306;&#23616;&#37096;&#21644;&#20840;&#23616;&#12290;&#31532;&#19977;&#65292;&#23427;&#25581;&#31034;&#20102;&#25991;&#26412;&#30340;&#22522;&#26412;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#35821;&#35328;&#26080;&#20851;&#65292;&#36866;&#29992;&#20110;&#30701;&#25991;&#26412;&#12290;&#32467;&#26524;&#36890;&#36807;&#25105;&#20204;&#30340;&#21476;&#20856;&#25991;&#23398;&#20316;&#21697;&#25968;&#25454;&#24211;&#30340;&#20855;&#22791;&#20808;&#21069;&#30693;&#35782;&#30340;&#20154;&#31867;&#27880;&#37322;&#32773;&#33719;&#24471;&#65288;&#27880;&#37322;&#32773;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20174;&#20013;&#31561;&#21040;&#37325;&#22823;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24471;&#21040;&#20102;&#22522;&#20110;&#25552;&#21462;&#20869;&#23481;&#35789;&#30340;&#24179;&#22343;&#38271;&#24230;&#21644;&#25552;&#21462;&#35789;&#20013;&#21517;&#35789;&#30340;&#24179;&#22343;&#25968;&#37327;&#30340;&#26080;&#20154;&#21442;&#19982;&#35770;&#35777;&#30340;&#25903;&#25345;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20851;&#38190;&#35789;&#19982;&#39640;&#38454;&#25991;&#26412;&#29305;&#24449;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an unsupervised, corpus-independent method to extract keywords from a single text. It is based on the spatial distribution of words and the response of this distribution to a random permutation of words. As compared to existing methods (such as e.g. YAKE) our method has three advantages. First, it is significantly more effective at extracting keywords from long texts. Second, it allows inference of two types of keywords: local and global. Third, it uncovers basic themes in texts. Additionally, our method is language-independent and applies to short texts. The results are obtained via human annotators with previous knowledge of texts from our database of classical literary works (the agreement between annotators is from moderate to substantial). Our results are supported via human-independent arguments based on the average length of extracted content words and on the average number of nouns in extracted words. We discuss relations of keywords with higher-order textual feature
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32454;&#31890;&#24230;&#35780;&#20272;&#26465;&#20214;&#30340;&#24773;&#24863;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26356;&#35814;&#32454;&#22320;&#20102;&#35299;&#29305;&#23450;&#24773;&#32490;&#30340;&#24418;&#25104;&#21407;&#22240;&#21644;&#23646;&#24615;&#65292;&#20174;&#32780;&#20351;&#25991;&#26412;&#29983;&#25104;&#26356;&#21152;&#20934;&#30830;&#21644;&#36924;&#30495;&#12290;</title><link>http://arxiv.org/abs/2307.14004</link><description>&lt;p&gt;
&#22522;&#20110;&#32454;&#31890;&#24230;&#35780;&#20272;&#26465;&#20214;&#30340;&#24773;&#24863;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20107;&#20214;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Affective Natural Language Generation of Event Descriptions through Fine-grained Appraisal Conditions. (arXiv:2307.14004v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32454;&#31890;&#24230;&#35780;&#20272;&#26465;&#20214;&#30340;&#24773;&#24863;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26356;&#35814;&#32454;&#22320;&#20102;&#35299;&#29305;&#23450;&#24773;&#32490;&#30340;&#24418;&#25104;&#21407;&#22240;&#21644;&#23646;&#24615;&#65292;&#20174;&#32780;&#20351;&#25991;&#26412;&#29983;&#25104;&#26356;&#21152;&#20934;&#30830;&#21644;&#36924;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36890;&#24120;&#20165;&#20381;&#36182;&#20110;&#22522;&#26412;&#24773;&#32490;&#29702;&#35770;&#25110;&#20215;&#20540;/&#21796;&#37266;&#20540;&#20316;&#20026;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#24773;&#24863;&#36890;&#24120;&#26159;&#38544;&#21547;&#20256;&#36798;&#30340;&#12290;&#25105;&#20204;&#20551;&#35774;&#24182;&#38543;&#21518;&#23637;&#31034;&#65292;&#22312;&#29983;&#25104;&#26694;&#26550;&#20013;&#21253;&#25324;&#35780;&#20272;&#21464;&#37327;&#20316;&#20026;&#26465;&#20214;&#20855;&#26377;&#20004;&#20010;&#20248;&#21183;&#12290;&#31532;&#19968;&#65292;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#26356;&#35814;&#32454;&#22320;&#20102;&#35299;&#29305;&#23450;&#24773;&#32490;&#30340;&#24418;&#25104;&#21407;&#22240;&#21644;&#23646;&#24615;&#12290;&#36825;&#23548;&#33268;&#25991;&#26412;&#29983;&#25104;&#26356;&#21152;&#20934;&#30830;&#21644;&#36924;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models for affective text generation have shown a remarkable progress, but they commonly rely only on basic emotion theories or valance/arousal values as conditions. This is appropriate when the goal is to create explicit emotion statements ("The kid is happy."). Emotions are, however, commonly communicated implicitly. For instance, the emotional interpretation of an event ("Their dog died.") does often not require an explicit emotion statement. In psychology, appraisal theories explain the link between a cognitive evaluation of an event and the potentially developed emotion. They put the assessment of the situation on the spot, for instance regarding the own control or the responsibility for what happens. We hypothesize and subsequently show that including appraisal variables as conditions in a generation framework comes with two advantages. (1) The generation model is informed in greater detail about what makes a specific emotion and what properties it has. This leads to text generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#36827;&#34892;&#21542;&#23450;&#24863;&#30693;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;NegBLEURT&#35780;&#20272;&#25351;&#26631;&#20197;&#25552;&#39640;&#23545;&#21542;&#23450;&#30340;&#25935;&#24863;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21542;&#23450;&#21477;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25351;&#26631;&#65292;&#24182;&#19988;&#22312;&#20854;&#20182;&#25200;&#21160;&#19978;&#20445;&#25345;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.13989</link><description>&lt;p&gt;
&#36825;&#26159;&#19981;&#27491;&#30830;&#30340;&#65281;&#23545;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#36827;&#34892;&#21542;&#23450;&#24863;&#30693;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
This is not correct! Negation-aware Evaluation of Language Generation Systems. (arXiv:2307.13989v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#36827;&#34892;&#21542;&#23450;&#24863;&#30693;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;NegBLEURT&#35780;&#20272;&#25351;&#26631;&#20197;&#25552;&#39640;&#23545;&#21542;&#23450;&#30340;&#25935;&#24863;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21542;&#23450;&#21477;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25351;&#26631;&#65292;&#24182;&#19988;&#22312;&#20854;&#20182;&#25200;&#21160;&#19978;&#20445;&#25345;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20302;&#20272;&#20102;&#21542;&#23450;&#23545;&#21477;&#23376;&#21547;&#20041;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#23398;&#20064;&#35780;&#20272;&#25351;&#26631;&#23545;&#21542;&#23450;&#19981;&#25935;&#24863;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NegBLEURT&#65292;&#19968;&#31181;&#38024;&#23545;&#21542;&#23450;&#24863;&#30693;&#30340;BLEURT&#35780;&#20272;&#25351;&#26631;&#30340;&#25913;&#36827;&#29256;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#21477;&#23376;&#21542;&#23450;&#24037;&#20855;&#65292;&#24182;&#29992;&#23427;&#26469;&#21019;&#24314;&#20102;CANNOT&#21542;&#23450;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#22522;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#21477;&#23376;&#36716;&#25442;&#22120;&#21644;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#23427;&#20204;&#23545;&#21542;&#23450;&#30340;&#25935;&#24863;&#24615;&#12290;&#22312;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#34920;&#26126;&#65292;&#25105;&#20204;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#22312;&#21542;&#23450;&#21477;&#19978;&#30340;&#34920;&#29616;&#36828;&#36828;&#20248;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23427;&#20204;&#22522;&#30784;&#27169;&#22411;&#22312;&#20854;&#20182;&#25200;&#21160;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models underestimate the impact of negations on how much they change the meaning of a sentence. Therefore, learned evaluation metrics based on these models are insensitive to negations. In this paper, we propose NegBLEURT, a negation-aware version of the BLEURT evaluation metric. For that, we designed a rule-based sentence negation tool and used it to create the CANNOT negation evaluation dataset. Based on this dataset, we fine-tuned a sentence transformer and an evaluation metric to improve their negation sensitivity. Evaluating these models on existing benchmarks shows that our fine-tuned models outperform existing metrics on the negated sentences by far while preserving their base models' performances on other perturbations.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#22312;&#36229;&#20986;&#20998;&#24067;(OOD)&#25968;&#25454;&#19978;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;OOD&#25968;&#25454;&#19978;&#20351;&#29992;&#25193;&#25955;&#24494;&#35843;PLMs&#20250;&#38477;&#20302;&#37325;&#24314;&#33021;&#21147;&#12290;&#27604;&#36739;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;OOD&#26679;&#26412;&#30340;&#37325;&#26500;&#33021;&#21147;&#21644;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13949</link><description>&lt;p&gt;
&#25193;&#25955;&#22914;&#20309;&#24433;&#21709;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Does Diffusion Influence Pretrained Language Models on Out-of-Distribution Data?. (arXiv:2307.13949v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13949
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#22312;&#36229;&#20986;&#20998;&#24067;(OOD)&#25968;&#25454;&#19978;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;OOD&#25968;&#25454;&#19978;&#20351;&#29992;&#25193;&#25955;&#24494;&#35843;PLMs&#20250;&#38477;&#20302;&#37325;&#24314;&#33021;&#21147;&#12290;&#27604;&#36739;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;OOD&#26679;&#26412;&#30340;&#37325;&#26500;&#33021;&#21147;&#21644;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#22312;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;PLMs&#30340;&#19968;&#20010;&#37325;&#35201;&#20248;&#21183;&#26159;&#33391;&#22909;&#30340;&#36229;&#20986;&#20998;&#24067;(OOD)&#40065;&#26834;&#24615;&#12290;&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#21560;&#24341;&#20102;&#24456;&#22810;&#30740;&#31350;&#23558;&#25193;&#25955;&#24212;&#29992;&#20110;PLMs&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#23545;PLMs&#22312;OOD&#25968;&#25454;&#19978;&#30340;&#24433;&#21709;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#21069;&#21521;&#25193;&#25955;&#36807;&#31243;&#65292;&#36880;&#28176;&#23545;&#36755;&#20837;&#24212;&#29992;&#39640;&#26031;&#22122;&#22768;&#65292;&#24182;&#19988;&#19968;&#20010;&#21453;&#21521;&#21435;&#22122;&#36807;&#31243;&#65292;&#29992;&#20110;&#21435;&#38500;&#22122;&#22768;&#12290;&#22122;&#22768;&#36755;&#20837;&#37325;&#24314;&#26159;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#37325;&#24314;&#25439;&#22833;&#30452;&#25509;&#20998;&#26512;OOD&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#27979;&#35797;&#37325;&#24314;OOD&#25968;&#25454;&#21644;&#26816;&#27979;OOD&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#30340;&#35757;&#32451;&#21442;&#25968;&#21644;&#25968;&#25454;&#32479;&#35745;&#29305;&#24449;&#22312;&#20843;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#25193;&#25955;&#24494;&#35843;PLMs&#20250;&#38477;&#20302;&#22312;OOD&#25968;&#25454;&#19978;&#30340;&#37325;&#24314;&#33021;&#21147;&#12290;&#27604;&#36739;&#36824;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;OOD&#26679;&#26412;&#30340;&#37325;&#26500;&#33021;&#21147;&#21644;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pretrained language models (PLMs) have achieved great success in modern NLP. An important advantage of PLMs is good out-of-distribution (OOD) robustness. Recently, diffusion models have attracted a lot of work to apply diffusion to PLMs. It remains under-explored how diffusion influences PLMs on OOD data. The core of diffusion models is a forward diffusion process which gradually applies Gaussian noise to inputs, and a reverse denoising process which removes noise. The noised input reconstruction is a fundamental ability of diffusion models. We directly analyze OOD robustness by measuring the reconstruction loss, including testing the abilities to reconstruct OOD data, and to detect OOD samples. Experiments are conducted by analyzing different training parameters and data statistical features on eight datasets. It shows that finetuning PLMs with diffusion degrades the reconstruction ability on OOD data. The comparison also shows that diffusion models can effectively d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GrammarGPT&#65292;&#19968;&#20010;&#24320;&#28304;LLM&#29992;&#20110;&#27597;&#35821;&#27721;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65292;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20462;&#27491;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13923</link><description>&lt;p&gt;
GrammarGPT: &#20351;&#29992;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#24320;&#28304;LLM&#25506;&#32034;&#27597;&#35821;&#27721;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning. (arXiv:2307.13923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GrammarGPT&#65292;&#19968;&#20010;&#24320;&#28304;LLM&#29992;&#20110;&#27597;&#35821;&#27721;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65292;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20462;&#27491;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26088;&#22312;&#33258;&#21160;&#20462;&#27491;&#19981;&#31526;&#21512;&#35821;&#27861;&#30340;&#21477;&#23376;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#38381;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65292;&#20363;&#22914;ChatGPT&#65289;&#22312;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24320;&#28304;LLM&#30340;&#28508;&#21147;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;GrammarGPT&#30340;&#24320;&#28304;LLM&#65292;&#26088;&#22312;&#21021;&#27493;&#25506;&#32034;&#20854;&#22312;&#27597;&#35821;&#27721;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;GrammarGPT&#30340;&#26680;&#24515;&#26041;&#27861;&#26159;&#21033;&#29992;ChatGPT&#29983;&#25104;&#30340;&#25968;&#25454;&#21644;&#20154;&#24037;&#26631;&#27880;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#12290;&#23545;&#20110;&#24102;&#26377;&#25552;&#31034;&#20449;&#24687;&#30340;&#35821;&#27861;&#38169;&#35823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25351;&#23548;ChatGPT&#36890;&#36807;&#25552;&#20379;&#36825;&#20123;&#25552;&#31034;&#20449;&#24687;&#29983;&#25104;&#19981;&#31526;&#21512;&#35821;&#27861;&#30340;&#21477;&#23376;&#12290;&#23545;&#20110;&#27809;&#26377;&#25552;&#31034;&#20449;&#24687;&#30340;&#35821;&#27861;&#38169;&#35823;&#65292;&#25105;&#20204;&#20174;&#20844;&#24320;&#21487;&#29992;&#30340;&#32593;&#31449;&#25910;&#38598;&#20102;&#19981;&#31526;&#21512;&#35821;&#27861;&#30340;&#21477;&#23376;&#65292;&#24182;&#36827;&#34892;&#25163;&#21160;&#20462;&#27491;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#38169;&#35823;&#19981;&#21464;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#35813;&#27169;&#22411;&#32416;&#27491;&#27597;&#35821;&#27721;&#35821;&#35821;&#27861;&#38169;&#35823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grammatical error correction aims to correct ungrammatical sentences automatically. Recently, some work has demonstrated the excellent capabilities of closed-source Large Language Models (LLMs, e.g., ChatGPT) in grammatical error correction. However, the potential of open-source LLMs remains unexplored. In this paper, we introduced GrammarGPT, an open-source LLM, to preliminary explore its potential for native Chinese grammatical error correction. The core recipe of GrammarGPT is to leverage the hybrid dataset of ChatGPT-generated and human-annotated. For grammatical errors with clues, we proposed a heuristic method to guide ChatGPT to generate ungrammatical sentences by providing those clues. For grammatical errors without clues, we collected ungrammatical sentences from publicly available websites and manually corrected them. In addition, we employed an error-invariant augmentation method to enhance the ability of the model to correct native Chinese grammatical errors. We ultimately 
&lt;/p&gt;</description></item><item><title>FinTree&#26159;&#19968;&#31181;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#65292;&#20854;&#37319;&#29992;&#20102;&#39044;&#27979;&#25513;&#30721;&#26631;&#35760;&#30340;&#26032;&#39062;&#32467;&#26500;&#65292;&#36890;&#36807;&#35757;&#32451;&#25552;&#20379;&#19978;&#19979;&#25991;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#37329;&#34701;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.13900</link><description>&lt;p&gt;
FinTree&#65306;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
FinTree: Financial Dataset Pretrain Transformer Encoder for Relation Extraction. (arXiv:2307.13900v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13900
&lt;/p&gt;
&lt;p&gt;
FinTree&#26159;&#19968;&#31181;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#65292;&#20854;&#37319;&#29992;&#20102;&#39044;&#27979;&#25513;&#30721;&#26631;&#35760;&#30340;&#26032;&#39062;&#32467;&#26500;&#65292;&#36890;&#36807;&#35757;&#32451;&#25552;&#20379;&#19978;&#19979;&#25991;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#37329;&#34701;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FinTree&#65292;&#20351;&#29992;&#37329;&#34701;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#12290;&#21033;&#29992;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#37329;&#34701;&#39046;&#22495;&#20219;&#21153;&#20013;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;FinTree&#12290;FinTree&#20197;&#20854;&#39044;&#27979;&#25513;&#30721;&#26631;&#35760;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;[CLS]&#26631;&#35760;&#30340;&#26032;&#39062;&#32467;&#26500;&#32780;&#33073;&#39062;&#32780;&#20986;&#65292;&#21463;&#21040;&#27169;&#24335;&#21033;&#29992;&#35757;&#32451;&#26041;&#27861;&#30340;&#21551;&#21457;&#12290;&#36825;&#31181;&#32467;&#26500;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20004;&#20010;&#32473;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#19968;&#31181;&#29420;&#29305;&#30340;&#36755;&#20837;&#27169;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#25552;&#20379;&#26377;&#20851;&#25152;&#20851;&#27880;&#23454;&#20307;&#30340;&#19978;&#19979;&#25991;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#21518;&#22788;&#29702;&#27493;&#39588;&#30830;&#20445;&#19982;&#23454;&#20307;&#31867;&#22411;&#19968;&#33268;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;FinTree&#22312;&#22823;&#35268;&#27169;&#37329;&#34701;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;REFinD&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/HJ-Ok/FinTree&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present FinTree, Financial Dataset Pretrain Transformer Encoder for Relation Extraction. Utilizing an encoder language model, we further pretrain FinTree on the financial dataset, adapting the model in financial domain tasks. FinTree stands out with its novel structure that predicts a masked token instead of the conventional [CLS] token, inspired by the Pattern Exploiting Training methodology. This structure allows for more accurate relation predictions between two given entities. The model is trained with a unique input pattern to provide contextual and positional information about the entities of interest, and a post-processing step ensures accurate predictions in line with the entity types. Our experiments demonstrate that FinTree outperforms on the REFinD, a large-scale financial relation extraction dataset. The code and pretrained models are available at https://github.com/HJ-Ok/FinTree.
&lt;/p&gt;</description></item><item><title>WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.13854</link><description>&lt;p&gt;
WebArena: &#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13854
&lt;/p&gt;
&lt;p&gt;
WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36827;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#28508;&#21147;&#36880;&#28176;&#26174;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26234;&#33021;&#20307;&#20027;&#35201;&#26159;&#22312;&#31616;&#21270;&#30340;&#21512;&#25104;&#29615;&#22659;&#20013;&#21019;&#24314;&#21644;&#27979;&#35797;&#30340;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#24230;&#36924;&#30495;&#19988;&#21487;&#22797;&#29616;&#30340;&#26234;&#33021;&#20307;&#25351;&#20196;&#21644;&#25511;&#21046;&#29615;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#22235;&#20010;&#24120;&#35265;&#39046;&#22495;&#30340;&#23436;&#20840;&#21151;&#33021;&#32593;&#31449;&#30340;&#29615;&#22659;&#65292;&#20998;&#21035;&#26159;&#30005;&#23376;&#21830;&#21153;&#12289;&#31038;&#20132;&#35770;&#22363;&#35752;&#35770;&#12289;&#21327;&#21516;&#36719;&#20214;&#24320;&#21457;&#21644;&#20869;&#23481;&#31649;&#29702;&#12290;&#25105;&#20204;&#30340;&#29615;&#22659;&#20351;&#29992;&#24037;&#20855;&#65288;&#22914;&#22320;&#22270;&#65289;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;&#22914;&#29992;&#25143;&#25163;&#20876;&#65289;&#26469;&#40723;&#21169;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#32452;&#37325;&#28857;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;&#25105;&#20204;&#22522;&#20934;&#20219;&#21153;&#20855;&#26377;&#22810;&#26679;&#24615;&#21644;&#38271;&#36828;&#30340;&#35270;&#37326;&#65292;&#24182;&#19988;&#34987;&#35774;&#35745;&#20026;&#40723;&#21169;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#28145;&#23618;&#27425;&#30340;&#20219;&#21153;&#29702;&#35299;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are desi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#12298;2023&#24180;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#20107;&#20214;&#26816;&#27979;&#12299;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#38598;&#25104;&#23398;&#20064;&#21644;&#21477;&#27861;&#12289;&#23454;&#20307;&#29305;&#24449;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#21644;&#23459;&#20256;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.13829</link><description>&lt;p&gt;
&#12298;2023&#24180;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#20107;&#20214;&#26816;&#27979;&#20013;&#30340;ARC-NLP: &#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#12289;&#21477;&#27861;&#21644;&#23454;&#20307;&#29305;&#24449;&#25552;&#21319;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#12299;
&lt;/p&gt;
&lt;p&gt;
ARC-NLP at Multimodal Hate Speech Event Detection 2023: Multimodal Methods Boosted by Ensemble Learning, Syntactical and Entity Features. (arXiv:2307.13829v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#12298;2023&#24180;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#20107;&#20214;&#26816;&#27979;&#12299;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#38598;&#25104;&#23398;&#20064;&#21644;&#21477;&#27861;&#12289;&#23454;&#20307;&#29305;&#24449;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#21644;&#23459;&#20256;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#22270;&#29255;&#21487;&#20197;&#20256;&#25773;&#20167;&#24680;&#35328;&#35770;&#12289;&#23459;&#20256;&#21644;&#26497;&#31471;&#20027;&#20041;&#20449;&#20208;&#12290;&#22312;&#20420;&#32599;&#26031;-&#20044;&#20811;&#20848;&#25112;&#20105;&#26399;&#38388;&#65292;&#21452;&#26041;&#37117;&#22823;&#37327;&#20381;&#36182;&#25991;&#26412;&#23884;&#20837;&#22270;&#29255;&#20256;&#25773;&#23459;&#20256;&#21644;&#20167;&#24680;&#35328;&#35770;&#12290;&#30830;&#20445;&#26377;&#25928;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#21644;&#23459;&#20256;&#23545;&#20110;&#20943;&#36731;&#20167;&#24680;&#35328;&#35770;&#20256;&#25773;&#30340;&#36127;&#38754;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#25105;&#20204;&#22312;&#12298;2023&#24180;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#20107;&#20214;&#26816;&#27979;&#12299;&#20004;&#20010;&#23376;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#65292;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#21644;&#21477;&#27861;&#25991;&#26412;&#23646;&#24615;&#25552;&#21319;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#23545;&#20110;&#31532;&#20108;&#20010;&#23376;&#20219;&#21153;&#65292;&#30446;&#26631;&#26816;&#27979;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#29305;&#24449;&#25552;&#21319;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#36739;&#20110;&#22312;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#20351;&#29992;&#30340;&#25152;&#26377;&#25991;&#26412;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;-&#35270;&#35273;&#22522;&#20934;&#32447;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-embedded images can serve as a means of spreading hate speech, propaganda, and extremist beliefs. Throughout the Russia-Ukraine war, both opposing factions heavily relied on text-embedded images as a vehicle for spreading propaganda and hate speech. Ensuring the effective detection of hate speech and propaganda is of utmost importance to mitigate the negative effect of hate speech dissemination. In this paper, we outline our methodologies for two subtasks of Multimodal Hate Speech Event Detection 2023. For the first subtask, hate speech detection, we utilize multimodal deep learning models boosted by ensemble learning and syntactical text attributes. For the second subtask, target detection, we employ multimodal deep learning models boosted by named entity features. Through experimentation, we demonstrate the superior performance of our models compared to all textual, visual, and text-visual baselines employed in multimodal hate speech detection. Furthermore, our models achieve th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#27700;&#21360;&#20197;&#36827;&#34892;AI&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35821;&#20041;&#24863;&#30693;&#27700;&#21360;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20445;&#25345;&#26816;&#27979;&#33021;&#21147;&#30340;&#21516;&#26102;&#65292;&#22312;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.13808</link><description>&lt;p&gt;
&#27700;&#21360;&#25216;&#26415;&#29992;&#20110;AI&#26816;&#27979;&#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#65306;&#25581;&#31034;&#25361;&#25112;&#21450;&#35821;&#20041;&#24863;&#30693;&#27700;&#21360;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy. (arXiv:2307.13808v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#27700;&#21360;&#20197;&#36827;&#34892;AI&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35821;&#20041;&#24863;&#30693;&#27700;&#21360;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20445;&#25345;&#26816;&#27979;&#33021;&#21147;&#30340;&#21516;&#26102;&#65292;&#22312;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#26368;&#36817;&#30340;AI&#26816;&#27979;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#38543;&#26426;&#38480;&#21046;&#35789;&#27719;&#24182;&#21033;&#29992;&#27492;&#20449;&#24687;&#36827;&#34892;&#26816;&#27979;&#30340;&#26041;&#24335;&#23558;&#27700;&#21360;&#23884;&#20837;&#21040;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#12290;&#34429;&#28982;&#36825;&#20123;&#27700;&#21360;&#21482;&#20250;&#23548;&#33268;&#22256;&#24785;&#24230;&#36731;&#24494;&#19979;&#38477;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#35777;&#35843;&#26597;&#25581;&#31034;&#20102;&#23545;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#24615;&#33021;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35821;&#20041;&#24863;&#30693;&#27700;&#21360;&#31639;&#27861;&#65292;&#32771;&#34385;&#20102;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#30340;&#29305;&#24615;&#21644;&#36755;&#20837;&#19978;&#19979;&#25991;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65288;&#21253;&#25324;BART&#21644;Flan-T5&#65289;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#22312;&#25688;&#35201;&#29983;&#25104;&#21644;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#20173;&#20445;&#25345;&#20102;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To mitigate potential risks associated with language models, recent AI detection research proposes incorporating watermarks into machine-generated text through random vocabulary restrictions and utilizing this information for detection. While these watermarks only induce a slight deterioration in perplexity, our empirical investigation reveals a significant detriment to the performance of conditional text generation. To address this issue, we introduce a simple yet effective semantic-aware watermarking algorithm that considers the characteristics of conditional text generation and the input context. Experimental results demonstrate that our proposed method yields substantial improvements across various text generation models, including BART and Flan-T5, in tasks such as summarization and data-to-text generation while maintaining detection ability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32452;&#20214;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;GPT&#31995;&#21015;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#12290;&#23613;&#31649;GPT&#30340;&#39044;&#27979;&#32467;&#26524;&#19982;&#20154;&#31867;&#25552;&#20379;&#30340;&#35780;&#20272;&#21644;&#24773;&#24863;&#26631;&#31614;&#26174;&#33879;&#19968;&#33268;&#65292;&#20294;&#22312;&#39044;&#27979;&#24773;&#24863;&#24378;&#24230;&#21644;&#24212;&#23545;&#21453;&#24212;&#26041;&#38754;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2307.13779</link><description>&lt;p&gt;
GPT&#26159;&#24773;&#24863;&#30340;&#35745;&#31639;&#27169;&#22411;&#21527;&#65311;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is GPT a Computational Model of Emotion? Detailed Analysis. (arXiv:2307.13779v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32452;&#20214;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;GPT&#31995;&#21015;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#12290;&#23613;&#31649;GPT&#30340;&#39044;&#27979;&#32467;&#26524;&#19982;&#20154;&#31867;&#25552;&#20379;&#30340;&#35780;&#20272;&#21644;&#24773;&#24863;&#26631;&#31614;&#26174;&#33879;&#19968;&#33268;&#65292;&#20294;&#22312;&#39044;&#27979;&#24773;&#24863;&#24378;&#24230;&#21644;&#24212;&#23545;&#21453;&#24212;&#26041;&#38754;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32452;&#20214;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;GPT&#31995;&#21015;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#32771;&#23519;&#20102;&#35813;&#27169;&#22411;&#23545;&#33258;&#20256;&#24335;&#35760;&#24518;&#30340;&#25512;&#29702;&#26041;&#24335;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#25913;&#21464;&#24773;&#22659;&#30340;&#21508;&#20010;&#26041;&#38754;&#26469;&#24433;&#21709;&#24773;&#24863;&#24378;&#24230;&#21644;&#24212;&#23545;&#20542;&#21521;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;GPT&#30340;&#39044;&#27979;&#32467;&#26524;&#19982;&#20154;&#31867;&#25552;&#20379;&#30340;&#35780;&#20272;&#21644;&#24773;&#24863;&#26631;&#31614;&#26174;&#33879;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;GPT&#22312;&#39044;&#27979;&#24773;&#24863;&#24378;&#24230;&#21644;&#24212;&#23545;&#21453;&#24212;&#26041;&#38754;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#23613;&#31649;GPT-4&#22312;&#21021;&#27493;&#30740;&#31350;&#20013;&#34920;&#29616;&#26368;&#22909;&#65292;&#20294;&#22312;&#31532;&#20108;&#39033;&#30740;&#31350;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#23613;&#31649;&#32463;&#36807;&#36731;&#24494;&#30340;&#25552;&#31034;&#24037;&#31243;&#21518;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#35780;&#20272;&#24341;&#21457;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#20248;&#28857;&#24182;&#35299;&#20915;&#20854;&#24369;&#28857;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#21709;&#24212;&#30340;&#21464;&#24322;&#24615;&#12290;&#36825;&#20123;&#30740;&#31350;&#31361;&#26174;&#20102;&#20174;&#32452;&#20214;&#30340;&#35282;&#24230;&#35780;&#20272;&#27169;&#22411;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the emotional reasoning abilities of the GPT family of large language models via a component perspective. The paper first examines how the model reasons about autobiographical memories. Second, it systematically varies aspects of situations to impact emotion intensity and coping tendencies. Even without the use of prompt engineering, it is shown that GPT's predictions align significantly with human-provided appraisals and emotional labels. However, GPT faces difficulties predicting emotion intensity and coping responses. GPT-4 showed the highest performance in the initial study but fell short in the second, despite providing superior results after minor prompt engineering. This assessment brings up questions on how to effectively employ the strong points and address the weak areas of these models, particularly concerning response variability. These studies underscore the merits of evaluating models from a componential perspective.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#21333;&#35821;&#35821;&#35328;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#21270;&#26144;&#23556;&#26426;&#21046;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#35789;&#20041;&#28040;&#27495;&#30340;&#22810;&#35821;&#35328;&#35781;&#21650;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13776</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#40784;&#31232;&#30095;&#19978;&#19979;&#25991;&#21270;&#30340;&#35789;&#34920;&#31034;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#35789;&#20041;&#28040;&#27495;&#20013;&#30340;&#22810;&#35821;&#35328;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Combating the Curse of Multilinguality in Cross-Lingual WSD by Aligning Sparse Contextualized Word Representations. (arXiv:2307.13776v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#21333;&#35821;&#35821;&#35328;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#21270;&#26144;&#23556;&#26426;&#21046;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#35789;&#20041;&#28040;&#27495;&#30340;&#22810;&#35821;&#35328;&#35781;&#21650;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20513;&#22312;&#36328;&#35821;&#35328;&#38646;&#26679;&#26412;&#35789;&#20041;&#28040;&#27495;&#65288;WSD&#65289;&#20013;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#21333;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#19978;&#19979;&#25991;&#21270;&#26144;&#23556;&#26426;&#21046;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#36807;&#31243;&#33719;&#24471;&#30340;&#31232;&#30095;&#19978;&#19979;&#25991;&#21270;&#35789;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19978;&#36848;&#25913;&#36827;&#20351;&#24471;17&#31181;&#35821;&#35328;&#30340;&#24179;&#22343;F&#20998;&#25968;&#26377;&#36817;6.5&#20010;&#30334;&#20998;&#28857;&#30340;&#26174;&#33879;&#25552;&#39640;&#65288;&#20174;62.0&#25552;&#39640;&#21040;68.5&#65289;&#12290;&#25105;&#20204;&#22312;https://github.com/begab/sparsity_makes_sense&#21457;&#24067;&#20102;&#29992;&#20110;&#22797;&#29616;&#25105;&#20204;&#23454;&#39564;&#30340;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we advocate for using large pre-trained monolingual language models in cross lingual zero-shot word sense disambiguation (WSD) coupled with a contextualized mapping mechanism. We also report rigorous experiments that illustrate the effectiveness of employing sparse contextualized word representations obtained via a dictionary learning procedure. Our experimental results demonstrate that the above modifications yield a significant improvement of nearly 6.5 points of increase in the average F-score (from 62.0 to 68.5) over a collection of 17 typologically diverse set of target languages. We release our source code for replicating our experiments at https://github.com/begab/sparsity_makes_sense.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22810;&#26679;&#24615;&#21644;&#35821;&#35328;&#25216;&#26415;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25351;&#20986;&#31185;&#25216;&#35821;&#35328;&#20559;&#35265;&#20250;&#23548;&#33268;&#35748;&#35782;&#35770;&#19981;&#20844;&#27491;&#30340;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;AI&#25216;&#26415;&#23545;&#20110;&#26410;&#26381;&#21153;&#35821;&#35328;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.13714</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#21644;&#35821;&#35328;&#25216;&#26415;&#65306;&#31185;&#25216;&#35821;&#35328;&#20559;&#35265;&#22914;&#20309;&#23548;&#33268;&#35748;&#35782;&#35770;&#19981;&#20844;&#27491;
&lt;/p&gt;
&lt;p&gt;
Diversity and Language Technology: How Techno-Linguistic Bias Can Cause Epistemic Injustice. (arXiv:2307.13714v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22810;&#26679;&#24615;&#21644;&#35821;&#35328;&#25216;&#26415;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25351;&#20986;&#31185;&#25216;&#35821;&#35328;&#20559;&#35265;&#20250;&#23548;&#33268;&#35748;&#35782;&#35770;&#19981;&#20844;&#27491;&#30340;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;AI&#25216;&#26415;&#23545;&#20110;&#26410;&#26381;&#21153;&#35821;&#35328;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35821;&#35328;&#25216;&#26415;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12289;&#22810;&#35821;&#35328;&#35789;&#20856;&#21644;&#35821;&#26009;&#24211;&#31561;&#65292;&#30446;&#21069;&#20165;&#38480;&#20110;&#19990;&#30028;&#19978;2%&#33267;3%&#30340;&#26368;&#24120;&#29992;&#12289;&#25919;&#27835;&#21644;&#36130;&#25919;&#25903;&#25345;&#26368;&#22909;&#30340;&#35821;&#35328;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21162;&#21147;&#35797;&#22270;&#23558;AI&#25216;&#26415;&#25193;&#23637;&#21040;&#8220;&#26410;&#26381;&#21153;&#30340;&#35821;&#35328;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#36825;&#20123;&#23581;&#35797;&#25152;&#20135;&#29983;&#30340;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#32570;&#38519;&#65292;&#36825;&#20123;&#26041;&#26696;&#36981;&#24490;&#20102;&#23545;&#26576;&#20123;&#35821;&#35328;&#30340;&#30828;&#32534;&#30721;&#34920;&#29616;&#20559;&#22909;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31185;&#25216;&#35821;&#35328;&#20559;&#35265;&#12290;&#31185;&#25216;&#35821;&#35328;&#20559;&#35265;&#19982;&#24050;&#32463;&#34987;&#24191;&#27867;&#35748;&#21487;&#30340;&#35821;&#35328;&#20559;&#35265;&#29616;&#35937;&#19981;&#21516;&#65292;&#23427;&#19981;&#28041;&#21450;&#25152;&#20195;&#34920;&#30340;&#35821;&#35328;&#65292;&#32780;&#26159;&#28041;&#21450;&#25216;&#26415;&#30340;&#35774;&#35745;&#12290;&#27491;&#22914;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25152;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#31185;&#25216;&#35821;&#35328;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#31995;&#32479;&#21482;&#33021;&#34920;&#36798;&#23646;&#20110;&#29305;&#23450;&#35821;&#35328;&#21644;&#25991;&#21270;&#30340;&#27010;&#24565;&#65292;&#26080;&#27861;&#27491;&#30830;&#34920;&#36798;&#20854;&#20182;&#31038;&#21306;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well known that AI-based language technology -- large language models, machine translation systems, multilingual dictionaries, and corpora -- is currently limited to 2 to 3 percent of the world's most widely spoken and/or financially and politically best supported languages. In response, recent research efforts have sought to extend the reach of AI technology to ``underserved languages.'' In this paper, we show that many of these attempts produce flawed solutions that adhere to a hard-wired representational preference for certain languages, which we call techno-linguistic bias. Techno-linguistic bias is distinct from the well-established phenomenon of linguistic bias as it does not concern the languages represented but rather the design of the technologies. As we show through the paper, techno-linguistic bias can result in systems that can only express concepts that are part of the language and culture of dominant powers, unable to correctly represent concepts from other communit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21542;&#36827;&#34892;&#24544;&#23454;&#30340;&#8220;&#38142;&#24335;&#24605;&#32500;&#8221;&#25512;&#29702;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23545;&#38142;&#24335;&#24605;&#32500;&#30340;&#20381;&#36182;&#31243;&#24230;&#26377;&#24456;&#22823;&#21464;&#21270;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#26356;&#22823;&#26356;&#33021;&#21147;&#36234;&#24378;&#65292;&#23427;&#20204;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20135;&#29983;&#30340;&#25512;&#29702;&#36234;&#26469;&#36234;&#19981;&#24544;&#23454;&#12290;</title><link>http://arxiv.org/abs/2307.13702</link><description>&lt;p&gt;
&#27979;&#37327;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#20013;&#30340;&#24544;&#35802;&#24230;
&lt;/p&gt;
&lt;p&gt;
Measuring Faithfulness in Chain-of-Thought Reasoning. (arXiv:2307.13702v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21542;&#36827;&#34892;&#24544;&#23454;&#30340;&#8220;&#38142;&#24335;&#24605;&#32500;&#8221;&#25512;&#29702;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23545;&#38142;&#24335;&#24605;&#32500;&#30340;&#20381;&#36182;&#31243;&#24230;&#26377;&#24456;&#22823;&#21464;&#21270;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#26356;&#22823;&#26356;&#33021;&#21147;&#36234;&#24378;&#65292;&#23427;&#20204;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20135;&#29983;&#30340;&#25512;&#29702;&#36234;&#26469;&#36234;&#19981;&#24544;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#65292;&#22914;&#26524;&#33021;&#22815;&#20135;&#29983;&#36880;&#27493;&#30340;&#8220;&#38142;&#24335;&#24605;&#32500;&#8221;&#25512;&#29702;&#65292;&#20854;&#34920;&#29616;&#20250;&#26356;&#22909;&#65292;&#20294;&#19981;&#28165;&#26970;&#25152;&#36848;&#30340;&#25512;&#29702;&#26159;&#21542;&#24544;&#23454;&#22320;&#35299;&#37322;&#20102;&#27169;&#22411;&#23454;&#38469;&#30340;&#25512;&#29702;&#36807;&#31243;&#65288;&#21363;&#22238;&#31572;&#38382;&#39064;&#30340;&#26041;&#24335;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#24403;&#20171;&#20837;&#38142;&#24335;&#24605;&#32500;&#26102;&#27169;&#22411;&#39044;&#27979;&#22914;&#20309;&#21457;&#29983;&#21464;&#21270;&#65288;&#20363;&#22914;&#65292;&#28155;&#21152;&#38169;&#35823;&#25110;&#25913;&#20889;&#23427;&#65289;&#65292;&#26469;&#30740;&#31350;&#38142;&#24335;&#24605;&#32500;&#21487;&#33021;&#19981;&#24544;&#23454;&#30340;&#20551;&#35774;&#12290;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#22312;&#39044;&#27979;&#31572;&#26696;&#26102;&#23545;&#38142;&#24335;&#24605;&#32500;&#30340;&#20381;&#36182;&#31243;&#24230;&#26377;&#24456;&#22823;&#21464;&#21270;&#65292;&#26377;&#26102;&#20250;&#20005;&#37325;&#20381;&#36182;&#38142;&#24335;&#24605;&#32500;&#65292;&#32780;&#20854;&#20182;&#26102;&#20505;&#21017;&#20027;&#35201;&#24573;&#35270;&#23427;&#12290;&#38142;&#24335;&#24605;&#32500;&#30340;&#24615;&#33021;&#25552;&#21319;&#20284;&#20046;&#19981;&#20165;&#20165;&#26469;&#33258;&#20110;&#20854;&#22686;&#21152;&#30340;&#27979;&#35797;&#35745;&#31639;&#37327;&#65292;&#20063;&#19981;&#20165;&#20165;&#26469;&#33258;&#20110;&#38142;&#24335;&#24605;&#32500;&#30340;&#29305;&#23450;&#25514;&#36766;&#25152;&#32534;&#30721;&#30340;&#20449;&#24687;&#12290;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#26356;&#22823;&#26356;&#26377;&#33021;&#21147;&#65292;&#23427;&#20204;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20135;&#29983;&#30340;&#25512;&#29702;&#36234;&#26469;&#36234;&#19981;&#24544;&#23454;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#29615;&#22659;&#36866;&#24403;&#65292;&#38142;&#24335;&#24605;&#32500;&#21487;&#20197;&#26159;&#24544;&#23454;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) perform better when they produce step-by-step, "Chain-of-Thought" (CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;67&#20301;&#26469;&#33258;&#39321;&#28207;&#22235;&#25152;&#20013;&#23398;&#30340;EFL&#23398;&#29983;&#23545;&#26426;&#22120;&#36741;&#21161;&#20889;&#20316;&#30340;&#24577;&#24230;&#21644;&#30683;&#30462;&#12290;&#30740;&#31350;&#21457;&#29616;&#23398;&#29983;&#23545;&#26426;&#22120;&#36741;&#21161;&#20889;&#20316;&#25345;&#31215;&#26497;&#24577;&#24230;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#36127;&#38754;&#25110;&#30683;&#30462;&#30340;&#24863;&#24773;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;EFL&#35838;&#22530;&#20013;&#23454;&#26045;&#26426;&#22120;&#36741;&#21161;&#20889;&#20316;&#30340;&#30410;&#22788;&#21644;&#25361;&#25112;&#65292;&#24314;&#35758;&#25945;&#32946;&#24037;&#20316;&#32773;&#23558;&#27963;&#21160;&#30446;&#26631;&#19982;&#23398;&#29983;&#30340;&#20215;&#20540;&#35266;&#12289;&#35821;&#35328;&#33021;&#21147;&#21644;AI&#33021;&#21147;&#30456;&#19968;&#33268;&#65292;&#20197;&#25552;&#39640;&#23398;&#29983;&#30340;&#27963;&#21160;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2307.13699</link><description>&lt;p&gt;
EFL&#23398;&#29983;&#23545;&#26426;&#22120;&#36741;&#21161;&#20889;&#20316;&#30340;&#24577;&#24230;&#21644;&#30683;&#30462;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
EFL Students' Attitudes and Contradictions in a Machine-in-the-loop Activity System. (arXiv:2307.13699v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13699
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;67&#20301;&#26469;&#33258;&#39321;&#28207;&#22235;&#25152;&#20013;&#23398;&#30340;EFL&#23398;&#29983;&#23545;&#26426;&#22120;&#36741;&#21161;&#20889;&#20316;&#30340;&#24577;&#24230;&#21644;&#30683;&#30462;&#12290;&#30740;&#31350;&#21457;&#29616;&#23398;&#29983;&#23545;&#26426;&#22120;&#36741;&#21161;&#20889;&#20316;&#25345;&#31215;&#26497;&#24577;&#24230;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#36127;&#38754;&#25110;&#30683;&#30462;&#30340;&#24863;&#24773;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;EFL&#35838;&#22530;&#20013;&#23454;&#26045;&#26426;&#22120;&#36741;&#21161;&#20889;&#20316;&#30340;&#30410;&#22788;&#21644;&#25361;&#25112;&#65292;&#24314;&#35758;&#25945;&#32946;&#24037;&#20316;&#32773;&#23558;&#27963;&#21160;&#30446;&#26631;&#19982;&#23398;&#29983;&#30340;&#20215;&#20540;&#35266;&#12289;&#35821;&#35328;&#33021;&#21147;&#21644;AI&#33021;&#21147;&#30456;&#19968;&#33268;&#65292;&#20197;&#25552;&#39640;&#23398;&#29983;&#30340;&#27963;&#21160;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24212;&#29992;&#27963;&#21160;&#29702;&#35770;&#25506;&#35752;&#20102;&#26469;&#33258;&#39321;&#28207;&#22235;&#25152;&#20013;&#23398;&#30340;67&#21517;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#65288;EFL&#65289;&#23398;&#29983;&#23545;&#26426;&#22120;&#36741;&#21161;&#20889;&#20316;&#30340;&#24577;&#24230;&#21644;&#30683;&#30462;&#12290;&#22312;&#20889;&#20316;&#36807;&#31243;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25552;&#20379;&#21019;&#20316;&#24819;&#27861;&#12290;&#23398;&#29983;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#34920;&#36798;&#20182;&#20204;&#23545;&#19982;AI&#19968;&#36215;&#20889;&#20316;&#30340;&#24863;&#21463;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#37096;&#20998;&#23398;&#29983;&#25345;&#31215;&#26497;&#24577;&#24230;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#36127;&#38754;&#25110;&#30683;&#30462;&#30340;&#24863;&#24773;&#12290;&#36890;&#36807;&#20027;&#39064;&#20998;&#26512;&#21457;&#29616;&#65292;&#23398;&#29983;&#21644;AI&#20043;&#38388;&#30340;&#30683;&#30462;&#25110;&#32039;&#24352;&#28857;&#28304;&#20110;AI&#30340;&#19981;&#36275;&#65292;&#23398;&#29983;&#22312;&#28909;&#24773;&#21644;&#20559;&#22909;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#20197;&#21450;&#20182;&#20204;&#36861;&#27714;&#35821;&#35328;&#33258;&#20027;&#24615;&#12290;&#30740;&#31350;&#31361;&#20986;&#20102;&#22312;EFL&#35838;&#22530;&#19978;&#23454;&#26045;&#26426;&#22120;&#36741;&#21161;&#20889;&#20316;&#30340;&#30410;&#22788;&#21644;&#25361;&#25112;&#65292;&#24314;&#35758;&#25945;&#32946;&#24037;&#20316;&#32773;&#23558;&#27963;&#21160;&#30446;&#26631;&#19982;&#23398;&#29983;&#30340;&#20215;&#20540;&#35266;&#12289;&#35821;&#35328;&#33021;&#21147;&#21644;AI&#33021;&#21147;&#30456;&#19968;&#33268;&#65292;&#20197;&#25552;&#39640;&#23398;&#29983;&#30340;&#27963;&#21160;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study applies Activity Theory and investigates the attitudes and contradictions of 67 English as a foreign language (EFL) students from four Hong Kong secondary schools towards machine-in-the-loop writing, where artificial intelligence (AI) suggests ideas during composition. Students answered an open-ended question about their feelings on writing with AI. Results revealed mostly positive attitudes, with some negative or mixed feelings. From a thematic analysis, contradictions or points of tension between students and AI stemmed from AI inadequacies, students' balancing enthusiasm with preference, and their striving for language autonomy. The research highlights the benefits and challenges of implementing machine-in-the-loop writing in EFL classrooms, suggesting educators align activity goals with students' values, language abilities, and AI capabilities to enhance students' activity systems.
&lt;/p&gt;</description></item><item><title>GPT-3&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;&#25512;&#29702;&#34920;&#29616;&#26377;&#38480;&#65292;&#38656;&#35201;&#20351;&#29992;&#29420;&#31435;&#30340;&#26816;&#32034;&#27169;&#22411;&#21644;&#36923;&#36753;&#24341;&#25806;&#26469;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.13617</link><description>&lt;p&gt;
GPT-3&#27169;&#22411;&#26159;&#23569;&#26679;&#26412;&#37329;&#34701;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
GPT-3 Models are Few-Shot Financial Reasoners. (arXiv:2307.13617v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13617
&lt;/p&gt;
&lt;p&gt;
GPT-3&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;&#25512;&#29702;&#34920;&#29616;&#26377;&#38480;&#65292;&#38656;&#35201;&#20351;&#29992;&#29420;&#31435;&#30340;&#26816;&#32034;&#27169;&#22411;&#21644;&#36923;&#36753;&#24341;&#25806;&#26469;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#20998;&#26512;&#26159;&#35780;&#20272;&#20844;&#21496;&#19994;&#32489;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#20174;&#19994;&#32773;&#36890;&#36807;&#28145;&#20837;&#30340;&#37327;&#21270;&#20998;&#26512;&#22238;&#31572;&#37329;&#34701;&#38382;&#39064;&#65292;&#20174;&#32780;&#20570;&#20986;&#26377;&#21033;&#21487;&#22270;&#30340;&#25237;&#36164;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#37329;&#34701;&#38382;&#31572;&#26159;&#19968;&#20010;&#38656;&#35201;&#23545;&#25968;&#23383;&#36827;&#34892;&#28145;&#20837;&#25512;&#29702;&#30340;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#25512;&#29702;&#33021;&#21147;&#22914;&#20309;&#12290;&#30446;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#38656;&#35201;&#19968;&#20010;&#26816;&#32034;&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25910;&#38598;&#19982;&#37329;&#34701;&#38382;&#39064;&#30456;&#20851;&#30340;&#20107;&#23454;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#29983;&#25104;&#22120;&#26469;&#29983;&#25104;&#26377;&#25928;&#30340;&#37329;&#34701;&#31243;&#24207;&#21644;&#26368;&#32456;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;GPT-3&#20165;&#20165;&#36890;&#36807;&#23569;&#37327;&#31034;&#20363;&#23601;&#23454;&#29616;&#20102;&#24191;&#27867;&#20219;&#21153;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;GPT-3&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#21457;&#29616;&#29420;&#31435;&#30340;&#26816;&#32034;&#27169;&#22411;&#21644;&#36923;&#36753;&#24341;&#25806;&#20173;&#28982;&#26159;&#23454;&#29616;&#36825;&#19968;&#20219;&#21153;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#23588;&#20854;&#26159;&#30001;&#20110;&#37329;&#34701;&#39046;&#22495;&#30340;&#31934;&#30830;&#24615;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial analysis is an important tool for evaluating company performance. Practitioners work to answer financial questions to make profitable investment decisions, and use advanced quantitative analyses to do so. As a result, Financial Question Answering (QA) is a question answering task that requires deep reasoning about numbers. Furthermore, it is unknown how well pre-trained language models can reason in the financial domain. The current state-of-the-art requires a retriever to collect relevant facts about the financial question from the text and a generator to produce a valid financial program and a final answer. However, recently large language models like GPT-3 have achieved state-of-the-art performance on wide variety of tasks with just a few shot examples. We run several experiments with GPT-3 and find that a separate retrieval model and logic engine continue to be essential components to achieving SOTA performance in this task, particularly due to the precise nature of finan
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FacTool&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#22411;&#38382;&#31572;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#31561;&#22235;&#20010;&#20219;&#21153;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.13528</link><description>&lt;p&gt;
FacTool&#65306;&#29983;&#25104;AI&#20013;&#30340;&#20107;&#23454;&#24615;&#26816;&#27979; &#8212;&#8212; &#19968;&#31181;&#20026;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#22330;&#26223;&#21152;&#24378;&#30340;&#24037;&#20855;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios. (arXiv:2307.13528v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13528
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FacTool&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#22411;&#38382;&#31572;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#31561;&#22235;&#20010;&#20219;&#21153;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#26041;&#20415;&#20102;&#39640;&#36136;&#37327;&#25991;&#26412;&#30340;&#21512;&#25104;&#65292;&#20294;&#20063;&#22312;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#38024;&#23545;&#20197;&#19979;&#38382;&#39064;&#25552;&#20986;&#20102;FacTool&#26694;&#26550;&#65306;&#65288;1&#65289;&#36234;&#26469;&#36234;&#22810;&#30340;&#20219;&#21153;&#30001;&#29983;&#25104;&#27169;&#22411;&#22788;&#29702;&#26102;&#65292;&#23384;&#22312;&#30528;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#30340;&#39118;&#38505;&#65307;&#65288;2&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#24448;&#24448;&#24456;&#38271;&#65292;&#32570;&#20047;&#28165;&#26224;&#23450;&#20041;&#30340;&#32454;&#31890;&#24230;&#20010;&#20307;&#20107;&#23454;&#65307;&#65288;3&#65289;&#22312;&#20107;&#23454;&#26816;&#26597;&#36807;&#31243;&#20013;&#32570;&#20047;&#26126;&#30830;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65288;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#65289;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MediaGPT&#65292;&#19968;&#20010;&#29992;&#20110;&#20013;&#22269;&#23186;&#20307;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#21644;&#19987;&#23478;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;MediaGPT&#22312;&#21508;&#31181;&#20013;&#22269;&#23186;&#20307;&#20219;&#21153;&#19978;&#20248;&#20110;&#20027;&#27969;&#27169;&#22411;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10930</link><description>&lt;p&gt;
MediaGPT&#65306;&#29992;&#20110;&#20013;&#22269;&#23186;&#20307;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MediaGPT : A Large Language Model For Chinese Media. (arXiv:2307.10930v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MediaGPT&#65292;&#19968;&#20010;&#29992;&#20110;&#20013;&#22269;&#23186;&#20307;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#21644;&#19987;&#23478;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;MediaGPT&#22312;&#21508;&#31181;&#20013;&#22269;&#23186;&#20307;&#20219;&#21153;&#19978;&#20248;&#20110;&#20027;&#27969;&#27169;&#22411;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#25991;&#26412;&#21644;&#22522;&#20110;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#23186;&#20307;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23186;&#20307;&#30340;&#29992;&#20363;&#19982;&#36890;&#29992;LLM&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#24322;&#21464;&#24471;&#36234;&#26469;&#36234;&#26126;&#26174;&#65292;&#29305;&#21035;&#26159;&#22312;&#20013;&#25991;&#26041;&#38754;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23186;&#20307;&#39046;&#22495;&#29305;&#23450;LLM&#19982;&#36890;&#29992;LLM&#20043;&#38388;&#30340;&#29420;&#29305;&#29305;&#28857;&#65292;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#25351;&#20196;&#31867;&#22411;&#20197;&#28385;&#36275;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#24182;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#23186;&#20307;&#39046;&#22495;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#12290;&#22522;&#20110;&#36825;&#20123;&#24037;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20013;&#22269;&#23186;&#20307;&#39046;&#22495;&#30340;&#39046;&#22495;&#29305;&#23450;LLM&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#21644;&#19987;&#23478;&#30340;SFT&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#22312;&#39564;&#35777;&#38598;&#19978;&#36827;&#34892;&#20154;&#24037;&#19987;&#23478;&#35780;&#20272;&#21644;&#24378;&#27169;&#22411;&#35780;&#20272;&#65292;&#26412;&#25991;&#35777;&#26126;&#20102;MediaGPT&#22312;&#21508;&#31181;&#20013;&#22269;&#23186;&#20307;&#39046;&#22495;&#20219;&#21153;&#19978;&#20248;&#20110;&#20027;&#27969;&#27169;&#22411;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable capabilities in generating high-quality text and making predictions based on large amounts of data, including the media domain. However, in practical applications, the differences between the media's use cases and the general-purpose applications of LLMs have become increasingly apparent, especially Chinese. This paper examines the unique characteristics of media-domain-specific LLMs compared to general LLMs, designed a diverse set of task instruction types to cater the specific requirements of the domain and constructed unique datasets that are tailored to the media domain. Based on these, we proposed MediaGPT, a domain-specific LLM for the Chinese media domain, training by domain-specific data and experts SFT data. By performing human experts evaluation and strong model evaluation on a validation set, this paper demonstrated that MediaGPT outperforms mainstream models on various Chinese media domain tasks and verifies the importance 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#37327;&#23376;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26032;&#20852;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;4&#20301;&#37327;&#21270;&#27169;&#22411;&#20013;&#36825;&#20123;&#26032;&#20852;&#33021;&#21147;&#20173;&#28982;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2307.08072</link><description>&lt;p&gt;
&#22312;&#37327;&#23376;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26032;&#20852;&#33021;&#21147;&#65306;&#19968;&#39033;&#32463;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study. (arXiv:2307.08072v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#37327;&#23376;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26032;&#20852;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;4&#20301;&#37327;&#21270;&#27169;&#22411;&#20013;&#36825;&#20123;&#26032;&#20852;&#33021;&#21147;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#37096;&#32626;&#21644;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#37327;&#23376;&#21270;&#26041;&#27861;&#26469;&#20943;&#23569;LLMs&#30340;&#20869;&#23384;&#21344;&#29992;&#20197;&#21450;&#22686;&#21152;&#25512;&#29702;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#20302;&#20301;&#37327;&#23376;&#21270;&#26041;&#27861;&#24448;&#24448;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20102;&#35299;&#37327;&#23376;&#21270;&#23545;LLMs&#33021;&#21147;&#30340;&#24433;&#21709;&#26159;&#37325;&#35201;&#30340;&#12290;&#19982;&#20197;&#24448;&#19987;&#27880;&#20110;&#24635;&#20307;&#24615;&#33021;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#37327;&#23376;&#21270;&#23545;&#8220;&#26032;&#20852;&#33021;&#21147;&#8221;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#33021;&#21147;&#26159;&#21306;&#20998;LLMs&#21644;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#37327;&#23376;&#21270;LLMs&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#32500;&#36830;&#36143;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#26032;&#20852;&#33021;&#21147;&#22312;4&#20301;&#37327;&#21270;&#27169;&#22411;&#20013;&#20173;&#28982;&#23384;&#22312;&#65292;&#32780;2&#20301;&#27169;&#22411;&#22312;&#36825;&#20123;&#33021;&#21147;&#19978;&#36935;&#21040;&#20102;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the superior performance, Large Language Models~(LLMs) require significant computational resources for deployment and use. To overcome this issue, quantization methods have been widely applied to reduce the memory footprint of LLMs as well as increasing the inference rate. However, a major challenge is that low-bit quantization methods often lead to performance degradation. It is important to understand how quantization impacts the capacity of LLMs. Different from previous studies focused on overall performance, this work aims to investigate the impact of quantization on \emph{emergent abilities}, which are important characteristics that distinguish LLMs from small language models. Specially, we examine the abilities of in-context learning, chain-of-thought reasoning, and instruction-following in quantized LLMs. Our empirical experiments show that these emergent abilities still exist in 4-bit quantization models, while 2-bit models encounter severe performance degradation on th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06440</link><description>&lt;p&gt;
&#27809;&#26377;&#35757;&#32451;&#23601;&#27809;&#26377;&#25910;&#30410;&#65306;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;Transformer-based&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#24613;&#21095;&#22686;&#21152;&#12290;&#36825;&#19968;&#36235;&#21183;&#20419;&#20351;&#30740;&#31350;&#32773;&#20204;&#24320;&#23637;&#20102;&#38024;&#23545;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#27604;&#26631;&#20934;&#35757;&#32451;&#26356;&#24555;&#22320;&#25913;&#21892;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#19977;&#31867;&#36825;&#26679;&#30340;&#31639;&#27861;&#65306;&#21160;&#24577;&#26550;&#26500;&#65288;&#23618;&#21472;&#12289;&#23618;&#20002;&#24323;&#65289;&#12289;&#25209;&#37327;&#36873;&#25321;&#65288;&#36873;&#25321;&#24615;&#21453;&#21521;&#20256;&#25773;&#12289;RHO&#25439;&#22833;&#65289;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#65288;Lion&#12289;Sophia&#65289;&#12290;&#24403;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;&#22266;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#23545;BERT&#21644;T5&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#30456;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;&#23436;&#20840;&#34928;&#20943;&#23398;&#20064;&#29575;&#30340;&#22522;&#32447;&#32780;&#35328;&#20250;&#28040;&#22833;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#25152;&#26377;&#35745;&#31639;&#26102;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#31216;&#20026;&#21442;&#32771;&#31995;&#32479;&#26102;&#38388;&#30340;&#21442;&#32771;&#26426;&#22120;&#19978;&#65292;&#22312;&#20219;&#24847;&#26426;&#22120;&#19978;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#21327;&#35758;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#65292;&#20197;&#40723;&#21169;&#23545;&#39640;&#25928;&#35757;&#32451;&#30340;&#20005;&#26684;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training p
&lt;/p&gt;</description></item><item><title>MMBench&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#24320;&#21457;&#20840;&#38754;&#30340;&#35780;&#20272;&#27969;&#31243;&#21644;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#32454;&#31890;&#24230;&#33021;&#21147;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.06281</link><description>&lt;p&gt;
MMBench: &#24744;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#20840;&#33021;&#29699;&#21592;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
MMBench: Is Your Multi-modal Model an All-around Player?. (arXiv:2307.06281v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06281
&lt;/p&gt;
&lt;p&gt;
MMBench&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#24320;&#21457;&#20840;&#38754;&#30340;&#35780;&#20272;&#27969;&#31243;&#21644;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#32454;&#31890;&#24230;&#33021;&#21147;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#20449;&#24687;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26377;&#25928;&#35780;&#20272;&#36825;&#20123;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#65292;&#38459;&#30861;&#20102;&#26410;&#26469;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#20256;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#22914;VQAv2&#25110;COCO Caption&#25552;&#20379;&#20102;&#23450;&#37327;&#30340;&#24615;&#33021;&#27979;&#37327;&#65292;&#20294;&#22312;&#32454;&#31890;&#24230;&#33021;&#21147;&#35780;&#20272;&#21644;&#38750;&#40065;&#26834;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#26368;&#36817;&#30340;&#20027;&#35266;&#22522;&#20934;&#27979;&#35797;&#65292;&#22914;OwlEval&#65292;&#36890;&#36807;&#25972;&#21512;&#20154;&#21147;&#36164;&#28304;&#65292;&#23545;&#27169;&#22411;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20294;&#19981;&#21487;&#25193;&#23637;&#24182;&#19988;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MMBench&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#12290;MMBench&#31995;&#32479;&#22320;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#20027;&#35201;&#30001;&#20004;&#20010;&#20803;&#32032;&#32452;&#25104;&#12290;&#31532;&#19968;&#20010;&#20803;&#32032;&#26159;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;&#35780;&#20272;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#31867;&#20284;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large vision-language models have recently achieved remarkable progress, exhibiting great perception and reasoning abilities concerning visual information. However, how to effectively evaluate these large vision-language models remains a major obstacle, hindering future model development. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but suffer from a lack of fine-grained ability assessment and non-robust evaluation metrics. Recent subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, but they are not scalable and display significant bias. In response to these challenges, we propose MMBench, a novel multi-modality benchmark. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of two elements. The first element is a meticulously curated dataset that surpasses existing similar benchmarks in terms of the number and variety of evaluatio
&lt;/p&gt;</description></item><item><title>SAS&#35270;&#39057;&#38382;&#31572;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#35299;&#20915;&#20102;&#35270;&#39057;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;</title><link>http://arxiv.org/abs/2307.04192</link><description>&lt;p&gt;
SAS&#35270;&#39057;&#38382;&#31572;&#65306;&#33258;&#36866;&#24212;&#37319;&#26679;&#29992;&#20110;&#39640;&#25928;&#35270;&#39057;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04192
&lt;/p&gt;
&lt;p&gt;
SAS&#35270;&#39057;&#38382;&#31572;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#35299;&#20915;&#20102;&#35270;&#39057;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#38382;&#31572;&#26159;&#35270;&#39057;&#29702;&#35299;&#39046;&#22495;&#30340;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#37197;&#22791;&#20102;&#35270;&#39057;&#21464;&#25442;&#22120;(Video Transformers)&#65292;&#23454;&#29616;&#20102;&#26102;&#38388;&#24314;&#27169;&#24182;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#65292;&#20294;&#20195;&#20215;&#26159;&#24040;&#22823;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#22240;&#27492;&#22312;&#23454;&#26102;&#24212;&#29992;&#22330;&#26223;&#20013;&#36807;&#20110;&#26114;&#36149;&#12290;&#19968;&#31181;&#32463;&#27982;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#21482;&#23545;&#35270;&#39057;&#30340;&#19968;&#23567;&#37096;&#20998;&#24103;&#36827;&#34892;&#37319;&#26679;&#65292;&#26469;&#20195;&#34920;&#35270;&#39057;&#30340;&#20027;&#35201;&#20869;&#23481;&#65292;&#24182;&#22312;&#36825;&#20123;&#37319;&#26679;&#24103;&#19978;&#35843;&#25972;&#22270;&#20687;-&#25991;&#26412;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#35270;&#39057;&#29702;&#35299;&#27169;&#22411;&#36890;&#24120;&#38543;&#26426;&#37319;&#26679;&#19968;&#32452;&#24103;&#25110;&#29255;&#27573;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#20869;&#37096;&#20851;&#32852;&#24615;&#21644;&#19982;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#26080;&#30446;&#26631;&#30340;&#37319;&#26679;&#21487;&#33021;&#20250;&#36951;&#28431;&#21487;&#20197;&#25512;&#23548;&#20986;&#27491;&#30830;&#31572;&#26696;&#30340;&#20851;&#38190;&#24103;&#65292;&#22312;&#37319;&#26679;&#31232;&#30095;&#31243;&#24230;&#22686;&#21152;&#26102;&#65292;&#24773;&#20917;&#20250;&#21464;&#24471;&#26356;&#31967;&#65292;&#32780;&#38543;&#30528;&#35270;&#39057;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#37319;&#26679;&#31232;&#30095;&#31243;&#24230;&#20063;&#20250;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24103;&#37319;&#26679;&#31574;&#30053;&#65292;&#20998;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
Video question--answering is a fundamental task in the field of video understanding. Although current vision--language models (VLMs) equipped with Video Transformers have enabled temporal modeling and yielded superior results, they are at the cost of huge computational power and thus too expensive to deploy in real-time application scenarios. An economical workaround only samples a small portion of frames to represent the main content of that video and tune an image--text model on these sampled frames. Recent video understanding models usually randomly sample a set of frames or clips, regardless of internal correlations between their visual contents, nor their relevance to the problem. We argue that such kinds of aimless sampling may omit the key frames from which the correct answer can be deduced, and the situation gets worse when the sampling sparsity increases, which always happens as the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#23548;&#25968;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#26041;&#27861;&#65288;DFWE&#65289;&#65292;&#29992;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20256;&#36882;&#12290;&#36890;&#36807;&#22312;&#20960;&#20010;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#30340;&#35282;&#24230;&#19978;&#23545;&#19987;&#23478;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#26080;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#25214;&#21040;&#20102;&#19968;&#20010;&#22909;&#30340;&#27169;&#22411;&#26435;&#37325;&#25554;&#20540;&#65292;&#20174;&#32780;&#22312;FETA-Friends&#19978;&#36229;&#36807;&#20102;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03506</link><description>&lt;p&gt;
&#26080;&#23548;&#25968;&#30340;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Derivative Free Weight-space Ensembling. (arXiv:2307.03506v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#23548;&#25968;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#26041;&#27861;&#65288;DFWE&#65289;&#65292;&#29992;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20256;&#36882;&#12290;&#36890;&#36807;&#22312;&#20960;&#20010;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#30340;&#35282;&#24230;&#19978;&#23545;&#19987;&#23478;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#26080;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#25214;&#21040;&#20102;&#19968;&#20010;&#22909;&#30340;&#27169;&#22411;&#26435;&#37325;&#25554;&#20540;&#65292;&#20174;&#32780;&#22312;FETA-Friends&#19978;&#36229;&#36807;&#20102;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#20004;&#20010;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#20043;&#38388;&#25554;&#20540;&#21487;&#20197;&#22312;&#20219;&#21153;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#25506;&#32034;&#22312;&#20004;&#20010;&#20197;&#19978;&#27169;&#22411;&#20043;&#38388;&#25554;&#20540;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#26377;&#19968;&#20010;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#23548;&#25968;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#26041;&#27861;&#65288;DFWE&#65289;&#65292;&#29992;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20256;&#36882;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21019;&#24314;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#19968;&#32452;&#28304;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#19987;&#23478;&#27169;&#22411;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#20960;&#20010;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#30340;&#35282;&#24230;&#26469;&#22788;&#29702;&#30446;&#26631;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26080;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#22312;&#27169;&#22411;&#26435;&#37325;&#20043;&#38388;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#65292;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#19968;&#20010;&#22909;&#30340;&#25554;&#20540;&#26435;&#37325;&#12290;&#25105;&#20204;&#22312;FETA-Friends&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work suggests that interpolating between the weights of two specialized language models can transfer knowledge between tasks in a way that multi-task learning cannot. However, very few have explored interpolation between more than two models, where each has a distinct knowledge base. In this paper, we introduce Derivative Free Weight-space Ensembling (DFWE), a new few-sample task transfer approach for open-domain dialogue. Our framework creates a set of diverse expert language models trained using a predefined set of source tasks. Next, we finetune each of the expert models on the target task, approaching the target task from several distinct knowledge bases. Finally, we linearly interpolate between the model weights using a gradient-free-optimization algorithm, to efficiently find a good interpolation weighting. We demonstrate the effectiveness of the method on FETA-Friends outperforming the standard pretrain-finetune approach.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#22312;CoDraw&#25968;&#25454;&#38598;&#20013;&#25193;&#23637;&#20102;&#24050;&#26377;&#30340;&#25351;&#20196;&#28548;&#28165;&#35831;&#27714;&#26631;&#35782;&#31526;&#65292;&#27880;&#37322;&#20102;&#19982;&#24213;&#23618;&#23545;&#35805;&#28216;&#25103;&#39033;&#30446;&#21644;&#21487;&#33021;&#21160;&#20316;&#30456;&#20851;&#30340;&#32454;&#33410;&#20449;&#24687;&#65292;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#21644;&#35780;&#20272;&#23545;&#35805;&#20195;&#29702;&#30340;&#20462;&#22797;&#33021;&#21147;</title><link>http://arxiv.org/abs/2306.02377</link><description>&lt;p&gt;
"&#20320;&#26159;&#35753;&#25105;&#32473;&#29399;&#25140;&#30524;&#38236;&#21527;&#65311;" &#22312;CoDraw&#25968;&#25454;&#38598;&#20013;&#27880;&#37322;&#25351;&#20196;&#28548;&#28165;&#35831;&#27714;&#30340;&#20869;&#23481;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
"Are you telling me to put glasses on the dog?'' Content-Grounded Annotation of Instruction Clarification Requests in the CoDraw Dataset. (arXiv:2306.02377v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02377
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#22312;CoDraw&#25968;&#25454;&#38598;&#20013;&#25193;&#23637;&#20102;&#24050;&#26377;&#30340;&#25351;&#20196;&#28548;&#28165;&#35831;&#27714;&#26631;&#35782;&#31526;&#65292;&#27880;&#37322;&#20102;&#19982;&#24213;&#23618;&#23545;&#35805;&#28216;&#25103;&#39033;&#30446;&#21644;&#21487;&#33021;&#21160;&#20316;&#30456;&#20851;&#30340;&#32454;&#33410;&#20449;&#24687;&#65292;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#21644;&#35780;&#20272;&#23545;&#35805;&#20195;&#29702;&#30340;&#20462;&#22797;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#28548;&#28165;&#35831;&#27714;&#26159;&#35299;&#20915;&#27807;&#36890;&#38382;&#39064;&#30340;&#26426;&#21046;&#65292;&#22312;&#25351;&#20196;&#36319;&#38543;&#20114;&#21160;&#20013;&#38750;&#24120;&#23454;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#35748;&#20026;CoDraw&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#33258;&#28982;&#21457;&#29983;iCR&#30340;&#26469;&#28304;&#12290;&#38500;&#20102;&#30830;&#23450;&#20309;&#26102;&#38656;&#35201;&#25552;&#20986;iCR&#22806;&#65292;&#23545;&#35805;&#27169;&#22411;&#36824;&#24212;&#35813;&#33021;&#22815;&#20197;&#21512;&#36866;&#30340;&#24418;&#24335;&#21644;&#20869;&#23481;&#29983;&#25104;&#23427;&#20204;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CoDraw-iCR(v2)&#65292;&#36890;&#36807;&#19982;&#24213;&#23618;&#23545;&#35805;&#28216;&#25103;&#39033;&#30446;&#21644;&#21487;&#33021;&#30340;&#21160;&#20316;&#30456;&#20851;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;iCR&#26631;&#35782;&#31526;&#12290;&#25105;&#20204;&#30340;&#27880;&#37322;&#21487;&#20197;&#29992;&#26469;&#24314;&#27169;&#21644;&#35780;&#20272;&#23545;&#35805;&#20195;&#29702;&#30340;&#20462;&#22797;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction Clarification Requests are a mechanism to solve communication problems, which is very functional in instruction-following interactions. Recent work has argued that the CoDraw dataset is a valuable source of naturally occurring iCRs. Beyond identifying when iCRs should be made, dialogue models should also be able to generate them with suitable form and content. In this work, we introduce CoDraw-iCR (v2), extending the existing iCR identifiers with fine-grained information grounded in the underlying dialogue game items and possible actions. Our annotation can serve to model and evaluate repair capabilities of dialogue agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#26684;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#20110;&#23454;&#20307;&#30340;&#30693;&#35782;&#22914;&#20309;&#34987;&#20445;&#30041;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;OFA&#22312;&#29983;&#25104;&#22270;&#20687;&#30340;&#36807;&#31243;&#20013;&#24536;&#35760;&#20102;&#37096;&#20998;&#23454;&#20307;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2306.02115</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#30693;&#35782;&#25506;&#31350;&#30340;&#34920;&#26684;&#21644;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models. (arXiv:2306.02115v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#26684;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#20110;&#23454;&#20307;&#30340;&#30693;&#35782;&#22914;&#20309;&#34987;&#20445;&#30041;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;OFA&#22312;&#29983;&#25104;&#22270;&#20687;&#30340;&#36807;&#31243;&#20013;&#24536;&#35760;&#20102;&#37096;&#20998;&#23454;&#20307;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#26684;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#20197;&#39564;&#35777;&#33258;&#28982;&#35821;&#35328;&#20013;&#33719;&#21462;&#30340;&#20851;&#20110;&#23454;&#20307;&#30340;&#30693;&#35782;&#22914;&#20309;&#34987;&#20445;&#30041;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#65288;V&amp;L&#65289;&#27169;&#22411;&#20013;&#12290;&#35813;&#20219;&#21153;&#30001;&#20004;&#20010;&#37096;&#20998;&#32452;&#25104;&#65306;&#31532;&#19968;&#20010;&#37096;&#20998;&#26159;&#29983;&#25104;&#19968;&#20010;&#21253;&#21547;&#20851;&#20110;&#23454;&#20307;&#21450;&#20854;&#30456;&#20851;&#22270;&#20687;&#30340;&#30693;&#35782;&#30340;&#34920;&#26684;&#65292;&#31532;&#20108;&#20010;&#37096;&#20998;&#26159;&#26681;&#25454;&#23454;&#20307;&#12289;&#26631;&#39064;&#21644;&#21253;&#21547;&#30456;&#20851;&#23454;&#20307;&#30693;&#35782;&#30340;&#34920;&#26684;&#29983;&#25104;&#22270;&#20687;&#12290;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#24517;&#39035;&#27491;&#30830;&#22320;&#20102;&#35299;&#29992;&#20110;&#25191;&#34892;&#29983;&#25104;&#30340;&#23454;&#20307;&#12290;&#25105;&#20204;&#20174;&#33521;&#25991;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#20013;&#30340;&#32422;200,000&#20010;&#20449;&#24687;&#26694;&#21019;&#24314;&#20102;&#32500;&#22522;&#30334;&#31185;&#34920;&#26684;&#21644;&#22270;&#20687;&#29983;&#25104;&#65288;WikiTIG&#65289;&#25968;&#25454;&#38598;&#26469;&#25191;&#34892;&#25552;&#20986;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;V&amp;L&#27169;&#22411;OFA&#23545;&#20219;&#21153;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;OFA&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#24536;&#35760;&#20102;&#37096;&#20998;&#23454;&#20307;&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#22270;&#20687;&#30456;&#20851;&#24615;&#33021;&#26159;&#19968;&#31181;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a table and image generation task to verify how the knowledge about entities acquired from natural language is retained in Vision &amp; Language (V&amp;L) models. This task consists of two parts: the first is to generate a table containing knowledge about an entity and its related image, and the second is to generate an image from an entity with a caption and a table containing related knowledge of the entity. In both tasks, the model must know the entities used to perform the generation properly. We created the Wikipedia Table and Image Generation (WikiTIG) dataset from about 200,000 infoboxes in English Wikipedia articles to perform the proposed tasks. We evaluated the performance on the tasks with respect to the above research question using the V&amp;L model OFA, which has achieved state-of-the-art results in multiple tasks. Experimental results show that OFA forgets part of its entity knowledge by pre-training as a complement to improve the performance of image relat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#32467;&#21512;&#31526;&#21495;&#34920;&#31034;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#27491;&#35821;&#35328;&#29702;&#35299;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#12290;</title><link>http://arxiv.org/abs/2306.00017</link><description>&lt;p&gt;
&#21521;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#36808;&#36827;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#31526;&#21495;&#36870;&#21521;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale. (arXiv:2306.00017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#32467;&#21512;&#31526;&#21495;&#34920;&#31034;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#27491;&#35821;&#35328;&#29702;&#35299;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#19968;&#20010;&#37324;&#31243;&#30865;&#65292;&#26080;&#21487;&#21542;&#35748;&#22320;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20013;&#35768;&#22810;&#20449;&#20208;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#30495;&#27491;&#30340;&#35821;&#35328;&#29702;&#35299;&#26102;&#65292;&#36825;&#20123;LLM&#30340;&#35768;&#22810;&#38480;&#21046;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#20123;&#38480;&#21046;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24213;&#23618;&#26550;&#26500;&#30340;&#21103;&#20135;&#21697;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#20122;&#31526;&#21495;&#24615;&#36136;&#65292;&#36825;&#20123;&#27169;&#22411;&#33719;&#24471;&#26377;&#20851;&#35821;&#35328;&#22914;&#20309;&#36816;&#20316;&#30340;&#20219;&#20309;&#30693;&#35782;&#37117;&#23558;&#34987;&#22475;&#22312;&#25968;&#21313;&#20159;&#20010;&#24494;&#29305;&#24449;&#65288;&#26435;&#37325;&#65289;&#20013;&#65292;&#20854;&#20013;&#27809;&#26377;&#19968;&#20010;&#21333;&#29420;&#30340;&#29305;&#24449;&#26377;&#24847;&#20041;&#65292;&#20351;&#24471;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#31526;&#21495;&#34920;&#31034;&#30340;&#24378;&#24230;&#19982;&#25105;&#20204;&#35748;&#20026;&#26159;LLMs&#25104;&#21151;&#30340;&#20851;&#38190;&#32467;&#21512;&#36215;&#26469;&#65292;&#21363;&#22312;&#35268;&#27169;&#19978;&#25104;&#21151;&#22320;&#36827;&#34892;&#33258;&#19979;&#32780;&#19978;&#30340;&#35821;&#35328;&#36870;&#21521;&#24037;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#22312;&#31526;&#21495;&#35774;&#32622;&#19979;&#23545;&#35821;&#35328;&#36827;&#34892;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#12290;&#19968;&#20123;&#20316;&#32773;&#25552;&#20986;&#20102;&#36825;&#20010;&#39033;&#30446;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#23558;&#36827;&#34892;&#35814;&#32454;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved a milestone that undenia-bly changed many held beliefs in artificial intelligence (AI). However, there remains many limitations of these LLMs when it comes to true language understanding, limitations that are a byproduct of the under-lying architecture of deep neural networks. Moreover, and due to their subsymbolic nature, whatever knowledge these models acquire about how language works will always be buried in billions of microfeatures (weights), none of which is meaningful on its own, making such models hopelessly unexplainable. To address these limitations, we suggest com-bining the strength of symbolic representations with what we believe to be the key to the success of LLMs, namely a successful bottom-up re-verse engineering of language at scale. As such we argue for a bottom-up reverse engineering of language in a symbolic setting. Hints on what this project amounts to have been suggested by several authors, and we discuss in some detail
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.12247</link><description>&lt;p&gt;
&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#65306;&#19968;&#31181;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12247
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#25152;&#38656;&#30340;&#20132;&#20114;&#22914;&#20309;&#36827;&#34892;&#37327;&#21270;&#65311;&#26368;&#36866;&#21512;&#25429;&#25417;&#36825;&#20123;&#20132;&#20114;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#20160;&#20040;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;&#26469;&#37327;&#21270;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#19977;&#20010;&#34913;&#37327;&#26631;&#20934;&#31216;&#20026;&#22810;&#27169;&#24577;&#20998;&#24067;&#65288;&#25110;&#31616;&#31216;PID&#65289;&#30340;PID&#32479;&#35745;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#20998;&#24067;&#12290;&#20026;&#20102;&#39564;&#35777;PID&#20272;&#35745;&#65292;&#25105;&#20204;&#22312;&#24050;&#30693;PID&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#23383;&#24149;&#30340;&#30452;&#25509;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#65292;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#29983;&#25104;&#30446;&#26631;&#35821;&#35328;&#30340;&#23383;&#24149;&#21450;&#20854;&#26102;&#38388;&#25139;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#35821;&#35328;&#23545;&#19978;&#23545;&#27604;&#32423;&#32852;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#29983;&#20135;&#24037;&#20855;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2209.13192</link><description>&lt;p&gt;
&#33258;&#21160;&#23383;&#24149;&#30340;&#30452;&#25509;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Direct Speech Translation for Automatic Subtitling. (arXiv:2209.13192v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#23383;&#24149;&#30340;&#30452;&#25509;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#65292;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#29983;&#25104;&#30446;&#26631;&#35821;&#35328;&#30340;&#23383;&#24149;&#21450;&#20854;&#26102;&#38388;&#25139;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#35821;&#35328;&#23545;&#19978;&#23545;&#27604;&#32423;&#32852;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#29983;&#20135;&#24037;&#20855;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#23383;&#24149;&#26159;&#23558;&#35270;&#21548;&#20869;&#23481;&#30340;&#35821;&#38899;&#33258;&#21160;&#32763;&#35793;&#20026;&#30701;&#26102;&#25991;&#26412;&#65292;&#21363;&#23383;&#24149;&#21450;&#20854;&#23545;&#24212;&#30340;&#26102;&#38388;&#25139;&#30340;&#20219;&#21153;&#12290;&#29983;&#25104;&#30340;&#23383;&#24149;&#38656;&#35201;&#31526;&#21512;&#31354;&#38388;&#21644;&#26102;&#38388;&#35201;&#27714;&#65292;&#21516;&#26102;&#19982;&#35821;&#38899;&#21516;&#27493;&#65292;&#24182;&#20197;&#20415;&#20110;&#29702;&#35299;&#30340;&#26041;&#24335;&#36827;&#34892;&#20998;&#21106;&#12290;&#37492;&#20110;&#20854;&#30456;&#24403;&#22797;&#26434;&#24615;&#65292;&#35813;&#20219;&#21153;&#36804;&#20170;&#20026;&#27490;&#36890;&#24120;&#36890;&#36807;&#19968;&#31995;&#21015;&#21333;&#29420;&#22788;&#29702;&#25991;&#26412;&#36716;&#24405;&#12289;&#32763;&#35793;&#21644;&#20998;&#21106;&#20026;&#23383;&#24149;&#20197;&#21450;&#39044;&#27979;&#26102;&#38388;&#25139;&#30340;&#32452;&#20214;&#26469;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#23383;&#24149;&#30340;&#31532;&#19968;&#20010;&#30452;&#25509;ST&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#29983;&#25104;&#30446;&#26631;&#35821;&#35328;&#30340;&#23383;&#24149;&#21450;&#20854;&#26102;&#38388;&#25139;&#12290;&#25105;&#20204;&#22312;7&#20010;&#35821;&#35328;&#23545;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#25968;&#25454;&#26465;&#20214;&#19979;&#20248;&#20110;&#32423;&#32852;&#31995;&#32479;&#65292;&#21516;&#26102;&#22312;&#28085;&#30422;&#26032;&#22330;&#26223;&#30340;&#39046;&#22495;&#20869;&#21644;&#26032;&#21457;&#24067;&#30340;&#39046;&#22495;&#22806;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#29983;&#20135;&#24037;&#20855;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic subtitling is the task of automatically translating the speech of audiovisual content into short pieces of timed text, i.e. subtitles and their corresponding timestamps. The generated subtitles need to conform to space and time requirements, while being synchronised with the speech and segmented in a way that facilitates comprehension. Given its considerable complexity, the task has so far been addressed through a pipeline of components that separately deal with transcribing, translating, and segmenting text into subtitles, as well as predicting timestamps. In this paper, we propose the first direct ST model for automatic subtitling that generates subtitles in the target language along with their timestamps with a single model. Our experiments on 7 language pairs show that our approach outperforms a cascade system in the same data condition, also being competitive with production tools on both in-domain and newly-released out-domain benchmarks covering new scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;SIMMC 2.0&#25361;&#25112;&#20013;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#34920;&#36798;&#23545;&#20110;&#27495;&#20041;&#26816;&#27979;&#21644;&#20849;&#35782;&#28040;&#35299;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#22522;&#20110;&#21333;&#27169;&#24577;&#30340;&#20849;&#35782;&#28040;&#35299;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2202.12645</link><description>&lt;p&gt;
&#22312;SIMMC 2.0&#25361;&#25112;&#20013;&#25506;&#32034;&#22810;&#27169;&#24577;&#34920;&#36798;&#23545;&#20110;&#27495;&#20041;&#26816;&#27979;&#21644;&#20849;&#35782;&#28040;&#35299;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring Multi-Modal Representations for Ambiguity Detection &amp; Coreference Resolution in the SIMMC 2.0 Challenge. (arXiv:2202.12645v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;SIMMC 2.0&#25361;&#25112;&#20013;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#34920;&#36798;&#23545;&#20110;&#27495;&#20041;&#26816;&#27979;&#21644;&#20849;&#35782;&#28040;&#35299;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#22522;&#20110;&#21333;&#27169;&#24577;&#30340;&#20849;&#35782;&#28040;&#35299;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20195;&#34920;&#36798;&#65292;&#27604;&#22914;&#20195;&#35789;&#21644;&#25351;&#31034;&#25551;&#36848;&#65292;&#26082;&#21463;&#21069;&#25991;&#30340;&#35821;&#35328;&#35821;&#22659;&#30340;&#24433;&#21709;&#65292;&#20063;&#21463;&#21040;&#24403;&#21069;&#35270;&#35273;&#29615;&#22659;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#21457;&#35328;&#32773;&#30340;&#25351;&#31034;&#25551;&#36848;&#24182;&#19981;&#33021;&#24635;&#26159;&#21807;&#19968;&#22320;&#30830;&#23450;&#25351;&#20195;&#29289;&#65292;&#23548;&#33268;&#38656;&#35201;&#36890;&#36807;&#21518;&#32493;&#30340;&#28548;&#28165;&#20132;&#27969;&#26469;&#28040;&#38500;&#27495;&#20041;&#12290;&#22240;&#27492;&#65292;&#22312;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#26377;&#25928;&#30340;&#27495;&#20041;&#26816;&#27979;&#21644;&#20849;&#35782;&#28040;&#35299;&#23545;&#20110;&#20219;&#21153;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20316;&#20026;SIMMC 2.0&#25361;&#25112;&#30340;&#19968;&#37096;&#20998;&#65292;&#25552;&#20986;&#20102;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;TOD-BERT&#21644;LXMERT&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#19968;&#20123;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#23545;&#27604;&#65292;&#24182;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#65288;1&#65289;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#26469;&#26816;&#27979;&#27495;&#20041;&#65307;&#65288;2&#65289;&#22522;&#20110;&#21333;&#27169;&#24577;&#30340;&#20849;&#35782;&#28040;&#35299;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#26234;&#33021;&#29289;&#20307;&#34920;&#31034;&#26469;&#36991;&#20813;&#38656;&#35201;&#35270;&#35273;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anaphoric expressions, such as pronouns and referential descriptions, are situated with respect to the linguistic context of prior turns, as well as, the immediate visual environment. However, a speaker's referential descriptions do not always uniquely identify the referent, leading to ambiguities in need of resolution through subsequent clarificational exchanges. Thus, effective Ambiguity Detection and Coreference Resolution are key to task success in Conversational AI. In this paper, we present models for these two tasks as part of the SIMMC 2.0 Challenge (Kottur et al. 2021). Specifically, we use TOD-BERT and LXMERT based models, compare them to a number of baselines and provide ablation experiments. Our results show that (1) language models are able to exploit correlations in the data to detect ambiguity; and (2) unimodal coreference resolution models can avoid the need for a vision component, through the use of smart object representations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#21457;&#29616;&#28155;&#21152;RNN&#23618;&#21487;&#20197;&#22312;&#30701;&#25991;&#26412;&#29702;&#35299;&#20013;&#33719;&#24471;&#26368;&#22823;&#30340;&#25913;&#36827;&#65292;&#20294;&#23545;&#20110;&#31867;&#20284;&#30340;BERT&#32467;&#26500;&#24182;&#27809;&#26377;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.11483</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Comparison of Pre-training Language Models. (arXiv:2106.11483v9 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#21457;&#29616;&#28155;&#21152;RNN&#23618;&#21487;&#20197;&#22312;&#30701;&#25991;&#26412;&#29702;&#35299;&#20013;&#33719;&#24471;&#26368;&#22823;&#30340;&#25913;&#36827;&#65292;&#20294;&#23545;&#20110;&#31867;&#20284;&#30340;BERT&#32467;&#26500;&#24182;&#27809;&#26377;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20351;&#24471;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#25991;&#26412;&#21644;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#26469;&#39044;&#35757;&#32451;&#19968;&#31995;&#21015;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#30701;&#25991;&#26412;&#29702;&#35299;&#65292;&#26368;&#22823;&#30340;&#25913;&#36827;&#26159;&#22312;&#21407;&#22987;BERT&#27169;&#22411;&#20013;&#28155;&#21152;RNN&#23618;&#20197;&#25429;&#25417;&#26356;&#22810;&#30340;&#35821;&#22659;&#20449;&#24687;&#12290;&#20294;&#26159;&#32467;&#35770;&#26159;&#65306;&#31867;&#20284;&#30340;BERT&#32467;&#26500;&#23545;&#20110;&#30701;&#25991;&#26412;&#29702;&#35299;&#27809;&#26377;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;[12]&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the development of pre-trained language models has brought natural language processing (NLP) tasks to the new state-of-the-art. In this paper we explore the efficiency of various pre-trained language models. We pre-train a list of transformer-based models with the same amount of text and the same training steps. The experimental results shows that the most improvement upon the origin BERT is adding the RNN-layer to capture more contextual information for short text understanding. But the conclusion is: There are no remarkable improvement for short text understanding for similar BERT structures. Data-centric method[12] can achieve better performance.
&lt;/p&gt;</description></item></channel></rss>