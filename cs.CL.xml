<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#22312;M-pox&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;COVID-19&#27169;&#22411;&#22312;&#21335;&#38750;&#30123;&#33495;&#29369;&#35947;&#26041;&#38754;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2310.04453</link><description>&lt;p&gt;
COVID-19&#21335;&#38750;&#30123;&#33495;&#29369;&#35947;&#27169;&#22411;&#22312;&#24494;&#35843;M-pox&#25512;&#25991;&#19978;&#23637;&#29616;&#20986;&#24615;&#33021;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
COVID-19 South African Vaccine Hesitancy Models Show Boost in Performance Upon Fine-Tuning on M-pox Tweets. (arXiv:2310.04453v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04453
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;M-pox&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;COVID-19&#27169;&#22411;&#22312;&#21335;&#38750;&#30123;&#33495;&#29369;&#35947;&#26041;&#38754;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;2022&#24180;5&#26376;&#24320;&#22987;&#65292;&#38750;&#22320;&#21306;&#24615;&#22269;&#23478;&#25253;&#21578;&#20102;&#22823;&#37327;M-pox&#30149;&#20363;&#65292;&#35753;&#24456;&#22810;&#20154;&#25285;&#24515;M-pox&#30123;&#24773;&#23558;&#36805;&#36895;&#36716;&#21464;&#20026;&#21478;&#19968;&#20010;&#22823;&#27969;&#34892;&#65292;&#32780;COVID-19&#30123;&#24773;&#20173;&#22312;&#32902;&#34384;&#12290;&#37492;&#20110;M-pox&#19982;COVID-19&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#36873;&#25321;&#27979;&#35797;&#22312;&#21335;&#38750;Twitter&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;COVID-19&#27169;&#22411;&#22312;&#25163;&#24037;&#26631;&#35760;&#30340;M-pox&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#24494;&#35843;&#21069;&#21644;&#24494;&#35843;&#21518;&#12290;&#36229;&#36807;20k&#26465;&#26469;&#33258;&#21335;&#38750;&#30340;M-pox&#30456;&#20851;&#25512;&#25991;&#34987;&#25163;&#24037;&#26631;&#35760;&#20026;&#31215;&#26497;&#12289;&#28040;&#26497;&#25110;&#20013;&#24615;&#12290;&#22312;&#23558;&#36825;&#20123;COVID-19&#27169;&#22411;&#24494;&#35843;&#21040;M-pox&#25968;&#25454;&#38598;&#21518;&#65292;F1-score&#25552;&#39640;&#20102;&#36229;&#36807;8%&#65292;&#25509;&#36817;70%&#65292;&#20294;&#20173;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#21644;&#20247;&#25152;&#21608;&#30693;&#30340;&#20998;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;LDA&#30340;&#20027;&#39064;&#24314;&#27169;&#31243;&#24207;&#23558;&#21407;&#22987;COVID-19 RoBERTa&#27169;&#22411;&#21644;&#20854;&#32463;&#36807;&#24494;&#35843;&#30340;&#29256;&#26412;&#30340;&#38169;&#20998;M-pox&#25512;&#25991;&#36827;&#34892;&#27604;&#36739;&#65292;&#36890;&#36807;&#36825;&#20010;&#20998;&#26512;&#65292;&#25105;&#20204;&#33021;&#24471;&#20986;&#20851;&#20110;&#29369;&#35947;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Very large numbers of M-pox cases have, since the start of May 2022, been reported in non-endemic countries leading many to fear that the M-pox Outbreak would rapidly transition into another pandemic, while the COVID-19 pandemic ravages on. Given the similarities of M-pox with COVID-19, we chose to test the performance of COVID-19 models trained on South African twitter data on a hand-labelled M-pox dataset before and after fine-tuning. More than 20k M-pox-related tweets from South Africa were hand-labelled as being either positive, negative or neutral. After fine-tuning these COVID-19 models on the M-pox dataset, the F1-scores increased by more than 8% falling just short of 70%, but still outperforming state-of-the-art models and well-known classification algorithms. An LDA-based topic modelling procedure was used to compare the miss-classified M-pox tweets of the original COVID-19 RoBERTa model with its fine-tuned version, and from this analysis, we were able to draw conclusions on h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#30701;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#23545;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#12290;&#20197;Twitter&#19978;&#30340;&#27668;&#20505;&#21464;&#21270;&#35805;&#39064;&#20026;&#20363;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;5750&#20010;&#25512;&#25991;&#30340;&#26032;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.04452</link><description>&lt;p&gt;
&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#30701;&#25991;&#26412;&#20998;&#31867;&#65306;&#20197;Twitter&#19978;&#30340;&#27668;&#20505;&#21464;&#21270;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Short text classification with machine learning in the social sciences: The case of climate change on Twitter. (arXiv:2310.04452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#30701;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#23545;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#12290;&#20197;Twitter&#19978;&#30340;&#27668;&#20505;&#21464;&#21270;&#35805;&#39064;&#20026;&#20363;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;5750&#20010;&#25512;&#25991;&#30340;&#26032;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20998;&#26512;&#22823;&#37327;&#30340;&#25991;&#26412;&#65292;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#32773;&#36234;&#26469;&#36234;&#38754;&#20020;&#25991;&#26412;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;&#24403;&#26080;&#27861;&#36827;&#34892;&#25163;&#21160;&#26631;&#27880;&#26102;&#65292;&#30740;&#31350;&#32773;&#24517;&#39035;&#25214;&#21040;&#33258;&#21160;&#21270;&#20998;&#31867;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#20026;&#31038;&#20250;&#31185;&#23398;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24037;&#20855;&#31665;&#65292;&#20854;&#24615;&#33021;&#22312;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#26368;&#24120;&#29992;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#20856;&#22411;&#30740;&#31350;&#22330;&#26223;&#65306;&#19968;&#20010;&#30456;&#23545;&#36739;&#23567;&#30340;&#34987;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#24863;&#20852;&#36259;&#30340;&#31867;&#21035;&#24456;&#23569;&#20986;&#29616;&#65292;&#24182;&#19988;&#20316;&#20026;&#19968;&#20010;&#22823;&#22411;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#12290;&#20197;&#27668;&#20505;&#21464;&#21270;&#30340;Twitter&#27807;&#36890;&#20026;&#20363;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#20132;&#21449;&#23398;&#31185;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#22269;&#38469;&#32452;&#32455;&#30340;5750&#20010;&#20851;&#20110;&#27668;&#20505;&#21464;&#21270;&#36825;&#20010;&#39640;&#24230;&#27169;&#31946;&#27010;&#24565;&#30340;&#25512;&#25991;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To analyse large numbers of texts, social science researchers are increasingly confronting the challenge of text classification. When manual labeling is not possible and researchers have to find automatized ways to classify texts, computer science provides a useful toolbox of machine-learning methods whose performance remains understudied in the social sciences. In this article, we compare the performance of the most widely used text classifiers by applying them to a typical research scenario in social science research: a relatively small labeled dataset with infrequent occurrence of categories of interest, which is a part of a large unlabeled dataset. As an example case, we look at Twitter communication regarding climate change, a topic of increasing scholarly interest in interdisciplinary social science research. Using a novel dataset including 5,750 tweets from various international organizations regarding the highly ambiguous concept of climate change, we evaluate the performance o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AutoDAN&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#22312;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#33258;&#21160;&#29983;&#25104;&#38544;&#34109;&#30340;&#36234;&#29425;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#36234;&#29425;&#25216;&#26415;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#38544;&#34109;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04451</link><description>&lt;p&gt;
AutoDAN: &#22312;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#29983;&#25104;&#38544;&#34109;&#30340;&#36234;&#29425;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models. (arXiv:2310.04451v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AutoDAN&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#22312;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#33258;&#21160;&#29983;&#25104;&#38544;&#34109;&#30340;&#36234;&#29425;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#36234;&#29425;&#25216;&#26415;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#38544;&#34109;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26159;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#20915;&#31574;&#24037;&#20855;&#65292;&#36890;&#36807;&#19982;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24191;&#27867;&#23545;&#40784;&#32780;&#21019;&#24314;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25805;&#32437;&#25552;&#31034;&#26469;&#24341;&#21457;&#23545;&#40784;&#30340;LLM&#19981;&#24212;&#32473;&#20986;&#30340;&#24694;&#24847;&#36755;&#20986;&#12290;&#30740;&#31350;&#36234;&#29425;&#25552;&#31034;&#21487;&#20197;&#35753;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;LLM&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#25351;&#23548;&#25105;&#20204;&#22914;&#20309;&#20445;&#25252;&#23427;&#20204;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#36234;&#29425;&#25216;&#26415;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;(1) &#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#25915;&#20987;&#22823;&#37327;&#20381;&#36182;&#25163;&#24037;&#21046;&#20316;&#25552;&#31034;&#65307;(2) &#38544;&#34109;&#24615;&#38382;&#39064;&#65292;&#25915;&#20987;&#20381;&#36182;&#22522;&#20110;&#26631;&#35760;&#30340;&#31639;&#27861;&#29983;&#25104;&#24120;&#24120;&#35821;&#20041;&#26080;&#24847;&#20041;&#30340;&#25552;&#31034;&#65292;&#23481;&#26131;&#36890;&#36807;&#22522;&#26412;&#22256;&#24785;&#24230;&#27979;&#35797;&#26816;&#27979;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24819;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65306;&#33021;&#21542;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#38544;&#34109;&#36234;&#29425;&#25552;&#31034;&#30340;&#26041;&#27861;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoDAN&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce Auto
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#35780;&#20272;&#29702;&#35770;&#21644;&#24212;&#28608;&#19982;&#24212;&#23545;&#36807;&#31243;&#38382;&#21367;&#65288;SCPQ&#65289;&#26469;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24773;&#24863;&#30340;&#24863;&#30693;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#30340;&#21709;&#24212;&#22312;&#35780;&#20272;&#21644;&#24212;&#23545;&#30340;&#21160;&#24577;&#26041;&#38754;&#19982;&#20154;&#31867;&#31867;&#20284;&#65292;&#20294;&#22312;&#20851;&#38190;&#35780;&#20272;&#32500;&#24230;&#19978;&#19982;&#39044;&#27979;&#30340;&#19981;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2310.04450</link><description>&lt;p&gt;
&#20351;&#29992;&#35780;&#20272;&#29702;&#35770;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24773;&#24863;&#30340;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Investigating Large Language Models' Perception of Emotion Using Appraisal Theory. (arXiv:2310.04450v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#35780;&#20272;&#29702;&#35770;&#21644;&#24212;&#28608;&#19982;&#24212;&#23545;&#36807;&#31243;&#38382;&#21367;&#65288;SCPQ&#65289;&#26469;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24773;&#24863;&#30340;&#24863;&#30693;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#30340;&#21709;&#24212;&#22312;&#35780;&#20272;&#21644;&#24212;&#23545;&#30340;&#21160;&#24577;&#26041;&#38754;&#19982;&#20154;&#31867;&#31867;&#20284;&#65292;&#20294;&#22312;&#20851;&#38190;&#35780;&#20272;&#32500;&#24230;&#19978;&#19982;&#39044;&#27979;&#30340;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;&#26368;&#36817;&#20960;&#24180;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#19988;&#29616;&#22312;&#27491;&#34987;&#20844;&#20247;&#20351;&#29992;&#12290;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#19982;&#36825;&#20123;&#31995;&#32479;&#20114;&#21160;&#65292;&#25552;&#39640;&#25105;&#20204;&#23545;&#36825;&#20123;&#40657;&#30418;&#27169;&#22411;&#30340;&#29702;&#35299;&#23588;&#20026;&#20851;&#38190;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#23427;&#20204;&#23545;&#20154;&#31867;&#24515;&#29702;&#26041;&#38754;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#24212;&#23545;&#21644;&#35780;&#20272;&#29702;&#35770;&#20013;&#30340;&#35780;&#20272;&#32500;&#24230;&#26469;&#35843;&#26597;&#23427;&#20204;&#23545;&#24773;&#24863;&#30340;&#24863;&#30693;&#65292;&#20351;&#29992;&#20102;&#24212;&#28608;&#19982;&#24212;&#23545;&#36807;&#31243;&#38382;&#21367;&#65288;SCPQ&#65289;&#12290;SCPQ&#26159;&#19968;&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;&#20020;&#24202;&#24037;&#20855;&#65292;&#30001;&#22810;&#20010;&#38543;&#26102;&#38388;&#32780;&#28436;&#21464;&#19988;&#22312;&#20851;&#38190;&#35780;&#20272;&#21464;&#37327;&#65288;&#22914;&#21487;&#25511;&#24615;&#21644;&#21487;&#21464;&#24615;&#65289;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#25925;&#20107;&#32452;&#25104;&#12290;&#25105;&#20204;&#23558;SCPQ&#24212;&#29992;&#20110;OpenAI&#30340;&#19977;&#20010;&#26368;&#26032;LLM&#65288;davinci-003&#12289;ChatGPT&#21644;GPT-4&#65289;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#35780;&#20272;&#29702;&#35770;&#21644;&#20154;&#31867;&#25968;&#25454;&#30340;&#39044;&#27979;&#36827;&#34892;&#23545;&#27604;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#30340;&#21709;&#24212;&#22312;&#35780;&#20272;&#21644;&#24212;&#23545;&#30340;&#21160;&#24577;&#26041;&#38754;&#19982;&#20154;&#31867;&#31867;&#20284;&#65292;&#20294;&#23427;&#20204;&#30340;&#21709;&#24212;&#22312;&#20851;&#38190;&#35780;&#20272;&#32500;&#24230;&#19978;&#19982;&#39044;&#27979;&#30340;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLM) like ChatGPT have significantly advanced in recent years and are now being used by the general public. As more people interact with these systems, improving our understanding of these black box models is crucial, especially regarding their understanding of human psychological aspects. In this work, we investigate their emotion perception through the lens of appraisal and coping theory using the Stress and Coping Process Questionaire (SCPQ). SCPQ is a validated clinical instrument consisting of multiple stories that evolve over time and differ in key appraisal variables such as controllability and changeability. We applied SCPQ to three recent LLMs from OpenAI, davinci-003, ChatGPT, and GPT-4 and compared the results with predictions from the appraisal theory and human data. The results show that LLMs' responses are similar to humans in terms of dynamics of appraisal and coping, but their responses did not differ along key appraisal dimensions as predicted by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoFT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19982;&#26377;&#23475;&#26597;&#35810;&#22788;&#20110;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#30340;&#30456;&#20284;&#26597;&#35810;&#19978;&#36827;&#34892;&#20195;&#29702;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#26469;&#25913;&#21892;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#20256;&#36882;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04445</link><description>&lt;p&gt;
LoFT: &#29992;&#20110;&#25913;&#36827;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#21487;&#20256;&#36882;&#24615;&#30340;&#26412;&#22320;&#20195;&#29702;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model. (arXiv:2310.04445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoFT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19982;&#26377;&#23475;&#26597;&#35810;&#22788;&#20110;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#30340;&#30456;&#20284;&#26597;&#35810;&#19978;&#36827;&#34892;&#20195;&#29702;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#26469;&#25913;&#21892;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#40784;&#21487;&#20197;&#36890;&#36807;&#38468;&#21152;&#29305;&#21046;&#30340;&#25915;&#20987;&#21518;&#32512;&#21644;&#26377;&#23475;&#26597;&#35810;&#26469;&#35268;&#36991;&#65292;&#20197;&#24341;&#21457;&#26377;&#23475;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#23545;&#26410;&#30693;&#29305;&#24449;&#30340;&#31169;&#26377;&#30446;&#26631;&#27169;&#22411;&#36827;&#34892;&#25915;&#20987;&#65292;&#21487;&#20197;&#20351;&#29992;&#20844;&#20849;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#26469;&#26500;&#24314;&#25915;&#20987;&#65292;&#24182;&#23558;&#25104;&#21151;&#30340;&#25915;&#20987;&#20174;&#20844;&#20849;&#20195;&#29702;&#20256;&#36882;&#21040;&#31169;&#26377;&#30446;&#26631;&#27169;&#22411;&#12290;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#21462;&#20915;&#20110;&#20195;&#29702;&#27169;&#22411;&#33021;&#22815;&#22810;&#22823;&#31243;&#24230;&#19978;&#36924;&#36817;&#31169;&#26377;&#27169;&#22411;&#12290;&#25105;&#20204;&#20551;&#35774;&#65292;&#23545;&#20110;&#25915;&#20987;&#21487;&#20256;&#36882;&#24615;&#26469;&#35828;&#65292;&#21482;&#35201;&#20195;&#29702;&#33021;&#22815;&#22312;&#26377;&#23475;&#26597;&#35810;&#30340;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#20869;&#36924;&#36817;&#30446;&#26631;&#27169;&#22411;&#21363;&#21487;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26412;&#22320;&#24494;&#35843;&#65288;LoFT&#65289;&#8221;&#65292;&#21363;&#22312;&#19982;&#26377;&#23475;&#26597;&#35810;&#22788;&#20110;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#30340;&#30456;&#20284;&#26597;&#35810;&#19978;&#36827;&#34892;&#20195;&#29702;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#20197;&#20943;&#23567;&#20195;&#29702;&#21644;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#19977;&#31181;&#20419;&#20351;&#31169;&#26377;&#30446;&#26631;&#27169;&#22411;&#21464;&#24471;&#26131;&#21463;&#25915;&#20987;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been shown that Large Language Model (LLM) alignments can be circumvented by appending specially crafted attack suffixes with harmful queries to elicit harmful responses. To conduct attacks against private target models whose characterization is unknown, public models can be used as proxies to fashion the attack, with successful attacks being transferred from public proxies to private target models. The success rate of attack depends on how closely the proxy model approximates the private model. We hypothesize that for attacks to be transferrable, it is sufficient if the proxy can approximate the target model in the neighborhood of the harmful query. Therefore, in this paper, we propose \emph{Local Fine-Tuning (LoFT)}, \textit{i.e.}, fine-tuning proxy models on similar queries that lie in the lexico-semantic neighborhood of harmful queries to decrease the divergence between the proxy and target models. First, we demonstrate three approaches to prompt private target models to obt
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04444</link><description>&lt;p&gt;
&#39764;&#27861;&#35789;&#26159;&#20160;&#20040;&#65311;LLM&#25552;&#31034;&#30340;&#25511;&#21046;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#22312;LLM&#30340;&#37096;&#32626;&#20013;&#26159;&#26377;&#25928;&#21644;&#37325;&#35201;&#30340;&#65292;&#20294;&#22312;&#25968;&#23398;&#19978;&#29702;&#35299;&#19981;&#36275;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#25552;&#31034;&#34987;&#35748;&#20026;&#26159;&#35843;&#33410;LLM&#36755;&#20986;&#20998;&#24067;&#30340;&#25511;&#21046;&#21464;&#37327;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;token&#24207;&#21015;&#65292;&#26159;&#21542;&#24635;&#23384;&#22312;&#19968;&#20010;&#25105;&#20204;&#21487;&#20197;&#28155;&#21152;&#30340;&#25552;&#31034;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65311;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#26368;&#20248;&#25552;&#31034;&#31216;&#20026;&#39764;&#27861;&#35789;&#65292;&#22240;&#20026;&#28155;&#21152;&#25552;&#31034;&#20250;&#23548;&#33268;LLM&#36755;&#20986;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#22914;&#26524;&#23384;&#22312;&#39764;&#27861;&#35789;&#65292;&#25105;&#20204;&#33021;&#21542;&#25214;&#21040;&#23427;&#20204;&#65311;&#22914;&#26524;&#21487;&#20197;&#65292;&#23427;&#20204;&#30340;&#29305;&#24615;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#25552;&#20379;&#20102;&#23558;&#25511;&#21046;&#29702;&#35770;&#24212;&#29992;&#20110;&#33258;&#27880;&#24847;&#21147;&#22836;&#30340;&#20998;&#26512;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#20854;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20989;&#25968;&#20026;&#21487;&#25511;&#21046;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#20511;&#37492;&#25511;&#21046;&#29702;&#35770;&#26469;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;$k-\epsilon$&#21487;&#25511;&#21046;&#24615;&#30340;&#25351;&#26631;&#65292;&#29992;&#20110;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of the self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k-\epsilon$ controllability to characterize LLM steerability. We comput
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#20154;&#31867;&#31227;&#21160;&#38382;&#39064;&#22238;&#31572;&#65288;MobQA&#65289;&#65292;&#26088;&#22312;&#35753;&#26234;&#33021;&#31995;&#32479;&#20174;&#31227;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#65292;&#22635;&#34917;&#20102;&#20851;&#20110;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#20026;&#31227;&#21160;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#24102;&#26469;&#20102;&#26032;&#30340;&#33539;&#24335;&#21464;&#38761;&#12290;</title><link>http://arxiv.org/abs/2310.04443</link><description>&lt;p&gt;
&#20154;&#31867;&#31227;&#21160;&#38382;&#39064;&#22238;&#31572;&#65288;&#23637;&#26395;&#35770;&#25991;&#65289;
&lt;/p&gt;
&lt;p&gt;
Human Mobility Question Answering (Vision Paper). (arXiv:2310.04443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#20154;&#31867;&#31227;&#21160;&#38382;&#39064;&#22238;&#31572;&#65288;MobQA&#65289;&#65292;&#26088;&#22312;&#35753;&#26234;&#33021;&#31995;&#32479;&#20174;&#31227;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#65292;&#22635;&#34917;&#20102;&#20851;&#20110;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#20026;&#31227;&#21160;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#24102;&#26469;&#20102;&#26032;&#30340;&#33539;&#24335;&#21464;&#38761;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#31995;&#32479;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#26681;&#25454;&#32473;&#23450;&#30340;&#30693;&#35782;&#28304;&#65288;&#20363;&#22914;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#22270;&#20687;&#65289;&#23398;&#20064;&#22238;&#31572;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#30740;&#31350;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#25366;&#25496;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#23545;&#20110;&#26234;&#33021;&#22478;&#24066;&#35268;&#21010;&#12289;&#30123;&#24773;&#31649;&#29702;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#31561;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24341;&#20837;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#20154;&#31867;&#31227;&#21160;&#38382;&#39064;&#22238;&#31572;&#65288;MobQA&#65289;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#35753;&#26234;&#33021;&#31995;&#32479;&#20174;&#31227;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#12290;&#35813;&#20219;&#21153;&#20026;&#31227;&#21160;&#39044;&#27979;&#30740;&#31350;&#24102;&#26469;&#20102;&#26032;&#30340;&#33539;&#24335;&#21464;&#38761;&#65292;&#24182;&#36827;&#19968;&#27493;&#20419;&#36827;&#20102;&#20154;&#31867;&#31227;&#21160;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25903;&#25345;&#36825;&#20010;&#26032;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#36825;&#31687;&#23637;&#26395;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#21021;&#27493;&#35774;&#35745;&#21644;&#19968;&#20010;&#28508;&#22312;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering (QA) systems have attracted much attention from the artificial intelligence community as they can learn to answer questions based on the given knowledge source (e.g., images in visual question answering). However, the research into question answering systems with human mobility data remains unexplored. Mining human mobility data is crucial for various applications such as smart city planning, pandemic management, and personalised recommendation system. In this paper, we aim to tackle this gap and introduce a novel task, that is, human mobility question answering (MobQA). The aim of the task is to let the intelligent system learn from mobility data and answer related questions. This task presents a new paradigm change in mobility prediction research and further facilitates the research of human mobility recommendation systems. To better support this novel research topic, this vision paper also proposes an initial design of the dataset and a potential deep learning mod
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#21644;&#29983;&#25104;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#28436;&#36827;&#21382;&#31243;&#65292;&#21253;&#25324;&#26089;&#26399;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24341;&#20837;&#65292;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#20197;&#35299;&#20915;&#20559;&#35265;&#21644;&#26292;&#38706;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;&#36824;&#35752;&#35770;&#20102;&#24494;&#35843;&#31574;&#30053;&#12289;&#25511;&#21046;&#20195;&#30721;&#21644;&#22522;&#20110;&#27169;&#26495;&#30340;&#29983;&#25104;&#30340;&#37325;&#22823;&#36129;&#29486;&#65292;&#20197;&#21450;&#20844;&#24179;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#21644;&#20302;&#36164;&#28304;&#36866;&#24212;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04438</link><description>&lt;p&gt;
&#19968;&#20221;&#20851;&#20110;&#25552;&#31034;&#24037;&#31243;&#30340;&#31616;&#35201;&#21382;&#21490;: &#21033;&#29992;&#35821;&#35328;&#27169;&#22411; (arXiv:2310.04438v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
A Brief History of Prompt: Leveraging Language Models. (arXiv:2310.04438v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04438
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#21644;&#29983;&#25104;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#28436;&#36827;&#21382;&#31243;&#65292;&#21253;&#25324;&#26089;&#26399;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24341;&#20837;&#65292;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#20197;&#35299;&#20915;&#20559;&#35265;&#21644;&#26292;&#38706;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;&#36824;&#35752;&#35770;&#20102;&#24494;&#35843;&#31574;&#30053;&#12289;&#25511;&#21046;&#20195;&#30721;&#21644;&#22522;&#20110;&#27169;&#26495;&#30340;&#29983;&#25104;&#30340;&#37325;&#22823;&#36129;&#29486;&#65292;&#20197;&#21450;&#20844;&#24179;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#21644;&#20302;&#36164;&#28304;&#36866;&#24212;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20013;&#25552;&#31034;&#24037;&#31243;&#21644;&#29983;&#25104;&#30340;&#28436;&#36827;&#21382;&#31243;&#12290;&#20174;&#26089;&#26399;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#24320;&#22987;&#65292;&#25105;&#20204;&#36861;&#28335;&#20102;&#36825;&#20123;&#24180;&#26469;&#22609;&#36896;&#25552;&#31034;&#24037;&#31243;&#30340;&#20851;&#38190;&#21457;&#23637;&#12290;2015&#24180;&#24341;&#20837;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#24443;&#24213;&#25913;&#21464;&#20102;&#35821;&#35328;&#29702;&#35299;&#65292;&#25512;&#21160;&#20102;&#21487;&#25511;&#24615;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36827;&#27493;&#12290;&#38543;&#21518;&#22312;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#26041;&#38754;&#30340;&#31361;&#30772;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#25552;&#31034;&#24037;&#31243;&#65292;&#35299;&#20915;&#20102;&#26292;&#38706;&#20559;&#24046;&#21644;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#37325;&#28857;&#32771;&#23519;&#20102;2018&#24180;&#21644;2019&#24180;&#30340;&#37325;&#22823;&#36129;&#29486;&#65292;&#38598;&#20013;&#22312;&#24494;&#35843;&#31574;&#30053;&#12289;&#25511;&#21046;&#20195;&#30721;&#21644;&#22522;&#20110;&#27169;&#26495;&#30340;&#29983;&#25104;&#19978;&#12290;&#26412;&#35770;&#25991;&#36824;&#35752;&#35770;&#20102;&#20844;&#24179;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21512;&#20316;&#20197;&#21450;&#20302;&#36164;&#28304;&#36866;&#24212;&#30340;&#26085;&#30410;&#37325;&#35201;&#24615;&#12290;&#22312;2020&#24180;&#21644;2021&#24180;&#65292;&#19978;&#19979;&#25991;&#25552;&#31034;&#21644;&#36801;&#31227;&#23398;&#20064;&#21464;&#24471;&#31361;&#20986;&#65292;&#32780;2022&#24180;&#21644;2023&#24180;&#35265;&#35777;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive exploration of the evolution of prompt engineering and generation in the field of natural language processing (NLP). Starting from the early language models and information retrieval systems, we trace the key developments that have shaped prompt engineering over the years. The introduction of attention mechanisms in 2015 revolutionized language understanding, leading to advancements in controllability and context-awareness. Subsequent breakthroughs in reinforcement learning techniques further enhanced prompt engineering, addressing issues like exposure bias and biases in generated text. We examine the significant contributions in 2018 and 2019, focusing on fine-tuning strategies, control codes, and template-based generation. The paper also discusses the growing importance of fairness, human-AI collaboration, and low-resource adaptation. In 2020 and 2021, contextual prompting and transfer learning gained prominence, while 2022 and 2023 witnessed the e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24605;&#32500;&#20256;&#25773;&#65288;TP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#31867;&#27604;&#38382;&#39064;&#21644;&#21033;&#29992;&#31867;&#27604;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03965</link><description>&lt;p&gt;
&#24605;&#32500;&#20256;&#25773;&#65306;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#26041;&#27861;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22797;&#26434;&#25512;&#29702;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models. (arXiv:2310.03965v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03965
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24605;&#32500;&#20256;&#25773;&#65288;TP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#31867;&#27604;&#38382;&#39064;&#21644;&#21033;&#29992;&#31867;&#27604;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#26080;&#27861;&#37325;&#29992;&#35299;&#20915;&#31867;&#20284;&#38382;&#39064;&#30340;&#35265;&#35299;&#65292;&#24182;&#19988;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#32047;&#31215;&#20102;&#38169;&#35823;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#27714;LLMs&#20174;&#38646;&#24320;&#22987;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24605;&#32500;&#20256;&#25773;&#8221;&#65288;TP&#65289;&#65292;&#23427;&#25506;&#32034;&#31867;&#20284;&#38382;&#39064;&#24182;&#21033;&#29992;&#23427;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;LLMs&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20123;&#31867;&#27604;&#38382;&#39064;&#19982;&#36755;&#20837;&#38382;&#39064;&#30456;&#20851;&#65292;&#20855;&#26377;&#21487;&#37325;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#23558;&#35299;&#20915;&#20808;&#21069;&#31867;&#20284;&#38382;&#39064;&#30340;&#35265;&#35299;&#20256;&#25773;&#20197;&#28608;&#21457;&#26032;&#30340;&#38382;&#39064;&#35299;&#20915;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;TP&#39318;&#20808;&#25552;&#31034;LLMs&#25552;&#20986;&#24182;&#35299;&#20915;&#19968;&#32452;&#19982;&#36755;&#20837;&#38382;&#39064;&#30456;&#20851;&#30340;&#31867;&#27604;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;TP&#37325;&#29992;&#31867;&#27604;&#38382;&#39064;&#30340;&#32467;&#26524;&#30452;&#25509;&#20135;&#29983;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#25110;&#32773;&#25512;&#23548;&#19968;&#20010;&#30693;&#35782;&#23494;&#38598;&#22411;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \textit{from scratch}. To address these issues, we propose \textbf{\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs. These analogous problems are related to the input one, with reusable solutions and problem-solving strategies. Thus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. To achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#22312;&#32654;&#27954;&#22303;&#33879;&#35821;&#35328;&#19978;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;ASR&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#26174;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03639</link><description>&lt;p&gt;
&#35780;&#20272;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23545;&#32654;&#27954;&#22303;&#33879;&#35821;&#35328;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating Self-Supervised Speech Representations for Indigenous American Languages. (arXiv:2310.03639v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#22312;&#32654;&#27954;&#22303;&#33879;&#35821;&#35328;&#19978;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;ASR&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#26174;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#20110;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#37327;&#30340;&#26080;&#26631;&#27880;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#35780;&#20272;&#26041;&#38754;&#65292;&#30446;&#21069;&#30340;&#36827;&#23637;&#20165;&#38598;&#20013;&#22312;&#32771;&#34385;&#33521;&#35821;&#30340;&#21333;&#35821;&#27169;&#22411;&#19978;&#12290;&#24456;&#23569;&#26377;&#27169;&#22411;&#32771;&#34385;&#20854;&#20182;&#35821;&#35328;&#65292;&#23588;&#20854;&#26159;&#22303;&#33879;&#35821;&#35328;&#12290;&#25105;&#20204;&#22312;ASRU 2023 ML-SUPERB Challenge&#30340;&#26032;&#35821;&#35328;&#36187;&#36947;&#20013;&#25552;&#20132;&#20102;&#19968;&#20010;&#29992;&#20110;Quechua&#30340;ASR&#35821;&#26009;&#24211;&#65292;&#23427;&#26159;&#19968;&#31181;&#21335;&#32654;&#22303;&#33879;&#35821;&#35328;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#22411;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#22312;Quechua&#21644;&#20854;&#20182;6&#31181;&#22303;&#33879;&#35821;&#35328;&#65288;&#22914;Guarani&#21644;Bribri&#65289;&#30340;&#20302;&#36164;&#28304;ASR&#19978;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22823;&#35268;&#27169;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#30340;&#28508;&#22312;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of self-supervision to speech representation learning has garnered significant interest in recent years, due to its scalability to large amounts of unlabeled data. However, much progress, both in terms of pre-training and downstream evaluation, has remained concentrated in monolingual models that only consider English. Few models consider other languages, and even fewer consider indigenous ones. In our submission to the New Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR corpus for Quechua, an indigenous South American Language. We benchmark the efficacy of large SSL models on Quechua, along with 6 other indigenous languages such as Guarani and Bribri, on low-resource ASR. Our results show surprisingly strong performance by state-of-the-art SSL models, showing the potential generalizability of large-scale models to real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#32771;&#34385;&#26356;&#24369;&#27169;&#22411;&#30340;&#31572;&#26696;&#19968;&#33268;&#24615;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#38382;&#39064;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#33410;&#32422;&#20351;&#29992;&#26356;&#24378;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.03094</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning. (arXiv:2310.03094v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#32771;&#34385;&#26356;&#24369;&#27169;&#22411;&#30340;&#31572;&#26696;&#19968;&#33268;&#24615;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#38382;&#39064;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#33410;&#32422;&#20351;&#29992;&#26356;&#24378;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#31181;&#24378;&#22823;&#30340;&#24615;&#33021;&#36890;&#24120;&#20276;&#38543;&#30528;&#20351;&#29992;&#20184;&#36153;API&#26381;&#21153;&#30340;&#39640;&#26114;&#36153;&#29992;&#12290;&#26412;&#25991;&#30340;&#30740;&#31350;&#21160;&#26426;&#26159;&#20026;&#20102;&#30740;&#31350;&#26500;&#24314;LLM&#32423;&#32852;&#20197;&#33410;&#32422;&#20351;&#29992;LLM&#30340;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#36827;&#34892;&#25512;&#29702;&#65288;&#20363;&#22914;&#25968;&#23398;&#12289;&#22240;&#26524;&#25512;&#29702;&#65289;&#20219;&#21153;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#32423;&#32852;&#31649;&#36947;&#36981;&#24490;&#19968;&#20010;&#30452;&#35266;&#30340;&#24605;&#24819;&#65292;&#21363;&#31616;&#21333;&#30340;&#38382;&#39064;&#21487;&#20197;&#30001;&#19968;&#20010;&#26356;&#24369;&#20294;&#26356;&#23454;&#24800;&#30340;LLM&#26469;&#35299;&#20915;&#65292;&#32780;&#21482;&#26377;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#25165;&#38656;&#35201;&#26356;&#24378;&#22823;&#12289;&#26356;&#26114;&#36149;&#30340;LLM&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#20915;&#31574;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#26356;&#24369;&#30340;LLM&#30340;&#8220;&#31572;&#26696;&#19968;&#33268;&#24615;&#8221;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#31572;&#26696;&#37319;&#26679;&#21644;&#19968;&#33268;&#24615;&#26816;&#26597;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#24605;&#32500;&#34920;&#31034;&#65288;&#21363;&#36830;&#32493;&#24605;&#32500;&#21644;&#31243;&#24207;&#24605;&#32500;&#65289;&#30340;&#28151;&#21512;&#12290;&#36890;&#36807;&#22312;&#20845;&#20010;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5-turbo&#21644;GPT-4&#20316;&#20026;&#36739;&#24369;&#30340;&#27169;&#22411;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the "answer consistency" of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#19981;&#21516;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#30340;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02469</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#33391;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Be Good Privacy Protection Learners. (arXiv:2310.02469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#19981;&#21516;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#30340;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20351;&#29992;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#65292;&#21019;&#24314;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29305;&#23450;&#39046;&#22495;&#30340;&#24494;&#35843;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#25935;&#24863;&#30340;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#65288;PII&#65289;&#12290;&#22312;&#27809;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#24494;&#35843; LLMs &#20250;&#23384;&#22312;&#20449;&#24687;&#27844;&#38706;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26032;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#27604;&#22914;&#35821;&#26009;&#24211;&#31574;&#23637;&#12289;&#22522;&#20110;&#24809;&#32602;&#30340;&#38750;&#27010;&#28982;&#24615;&#35757;&#32451;&#25439;&#22833;&#20197;&#21450;&#22522;&#20110;&#25351;&#20196;&#30340;&#24494;&#35843;&#31561;&#31561;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#65292;&#26174;&#31034;&#20986;&#24456;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of Large Language Models (LLMs) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. Nevertheless, such domain-specific fine-tuning data often contains sensitive personally identifiable information (PII). Direct fine-tuning LLMs on this data without privacy protection poses a risk of leakage. To address this challenge, we introduce Privacy Protection Language Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects domain-specific knowledge while safeguarding data privacy. Our work offers a theoretical analysis for model design and delves into various techniques such as corpus curation, penalty-based unlikelihood in training loss, and instruction-based tuning, etc. Extensive experiments across diverse datasets and scenarios demonstrate the effectiveness of our approaches. In particular, instruction tuning with both positive and negative examples, stands out as a promising method, eff
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#21517;&#20026;MedTem&#30340;&#20020;&#24202;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#21644;&#26102;&#38388;&#20851;&#31995;&#25552;&#21462;&#65292;&#26469;&#25552;&#21462;&#20020;&#24202;&#25991;&#26412;&#20013;&#30340;&#33647;&#29289;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#26356;&#22909;&#22320;&#20102;&#35299;&#24739;&#32773;&#30340;&#27835;&#30103;&#21382;&#21490;&#12290;</title><link>http://arxiv.org/abs/2310.02229</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#33647;&#29289;&#21644;&#26102;&#38388;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Extraction of Medication and Temporal Relation from Clinical Text using Neural Language Models. (arXiv:2310.02229v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#21517;&#20026;MedTem&#30340;&#20020;&#24202;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#21644;&#26102;&#38388;&#20851;&#31995;&#25552;&#21462;&#65292;&#26469;&#25552;&#21462;&#20020;&#24202;&#25991;&#26412;&#20013;&#30340;&#33647;&#29289;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#26356;&#22909;&#22320;&#20102;&#35299;&#24739;&#32773;&#30340;&#27835;&#30103;&#21382;&#21490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#25991;&#26412;&#22312;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#65288;EMRs&#65289;&#20013;&#34920;&#31034;&#65292;&#21253;&#21547;&#20016;&#23500;&#30340;&#21307;&#23398;&#20449;&#24687;&#65292;&#23545;&#30142;&#30149;&#39044;&#27979;&#12289;&#20010;&#24615;&#21270;&#20449;&#24687;&#25512;&#33616;&#12289;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#20197;&#21450;&#33647;&#29289;&#27169;&#24335;&#25366;&#25496;&#21644;&#27979;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#33647;&#29289;&#25552;&#21462;&#21644;&#26102;&#38388;&#20851;&#31995;&#20998;&#31867;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#19968;&#27493;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#26356;&#22909;&#22320;&#20102;&#35299;&#24739;&#32773;&#30340;&#27835;&#30103;&#21382;&#21490;&#12290;&#20026;&#20102;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33647;&#29289;&#25552;&#21462;&#21644;&#26102;&#38388;&#20851;&#31995;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23545;&#21517;&#20026;MedTem&#30340;&#20020;&#24202;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#20351;&#29992;&#20102;&#20960;&#31181;&#20808;&#36827;&#30340;&#23398;&#20064;&#32467;&#26500;&#65292;&#21253;&#25324;BiLSTM-CRF&#21644;CNN-BiLSTM&#65292;&#20197;&#21450;&#29992;&#20110;&#26102;&#38388;&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#30340;BERT-CNN&#65292;&#27492;&#22806;&#65292;&#36824;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#35789;&#23884;&#20837;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#22871;&#21518;&#22788;&#29702;&#35268;&#21017;&#65292;&#20197;&#29983;&#25104;&#20851;&#20110;&#33647;&#29289;&#21644;&#26102;&#38388;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical texts, represented in electronic medical records (EMRs), contain rich medical information and are essential for disease prediction, personalised information recommendation, clinical decision support, and medication pattern mining and measurement. Relation extractions between medication mentions and temporal information can further help clinicians better understand the patients' treatment history. To evaluate the performances of deep learning (DL) and large language models (LLMs) in medication extraction and temporal relations classification, we carry out an empirical investigation of \textbf{MedTem} project using several advanced learning structures including BiLSTM-CRF and CNN-BiLSTM for a clinical domain named entity recognition (NER), and BERT-CNN for temporal relation extraction (RE), in addition to the exploration of different word embedding techniques. Furthermore, we also designed a set of post-processing roles to generate structured output on medications and the tempor
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#12290;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#20998;&#26512;&#21644;&#32467;&#26500;&#35782;&#21035;&#65292;&#26500;&#24314;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#24615;&#30340;KV&#32531;&#23384;&#65292;&#36890;&#36807;&#28165;&#38500;&#21644;&#20002;&#24323;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#21450;&#21482;&#23545;&#29305;&#23450;&#30340;&#27880;&#24847;&#21147;&#22836;&#20351;&#29992;&#26631;&#20934;KV&#32531;&#23384;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20869;&#23384;&#21344;&#29992;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2310.01801</link><description>&lt;p&gt;
&#27169;&#22411;&#21578;&#35785;&#20320;&#35813;&#20002;&#24323;&#20160;&#20040;&#65306;&#36866;&#24212;&#24615;KV&#32531;&#23384;&#21387;&#32553;&#29992;&#20110;LLMs
&lt;/p&gt;
&lt;p&gt;
Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. (arXiv:2310.01801v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01801
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#12290;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#20998;&#26512;&#21644;&#32467;&#26500;&#35782;&#21035;&#65292;&#26500;&#24314;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#24615;&#30340;KV&#32531;&#23384;&#65292;&#36890;&#36807;&#28165;&#38500;&#21644;&#20002;&#24323;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#21450;&#21482;&#23545;&#29305;&#23450;&#30340;&#27880;&#24847;&#21147;&#22836;&#20351;&#29992;&#26631;&#20934;KV&#32531;&#23384;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20869;&#23384;&#21344;&#29992;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#25512;&#29702;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#19982;&#20256;&#32479;&#30340;KV&#32531;&#23384;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#20998;&#26512;&#26469;&#35782;&#21035;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#22522;&#20110;&#35782;&#21035;&#20986;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#26500;&#24314;KV&#32531;&#23384;&#65306;&#22312;&#24378;&#35843;&#26412;&#22320;&#19978;&#19979;&#25991;&#30340;&#27880;&#24847;&#21147;&#22836;&#19978;&#28165;&#38500;&#38271;&#36317;&#31163;&#19978;&#19979;&#25991;&#65292;&#22312;&#20197;&#29305;&#27530;&#26631;&#35760;&#20026;&#20013;&#24515;&#30340;&#27880;&#24847;&#21147;&#22836;&#19978;&#20002;&#24323;&#38750;&#29305;&#27530;&#26631;&#35760;&#65292;&#24182;&#19988;&#20165;&#23545;&#24191;&#27867;&#20851;&#27880;&#25152;&#26377;&#26631;&#35760;&#30340;&#27880;&#24847;&#21147;&#22836;&#20351;&#29992;&#26631;&#20934;&#30340;KV&#32531;&#23384;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#27880;&#24847;&#21147;&#20998;&#26512;&#26469;&#25351;&#23548;&#33258;&#36866;&#24212;KV&#32531;&#23384;&#30340;&#26500;&#24314;&#65292;FastGen&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#24494;&#35843;&#25110;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#37096;&#32626;&#12290;&#22312;&#21508;&#31181;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;FastGen&#22312;GPU&#20869;&#23384;&#28040;&#32791;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with 
&lt;/p&gt;</description></item><item><title>PORTIA&#26159;&#19968;&#20010;&#26088;&#22312;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22120;&#30340;&#20301;&#32622;&#20559;&#24046;&#30340;&#23545;&#40784;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#31572;&#26696;&#20998;&#21106;&#25104;&#22810;&#20010;&#29255;&#27573;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#23545;&#40784;&#65292;&#28982;&#21518;&#23558;&#20854;&#21512;&#24182;&#22238;&#19968;&#20010;&#21333;&#19968;&#30340;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01432</link><description>&lt;p&gt;
&#20998;&#21106;&#19982;&#21512;&#24182;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20301;&#32622;&#20559;&#24046;&#36827;&#34892;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Split and Merge: Aligning Position Biases in Large Language Model based Evaluators. (arXiv:2310.01432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01432
&lt;/p&gt;
&lt;p&gt;
PORTIA&#26159;&#19968;&#20010;&#26088;&#22312;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22120;&#30340;&#20301;&#32622;&#20559;&#24046;&#30340;&#23545;&#40784;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#31572;&#26696;&#20998;&#21106;&#25104;&#22810;&#20010;&#29255;&#27573;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#23545;&#40784;&#65292;&#28982;&#21518;&#23558;&#20854;&#21512;&#24182;&#22238;&#19968;&#20010;&#21333;&#19968;&#30340;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20316;&#20026;&#33258;&#21160;&#21270;&#35780;&#20272;&#22120;&#65292;&#29992;&#20110;&#35780;&#20272;AI&#31995;&#32479;&#29983;&#25104;&#30340;&#31572;&#26696;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#22312;&#20351;&#29992;&#23545;&#27604;&#35780;&#20272;&#20505;&#36873;&#31572;&#26696;&#26102;&#23384;&#22312;&#20301;&#32622;&#20559;&#24046;&#25110;&#19981;&#19968;&#33268;&#24615;&#65292;&#26080;&#35270;&#20869;&#23481;&#32780;&#20559;&#21521;&#20110;&#31532;&#19968;&#20010;&#25110;&#31532;&#20108;&#20010;&#31572;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PORTIA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#40784;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#27169;&#25311;&#20154;&#31867;&#30340;&#27604;&#36739;&#31574;&#30053;&#65292;&#20197;&#36731;&#37327;&#32423;&#20294;&#26377;&#25928;&#30340;&#26041;&#24335;&#26657;&#20934;&#20301;&#32622;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PORTIA&#23558;&#31572;&#26696;&#20998;&#21106;&#25104;&#22810;&#20010;&#29255;&#27573;&#65292;&#23545;&#27604;&#20505;&#36873;&#31572;&#26696;&#20013;&#30340;&#30456;&#20284;&#20869;&#23481;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#23558;&#23427;&#20204;&#21512;&#24182;&#22238;&#19968;&#20010;&#21333;&#19968;&#30340;&#25552;&#31034;&#65292;&#20197;&#20379;LLMs&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#30340;LLM&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;11,520&#20010;&#31572;&#26696;&#23545;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;PORTIA&#26174;&#33879;&#25552;&#39640;&#20102;&#25152;&#26377;&#27169;&#22411;&#21644;&#23545;&#27604;&#24418;&#24335;&#30340;&#19968;&#33268;&#24615;&#29575;&#65292;&#24179;&#22343;&#30456;&#23545;&#25913;&#36827;&#29575;&#36798;&#21040;47.46%&#12290;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#65292;PORTIA&#20351;&#24471;LLMs&#33021;&#22815;&#35780;&#20272;&#20013;&#23545;&#20301;&#32622;&#20559;&#24046;&#36827;&#34892;&#26657;&#20934;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, these LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. To address this limitation, we propose PORTIA, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Specifically, PORTIA splits the answers into multiple segments, aligns similar content across candidate answers, and then merges them back into a single prompt for evaluation by LLMs. We conducted extensive experiments with six diverse LLMs to evaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances the consistency rates for all the models and comparison forms tested, achieving an average relative improvement of 47.46%. Remarkably, PORTIA enables le
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#26041;&#27861;RA-DIT&#65292;&#36890;&#36807;&#20026;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26816;&#32034;&#33021;&#21147;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;&#19968;&#26159;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#65292;&#20108;&#26159;&#26356;&#26032;&#26816;&#32034;&#22120;&#20197;&#36820;&#22238;&#26356;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#27599;&#20010;&#27493;&#39588;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#27493;&#39588;&#21487;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.01352</link><description>&lt;p&gt;
RA-DIT: &#26816;&#32034;&#22686;&#24378;&#30340;&#21452;&#37325;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
RA-DIT: Retrieval-Augmented Dual Instruction Tuning. (arXiv:2310.01352v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#26041;&#27861;RA-DIT&#65292;&#36890;&#36807;&#20026;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26816;&#32034;&#33021;&#21147;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;&#19968;&#26159;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#65292;&#20108;&#26159;&#26356;&#26032;&#26816;&#32034;&#22120;&#20197;&#36820;&#22238;&#26356;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#27599;&#20010;&#27493;&#39588;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#27493;&#39588;&#21487;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RALMs&#65289;&#36890;&#36807;&#35775;&#38382;&#22806;&#37096;&#25968;&#25454;&#23384;&#20648;&#20013;&#30340;&#38271;&#23614;&#21644;&#26368;&#26032;&#30693;&#35782;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#26500;&#24314;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#26816;&#32034;&#29305;&#23450;&#20462;&#25913;&#26469;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#65292;&#35201;&#20040;&#20351;&#29992;&#20107;&#21518;&#38598;&#25104;&#25968;&#25454;&#23384;&#20648;&#30340;&#26041;&#27861;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#29702;&#24819;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#26041;&#27861;&#8212;&#8212;&#26816;&#32034;&#22686;&#24378;&#30340;&#21452;&#37325;&#25351;&#20196;&#35843;&#20248;&#65288;RA-DIT&#65289;&#65292;&#36890;&#36807;&#20026;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26816;&#32034;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#24494;&#35843;&#27493;&#39588;&#65306;&#65288;1&#65289;&#19968;&#20010;&#26356;&#26032;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#65292;&#65288;2&#65289;&#21478;&#19968;&#20010;&#26356;&#26032;&#26816;&#32034;&#22120;&#20197;&#36820;&#22238;&#26356;&#30456;&#20851;&#30340;&#32467;&#26524;&#65292;&#31526;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#22312;&#38656;&#35201;&#30693;&#35782;&#21033;&#29992;&#21644;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27599;&#20010;&#38454;&#27573;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#21487;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25910;&#30410;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#26159;RA-DIT 65B&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#21487;&#35299;&#37322;&#30340;&#26102;&#38388;&#25512;&#29702;&#65292;&#26088;&#22312;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#25139;&#19978;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#38656;&#35201;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#21644;&#22810;&#20010;&#20107;&#20214;&#30340;&#32508;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.01074</link><description>&lt;p&gt;
&#37325;&#36820;&#26410;&#26469;&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#30340;&#26102;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models. (arXiv:2310.01074v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#21487;&#35299;&#37322;&#30340;&#26102;&#38388;&#25512;&#29702;&#65292;&#26088;&#22312;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#25139;&#19978;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#38656;&#35201;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#21644;&#22810;&#20010;&#20107;&#20214;&#30340;&#32508;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#25512;&#29702;&#26159;&#19968;&#39033;&#20851;&#38190;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#21487;&#20197;&#22312;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#20379;&#23545;&#26102;&#38388;&#25935;&#24863;&#29615;&#22659;&#30340;&#32454;&#33268;&#29702;&#35299;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;LLM&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#26102;&#38388;&#25512;&#29702;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20294;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#35832;&#22914;&#26102;&#38388;&#34920;&#36798;&#21644;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;&#31561;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#25552;&#21462;&#30452;&#25509;&#21644;&#36807;&#21435;&#30340;&#26102;&#38388;&#32447;&#32034;&#65292;&#24182;&#36827;&#34892;&#31616;&#21333;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;&#32771;&#34385;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#20107;&#20214;&#39044;&#27979;&#65289;&#26102;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#65292;&#36825;&#38656;&#35201;&#23545;&#20107;&#20214;&#36827;&#34892;&#22810;&#27493;&#30340;&#26102;&#38388;&#25512;&#29702;&#65292;&#24182;&#23545;&#26410;&#26469;&#26102;&#38388;&#25139;&#36827;&#34892;&#39044;&#27979;&#12290;&#29616;&#26377;&#26041;&#27861;&#30340;&#21478;&#19968;&#20010;&#26174;&#33879;&#23616;&#38480;&#26159;&#23427;&#20204;&#26080;&#27861;&#25552;&#20379;&#25512;&#29702;&#36807;&#31243;&#30340;&#35828;&#26126;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#26102;&#38388;&#25512;&#29702;&#30340;&#31532;&#19968;&#20010;&#20219;&#21153;&#65292;&#29992;&#20110;&#22522;&#20110;&#19978;&#19979;&#25991;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#25139;&#19978;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#36825;&#38656;&#35201;&#23545;&#22810;&#20010;&#20107;&#20214;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal reasoning is a crucial NLP task, providing a nuanced understanding of time-sensitive contexts within textual data. Although recent advancements in LLMs have demonstrated their potential in temporal reasoning, the predominant focus has been on tasks such as temporal expression and temporal relation extraction. These tasks are primarily designed for the extraction of direct and past temporal cues and to engage in simple reasoning processes. A significant gap remains when considering complex reasoning tasks such as event forecasting, which requires multi-step temporal reasoning on events and prediction on the future timestamp. Another notable limitation of existing methods is their incapability to provide an illustration of their reasoning process, hindering explainability. In this paper, we introduce the first task of explainable temporal reasoning, to predict an event's occurrence at a future timestamp based on context which requires multiple reasoning over multiple events, and
&lt;/p&gt;</description></item><item><title>GeRA&#26159;&#19968;&#31181;&#26631;&#31614;&#25928;&#29575;&#30340;&#20960;&#20309;&#27491;&#21017;&#21270;&#23545;&#40784;&#26041;&#27861;&#65292;&#21033;&#29992;&#26410;&#37197;&#23545;&#25968;&#25454;&#30340;&#27969;&#24418;&#20960;&#20309;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#23545;&#40784;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00672</link><description>&lt;p&gt;
GeRA: &#26631;&#31614;&#25928;&#29575;&#30340;&#20960;&#20309;&#27491;&#21017;&#21270;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
GeRA: Label-Efficient Geometrically Regularized Alignment. (arXiv:2310.00672v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00672
&lt;/p&gt;
&lt;p&gt;
GeRA&#26159;&#19968;&#31181;&#26631;&#31614;&#25928;&#29575;&#30340;&#20960;&#20309;&#27491;&#21017;&#21270;&#23545;&#40784;&#26041;&#27861;&#65292;&#21033;&#29992;&#26410;&#37197;&#23545;&#25968;&#25454;&#30340;&#27969;&#24418;&#20960;&#20309;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#23545;&#40784;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#23558;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#34701;&#20837;&#23884;&#20837;&#31354;&#38388;&#32467;&#26500;&#20013;&#12290;&#20026;&#20102;&#20855;&#26377;&#31867;&#20284;&#30340;&#20449;&#24687;&#37327;&#65292;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#25104;&#23545;&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#21644;&#35757;&#32451;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#20960;&#20309;&#27491;&#21017;&#21270;&#23545;&#40784;&#26041;&#27861;&#65288;GeRA&#65289;&#65292;&#20197;&#26631;&#31614;&#39640;&#25928;&#30340;&#26041;&#24335;&#23545;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26410;&#37197;&#23545;&#65288;&#26410;&#26631;&#35760;&#65289;&#25968;&#25454;&#30340;&#27969;&#24418;&#20960;&#20309;&#26469;&#25552;&#39640;&#23545;&#40784;&#24615;&#33021;&#12290;&#20026;&#20102;&#38450;&#27490;&#22312;&#23545;&#40784;&#36807;&#31243;&#20013;&#23545;&#23616;&#37096;&#20960;&#20309;&#36896;&#25104;&#22833;&#30495;&#65292;&#21487;&#33021;&#30772;&#22351;&#35821;&#20041;&#37051;&#22495;&#32467;&#26500;&#24182;&#23548;&#33268;&#26410;&#35266;&#23519;&#21040;&#30340;&#23545;&#20135;&#29983;&#38169;&#20301;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20309;&#25439;&#22833;&#39033;&#12290;&#35813;&#39033;&#22522;&#20110;&#19968;&#20010;&#25429;&#33719;&#21333;&#27169;&#24577;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#23616;&#37096;&#27969;&#24418;&#20960;&#20309;&#30340;&#25193;&#25955;&#31639;&#23376;&#26500;&#24314;&#12290;GeRA&#26159;&#27169;&#24577;&#26080;&#20851;&#30340;&#65292;&#22240;&#27492;&#21487;&#20197;&#29992;&#20110;&#23545;&#40784;&#20219;&#20309;&#25968;&#25454;&#27169;&#24577;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#26469;&#25903;&#25345;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained unimodal encoders incorporate rich semantic information into embedding space structures. To be similarly informative, multi-modal encoders typically require massive amounts of paired data for alignment and training. We introduce a semi-supervised Geometrically Regularized Alignment (GeRA) method to align the embedding spaces of pretrained unimodal encoders in a label-efficient way. Our method leverages the manifold geometry of unpaired (unlabeled) data to improve alignment performance. To prevent distortions to local geometry during the alignment process, potentially disrupting semantic neighborhood structures and causing misalignment of unobserved pairs, we introduce a geometric loss term. This term is built upon a diffusion operator that captures the local manifold geometry of the unimodal pretrained encoders. GeRA is modality-agnostic and thus can be used to align pretrained encoders from any data modalities. We provide empirical evidence to the effectiveness of our metho
&lt;/p&gt;</description></item><item><title>SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00533</link><description>&lt;p&gt;
SELF&#65306;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
SELF: Language-Driven Self-Evolution for Large Language Model. (arXiv:2310.00533v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00533
&lt;/p&gt;
&lt;p&gt;
SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#21331;&#36234;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23398;&#20064;&#21644;&#25512;&#21160;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#8212;&#8212;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;&#30340;&#36335;&#24452;&#20173;&#28982;&#26410;&#30693;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;"SELF"&#65288;&#24102;&#26377;&#35821;&#35328;&#21453;&#39304;&#30340;&#33258;&#20027;&#36827;&#21270;&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLM&#33021;&#22815;&#19981;&#26029;&#22320;&#33258;&#25105;&#36827;&#21270;&#12290;&#27492;&#22806;&#65292;SELF&#21033;&#29992;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#12289;&#20840;&#38754;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#31934;&#30830;&#23450;&#20301;&#21709;&#24212;&#25913;&#36827;&#30340;&#39046;&#22495;&#65292;&#24182;&#25552;&#39640;&#33258;&#20027;&#36827;&#21270;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;SELF&#39318;&#20808;&#36827;&#34892;&#20803;&#25216;&#33021;&#23398;&#20064;&#65292;&#19987;&#27880;&#20110;&#33258;&#25105;&#21453;&#39304;&#21644;&#33258;&#25105;&#31934;&#28860;&#12290;&#36825;&#20123;&#20803;&#25216;&#33021;&#26159;&#20851;&#38190;&#65292;&#24341;&#23548;&#27169;&#22411;&#22312;&#33258;&#21046;&#25968;&#25454;&#30340;&#25345;&#32493;&#35757;&#32451;&#21608;&#26399;&#20013;&#36827;&#34892;&#21518;&#32493;&#30340;&#33258;&#25105;&#36827;&#21270;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#20869;&#22312;&#33021;&#21147;&#12290;&#22312;&#32473;&#23450;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;SELF&#20351;&#27169;&#22411;&#20855;&#22791;&#20102;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased remarkable versatility across diverse domains. However, the pathway toward autonomous model development, a cornerstone for achieving human-level learning and advancing autonomous AI, remains largely uncharted. We introduce an innovative approach, termed "SELF" (Self-Evolution with Language Feedback). This methodology empowers LLMs to undergo continual self-evolution. Furthermore, SELF employs language-based feedback as a versatile and comprehensive evaluative tool, pinpointing areas for response refinement and bolstering the stability of self-evolutionary training. Initiating with meta-skill learning, SELF acquires foundational meta-skills with a focus on self-feedback and self-refinement. These meta-skills are critical, guiding the model's subsequent self-evolution through a cycle of perpetual training with self-curated data, thereby enhancing its intrinsic abilities. Given unlabeled instructions, SELF equips the model with the capability to
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;LLMs&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;LLMs&#30340;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#26377;&#36739;&#22823;&#24433;&#21709;&#65292;&#32780;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.00378</link><description>&lt;p&gt;
&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#20215;&#20540;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Measuring Value Understanding in Language Models through Discriminator-Critique Gap. (arXiv:2310.00378v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00378
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;LLMs&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;LLMs&#30340;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#26377;&#36739;&#22823;&#24433;&#21709;&#65292;&#32780;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20043;&#38388;&#28508;&#22312;&#19981;&#19968;&#33268;&#24615;&#30340;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#22797;&#26434;&#21644;&#36866;&#24212;&#24615;&#65292;&#35780;&#20272;&#23427;&#20204;&#23545;&#36825;&#20123;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#26159;&#22797;&#26434;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#30495;&#27491;&#29702;&#35299;LLMs&#20013;&#30340;&#20215;&#20540;&#35266;&#38656;&#35201;&#32771;&#34385;&#21040;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#21644;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#20004;&#20010;&#26041;&#38754;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#26469;&#23450;&#37327;&#35780;&#20272;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#21644;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#12290;&#21033;&#29992;&#26045;&#29926;&#33576;&#20215;&#20540;&#35266;&#35843;&#26597;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#35780;&#20272;&#20215;&#20540;&#35266;&#30340;&#26631;&#20934;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32771;&#23519;&#20102;LLMs&#30340;&#36755;&#20986;&#19982;&#22522;&#20934;&#31572;&#26696;&#20043;&#38388;&#30340;&#20215;&#20540;&#35266;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;LLMs&#30340;&#22238;&#31572;&#19982;GPT-4&#30340;&#27880;&#37322;&#22312;&#20215;&#20540;&#35748;&#30693;&#21407;&#22240;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20116;&#20010;&#20195;&#34920;&#24615;LLMs&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#30340;&#24433;&#21709;&#36739;&#22823;&#65292;&#20294;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs) have heightened concerns about their potential misalignment with human values. However, evaluating their grasp of these values is complex due to their intricate and adaptable nature. We argue that truly understanding values in LLMs requires considering both "know what" and "know why". To this end, we present the Value Understanding Measurement (VUM) framework that quantitatively assess both "know what" and "know why" by measuring the discriminator-critique gap related to human values. Using the Schwartz Value Survey, we specify our evaluation values and develop a thousand-level dialogue dataset with GPT-4. Our assessment looks at both the value alignment of LLM's outputs compared to baseline answers and how LLM responses align with reasons for value recognition versus GPT-4's annotations. We evaluate five representative LLMs and provide strong evidence that the scaling law significantly impacts "know what" but not much on "know why", 
&lt;/p&gt;</description></item><item><title>RelBERT&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#30456;&#23545;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#20851;&#31995;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;RoBERTa&#27169;&#22411;&#65292;&#21482;&#38656;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#21363;&#21487;&#25429;&#25417;&#20851;&#31995;&#30456;&#20284;&#24615;&#65292;&#24182;&#22312;&#31867;&#27604;&#22522;&#20934;&#20013;&#21462;&#24471;&#26368;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.00299</link><description>&lt;p&gt;
RelBERT: &#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
RelBERT: Embedding Relations with Language Models. (arXiv:2310.00299v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00299
&lt;/p&gt;
&lt;p&gt;
RelBERT&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#30456;&#23545;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#20851;&#31995;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;RoBERTa&#27169;&#22411;&#65292;&#21482;&#38656;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#21363;&#21487;&#25429;&#25417;&#20851;&#31995;&#30456;&#20284;&#24615;&#65292;&#24182;&#22312;&#31867;&#27604;&#22522;&#20934;&#20013;&#21462;&#24471;&#26368;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#35775;&#38382;&#26377;&#20851;&#19981;&#21516;&#27010;&#24565;&#21644;&#23454;&#20307;&#20043;&#38388;&#20851;&#31995;&#30340;&#32972;&#26223;&#30693;&#35782;&#12290;&#23613;&#31649;&#30693;&#35782;&#22270;&#35889;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#28385;&#36275;&#36825;&#31181;&#38656;&#27714;&#65292;&#20294;&#30693;&#35782;&#22270;&#35889;&#19981;&#21487;&#36991;&#20813;&#22320;&#26159;&#19981;&#23436;&#25972;&#30340;&#65292;&#20854;&#20851;&#31995;&#27169;&#24335;&#36890;&#24120;&#36807;&#20110;&#31895;&#31890;&#24230;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21017;&#25928;&#29575;&#20302;&#19979;&#19988;&#38590;&#20197;&#25511;&#21046;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#30456;&#23545;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#20851;&#31995;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#30452;&#25509;&#24494;&#35843;&#36974;&#30422;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;RoBERTa&#65289;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#20165;&#20351;&#29992;&#20102;&#23569;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;RelBERT&#65292;&#20197;&#24847;&#22806;&#31934;&#32454;&#30340;&#26041;&#24335;&#25429;&#25417;&#20102;&#20851;&#31995;&#30456;&#20284;&#24615;&#65292;&#20351;&#25105;&#20204;&#22312;&#31867;&#27604;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;&#20851;&#38190;&#26159;&#65292;RelBERT&#33021;&#22815;&#23545;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#30340;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#39046;&#22495;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications need access to background knowledge about how different concepts and entities are related. Although Knowledge Graphs (KG) and Large Language Models (LLM) can address this need to some extent, KGs are inevitably incomplete and their relational schema is often too coarse-grained, while LLMs are inefficient and difficult to control. As an alternative, we propose to extract relation embeddings from relatively small language models. In particular, we show that masked language models such as RoBERTa can be straightforwardly fine-tuned for this purpose, using only a small amount of training data. The resulting model, which we call RelBERT, captures relational similarity in a surprisingly fine-grained way, allowing us to set a new state-of-the-art in analogy benchmarks. Crucially, RelBERT is capable of modelling relations that go well beyond what the model has seen during training. For instance, we obtained strong results on relations between named entities with a model that 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#30456;&#23545;&#21453;&#39304;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20248;&#21270;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#31639;&#27861;&#35774;&#35745;&#21644;&#20989;&#25968;&#36924;&#36817;&#12290;</title><link>http://arxiv.org/abs/2310.00212</link><description>&lt;p&gt;
&#20004;&#20004;&#37051;&#36817;&#31574;&#30053;&#20248;&#21270;: &#21033;&#29992;&#30456;&#23545;&#21453;&#39304;&#36827;&#34892;LLM&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment. (arXiv:2310.00212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#30456;&#23545;&#21453;&#39304;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20248;&#21270;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#31639;&#27861;&#35774;&#35745;&#21644;&#20989;&#25968;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#39044;&#20808;&#35757;&#32451;&#26469;&#33719;&#21462;&#24191;&#27867;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25509;&#35302;&#21040;&#20302;&#36136;&#37327;&#25968;&#25454;&#65292;LLMs&#21487;&#33021;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#19981;&#19968;&#33268;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#24341;&#23548;LLMs&#26397;&#30528;&#26377;&#30410;&#34892;&#20026;&#26041;&#21521;&#21457;&#23637;&#30340;&#20027;&#23548;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#20854;&#20013;Proximal Policy Optimization&#65288;PPO&#65289;&#26159;&#40664;&#35748;&#30340;RL&#20248;&#21270;&#22120;&#12290;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;PPO&#22312;&#20248;&#21270;&#22522;&#20110;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#65292;&#30001;&#20110;&#38656;&#35201;&#26657;&#20934;&#22870;&#21169;&#23610;&#24230;&#65292;PPO&#23545;&#20110;&#21253;&#21547;&#30456;&#21516;&#20559;&#22909;&#20449;&#24687;&#30340;&#31561;&#20215;&#22870;&#21169;&#20989;&#25968;&#19981;&#20855;&#22791;&#19981;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;&#36712;&#36857;&#30340;&#20248;&#21270;&#30456;&#27604;&#65292;PPO&#23545;&#20110;&#22522;&#20110;&#20196;&#29260;&#30340;&#26356;&#26032;&#30340;&#38656;&#27714;&#24341;&#20837;&#20102;&#20989;&#25968;&#36924;&#36817;&#21644;&#31639;&#27861;&#35774;&#35745;&#26041;&#38754;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#30456;&#23545;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;Pairwise Proximal Policy Optimization&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) can acquire extensive world knowledge through pre-training on large corpora. However, due to exposure to low-quality data, LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior involves Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy Optimization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO has limitations when optimizing rewards trained from comparison-based loss. Primarily, PPO is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. Additionally, PPO's necessity for token-wise updates introduces complexity in both function approximation and algorithm design compared to trajectory-wise optimization. This paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, Pair
&lt;/p&gt;</description></item><item><title>&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20854;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.15857</link><description>&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
A Survey on Image-text Multimodal Models. (arXiv:2309.15857v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15857
&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20854;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#25104;&#20026;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#39046;&#22495;&#65292;&#23548;&#33268;&#20102;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#26412;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30340;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#20215;&#20540;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#21457;&#23637;&#37324;&#31243;&#30865;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#23427;&#20204;&#30340;&#21457;&#23637;&#20998;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#65292;&#22522;&#20110;&#23427;&#20204;&#34987;&#24341;&#20837;&#30340;&#26102;&#38388;&#21644;&#23545;&#23398;&#31185;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#20219;&#21153;&#22312;&#23398;&#26415;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#26222;&#21450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#19982;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30456;&#20851;&#30340;&#20219;&#21153;&#21010;&#20998;&#20026;&#20116;&#20010;&#20027;&#35201;&#31867;&#22411;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#38416;&#26126;&#20102;&#27599;&#20010;&#31867;&#21035;&#20869;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#20851;&#38190;&#25216;&#26415;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#20294;&#20173;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amidst the evolving landscape of artificial intelligence, the convergence of visual and textual information has surfaced as a crucial frontier, leading to the advent of image-text multimodal models. This paper provides a comprehensive review of the evolution and current state of image-text multimodal models, exploring their application value, challenges, and potential research trajectories. Initially, we revisit the basic concepts and developmental milestones of these models, introducing a novel classification that segments their evolution into three distinct phases, based on their time of introduction and subsequent impact on the discipline. Furthermore, based on the tasks' significance and prevalence in the academic landscape, we propose a categorization of the tasks associated with image-text multimodal models into five major types, elucidating the recent progress and key technologies within each category. Despite the remarkable accomplishments of these models, numerous challenges a
&lt;/p&gt;</description></item><item><title>Lyra&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#20462;&#27491;&#21644;&#29468;&#24819;&#20462;&#27491;&#20004;&#31181;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#24182;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15806</link><description>&lt;p&gt;
Lyra: &#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#30340;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Lyra: Orchestrating Dual Correction in Automated Theorem Proving. (arXiv:2309.15806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15806
&lt;/p&gt;
&lt;p&gt;
Lyra&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#20462;&#27491;&#21644;&#29468;&#24819;&#20462;&#27491;&#20004;&#31181;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#24182;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#25506;&#32034;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#24187;&#35273;&#30340;&#20943;&#36731;&#21644;&#36890;&#36807;&#35777;&#26126;&#22120;&#38169;&#35823;&#28040;&#24687;&#30340;&#32454;&#21270;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#22686;&#24378;LLMs&#22312;&#35813;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Lyra&#65292;&#19968;&#31181;&#37319;&#29992;&#20004;&#31181;&#19981;&#21516;&#20462;&#27491;&#26426;&#21046;&#30340;&#26032;&#26694;&#26550;&#65306;&#24037;&#20855;&#20462;&#27491;&#65288;TC&#65289;&#21644;&#29468;&#24819;&#20462;&#27491;&#65288;CC&#65289;&#12290;&#20026;&#20102;&#22312;&#24418;&#24335;&#35777;&#26126;&#30340;&#21518;&#22788;&#29702;&#20013;&#23454;&#29616;&#24037;&#20855;&#20462;&#27491;&#65292;&#25105;&#20204;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#35777;&#26126;&#24037;&#20855;&#65288;&#22914;Sledgehammer&#65289;&#26469;&#25351;&#23548;&#26367;&#25442;&#19981;&#27491;&#30830;&#30340;&#24037;&#20855;&#12290;&#24037;&#20855;&#20462;&#27491;&#26174;&#33879;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29468;&#24819;&#20462;&#27491;&#65292;&#19968;&#31181;&#38169;&#35823;&#21453;&#39304;&#26426;&#21046;&#65292;&#26088;&#22312;&#19982;&#35777;&#26126;&#22120;&#20114;&#21160;&#65292;&#36890;&#36807;&#35777;&#26126;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#36827;&#19968;&#27493;&#23436;&#21892;&#24418;&#24335;&#35777;&#26126;&#30340;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) present an intriguing avenue for exploration in the field of formal theorem proving. Nevertheless, their full potential, particularly concerning the mitigation of hallucinations and refinement through prover error messages, remains an area that has yet to be thoroughly investigated. To enhance the effectiveness of LLMs in the field, we introduce the Lyra, a new framework that employs two distinct correction mechanisms: Tool Correction (TC) and Conjecture Correction (CC). To implement Tool Correction in the post-processing of formal proofs, we leverage prior knowledge to utilize predefined prover tools (e.g., Sledgehammer) for guiding the replacement of incorrect tools. Tool Correction significantly contributes to mitigating hallucinations, thereby improving the overall accuracy of the proof. In addition, we introduce Conjecture Correction, an error feedback mechanism designed to interact with prover to refine formal proof conjectures with prover error messa
&lt;/p&gt;</description></item><item><title>NLPBench&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;NLP&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20026;&#22635;&#34917;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20316;&#32773;&#25910;&#38598;&#20102;&#26469;&#33258;&#32822;&#40065;&#22823;&#23398;&#26399;&#26411;&#32771;&#35797;&#30340;378&#20010;&#28085;&#30422;&#22810;&#20010;NLP&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#20351;&#29992;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#19981;&#31283;&#23450;&#65292;&#24182;&#21487;&#33021;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.15630</link><description>&lt;p&gt;
NLPBench&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;NLP&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
NLPBench: Evaluating Large Language Models on Solving NLP Problems. (arXiv:2309.15630v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15630
&lt;/p&gt;
&lt;p&gt;
NLPBench&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;NLP&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20026;&#22635;&#34917;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20316;&#32773;&#25910;&#38598;&#20102;&#26469;&#33258;&#32822;&#40065;&#22823;&#23398;&#26399;&#26411;&#32771;&#35797;&#30340;378&#20010;&#28085;&#30422;&#22810;&#20010;NLP&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#20351;&#29992;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#19981;&#31283;&#23450;&#65292;&#24182;&#21487;&#33021;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#26174;&#31034;&#20986;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#22312;LLMs&#30340;NLP&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#20173;&#28982;&#32570;&#20047;&#19987;&#38376;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;NLPBench&#65292;&#21253;&#25324;378&#20010;&#28085;&#30422;&#21508;&#31181;NLP&#20027;&#39064;&#30340;&#22823;&#23398;&#27700;&#24179;NLP&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#28304;&#33258;&#32822;&#40065;&#22823;&#23398;&#20197;&#21069;&#30340;&#26399;&#26411;&#32771;&#35797;&#12290;NLPBench&#21253;&#25324;&#20855;&#26377;&#19978;&#19979;&#25991;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#20010;&#23376;&#38382;&#39064;&#20998;&#20139;&#30456;&#21516;&#30340;&#20844;&#20849;&#20449;&#24687;&#65292;&#24182;&#19988;&#21253;&#25324;&#22810;&#36873;&#39064;&#12289;&#31616;&#31572;&#39064;&#21644;&#25968;&#23398;&#39064;&#31561;&#22810;&#31181;&#38382;&#39064;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#20197;GPT-3.5/4&#12289;PaLM-2&#21644;LLAMA-2&#31561;LLMs&#20026;&#20013;&#24515;&#65292;&#37319;&#29992;&#20102;&#35832;&#22914;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#21644;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#31561;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#39640;&#32423;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#21487;&#33021;&#19981;&#19968;&#33268;&#65292;&#26377;&#26102;&#20250;&#25439;&#23475;LLMs&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#65288;LLA&#65289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in large language models (LLMs) have shown promise in enhancing the capabilities of natural language processing (NLP). Despite these successes, there remains a dearth of research dedicated to the NLP problem-solving abilities of LLMs. To fill the gap in this area, we present a unique benchmarking dataset, NLPBench, comprising 378 college-level NLP questions spanning various NLP topics sourced from Yale University's prior final exams. NLPBench includes questions with context, in which multiple sub-questions share the same public information, and diverse question types, including multiple choice, short answer, and math. Our evaluation, centered on LLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting strategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance, especially in smaller models like the LLA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;QA-LoRA&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#37327;&#21270;&#24847;&#35782;&#20197;&#21450;&#32452;&#20869;&#36816;&#31639;&#31526;&#26469;&#23454;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20302;&#31209;&#36866;&#24212;&#12290;QA-LoRA&#33021;&#22815;&#23558;&#27169;&#22411;&#26435;&#37325;&#37327;&#21270;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23558;&#27169;&#22411;&#38598;&#25104;&#20026;&#19968;&#20010;&#37327;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14717</link><description>&lt;p&gt;
QA-LoRA: &#22522;&#20110;&#37327;&#21270;&#24847;&#35782;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20302;&#31209;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models. (arXiv:2309.14717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;QA-LoRA&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#37327;&#21270;&#24847;&#35782;&#20197;&#21450;&#32452;&#20869;&#36816;&#31639;&#31526;&#26469;&#23454;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20302;&#31209;&#36866;&#24212;&#12290;QA-LoRA&#33021;&#22815;&#23558;&#27169;&#22411;&#26435;&#37325;&#37327;&#21270;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23558;&#27169;&#22411;&#38598;&#25104;&#20026;&#19968;&#20010;&#37327;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#27785;&#37325;&#30340;&#35745;&#31639;&#36127;&#25285;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#38480;&#21046;&#20102;LLMs&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#24403;&#38656;&#35201;&#23558;&#23427;&#20204;&#37096;&#32626;&#21040;&#36793;&#32536;&#35774;&#22791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#24847;&#35782;&#30340;&#20302;&#31209;&#36866;&#24212;&#65288;QA-LoRA&#65289;&#31639;&#27861;&#12290;&#21160;&#26426;&#22312;&#20110;&#37327;&#21270;&#21644;&#36866;&#24212;&#30340;&#33258;&#30001;&#24230;&#19981;&#24179;&#34913;&#65292;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#32452;&#20869;&#36816;&#31639;&#31526;&#65292;&#22686;&#21152;&#37327;&#21270;&#30340;&#33258;&#30001;&#24230;&#65292;&#21516;&#26102;&#20943;&#23569;&#36866;&#24212;&#30340;&#33258;&#30001;&#24230;&#12290;QA-LoRA&#21487;&#20197;&#29992;&#20960;&#34892;&#20195;&#30721;&#36731;&#26494;&#23454;&#29616;&#65292;&#24182;&#20351;&#21407;&#22987;&#30340;LoRA&#20855;&#22791;&#20102;&#20004;&#20010;&#33021;&#21147;&#65306;&#65288;i&#65289;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;LLM&#30340;&#26435;&#37325;&#34987;&#37327;&#21270;&#65288;&#20363;&#22914;&#36716;&#25442;&#20026;INT4&#65289;&#65292;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#20351;&#29992;&#65307;&#65288;ii&#65289;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;LLM&#21644;&#36741;&#21161;&#26435;&#37325;&#33258;&#28982;&#22320;&#38598;&#25104;&#21040;&#19968;&#20010;&#37327;&#21270;&#27169;&#22411;&#20013;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23558;QA-LoRA&#24212;&#29992;&#21040;LLaMA&#21644;LLaMA2&#27169;&#22411;&#23478;&#26063;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this paper, we propose a quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model famil
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23450;&#20041;&#38382;&#39064;&#27979;&#35797;&#27979;&#37327;LLMs&#30340;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#65292;&#26089;&#26399;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;ChatGPT&#12289;Llama2-Chat&#12289;PaLM-2&#21644;GPT-4&#22312;&#36825;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#19982;&#25104;&#24180;&#20154;&#30456;&#24403;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#22256;&#22659;&#19979;&#30340;&#34920;&#29616;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.13356</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#20041;&#38382;&#39064;&#27979;&#35797;&#25506;&#31350;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Probing the Moral Development of Large Language Models through Defining Issues Test. (arXiv:2309.13356v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13356
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23450;&#20041;&#38382;&#39064;&#27979;&#35797;&#27979;&#37327;LLMs&#30340;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#65292;&#26089;&#26399;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;ChatGPT&#12289;Llama2-Chat&#12289;PaLM-2&#21644;GPT-4&#22312;&#36825;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#19982;&#25104;&#24180;&#20154;&#30456;&#24403;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#22256;&#22659;&#19979;&#30340;&#34920;&#29616;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#23450;&#20041;&#38382;&#39064;&#27979;&#35797;(DIT)&#26469;&#27979;&#37327;LLMs&#30340;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#65292;DIT&#26159;&#19968;&#31181;&#24515;&#29702;&#27979;&#37327;&#24037;&#20855;&#65292;&#29992;&#20110;&#26681;&#25454;&#31185;&#23572;&#20271;&#26684;&#30340;&#35748;&#30693;&#36947;&#24503;&#21457;&#23637;&#27169;&#22411;&#26469;&#34913;&#37327;&#20010;&#20154;&#30340;&#36947;&#24503;&#21457;&#23637;&#38454;&#27573;&#12290;DIT&#20351;&#29992;&#36947;&#24503;&#22256;&#22659;&#65292;&#24182;&#35201;&#27714;&#34987;&#35843;&#26597;&#32773;&#26681;&#25454;&#36947;&#24503;&#32771;&#34385;&#30340;&#37325;&#35201;&#24615;&#26469;&#21028;&#26029;&#21644;&#25490;&#24207;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#26089;&#26399;&#30340;LLMs&#65288;&#22914;GPT-3&#65289;&#22312;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#19978;&#24182;&#19981;&#27604;&#38543;&#26426;&#22522;&#32447;&#26356;&#22909;&#65292;&#32780;ChatGPT&#12289;Llama2-Chat&#12289;PaLM-2&#21644;GPT-4&#22312;&#36825;&#39033;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26126;&#26174;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21487;&#19982;&#25104;&#24180;&#20154;&#30456;&#23218;&#32654;&#12290;&#23454;&#38469;&#19978;&#65292;GPT-4&#20855;&#26377;&#26368;&#39640;&#30340;&#21518;&#24120;&#35268;&#36947;&#24503;&#25512;&#29702;&#20998;&#25968;&#65292;&#30456;&#24403;&#20110;&#20856;&#22411;&#30740;&#31350;&#29983;&#30340;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#35266;&#23519;&#21040;&#36825;&#20123;&#27169;&#22411;&#22312;&#25152;&#26377;&#22256;&#22659;&#19978;&#30340;&#34920;&#29616;&#24182;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we measure the moral reasoning ability of LLMs using the Defining Issues Test - a psychometric instrument developed for measuring the moral development stage of a person according to the Kohlberg's Cognitive Moral Development Model. DIT uses moral dilemmas followed by a set of ethical considerations that the respondent has to judge for importance in resolving the dilemma, and then rank-order them by importance. A moral development stage score of the respondent is then computed based on the relevance rating and ranking.  Our study shows that early LLMs such as GPT-3 exhibit a moral reasoning ability no better than that of a random baseline, while ChatGPT, Llama2-Chat, PaLM-2 and GPT-4 show significantly better performance on this task, comparable to adult humans. GPT-4, in fact, has the highest post-conventional moral reasoning score, equivalent to that of typical graduate school students. However, we also observe that the models do not perform consistently across all dil
&lt;/p&gt;</description></item><item><title>MetaMath&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#29983;&#25104;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.12284</link><description>&lt;p&gt;
MetaMath&#65306;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#33258;&#24049;&#30340;&#25968;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. (arXiv:2309.12284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12284
&lt;/p&gt;
&lt;p&gt;
MetaMath&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#29983;&#25104;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#26497;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;LLMs&#65288;&#20363;&#22914;LLaMA-2&#65289;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#20173;&#28982;&#36828;&#36828;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#65292;&#21407;&#22240;&#26159;&#22797;&#26434;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MetaMath&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27809;&#26377;&#39069;&#22806;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20197;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#20889;&#20837;&#38382;&#39064;&#26469;&#24341;&#23548;&#25968;&#23398;&#38382;&#39064;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#21517;&#20026;MetaMathQA&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;MetaMathQA&#19978;&#23545;LLaMA-2&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#23545;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#20004;&#20010;&#27969;&#34892;&#22522;&#20934;&#27979;&#35797;&#65288;&#21363;GSM8K&#21644;MATH&#65289;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MetaMath&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#19968;&#22871;&#24320;&#28304;LLMs&#12290;&#25105;&#20204;&#30340;MetaMath-7B&#27169;&#22411;&#22312;GSM8K&#19978;&#36798;&#21040;&#20102;66.4&#65285;&#65292;&#22312;MATH&#19978;&#36798;&#21040;&#20102;19.4&#65285;&#65292;&#36229;&#36807;&#20102;&#30456;&#21516;&#35268;&#27169;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (\eg, LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose \emph{MetaMath}, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called {MetaMathQA}. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (\ie, GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves $66.4\%$ on GSM8K and $19.4\%$ on MATH, exceeding the state-of-the-art models of the same size by 
&lt;/p&gt;</description></item><item><title>Agents&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#25903;&#25345;&#26500;&#24314;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#30340;&#21508;&#31181;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#25509;&#21475;&#21644;&#23545;&#30740;&#31350;&#20154;&#21592;&#30340;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07870</link><description>&lt;p&gt;
&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#30340;&#24320;&#28304;&#26694;&#26550;&#65306;Agents
&lt;/p&gt;
&lt;p&gt;
Agents: An Open-source Framework for Autonomous Language Agents. (arXiv:2309.07870v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07870
&lt;/p&gt;
&lt;p&gt;
Agents&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#25903;&#25345;&#26500;&#24314;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#30340;&#21508;&#31181;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#25509;&#21475;&#21644;&#23545;&#30740;&#31350;&#20154;&#21592;&#30340;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39640;&#32423;&#36827;&#23637;&#20351;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#26500;&#24314;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#65292;&#36825;&#20123;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#33258;&#21160;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#24182;&#19982;&#29615;&#22659;&#12289;&#20154;&#31867;&#21644;&#20854;&#20182;&#20195;&#29702;&#20132;&#20114;&#12290;&#25105;&#20204;&#23558;&#35821;&#35328;&#20195;&#29702;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#24182;&#21457;&#24067;Agents&#65292;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#26088;&#22312;&#21521;&#26356;&#24191;&#27867;&#30340;&#38750;&#19987;&#19994;&#20154;&#22763;&#24320;&#25918;&#36825;&#20123;&#36827;&#23637;&#12290;Agents&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#25903;&#25345;&#37325;&#35201;&#21151;&#33021;&#65292;&#21253;&#25324;&#35268;&#21010;&#12289;&#35760;&#24518;&#12289;&#24037;&#20855;&#20351;&#29992;&#12289;&#22810;&#20195;&#29702;&#36890;&#20449;&#21644;&#32454;&#31890;&#24230;&#30340;&#31526;&#21495;&#25511;&#21046;&#12290;Agents&#29992;&#25143;&#21451;&#22909;&#65292;&#20351;&#38750;&#19987;&#19994;&#20154;&#22763;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#32534;&#20889;&#22826;&#22810;&#20195;&#30721;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#12289;&#23450;&#21046;&#12289;&#27979;&#35797;&#12289;&#35843;&#20248;&#21644;&#37096;&#32626;&#26368;&#20808;&#36827;&#30340;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#12290;&#35813;&#24211;&#20063;&#23545;&#30740;&#31350;&#20154;&#21592;&#21451;&#22909;&#65292;&#20854;&#27169;&#22359;&#21270;&#35774;&#35745;&#20351;&#20854;&#26131;&#20110;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances on large language models (LLMs) enable researchers and developers to build autonomous language agents that can automatically solve various tasks and interact with environments, humans, and other agents using natural language interfaces. We consider language agents as a promising direction towards artificial general intelligence and release Agents, an open-source library with the goal of opening up these advances to a wider non-specialist audience. Agents is carefully engineered to support important features including planning, memory, tool usage, multi-agent communication, and fine-grained symbolic control. Agents is user-friendly as it enables non-specialists to build, customize, test, tune, and deploy state-of-the-art autonomous language agents without much coding. The library is also research-friendly as its modularized design makes it easily extensible for researchers. Agents is available at https://github.com/aiwaves-cn/agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RAIN&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26080;&#38656;&#24494;&#35843;&#25110;&#39069;&#22806;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#25105;&#35780;&#20272;&#21644;&#22238;&#28378;&#26426;&#21046;&#23454;&#29616;&#23545;&#40784;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#30452;&#25509;&#20135;&#29983;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.07124</link><description>&lt;p&gt;
RAIN: &#24744;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33258;&#25105;&#35843;&#25972;&#32780;&#26080;&#38656;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
RAIN: Your Language Models Can Align Themselves without Finetuning. (arXiv:2309.07124v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RAIN&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26080;&#38656;&#24494;&#35843;&#25110;&#39069;&#22806;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#25105;&#35780;&#20272;&#21644;&#22238;&#28378;&#26426;&#21046;&#23454;&#29616;&#23545;&#40784;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#30452;&#25509;&#20135;&#29983;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24120;&#24120;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#65292;&#28982;&#21518;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25110;&#25351;&#23548;&#35843;&#20248;&#31561;&#26041;&#27861;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#23454;&#29616;&#23545;&#40784;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#25968;&#25454;&#23545;&#40784;&#20923;&#32467;&#30340;LLM&#26356;&#26377;&#21560;&#24341;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21518;&#19968;&#31181;&#24773;&#26223;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#23558;&#33258;&#25105;&#35780;&#20272;&#21644;&#22238;&#28378;&#26426;&#21046;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#19981;&#23545;&#40784;&#30340;LLM&#21487;&#20197;&#36890;&#36807;&#33258;&#25105;&#22686;&#24378;&#30452;&#25509;&#20135;&#29983;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#21487;&#22238;&#28378;&#30340;&#33258;&#22238;&#24402;&#25512;&#29702;&#65288;RAIN&#65289;&#65292;&#23427;&#20801;&#35768;&#39044;&#35757;&#32451;&#30340;LLM&#35780;&#20272;&#33258;&#24049;&#30340;&#29983;&#25104;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#32467;&#26524;&#26469;&#24341;&#23548;&#21521;&#21518;&#22238;&#28378;&#21644;&#21521;&#21069;&#29983;&#25104;&#20197;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;RAIN&#22312;&#27169;&#22411;&#23545;&#40784;&#26102;&#26080;&#38656;&#39069;&#22806;&#25968;&#25454;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#12289;&#26799;&#24230;&#35745;&#31639;&#25110;&#21442;&#25968;&#26356;&#26032;&#65307;&#22312;&#33258;&#25105;&#35780;&#20272;&#38454;&#27573;&#65292;&#27169;&#22411;&#25509;&#25910;&#30340;&#26159;&#19968;&#20123;&#38543;&#26426;&#22238;&#28378;&#30340;&#29983;&#25104;&#26679;&#26412;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, the so-called finetuning step. In contrast, aligning frozen LLMs without any extra data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide backward rewind and forward generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates; during the self-evaluation phase, the model recei
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#32534;&#30721;&#26694;&#26550;&#30340;&#22343;&#22330;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20551;&#35774;&#31361;&#35302;&#26435;&#37325;&#36981;&#24490;&#33033;&#20914;&#21644;&#26001;&#28857;&#20998;&#24067;&#24182;&#21482;&#23545;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#65292;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2309.04106</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#30340;&#20803;&#39044;&#27979;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Meta predictive learning model of natural languages. (arXiv:2309.04106v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#32534;&#30721;&#26694;&#26550;&#30340;&#22343;&#22330;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20551;&#35774;&#31361;&#35302;&#26435;&#37325;&#36981;&#24490;&#33033;&#20914;&#21644;&#26001;&#28857;&#20998;&#24067;&#24182;&#21482;&#23545;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#65292;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#22312;&#33258;&#28982;&#35821;&#35328;&#26412;&#36523;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#34920;&#29616;&#65292;&#32780;&#19988;&#22312;&#21508;&#31181;&#19981;&#21516;&#24615;&#36136;&#30340;&#20219;&#21153;&#20013;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35821;&#35328;&#22788;&#29702;&#65292;&#25105;&#20204;&#30340;&#20154;&#33041;&#21487;&#33021;&#19981;&#26159;&#25353;&#29031;&#21516;&#26679;&#30340;&#21407;&#29702;&#36816;&#20316;&#12290;&#22240;&#27492;&#65292;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37319;&#29992;&#30340;&#20154;&#24037;&#33258;&#25105;&#30417;&#30563;&#19982;&#33041;&#35745;&#31639;&#20043;&#38388;&#30340;&#32852;&#31995;&#24341;&#36215;&#20102;&#19968;&#22330;&#36777;&#35770;&#12290;&#22312;&#33041;&#35745;&#31639;&#20013;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#20551;&#35774;&#20043;&#19968;&#26159;&#39044;&#27979;&#32534;&#30721;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25552;&#20986;&#36890;&#36807;&#23616;&#37096;&#23398;&#20064;&#26469;&#26368;&#23567;&#21270;&#39044;&#27979;&#35823;&#24046;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#32534;&#30721;&#21644;&#30456;&#20851;&#30340;&#23398;&#20998;&#20998;&#37197;&#22312;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20316;&#29992;&#20173;&#28982;&#26410;&#30693;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39044;&#27979;&#32534;&#30721;&#26694;&#26550;&#30340;&#22343;&#22330;&#23398;&#20064;&#27169;&#22411;&#65292;&#20551;&#35774;&#27599;&#20010;&#36830;&#25509;&#30340;&#31361;&#35302;&#26435;&#37325;&#36981;&#24490;&#33033;&#20914;&#21644;&#26001;&#28857;&#20998;&#24067;&#65292;&#21482;&#23545;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#20803;&#39044;&#27979;&#23398;&#20064;&#22312;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#19978;&#24471;&#21040;&#20102;&#25104;&#21151;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models based on self-attention mechanisms have achieved astonishing performances not only in natural language itself, but also in a variety of tasks of different nature. However, regarding processing language, our human brain may not operate using the same principle. Then, a debate is established on the connection between brain computation and artificial self-supervision adopted in large language models. One of most influential hypothesis in brain computation is the predictive coding framework, which proposes to minimize the prediction error by local learning. However, the role of predictive coding and the associated credit assignment in language processing remains unknown. Here, we propose a mean-field learning model within the predictive coding framework, assuming that the synaptic weight of each connection follows a spike and slab distribution, and only the distribution is trained. This meta predictive learning is successfully validated on classifying handwritten digi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#36164;&#28304;&#24187;&#35273;&#39044;&#38450;&#26041;&#27861;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#23545;&#36755;&#20837;&#25351;&#20196;&#20013;&#27010;&#24565;&#30340;&#29087;&#24713;&#31243;&#24230;&#65292;&#22312;&#36935;&#21040;&#19981;&#29087;&#24713;&#30340;&#27010;&#24565;&#26102;&#19981;&#29983;&#25104;&#21709;&#24212;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.02654</link><description>&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#36164;&#28304;&#24187;&#35273;&#39044;&#38450;
&lt;/p&gt;
&lt;p&gt;
Zero-Resource Hallucination Prevention for Large Language Models. (arXiv:2309.02654v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#36164;&#28304;&#24187;&#35273;&#39044;&#38450;&#26041;&#27861;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#23545;&#36755;&#20837;&#25351;&#20196;&#20013;&#27010;&#24565;&#30340;&#29087;&#24713;&#31243;&#24230;&#65292;&#22312;&#36935;&#21040;&#19981;&#29087;&#24713;&#30340;&#27010;&#24565;&#26102;&#19981;&#29983;&#25104;&#21709;&#24212;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24341;&#36215;&#20102;&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#30340;&#20851;&#27880;&#65292;&#36825;&#25351;&#30340;&#26159;LLMs&#29983;&#25104;&#20107;&#23454;&#19981;&#20934;&#30830;&#25110;&#27809;&#26377;&#26681;&#25454;&#30340;&#20449;&#24687;&#30340;&#24773;&#20917;&#12290;&#29616;&#26377;&#30340;&#35821;&#35328;&#21161;&#25163;&#20013;&#24187;&#35273;&#26816;&#27979;&#25216;&#26415;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#27169;&#31946;&#12289;&#22522;&#20110;&#33258;&#30001;&#35821;&#35328;&#30340;&#24605;&#32500;&#38142;&#26465;(CoT)&#25216;&#26415;&#25110;&#22522;&#20110;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#23384;&#22312;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#35782;&#21035;&#29983;&#25104;&#21518;&#24187;&#35273;&#30340;&#26041;&#27861;&#26080;&#27861;&#39044;&#38450;&#20854;&#21457;&#29983;&#65292;&#24182;&#19988;&#30001;&#20110;&#25351;&#20196;&#26684;&#24335;&#21644;&#27169;&#22411;&#39118;&#26684;&#30340;&#24433;&#21709;&#65292;&#24615;&#33021;&#19981;&#19968;&#33268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#26816;&#27979;&#33258;&#25105;&#35780;&#20272;&#25216;&#26415;&#65292;&#31216;&#20026;{\method}&#65292;&#23427;&#19987;&#27880;&#20110;&#35780;&#20272;&#27169;&#22411;&#23545;&#36755;&#20837;&#25351;&#20196;&#20013;&#27010;&#24565;&#30340;&#29087;&#24713;&#31243;&#24230;&#65292;&#24182;&#22312;&#36935;&#21040;&#19981;&#29087;&#24713;&#30340;&#27010;&#24565;&#26102;&#19981;&#29983;&#25104;&#21709;&#24212;&#12290;&#36825;&#31181;&#26041;&#27861;&#27169;&#25311;&#20102;&#20154;&#31867;&#33021;&#22815;&#22312;&#27809;&#26377;&#25226;&#25569;&#26102;&#19981;&#20316;&#22238;&#24212;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalent use of large language models (LLMs) in various domains has drawn attention to the issue of "hallucination," which refers to instances where LLMs generate factually inaccurate or ungrounded information. Existing techniques for hallucination detection in language assistants rely on intricate fuzzy, specific free-language-based chain of thought (CoT) techniques or parameter-based methods that suffer from interpretability issues. Additionally, the methods that identify hallucinations post-generation could not prevent their occurrence and suffer from inconsistent performance due to the influence of the instruction format and model style. In this paper, we introduce a novel pre-detection self-evaluation technique, referred to as {\method}, which focuses on evaluating the model's familiarity with the concepts present in the input instruction and withholding the generation of response in case of unfamiliar concepts. This approach emulates the human ability to refrain from respond
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19979;&#37319;&#26679;&#22768;&#23398;&#34920;&#31034;&#26469;&#23545;&#40784;&#25991;&#26412;&#27169;&#24577;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#32431;&#25991;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26032;&#39046;&#22495;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.02459</link><description>&lt;p&gt;
&#36890;&#36807;&#19979;&#37319;&#26679;&#30340;&#22768;&#23398;&#34920;&#31034;&#36827;&#34892;&#32431;&#25991;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation. (arXiv:2309.02459v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19979;&#37319;&#26679;&#22768;&#23398;&#34920;&#31034;&#26469;&#23545;&#40784;&#25991;&#26412;&#27169;&#24577;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#32431;&#25991;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26032;&#39046;&#22495;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20004;&#31181;&#24418;&#24335;&#30340;&#36164;&#26009;&#65292;&#22768;&#38899;&#21644;&#25991;&#26412;&#65292;&#26144;&#23556;&#21040;&#20849;&#20139;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#26159;&#19968;&#31181;&#21033;&#29992;&#32431;&#25991;&#26412;&#25968;&#25454;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#24615;&#33021;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#22768;&#38899;&#21644;&#25991;&#26412;&#30340;&#34920;&#31034;&#38271;&#24230;&#19981;&#19968;&#33268;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#19978;&#37319;&#26679;&#25991;&#26412;&#34920;&#31034;&#26469;&#19982;&#38899;&#39057;&#27169;&#24577;&#36827;&#34892;&#23545;&#40784;&#65292;&#20294;&#21487;&#33021;&#19981;&#21305;&#37197;&#39044;&#26399;&#30340;&#23454;&#38469;&#25345;&#32493;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#19979;&#37319;&#26679;&#22768;&#23398;&#34920;&#31034;&#26469;&#19982;&#25991;&#26412;&#27169;&#24577;&#23545;&#40784;&#30340;&#26032;&#22411;&#34920;&#31034;&#21305;&#37197;&#31574;&#30053;&#12290;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#31215;&#20998;-&#28779;&#28846; (CIF) &#27169;&#22359;&#29983;&#25104;&#19982;&#26631;&#35760;&#38271;&#24230;&#19968;&#33268;&#30340;&#22768;&#23398;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;ASR&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#20174;&#20004;&#31181;&#27169;&#24577;&#20013;&#23398;&#20064;&#32479;&#19968;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#33021;&#22815;&#20351;&#29992;&#30446;&#26631;&#39046;&#22495;&#30340;&#32431;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#26032;&#39046;&#22495;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mapping two modalities, speech and text, into a shared representation space, is a research topic of using text-only data to improve end-to-end automatic speech recognition (ASR) performance in new domains. However, the length of speech representation and text representation is inconsistent. Although the previous method up-samples the text representation to align with acoustic modality, it may not match the expected actual duration. In this paper, we proposed novel representations match strategy through down-sampling acoustic representation to align with text modality. By introducing a continuous integrate-and-fire (CIF) module generating acoustic representations consistent with token length, our ASR model can learn unified representations from both modalities better, allowing for domain adaptation using text-only data of the target domain. Experiment results of new domain data demonstrate the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>Wordle&#26159;&#19968;&#27454;&#27969;&#34892;&#30340;&#22312;&#32447;&#21333;&#35789;&#28216;&#25103;&#65292;&#29609;&#23478;&#38656;&#35201;&#22312;6&#27425;&#29468;&#27979;&#20013;&#29468;&#20986;&#27599;&#26085;&#30446;&#26631;&#21333;&#35789;&#12290;&#36890;&#36807;&#25910;&#38598;&#29609;&#23478;&#30340;&#25968;&#25454;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#27599;&#22825;&#32422;&#26377;0.2-0.5%&#30340;&#29609;&#23478;&#33021;&#22312;&#19968;&#27425;&#23581;&#35797;&#20013;&#35299;&#20915;&#35868;&#39064;&#65292;&#23637;&#31034;&#20102;&#29609;&#23478;&#20204;&#30340;&#25216;&#24039;&#21644;&#36816;&#27668;&#12290;</title><link>http://arxiv.org/abs/2309.02110</link><description>&lt;p&gt;
Wordle: &#29983;&#27963;&#30340;&#32553;&#24433;&#12290;&#24184;&#36816;&#12289;&#25216;&#24039;&#12289;&#20316;&#24330;&#12289;&#24544;&#35802;&#21644;&#24433;&#21709;&#21147;&#65281;
&lt;/p&gt;
&lt;p&gt;
Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!. (arXiv:2309.02110v2 [math.HO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02110
&lt;/p&gt;
&lt;p&gt;
Wordle&#26159;&#19968;&#27454;&#27969;&#34892;&#30340;&#22312;&#32447;&#21333;&#35789;&#28216;&#25103;&#65292;&#29609;&#23478;&#38656;&#35201;&#22312;6&#27425;&#29468;&#27979;&#20013;&#29468;&#20986;&#27599;&#26085;&#30446;&#26631;&#21333;&#35789;&#12290;&#36890;&#36807;&#25910;&#38598;&#29609;&#23478;&#30340;&#25968;&#25454;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#27599;&#22825;&#32422;&#26377;0.2-0.5%&#30340;&#29609;&#23478;&#33021;&#22312;&#19968;&#27425;&#23581;&#35797;&#20013;&#35299;&#20915;&#35868;&#39064;&#65292;&#23637;&#31034;&#20102;&#29609;&#23478;&#20204;&#30340;&#25216;&#24039;&#21644;&#36816;&#27668;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wordle&#26159;&#19968;&#27454;&#30001;&#32445;&#32422;&#26102;&#25253;&#25552;&#20379;&#30340;&#27969;&#34892;&#22312;&#32447;&#21333;&#35789;&#28216;&#25103;&#12290;&#30446;&#21069;&#20840;&#29699;&#26377;&#22823;&#32422;200&#19975;&#21517;&#33521;&#25991;&#29256;&#26412;&#30340;&#29609;&#23478;&#12290;&#29609;&#23478;&#26377;6&#27425;&#26426;&#20250;&#26469;&#29468;&#27979;&#27599;&#26085;&#30340;&#30446;&#26631;&#21333;&#35789;&#65292;&#24182;&#22312;&#27599;&#27425;&#29468;&#27979;&#21518;&#65292;&#26681;&#25454;&#27599;&#20010;&#23383;&#27597;&#30340;&#20301;&#32622;&#21644;&#27491;&#30830;&#24615;&#65292;&#29609;&#23478;&#20250;&#24471;&#21040;&#24425;&#33394;&#32534;&#30721;&#30340;&#20449;&#24687;&#12290;&#22312;&#25104;&#21151;&#23436;&#25104;&#35868;&#39064;&#25110;&#26368;&#21518;&#19968;&#27425;&#26410;&#25104;&#21151;&#30340;&#23581;&#35797;&#20043;&#21518;&#65292;&#36719;&#20214;&#21487;&#20197;&#20351;&#29992;&#20449;&#24687;&#35770;&#26469;&#35780;&#20272;&#29609;&#23478;&#30340;&#36816;&#27668;&#21644;&#25216;&#24039;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#31034;&#38543;&#26426;&#25277;&#26679;&#30340;&#25152;&#26377;&#29609;&#23478;&#30340;&#31532;&#19968;&#27425;&#12289;&#31532;&#20108;&#27425;...&#31532;&#20845;&#27425;&#29468;&#27979;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#25105;&#21457;&#29616;&#21518;&#38754;&#30340;&#25968;&#25454;&#20197;&#19968;&#31181;&#26041;&#20415;&#22797;&#21046;&#31896;&#36148;&#21040;&#30005;&#23376;&#34920;&#26684;&#20013;&#30340;&#26684;&#24335;&#21576;&#29616;&#20986;&#26469;&#12290;&#25105;&#20174;2023&#24180;5&#26376;&#33267;2023&#24180;8&#26376;&#25910;&#38598;&#20102;Wordle&#29609;&#23478;&#30340;&#31532;&#19968;&#27425;&#29468;&#27979;&#25968;&#25454;&#65292;&#24182;&#25512;&#26029;&#20986;&#19968;&#20123;&#26377;&#36259;&#30340;&#26377;&#20851;Wordle&#29609;&#23478;&#30340;&#20449;&#24687;&#12290;A&#65289;&#27599;&#22825;&#32422;&#26377;0.2-0.5%&#30340;&#29609;&#23478;&#22312;&#19968;&#27425;&#23581;&#35797;&#20013;&#35299;&#20915;&#20102;&#35868;&#39064;&#12290;&#30001;&#20110;&#29468;&#23545;2315&#20010;&#20301;&#32622;&#19978;&#30340;&#21333;&#35789;&#30340;&#20960;&#29575;&#20026;1/2315&#65292;&#36825;&#19968;&#27604;&#20363;&#26174;&#31034;&#20102;&#29609;&#23478;&#20204;&#22312;&#28216;&#25103;&#20013;&#30340;&#25216;&#24039;&#21644;&#36816;&#27668;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wordle is a popular, online word game offered by the New York Times (nytimes.com). Currently there are some 2 million players of the English version worldwide. Players have 6 attempts to guess the daily word (target word) and after each attempt, the player receives color-coded information about the correctness and position of each letter in the guess. After either a successful completion of the puzzle or the final unsuccessful attempt, software can assess the player's luck and skill using Information Theory and can display data for the first, second, ..., sixth guesses of a random sample of all players. Recently, I discovered that the latter data is presented in a format that can easily be copied and pasted into a spreadsheet. I compiled data on Wordle players' first guesses from May 2023 - August 2023 and inferred some interesting information about Wordle players. A) Every day, about 0.2-0.5% of players solve the puzzle in one attempt. Because the odds of guessing the one of 2,315 pos
&lt;/p&gt;</description></item><item><title>SememeASR&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30693;&#35782;&#30340;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#21333;&#20803;&#8212;&#8212;sememe&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#25913;&#21892;&#23545;&#38271;&#23614;&#25968;&#25454;&#30340;&#35782;&#21035;&#21644;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.01437</link><description>&lt;p&gt;
SememeASR&#65306;&#29992;&#35821;&#20041;&#30693;&#35782;&#25552;&#21319;&#19982;&#39046;&#22495;&#21644;&#38271;&#23614;&#25968;&#25454;&#36716;&#31227;&#25239;&#24615;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge. (arXiv:2309.01437v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01437
&lt;/p&gt;
&lt;p&gt;
SememeASR&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30693;&#35782;&#30340;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#21333;&#20803;&#8212;&#8212;sememe&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#25913;&#21892;&#23545;&#38271;&#23614;&#25968;&#25454;&#30340;&#35782;&#21035;&#21644;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35821;&#38899;&#35782;&#21035;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#32431;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#39046;&#22495;&#19981;&#21305;&#37197;&#21644;&#38271;&#23614;&#25968;&#25454;&#30340;&#38382;&#39064;&#19978;&#20173;&#28982;&#26377;&#22256;&#38590;&#12290;&#32771;&#34385;&#21040;&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#20943;&#36731;&#20854;&#32570;&#38519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35821;&#20041;&#21333;&#20803;&#8212;&#8212;sememe&#30340;&#35821;&#20041;&#30693;&#35782;&#20449;&#24687;&#21040;&#35821;&#38899;&#35782;&#21035;&#20013;&#65288;SememeASR&#65289;&#12290;Sememe&#26681;&#25454;&#35821;&#35328;&#23398;&#30340;&#23450;&#20041;&#26159;&#35821;&#35328;&#20013;&#30340;&#26368;&#23567;&#35821;&#20041;&#21333;&#20301;&#65292;&#24182;&#19988;&#33021;&#22815;&#24456;&#22909;&#22320;&#20195;&#34920;&#27599;&#20010;&#21333;&#35789;&#32972;&#21518;&#30340;&#38544;&#21547;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24341;&#20837;sememe&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#36827;&#19968;&#27493;&#23454;&#39564;&#26174;&#31034;&#65292;sememe&#30693;&#35782;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#38271;&#23614;&#25968;&#25454;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#24182;&#22686;&#24378;&#27169;&#22411;&#30340;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, excellent progress has been made in speech recognition. However, pure data-driven approaches have struggled to solve the problem in domain-mismatch and long-tailed data. Considering that knowledge-driven approaches can help data-driven approaches alleviate their flaws, we introduce sememe-based semantic knowledge information to speech recognition (SememeASR). Sememe, according to the linguistic definition, is the minimum semantic unit in a language and is able to represent the implicit semantic information behind each word very well. Our experiments show that the introduction of sememe information can improve the effectiveness of speech recognition. In addition, our further experiments show that sememe knowledge can improve the model's recognition of long-tailed data and enhance the model's domain generalization ability.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeBERT&#30340;SNN&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;Spikformer&#26550;&#26500;&#21644;&#20351;&#29992;&#20004;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#20854;&#20182;SNN&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#29978;&#33267;&#36798;&#21040;&#20102;&#19982;BERT&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15122</link><description>&lt;p&gt;
SpikeBERT&#65306;&#19968;&#31181;&#37319;&#29992;&#20004;&#38454;&#27573;BERT&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#30340;&#35821;&#35328;Spikformer
&lt;/p&gt;
&lt;p&gt;
SpikeBERT: A Language Spikformer Trained with Two-Stage Knowledge Distillation from BERT. (arXiv:2308.15122v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeBERT&#30340;SNN&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;Spikformer&#26550;&#26500;&#21644;&#20351;&#29992;&#20004;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#20854;&#20182;SNN&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#29978;&#33267;&#36798;&#21040;&#20102;&#19982;BERT&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22312;&#20197;&#26356;&#33410;&#33021;&#30340;&#26041;&#24335;&#23454;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#35821;&#35328;&#20219;&#21153;&#30340;SNN&#32593;&#32476;&#26550;&#26500;&#36807;&#20110;&#31616;&#21333;&#65292;&#28145;&#24230;&#26550;&#26500;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#65292;&#19982;BERT&#31561;&#20027;&#27969;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#30456;&#27604;&#65292;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#33033;&#20914;Transformer&#65288;&#21363;Spikformer&#65289;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#35821;&#35328;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#26469;&#35757;&#32451;&#23427;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#36890;&#36807;&#20174;BERT&#21644;&#22823;&#37327;&#26410;&#26631;&#35760;&#25991;&#26412;&#20013;&#33976;&#39311;&#30693;&#35782;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#20877;&#27425;&#20174;&#22312;&#30456;&#21516;&#35757;&#32451;&#31034;&#20363;&#19978;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#23454;&#20363;&#30693;&#35782;&#33976;&#39311;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;SpikeBERT&#65292;&#22312;&#23454;&#29616;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;SNN&#65292;&#24182;&#19988;&#29978;&#33267;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#19982;BERT&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) offer a promising avenue to implement deep neural networks in a more energy-efficient way. However, the network architectures of existing SNNs for language tasks are too simplistic, and deep architectures have not been fully explored, resulting in a significant performance gap compared to mainstream transformer-based networks such as BERT. To this end, we improve a recently-proposed spiking transformer (i.e., Spikformer) to make it possible to process language tasks and propose a two-stage knowledge distillation method for training it, which combines pre-training by distilling knowledge from BERT with a large collection of unlabelled texts and fine-tuning with task-specific instances via knowledge distillation again from the BERT fine-tuned on the same training examples. Through extensive experimentation, we show that the models trained with our method, named SpikeBERT, outperform state-of-the-art SNNs and even achieve comparable results to BERTs on text 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11534</link><description>&lt;p&gt;
&#20316;&#20026;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as a User Simulator. (arXiv:2308.11534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38381;&#28304;ChatGPT&#30340;&#21331;&#36234;&#24615;&#33021;&#24341;&#21457;&#20102;&#23545;&#20854;&#27665;&#20027;&#21270;&#30340;&#21162;&#21147;&#65292;&#20511;&#21161;&#30495;&#23454;&#29992;&#25143;&#21644;ChatGPT&#23545;&#35805;&#30340;&#21162;&#21147;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;Vicuna&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#20363;&#23376;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;Baize&#21644;UltraChat&#31561;&#21162;&#21147;&#20027;&#35201;&#20381;&#38752;ChatGPT&#26681;&#25454;&#25351;&#20196;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#65292;&#32780;&#19981;&#26159;&#30495;&#23454;&#30340;&#20154;&#31867;&#23398;&#20064;&#65292;&#23548;&#33268;&#33539;&#22260;&#26377;&#38480;&#65292;&#22810;&#26679;&#24615;&#20943;&#24369;&#65292;&#32570;&#20047;&#30495;&#27491;&#30340;&#22810;&#36718;&#23545;&#35805;&#21160;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#25226;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#12290;&#38543;&#21518;&#65292;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#25105;&#20204;&#30340;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;Vicuna-Bench&#21644;MT-Bench&#20013;&#22343;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT conversations, as evidenced by Vicuna. However, while current endeavors like Baize and UltraChat aim to auto-generate conversational data due to challenges in gathering human participation, they primarily rely on ChatGPT to simulate human behaviors based on directives rather than genuine human learning. This results in a limited scope, diminished diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we innovatively target human questions extracted from genuine human-machine conversations as a learning goal and train a user simulator, UserGPT, to produce a high-quality human-centric synthetic conversation dataset, RealChat. Subsequently, this dataset trains our assistant model, ReaLM. Experimentally, ReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise
&lt;/p&gt;</description></item><item><title>AgentVerse&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#21327;&#21516;&#21644;&#21160;&#24577;&#35843;&#25972;&#21512;&#20316;&#22242;&#38431;&#30340;&#32452;&#25104;&#65292;&#23454;&#29616;&#36229;&#36234;&#21333;&#20010;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;&#22312;&#21512;&#20316;&#20219;&#21153;&#20013;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24341;&#21457;&#20986;&#32676;&#20307;&#20869;&#20010;&#20307;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31038;&#20250;&#34892;&#20026;&#65292;&#20174;&#32780;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#22242;&#38431;&#30340;&#21327;&#20316;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.10848</link><description>&lt;p&gt;
AgentVerse: &#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#21644;&#25506;&#32034; emergent behaviors&#12290;
&lt;/p&gt;
&lt;p&gt;
AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. (arXiv:2308.10848v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10848
&lt;/p&gt;
&lt;p&gt;
AgentVerse&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#21327;&#21516;&#21644;&#21160;&#24577;&#35843;&#25972;&#21512;&#20316;&#22242;&#38431;&#30340;&#32452;&#25104;&#65292;&#23454;&#29616;&#36229;&#36234;&#21333;&#20010;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;&#22312;&#21512;&#20316;&#20219;&#21153;&#20013;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24341;&#21457;&#20986;&#32676;&#20307;&#20869;&#20010;&#20307;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31038;&#20250;&#34892;&#20026;&#65292;&#20174;&#32780;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#22242;&#38431;&#30340;&#21327;&#20316;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36171;&#33021;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#33539;&#22260;&#20869;&#36827;&#34892;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#22659;&#20013;&#65292;&#20010;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#32463;&#24120;&#38656;&#35201;&#20197;&#22686;&#24378;&#20219;&#21153;&#23436;&#25104;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#21463;&#21040;&#20154;&#31867;&#32676;&#20307;&#21160;&#21147;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#22823;&#20110;&#20854;&#21508;&#20010;&#37096;&#20998;&#20043;&#21644;&#30340;&#31995;&#32479;&#36827;&#34892;&#21327;&#21516;&#21644;&#21160;&#24577;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20010;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#37096;&#32626;&#36229;&#36807;&#21333;&#20010;&#26234;&#33021;&#20307;&#30340;&#22810;&#26234;&#33021;&#20307;&#32452;&#65292;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#22312;&#21512;&#20316;&#20219;&#21153;&#23436;&#25104;&#36807;&#31243;&#20013;&#65292;&#32676;&#20307;&#20869;&#20010;&#20307;&#26234;&#33021;&#20307;&#20043;&#38388;&#20986;&#29616;&#30340;&#31038;&#20250;&#34892;&#20026;&#30340;&#28044;&#29616;&#12290;&#22522;&#20110;&#36825;&#20123;&#34892;&#20026;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20123;&#21487;&#33021;&#30340;&#31574;&#30053;&#65292;&#20197;&#21033;&#29992;&#31215;&#26497;&#30340;&#34892;&#20026;&#65292;&#24182;&#20943;&#36731;&#28040;&#26497;&#30340;&#34892;&#20026;&#65292;&#20197;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#32452;&#30340;&#21327;&#20316;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340; \framework &#26694;&#26550;&#30340;&#20195;&#30721;&#23558;&#24456;&#24555;&#20844;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework \framework that can collaboratively and dynamically adjust its composition as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that \framework framework can effectively deploy multi-agent groups that outperform a single agent. Furthermore, we delve into the emergence of social behaviors among individual agents within a group during collaborative task accomplishment. In view of these behaviors, we discuss some possible strategies to leverage positive ones and mitigate negative ones for improving the collaborative potential of multi-agent groups. Our codes for \framework will soon be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.10792</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#35843;&#20248;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#65288;IT&#65289;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#31181;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#25351;&#20196;&#35843;&#20248;&#26159;&#25351;&#20197;&#30417;&#30563;&#26041;&#24335;&#22312;&#21253;&#21547;&#8220;&#25351;&#20196;-&#36755;&#20986;&#8221;&#23545;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;LLM&#65292;&#36825;&#23558;LLM&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#30446;&#26631;&#19982;&#29992;&#25143;&#24076;&#26395;LLM&#36981;&#23432;&#20154;&#31867;&#25351;&#20196;&#30340;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#23545;IT&#30340;&#24120;&#35268;&#26041;&#27861;&#12289;IT&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#12289;IT&#27169;&#22411;&#30340;&#35757;&#32451;&#20197;&#21450;&#24212;&#29992;&#20110;&#19981;&#21516;&#27169;&#24577;&#12289;&#39046;&#22495;&#21644;&#24212;&#29992;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#23545;&#24433;&#21709;IT&#32467;&#26524;&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#20998;&#26512;&#65288;&#20363;&#22914;&#65292;&#25351;&#20196;&#36755;&#20986;&#30340;&#29983;&#25104;&#12289;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#31561;&#65289;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;IT&#30340;&#28508;&#22312;&#38382;&#39064;&#20197;&#21450;&#38024;&#23545;&#20854;&#30340;&#25209;&#35780;&#65292;&#20197;&#21450;&#25351;&#20986;&#24403;&#21069;&#19981;&#36275;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#20041;&#23545;&#40784;&#26469;&#22686;&#24378;&#38750;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#36328;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#22312;&#32763;&#35793;&#25968;&#25454;&#20013;&#21152;&#20837;&#38750;&#33521;&#35821;&#25991;&#26412;&#21487;&#20197;&#26377;&#25928;&#25552;&#21319;&#27169;&#22411;&#33021;&#21147;&#65292;&#19988;LLM&#20869;&#37096;&#30340;&#35821;&#20041;&#23545;&#40784;&#20063;&#21487;&#20197;&#36827;&#19968;&#27493;&#21152;&#24378;&#12290;</title><link>http://arxiv.org/abs/2308.04948</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#23545;&#40784;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#24191;&#21040;&#38750;&#33521;&#35821;&#20013;
&lt;/p&gt;
&lt;p&gt;
Extrapolating Large Language Models to Non-English by Aligning Languages. (arXiv:2308.04948v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#20041;&#23545;&#40784;&#26469;&#22686;&#24378;&#38750;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#36328;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#22312;&#32763;&#35793;&#25968;&#25454;&#20013;&#21152;&#20837;&#38750;&#33521;&#35821;&#25991;&#26412;&#21487;&#20197;&#26377;&#25928;&#25552;&#21319;&#27169;&#22411;&#33021;&#21147;&#65292;&#19988;LLM&#20869;&#37096;&#30340;&#35821;&#20041;&#23545;&#40784;&#20063;&#21487;&#20197;&#36827;&#19968;&#27493;&#21152;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#34913;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24448;&#24448;&#22312;&#35821;&#35328;&#33021;&#21147;&#19978;&#20559;&#21521;&#33521;&#35821;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#26500;&#24314;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#23545;&#40784;&#65292;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#32763;&#35793;&#20219;&#21153;&#25968;&#25454;&#21644;&#36328;&#35821;&#35328;&#36890;&#29992;&#20219;&#21153;&#25968;&#25454;&#23545;LLaMA&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65292;&#24471;&#21040;&#36328;&#35821;&#35328;&#27169;&#22411;&#65288;x-LLaMA&#65289;&#12290;&#22312;&#36328;&#35821;&#35328;&#22522;&#20934;XQUAD&#21644;MLQA&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;x-LLaMA&#27169;&#22411;&#22312;&#20845;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#24179;&#22343;&#36229;&#36807;&#33521;&#35821;&#25351;&#20196;&#35843;&#25972;&#30340;&#27169;&#22411;&#65288;Alpaca&#65289;42.50%&#12290;&#22312;&#20013;&#25991;&#22522;&#20934;C-Eval&#19978;&#30340;&#36827;&#19968;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;x-LLaMA&#22312;&#20013;&#25991;&#20154;&#25991;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#36229;&#36807;Alpaca 8.2%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#32763;&#35793;&#25968;&#25454;&#30340;&#30446;&#26631;&#31471;&#21152;&#20837;&#38750;&#33521;&#35821;&#25991;&#26412;&#21487;&#20197;&#26377;&#25928;&#25552;&#21319;&#38750;&#33521;&#35821;&#33021;&#21147;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;LLM&#20869;&#37096;&#30340;&#35821;&#20041;&#23545;&#40784;&#21487;&#20197;&#36827;&#19968;&#27493;&#21152;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the unbalanced training data distribution, the language ability of large language models (LLMs) is often biased towards English. In this paper, we propose to empower pre-trained LLMs on non-English languages by building semantic alignment across languages. We perform instruction-tuning on LLaMA with both translation task data and cross-lingual general task data to obtain cross-lingual models (x-LLaMA). Experiment results on cross-lingual benchmark XQUAD and MLQA show that x-LLaMA models outperform the English instruction-tuned counterpart (Alpaca) by 42.50% on average on six non-English languages. Further experiments on Chinese benchmark C-Eval show that x-LLaMA achieves significant improvement on Chinese humanities tasks, outperforming Alpaca by 8.2%. We also discover that incorporating non-English text on the target side of translation data is particularly effective for boosting non-English ability. Besides, we find that semantic alignment within LLM can be further strengthene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#36827;&#32780;&#25552;&#39640;&#38646;&#26679;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01313</link><description>&lt;p&gt;
&#26356;&#22810;&#19978;&#19979;&#25991;&#65292;&#26356;&#23569;&#24178;&#25200;&#65306;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#36827;&#34892;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes. (arXiv:2308.01313v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#36827;&#32780;&#25552;&#39640;&#38646;&#26679;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#20316;&#20026;&#19968;&#31181;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#29702;&#35299;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#33021;&#21147;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20805;&#20998;&#21033;&#29992;CLIP&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#20154;&#31867;&#33324;&#29702;&#35299;&#33021;&#21147;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#20154;&#31867;&#30340;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#20013;&#24471;&#21040;&#21551;&#21457;&#65306;&#29616;&#20195;&#31070;&#32463;&#31185;&#23398;&#35266;&#28857;&#35748;&#20026;&#65292;&#22312;&#23545;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;&#20154;&#31867;&#39318;&#20808;&#25512;&#26029;&#20854;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#23646;&#24615;&#65288;&#22914;&#32972;&#26223;&#21644;&#26041;&#21521;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#23558;&#21069;&#26223;&#23545;&#35937;&#19982;&#32972;&#26223;&#21306;&#20998;&#24320;&#26469;&#65292;&#28982;&#21518;&#20197;&#27492;&#20449;&#24687;&#20026;&#22522;&#30784;&#36827;&#34892;&#20915;&#31574;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20026;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#21487;&#20197;&#25913;&#21892;&#38646;&#26679;&#26412;&#20998;&#31867;&#24182;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;CLIP&#26412;&#36523;&#21487;&#20197;&#21512;&#29702;&#22320;&#20174;&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#36825;&#20123;&#23646;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#35757;&#32451;&#12289;&#20004;&#27493;&#39588;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
CLIP, as a foundational vision language model, is widely used in zero-shot image classification due to its ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better zero-shot classification is still an open question. This paper draws inspiration from the human visual perception process: a modern neuroscience view suggests that in classifying an object, humans first infer its class-independent attributes (e.g., background and orientation) which help separate the foreground object from the background, and then make decisions based on this information. Inspired by this, we observe that providing CLIP with contextual attributes improves zero-shot classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot cl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31169;&#23494;&#27700;&#21360;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#65292;&#24182;&#20849;&#20139;&#37096;&#20998;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#39640;&#20934;&#30830;&#24615;&#30340;&#26816;&#27979;&#65292;&#21516;&#26102;&#23545;&#29983;&#25104;&#21644;&#26816;&#27979;&#36895;&#24230;&#24433;&#21709;&#26368;&#23567;&#12290;</title><link>http://arxiv.org/abs/2307.16230</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31169;&#23494;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
A Private Watermark for Large Language Models. (arXiv:2307.16230v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16230
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31169;&#23494;&#27700;&#21360;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#65292;&#24182;&#20849;&#20139;&#37096;&#20998;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#39640;&#20934;&#30830;&#24615;&#30340;&#26816;&#27979;&#65292;&#21516;&#26102;&#23545;&#29983;&#25104;&#21644;&#26816;&#27979;&#36895;&#24230;&#24433;&#21709;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#24050;&#32463;&#20943;&#36731;&#20102;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#24102;&#26469;&#30340;&#20266;&#26032;&#38395;&#21644;&#29256;&#26435;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#30340;&#27700;&#21360;&#26816;&#27979;&#38656;&#35201;&#29983;&#25104;&#36807;&#31243;&#30340;&#23494;&#38053;&#65292;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#36829;&#35268;&#21644;&#20266;&#36896;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31169;&#23494;&#27700;&#21360;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#38454;&#27573;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#32780;&#19981;&#26159;&#20351;&#29992;&#30456;&#21516;&#30340;&#23494;&#38053;&#26469;&#25193;&#23637;&#24403;&#21069;&#30340;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#32593;&#32476;&#30340;&#37096;&#20998;&#21442;&#25968;&#26159;&#20849;&#20139;&#30340;&#65292;&#36825;&#20351;&#24471;&#26816;&#27979;&#32593;&#32476;&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30001;&#20110;&#20004;&#20010;&#32593;&#32476;&#30340;&#21442;&#25968;&#35268;&#27169;&#36739;&#23567;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30830;&#20445;&#20102;&#39640;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;&#29983;&#25104;&#21644;&#26816;&#27979;&#36895;&#24230;&#30340;&#24433;&#21709;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, text watermarking algorithms for large language models (LLMs) have been mitigating the potential harms of text generated by the LLMs, including fake news and copyright issues. However, the watermark detection of current text algorithms requires the key from the generation process, making them susceptible to breaches and counterfeiting. In this work, we propose the first private watermarking algorithm, which extends the current text watermarking algorithms by using two different neural networks respectively for watermark generation and detection, rather than using the same key at both stages. Meanwhile, part of the parameters of the watermark generation and detection networks are shared, which makes the detection network achieve a high accuracy very efficiently. Experiments show that our algorithm ensures high detection accuracy with minimal impact on generation and detection speed, due to the small parameter size of both networks. Additionally, our subsequent analysis demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24605;&#32500;&#30340;&#39592;&#26550;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#35299;&#30721;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.15337</link><description>&lt;p&gt;
&#24605;&#32500;&#30340;&#39592;&#26550;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#24182;&#34892;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding. (arXiv:2307.15337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24605;&#32500;&#30340;&#39592;&#26550;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#35299;&#30721;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#24310;&#36831;&#12290;&#39640;&#29983;&#25104;&#24310;&#36831;&#30340;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#20960;&#20046;&#25152;&#26377;&#26368;&#20808;&#36827;&#30340;LLMs&#37117;&#37319;&#29992;&#20102;&#39034;&#24207;&#35299;&#30721;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#21463;&#21040;&#20154;&#31867;&#30340;&#24605;&#32771;&#21644;&#20889;&#20316;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24605;&#32500;&#30340;&#39592;&#26550;&#8221;&#65288;SoT&#65289;&#65292;&#23427;&#25351;&#23548;LLMs&#39318;&#20808;&#29983;&#25104;&#31572;&#26696;&#30340;&#39592;&#26550;&#65292;&#28982;&#21518;&#36890;&#36807;&#24182;&#34892;API&#35843;&#29992;&#25110;&#25209;&#37327;&#35299;&#30721;&#26469;&#24182;&#34892;&#23436;&#25104;&#27599;&#20010;&#39592;&#26550;&#28857;&#30340;&#20869;&#23481;&#12290;SoT&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#65288;&#22312;11&#20010;&#19981;&#21516;&#30340;LLMs&#19978;&#25552;&#39640;&#20102;&#26368;&#22810;2.39&#20493;&#65289;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#22312;&#22810;&#20010;&#38382;&#39064;&#31867;&#21035;&#19978;&#30340;&#31572;&#26696;&#36136;&#37327;&#65292;&#21253;&#25324;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;SoT&#26159;&#19968;&#31181;&#38024;&#23545;&#25928;&#29575;&#30340;&#25968;&#25454;&#23548;&#21521;&#20248;&#21270;&#30340;&#21021;&#27493;&#23581;&#35797;&#65292;&#24182;&#25581;&#31034;&#20102;&#23558;LLMs&#25512;&#21160;&#26356;&#20687;&#20154;&#31867;&#24605;&#32771;&#20197;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose "Skeleton-of-Thought" (SoT), which guides LLMs to first generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-up (up to 2.39x across 11 different LLMs), but it can also potentially improve the answer quality on several question categories in terms of diversity and relevance. SoT is an initial attempt at data-centric optimization for efficiency, and reveal the potential of pushing LLMs to think more like a human for answer quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10864</link><description>&lt;p&gt;
&#23558;&#27880;&#24847;&#21147;&#20998;&#21106;&#19982;&#32465;&#23450;&#29992;&#20110;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing. (arXiv:2307.10864v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#65292;&#23637;&#31034;&#20102;&#39640;&#24230;&#36924;&#30495;&#30340;&#21387;&#20498;&#24615;&#32467;&#26524;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#23436;&#20840;&#20381;&#29031;&#36755;&#20837;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#8212;&#8212;&#20851;&#27880;&#19982;&#28608;&#21457;&#65292;&#24341;&#20837;&#20102;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#65288;GSN&#65289;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#22312;&#25512;&#26029;&#26102;&#20248;&#21270;&#36328;&#27880;&#24847;&#21147;&#20197;&#26356;&#22909;&#22320;&#34701;&#20837;&#35821;&#20041;&#12290;&#23427;&#22312;&#29983;&#25104;&#31616;&#21333;&#25552;&#31034;&#65292;&#22914;&#8220;&#19968;&#21482;&#29483;&#21644;&#19968;&#21482;&#29399;&#8221;&#65292;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#25552;&#31034;&#20197;&#21450;&#35299;&#20915;&#19981;&#36866;&#24403;&#30340;&#23646;&#24615;&#32465;&#23450;&#38382;&#39064;&#26041;&#38754;&#30340;&#21151;&#25928;&#26377;&#25152;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#22797;&#26434;&#25552;&#31034;&#25110;&#28041;&#21450;&#22810;&#20010;&#23454;&#20307;&#30340;&#22330;&#26223;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#25913;&#36827;&#30340;&#23646;&#24615;&#32465;&#23450;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#21106;&#19982;&#32465;&#23450;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;GSN&#25439;&#22833;&#30446;&#26631;&#65306;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#20002;&#22833;&#21644;&#19968;&#31181;&#32465;&#23450;&#20002;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#23558;&#35821;&#20041;&#32435;&#20837;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#29305;&#28857;&#19978;&#33073;&#39062;&#32780;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend &amp; Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide &amp; Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to fa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#29983;&#25104;&#30340;&#20851;&#38190;&#35789;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#37325;&#35201;&#30340;&#20196;&#29260;&#21644;&#21547;&#26377;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#34987;&#21516;&#31561;&#25110;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01379</link><description>&lt;p&gt;
&#23558;&#20851;&#27880;&#28857;&#36716;&#31227;&#21040;&#30456;&#20851;&#24615;&#19978;: &#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models. (arXiv:2307.01379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#29983;&#25104;&#30340;&#20851;&#38190;&#35789;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#37325;&#35201;&#30340;&#20196;&#29260;&#21644;&#21547;&#26377;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#34987;&#21516;&#31561;&#25110;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#23545;&#20110;&#27169;&#22411;&#29983;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#29305;&#24449;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#29992;&#25143;&#20309;&#26102;&#21487;&#20197;&#20449;&#20219;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#19968;&#20123;&#21551;&#21457;&#24615;&#30340;&#20107;&#23454;&#65292;&#21363;&#22312;&#33258;&#22238;&#24402;&#30340;LLMs&#20013;&#65292;&#20196;&#29260;&#22312;&#21453;&#26144;&#29983;&#25104;&#30340;&#21547;&#20041;&#26041;&#38754;&#26159;&#19981;&#24179;&#31561;&#30340;&#65292;&#21363;&#19968;&#20123;&#20196;&#29260;&#27604;&#20854;&#20182;&#20196;&#29260;&#26356;&#30456;&#20851;&#65288;&#25110;&#26356;&#20855;&#20195;&#34920;&#24615;&#65289;&#65292;&#28982;&#32780;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#25152;&#26377;&#30340;&#20196;&#29260;&#34987;&#31561;&#20540;&#23545;&#24453;&#12290;&#36825;&#26159;&#30001;&#20110;&#35821;&#35328;&#20887;&#20313;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#65292;&#21482;&#38656;&#35201;&#20960;&#20010;&#20851;&#38190;&#35789;&#23601;&#36275;&#20197;&#20256;&#36798;&#19968;&#20010;&#38271;&#21477;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#19981;&#24179;&#31561;&#31216;&#20026;&#29983;&#25104;&#30340;&#19981;&#24179;&#31561;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#19981;&#30830;&#23450;&#24615;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#65292;&#30456;&#24403;&#25968;&#37327;&#30340;&#20196;&#29260;&#21644;&#21253;&#21547;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#65292;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#34987;&#21516;&#31561;&#25110;&#29978;&#33267;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#29983;&#25104;&#30340;&#19981;&#24179;&#31561;&#24341;&#36215;&#30340;&#36825;&#20123;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20294;&#26377;&#25928;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;ContextSpeech&#65292;&#36890;&#36807;&#35774;&#35745;&#20869;&#23384;&#32531;&#23384;&#30340;&#24490;&#29615;&#26426;&#21046;&#21644;&#26500;&#24314;&#23618;&#27425;&#21270;&#30340;&#25991;&#26412;&#35821;&#20041;&#32467;&#26500;&#65292;&#23558;&#20840;&#23616;&#25991;&#26412;&#21644;&#35821;&#38899;&#19978;&#19979;&#25991;&#34701;&#20837;&#21040;&#21477;&#23376;&#32534;&#30721;&#20013;&#65292;&#20197;&#35299;&#20915;&#27573;&#33853;&#38405;&#35835;&#20013;&#30340;&#35821;&#38899;&#29983;&#25104;&#25361;&#25112;&#65292;&#24182;&#19988;&#20351;&#29992;&#32447;&#24615;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.00782</link><description>&lt;p&gt;
ContextSpeech&#65306;&#29992;&#20110;&#27573;&#33853;&#38405;&#35835;&#30340;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading. (arXiv:2307.00782v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20294;&#26377;&#25928;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;ContextSpeech&#65292;&#36890;&#36807;&#35774;&#35745;&#20869;&#23384;&#32531;&#23384;&#30340;&#24490;&#29615;&#26426;&#21046;&#21644;&#26500;&#24314;&#23618;&#27425;&#21270;&#30340;&#25991;&#26412;&#35821;&#20041;&#32467;&#26500;&#65292;&#23558;&#20840;&#23616;&#25991;&#26412;&#21644;&#35821;&#38899;&#19978;&#19979;&#25991;&#34701;&#20837;&#21040;&#21477;&#23376;&#32534;&#30721;&#20013;&#65292;&#20197;&#35299;&#20915;&#27573;&#33853;&#38405;&#35835;&#20013;&#30340;&#35821;&#38899;&#29983;&#25104;&#25361;&#25112;&#65292;&#24182;&#19988;&#20351;&#29992;&#32447;&#24615;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;&#21487;&#20197;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#29983;&#25104;&#38750;&#24120;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#38899;&#65292;&#20294;&#23427;&#20204;&#22312;&#27573;&#33853;/&#38271;&#31687;&#38405;&#35835;&#30340;&#35821;&#38899;&#29983;&#25104;&#26041;&#38754;&#20173;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#19981;&#36275;&#20043;&#22788;&#20027;&#35201;&#26159;&#22240;&#20026;&#65306;&#19968;&#26159;&#24573;&#35270;&#20102;&#36328;&#21477;&#23376;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20108;&#26159;&#38271;&#31687;&#21512;&#25104;&#36807;&#31243;&#20013;&#30340;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20294;&#26377;&#25928;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;ContextSpeech&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#20869;&#23384;&#32531;&#23384;&#30340;&#24490;&#29615;&#26426;&#21046;&#65292;&#23558;&#20840;&#23616;&#25991;&#26412;&#21644;&#35821;&#38899;&#19978;&#19979;&#25991;&#34701;&#20837;&#21040;&#21477;&#23376;&#32534;&#30721;&#20013;&#65307;&#28982;&#21518;&#26500;&#24314;&#20102;&#23618;&#27425;&#21270;&#30340;&#25991;&#26412;&#35821;&#20041;&#32467;&#26500;&#65292;&#20197;&#25193;&#22823;&#20840;&#23616;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#33539;&#22260;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25972;&#21512;&#20102;&#32447;&#24615;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ContextSpeech&#22312;&#27573;&#33853;&#38405;&#35835;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#35821;&#38899;&#36136;&#37327;&#21644;&#38901;&#24459;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#20855;&#22791;&#31454;&#20105;&#21147;&#30340;&#27169;&#22411;&#25928;&#29575;&#12290;&#38899;&#39057;&#26679;&#26412;&#21487;&#35775;&#38382;&#38142;&#25509;&#65306;https://contextspeech.
&lt;/p&gt;
&lt;p&gt;
While state-of-the-art Text-to-Speech systems can generate natural speech of very high quality at sentence level, they still meet great challenges in speech generation for paragraph / long-form reading. Such deficiencies are due to i) ignorance of cross-sentence contextual information, and ii) high computation and memory cost for long-form synthesis. To address these issues, this work develops a lightweight yet effective TTS system, ContextSpeech. Specifically, we first design a memory-cached recurrence mechanism to incorporate global text and speech context into sentence encoding. Then we construct hierarchically-structured textual semantics to broaden the scope for global context enhancement. Additionally, we integrate linearized self-attention to improve model efficiency. Experiments show that ContextSpeech significantly improves the voice quality and prosody expressiveness in paragraph reading with competitive model efficiency. Audio samples are available at: https://contextspeech.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#21407;&#22987;GP&#31508;&#35760;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#20013;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17175</link><description>&lt;p&gt;
&#20174;&#21407;&#22987;&#30340;GP&#31508;&#35760;&#20013;&#25366;&#25496;&#30693;&#35782;&#22270;&#35889;&#65292;&#29992;&#20110;&#36828;&#31243;COVID-19&#21021;&#32423;&#20445;&#20581;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19 Assessment in Primary Care. (arXiv:2306.17175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#21407;&#22987;GP&#31508;&#35760;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#20013;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20915;&#31574;&#26159;&#21521;&#24739;&#32773;&#25552;&#20379;&#36866;&#24403;&#25252;&#29702;&#30340;&#22522;&#26412;&#38454;&#27573;&#12290;&#36817;&#24180;&#26469;&#65292;&#20026;&#20102;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#20570;&#20986;&#20915;&#31574;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#20010;&#20915;&#31574;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20351;&#29992;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#31616;&#21333;&#30340;&#22238;&#24402;&#27169;&#22411;&#65292;&#21482;&#33021;&#32771;&#34385;&#31616;&#21333;&#30340;&#39044;&#23450;&#20041;&#22810;&#36873;&#29305;&#24449;&#65292;&#22914;&#24739;&#32773;&#24180;&#40836;&#12289;&#26082;&#24448;&#30149;&#21490;&#12289;&#21560;&#28895;&#32773;&#29366;&#20917;&#31561;&#12290;&#20915;&#31574;&#31995;&#32479;&#24403;&#21069;&#26080;&#27861;&#22788;&#29702;&#30340;&#19968;&#20010;&#29305;&#23450;&#24739;&#32773;&#25968;&#25454;&#26469;&#28304;&#26159;&#24739;&#32773;&#20250;&#35786;&#30340;GP&#31508;&#35760;&#30340;&#25910;&#38598;&#12290;&#36825;&#20123;&#31508;&#35760;&#21253;&#21547;&#20102;&#20020;&#24202;&#21307;&#29983;&#29992;&#26469;&#20570;&#20986;&#26368;&#32456;&#20915;&#31574;&#24182;&#23558;&#24739;&#32773;&#24341;&#23548;&#21040;&#36866;&#24403;&#25252;&#29702;&#30340;&#20851;&#38190;&#20307;&#24449;&#21644;&#30151;&#29366;&#12290;&#20174;GP&#31508;&#35760;&#20013;&#25552;&#21462;&#20449;&#24687;&#26159;&#19968;&#20010;&#25216;&#26415;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#21253;&#21547;&#32553;&#20889;&#12289;&#25171;&#23383;&#38169;&#35823;&#21644;&#19981;&#23436;&#25972;&#30340;&#21477;&#23376;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20010;&#20844;&#24320;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#25191;&#34892;&#20174;&#21407;&#22987;GP&#31508;&#35760;&#20013;&#25552;&#21462;&#20986;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical decision-making is a fundamental stage in delivering appropriate care to patients. In recent years several decision-making systems designed to aid the clinician in this process have been developed. However, technical solutions currently in use are based on simple regression models and are only able to take into account simple pre-defined multiple-choice features, such as patient age, pre-existing conditions, smoker status, etc. One particular source of patient data, that available decision-making systems are incapable of processing is the collection of patient consultation GP notes. These contain crucial signs and symptoms - the information used by clinicians in order to make a final decision and direct the patient to the appropriate care. Extracting information from GP notes is a technically challenging problem, as they tend to include abbreviations, typos, and incomplete sentences.  This paper addresses this open challenge. We present a framework that performs knowledge grap
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#20248;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#23884;&#20837;&#30340;&#32452;&#21512;&#20250;&#30053;&#24494;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#24182;&#19988;&#32452;&#21512;&#26041;&#24335;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2306.14939</link><description>&lt;p&gt;
&#23884;&#20837;&#34701;&#21512;&#30340;&#33402;&#26415;&#65306;&#20248;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
The Art of Embedding Fusion: Optimizing Hate Speech Detection. (arXiv:2306.14939v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14939
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#20248;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#23884;&#20837;&#30340;&#32452;&#21512;&#20250;&#30053;&#24494;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#24182;&#19988;&#32452;&#21512;&#26041;&#24335;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#38656;&#35201;&#25429;&#25417;&#35821;&#35328;&#21644;&#35821;&#22659;&#32454;&#24494;&#24046;&#21035;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#25991;&#26412;&#35821;&#20041;&#34920;&#31034;&#65292;&#21487;&#20197;&#25913;&#36827;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26377;&#25928;&#22320;&#32452;&#21512;PLMs&#30340;&#34920;&#31034;&#21644;&#21033;&#29992;&#23427;&#20204;&#30340;&#20114;&#34917;&#20248;&#21183;&#30340;&#26041;&#27861;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20960;&#31181;PLMs&#32452;&#21512;&#25216;&#26415;&#30340;&#26041;&#24335;&#65292;&#24182;&#20840;&#38754;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32452;&#21512;&#23884;&#20837;&#21487;&#20197;&#30053;&#24494;&#25913;&#21892;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#65292;&#32452;&#21512;&#26041;&#24335;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;&#25105;&#20204;&#36824;&#22312;https://github.com/aflah02/The-Art-of-Embedding-Fusion-Optimizing-Hate-Speech-Detection&#19978;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech detection is a challenging natural language processing task that requires capturing linguistic and contextual nuances. Pre-trained language models (PLMs) offer rich semantic representations of text that can improve this task. However there is still limited knowledge about ways to effectively combine representations across PLMs and leverage their complementary strengths. In this work, we shed light on various combination techniques for several PLMs and comprehensively analyze their effectiveness. Our findings show that combining embeddings leads to slight improvements but at a high computational cost and the choice of combination has marginal effect on the final outcome. We also make our codebase public at https://github.com/aflah02/The-Art-of-Embedding-Fusion-Optimizing-Hate-Speech-Detection .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23545;&#29031;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#30456;&#21453;&#24773;&#24863;&#26497;&#24615;&#30340;&#35266;&#28857;&#34920;&#36798;&#65292;&#24182;&#21033;&#29992;T5&#27169;&#22411;&#26816;&#32034;&#25513;&#30721;&#65292;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;ABSA&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.11260</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23545;&#29031;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A novel Counterfactual method for aspect-based sentiment analysis. (arXiv:2306.11260v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23545;&#29031;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#30456;&#21453;&#24773;&#24863;&#26497;&#24615;&#30340;&#35266;&#28857;&#34920;&#36798;&#65292;&#24182;&#21033;&#29992;T5&#27169;&#22411;&#26816;&#32034;&#25513;&#30721;&#65292;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;ABSA&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#38754;&#24773;&#24863;&#20998;&#26512; (ABSA) &#26159;&#19968;&#39033;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#35780;&#20272;&#20219;&#21153;&#65292;&#23427;&#20998;&#26512;&#35780;&#20272;&#26041;&#38754;&#30340;&#24773;&#24863;&#26497;&#24615;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#24037;&#20316;&#20165;&#20851;&#27880;&#35266;&#28857;&#34920;&#36798;&#30340;&#35782;&#21035;&#65292;&#32780;&#24573;&#30053;&#20102;&#35266;&#28857;&#34920;&#36798;&#30340;&#22810;&#26679;&#24615;&#23545;ABSA&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23545;&#29031;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#30456;&#21453;&#24773;&#24863;&#26497;&#24615;&#30340;&#35266;&#28857;&#34920;&#36798;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#38598;&#25104;&#26799;&#24230;&#26469;&#35782;&#21035;&#21644;&#23631;&#34109;&#35266;&#28857;&#34920;&#36798;&#12290;&#28982;&#21518;&#23558;&#21453;&#21521;&#26631;&#31614;&#30340;&#25552;&#31034;&#32452;&#21512;&#21040;&#21407;&#22987;&#25991;&#26412;&#20013;&#65292;&#24182;&#26368;&#32456;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (PLM) T5 &#26469;&#26816;&#32034;&#25513;&#30721;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#23545;&#29031;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#22312;&#19977;&#20010;ABSA&#25968;&#25454;&#38598; (&#21363;Laptop&#65292;Restaurant&#21644;MAMS) &#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based-sentiment-analysis (ABSA) is a fine-grained sentiment evaluation task, which analyze the emotional polarity of the evaluation aspects. However, previous works only focus on the identification of opinion expressions, forget that the diversity of opinion expressions also has great impacts on the ABSA task. To mitigate this problem, we propose a novel counterfactual data augmentation method to generate opinion expression with reversed sentiment polarity. Specially, the integrated gradients are calculated to identify and mask the opinion expression. Then, a prompt with the reverse label is combined to the original text, and a pre-trained language model (PLM), T5, is finally employed to retrieve the masks. The experimental results show the proposed counterfactual data augmentation method perform better than current augmentation methods on three ABSA datasets, i.e. Laptop, Restaurant and MAMS.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#36890;&#29992;&#27169;&#22359;&#20197;&#35299;&#20915; ChatGPT &#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24369;&#28857;&#65292;&#21253;&#25324;&#21033;&#29992;&#22810;&#20010;&#25552;&#31034;&#31526;&#26469;&#36866;&#24212;&#26356;&#22810;&#28436;&#31034;&#12289;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28436;&#31034;&#26816;&#32034;&#12289;&#36716;&#25442;&#20219;&#21153;&#20026;&#26356;&#36866;&#21512;&#29983;&#25104;&#24615;&#36136;&#30340;&#26684;&#24335;&#20197;&#21450;&#37319;&#29992;&#38024;&#23545; NLP &#20219;&#21153;&#35774;&#35745;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.09719</link><description>&lt;p&gt;
&#25512;&#21160; ChatGPT &#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Pushing the Limits of ChatGPT on NLP Tasks. (arXiv:2306.09719v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#36890;&#29992;&#27169;&#22359;&#20197;&#35299;&#20915; ChatGPT &#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24369;&#28857;&#65292;&#21253;&#25324;&#21033;&#29992;&#22810;&#20010;&#25552;&#31034;&#31526;&#26469;&#36866;&#24212;&#26356;&#22810;&#28436;&#31034;&#12289;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28436;&#31034;&#26816;&#32034;&#12289;&#36716;&#25442;&#20219;&#21153;&#20026;&#26356;&#36866;&#21512;&#29983;&#25104;&#24615;&#36136;&#30340;&#26684;&#24335;&#20197;&#21450;&#37319;&#29992;&#38024;&#23545; NLP &#20219;&#21153;&#35774;&#35745;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649; ChatGPT &#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#65292;&#20854;&#34920;&#29616;&#20173;&#36828;&#20302;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20854;&#20013;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#20854;&#34920;&#29616;&#27424;&#20339;&#30340;&#21407;&#22240;&#20027;&#35201;&#26377;&#65306;&#65288;1&#65289;&#25552;&#31034;&#31526;&#20013;&#30340;&#20196;&#29260;&#38480;&#21046;&#19981;&#20801;&#35768;&#20805;&#20998;&#21033;&#29992;&#30417;&#30563;&#25968;&#25454;&#38598;&#65307;&#65288;2&#65289;ChatGPT &#29983;&#25104;&#24615;&#36136;&#19982; NLP &#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#65307;&#65288;3&#65289;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22266;&#26377;&#24369;&#28857;&#65292;&#22914;&#20135;&#29983;&#24187;&#35273;&#12289;&#36807;&#24230;&#20851;&#27880;&#29305;&#23450;&#20851;&#38190;&#35789;&#31561;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#36890;&#29992;&#27169;&#22359;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26088;&#22312;&#25512;&#21160; ChatGPT &#22312; NLP &#20219;&#21153;&#19978;&#30340;&#26497;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22359;&#21253;&#25324;&#65306;&#65288;1&#65289;&#19968;&#31181;&#36755;&#20837;&#22810;&#25552;&#31034;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#22810;&#20010;&#25552;&#31034;&#31526;&#26469;&#36866;&#24212;&#26356;&#22810;&#28436;&#31034;&#65307;&#65288;2&#65289;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28436;&#31034;&#26816;&#32034;&#65307;&#65288;3&#65289;&#23558;&#20219;&#21153;&#36716;&#25442;&#20026;&#26356;&#36866;&#21512;&#29983;&#25104;&#24615;&#36136;&#30340;&#26684;&#24335;&#65307;&#65288;4&#65289;&#37319;&#29992;&#38024;&#23545; NLP &#20219;&#21153;&#35774;&#35745;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of ChatGPT, its performances on most NLP tasks are still well below the supervised baselines. In this work, we looked into the causes, and discovered that its subpar performance was caused by the following factors: (1) token limit in the prompt does not allow for the full utilization of the supervised datasets; (2) mismatch between the generation nature of ChatGPT and NLP tasks; (3) intrinsic pitfalls of LLMs models, e.g., hallucination, overly focus on certain keywords, etc.  In this work, we propose a collection of general modules to address these issues, in an attempt to push the limits of ChatGPT on NLP tasks. Our proposed modules include (1) a one-input-multiple-prompts strategy that employs multiple prompts for one input to accommodate more demonstrations; (2) using fine-tuned models for better demonstration retrieval; (3) transforming tasks to formats that are more tailored to the generation nature; (4) employing reasoning strategies that are tailored to addr
&lt;/p&gt;</description></item><item><title>PEACE&#26159;&#19968;&#20010;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#22240;&#26524;&#25351;&#23548;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#28304;&#24179;&#21488;&#25968;&#25454;&#24182;&#25512;&#24191;&#21040;&#30446;&#26631;&#24179;&#21488;&#65292;&#25506;&#32034;&#22914;&#20309;&#24314;&#31435;&#36866;&#29992;&#20110;&#19981;&#21516;&#24179;&#21488;&#30340;&#36890;&#29992;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.08804</link><description>&lt;p&gt;
PEACE: &#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;- &#19968;&#20010;&#22240;&#26524;&#25351;&#23548;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PEACE: Cross-Platform Hate Speech Detection- A Causality-guided Framework. (arXiv:2306.08804v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08804
&lt;/p&gt;
&lt;p&gt;
PEACE&#26159;&#19968;&#20010;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#22240;&#26524;&#25351;&#23548;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#28304;&#24179;&#21488;&#25968;&#25454;&#24182;&#25512;&#24191;&#21040;&#30446;&#26631;&#24179;&#21488;&#65292;&#25506;&#32034;&#22914;&#20309;&#24314;&#31435;&#36866;&#29992;&#20110;&#19981;&#21516;&#24179;&#21488;&#30340;&#36890;&#29992;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26159;&#25351;&#26816;&#27979;&#22522;&#20110;&#23447;&#25945;&#12289;&#24615;&#21035;&#12289;&#24615;&#21462;&#21521;&#25110;&#20854;&#20182;&#29305;&#24449;&#32780;&#38024;&#23545;&#20010;&#20154;&#25110;&#32676;&#20307;&#30340;&#24694;&#24847;&#20869;&#23481;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#19981;&#21516;&#24179;&#21488;&#30340;&#25919;&#31574;&#19981;&#21516;&#65292;&#19981;&#21516;&#32676;&#20307;&#20197;&#19981;&#21516;&#26041;&#24335;&#34920;&#36798;&#20167;&#24680;&#35328;&#35770;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#26576;&#20123;&#24179;&#21488;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#65292;&#26500;&#24314;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#26159;&#21542;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;&#36866;&#29992;&#20110;&#36328;&#24179;&#21488;&#35774;&#32622;&#30340;&#21487;&#36801;&#31227;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#21363;&#25105;&#20204;&#22312;&#19968;&#20010;&#65288;&#28304;&#65289;&#24179;&#21488;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#27169;&#22411;&#25512;&#24191;&#21040;&#22810;&#20010;&#65288;&#30446;&#26631;&#65289;&#24179;&#21488;&#12290;&#29616;&#26377;&#30340;&#25512;&#24191;&#27169;&#22411;&#20381;&#36182;&#20110;&#35821;&#35328;&#32447;&#32034;&#25110;&#36741;&#21161;&#20449;&#24687;&#65292;&#20351;&#20854;&#20559;&#21521;&#20110;&#28304;&#24179;&#21488;&#19978;&#30340;&#26576;&#20123;&#26631;&#31614;&#25110;&#26576;&#20123;&#31867;&#22411;&#30340;&#35789;&#65288;&#22914;&#36785;&#39554;&#24615;&#35789;&#35821;&#65289;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#30446;&#26631;&#24179;&#21488;&#12290;&#21463;&#21040;&#31038;&#20250;&#21644;&#24515;&#29702;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21162;&#21147;&#25506;&#32034;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#22240;&#26524;&#20851;&#31995;&#65292;&#21487;&#20197;&#20174;&#28304;&#24179;&#21488;&#23398;&#20064;&#29305;&#24449;&#24182;&#25512;&#24191;&#21040;&#30446;&#26631;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech detection refers to the task of detecting hateful content that aims at denigrating an individual or a group based on their religion, gender, sexual orientation, or other characteristics. Due to the different policies of the platforms, different groups of people express hate in different ways. Furthermore, due to the lack of labeled data in some platforms it becomes challenging to build hate speech detection models. To this end, we revisit if we can learn a generalizable hate speech detection model for the cross platform setting, where we train the model on the data from one (source) platform and generalize the model across multiple (target) platforms. Existing generalization models rely on linguistic cues or auxiliary information, making them biased towards certain tags or certain kinds of words (e.g., abusive words) on the source platform and thus not applicable to the target platforms. Inspired by social and psychological theories, we endeavor to explore if there exist in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#26469;&#35780;&#20272;&#32452;&#20214;&#26159;&#21542;&#34920;&#31034;&#23646;&#24615;&#65292;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20851;&#20110;&#8220;&#34920;&#31034;&#8221;&#30340;&#21746;&#23398;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2306.08193</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#31034;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Operationalising Representation in Natural Language Processing. (arXiv:2306.08193v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#26469;&#35780;&#20272;&#32452;&#20214;&#26159;&#21542;&#34920;&#31034;&#23646;&#24615;&#65292;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20851;&#20110;&#8220;&#34920;&#31034;&#8221;&#30340;&#21746;&#23398;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#8220;&#34920;&#31034;&#8221;&#22312;&#35748;&#30693;&#31185;&#23398;&#21746;&#23398;&#20013;&#20855;&#26377;&#26680;&#24515;&#22320;&#20301;&#65292;&#20294;&#22312;&#24403;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23454;&#36341;&#20013;&#65292;&#20960;&#20046;&#27809;&#26377;&#21746;&#23398;&#39046;&#22495;&#30340;&#20808;&#21069;&#30740;&#31350;&#19982;&#20043;&#28041;&#21450;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65306;&#32467;&#21512;&#35748;&#30693;&#31185;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#32452;&#20214;&#25152;&#20316;&#20986;&#30340;&#34920;&#31034;&#24615;&#22768;&#26126;&#65292;&#24182;&#25552;&#20986;&#19977;&#20010;&#35780;&#20272;&#32452;&#20214;&#26159;&#21542;&#34920;&#31034;&#23646;&#24615;&#30340;&#26631;&#20934;&#65292;&#24182;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#26469;&#23454;&#29616;&#36825;&#20123;&#26631;&#20934;&#30340;&#25805;&#20316;&#21270;&#65292;&#25506;&#27979;&#20998;&#31867;&#22120;&#26159;NLP&#65288;&#21644;&#26356;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#65289;&#20013;&#27969;&#34892;&#30340;&#20998;&#26512;&#25216;&#26415;&#12290;&#25805;&#20316;&#21270;&#19968;&#20010;&#22312;&#21746;&#23398;&#19978;&#21463;&#21040;&#21551;&#21457;&#30340;&#8220;&#34920;&#31034;&#8221;&#27010;&#24565;&#30340;&#39033;&#30446;&#24212;&#35813;&#24341;&#36215;&#31185;&#23398;&#21746;&#23398;&#23478;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23454;&#36341;&#32773;&#30340;&#20852;&#36259;&#12290;&#23545;&#20110;&#21746;&#23398;&#23478;&#26469;&#35828;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#26377;&#20851;&#34920;&#31034;&#30340;&#26412;&#36136;&#30340;&#35770;&#25454;&#30340;&#26032;&#39062;&#22330;&#22320;&#65292;&#24182;&#24110;&#21161;NLPers&#32452;&#32455;&#26377;&#20851;&#25506;&#27979;&#23454;&#39564;&#30340;&#22823;&#37327;&#25991;&#29486;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#32463;&#39564;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its centrality in the philosophy of cognitive science, there has been little prior philosophical work engaging with the notion of representation in contemporary NLP practice. This paper attempts to fill that lacuna: drawing on ideas from cognitive science, I introduce a framework for evaluating the representational claims made about components of neural NLP models, proposing three criteria with which to evaluate whether a component of a model represents a property and operationalising these criteria using probing classifiers, a popular analysis technique in NLP (and deep learning more broadly).  The project of operationalising a philosophically-informed notion of representation should be of interest to both philosophers of science and NLP practitioners. It affords philosophers a novel testing-ground for claims about the nature of representation, and helps NLPers organise the large literature on probing experiments, suggesting novel avenues for empirical research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Valley&#30340;&#35270;&#39057;&#21161;&#25163;&#65292;&#23427;&#26159;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#29702;&#35299;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2306.07207</link><description>&lt;p&gt;
Valley: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#35270;&#39057;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Valley: Video Assistant with Large Language model Enhanced abilitY. (arXiv:2306.07207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Valley&#30340;&#35270;&#39057;&#21161;&#25163;&#65292;&#23427;&#26159;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#29702;&#35299;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20197;&#20854;&#21331;&#36234;&#30340;&#20250;&#35805;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#25104;&#20026;&#24378;&#22823;&#30340;AI&#21161;&#25163;&#12290;&#37492;&#20110;&#27492;&#65292;&#19968;&#20010;&#30452;&#35266;&#30340;&#38382;&#39064;&#26159;&#65306;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#26500;&#24314;&#22810;&#27169;&#24577;&#30340;&#35270;&#35273;&#24212;&#29992;AI&#21161;&#25163;&#65311;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#12290;&#23427;&#20204;&#36890;&#24120;&#39044;&#20808;&#35757;&#32451;&#19968;&#20010;&#36866;&#24212;&#27169;&#22359;&#26469;&#23545;&#40784;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#65292;&#28982;&#21518;&#22312;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20010;&#27969;&#31243;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#22312;&#35270;&#39057;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#36824;&#27809;&#26377;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#29702;&#35299;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#35821;&#35328;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Valley&#65292;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#35270;&#39057;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), with their remarkable conversational capabilities, have demonstrated impressive performance across various applications and have emerged as formidable AI assistants. In view of this, it raises an intuitive question: Can we harness the power of LLMs to build multimodal AI assistants for visual applications? Recently, several multi-modal models have been developed for this purpose. They typically pre-train an adaptation module to align the semantics of the vision encoder and language model, followed by fine-tuning on instruction-following data. However, despite the success of this pipeline in image and language understanding, its effectiveness in joint video and language understanding has not been widely explored. In this paper, we aim to develop a novel multi-modal foundation model capable of comprehending video, image, and language within a general framework. To achieve this goal, we introduce Valley, a Video Assistant with Large Language model Enhanced ab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#26469;&#20943;&#23569;&#38750;&#27954;&#21475;&#38899;&#20020;&#24202;ASR&#35757;&#32451;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36229;&#36807;&#29616;&#26377;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#24182;&#25552;&#39640;&#20302;&#36164;&#28304;&#21475;&#38899;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.02105</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#20197;&#24212;&#23545;&#20302;&#36164;&#28304;&#20020;&#24202;&#35821;&#38899;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Adapting Pretrained ASR Models to Low-resource Clinical Speech using Epistemic Uncertainty-based Data Selection. (arXiv:2306.02105v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#26469;&#20943;&#23569;&#38750;&#27954;&#21475;&#38899;&#20020;&#24202;ASR&#35757;&#32451;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36229;&#36807;&#29616;&#26377;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#24182;&#25552;&#39640;&#20302;&#36164;&#28304;&#21475;&#38899;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;ASR&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#23545;&#20110;&#38750;&#27954;&#21475;&#38899;&#30340;&#20020;&#24202;ASR&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#12290;&#22312;&#36825;&#19968;&#39046;&#22495;&#26500;&#24314;&#24378;&#22823;&#30340;ASR&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#29992;&#20110;&#21508;&#31181;&#35821;&#35328;&#21644;&#24418;&#24577;&#20016;&#23500;&#30340;&#21475;&#38899;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#30340;&#21019;&#24314;&#25104;&#26412;&#36739;&#39640;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22522;&#20110;&#20449;&#24687;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#26469;&#20943;&#23569;&#27880;&#37322;&#36153;&#29992;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23558;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#21487;&#20197;&#36229;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;ASR&#27169;&#22411;&#24314;&#31435;&#30340;&#20960;&#20010;&#22522;&#20934;&#32467;&#26524;&#65292;&#21516;&#26102;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#65292;&#20174;&#32780;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#25913;&#21892;&#20102;&#20302;&#36164;&#28304;&#21475;&#38899;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38750;&#27954;&#20020;&#24202;ASR&#30340;&#32972;&#26223;&#19979;&#26500;&#24314;&#27867;&#21270;&#22411;ASR&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#32780;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#20027;&#35201;&#26159;&#31232;&#32570;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
While there has been significant progress in ASR, African-accented clinical ASR has been understudied due to a lack of training datasets. Building robust ASR systems in this domain requires large amounts of annotated or labeled data, for a wide variety of linguistically and morphologically rich accents, which are expensive to create. Our study aims to address this problem by reducing annotation expenses through informative uncertainty-based data selection. We show that incorporating epistemic uncertainty into our adaptation rounds outperforms several baseline results, established using state-of-the-art (SOTA) ASR models, while reducing the required amount of labeled data, and hence reducing annotation costs. Our approach also improves out-of-distribution generalization for very low-resource accents, demonstrating the viability of our approach for building generalizable ASR models in the context of accented African clinical ASR, where training datasets are predominantly scarce.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20449;&#24230;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#40657;&#30418;&#27169;&#22411;&#20013;&#32622;&#20449;&#24230;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#36873;&#25321;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.19187</link><description>&lt;p&gt;
&#29983;&#25104;&#21487;&#20449;&#30340;&#25991;&#26412;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models. (arXiv:2305.19187v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20449;&#24230;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#40657;&#30418;&#27169;&#22411;&#20013;&#32622;&#20449;&#24230;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#36873;&#25321;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#19987;&#38376;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#35780;&#20272;LLMs&#29983;&#25104;&#30340;&#32467;&#26524;&#30340;&#21487;&#20449;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#30740;&#31350;&#20063;&#36739;&#23569;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25991;&#29486;&#36890;&#24120;&#20551;&#23450;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#30333;&#30418;&#35775;&#38382;&#65292;&#36825;&#35201;&#20040;&#26159;&#30001;&#20110;&#26368;&#26032;&#30340;LLMs&#30340;&#23553;&#38381;&#28304;&#20195;&#30721;&#30340;&#24615;&#36136;&#65292;&#35201;&#20040;&#26159;&#30001;&#20110;&#35745;&#31639;&#38480;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#40657;&#30418;LLMs&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#21306;&#20998;&#20102;&#20004;&#31181;&#23494;&#20999;&#30456;&#20851;&#30340;&#27010;&#24565;: &#21482;&#19982;&#36755;&#20837;&#26377;&#20851;&#30340;&#8220;&#19981;&#30830;&#23450;&#24615;&#8221;&#21644;&#36824;&#19982;&#29983;&#25104;&#30340;&#22238;&#22797;&#26377;&#20851;&#30340;&#8220;&#32622;&#20449;&#24230;&#8221;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#20960;&#20010;&#32622;&#20449;&#24230;/&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#8220;&#36873;&#25321;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#8221;&#65292;&#20854;&#20013;&#19981;&#21487;&#38752;&#30340;&#32467;&#26524;&#21487;&#20197;&#34987;&#24573;&#30053;&#25110;&#32773;&#31227;&#20132;&#32473;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or due to computational constraints. In this work, we investigate uncertainty quantification in NLG for $\textit{black-box}$ LLMs. We first differentiate two closely-related notions: $\textit{uncertainty}$, which depends only on the input, and $\textit{confidence}$, which additionally depends on the generated response. We then propose and compare several confidence/uncertainty metrics, applying them to $\textit{selective NLG}$, where unreliable results could either be ignored or yielded for further 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#30340;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#26469;&#26089;&#26399;&#35782;&#21035;&#21387;&#21147;&#21644;&#25233;&#37057;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#20316;&#20026;&#20027;&#20219;&#21153;&#21644;&#36741;&#21161;&#20219;&#21153;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20004;&#31181;&#24773;&#32490;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2305.18907</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#35782;&#21035;&#21387;&#21147;&#21644;&#25233;&#37057;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multitask learning for recognizing stress and depression in social media. (arXiv:2305.18907v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18907
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#30340;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#26469;&#26089;&#26399;&#35782;&#21035;&#21387;&#21147;&#21644;&#25233;&#37057;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#20316;&#20026;&#20027;&#20219;&#21153;&#21644;&#36741;&#21161;&#20219;&#21153;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20004;&#31181;&#24773;&#32490;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#31038;&#20132;&#23186;&#20307;&#20026;&#20154;&#20204;&#34920;&#36798;&#24773;&#24863;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#20449;&#24687;&#26469;&#28304;&#65292;&#32780;&#21387;&#21147;&#21644;&#25233;&#37057;&#22312;&#29616;&#20195;&#20154;&#29983;&#27963;&#30340;&#24555;&#33410;&#22863;&#19979;&#21464;&#24471;&#26222;&#36941;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#26089;&#26399;&#35782;&#21035;&#21387;&#21147;&#21644;&#25233;&#37057;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#19968;&#20123;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#24335;&#34987;&#25552;&#20986;&#65292;&#23558;&#25233;&#37057;&#21644;&#24773;&#32490;&#65288;&#25110;&#24418;&#35937;&#21270;&#35821;&#35328;&#65289;&#20316;&#20026;&#20027;&#20219;&#21153;&#21644;&#36741;&#21161;&#20219;&#21153;&#65292;&#28982;&#32780;&#30740;&#31350;&#20154;&#21592;&#23558;&#21387;&#21147;&#21644;&#25233;&#37057;&#35270;&#20026;&#20004;&#20010;&#29420;&#31435;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#21033;&#29992;&#20004;&#20010;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#25233;&#37057;&#20026;&#20027;&#20219;&#21153;&#65292;&#21387;&#21147;&#20026;&#36741;&#21161;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#25233;&#37057;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#21387;&#21147;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stress and depression are prevalent nowadays across people of all ages due to the quick paces of life. People use social media to express their feelings. Thus, social media constitute a valuable form of information for the early detection of stress and depression. Although many research works have been introduced targeting the early recognition of stress and depression, there are still limitations. There have been proposed multi-task learning settings, which use depression and emotion (or figurative language) as the primary and auxiliary tasks respectively. However, although stress is inextricably linked with depression, researchers face these two tasks as two separate tasks. To address these limitations, we present the first study, which exploits two different datasets collected under different conditions, and introduce two multitask learning frameworks, which use depression and stress as the main and auxiliary tasks respectively. Specifically, we use a depression dataset and a stress
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#25552;&#31034;&#26041;&#27861;&#8212;&#8212;&#20195;&#30721;&#25552;&#31034;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35302;&#21457;&#20195;&#30721;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#27604;&#65292;&#20195;&#30721;&#25552;&#31034;&#26377;&#30528;&#20960;&#20010;&#29420;&#29305;&#20248;&#21183;&#65292;&#33021;&#22815;&#25552;&#39640;&#31526;&#21495;&#25512;&#29702;&#21644;&#31639;&#26415;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#24120;&#20248;&#20110;&#24605;&#36335;&#38142;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.18507</link><description>&lt;p&gt;
&#20195;&#30721;&#25552;&#31034;&#65306;&#29992;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25512;&#29702;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models. (arXiv:2305.18507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#25552;&#31034;&#26041;&#27861;&#8212;&#8212;&#20195;&#30721;&#25552;&#31034;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35302;&#21457;&#20195;&#30721;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#27604;&#65292;&#20195;&#30721;&#25552;&#31034;&#26377;&#30528;&#20960;&#20010;&#29420;&#29305;&#20248;&#21183;&#65292;&#33021;&#22815;&#25552;&#39640;&#31526;&#21495;&#25512;&#29702;&#21644;&#31639;&#26415;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#24120;&#20248;&#20110;&#24605;&#36335;&#38142;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#36890;&#36807;&#21508;&#31181;&#25552;&#31034;&#26041;&#27861;&#25193;&#22823;&#20102;&#35268;&#27169;&#65292;&#20197;&#35299;&#38145;&#24191;&#27867;&#30340;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25552;&#31034;&#26041;&#27861;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#20013;&#38388;&#27493;&#39588;&#20197;&#24110;&#21161;&#25512;&#29702;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#23436;&#21892;&#30340;&#20219;&#21153;&#32553;&#20943;&#21644;&#28151;&#28102;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#26679;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20195;&#30721;&#25552;&#31034;&#65292;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#25552;&#31034;&#26041;&#27861;&#65292;&#20855;&#26377;&#38646;-shot&#21644;&#23569;-shot&#29256;&#26412;&#65292;&#21487;&#20197;&#35302;&#21457;&#20195;&#30721;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#31526;&#21495;&#25512;&#29702;&#21644;&#31639;&#26415;&#25512;&#29702;&#30340;7&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#20195;&#30721;&#25552;&#31034;&#36890;&#24120;&#20248;&#20110;&#24605;&#36335;&#38142;&#25552;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20102;&#35299;&#20195;&#30721;&#25552;&#31034;&#30340;&#24615;&#33021;&#21644;&#38480;&#21046;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#21644;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#30830;&#23450;&#20102;&#20351;&#29992;&#31526;&#21495;&#25552;&#31034;&#30456;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20960;&#20010;&#29420;&#29305;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#20195;&#30721;&#25552;&#31034;&#21644;&#24605;&#36335;&#38142;&#25552;&#31034;&#30340;&#38598;&#21512;&#65292;&#20197;&#32467;&#21512;&#20004;&#32773;&#30340;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have scaled up to unlock a wide range of complex reasoning tasks with the aid of various prompting methods. However, current prompting methods generate natural language intermediate steps to help reasoning, which can cause imperfect task reduction and confusion. To mitigate such limitations, we explore code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps. We conduct experiments on 7 widely-used benchmarks involving symbolic reasoning and arithmetic reasoning. Code prompting generally outperforms chain-of-thought (CoT) prompting. To further understand the performance and limitations of code prompting, we perform extensive ablation studies and error analyses, and identify several exclusive advantages of using symbolic promptings compared to natural language. We also consider the ensemble of code prompting and CoT prompting to combine the strengths of both. Finally, we show throu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#27169;&#24577;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;VAST&#21450;&#20854;&#25968;&#25454;&#38598;VAST-27M&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24863;&#30693;&#21644;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#27169;&#24577;&#65292;&#36890;&#36807;&#33258;&#21160;&#38598;&#25104;&#22810;&#27169;&#24577;&#23383;&#24149;&#21644;&#36164;&#28304;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#25991;&#26412;&#20851;&#32852;&#30340;&#22810;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.18500</link><description>&lt;p&gt;
VAST&#65306;&#19968;&#31181;&#35270;&#21548;&#23383;&#24149;&#25991;&#26412;&#20840;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#19982;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset. (arXiv:2305.18500v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#27169;&#24577;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;VAST&#21450;&#20854;&#25968;&#25454;&#38598;VAST-27M&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24863;&#30693;&#21644;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#27169;&#24577;&#65292;&#36890;&#36807;&#33258;&#21160;&#38598;&#25104;&#22810;&#27169;&#24577;&#23383;&#24149;&#21644;&#36164;&#28304;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#25991;&#26412;&#20851;&#32852;&#30340;&#22810;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#23436;&#20840;&#25506;&#32034;&#20102;&#35270;&#35273;&#21644;&#25991;&#26412;&#65292;&#32780;&#20854;&#20182;&#27169;&#24577;&#65292;&#22914;&#35270;&#39057;&#20013;&#30340;&#38899;&#39057;&#21644;&#23383;&#24149;&#65292;&#21364;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#20840;&#27169;&#24577;&#35270;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;VAST-27M&#65292;&#24314;&#31435;&#22810;&#27169;&#24577;&#35270;&#39057;&#36712;&#36857;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#65292;&#24182;&#19982;&#25991;&#26412;&#36827;&#34892;&#20851;&#32852;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;2700&#19975;&#20010;&#24320;&#25918;&#39046;&#22495;&#35270;&#39057;&#29255;&#27573;&#65292;&#24182;&#20998;&#21035;&#35757;&#32451;&#35270;&#35273;&#21644;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#22120;&#20197;&#29983;&#25104;&#35270;&#35273;&#21644;&#38899;&#39057;&#23383;&#24149;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29616;&#26377;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#29983;&#25104;&#30340;&#23383;&#24149;&#12289;&#23383;&#24149;&#21644;&#25351;&#23548;&#25552;&#31034;&#38598;&#25104;&#21040;&#20840;&#27169;&#24577;&#23383;&#24149;&#20013;&#12290;&#22522;&#20110;&#25552;&#20986;&#30340;VAST-27M&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#20840;&#27169;&#24577;&#35270;&#39057;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;VAST&#65292;&#23427;&#21487;&#20197;&#24863;&#30693;&#21644;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#23383;&#24149;&#27169;&#24577;&#65292;&#24182;&#26356;&#22909;&#22320;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision and text have been fully explored in contemporary video-text foundational models, while other modalities such as audio and subtitles in videos have not received sufficient attention. In this paper, we resort to establish connections between multi-modality video tracks, including Vision, Audio, and Subtitle, and Text by exploring an automatically generated large-scale omni-modality video caption dataset called VAST-27M. Specifically, we first collect 27 million open-domain video clips and separately train a vision and an audio captioner to generate vision and audio captions. Then, we employ an off-the-shelf Large Language Model (LLM) to integrate the generated captions, together with subtitles and instructional prompts into omni-modality captions. Based on the proposed VAST-27M dataset, we train an omni-modality video-text foundational model named VAST, which can perceive and process vision, audio, and subtitle modalities from video, and better support various tasks including vis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#21644;&#39044;&#27979;&#26694;&#26550;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#32477;&#23545;&#25512;&#29702;&#33021;&#21147;&#26469;&#25552;&#39640;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16646</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23569;&#26679;&#26412;&#30340;&#32477;&#23545;&#25512;&#29702;&#26469;&#25552;&#39640;&#20107;&#20214;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning. (arXiv:2305.16646v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#21644;&#39044;&#27979;&#26694;&#26550;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#32477;&#23545;&#25512;&#29702;&#33021;&#21147;&#26469;&#25552;&#39640;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#25512;&#29702;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20107;&#20214;&#65292;&#24110;&#21161;&#25552;&#39640;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24314;&#27169;&#21644;&#39044;&#27979;&#26694;&#26550;&#65292;&#20854;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#32477;&#23545;&#25512;&#29702;&#20197;&#36741;&#21161;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#65306;&#20107;&#20214;&#27169;&#22411;&#22312;&#32473;&#23450;&#36807;&#21435;&#30340;&#24773;&#20917;&#19979;&#25552;&#20986;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;; &#22312;&#20960;&#20010;&#19987;&#23478;&#27880;&#37322;&#31034;&#33539;&#30340;&#25351;&#23548;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#20102;&#20026;&#27599;&#20010;&#25552;&#35758;&#25552;&#20379;&#21487;&#33021;&#30340;&#21407;&#22240;; &#19968;&#20010;&#25628;&#32034;&#27169;&#22359;&#25214;&#21040;&#19982;&#21407;&#22240;&#21305;&#37197;&#30340;&#20808;&#21069;&#20107;&#20214;; &#19968;&#20010;&#35780;&#20998;&#20989;&#25968;&#23398;&#20250;&#26816;&#26597;&#26816;&#32034;&#21040;&#30340;&#20107;&#20214;&#26159;&#21542;&#23454;&#38469;&#19978;&#21487;&#20197;&#23548;&#33268;&#25552;&#35758;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65288;&#20122;&#39532;&#36874;&#35780;&#35770;&#21644;GDELT&#65289;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550; - &#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147; - &#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have shown astonishing performance on a wide range of reasoning tasks. In this paper, we investigate whether they could reason about real-world events and help improve the prediction accuracy of event sequence models. We design a modeling and prediction framework where a large language model performs abductive reasoning to assist an event sequence model: the event model proposes predictions on future events given the past; instructed by a few expert-annotated demonstrations, the language model learns to suggest possible causes for each proposal; a search module finds out the previous events that match the causes; a scoring function learns to examine whether the retrieved events could actually cause the proposal. Through extensive experiments on two challenging real-world datasets (Amazon Review and GDELT), we demonstrate that our framework -- thanks to the reasoning ability of language models -- could significantly outperform the state-of-the-art event sequence mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SummIt&#65292;&#19968;&#20010;&#22522;&#20110;ChatGPT&#30340;&#36845;&#20195;&#25991;&#26412;&#25688;&#35201;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#21644;&#21453;&#39304;&#65292;&#27169;&#22411;&#33021;&#22815;&#36845;&#20195;&#22320;&#23436;&#21892;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#35299;&#20915;&#20102;&#19968;&#27425;&#24615;&#29983;&#25104;&#25688;&#35201;&#21487;&#33021;&#23384;&#22312;&#30340;&#34394;&#26500;&#21644;&#36951;&#28431;&#20851;&#38190;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14835</link><description>&lt;p&gt;
SummIt&#65306;&#36890;&#36807;ChatGPT&#36827;&#34892;&#36845;&#20195;&#25991;&#26412;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
SummIt: Iterative Text Summarization via ChatGPT. (arXiv:2305.14835v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SummIt&#65292;&#19968;&#20010;&#22522;&#20110;ChatGPT&#30340;&#36845;&#20195;&#25991;&#26412;&#25688;&#35201;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#21644;&#21453;&#39304;&#65292;&#27169;&#22411;&#33021;&#22815;&#36845;&#20195;&#22320;&#23436;&#21892;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#35299;&#20915;&#20102;&#19968;&#27425;&#24615;&#29983;&#25104;&#25688;&#35201;&#21487;&#33021;&#23384;&#22312;&#30340;&#34394;&#26500;&#21644;&#36951;&#28431;&#20851;&#38190;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#25991;&#26412;&#25688;&#35201;&#31995;&#32479;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36890;&#24120;&#26159;&#22312;&#19968;&#20010;&#27493;&#39588;&#20013;&#29983;&#25104;&#25688;&#35201;&#12290;&#28982;&#32780;&#65292;&#19968;&#27425;&#24615;&#29983;&#25104;&#25688;&#35201;&#30340;&#35774;&#32622;&#26377;&#26102;&#26159;&#19981;&#22815;&#30340;&#65292;&#22240;&#20026;&#29983;&#25104;&#30340;&#25688;&#35201;&#21487;&#33021;&#21253;&#21547;&#34394;&#26500;&#30340;&#20869;&#23481;&#65292;&#25110;&#32773;&#24573;&#30053;&#20102;&#19982;&#35835;&#32773;&#20852;&#36259;&#30456;&#20851;&#30340;&#37325;&#35201;&#32454;&#33410;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;SummIt&#65292;&#22522;&#20110;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36845;&#20195;&#25991;&#26412;&#25688;&#35201;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#21644;&#21453;&#39304;&#20351;&#27169;&#22411;&#33021;&#22815;&#36845;&#20195;&#22320;&#23436;&#21892;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#22312;&#36215;&#33609;&#21644;&#20462;&#35746;&#25688;&#35201;&#26102;&#30340;&#36845;&#20195;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#23558;&#30693;&#35782;&#21644;&#20027;&#39064;&#25552;&#21462;&#22120;&#25972;&#21512;&#21040;&#26694;&#26550;&#20013;&#20197;&#22686;&#24378;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#33258;&#21160;&#35780;&#20272;&#20102;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#20154;&#31867;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;&#36845;&#20195;&#25913;&#36827;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text summarization systems have made significant progress in recent years, but typically generate summaries in one single step. However, the one-shot summarization setting is sometimes inadequate, as the generated summary may contain hallucinations or overlook essential details related to the reader's interests. This paper addresses this limitation by proposing SummIt, an iterative text summarization framework based on large language models like ChatGPT. Our framework enables the model to refine the generated summary iteratively through self-evaluation and feedback, resembling humans' iterative process when drafting and revising summaries. Furthermore, we explore the potential benefits of integrating knowledge and topic extractors into the framework to enhance summary faithfulness and controllability. We automatically evaluate the performance of our framework on three benchmark summarization datasets. We also conduct a human evaluation to validate the effectiveness of the iterative ref
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UniChart&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;UniChart&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;&#21508;&#31181;&#19981;&#21516;&#20027;&#39064;&#21644;&#35270;&#35273;&#39118;&#26684;&#30340;&#22823;&#37327;&#22270;&#34920;&#35821;&#26009;&#24211;&#65292;&#28982;&#21518;&#20351;&#29992;&#22270;&#34920;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#32534;&#30721;&#22270;&#34920;&#65292;&#24182;&#22312;&#20960;&#20010;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.14761</link><description>&lt;p&gt;
UniChart&#65306;&#38754;&#21521;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning. (arXiv:2305.14761v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UniChart&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;UniChart&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;&#21508;&#31181;&#19981;&#21516;&#20027;&#39064;&#21644;&#35270;&#35273;&#39118;&#26684;&#30340;&#22823;&#37327;&#22270;&#34920;&#35821;&#26009;&#24211;&#65292;&#28982;&#21518;&#20351;&#29992;&#22270;&#34920;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#32534;&#30721;&#22270;&#34920;&#65292;&#24182;&#22312;&#20960;&#20010;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#22312;&#25968;&#25454;&#20998;&#26512;&#12289;&#21487;&#35270;&#21270;&#37325;&#35201;&#35265;&#35299;&#21644;&#22238;&#31572;&#25968;&#25454;&#30340;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#27969;&#34892;&#12290;&#20026;&#20102;&#26041;&#20415;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#22522;&#20110;&#22270;&#34920;&#30340;&#25968;&#25454;&#20998;&#26512;&#65292;&#26368;&#36817;&#24341;&#20837;&#20102;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#20363;&#22914;&#22270;&#34920;&#38382;&#31572;&#21644;&#22270;&#34920;&#24635;&#32467;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#30340;&#26041;&#27861;&#37117;&#20351;&#29992;&#35821;&#35328;&#25110;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#65292;&#32780;&#19981;&#35797;&#22270;&#26126;&#30830;&#24314;&#27169;&#22270;&#34920;&#30340;&#32467;&#26500;&#65288;&#20363;&#22914;&#65292;&#22914;&#20309;&#35270;&#35273;&#32534;&#30721;&#25968;&#25454;&#20197;&#21450;&#22914;&#20309;&#23558;&#22270;&#34920;&#20803;&#32032;&#30456;&#20114;&#20851;&#32852;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;&#21508;&#31181;&#19981;&#21516;&#20027;&#39064;&#21644;&#35270;&#35273;&#39118;&#26684;&#30340;&#22823;&#37327;&#22270;&#34920;&#35821;&#26009;&#24211;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UniChart&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;UniChart&#23545;&#22270;&#34920;&#30340;&#30456;&#20851;&#25991;&#26412;&#12289;&#25968;&#25454;&#21644;&#35270;&#35273;&#20803;&#32032;&#36827;&#34892;&#32534;&#30721;&#65292;&#28982;&#21518;&#20351;&#29992;&#22522;&#20110;&#22270;&#34920;&#30340;&#25991;&#26412;&#35299;&#30721;&#22120;&#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39044;&#26399;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#38754;&#21521;&#22270;&#34920;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#20302;&#23618;&#27425;&#35270;&#35273;&#32534;&#30721;&#39044;&#27979;&#65292;&#65288;ii&#65289;&#22270;&#34920;&#20803;&#32032;&#20851;&#31995;&#39044;&#27979;&#21644;&#65288;iii&#65289;&#22270;&#34920;&#38382;&#39064;&#22238;&#31572;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;UniChart&#22312;&#20960;&#20010;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Charts are very popular for analyzing data, visualizing key insights and answering complex reasoning questions about data. To facilitate chart-based data analysis using natural language, several downstream tasks have been introduced recently such as chart question answering and chart summarization. However, most of the methods that solve these tasks use pretraining on language or vision-language tasks that do not attempt to explicitly model the structure of the charts (e.g., how data is visually encoded and how chart elements are related to each other). To address this, we first build a large corpus of charts covering a wide variety of topics and visual styles. We then present UniChart, a pretrained model for chart comprehension and reasoning. UniChart encodes the relevant text, data, and visual elements of charts and then uses a chart-grounded text decoder to generate the expected output in natural language. We propose several chart-specific pretraining tasks that include: (i) low-lev
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#25552;&#31034;&#20301;&#32622;&#23545;&#20110;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#20855;&#26377;&#23454;&#36136;&#24615;&#24433;&#21709;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#20301;&#32622;&#36890;&#24120;&#26159;&#27425;&#20248;&#30340;&#65292;&#25552;&#31034;&#20301;&#32622;&#20248;&#21270;&#24212;&#25104;&#20026;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.14493</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;NLU&#20219;&#21153;&#20013;&#25552;&#31034;&#20301;&#32622;&#30830;&#23454;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Prompt position really matters in few-shot and zero-shot NLU tasks. (arXiv:2305.14493v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14493
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#25552;&#31034;&#20301;&#32622;&#23545;&#20110;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#20855;&#26377;&#23454;&#36136;&#24615;&#24433;&#21709;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#20301;&#32622;&#36890;&#24120;&#26159;&#27425;&#20248;&#30340;&#65292;&#25552;&#31034;&#20301;&#32622;&#20248;&#21270;&#24212;&#25104;&#20026;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21560;&#24341;&#20102;&#20247;&#22810;&#30740;&#31350;&#32773;&#30340;&#20851;&#27880;&#12290;&#20294;&#26159;&#65292;&#26377;&#25928;&#25552;&#31034;&#27169;&#26495;&#30340;&#24320;&#21457;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#31034;&#35789;&#27719;&#36873;&#25321;&#25110;&#20445;&#30041;&#25552;&#31034;&#20301;&#32622;&#30340;&#23884;&#20837;&#21021;&#22987;&#21270;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#25552;&#31034;&#20301;&#32622;&#36873;&#39033;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#37327;&#21270;&#20102;&#25552;&#31034;&#20301;&#32622;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#23454;&#36136;&#24615;&#24433;&#21709;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#20301;&#32622;&#23545;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#36890;&#24120;&#26159;&#27425;&#20248;&#30340;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#25552;&#31034;&#20301;&#32622;&#20248;&#21270;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#19982;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#37325;&#24515;&#24182;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based models have made remarkable advancements in the fields of zero-shot and few-shot learning, attracting a lot of attention from researchers. Developing an effective prompt template plays a critical role. However, prior studies have mainly focused on prompt vocabulary selection or embedding initialization with the reserved prompt position fixed. In this empirical study, we conduct the most comprehensive analysis to date of prompt position option for natural language understanding tasks. Our findings quantify the substantial impact prompt position has on model performance. We observe that the prompt position used in prior studies is often sub-optimal for both zero-shot and few-shot settings. These findings suggest prompt position optimisation as an interesting research direction alongside the existing focus on prompt engineering.
&lt;/p&gt;</description></item><item><title>CREATOR&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#21033;&#29992;&#25991;&#26723;&#21644;&#20195;&#30721;&#23454;&#29616;&#21019;&#24314;&#33258;&#24049;&#30340;&#24037;&#20855;&#12290;&#36890;&#36807;&#23558;&#25277;&#35937;&#24037;&#20855;&#21019;&#24314;&#21644;&#20855;&#20307;&#20915;&#31574;&#25191;&#34892;&#35299;&#32806;&#65292;CREATOR&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Creation Challenge&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;LLMs&#30340;&#24037;&#20855;&#21019;&#24314;&#33021;&#21147;&#30340;&#24517;&#35201;&#24615;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.14318</link><description>&lt;p&gt;
CREATOR: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25277;&#35937;&#21644;&#20855;&#20307;&#25512;&#29702;&#35299;&#32806;&#24037;&#20855;&#30340;&#21019;&#24314;
&lt;/p&gt;
&lt;p&gt;
CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models. (arXiv:2305.14318v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14318
&lt;/p&gt;
&lt;p&gt;
CREATOR&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#21033;&#29992;&#25991;&#26723;&#21644;&#20195;&#30721;&#23454;&#29616;&#21019;&#24314;&#33258;&#24049;&#30340;&#24037;&#20855;&#12290;&#36890;&#36807;&#23558;&#25277;&#35937;&#24037;&#20855;&#21019;&#24314;&#21644;&#20855;&#20307;&#20915;&#31574;&#25191;&#34892;&#35299;&#32806;&#65292;CREATOR&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Creation Challenge&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;LLMs&#30340;&#24037;&#20855;&#21019;&#24314;&#33021;&#21147;&#30340;&#24517;&#35201;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21033;&#29992;&#24037;&#20855;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;API&#21487;&#29992;&#24615;&#21644;&#38544;&#24335;&#25512;&#29702;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#23427;&#20204;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#35268;&#21010;&#21644;&#25191;&#34892;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;CREATOR&#65292;&#21487;&#20197;&#20351;LLMs&#21033;&#29992;&#25991;&#26723;&#21644;&#20195;&#30721;&#23454;&#29616;&#26469;&#21019;&#24314;&#33258;&#24049;&#30340;&#24037;&#20855;&#12290;CREATOR&#23558;&#25277;&#35937;&#24037;&#20855;&#21019;&#24314;&#21644;&#20855;&#20307;&#20915;&#31574;&#25191;&#34892;&#35299;&#32806;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;MATH&#21644;TabMWP&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;CREATOR&#65292;&#20998;&#21035;&#21253;&#21547;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#31454;&#36187;&#38382;&#39064;&#21644;&#22810;&#26679;&#30340;&#34920;&#26684;&#20869;&#23481;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CREATOR&#20248;&#20110;&#29616;&#26377;&#30340;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#31243;&#24207;&#21644;&#20351;&#29992;&#24037;&#20855;&#30340;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Creation Challenge&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;2K&#20010;&#22810;&#26679;&#30340;&#38382;&#39064;&#65292;&#20197;&#24378;&#35843;LLMs&#21019;&#24314;&#24037;&#20855;&#30340;&#24517;&#35201;&#24615;&#21644;&#30410;&#22788;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;LLMs&#29992;&#20316;&#24037;&#20855;&#21019;&#24314;&#32773;&#26377;&#21161;&#20110;&#25552;&#21319;&#30693;&#35782;&#30340;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made significant progress in utilizing tools, but their ability is limited by API availability and the instability of implicit reasoning, particularly when both planning and execution are involved. To overcome these limitations, we propose CREATOR, a novel framework that enables LLMs to create their own tools using documentation and code realization. CREATOR disentangles abstract tool creation and concrete decision execution, resulting in improved performance. We evaluate CREATOR on MATH and TabMWP benchmarks, respectively consisting of challenging math competition problems and diverse tabular contents. Remarkably, CREATOR outperforms existing chain-of-thought, program-of-thought, and tool-using baselines. Additionally, we introduce the Creation Challenge dataset, featuring 2K diverse questions, to emphasize the necessity and benefits of LLMs' tool creation ability. Further research demonstrates that leveraging LLMs as tool creators facilitates knowled
&lt;/p&gt;</description></item><item><title>INSTRUCTSCORE&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#24230;&#37327;&#65292;&#36890;&#36807;&#21033;&#29992;&#26126;&#30830;&#30340;&#20154;&#31867;&#25351;&#20196;&#21644;GPT-4&#30340;&#38544;&#24335;&#30693;&#35782;&#65292;&#23427;&#33021;&#29983;&#25104;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#25968;&#21644;&#20154;&#31867;&#21487;&#35835;&#30340;&#35786;&#26029;&#25253;&#21578;&#65292;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#24230;&#37327;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.14282</link><description>&lt;p&gt;
INSTRUCTSCORE: &#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#19982;&#32454;&#31890;&#24230;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback. (arXiv:2305.14282v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14282
&lt;/p&gt;
&lt;p&gt;
INSTRUCTSCORE&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#24230;&#37327;&#65292;&#36890;&#36807;&#21033;&#29992;&#26126;&#30830;&#30340;&#20154;&#31867;&#25351;&#20196;&#21644;GPT-4&#30340;&#38544;&#24335;&#30693;&#35782;&#65292;&#23427;&#33021;&#29983;&#25104;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#25968;&#21644;&#20154;&#31867;&#21487;&#35835;&#30340;&#35786;&#26029;&#25253;&#21578;&#65292;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#24230;&#37327;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#35821;&#35328;&#29983;&#25104;&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#36817;&#23398;&#20064;&#24230;&#37327;&#34920;&#26174;&#31034;&#19982;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#65292;&#20294;&#36825;&#20123;&#24230;&#37327;&#26080;&#27861;&#35299;&#37322;&#20854;&#21028;&#26029;&#25110;&#23558;&#20998;&#25968;&#19982;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#32570;&#38519;&#20851;&#32852;&#36215;&#26469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructScore&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#24230;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#26126;&#30830;&#30340;&#20154;&#31867;&#25351;&#20196;&#21644;GPT-4&#30340;&#38544;&#24335;&#30693;&#35782;&#65292;&#25105;&#20204;&#22522;&#20110;LLaMA&#23545;&#25991;&#26412;&#35780;&#20272;&#24230;&#37327;&#36827;&#34892;&#24494;&#35843;&#65292;&#29983;&#25104;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#25968;&#21644;&#20154;&#31867;&#21487;&#35835;&#30340;&#35786;&#26029;&#25253;&#21578;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;InstructScore&#65292;&#21253;&#25324;&#32763;&#35793;&#12289;&#23383;&#24149;&#29983;&#25104;&#12289;&#25968;&#25454;&#21040;&#25991;&#26412;&#21644;&#24120;&#35782;&#29983;&#25104;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;7B&#27169;&#22411;&#36229;&#36807;&#20102;&#25152;&#26377;&#20854;&#20182;&#26080;&#30417;&#30563;&#24230;&#37327;&#65292;&#21253;&#25324;&#22522;&#20110;175B GPT-3&#21644;GPT-4&#30340;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#27809;&#26377;&#26469;&#33258;&#20154;&#24037;&#35780;&#32423;&#25968;&#25454;&#30340;&#30452;&#25509;&#30417;&#30563;&#65292;&#25105;&#20204;&#30340;InstructScore&#30340;&#24615;&#33021;&#27700;&#24179;&#20063;&#19982;COMET2&#31561;&#26368;&#20808;&#36827;&#30340;&#24230;&#37327;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically evaluating the quality of language generation is critical. Although recent learned metrics show high correlation with human judgement, these metrics can not explain their verdict or associate the scores with defects in generated text. To address this limitation, we present InstructScore, an explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT-4, we fine-tune a text evaluation metric based on LLaMA, producing both a score for generated text and a human readable diagnostic report. We evaluate InstructScore on a variety of generation tasks, including translation, captioning, data-to-text and commonsense generation. Experiments show that our 7B model surpasses all other unsupervised metrics, including those based on 175B GPT-3 and GPT-4. Surprisingly, our InstructScore, even without direct supervision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.14259</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#25991;&#29486;&#30340;&#35821;&#22659;&#21270;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#65288;LBD&#65289;&#26088;&#22312;&#36890;&#36807;&#25366;&#25496;&#35770;&#25991;&#24182;&#29983;&#25104;&#20551;&#35774;&#26469;&#21457;&#29616;&#26032;&#30340;&#31185;&#23398;&#30693;&#35782;&#12290;&#26631;&#20934;&#30340;LBD&#20165;&#38480;&#20110;&#39044;&#27979;&#31163;&#25955;&#27010;&#24565;&#20043;&#38388;&#30340;&#20004;&#20004;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#21644;&#30142;&#30149;&#30340;&#20851;&#32852;&#65289;&#12290;LBD&#36824;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23454;&#39564;&#35774;&#32622;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#35780;&#20272;&#30340;&#29305;&#23450;&#24739;&#32773;&#32676;&#20307;&#65289;&#21644;&#20154;&#31867;&#31185;&#23398;&#23478;&#32771;&#34385;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#21160;&#26426;&#65288;&#20363;&#22914;&#65292;&#25214;&#21040;&#27809;&#26377;&#29305;&#23450;&#21103;&#20316;&#29992;&#30340;&#33647;&#29289;&#20505;&#36873;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#21270;LBD&#65288;C-LBD&#65289;&#34920;&#36848;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65306;&#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31185;&#23398;&#20551;&#35774;&#65292;&#21516;&#26102;&#23558;&#23427;&#20204;&#32852;&#31995;&#21040;&#25511;&#21046;&#20551;&#35774;&#25628;&#32034;&#31354;&#38388;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#26694;&#26550;&#65292;&#20351;&#29992;&#33719;&#24471;&#30340;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#24322;&#26500;&#32593;&#32476;&#20013;&#30340;&#8220;&#28789;&#24863;&#8221;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#20174;&#35770;&#25991;&#20013;&#27966;&#29983;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#20542;&#21521;&#20110;&#29983;&#25104;&#20855;&#26377;&#21019;&#26032;&#24615;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#26469;&#21387;&#32553;&#22810;&#35821;&#35328;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20302;&#31209;&#30697;&#38453;&#26469;&#26500;&#24314;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#65292;&#24182;&#20351;&#29992;Fuse Distillation&#25216;&#26415;&#23558;&#22810;&#20010;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#20013;&#30340;&#30693;&#35782;&#21387;&#32553;&#21040;&#19968;&#20010;&#20849;&#20139;&#27169;&#22359;&#20013;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#21644;&#27169;&#22411;&#24207;&#21015;&#21270;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.13993</link><description>&lt;p&gt;
&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#21387;&#32553;&#22810;&#35821;&#35328;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Condensing Multilingual Knowledge with Lightweight Language-Specific Modules. (arXiv:2305.13993v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#26469;&#21387;&#32553;&#22810;&#35821;&#35328;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20302;&#31209;&#30697;&#38453;&#26469;&#26500;&#24314;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#65292;&#24182;&#20351;&#29992;Fuse Distillation&#25216;&#26415;&#23558;&#22810;&#20010;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#20013;&#30340;&#30693;&#35782;&#21387;&#32553;&#21040;&#19968;&#20010;&#20849;&#20139;&#27169;&#22359;&#20013;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#21644;&#27169;&#22411;&#24207;&#21015;&#21270;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#29305;&#23450;&#65288;LS&#65289;&#27169;&#22359;&#32435;&#20837;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#26159;&#19968;&#31181;&#25552;&#21319;&#24615;&#33021;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#26041;&#27861;&#30456;&#20284;&#65292;&#22240;&#20026;&#23427;&#19981;&#20250;&#22686;&#21152;FLOPs&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20840;&#36830;&#25509;&#23618;&#20013;&#24341;&#20837;&#30340;&#20840;&#31209;&#30697;&#38453;&#25152;&#24102;&#26469;&#30340;&#21442;&#25968;&#25968;&#37327;&#22826;&#22810;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#25193;&#23637;&#21040;&#25968;&#30334;&#31181;&#35821;&#35328;&#65288;&#19987;&#23478;&#65289;&#26102;&#24448;&#24448;&#38590;&#20197;&#31649;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35821;&#35328;&#29305;&#23450;&#30697;&#38453;&#21512;&#25104;&#65288;LMS&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#20004;&#20010;&#26174;&#33879;&#36739;&#23567;&#30340;&#30697;&#38453;&#29983;&#25104;&#20302;&#31209;&#30697;&#38453;&#26469;&#26500;&#24314;LS&#27169;&#22359;&#20197;&#36817;&#20284;&#20840;&#31209;&#30697;&#38453;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;Fuse Distillation&#65288;FD&#65289;&#25216;&#26415;&#23558;&#22810;&#20010;LS&#27169;&#22359;&#20013;&#30340;&#22810;&#35821;&#35328;&#30693;&#35782;&#21387;&#32553;&#21040;&#19968;&#20010;&#20849;&#20139;&#27169;&#22359;&#20013;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#21644;&#27169;&#22411;&#24207;&#21015;&#21270;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;LMS&#26041;&#27861;&#22312;&#19982;&#21516;&#26679;&#25968;&#37327;&#30340;&#39069;&#22806;&#21442;&#25968;&#24773;&#20917;&#19979;&#26174;&#33879;&#20248;&#20110;&#20043;&#21069;&#30340;LS&#26041;&#27861;&#21644;MoE&#26041;&#27861;&#65292;&#20363;&#22914;1.73 BLE
&lt;/p&gt;
&lt;p&gt;
Incorporating language-specific (LS) modules is a proven method to boost performance in multilingual machine translation. This approach bears similarity to Mixture-of-Experts (MoE) because it does not inflate FLOPs. However, the scalability of this approach to hundreds of languages (experts) tends to be unmanageable due to the prohibitive number of parameters introduced by full-rank matrices in fully-connected layers. In this work, we introduce the Language-Specific Matrix Synthesis (LMS) method. This approach constructs LS modules by generating low-rank matrices from two significantly smaller matrices to approximate the full-rank matrix. Furthermore, we condense multilingual knowledge from multiple LS modules into a single shared module with the Fuse Distillation (FD) technique to improve the efficiency of inference and model serialization. We show that our LMS method significantly outperforms previous LS methods and MoE methods with the same amount of extra parameters, e.g., 1.73 BLE
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550; SALAM&#65292;&#36890;&#36807;&#21327;&#20316;&#20132;&#20114;&#19982;&#23398;&#20064;&#21161;&#25163;&#26469;&#24110;&#21161; LLM &#22312;&#21453;&#24605;&#21644;&#25913;&#36827;&#36807;&#31243;&#20013;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#25910;&#38598;&#38169;&#35823;&#24182;&#22312;&#25512;&#29702;&#26102;&#25552;&#20379;&#25351;&#23548;&#26041;&#38024;&#65292;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13829</link><description>&lt;p&gt;
&#36890;&#36807;&#21327;&#20316;&#20132;&#20114;&#19982;&#23398;&#20064;&#21161;&#25163;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learn from Mistakes through Cooperative Interaction with Study Assistant. (arXiv:2305.13829v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550; SALAM&#65292;&#36890;&#36807;&#21327;&#20316;&#20132;&#20114;&#19982;&#23398;&#20064;&#21161;&#25163;&#26469;&#24110;&#21161; LLM &#22312;&#21453;&#24605;&#21644;&#25913;&#36827;&#36807;&#31243;&#20013;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#25910;&#38598;&#38169;&#35823;&#24182;&#22312;&#25512;&#29702;&#26102;&#25552;&#20379;&#25351;&#23548;&#26041;&#38024;&#65292;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#33258;&#25105;&#21453;&#24605;&#21644;&#25913;&#36827;&#29983;&#25104;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#36825;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21453;&#39304;&#26426;&#21046;&#38754;&#20020;&#25361;&#25112;&#65292;&#20363;&#22914;&#19981;&#33021;&#20445;&#35777;&#27491;&#30830;&#24615;&#21644;&#23545;&#27169;&#22411;&#24369;&#28857;&#32570;&#20047;&#20840;&#23616;&#27934;&#23519;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550; Study Assistant for Large Language Model (SALAM)&#65292;&#20197;&#24110;&#21161; LLM &#22312;&#21453;&#24605;&#21644;&#25913;&#36827;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#26681;&#25454;&#20154;&#31867;&#21161;&#29702;&#30740;&#31350;&#30340;&#28789;&#24863;&#65292;&#36890;&#36807;&#23558;&#20808;&#21069;&#30340;&#21709;&#24212;&#19982;&#30495;&#23454;&#20540;&#36827;&#34892;&#23450;&#37327;&#20998;&#32423;&#65292;&#24182;&#22312;&#35757;&#32451;&#38454;&#27573;&#25910;&#38598;&#38169;&#35823;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;&#23427;&#26681;&#25454;&#38169;&#35823;&#25910;&#38598;&#30830;&#23450;&#24120;&#35265;&#35823;&#35299;&#65292;&#24182;&#25552;&#20379;&#25351;&#23548;&#26041;&#38024;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#22312;&#25512;&#29702;&#26399;&#38388;&#36991;&#20813;&#31867;&#20284;&#30340;&#38169;&#35823;&#12290;SALAM &#26159;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;&#65292;&#19987;&#27880;&#20110;&#25552;&#20379;&#19968;&#33324;&#24615;&#30340;&#21453;&#39304;&#65292;&#24182;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545; SALAM &#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23427;&#22312;&#21508;&#31181;&#22522;&#32447;&#19978;&#37117;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated their ability to self-reflect and refine their generation, which can further improve their performance. However, this feedback mechanism faces challenges such as no guarantee of correctness and the lack of global insight into the model's weaknesses. In this paper, we propose a novel framework, Study Assistant for Large Language Model (SALAM), to aid LLMs in the reflection and refinement process. Motivated by the human study assistant, this framework grades previous responses with the ground truth and collects mistakes in the training phase. During inference, it identifies common misunderstandings based on the mistake collections and provides guidelines for the model to help the model avoid similar mistakes during inference. SALAM is a model-agnostic framework, focusing on providing general feedback and can adapt to any base model. Our evaluation of SALAM on two challenging benchmarks demonstrated a significant improvement over various baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#35843;&#26597;&#25506;&#35752;&#20102;&#22312;&#20196;&#29260;&#21361;&#26426;&#19979;&#25193;&#23637;LLM&#30340;&#37325;&#22797;&#39044;&#35757;&#32451;&#25968;&#25454;&#26041;&#27861;&#65292;&#21457;&#29616;&#27169;&#22411;&#23481;&#26131;&#36807;&#25311;&#21512;&#24182;&#23548;&#33268;&#22810;&#36718;&#27425;&#24615;&#33021;&#19979;&#38477;&#65292;&#20851;&#38190;&#22240;&#32032;&#21253;&#25324;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#27491;&#21017;&#21270;&#25216;&#26415;&#24182;&#19981;&#33021;&#26126;&#26174;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13230</link><description>&lt;p&gt;
&#26159;&#21542;&#37325;&#22797;&#30340;&#30097;&#38382;: &#22312;&#20196;&#29260;&#21361;&#26426;&#19979;&#25193;&#23637;LLM&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis. (arXiv:2305.13230v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13230
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#35843;&#26597;&#25506;&#35752;&#20102;&#22312;&#20196;&#29260;&#21361;&#26426;&#19979;&#25193;&#23637;LLM&#30340;&#37325;&#22797;&#39044;&#35757;&#32451;&#25968;&#25454;&#26041;&#27861;&#65292;&#21457;&#29616;&#27169;&#22411;&#23481;&#26131;&#36807;&#25311;&#21512;&#24182;&#23548;&#33268;&#22810;&#36718;&#27425;&#24615;&#33021;&#19979;&#38477;&#65292;&#20851;&#38190;&#22240;&#32032;&#21253;&#25324;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#27491;&#21017;&#21270;&#25216;&#26415;&#24182;&#19981;&#33021;&#26126;&#26174;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#25968;&#25454;&#38598;&#35268;&#27169;&#23545;&#20110;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#38750;&#24120;&#20381;&#36182;&#20110;&#20196;&#29260;&#65292;&#24182;&#19988;&#32593;&#32476;&#19978;&#30340;&#39640;&#36136;&#37327;&#25991;&#26412;&#25968;&#25454;&#24050;&#25509;&#36817;LLMs&#30340;&#25193;&#23637;&#38480;&#21046;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;LLMs&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26159;&#37325;&#22797;&#39044;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#36718;&#27425;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#23454;&#35777;&#35282;&#24230;&#25506;&#35752;&#20102;&#36825;&#31181;&#26041;&#27861;&#19979;&#30340;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#37325;&#22797;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#21518;&#26524;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#23481;&#26131;&#36807;&#25311;&#21512;&#65292;&#23548;&#33268;&#22810;&#36718;&#27425;&#24615;&#33021;&#19979;&#38477;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23548;&#33268;&#22810;&#36718;&#27425;&#24615;&#33021;&#19979;&#38477;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21457;&#29616;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#30446;&#26631;&#26159;&#26174;&#33879;&#22240;&#32032;&#65292;&#32780;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#27169;&#22411;FLOP&#21017;&#24433;&#21709;&#36739;&#23567;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26159;&#21542;&#21487;&#20197;&#32531;&#35299;&#22810;&#36718;&#27425;&#24615;&#33021;&#19979;&#38477;&#12290;&#22823;&#22810;&#25968;&#27491;&#21017;&#21270;&#25216;&#26415;&#24182;&#19981;&#33021;&#26126;&#26174;&#32531;&#35299;&#36825;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has highlighted the importance of dataset size in scaling language models. However, large language models (LLMs) are notoriously token-hungry during pre-training, and high-quality text data on the web is approaching its scaling limit for LLMs. To further enhance LLMs, a straightforward approach is to repeat the pre-training data for additional epochs. In this study, we empirically investigate three key aspects under this approach. First, we explore the consequences of repeating pre-training data, revealing that the model is susceptible to overfitting, leading to multi-epoch degradation. Second, we examine the key factors contributing to multi-epoch degradation, finding that significant factors include dataset size, model parameters, and training objectives, while less influential factors consist of dataset quality and model FLOPs. Finally, we explore whether widely used regularization can alleviate multi-epoch degradation. Most regularization techniques do not yield sig
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;LLMs&#24212;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#65292;&#36890;&#36807;&#22312;&#20889;&#20316;&#36741;&#21161;&#22330;&#26223;&#20013;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.13225</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#25351;&#20196;&#35843;&#25972;LLaMa&#20197;&#36866;&#24212;&#29305;&#23450;&#22330;&#26223;&#65306;&#20851;&#20110;&#20889;&#20316;&#36741;&#21161;&#30340;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance. (arXiv:2305.13225v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13225
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;LLMs&#24212;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#65292;&#36890;&#36807;&#22312;&#20889;&#20316;&#36741;&#21161;&#22330;&#26223;&#20013;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#19968;&#20123;&#29305;&#23450;&#20219;&#21153;&#32780;&#38750;&#36890;&#29992;&#25351;&#20196;&#36981;&#24490;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#23454;&#38469;&#38382;&#39064;&#35774;&#32622;&#65292;&#24182;&#25506;&#32034;&#20102;LLMs&#22312;&#36825;&#20123;&#26377;&#38024;&#23545;&#24615;&#22330;&#26223;&#20013;&#26159;&#21542;&#26377;&#21033;&#21644;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#20889;&#20316;&#36741;&#21161;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#20854;&#20013;&#21253;&#25324;&#19971;&#20010;&#20889;&#20316;&#20219;&#21153;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#36825;&#20123;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25353;&#29031;&#25351;&#20196;&#36981;&#24490;&#30340;&#26684;&#24335;&#37325;&#26032;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#23545;LLM&#65292;&#29305;&#21035;&#26159;LLaMa&#36827;&#34892;&#20248;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20889;&#20316;&#25351;&#20196;&#25968;&#25454;&#23545;LLaMa&#36827;&#34892;&#24494;&#35843;&#26174;&#33879;&#25913;&#21892;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proprietary Large Language Models (LLMs), such as ChatGPT, have garnered significant attention due to their exceptional capabilities in handling a diverse range of tasks. Recent studies demonstrate that open-sourced smaller foundational models, such as 7B-size LLaMA, can also display remarkable proficiency in tackling diverse tasks when fine-tuned using instruction-driven data. In this work, we investigate a practical problem setting where the primary focus is on one or a few particular tasks rather than general-purpose instruction following, and explore whether LLMs can be beneficial and further improved for such targeted scenarios. We choose the writing-assistant scenario as the testbed, which includes seven writing tasks. We collect training data for these tasks, reframe them in an instruction-following format, and subsequently refine the LLM, specifically LLaMA, via instruction tuning. Experimental results show that fine-tuning LLaMA on writing instruction data significantly improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21477;&#23376;&#23545;&#40784;&#35774;&#35745;&#65292;&#24182;&#21457;&#29616;&#24403;&#21069;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#21644;&#23545;&#40784;&#31561;&#38382;&#39064;&#19978;&#36824;&#23384;&#22312;&#22256;&#38590;&#65292;&#24615;&#33021;&#30456;&#23545;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#20173;&#26377;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2305.12878</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#30340;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Non-Autoregressive Document-Level Machine Translation. (arXiv:2305.12878v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21477;&#23376;&#23545;&#40784;&#35774;&#35745;&#65292;&#24182;&#21457;&#29616;&#24403;&#21069;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#21644;&#23545;&#40784;&#31561;&#38382;&#39064;&#19978;&#36824;&#23384;&#22312;&#22256;&#38590;&#65292;&#24615;&#33021;&#30456;&#23545;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#20173;&#26377;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#27169;&#22411;&#22312;&#21477;&#23376;&#32423;&#26426;&#22120;&#32763;&#35793;&#20013;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#36895;&#24230;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#33021;&#21147;&#23578;&#26410;&#34987;&#28145;&#20837;&#30740;&#31350;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#23545;&#20856;&#22411;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20013;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#30340;&#21477;&#23376;&#23545;&#40784;&#35774;&#35745;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#25991;&#26723;&#19978;&#21462;&#24471;&#20102;&#39640;&#24230;&#30340;&#21152;&#36895;&#65292;&#24182;&#19988;&#21477;&#23376;&#23545;&#40784;&#26126;&#26174;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#20173;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#36827;&#19968;&#27493;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#30340;&#32972;&#26223;&#19979;&#65292;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#21644;&#23545;&#40784;&#31561;&#38382;&#39064;&#19978;&#38754;&#20020;&#30528;&#26356;&#22810;&#30340;&#22256;&#38590;&#65292;&#30446;&#21069;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#20063;&#38590;&#20197;&#20805;&#20998;&#21033;&#29992;&#25991;&#26723;&#19978;&#19979;&#25991;&#21644;&#22788;&#29702;&#35805;&#35821;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive translation (NAT) models achieve comparable performance and superior speed compared to auto-regressive translation (AT) models in the context of sentence-level machine translation (MT). However, their abilities are unexplored in document-level MT, hindering their usage in real scenarios. In this paper, we conduct a comprehensive examination of typical NAT models in the context of document-level MT and further propose a simple but effective design of sentence alignment between source and target. Experiments show that NAT models achieve high acceleration on documents, and sentence alignment significantly enhances their performance.  However, current NAT models still have a significant performance gap compared to their AT counterparts. Further investigation reveals that NAT models suffer more from the multi-modality and misalignment issues in the context of document-level MT, and current NAT models struggle with exploiting document context and handling discourse phenome
&lt;/p&gt;</description></item><item><title>AutoTrial&#26159;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20020;&#24202;&#35797;&#39564;&#32435;&#20837;/&#25490;&#38500;&#26631;&#20934;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21487;&#25511;&#29983;&#25104;&#12289;&#21487;&#25193;&#23637;&#23398;&#20064;&#12289;&#25552;&#20379;&#25512;&#29702;&#38142;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#27969;&#30021;&#20934;&#30830;&#30340;&#26631;&#20934;&#25991;&#26412;&#65292;&#19982;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#65292;&#20294;&#36164;&#28304;&#21344;&#29992;&#26356;&#23569;&#12290;</title><link>http://arxiv.org/abs/2305.11366</link><description>&lt;p&gt;
AutoTrial&#65306;&#29992;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20020;&#24202;&#35797;&#39564;&#35774;&#35745;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
AutoTrial: Prompting Language Models for Clinical Trial Design. (arXiv:2305.11366v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11366
&lt;/p&gt;
&lt;p&gt;
AutoTrial&#26159;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20020;&#24202;&#35797;&#39564;&#32435;&#20837;/&#25490;&#38500;&#26631;&#20934;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21487;&#25511;&#29983;&#25104;&#12289;&#21487;&#25193;&#23637;&#23398;&#20064;&#12289;&#25552;&#20379;&#25512;&#29702;&#38142;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#27969;&#30021;&#20934;&#30830;&#30340;&#26631;&#20934;&#25991;&#26412;&#65292;&#19982;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#65292;&#20294;&#36164;&#28304;&#21344;&#29992;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#23545;&#20110;&#33647;&#29289;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#24739;&#32773;&#25311;&#23450;&#21512;&#36866;&#30340;&#32435;&#20837;/&#25490;&#38500;&#26631;&#20934;&#26159;&#35797;&#39564;&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;&#20020;&#24202;&#35797;&#39564;&#26041;&#26696;&#30340;&#27491;&#30830;&#35774;&#35745;&#24212;&#32771;&#34385;&#21040;&#31867;&#20284;&#30340;&#20808;&#20363;&#35797;&#39564;&#21450;&#20854;&#32435;&#20837;/&#25490;&#38500;&#26631;&#20934;&#65292;&#20197;&#30830;&#20445;&#24739;&#32773;&#30340;&#20805;&#20998;&#35206;&#30422;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoTrial&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#26469;&#24110;&#21161;&#35774;&#35745;&#20020;&#24202;&#32435;&#20837;/&#25490;&#38500;&#26631;&#20934;&#12290;&#23427;&#20801;&#35768;&#65288;1&#65289;&#36890;&#36807;&#31163;&#25955;&#21644;&#31070;&#32463;&#25552;&#31034;&#30340;&#28151;&#21512;&#36827;&#34892;&#21487;&#25511;&#30340;&#25351;&#23548;&#29983;&#25104;&#65292;&#65288;2&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#34701;&#21512;&#65292;&#20197;&#21450;&#65288;3&#65289;&#25552;&#20379;&#26126;&#30830;&#30340;&#25512;&#29702;&#38142;&#20197;&#29702;&#35299;&#36755;&#20986;&#30340;&#21512;&#29702;&#24615;&#12290;&#23545;&#36229;&#36807;70,000&#39033;&#20020;&#24202;&#35797;&#39564;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;AutoTrial&#29983;&#25104;&#30340;&#26631;&#20934;&#25991;&#26412;&#20855;&#26377;&#27969;&#30021;&#24615;&#21644;&#36830;&#36143;&#24615;&#65292;&#24182;&#19988;&#22312;&#25429;&#25417;&#30446;&#26631;&#35797;&#39564;&#30456;&#20851;&#20020;&#24202;&#27010;&#24565;&#26041;&#38754;&#20934;&#30830;&#24230;&#39640;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#36164;&#28304;&#21344;&#29992;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#21487;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical trials are critical for drug development. Constructing the appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for patient recruitment) is essential for the trial's success. Proper design of clinical trial protocols should consider similar precedent trials and their eligibility criteria to ensure sufficient patient coverage. In this paper, we present a method named AutoTrial to aid the design of clinical eligibility criteria using language models. It allows (1) controllable generation under instructions via a hybrid of discrete and neural prompting, (2) scalable knowledge incorporation via in-context learning, and (3) explicit reasoning chains to provide rationales for understanding the outputs. Experiments on over 70K clinical trials verify that AutoTrial generates high-quality criteria texts that are fluent and coherent and with high accuracy in capturing the relevant clinical concepts to the target trial. It is noteworthy that our method, with a much sm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#20139;&#20102;&#31532;&#19968;&#27425;&#38024;&#23545;&#22312;&#30452;&#25773;&#24179;&#21488;&#19978;&#26816;&#27979;&#35268;&#33539;&#36829;&#32972;&#29616;&#35937;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#65292;&#36890;&#36807;&#20998;&#31867;&#21035;&#26631;&#27880;&#21644;&#29992;&#25143;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#30452;&#25773;&#32842;&#22825;&#35268;&#33539;&#24418;&#25104;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.10731</link><description>&lt;p&gt;
&#22312;&#30452;&#25773;&#32842;&#22825;&#20013;&#20998;&#26512;&#35268;&#33539;&#36829;&#32972;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Analyzing Norm Violations in Live-Stream Chat. (arXiv:2305.10731v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#20139;&#20102;&#31532;&#19968;&#27425;&#38024;&#23545;&#22312;&#30452;&#25773;&#24179;&#21488;&#19978;&#26816;&#27979;&#35268;&#33539;&#36829;&#32972;&#29616;&#35937;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#65292;&#36890;&#36807;&#20998;&#31867;&#21035;&#26631;&#27880;&#21644;&#29992;&#25143;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#30452;&#25773;&#32842;&#22825;&#35268;&#33539;&#24418;&#25104;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27602;&#24615;&#35328;&#35821;&#65292;&#22914;&#20167;&#24680;&#35328;&#35770;&#65292;&#21487;&#33021;&#20250;&#38459;&#27490;&#29992;&#25143;&#21442;&#19982;&#22312;&#32447;&#31038;&#21306;&#21644;&#27969;&#34892;&#24179;&#21488;&#65292;&#24433;&#21709;&#20182;&#20204;&#30340;&#20307;&#39564;&#12290;&#20197;&#21069;&#30340;&#26816;&#27979;&#24037;&#20855;&#20027;&#35201;&#20851;&#27880;&#20110;&#22312;&#32447;&#35770;&#22363;&#21644;&#31038;&#20132;&#23186;&#20307;&#65288;&#22914;Reddit&#21644;Twitter&#65289;&#30340;&#23545;&#35805;&#12290;&#20294;&#26159;&#65292;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#30452;&#25773;&#24179;&#21488;&#65288;&#22914;Twitch&#21644;YouTube Live&#65289;&#20013;&#30340;&#23545;&#35805;&#26102;&#65292;&#30001;&#20110;&#27599;&#20010;&#35780;&#35770;&#20165;&#21487;&#35265;&#19968;&#27573;&#26102;&#38388;&#65292;&#24182;&#19988;&#32570;&#23569;&#19982;&#20854;&#20182;&#35780;&#35770;&#24314;&#31435;&#20851;&#31995;&#30340;&#32447;&#31243;&#32467;&#26500;&#65292;&#22240;&#27492;&#36825;&#20123;&#26041;&#27861;&#30340;&#25928;&#26524;&#36739;&#24046;&#12290;&#26412;&#25991;&#20998;&#20139;&#20102;&#31532;&#19968;&#27425;&#38024;&#23545;&#22312;&#30452;&#25773;&#24179;&#21488;&#19978;&#26816;&#27979;&#35268;&#33539;&#36829;&#32972;&#29616;&#35937;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;Twitch&#19978;&#23545;4,583&#20010;&#21463;&#36807;&#23457;&#26597;&#30340;&#35780;&#35770;&#36827;&#34892;&#20102;&#20998;&#31867;&#21035;&#26631;&#27880;&#65292;&#24182;&#23450;&#20041;&#20102;&#30452;&#25773;&#32842;&#22825;&#20013;&#30340;&#35268;&#33539;&#36829;&#21453;&#31867;&#21035;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#30452;&#25773;&#25968;&#25454;&#19982;&#20854;&#20182;&#35770;&#22363;&#30340;&#20960;&#20010;&#21306;&#21035;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#21069;&#30340;&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25991;&#26412;&#21644;&#35821;&#38899;&#20013;&#35268;&#33539;&#36829;&#32972;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#25581;&#31034;&#20102;&#30452;&#25773;&#32842;&#22825;&#35268;&#33539;&#24418;&#25104;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Toxic language, such as hate speech, can deter users from participating in online communities and enjoying popular platforms. Previous approaches to detecting toxic language and norm violations have been primarily concerned with conversations from online forums and social media, such as Reddit and Twitter. These approaches are less effective when applied to conversations on live-streaming platforms, such as Twitch and YouTube Live, as each comment is only visible for a limited time and lacks a thread structure that establishes its relationship with other comments. In this work, we share the first NLP study dedicated to detecting norm violations in conversations on live-streaming platforms. We define norm violation categories in live-stream chats and annotate 4,583 moderated comments from Twitch. We articulate several facets of live-stream data that differ from other forums, and demonstrate that existing models perform poorly in this setting. By conducting a user study, we identify the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;&#65292;&#23588;&#20854;&#26159;&#19981;&#38656;&#35201;&#20219;&#20309;&#26174;&#24335;&#27169;&#22359;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#31867;&#39044;&#27979;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#38544;&#24335;&#26377;&#25928;&#22320;&#32534;&#30721;&#19978;&#19979;&#25991;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.10613</link><description>&lt;p&gt;
&#19981;&#20381;&#38752;&#20808;&#39564;&#30693;&#35782;&#30340;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning. (arXiv:2305.10613v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;&#65292;&#23588;&#20854;&#26159;&#19981;&#38656;&#35201;&#20219;&#20309;&#26174;&#24335;&#27169;&#22359;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#31867;&#39044;&#27979;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#38544;&#24335;&#26377;&#25928;&#22320;&#32534;&#30721;&#19978;&#19979;&#25991;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#65288;TKG&#65289;&#39044;&#27979;&#26159;&#19968;&#20010;&#25361;&#25112;&#27169;&#22411;&#20351;&#29992;&#36807;&#21435;&#30340;&#30693;&#35782;&#26469;&#39044;&#27979;&#26410;&#26469;&#20107;&#23454;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20110;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;LLMs&#22312;TKG&#39044;&#27979;&#20013;&#21487;&#20197;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20351;&#29992;&#65292;&#29305;&#21035;&#26159;&#27809;&#26377;&#20219;&#20309;&#24494;&#35843;&#25110;&#25429;&#25417;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#30340;&#26174;&#24335;&#27169;&#22359;&#12290;&#20026;&#20102;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#30456;&#20851;&#21382;&#21490;&#20107;&#23454;&#36716;&#25442;&#20026;&#25552;&#31034;&#24182;&#20351;&#29992;&#20196;&#29260;&#27010;&#29575;&#29983;&#25104;&#25490;&#21517;&#39044;&#27979;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#30340;&#24615;&#33021;&#19982;&#20026;TKG&#39044;&#27979;&#31934;&#24515;&#35774;&#35745;&#21644;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;TKG&#27169;&#22411;&#30456;&#24403;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#35780;&#20272;&#23637;&#31034;&#20102;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#29305;&#24615;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#27604;&#36739;&#20102;&#20934;&#22791;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26367;&#20195;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#19982;&#33879;&#21517;&#30340;TKG&#26041;&#27861;&#21644;&#31616;&#21333;&#30340;&#39057;&#29575;&#21644;&#26368;&#36817;&#24615;&#22522;&#32447;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#36824;&#26816;&#26597;&#20102;&#29983;&#25104;&#30340;&#25552;&#31034;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#25152;&#39044;&#27979;&#30340;&#20107;&#23454;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#30830;&#23454;&#21487;&#20197;&#38544;&#24335;&#26377;&#25928;&#22320;&#32534;&#30721;&#19978;&#19979;&#25991;&#21644;&#26102;&#38388;&#20449;&#24687;&#65292;&#36827;&#34892;TKG&#39044;&#27979;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#30693;&#35782;&#25110;&#39046;&#22495;&#29305;&#23450;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal knowledge graph (TKG) forecasting benchmarks challenge models to predict future facts using knowledge of past facts. In this paper, we apply large language models (LLMs) to these benchmarks using in-context learning (ICL). We investigate whether and to what extent LLMs can be used for TKG forecasting, especially without any fine-tuning or explicit modules for capturing structural and temporal information. For our experiments, we present a framework that converts relevant historical facts into prompts and generates ranked predictions using token probabilities. Surprisingly, we observe that LLMs, out-of-the-box, perform on par with state-of-the-art TKG models carefully designed and trained for TKG forecasting. Our extensive evaluation presents performances across several models and datasets with different characteristics, compares alternative heuristics for preparing contextual information, and contrasts to prominent TKG methods and simple frequency and recency baselines. We als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#24050;&#32463;&#32534;&#30721;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#30340;&#30456;&#20851;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#28155;&#21152;&#25552;&#31034;&#65292;&#24182;&#23558;&#30456;&#20851;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.08732</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Knowledge Rumination for Pre-trained Language Models. (arXiv:2305.08732v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#24050;&#32463;&#32534;&#30721;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#30340;&#30456;&#20851;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#28155;&#21152;&#25552;&#31034;&#65292;&#24182;&#23558;&#30456;&#20851;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#26222;&#36890;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21333;&#29420;&#22788;&#29702;&#30693;&#35782;&#23494;&#38598;&#22411;NLP&#20219;&#21153;&#30340;&#33021;&#21147;&#19981;&#36275;&#65292;&#22240;&#27492;&#65292;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#23558;&#22806;&#37096;&#30693;&#35782;&#38598;&#25104;&#21040;PLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#30528;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#21040;&#65292;PLM&#21487;&#33021;&#24050;&#32463;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#32534;&#30721;&#20102;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#20294;&#22312;&#24212;&#29992;&#21040;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#26102;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#20197;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#30456;&#20851;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#23427;&#20204;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#22312;PLMs&#20013;&#28155;&#21152;&#19968;&#20010;&#22914;&#8220;&#25454;&#25105;&#25152;&#30693;&#8221;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#35797;&#22270;&#22238;&#39038;&#30456;&#20851;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#27169;&#22411;&#20197;&#36827;&#34892;&#30693;&#35782;&#25972;&#21512;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#30693;&#35782;&#21453;&#24605;&#24212;&#29992;&#20110;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;RoBERTa&#12289;DeBERTa&#21644;GPT-3&#12290;&#22312;&#20845;&#20010;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;.....
&lt;/p&gt;
&lt;p&gt;
Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fail to fully utilize them when applying them to knowledge-intensive tasks. In this paper, we propose a new paradigm dubbed Knowledge Rumination to help the pre-trained language model utilize that related latent knowledge without retrieving it from the external corpus. By simply adding a prompt like "As far as I know" to the PLMs, we try to review related latent knowledge and inject them back into the model for knowledge consolidation. We apply the proposed knowledge rumination to various language models, including RoBERTa, DeBERTa, and GPT-3. Experimental results on six commonsense reasoning tasks and GLUE benchmarks dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Clue And Reasoning Prompting (CARP)&#31639;&#27861;&#65292;&#37319;&#29992;&#36880;&#27493;&#25512;&#29702;&#31574;&#30053;&#20248;&#21270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#22788;&#29702;&#22797;&#26434;&#35821;&#35328;&#29616;&#35937;&#30340;&#33021;&#21147;&#65307;&#24182;&#36890;&#36807;&#22312;&#30417;&#30563;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;$k$NN&#28436;&#31034;&#25628;&#32034;&#65292;&#35299;&#20915;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#26377;&#38480;&#26631;&#35760;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08377</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text Classification via Large Language Models. (arXiv:2305.08377v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Clue And Reasoning Prompting (CARP)&#31639;&#27861;&#65292;&#37319;&#29992;&#36880;&#27493;&#25512;&#29702;&#31574;&#30053;&#20248;&#21270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#22788;&#29702;&#22797;&#26434;&#35821;&#35328;&#29616;&#35937;&#30340;&#33021;&#21147;&#65307;&#24182;&#36890;&#36807;&#22312;&#30417;&#30563;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;$k$NN&#28436;&#31034;&#25628;&#32034;&#65292;&#35299;&#20915;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#26377;&#38480;&#26631;&#35760;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20173;&#28982;&#26174;&#33879;&#19981;&#21450;&#24494;&#35843;&#27169;&#22411;&#12290;&#36825;&#26159;&#30001;&#20110;(1)&#32570;&#20047;&#22788;&#29702;&#22797;&#26434;&#35821;&#35328;&#29616;&#35937;&#65288;&#20363;&#22914;&#24378;&#35843;&#12289;&#23545;&#27604;&#12289;&#21453;&#35773;&#31561;&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65307; (2)&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21482;&#20801;&#35768;&#26377;&#38480;&#25968;&#37327;&#30340;&#26631;&#35760;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Clue And Reasoning Prompting (CARP)&#65292;CARP&#37319;&#29992;&#19968;&#31181;&#36880;&#27493;&#25512;&#29702;&#31574;&#30053;&#65292;&#26088;&#22312;&#24212;&#23545;&#28041;&#21450;&#25991;&#26412;&#20998;&#31867;&#30340;&#22797;&#26434;&#35821;&#35328;&#29616;&#35937;&#65306;CARP&#39318;&#20808;&#25552;&#31034;LLMs&#25214;&#21040;&#34920;&#38754;&#32447;&#32034;&#65288;&#20363;&#22914;&#20851;&#38190;&#35789;&#12289;&#35821;&#27668;&#12289;&#35821;&#20041;&#20851;&#31995;&#12289;&#21442;&#32771;&#31561;&#65289;&#65292;&#28982;&#21518;&#35825;&#23548;&#35786;&#26029;&#24615;&#25512;&#29702;&#36807;&#31243;&#20316;&#20986;&#26368;&#32456;&#20915;&#31574;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35299;&#20915;&#26377;&#38480;&#26631;&#35760;&#30340;&#38382;&#39064;&#65292;CARP&#22312;&#30417;&#30563;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;$k$NN&#28436;&#31034;&#25628;&#32034;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20805;&#20998;&#21033;&#29992;&#20102;LLM&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable success of large-scale Language Models (LLMs) such as GPT-3, their performances still significantly underperform fine-tuned models in the task of text classification. This is due to (1) the lack of reasoning ability in addressing complex linguistic phenomena (e.g., intensification, contrast, irony etc); (2) limited number of tokens allowed in in-context learning.  In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts a progressive reasoning strategy tailored to addressing the complex linguistic phenomena involved in text classification: CARP first prompts LLMs to find superficial clues (e.g., keywords, tones, semantic relations, references, etc), based on which a diagnostic reasoning process is induced for final decisions. To further address the limited-token issue, CARP uses a fine-tuned model on the supervised dataset for $k$NN demonstration search in the in-context learning, allowing the model to take the advantage of both LLM's generali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24402;&#23646;&#39564;&#35777;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#25552;&#31034;LLMs&#21644;&#24494;&#35843;&#36739;&#23567;&#30340;LLMs&#20004;&#31181;&#26041;&#27861;&#37117;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#20102;&#38169;&#35823;&#30340;&#24402;&#23646;&#38472;&#36848;&#12290;</title><link>http://arxiv.org/abs/2305.06311</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#24402;&#23646;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Automatic Evaluation of Attribution by Large Language Models. (arXiv:2305.06311v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24402;&#23646;&#39564;&#35777;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#25552;&#31034;LLMs&#21644;&#24494;&#35843;&#36739;&#23567;&#30340;LLMs&#20004;&#31181;&#26041;&#27861;&#37117;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#20102;&#38169;&#35823;&#30340;&#24402;&#23646;&#38472;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#26041;&#21521;&#26159;&#36890;&#36807;&#24341;&#29992;&#22806;&#37096;&#21442;&#32771;&#26469;&#29983;&#25104;&#21644;&#25903;&#25345;&#23427;&#20204;&#30340;&#20027;&#24352;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#24402;&#23646;&#38382;&#39064;&#65292;&#21363;&#39564;&#35777;&#29983;&#25104;&#30340;&#38472;&#36848;&#26159;&#21542;&#30830;&#23454;&#34987;&#24341;&#29992;&#21442;&#32771;&#20840;&#38754;&#25903;&#25345;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24402;&#23646;&#39564;&#35777;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#24402;&#23646;&#30340;&#23450;&#20041;&#65292;&#28982;&#21518;&#25506;&#35752;&#20102;&#20004;&#31181;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65306;&#25552;&#31034;LLMs&#21644;&#24494;&#35843;&#36739;&#23567;&#30340;LLMs&#12290;&#24494;&#35843;&#25968;&#25454;&#20174;&#30456;&#20851;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#38382;&#31572;&#12289;&#20107;&#23454;&#26816;&#26597;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#25688;&#35201;&#65289;&#20013;&#37325;&#26032;&#21033;&#29992;&#12290;&#20026;&#20102;&#20415;&#20110;&#35780;&#20272;&#65292;&#25105;&#20204;&#25163;&#21160;&#31574;&#21010;&#20102;&#19968;&#32452;&#27979;&#35797;&#20363;&#23376;&#65292;&#20854;&#20013;&#21253;&#25324;12&#20010;&#39046;&#22495;&#30340;&#26469;&#33258;&#26032;&#24517;&#24212;&#21457;&#29983;&#22120;&#30340;&#27979;&#35797;&#20363;&#23376;&#12290;&#25105;&#20204;&#22312;&#32463;&#36807;&#31574;&#21010;&#30340;&#27979;&#35797;&#38598;&#21644;&#26469;&#33258;&#22806;&#37096;&#35821;&#26009;&#24211;&#30340;&#27169;&#25311;&#27979;&#35797;&#20363;&#23376;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#38169;&#35823;&#30340;&#24402;&#23646;&#38472;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support their claims. However, evaluating the attribution, i.e., verifying whether the generated statement is indeed fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate the automatic evaluation of attribution by LLMs. We begin by providing a definition of attribution and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks, such as question answering, fact-checking, natural language inference, and summarization. To facilitate the evaluation, we manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on the curated test set and simulated test examples from ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21462;&#20195;&#20165;&#21463;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#20174;&#32780;&#28040;&#38500;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#38480;&#21046;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06176</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Language Models with Generative Adversarial Feedback. (arXiv:2305.06176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21462;&#20195;&#20165;&#21463;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#20174;&#32780;&#28040;&#38500;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#38480;&#21046;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#36755;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#30340;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;RLHF&#21463;&#21040;&#20154;&#31867;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#29983;&#20135;&#21147;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;: &#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#20195;&#26367;RLHF&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;&#65292;RLGAF&#21487;&#20197;&#24110;&#21161;&#23545;&#40784;LLM&#30340;&#36755;&#20986;&#65292;&#21516;&#26102;&#19981;&#20250;&#21463;&#21040;RLHF&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#20026;&#36827;&#19968;&#27493;&#33258;&#21160;&#21270;AI&#23545;&#40784;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to significantly enhance the performance of large language models (LLMs) by aligning their outputs with desired human values. However, RLHF is constrained by the expertise and productivity limitations of human evaluators. In this study, we investigate an alternative approach: Reinforcement Learning with Generative Adversarial Feedback (RLGAF) to RLHF. Our preliminary findings indicate that RLGAF can help align LLMs outputs while not suffering from the inherent restrictions of RLHF, suggesting promising avenues for further research on automating AI alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861; HEER&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.05640</link><description>&lt;p&gt;
&#38754;&#21521;&#20010;&#20154;&#25110;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65306;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare. (arXiv:2305.05640v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861; HEER&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#19968;&#31181;&#25353;&#26412;&#20307;&#25110;&#27169;&#24335;&#32452;&#32455;&#20449;&#24687;&#30340;&#27969;&#34892;&#26041;&#24335;&#65292;&#24050;&#32463;&#22312;&#20174;&#25628;&#32034;&#21040;&#25512;&#33616;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#30693;&#35782;&#22270;&#35889;&#26041;&#38754;&#26377;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#30693;&#35782;&#34920;&#31034;&#20173;&#28982;&#26159;&#36328;&#34892;&#19994;&#30340;&#19968;&#20010;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#30001;&#20110;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20851;&#31995;&#12289;&#24322;&#36136;&#24615;&#12289;&#32570;&#20047;&#26631;&#20934;&#21270;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#31561;&#22240;&#32032;&#65292;&#36825;&#19968;&#20219;&#21153;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#25429;&#25417;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21517;&#20026;HEER&#65288;Healthcare Entity-Entity Representation learning&#65289;&#65292;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;HEER&#22312;&#25913;&#21892;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) are a popular way to organise information based on ontologies or schemas and have been used across a variety of scenarios from search to recommendation. Despite advances in KGs, representing knowledge remains a non-trivial task across industries and it is especially challenging in the biomedical and healthcare domains due to complex interdependent relations between entities, heterogeneity, lack of standardization, and sparseness of data. KGs are used to discover diagnoses or prioritize genes relevant to disease, but they often rely on schemas that are not centred around a node or entity of interest, such as a person. Entity-centric KGs are relatively unexplored but hold promise in representing important facets connected to a central node and unlocking downstream tasks beyond graph traversal and reasoning, such as generating graph embeddings and training graph neural networks for a wide range of predictive tasks. This paper presents an end-to-end representation le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; MoT &#30340;&#26694;&#26550;&#65292;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#8220;&#24605;&#24819;&#35760;&#24518;&#8221;&#33258;&#25105;&#36827;&#21270;&#65292;&#26080;&#38656;&#27880;&#37322;&#25968;&#25454;&#38598;&#21644;&#21442;&#25968;&#26356;&#26032;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640; ChatGPT &#22312;&#25968;&#23398;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#20107;&#23454;&#25512;&#29702;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05181</link><description>&lt;p&gt;
MoT&#65306;&#39044;&#24605;&#32771;&#21644;&#22238;&#24518;&#21151;&#33021;&#20351; ChatGPT &#22312;&#8220;&#24605;&#24819;&#35760;&#24518;&#8221;&#20013;&#33258;&#25105;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
MoT: Pre-thinking and Recalling Enable ChatGPT to Self-Improve with Memory-of-Thoughts. (arXiv:2305.05181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; MoT &#30340;&#26694;&#26550;&#65292;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#8220;&#24605;&#24819;&#35760;&#24518;&#8221;&#33258;&#25105;&#36827;&#21270;&#65292;&#26080;&#38656;&#27880;&#37322;&#25968;&#25454;&#38598;&#21644;&#21442;&#25968;&#26356;&#26032;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640; ChatGPT &#22312;&#25968;&#23398;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#20107;&#23454;&#25512;&#29702;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#20294;&#35201;&#23454;&#29616;&#23427;&#20204;&#30340;&#26681;&#26412;&#24615;&#25913;&#36827;&#65292;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#25110;&#35745;&#31639;&#26114;&#36149;&#30340;&#24494;&#35843;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#24605;&#32771;&#21644;&#35760;&#24518;&#36731;&#26494;&#25552;&#39640;&#33258;&#25105;&#27700;&#24179;&#65292;&#32780;&#19981;&#38656;&#35201;&#22806;&#37096;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550; MoT&#65292;&#22312;&#27809;&#26377;&#27880;&#37322;&#25968;&#25454;&#38598;&#21644;&#21442;&#25968;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24605;&#24819;&#35760;&#24518;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#36827;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;1. &#22312;&#27979;&#35797;&#38454;&#27573;&#20043;&#21069;&#65292;&#25105;&#20204;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#21152;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#24605;&#32771;&#65292;&#24182;&#23558;&#39640;&#32622;&#20449;&#24230;&#30340;&#24819;&#27861;&#20445;&#23384;&#20026;&#22806;&#37096;&#35760;&#24518;&#12290;2. &#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#27979;&#35797;&#38382;&#39064;&#65292;&#25105;&#20204;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22238;&#24518;&#30456;&#20851;&#30340;&#35760;&#24518;&#65292;&#24110;&#21161;&#33258;&#24049;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161; ChatGPT &#22312;&#25968;&#23398;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#20107;&#23454;&#25512;&#29702;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#20854;&#33021;&#21147;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#27599;&#20010;&#32452;&#20214;&#37117;&#21457;&#25381;&#20102;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have shown impressive abilities on various tasks. However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning. On the contrary, human can easily improve themselves by thinking and memory, without external resources. In this paper, we propose a framework, MoT, to let the LLM self-improve through Memory of Thoughts, without annotated datasets and parameter updates. Specifically, the framework is divided into two stages: 1. before the test stage, we let the LLM pre-think on the unlabeled dataset and save the high-confidence thoughts as external memory; 2. during inference, given a test question, we let the LLM recall relevant memory to help itself reason and answer it. Experimental results show that the proposed framework can help ChatGPT significantly improve its abilities in math reasoning, commonsense reasoning, factual reasoning and natural language inference. Further analyses show that each component contribute
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PeerSum&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20803;&#35780;&#35770;&#29983;&#25104;&#30340;&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#27169;&#22411;Rammer&#12290;</title><link>http://arxiv.org/abs/2305.01498</link><description>&lt;p&gt;
&#26088;&#22312;&#24635;&#32467;&#24102;&#26377;&#23618;&#27425;&#20851;&#31995;&#30340;&#22810;&#31687;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
Towards Summarizing Multiple Documents with Hierarchical Relationships. (arXiv:2305.01498v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01498
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PeerSum&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20803;&#35780;&#35770;&#29983;&#25104;&#30340;&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#27169;&#22411;Rammer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25968;&#29616;&#23384;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;(MDS)&#25968;&#25454;&#38598;&#32570;&#23569;&#20154;&#24037;&#29983;&#25104;&#30340;&#12289;&#30495;&#23454;&#30340;(&#21363;&#38750;&#21512;&#25104;&#30340;)&#25688;&#35201;&#25110;&#32773;&#24102;&#26377;&#26174;&#24335;&#25991;&#26723;&#38388;&#20851;&#31995;&#30340;&#28304;&#25991;&#26723;&#12290;&#20026;&#20102;&#22686;&#24378;MDS&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;PeerSum&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#20854;&#20013;&#20803;&#35780;&#35770;&#26159;&#23545;&#35780;&#35770;&#21644;&#30456;&#24212;&#35752;&#35770;&#30340;&#39640;&#24230;&#27010;&#25324;&#19988;&#30495;&#23454;&#30340;&#25688;&#35201;&#12290;&#36825;&#20123;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#21253;&#25324;&#20132;&#21449;&#24341;&#29992;&#21644;&#32463;&#24120;&#20986;&#29616;&#30340;&#20914;&#31361;&#12290;&#37492;&#20110;&#24456;&#23569;&#26377;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#25805;&#32437;&#26469;&#23558;&#23618;&#27425;&#20851;&#31995;&#32435;&#20837;MDS&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;Rammer(&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#20803;&#35780;&#35770;&#29983;&#25104;&#22120;)&#65292;&#36825;&#26159;&#19968;&#31181;&#20803;&#35780;&#35770;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#22522;&#20110;&#23618;&#27425;&#20851;&#31995;&#30340;&#31232;&#30095;&#27880;&#24847;&#21147;&#21644;&#22810;&#20219;&#21153;&#30446;&#26631;&#65292;&#21487;&#20197;&#39044;&#27979;&#22810;&#20010;&#24230;&#37327;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing multi-document summarization (MDS) datasets lack human-generated and genuine (i.e., not synthetic) summaries or source documents with explicit inter-document relationships that a summary must capture. To enhance the capabilities of MDS systems we present PeerSum, a novel dataset for generating meta-reviews of scientific papers, where the meta-reviews are highly abstractive and genuine summaries of reviews and corresponding discussions. These source documents have rich inter-document relationships of an explicit hierarchical structure with cross-references and often feature conflicts. As there is a scarcity of research that incorporates hierarchical relationships into MDS systems through attention manipulation on pre-trained language models, we additionally present Rammer (Relationship-aware Multi-task Meta-review Generator), a meta-review generation model that uses sparse attention based on the hierarchical relationships and a multi-task objective that predicts several me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GPT-NER&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65288;NER&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#23558;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#36716;&#21270;&#20026;&#29983;&#25104;&#20219;&#21153;&#65292;&#23558;LLM&#33021;&#22815;&#23481;&#26131;&#22320;&#36866;&#24212;NER&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;LLMs&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#39564;&#35777;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#35810;&#38382;&#33258;&#36523;&#26469;&#30830;&#23450;&#25552;&#21462;&#30340;&#23454;&#20307;&#26159;&#21542;&#23646;&#20110;&#23454;&#38469;&#23384;&#22312;&#30340;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2304.10428</link><description>&lt;p&gt;
GPT-NER&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
GPT-NER: Named Entity Recognition via Large Language Models. (arXiv:2304.10428v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GPT-NER&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65288;NER&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#23558;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#36716;&#21270;&#20026;&#29983;&#25104;&#20219;&#21153;&#65292;&#23558;LLM&#33021;&#22815;&#23481;&#26131;&#22320;&#36866;&#24212;NER&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;LLMs&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#39564;&#35777;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#35810;&#38382;&#33258;&#36523;&#26469;&#30830;&#23450;&#25552;&#21462;&#30340;&#23454;&#20307;&#26159;&#21542;&#23646;&#20110;&#23454;&#38469;&#23384;&#22312;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;NER&#24615;&#33021;&#20173;&#28982;&#26126;&#26174;&#20302;&#20110;&#30417;&#30563;&#22522;&#32447;&#12290;&#36825;&#26159;&#30001;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;LLMs&#20043;&#38388;&#30340;&#24046;&#36317;&#65306;&#21069;&#32773;&#22312;&#26412;&#36136;&#19978;&#26159;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#65292;&#21518;&#32773;&#26159;&#19968;&#31181;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPT-NER&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290; GPT-NER&#36890;&#36807;&#23558;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#36716;&#25442;&#20026;&#29983;&#25104;&#20219;&#21153;&#26469;&#24357;&#21512;&#24046;&#36317;&#65292;LLMs&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#12290;&#20363;&#22914;&#65292;&#23558;&#22312;&#36755;&#20837;&#25991;&#26412;&#8220;&#21733;&#20262;&#24067;&#26159;&#19968;&#24231;&#22478;&#24066;&#8221;&#20013;&#26597;&#25214;&#20301;&#32622;&#23454;&#20307;&#30340;&#20219;&#21153;&#36716;&#25442;&#20026;&#29983;&#25104;&#25991;&#26412;&#24207;&#21015;&#8220;@@&#21733;&#20262;&#24067;##&#26159;&#19968;&#24231;&#22478;&#24066;&#8221;&#65292;&#20854;&#20013;&#29305;&#27530;&#26631;&#35760;@@##&#26631;&#35760;&#35201;&#25552;&#21462;&#30340;&#23454;&#20307;&#12290;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;LLMs&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#21363;LLMs&#26377;&#24456;&#24378;&#30340;&#20542;&#21521;&#23558;&#31354;&#36755;&#20837;&#36807;&#24230;&#33258;&#20449;&#22320;&#26631;&#35760;&#20026;&#23454;&#20307;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#39564;&#35777;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#35810;&#38382;&#33258;&#36523;&#26469;&#30830;&#23450;&#25552;&#21462;&#30340;&#23454;&#20307;&#26159;&#21542;&#23646;&#20110;&#23454;&#38469;&#23384;&#22312;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model.  In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text "Columbus is a city" is transformed to generate the text sequence "@@Columbus## is a city", where special tokens @@## marks the entity to extract. To efficiently address the "hallucination" issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a lab
&lt;/p&gt;</description></item><item><title>RRHF&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#27010;&#29575;&#19982;&#20154;&#31867;&#20559;&#22909;&#65292;&#23427;&#36890;&#36807;&#25490;&#24207;&#25439;&#22833;&#23545;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#22312;&#35843;&#25972;&#36807;&#31243;&#20013;&#21482;&#38656;1&#21040;2&#20010;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.05302</link><description>&lt;p&gt;
RRHF: &#26080;&#38656;&#28902;&#24700;&#22320;&#20351;&#29992;&#25490;&#21517;&#21709;&#24212;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
RRHF: Rank Responses to Align Language Models with Human Feedback without tears. (arXiv:2304.05302v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05302
&lt;/p&gt;
&lt;p&gt;
RRHF&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#27010;&#29575;&#19982;&#20154;&#31867;&#20559;&#22909;&#65292;&#23427;&#36890;&#36807;&#25490;&#24207;&#25439;&#22833;&#23545;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#22312;&#35843;&#25972;&#36807;&#31243;&#20013;&#21482;&#38656;1&#21040;2&#20010;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#21487;&#20197;&#24110;&#21161;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20154;&#31867;&#19982;&#36825;&#20123;&#27169;&#22411;&#38388;&#30340;&#20132;&#20114;&#36136;&#37327;&#12290;&#19982;PPO&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#8212;&#8212;RRHF&#65292;&#23427;&#36890;&#36807;&#25490;&#24207;&#25439;&#22833;&#23545;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#23398;&#20064;&#23558;&#23427;&#20204;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;RRHF&#21487;&#20197;&#39640;&#25928;&#22320;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#27010;&#29575;&#19982;&#20154;&#31867;&#20559;&#22909;&#65292;&#20854;&#25928;&#26524;&#21644;Fine-Tuning&#19968;&#26679;&#31283;&#20581;&#65292;&#32780;&#22312;&#35843;&#25972;&#36807;&#31243;&#20013;&#21482;&#38656;1&#21040;2&#20010;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;RRHF&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;SFT&#21644;&#22870;&#21169;&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#19982;PPO&#30456;&#27604;&#22312;&#32534;&#30721;&#21644;&#27169;&#22411;&#25968;&#37327;&#26041;&#38754;&#26356;&#20026;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment of large language models with human preferences, significantly enhancing the quality of interactions between humans and these models. InstructGPT implements RLHF through several stages, including Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). PPO, however, is sensitive to hyperparameters and requires a minimum of four models in its standard implementation, which makes it hard to train. In contrast, we propose a novel learning paradigm called RRHF, which scores responses generated by different sampling policies and learns to align them with human preferences through ranking loss. RRHF can efficiently align language model output probabilities with human preferences as robust as fine-tuning and it only needs 1 to 2 models during tuning. In addition, RRHF can be considered an extension of SFT and reward models while being simpler than PPO in terms of coding, model count
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;POMP&#65292;&#21487;&#20197;&#22312;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#22312;&#20869;&#30340;&#21508;&#31181;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#25552;&#21319;&#35782;&#21035;&#24615;&#33021;&#65292;&#36890;&#36807;&#21387;&#32553;&#35821;&#20041;&#20449;&#24687;&#65292;&#25903;&#25345;&#36229;&#36807;&#20004;&#19975;&#20010;&#31867;&#21035;&#30340;&#35270;&#35273;&#27010;&#24565;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;POMP&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.04704</link><description>&lt;p&gt;
&#20351;&#29992;&#20004;&#19975;&#20010;&#31867;&#21035;&#36827;&#34892;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#35782;&#21035;&#30340;&#25552;&#31034;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition. (arXiv:2304.04704v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;POMP&#65292;&#21487;&#20197;&#22312;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#22312;&#20869;&#30340;&#21508;&#31181;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#25552;&#21319;&#35782;&#21035;&#24615;&#33021;&#65292;&#36890;&#36807;&#21387;&#32553;&#35821;&#20041;&#20449;&#24687;&#65292;&#25903;&#25345;&#36229;&#36807;&#20004;&#19975;&#20010;&#31867;&#21035;&#30340;&#35270;&#35273;&#27010;&#24565;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;POMP&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;POMP&#65292;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;POMP&#26082;&#20855;&#26377;&#23384;&#20648;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#21448;&#33021;&#22815;&#20026;&#36229;&#36807;&#20004;&#19975;&#20010;&#31867;&#21035;&#30340;&#20016;&#23500;&#35270;&#35273;&#27010;&#24565;&#21387;&#32553;&#35821;&#20041;&#20449;&#24687;&#12290;&#19968;&#26086;&#39044;&#35757;&#32451;&#23436;&#25104;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#20256;&#36882;&#33021;&#21147;&#30340;&#25552;&#31034;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#65292;&#20197;&#38646;-shot&#30340;&#26041;&#24335;&#25552;&#21319;&#35782;&#21035;&#24615;&#33021;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;POMP&#22312;21&#20010;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#22312;10&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;67.0%&#65288;&#27604;CoOp&#39640;&#20986;3.1%&#65289;&#65292;&#22312;&#24320;&#25918;&#35789;&#27719;&#30340;Pascal VOC&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;hIoU&#20026;84.4&#65288;&#27604;ZSSeg&#39640;&#20986;6.9&#65289;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/amazon-science/prompt-pretraining&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes POMP, a prompt pre-training method for vision-language models. Being memory and computation efficient, POMP enables the learned prompt to condense semantic information for a rich set of visual concepts with over twenty-thousand classes. Once pre-trained, the prompt with a strong transferable ability can be directly plugged into a variety of visual recognition tasks including image classification, semantic segmentation, and object detection, to boost recognition performances in a zero-shot manner. Empirical evaluation shows that POMP achieves state-of-the-art performances on 21 datasets, e.g., 67.0% average accuracy on 10 classification datasets (+3.1% compared to CoOp) and 84.4 hIoU on open-vocabulary Pascal VOC segmentation (+6.9 compared to ZSSeg). Our code is available at https://github.com/amazon-science/prompt-pretraining.
&lt;/p&gt;</description></item><item><title>GEMINI&#27169;&#22411;&#23558;&#21477;&#23376;&#37325;&#20889;&#21644;&#34701;&#21512;&#25216;&#26415;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#21477;&#23376;&#32423;&#20889;&#20316;&#39118;&#26684;&#25511;&#21046;&#65292;&#35813;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#38598;&#20855;&#26377;&#24179;&#34913;&#30340;&#39118;&#26684;&#20998;&#24067;&#26102;&#12290;</title><link>http://arxiv.org/abs/2304.03548</link><description>&lt;p&gt;
GEMINI&#65306;&#38024;&#23545;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#25511;&#21046;&#21477;&#23376;&#32423;&#20889;&#20316;&#39118;&#26684;
&lt;/p&gt;
&lt;p&gt;
GEMINI: Controlling the Sentence-level Writing Style for Abstractive Text Summarization. (arXiv:2304.03548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03548
&lt;/p&gt;
&lt;p&gt;
GEMINI&#27169;&#22411;&#23558;&#21477;&#23376;&#37325;&#20889;&#21644;&#34701;&#21512;&#25216;&#26415;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#21477;&#23376;&#32423;&#20889;&#20316;&#39118;&#26684;&#25511;&#21046;&#65292;&#35813;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#38598;&#20855;&#26377;&#24179;&#34913;&#30340;&#39118;&#26684;&#20998;&#24067;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#19987;&#23478;&#20351;&#29992;&#19981;&#21516;&#25216;&#26415;&#32534;&#20889;&#25688;&#35201;&#65292;&#21253;&#25324;&#37325;&#20889;&#25991;&#26723;&#20013;&#30340;&#21477;&#23376;&#25110;&#21512;&#24182;&#22810;&#20010;&#21477;&#23376;&#29983;&#25104;&#25688;&#35201;&#21477;&#12290;&#36825;&#20123;&#25216;&#26415;&#26159;&#28789;&#27963;&#30340;&#65292;&#22240;&#27492;&#24456;&#38590;&#36890;&#36807;&#20219;&#20309;&#21333;&#19968;&#26041;&#27861;&#27169;&#25311;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27169;&#22411;GEMINI&#65292;&#23558;&#37325;&#20889;&#22120;&#21644;&#34701;&#21512;&#22120;&#38598;&#25104;&#36215;&#26469;&#65292;&#20197;&#27169;&#25311;&#21477;&#23376;&#37325;&#20889;&#21644;&#34701;&#21512;&#25216;&#26415;&#12290;GEMINI&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#37325;&#20889;&#29305;&#23450;&#30340;&#25991;&#26723;&#21477;&#23376;&#25110;&#20174;&#22836;&#29983;&#25104;&#25688;&#35201;&#21477;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#32431;&#25277;&#35937;&#21644;&#37325;&#20889;&#22522;&#32447;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#38598;&#20855;&#26377;&#24179;&#34913;&#30340;&#39118;&#26684;&#20998;&#24067;&#26102;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27599;&#20010;&#25688;&#35201;&#21477;&#30340;&#20154;&#31867;&#20889;&#20316;&#39118;&#26684;&#22312;&#20854;&#19978;&#19979;&#25991;&#20013;&#26159;&#21487;&#20197;&#39044;&#27979;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human experts write summaries using different techniques, including rewriting a sentence in the document or fusing multiple sentences to generate a summary sentence. These techniques are flexible and thus difficult to be imitated by any single method. To address this issue, we propose an adaptive model, GEMINI, that integrates a rewriter and a fuser to mimic the sentence rewriting and fusion techniques, respectively. GEMINI adaptively chooses to rewrite a specific document sentence or generate a summary sentence from scratch. Experiments demonstrate that our adaptive approach outperforms the pure abstractive and rewriting baselines on various benchmark datasets, especially when the dataset has a balanced distribution of styles. Interestingly, empirical results show that the human writing style of each summary sentence is consistently predictable given its context.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LLM-Adapters&#65292;&#19968;&#20010;&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;LLMs&#20013;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.01933</link><description>&lt;p&gt;
LLM-Adapters&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#36866;&#37197;&#22120;&#31995;&#21015;
&lt;/p&gt;
&lt;p&gt;
LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. (arXiv:2304.01933v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLM-Adapters&#65292;&#19968;&#20010;&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;LLMs&#20013;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;GPT-3&#21644;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#65292;&#23548;&#33268;&#20102;&#35768;&#22810;&#32463;&#27982;&#23454;&#24800;&#21644;&#26131;&#20110;&#35775;&#38382;&#30340;&#26367;&#20195;&#21697;&#30340;&#24320;&#21457;&#65292;&#36825;&#20123;&#26367;&#20195;&#21697;&#26159;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#65288;&#20363;&#22914;ChatDoctor&#65289;&#25110;&#25351;&#23548;&#25968;&#25454;&#65288;&#20363;&#22914;Alpaca&#65289;&#24494;&#35843;&#24320;&#25918;&#24335;LLMs&#32780;&#21019;&#24314;&#30340;&#12290;&#22312;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26080;&#30097;&#26159;&#26368;&#26377;&#21560;&#24341;&#21147;&#30340;&#30740;&#31350;&#20027;&#39064;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#24494;&#35843;&#23569;&#37327;&#22806;&#37096;&#21442;&#25968;&#32780;&#19981;&#26159;&#25972;&#20010;LLMs&#65292;&#21516;&#26102;&#23454;&#29616;&#21487;&#27604;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;LLMs&#30340;PEFT&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;LLM-Adapters&#65292;&#36825;&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#23558;&#21508;&#31181;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;LLMs&#20013;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#19981;&#21516;&#20219;&#21153;&#25191;&#34892;&#36825;&#20123;&#36866;&#37197;&#22120;&#30340;PEFT&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#24335;LLMs&#65292;&#20363;&#22914;LLaMA&#65292;BLOOM&#65292;OPT&#21644;GPT-J&#65292;&#20197;&#21450;&#24191;&#27867;&#20351;&#29992;&#30340;&#36866;&#37197;&#22120;&#65292;&#20363;&#22914;&#20018;&#32852;&#36866;&#37197;&#22120;&#65292;&#24182;&#32852;&#36866;&#37197;&#22120;&#21644;LoRA&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#39640;&#25928;&#19988;&#28789;&#27963;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#26368;&#23569;&#30340;&#38468;&#21152;&#35757;&#32451;&#25968;&#25454;&#25110;&#35745;&#31639;&#36164;&#28304;&#36731;&#26494;&#24494;&#35843;LLMs&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LLM-Adapters&#21487;&#20197;&#27604;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of large language models (LLMs), like GPT-3 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by fine-tuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, OPT, and GPT-J, as well as widely used adapters such as Series adapter, Parallel adapter, and LoRA. The framework is designed to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.15714</link><description>&lt;p&gt;
&#26174;&#24335;&#35268;&#21010;&#26377;&#21161;&#20110;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;&#26174;&#24335;&#35268;&#21010;&#32435;&#20837;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#23637;&#26395;&#26410;&#26469;&#30340;&#25928;&#26524;&#26469;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#20840;&#22871;&#31995;&#32479;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#39064;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#65292;&#23613;&#31649;&#21482;&#26377;&#32422;15&#20159;&#20010;&#21442;&#25968;&#65292;&#20294;&#19982;GPT-3-davinci&#34920;&#29616;&#30456;&#24403;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#28040;&#34701;&#30740;&#31350;&#20197;&#35777;&#26126;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose a novel system that uses language models to perform multi-step logical reasoning. Our system incorporates explicit planning into its inference procedure, thus able to make more informed reasoning decisions at each step by looking ahead into their future effects. In our experiments, our full system significantly outperforms other competing systems. On a multiple-choice question answering task, our system performs competitively compared to GPT-3-davinci despite having only around 1.5B parameters. We conduct several ablation studies to demonstrate that explicit planning plays a crucial role in the system's performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21435;&#20559;&#32622;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#30495;&#23454;&#24863;&#12290;</title><link>http://arxiv.org/abs/2303.15413</link><description>&lt;p&gt;
2D&#25193;&#25955;&#31639;&#27861;&#30340;&#21435;&#20559;&#32622;&#26041;&#27861;&#29992;&#20110;&#25991;&#26412;&#21040;3D&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation. (arXiv:2303.15413v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21435;&#20559;&#32622;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#30495;&#23454;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#20013;&#20986;&#29616;&#30340;&#35270;&#35282;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#20063;&#31216;&#20026;Janus&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#26469;&#33258;&#20110;2D&#25193;&#25955;&#27169;&#22411;&#30340;&#22266;&#26377;&#20559;&#32622;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;3D&#23545;&#35937;&#19981;&#30495;&#23454;&#12290;&#36890;&#36807;&#23545;&#20854;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21435;&#38500;&#20559;&#32622;&#20197;&#23454;&#29616;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#30340;&#40065;&#26834;&#24615;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#21483;&#20570;score debiasing&#65292;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#26469;&#36798;&#21040;&#21435;&#38500;&#20559;&#32622;&#30340;&#25928;&#26524;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#21483;&#20570;prompt debiasing&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30830;&#23450;&#29992;&#25143;&#25552;&#31034;&#21644;&#35270;&#35282;&#25552;&#31034;&#20043;&#38388;&#30340;&#30683;&#30462;&#35789;&#35821;&#65292;&#24182;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#20102;&#30495;&#23454;&#24863;&#65292;&#24182;&#22312;&#36136;&#37327;&#19982;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The view inconsistency problem in score-distilling text-to-3D generation, also known as the Janus problem, arises from the intrinsic bias of 2D diffusion models, which leads to the unrealistic generation of 3D objects. In this work, we explore score-distilling text-to-3D generation and identify the main causes of the Janus problem. Based on these findings, we propose two approaches to debias the score-distillation frameworks for robust text-to-3D generation. Our first approach, called score debiasing, involves gradually increasing the truncation value for the score estimated by 2D diffusion models throughout the optimization process. Our second approach, called prompt debiasing, identifies conflicting words between user prompts and view prompts utilizing a language model and adjusts the discrepancy between view prompts and object-space camera poses. Our experimental results show that our methods improve realism by significantly reducing artifacts and achieve a good trade-off between fa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;Error Analysis Prompting&#21487;&#25913;&#21892;LLMs&#22312;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#19978;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2303.13809</link><description>&lt;p&gt;
&#38169;&#35823;&#20998;&#26512;&#25552;&#31034;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32763;&#35793;&#35780;&#20272;&#26041;&#38754;&#23454;&#29616;&#20102;&#20154;&#31867;&#27700;&#24179;&#65306;&#20197;ChatGPT&#20026;&#20363;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT. (arXiv:2303.13809v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;Error Analysis Prompting&#21487;&#25913;&#21892;LLMs&#22312;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#19978;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20363;&#22914;ChatGPT&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31561;&#22810;&#20010;NLP&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;ChatGPT&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#22312;&#31995;&#32479;&#27700;&#24179;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#27573;&#33853;&#27700;&#24179;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;LLM&#22312;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#19978;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20851;&#20110;&#20960;&#31181;&#25552;&#31034;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;Chain-of-Thoughts&#21644;Error Analysis&#32467;&#21512;&#36215;&#26469;&#65292;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;Error Analysis Prompting&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;LLM&#21487;&#20197;&#22312;&#31995;&#32479;&#21644;&#27573;&#33853;&#32423;&#21035;&#19978;&#29983;&#25104;&#20154;&#31867;&#33324;&#30340;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;ChatGPT&#20316;&#20026;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#22120;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#22312;&#25552;&#20379;&#21333;&#20010;&#26597;&#35810;&#20013;&#30340;&#22810;&#20010;&#35793;&#25991;&#26102;&#23384;&#22312;&#19981;&#31283;&#23450;&#30340;&#35780;&#20998;&#21644;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks such as machine translation, question answering, text summarization, and natural language understanding. Recent research has shown that utilizing ChatGPT for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but performs poorly at the segment level. To further improve the performance of LLMs on MT quality assessment, we conducted an investigation into several prompting methods. Our results indicate that by combining Chain-of-Thoughts and Error Analysis, a new prompting method called \textbf{\texttt{Error Analysis Prompting}}, LLMs like ChatGPT can \textit{generate human-like MT evaluations at both the system and segment level}. Additionally, we discovered some limitations of ChatGPT as an MT evaluator, such as unstable scoring and biases when provided with multiple translations in a single query. Our findings
&lt;/p&gt;</description></item><item><title>&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#20998;&#23376;&#32467;&#26500;&#30340;&#25972;&#20307;&#24615;&#21644;&#25163;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.11593</link><description>&lt;p&gt;
&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#25163;&#24615;&#26102;&#23384;&#22312;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Difficulty in learning chirality for Transformer fed with SMILES. (arXiv:2303.11593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11593
&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#20998;&#23376;&#32467;&#26500;&#30340;&#25972;&#20307;&#24615;&#21644;&#25163;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#23545;&#26497;&#20854;&#22810;&#26679;&#30340;&#20998;&#23376;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#25551;&#36848;&#31526;&#29983;&#25104;&#24050;&#32463;&#24471;&#21040;&#20102;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#24212;&#29992;&#20110;SMILES&#65292;&#21363;&#20998;&#23376;&#32467;&#26500;&#30340;&#25991;&#23383;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29702;&#35299;&#21270;&#23398;&#32467;&#26500;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;NLP&#27169;&#22411;&#8212;&#8212;Transformer&#65292;&#22312;&#23398;&#20064;SMILES&#21644;&#21270;&#23398;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;Transformer&#24555;&#36895;&#23398;&#20064;&#20998;&#23376;&#30340;&#37096;&#20998;&#32467;&#26500;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#25165;&#33021;&#29702;&#35299;&#25972;&#20307;&#32467;&#26500;&#12290;&#19982;&#20043;&#19968;&#33268;&#30340;&#26159;&#65292;&#22312;&#19981;&#21516;&#30340;&#23398;&#20064;&#27493;&#39588;&#20013;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;Transformer&#38656;&#35201;&#29305;&#21035;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#25165;&#33021;&#23398;&#20064;&#25163;&#24615;&#65292;&#24182;&#19988;&#26377;&#26102;&#20250;&#20986;&#29616;&#20302;&#32763;&#35793;&#20934;&#30830;&#29575;&#30340;&#20572;&#28382;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen development of descriptor generation based on representation learning of extremely diverse molecules, especially those that apply natural language processing (NLP) models to SMILES, a literal representation of molecular structure. However, little research has been done on how these models understand chemical structure. To address this, we investigated the relationship between the learning progress of SMILES and chemical structure using a representative NLP model, the Transformer. The results suggest that while the Transformer learns partial structures of molecules quickly, it requires extended training to understand overall structures. Consistently, the accuracy of molecular property predictions using descriptors generated from models at different learning steps was similar from the beginning to the end of training. Furthermore, we found that the Transformer requires particularly long training to learn chirality and sometimes stagnates with low translation accura
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#32508;&#36848;&#20102;&#36890;&#36807;&#26816;&#32034;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#21327;&#21161;&#21644;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#20102;&#35299;&#20915;&#20934;&#30830;&#24615;&#12289;&#25512;&#29702;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#37325;&#35201;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.10868</link><description>&lt;p&gt;
&#26816;&#32034;&#22810;&#27169;&#24577;&#20449;&#24687;&#29992;&#20110;&#22686;&#24378;&#29983;&#25104;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Retrieving Multimodal Information for Augmented Generation: A Survey. (arXiv:2303.10868v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10868
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#32508;&#36848;&#20102;&#36890;&#36807;&#26816;&#32034;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#21327;&#21161;&#21644;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#20102;&#35299;&#20915;&#20934;&#30830;&#24615;&#12289;&#25512;&#29702;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#37325;&#35201;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#21450;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#36235;&#21183;&#65292;&#36825;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#26356;&#22909;&#22320;&#19982;&#19990;&#30028;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#21738;&#20010;&#38454;&#27573;&#20197;&#21450;&#22914;&#20309;&#34701;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#38382;&#39064;&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#35748;&#35782;&#12290;&#22312;&#26412;&#35843;&#30740;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#36890;&#36807;&#26816;&#32034;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#21327;&#21161;&#21644;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#30693;&#35782;&#30340;&#26684;&#24335;&#21253;&#25324;&#22270;&#20687;&#12289;&#20195;&#30721;&#12289;&#34920;&#26684;&#12289;&#22270;&#24418;&#21644;&#38899;&#39057;&#12290;&#36825;&#20123;&#26041;&#27861;&#20026;&#23454;&#29616;&#20934;&#30830;&#24615;&#12289;&#25512;&#29702;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#37325;&#35201;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#25552;&#20379;&#28145;&#20837;&#30340;&#22238;&#39038;&#65292;&#26412;&#35843;&#30740;&#26088;&#22312;&#35753;&#23398;&#32773;&#20204;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#24182;&#40723;&#21169;&#20182;&#20204;&#23558;&#29616;&#26377;&#25216;&#26415;&#24212;&#29992;&#20110;&#24555;&#36895;&#21457;&#23637;&#30340;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) become popular, there emerged an important trend of using multimodality to augment the LLMs' generation ability, which enables LLMs to better interact with the world. However, there lacks a unified perception of at which stage and how to incorporate different modalities. In this survey, we review methods that assist and augment generative models by retrieving multimodal knowledge, whose formats range from images, codes, tables, graphs, to audio. Such methods offer a promising solution to important concerns such as factuality, reasoning, interpretability, and robustness. By providing an in-depth review, this survey is expected to provide scholars with a deeper understanding of the methods' applications and encourage them to adapt existing techniques to the fast-growing field of LLMs.
&lt;/p&gt;</description></item><item><title>DeltaScore&#21033;&#29992;&#24046;&#20998;&#25200;&#21160;&#26469;&#35780;&#20272;&#25925;&#20107;&#29983;&#25104;&#30340;&#32454;&#31890;&#24230;&#26041;&#38754;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#25925;&#20107;&#22312;&#29305;&#23450;&#26041;&#38754;&#25200;&#21160;&#21069;&#21518;&#30340;&#21487;&#33021;&#24615;&#24046;&#24322;&#26469;&#34913;&#37327;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25925;&#20107;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.08991</link><description>&lt;p&gt;
DeltaScore: &#21033;&#29992;&#24046;&#20998;&#25200;&#21160;&#35780;&#20215;&#25925;&#20107;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DeltaScore: Evaluating Story Generation with Differentiating Perturbations. (arXiv:2303.08991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08991
&lt;/p&gt;
&lt;p&gt;
DeltaScore&#21033;&#29992;&#24046;&#20998;&#25200;&#21160;&#26469;&#35780;&#20272;&#25925;&#20107;&#29983;&#25104;&#30340;&#32454;&#31890;&#24230;&#26041;&#38754;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#25925;&#20107;&#22312;&#29305;&#23450;&#26041;&#38754;&#25200;&#21160;&#21069;&#21518;&#30340;&#21487;&#33021;&#24615;&#24046;&#24322;&#26469;&#34913;&#37327;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25925;&#20107;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#21508;&#31181;&#35780;&#20215;&#25351;&#26631;&#23384;&#22312;&#65292;&#20294;&#23545;&#20110;&#25925;&#20107;&#29983;&#25104;&#30340;&#23454;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#19981;&#24378;&#65292;&#20063;&#19981;&#33021;&#27979;&#37327;&#32454;&#31890;&#24230;&#30340;&#25925;&#20107;&#26041;&#38754;&#65292;&#20363;&#22914;&#27969;&#30021;&#24230;&#19982;&#30456;&#20851;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26088;&#22312;&#35780;&#20272;&#25972;&#20307;&#29983;&#25104;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;DeltaScore&#65292;&#19968;&#31181;&#21033;&#29992;&#25200;&#21160;&#26469;&#35780;&#20272;&#32454;&#31890;&#24230;&#25925;&#20107;&#26041;&#38754;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22522;&#20110;&#36825;&#26679;&#30340;&#20551;&#35774;&#65306;&#25925;&#20107;&#22312;&#29305;&#23450;&#26041;&#38754;&#34920;&#29616;&#24471;&#36234;&#22909;&#65288;&#20363;&#22914;&#27969;&#30021;&#24230;&#65289;&#65292;&#23427;&#23601;&#20250;&#21463;&#21040;&#29305;&#23450;&#25200;&#21160;&#65288;&#20363;&#22914;&#24341;&#20837;&#38169;&#21035;&#23383;&#65289;&#30340;&#24433;&#21709;&#36234;&#22823;&#12290;&#20026;&#20102;&#34913;&#37327;&#24433;&#21709;&#65292;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#25200;&#21160;&#21069;&#21518;&#25925;&#20107;&#30340;&#21487;&#33021;&#24615;&#24046;&#24322;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25925;&#20107;&#39046;&#22495;&#20013;&#20351;&#29992;DeltaScore&#35780;&#20272;&#20102;&#22522;&#20110;&#29366;&#24577;&#30340;&#26368;&#26032;&#27169;&#22411;&#21644;&#20256;&#32479;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#25351;&#26631;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#19982;&#20154;&#31867;&#22312;&#20116;&#20010;&#32454;&#31890;&#24230;&#25925;&#20107;&#26041;&#38754;&#30340;&#21028;&#26029;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various evaluation metrics exist for natural language generation tasks, but they have limited utility for story generation since they generally do not correlate well with human judgments and do not measure fine-grained story aspects, such as fluency versus relatedness, as they are intended to assess overall generation quality. In this paper, we propose deltascore, an approach that utilizes perturbation to evaluate fine-grained story aspects. Our core idea is based on the hypothesis that the better the story performs in a specific aspect (e.g., fluency), the more it will be affected by a particular perturbation (e.g., introducing typos). To measure the impact, we calculate the likelihood difference between the pre- and post-perturbation stories using a language model. We evaluate deltascore against state-of-the-art model-based and traditional similarity-based metrics across multiple story domains, and investigate its correlation with human judgments on five fine-grained story aspects: f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#34588;&#34562;&#31639;&#27861;&#20248;&#21270;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#21307;&#23398;&#25991;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#39640;&#20934;&#30830;&#29575;&#22312;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;99.63%&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;88%&#12290;</title><link>http://arxiv.org/abs/2303.08021</link><description>&lt;p&gt;
&#29992;&#34588;&#34562;&#31639;&#27861;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#25552;&#39640;&#21307;&#23398;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Optimizing Deep Learning Model Parameters with the Bees Algorithm for Improved Medical Text Classification. (arXiv:2303.08021v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#34588;&#34562;&#31639;&#27861;&#20248;&#21270;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#21307;&#23398;&#25991;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#39640;&#20934;&#30830;&#29575;&#22312;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;99.63%&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;88%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#34588;&#34562;&#31639;&#27861;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21442;&#25968;&#20248;&#21270;&#30340;&#26032;&#26426;&#21046;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#36817;&#24456;&#26377;&#21069;&#36884;&#30340;&#32676;&#26234;&#33021;&#31639;&#27861;&#12290;&#20248;&#21270;&#38382;&#39064;&#26159;&#22312;&#32473;&#23450;&#21021;&#22987;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#30830;&#23450;&#30340;&#36845;&#20195;&#27425;&#25968;&#26469;&#26368;&#22823;&#21270;&#22522;&#20110;&#21307;&#23398;&#25991;&#26412;&#20998;&#31867;&#30142;&#30149;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#21253;&#25324;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65306;&#33521;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#12290;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518; (LSTM) &#21644;&#34588;&#34562;&#31639;&#27861;&#65292;&#22312;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;99.63%&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;AraBERT&#33719;&#24471;&#20102;88%&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel mechanism to obtain the optimal parameters of a deep learning model using the Bees Algorithm, which is a recent promising swarm intelligence algorithm. The optimization problem is to maximize the accuracy of classifying ailments based on medical text given the initial hyper-parameters to be adjusted throughout a definite number of iterations. Experiments included two different datasets: English and Arabic. The highest accuracy achieved is 99.63% on the English dataset using Long Short-Term Memory (LSTM) along with the Bees Algorithm, and 88% on the Arabic dataset using AraBERT.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LENS&#30340;&#36807;&#28388;-&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25903;&#25345;&#20363;&#23376;&#12290;LENS&#36890;&#36807;&#20998;&#38454;&#27573;&#30340;&#36807;&#28388;&#21644;&#25628;&#32034;&#65292;&#22312;&#25968;&#25454;&#38598;&#20013;&#31579;&#36873;&#20449;&#24687;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#24182;&#36890;&#36807;&#22810;&#26679;&#24615;&#24341;&#23548;&#30340;&#31034;&#20363;&#25628;&#32034;&#26041;&#27861;&#25214;&#21040;&#33021;&#22815;&#20805;&#20998;&#25551;&#32472;&#20219;&#21153;&#30340;&#31034;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LENS&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.13539</link><description>&lt;p&gt;
&#23547;&#25214;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25903;&#25345;&#20363;&#23376;
&lt;/p&gt;
&lt;p&gt;
Finding Support Examples for In-Context Learning. (arXiv:2302.13539v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13539
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LENS&#30340;&#36807;&#28388;-&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25903;&#25345;&#20363;&#23376;&#12290;LENS&#36890;&#36807;&#20998;&#38454;&#27573;&#30340;&#36807;&#28388;&#21644;&#25628;&#32034;&#65292;&#22312;&#25968;&#25454;&#38598;&#20013;&#31579;&#36873;&#20449;&#24687;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#24182;&#36890;&#36807;&#22810;&#26679;&#24615;&#24341;&#23548;&#30340;&#31034;&#20363;&#25628;&#32034;&#26041;&#27861;&#25214;&#21040;&#33021;&#22815;&#20805;&#20998;&#25551;&#32472;&#20219;&#21153;&#30340;&#31034;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LENS&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27492;&#22806;&#65292;&#22312;&#19978;&#19979;&#25991;&#31034;&#20363;&#20043;&#38388;&#23384;&#22312;&#24378;&#20381;&#36182;&#20851;&#31995;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;NP&#38590;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#21015;&#20030;&#25152;&#26377;&#30340;&#25490;&#21015;&#32452;&#21512;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LENS&#65292;&#19968;&#31181;&#36807;&#28388;-&#25628;&#32034;&#26041;&#27861;&#65292;&#20197;&#20004;&#20010;&#38454;&#27573;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#36807;&#28388;&#25968;&#25454;&#38598;&#26469;&#20998;&#21035;&#33719;&#21462;&#20449;&#24687;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;InfoScore&#65292;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#35780;&#20272;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#36807;&#28388;&#36807;&#31243;&#26469;&#36807;&#28388;&#25481;&#26080;&#20449;&#24687;&#30340;&#20363;&#23376;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26679;&#24615;&#24341;&#23548;&#30340;&#31034;&#20363;&#25628;&#32034;&#65292;&#36890;&#36807;&#36845;&#20195;&#25913;&#36827;&#21644;&#35780;&#20272;&#25152;&#36873;&#31034;&#20363;&#30340;&#25490;&#21015;&#65292;&#26469;&#25214;&#21040;&#20805;&#20998;&#25551;&#32472;&#20219;&#21153;&#30340;&#31034;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LENS&#26126;&#26174;&#20248;&#20110;&#24191;&#27867;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Additionally, the strong dependency among in-context examples makes it an NP-hard combinatorial optimization problem and enumerating all permutations is infeasible. Hence we propose LENS, a fiLter-thEN-Search method to tackle this challenge in two stages: First we filter the dataset to obtain informative in-context examples individually. Specifically, we propose a novel metric, InfoScore, to evaluate the example's in-context informativeness based on the language model's feedback, and further propose a progressive filtering process to filter out uninformative examples. Then we propose diversity-guided example search which iteratively refines and evaluates the selected example permutations, to find examples that fully depict the task. The experimental results show that LENS significantly outperforms a wide range of baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#26080;&#27861;&#20165;&#20973;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#30340;&#20449;&#24687;&#23547;&#27714;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;InfoSeek&#12290;&#20351;&#29992;InfoSeek&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#28608;&#21457;&#27169;&#22411;&#20351;&#29992;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2302.11713</link><description>&lt;p&gt;
Pre-trained Vision and Language Models&#33021;&#21542;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?. (arXiv:2302.11713v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#26080;&#27861;&#20165;&#20973;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#30340;&#20449;&#24687;&#23547;&#27714;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;InfoSeek&#12290;&#20351;&#29992;InfoSeek&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#28608;&#21457;&#27169;&#22411;&#20351;&#29992;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pre-trained vision and language models&#22312;&#28041;&#21450;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#39046;&#20808;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#35270;&#35273;&#38382;&#31572;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#20855;&#22791;&#22238;&#31572;&#19981;&#20165;&#20165;&#26597;&#35810;&#35270;&#35273;&#20869;&#23481;&#65292;&#32780;&#19988;&#36824;&#20855;&#26377;&#30693;&#35782;&#23494;&#38598;&#21644;&#20449;&#24687;&#23547;&#27714;&#24615;&#36136;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;InfoSeek&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#26080;&#27861;&#20165;&#20973;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#30340;&#20449;&#24687;&#23547;&#27714;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;InfoSeek&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#65292;&#24182;&#28145;&#20837;&#20102;&#35299;&#23427;&#20204;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;PaLI-X&#65292;BLIP2&#31561;&#65289;&#22312;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#22312;InfoSeek&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#33021;&#22815;&#28608;&#21457;&#27169;&#22411;&#20351;&#29992;&#20182;&#20204;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#21040;&#30340;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20934;&#30830;&#30340;&#35270;&#35273;&#23454;&#20307;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained vision and language models have demonstrated state-of-the-art capabilities over existing tasks involving images and texts, including visual question answering. However, it remains unclear whether these models possess the capability to answer questions that are not only querying visual content but knowledge-intensive and information-seeking. In this study, we introduce InfoSeek, a visual question answering dataset tailored for information-seeking questions that cannot be answered with only common sense knowledge. Using InfoSeek, we analyze various pre-trained visual question answering models and gain insights into their characteristics. Our findings reveal that state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.) face challenges in answering visual information-seeking questions, but fine-tuning on the InfoSeek dataset elicits models to use fine-grained knowledge that was learned during their pre-training. Furthermore, we show that accurate visual entit
&lt;/p&gt;</description></item><item><title>RETVec&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#24377;&#24615;&#21644;&#22810;&#35821;&#35328;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;&#22120;&#65292;&#36890;&#36807;&#37319;&#29992;&#26032;&#39062;&#30340;&#23383;&#31526;&#32534;&#30721;&#21644;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#25340;&#20889;&#38169;&#35823;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26356;&#22909;&#36866;&#24212;&#24615;&#12290;&#19982;&#20854;&#20182;&#21521;&#37327;&#21270;&#22120;&#21644;&#35789;&#23884;&#20837;&#27169;&#22411;&#30456;&#27604;&#65292;RETVec&#22312;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#21644;&#26174;&#33879;&#30340;&#24377;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.09207</link><description>&lt;p&gt;
RETVec&#65306;&#24377;&#24615;&#21644;&#39640;&#25928;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
RETVec: Resilient and Efficient Text Vectorizer. (arXiv:2302.09207v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09207
&lt;/p&gt;
&lt;p&gt;
RETVec&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#24377;&#24615;&#21644;&#22810;&#35821;&#35328;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;&#22120;&#65292;&#36890;&#36807;&#37319;&#29992;&#26032;&#39062;&#30340;&#23383;&#31526;&#32534;&#30721;&#21644;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#25340;&#20889;&#38169;&#35823;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26356;&#22909;&#36866;&#24212;&#24615;&#12290;&#19982;&#20854;&#20182;&#21521;&#37327;&#21270;&#22120;&#21644;&#35789;&#23884;&#20837;&#27169;&#22411;&#30456;&#27604;&#65292;RETVec&#22312;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#21644;&#26174;&#33879;&#30340;&#24377;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;RETVec&#65292;&#19968;&#31181;&#19987;&#20026;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#22788;&#29702;&#32780;&#35774;&#35745;&#30340;&#39640;&#25928;&#12289;&#24377;&#24615;&#21644;&#22810;&#35821;&#35328;&#30340;&#25991;&#26412;&#21521;&#37327;&#21270;&#22120;&#12290;RETVec&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23383;&#31526;&#32534;&#30721;&#21644;&#21487;&#36873;&#30340;&#23567;&#22411;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#35789;&#35821;&#23884;&#20837;&#21040;256&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#12290;RETVec&#30340;&#23884;&#20837;&#27169;&#22411;&#20351;&#29992;&#23545;&#27604;&#24230;&#23398;&#20064;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#38024;&#23545;&#25340;&#20889;&#38169;&#35823;&#21644;&#23383;&#31526;&#32423;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;RETVec&#22312;&#27969;&#34892;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#36825;&#20123;&#27604;&#36739;&#34920;&#26126;&#65292;RETVec&#33021;&#22815;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#25340;&#20889;&#38169;&#35823;&#21644;&#23545;&#25239;&#24615;&#25991;&#26412;&#25915;&#20987;&#20855;&#26377;&#26174;&#33879;&#30340;&#24377;&#24615;&#12290;RETVec&#22312;Apache 2&#35768;&#21487;&#19979;&#21487;&#22312;https://github.com/google-research/retvec&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes RETVec, an efficient, resilient, and multilingual text vectorizer designed for neural-based text processing. RETVec combines a novel character encoding with an optional small embedding model to embed words into a 256-dimensional vector space. The RETVec embedding model is pre-trained using pair-wise metric learning to be robust against typos and character-level adversarial attacks. In this paper, we evaluate and compare RETVec to state-of-the-art vectorizers and word embeddings on popular model architectures and datasets. These comparisons demonstrate that RETVec leads to competitive, multilingual models that are significantly more resilient to typos and adversarial text attacks. RETVec is available under the Apache 2 license at https://github.com/google-research/retvec.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;Penguin&#65292;&#26088;&#22312;&#20419;&#36827;&#20013;&#25991;&#38405;&#35835;&#29702;&#35299;&#20013;&#33258;&#28982;&#21709;&#24212;&#29983;&#25104;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#23545;&#36739;&#22823;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#24179;&#21488;&#12290;&#36890;&#36807;&#24320;&#21457;&#20004;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;Penguin&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.08817</link><description>&lt;p&gt;
&#20013;&#25991;&#38405;&#35835;&#29702;&#35299;&#30340;&#33258;&#28982;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Natural Response Generation for Chinese Reading Comprehension. (arXiv:2302.08817v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;Penguin&#65292;&#26088;&#22312;&#20419;&#36827;&#20013;&#25991;&#38405;&#35835;&#29702;&#35299;&#20013;&#33258;&#28982;&#21709;&#24212;&#29983;&#25104;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#23545;&#36739;&#22823;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#24179;&#21488;&#12290;&#36890;&#36807;&#24320;&#21457;&#20004;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;Penguin&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;(MRC)&#26159;&#23545;&#35805;&#20195;&#29702;&#30340;&#37325;&#35201;&#39046;&#22495;&#65292;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;MRC&#22522;&#20934;&#30340;&#19968;&#20010;&#26126;&#26174;&#38480;&#21046;&#26159;&#65306;&#26631;&#35760;&#30340;&#31572;&#26696;&#22823;&#22810;&#25968;&#26159;&#20174;&#30446;&#26631;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#30340;&#29255;&#27573;&#25110;&#32473;&#23450;&#20505;&#36873;&#39033;&#30340;&#36873;&#25321;&#65292;&#24573;&#30053;&#20102;&#39640;&#36136;&#37327;&#21709;&#24212;&#30340;&#33258;&#28982;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;MRC&#27169;&#22411;&#26080;&#27861;&#22312;&#30495;&#23454;&#30340;&#38382;&#31572;&#22330;&#26223;&#20013;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#21709;&#24212;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Penguin&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;MRC&#30740;&#31350;&#65292;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#25552;&#20379;&#33258;&#28982;&#21709;&#24212;&#29983;&#25104;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#22522;&#30784;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Penguin&#21253;&#21547;20&#19975;&#20010;&#35757;&#32451;&#25968;&#25454;&#65292;&#20855;&#22791;&#39640;&#36136;&#37327;&#12289;&#27969;&#30021;&#12289;&#20805;&#20998;&#20449;&#24687;&#30340;&#21709;&#24212;&#12290;Penguin&#26159;&#30456;&#23545;&#35268;&#27169;&#36739;&#22823;&#30340;&#20013;&#25991;MRC&#39046;&#22495;&#33258;&#28982;&#21709;&#24212;&#29983;&#25104;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#12290;&#20026;&#20102;&#35299;&#20915;Penguin&#20013;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#27169;&#22411;&#65306;&#31471;&#21040;&#31471;&#21644;&#20004;&#38454;&#27573;&#26694;&#26550;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20986;Prompt-BART&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine reading comprehension (MRC) is an important area of conversation agents and draws a lot of attention. However, there is a notable limitation to current MRC benchmarks: The labeled answers are mostly either spans extracted from the target corpus or the choices of the given candidates, ignoring the natural aspect of high-quality responses. As a result, MRC models trained on these datasets can not generate human-like responses in real QA scenarios. To this end, we construct a new dataset called Penguin to promote the research of MRC, providing a training and test bed for natural response generation to real scenarios. Concretely, Penguin consists of 200k training data with high-quality fluent, and well-informed responses. Penguin is the first benchmark towards natural response generation in Chinese MRC on a relatively large scale. To address the challenges in Penguin, we develop two strong baselines: end-to-end and two-stage frameworks. Following that, we further design Prompt-BART
&lt;/p&gt;</description></item><item><title>PK-ICR&#26159;&#19968;&#31181;&#22522;&#20110;&#35282;&#33394;&#21644;&#30693;&#35782;&#30340;&#20114;&#21160;&#19978;&#19979;&#25991;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#30340;&#22810;&#22330;&#26223;&#23545;&#35805;&#20013;&#21516;&#26102;&#35782;&#21035;&#35282;&#33394;&#21644;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#38382;&#31572;&#26816;&#32034;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#23454;&#29616;&#26816;&#32034;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#31354;-&#27491;&#21521;&#25490;&#21517;&#27979;&#35797;&#26041;&#27861;&#26469;&#25552;&#39640;&#25490;&#21517;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06674</link><description>&lt;p&gt;
PK-ICR: &#22522;&#20110;&#35282;&#33394;&#21644;&#30693;&#35782;&#30340;&#20114;&#21160;&#19978;&#19979;&#25991;&#26816;&#32034;&#36827;&#34892;&#22522;&#20110;&#22330;&#26223;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
PK-ICR: Persona-Knowledge Interactive Context Retrieval for Grounded Dialogue. (arXiv:2302.06674v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06674
&lt;/p&gt;
&lt;p&gt;
PK-ICR&#26159;&#19968;&#31181;&#22522;&#20110;&#35282;&#33394;&#21644;&#30693;&#35782;&#30340;&#20114;&#21160;&#19978;&#19979;&#25991;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#30340;&#22810;&#22330;&#26223;&#23545;&#35805;&#20013;&#21516;&#26102;&#35782;&#21035;&#35282;&#33394;&#21644;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#38382;&#31572;&#26816;&#32034;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#23454;&#29616;&#26816;&#32034;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#31354;-&#27491;&#21521;&#25490;&#21517;&#27979;&#35797;&#26041;&#27861;&#26469;&#25552;&#39640;&#25490;&#21517;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#21035;&#19982;&#23545;&#35805;&#31995;&#32479;&#30456;&#20851;&#30340;&#35282;&#33394;&#21644;&#30693;&#35782;&#23545;&#20110;&#22522;&#20110;&#22330;&#26223;&#30340;&#23545;&#35805;&#24212;&#31572;&#29983;&#25104;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27599;&#20010;&#23545;&#35805;&#22522;&#26412;&#19978;&#37117;&#26159;&#23396;&#31435;&#30740;&#31350;&#30340;&#65292;&#32780;&#26368;&#36817;&#30340;&#24037;&#20316;&#20013;&#24341;&#20837;&#20102;&#26356;&#23454;&#38469;&#30340;&#22810;&#22330;&#26223;&#23545;&#35805;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#35282;&#33394;&#21644;&#30693;&#35782;&#21452;&#19978;&#19979;&#25991;&#35782;&#21035;&#23450;&#20041;&#20026;&#20026;&#32473;&#23450;&#30340;&#23545;&#35805;&#21516;&#26102;&#35782;&#21035;&#35282;&#33394;&#21644;&#30693;&#35782;&#30340;&#20219;&#21153;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#22330;&#26223;&#23545;&#35805;&#35774;&#32622;&#20013;&#21487;&#33021;&#20855;&#26377;&#25552;&#21319;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#23545;&#35805;&#30340;&#25152;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#38382;&#31572;&#26816;&#32034;&#27169;&#22411;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31354;-&#27491;&#21521;&#25490;&#21517;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#19982;&#25968;&#25454;&#22686;&#24378;&#30456;&#20851;&#30340;&#35821;&#20041;&#24046;&#24322;&#26679;&#26412;&#65288;&#21363;&#22256;&#38590;&#36127;&#26679;&#26412;&#65289;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying relevant persona or knowledge for conversational systems is critical to grounded dialogue response generation. However, each grounding has been mostly researched in isolation with more practical multi-context dialogue tasks introduced in recent works. We define Persona and Knowledge Dual Context Identification as the task to identify persona and knowledge jointly for a given dialogue, which could be of elevated importance in complex multi-context dialogue settings. We develop a novel grounding retrieval method that utilizes all contexts of dialogue simultaneously. Our method requires less computational power via utilizing neural QA retrieval models. We further introduce our novel null-positive rank test which measures ranking performance on semantically dissimilar samples (i.e. hard negatives) in relation to data augmentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.04054</link><description>&lt;p&gt;
&#36861;&#27714;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#25512;&#29702;&#22797;&#29616;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#8212;&#8212;&#21363;&#22312;&#22797;&#21046;&#30340;&#27169;&#22411;&#35757;&#32451;&#36816;&#34892;&#20013;&#35266;&#23519;&#21040;&#30340;&#35780;&#20272;&#20998;&#25968;&#30340;&#19968;&#33268;&#24615;&#8212;&#8212;&#21463;&#21040;&#20960;&#31181;&#38750;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#34987;&#35270;&#20026;&#27979;&#37327;&#22122;&#22768;&#12290;&#30446;&#21069;&#30340;&#36235;&#21183;&#26159;&#21435;&#38500;&#22122;&#22768;&#65292;&#20197;&#24378;&#21046;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#24573;&#30053;&#20102;&#23454;&#29616;&#23618;&#38754;&#22266;&#26377;&#30340;&#38750;&#30830;&#23450;&#24615;&#20197;&#21450;&#31639;&#27861;&#22122;&#22768;&#22240;&#32032;&#21644;&#25968;&#25454;&#29305;&#24615;&#20043;&#38388;&#30340;&#20851;&#38190;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#12290;&#36825;&#38480;&#21046;&#20102;&#20174;&#36825;&#20123;&#23454;&#39564;&#20013;&#21487;&#20197;&#24471;&#20986;&#30340;&#32467;&#35770;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#23558;&#20960;&#20010;&#26041;&#24046;&#26469;&#28304;&#65292;&#21253;&#25324;&#23427;&#20204;&#19982;&#25968;&#25454;&#29305;&#24615;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#26174;&#33879;&#24615;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#20013;&#65292;&#20197;&#26399;&#20174;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#23450;&#23454;&#20363;&#24471;&#20986;&#25512;&#29702;&#32467;&#35770;, &#32780;&#38750;&#21435;&#38500;&#22122;&#22768;&#12290;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#20351;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#29992;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#24335;&#26469;&#32771;&#34385;&#31639;&#27861;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#22122;&#22768;&#26469;&#28304;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#21508;&#20010;&#26041;&#24046;&#26469;&#28304;&#23545;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability of machine learning evaluation -- the consistency of observed evaluation scores across replicated model training runs -- is affected by several sources of nondeterminism which can be regarded as measurement noise. Current tendencies to remove noise in order to enforce reproducibility of research results neglect inherent nondeterminism at the implementation level and disregard crucial interaction effects between algorithmic noise factors and data properties. This limits the scope of conclusions that can be drawn from such experiments. Instead of removing noise, we propose to incorporate several sources of variance, including their interaction with data properties, into an analysis of significance and reliability of machine learning evaluation, with the aim to draw inferences beyond particular instances of trained models. We show how to use linear mixed effects models (LMEMs) to analyze performance evaluation scores, and to conduct statistical inference with a generalized lik
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#30340;&#26465;&#20214;&#21477;&#24448;&#24448;&#23384;&#22312;&#30528;&#19981;&#19968;&#33268;&#24615;&#65292;&#26080;&#27861;&#20174;&#19968;&#20010;&#36830;&#36143;&#30340;&#32852;&#21512;&#20998;&#24067;&#20013;&#25512;&#23548;&#20986;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#21457;&#29616;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#26222;&#36941;&#23384;&#22312;&#20110;&#19981;&#21516;&#23610;&#23544;&#21644;&#37197;&#32622;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#21477;&#38598;&#21512;&#26041;&#27861;&#26469;&#22312;&#25512;&#26029;&#38454;&#27573;&#22788;&#29702;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.00068</link><description>&lt;p&gt;
&#20851;&#20110;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#26465;&#20214;&#21477;&#30340;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Inconsistencies of Conditionals Learned by Masked Language Models. (arXiv:2301.00068v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#30340;&#26465;&#20214;&#21477;&#24448;&#24448;&#23384;&#22312;&#30528;&#19981;&#19968;&#33268;&#24615;&#65292;&#26080;&#27861;&#20174;&#19968;&#20010;&#36830;&#36143;&#30340;&#32852;&#21512;&#20998;&#24067;&#20013;&#25512;&#23548;&#20986;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#21457;&#29616;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#26222;&#36941;&#23384;&#22312;&#20110;&#19981;&#21516;&#23610;&#23544;&#21644;&#37197;&#32622;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#21477;&#38598;&#21512;&#26041;&#27861;&#26469;&#22312;&#25512;&#26029;&#38454;&#27573;&#22788;&#29702;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#20102;&#22312;&#24207;&#21015;&#20013;&#23398;&#20064;&#39044;&#27979;&#36974;&#34109;&#26631;&#35760;&#26159;&#19968;&#20010;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#24456;&#26377;&#21147;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#35757;&#32451;&#21518;&#65292;&#36825;&#20123;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#22522;&#20110;&#21452;&#21521;&#19978;&#19979;&#25991;&#30340;&#26631;&#35760;&#20998;&#24067;&#12290;&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19982;&#24120;&#35265;&#20551;&#35774;&#30456;&#21453;&#65292;&#36825;&#31181;&#21452;&#21521;&#26465;&#20214;&#21477;&#32463;&#24120;&#34920;&#29616;&#20986;&#30456;&#24403;&#22823;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#22312;&#32771;&#34385;&#22312;&#19968;&#36215;&#26102;&#19981;&#33021;&#20174;&#19968;&#20010;&#36830;&#36143;&#30340;&#32852;&#21512;&#20998;&#24067;&#23548;&#20986;&#23427;&#20204;&#12290;&#25105;&#20204;&#22312;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#20004;&#31181;&#24120;&#35265;&#39118;&#26684;&#65288;T5&#39118;&#26684;&#21644;BERT&#39118;&#26684;&#65289;&#30340;&#31616;&#21333;&#21452;&#23383;&#27597;&#35789;&#27604;&#36739;&#22330;&#26223;&#20013;&#36890;&#36807;&#23454;&#35777;&#37327;&#21270;&#20102;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;T5&#27169;&#22411;&#32463;&#24120;&#28151;&#28102;&#33258;&#24049;&#23545;&#20004;&#20010;&#30456;&#20284;&#21452;&#23383;&#27597;&#35789;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19981;&#19968;&#33268;&#24615;&#22312;&#19981;&#21516;&#23610;&#23544;&#21644;&#37197;&#32622;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20174;RoBERTa-base&#21040;GLM-130B&#12290;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#21021;&#22987;&#23581;&#35797;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#21477;&#38598;&#21512;&#65292;&#22312;&#25512;&#26029;&#38454;&#27573;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to predict masked tokens in a sequence has been shown to be a powerful pretraining objective for large language models. After training, such masked language models can provide distributions of tokens conditioned on bidirectional context.  In this paper, we show that contrary to popular assumptions, such bidirectional conditionals often demonstrate considerable inconsistencies, i.e., they cannot be derived from a coherent joint distribution when considered together. We empirically quantify such inconsistencies in the simple scenario of bigram comparison for two common styles of masked language models: T5-style and BERT-style. For example, we show that T5 models often confuse their own preference regarding two similar bigrams. We show that inconsistencies exist ubiquitously in masked language models of diverse sizes and configurations, from RoBERTa-base to GLM-130B.  As an initial attempt to address this issue during the inference phase, we propose Ensemble of Conditionals, a se
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#30340;&#35821;&#20041;&#27969;&#31243;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#32858;&#21512;&#26041;&#27861;&#23545;&#25233;&#37057;&#30151;&#20005;&#37325;&#31243;&#24230;&#36827;&#34892;&#20272;&#35745;&#12290;&#22312;Reddit&#19978;&#30340;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;30&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.07624</link><description>&lt;p&gt;
&#29992;&#20110;&#25233;&#37057;&#30151;&#20005;&#37325;&#31243;&#24230;&#20272;&#35745;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Semantic Similarity Models for Depression Severity Estimation. (arXiv:2211.07624v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#30340;&#35821;&#20041;&#27969;&#31243;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#32858;&#21512;&#26041;&#27861;&#23545;&#25233;&#37057;&#30151;&#20005;&#37325;&#31243;&#24230;&#36827;&#34892;&#20272;&#35745;&#12290;&#22312;Reddit&#19978;&#30340;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;30&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#20840;&#29699;&#20005;&#37325;&#30340;&#20844;&#20849;&#20581;&#24247;&#38382;&#39064;&#65292;&#28982;&#32780;&#65292;&#20844;&#20849;&#21355;&#29983;&#31995;&#32479;&#22312;&#30149;&#20363;&#26816;&#27979;&#21644;&#35786;&#26029;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#24191;&#27867;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#33719;&#21462;&#22823;&#35268;&#27169;&#20844;&#20849;&#20449;&#24687;&#30340;&#36884;&#24452;&#12290;&#35745;&#31639;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#29992;&#25143;&#29983;&#25104;&#30340;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#20316;&#20026;&#24555;&#36895;&#31579;&#26597;&#30340;&#25903;&#25345;&#24037;&#20855;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35821;&#20041;&#27969;&#31243;&#65292;&#22522;&#20110;&#29992;&#25143;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#25991;&#23383;&#65292;&#30740;&#31350;&#20010;&#20307;&#30340;&#25233;&#37057;&#30151;&#20005;&#37325;&#31243;&#24230;&#12290;&#25105;&#20204;&#36873;&#25321;&#27979;&#35797;&#29992;&#25143;&#30340;&#21477;&#23376;&#65292;&#23545;&#24212;&#20110;&#25233;&#37057;&#30151;&#29366;&#21644;&#20005;&#37325;&#31243;&#24230;&#27700;&#24179;&#30340;&#20195;&#34920;&#24615;&#35757;&#32451;&#21477;&#23376;&#30340;&#32034;&#24341;&#65292;&#20135;&#29983;&#35821;&#20041;&#25490;&#21517;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#32467;&#26524;&#20013;&#30340;&#21477;&#23376;&#20316;&#20026;&#39044;&#27979;&#29992;&#25143;&#30151;&#29366;&#20005;&#37325;&#31243;&#24230;&#30340;&#35777;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#22238;&#31572;&#22235;&#20010;&#36125;&#20811;&#25233;&#37057;&#33258;&#35780;&#37327;&#34920;(BDI)&#36873;&#39033;&#20013;&#30340;&#27599;&#20010;&#30151;&#29366;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;Reddit&#30340;&#20004;&#20010;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;30&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depressive disorders constitute a severe public health issue worldwide. However, public health systems have limited capacity for case detection and diagnosis. In this regard, the widespread use of social media has opened up a way to access public information on a large scale. Computational methods can serve as support tools for rapid screening by exploiting this user-generated social media content. This paper presents an efficient semantic pipeline to study depression severity in individuals based on their social media writings. We select test user sentences for producing semantic rankings over an index of representative training sentences corresponding to depressive symptoms and severity levels. Then, we use the sentences from those results as evidence for predicting users' symptom severity. For that, we explore different aggregation methods to answer one of four Beck Depression Inventory (BDI) options per symptom. We evaluate our methods on two Reddit-based benchmarks, achieving 30\%
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#21704;&#21033;&#183;&#27874;&#29305;&#23545;&#35805;&#65288;HPD&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#23545;&#35805;&#20195;&#29702;&#21644;&#35282;&#33394;&#23545;&#40784;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#21704;&#21033;&#183;&#27874;&#29305;&#31995;&#21015;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#24182;&#27880;&#37322;&#20102;&#23545;&#35805;&#32972;&#26223;&#20449;&#24687;&#12289;&#35828;&#35805;&#32773;&#12289;&#35282;&#33394;&#20851;&#31995;&#21644;&#23646;&#24615;&#12290;&#36890;&#36807;&#22312;HPD&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21487;&#20197;&#25512;&#21160;&#23545;&#35805;&#20195;&#29702;&#30340;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#36890;&#29992;&#22522;&#20934;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#29305;&#23450;&#35282;&#33394;&#23545;&#40784;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.06869</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21704;&#21033;&#183;&#27874;&#29305;&#30456;&#36935;&#65306;&#29992;&#20110;&#19982;&#35282;&#33394;&#23545;&#40784;&#30340;&#21452;&#35821;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Meet Harry Potter: A Bilingual Dataset for Aligning Dialogue Agents with Characters. (arXiv:2211.06869v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06869
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#21704;&#21033;&#183;&#27874;&#29305;&#23545;&#35805;&#65288;HPD&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#23545;&#35805;&#20195;&#29702;&#21644;&#35282;&#33394;&#23545;&#40784;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#21704;&#21033;&#183;&#27874;&#29305;&#31995;&#21015;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#24182;&#27880;&#37322;&#20102;&#23545;&#35805;&#32972;&#26223;&#20449;&#24687;&#12289;&#35828;&#35805;&#32773;&#12289;&#35282;&#33394;&#20851;&#31995;&#21644;&#23646;&#24615;&#12290;&#36890;&#36807;&#22312;HPD&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21487;&#20197;&#25512;&#21160;&#23545;&#35805;&#20195;&#29702;&#30340;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#36890;&#29992;&#22522;&#20934;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#29305;&#23450;&#35282;&#33394;&#23545;&#40784;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20687;ChatGPT&#21644;GPT4&#36825;&#26679;&#30340;&#23545;&#35805;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#26500;&#24314;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20195;&#29702;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35282;&#33394;&#34920;&#29616;&#30340;&#22797;&#26434;&#24615;&#21644;&#32570;&#20047;&#20840;&#38754;&#30340;&#27880;&#37322;&#65292;&#23558;&#36825;&#20123;&#20195;&#29702;&#19982;&#29305;&#23450;&#35282;&#33394;&#25110;&#20010;&#20307;&#23545;&#40784;&#20173;&#28982;&#26159;&#19968;&#20010;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21704;&#21033;&#183;&#27874;&#29305;&#23545;&#35805;&#65288;HPD&#65289;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25512;&#21160;&#23545;&#35805;&#20195;&#29702;&#21644;&#35282;&#33394;&#23545;&#40784;&#30340;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#21704;&#21033;&#183;&#27874;&#29305;&#31995;&#21015;&#30340;&#25152;&#26377;&#23545;&#35805;&#22330;&#26223;&#65288;&#21253;&#25324;&#33521;&#25991;&#21644;&#20013;&#25991;&#65289;&#65292;&#24182;&#27880;&#37322;&#20102;&#37325;&#35201;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#21253;&#25324;&#23545;&#35805;&#22330;&#26223;&#12289;&#35828;&#35805;&#32773;&#12289;&#35282;&#33394;&#20851;&#31995;&#21644;&#23646;&#24615;&#12290;&#36825;&#20123;&#35814;&#32454;&#30340;&#27880;&#37322;&#21487;&#33021;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#22522;&#20110;&#35282;&#33394;&#30340;&#23545;&#35805;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#36890;&#29992;&#22522;&#20934;&#65292;&#35780;&#20272;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#29305;&#23450;&#35282;&#33394;&#23545;&#40784;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;HPD&#19978;&#20351;&#29992;&#32454;&#33268;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Dialogue-style Large Language Models (LLMs) such as ChatGPT and GPT4 have demonstrated immense potential in constructing open-domain dialogue agents. However, aligning these agents with specific characters or individuals remains a considerable challenge due to the complexities of character representation and the lack of comprehensive annotations. In this paper, we introduce the Harry Potter Dialogue (HPD) dataset, designed to advance the study of dialogue agents and character alignment. The dataset encompasses all dialogue sessions (in both English and Chinese) from the Harry Potter series and is annotated with vital background information, including dialogue scenes, speakers, character relationships, and attributes. These extensive annotations may empower LLMs to unlock character-driven dialogue capabilities. Furthermore, it can serve as a universal benchmark for evaluating how well can a LLM aligning with a specific character. We benchmark LLMs on HPD using both fine
&lt;/p&gt;</description></item><item><title>Small-Text&#26159;&#19968;&#20010;Python&#20013;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#20027;&#21160;&#23398;&#20064;&#24211;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;&#20808;&#36827;&#30340;&#26597;&#35810;&#31574;&#30053;&#21644;&#33879;&#21517;&#30340;&#26426;&#22120;&#23398;&#20064;&#24211;&#65292;&#25903;&#25345;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;&#35813;&#24211;&#35843;&#30740;&#20102;&#26368;&#26032;&#30340;SetFit&#35757;&#32451;&#33539;&#24335;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2107.10314</link><description>&lt;p&gt;
Small-Text: Python&#20013;&#30340;&#25991;&#26412;&#20998;&#31867;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Small-Text: Active Learning for Text Classification in Python. (arXiv:2107.10314v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.10314
&lt;/p&gt;
&lt;p&gt;
Small-Text&#26159;&#19968;&#20010;Python&#20013;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#20027;&#21160;&#23398;&#20064;&#24211;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;&#20808;&#36827;&#30340;&#26597;&#35810;&#31574;&#30053;&#21644;&#33879;&#21517;&#30340;&#26426;&#22120;&#23398;&#20064;&#24211;&#65292;&#25903;&#25345;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;&#35813;&#24211;&#35843;&#30740;&#20102;&#26368;&#26032;&#30340;SetFit&#35757;&#32451;&#33539;&#24335;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#20027;&#21160;&#23398;&#20064;&#24211;small-text&#65292;&#25552;&#20379;Python&#20013;&#30340;&#27744;&#24335;&#20027;&#21160;&#23398;&#20064;&#65292;&#29992;&#20110;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#23427;&#25552;&#20379;&#20102;&#35768;&#22810;&#39044;&#20808;&#23454;&#29616;&#30340;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#31574;&#30053;&#65292;&#21253;&#25324;&#19968;&#20123;&#21033;&#29992;GPU&#30340;&#31574;&#30053;&#12290;&#26631;&#20934;&#21270;&#30340;&#25509;&#21475;&#20801;&#35768;&#32452;&#21512;&#21508;&#31181;&#20998;&#31867;&#22120;&#12289;&#26597;&#35810;&#31574;&#30053;&#21644;&#20572;&#27490;&#20934;&#21017;&#65292;&#20415;&#20110;&#24555;&#36895;&#28151;&#25645;&#65292;&#26041;&#20415;&#24555;&#36895;&#24320;&#21457;&#20027;&#21160;&#23398;&#20064;&#23454;&#39564;&#21644;&#24212;&#29992;&#12290;&#20026;&#20102;&#20351;&#21508;&#31181;&#20998;&#31867;&#22120;&#21644;&#26597;&#35810;&#31574;&#30053;&#23545;&#20027;&#21160;&#23398;&#20064;&#21487;&#35775;&#38382;&#65292;small-text&#38598;&#25104;&#20102;&#20960;&#20010;&#33879;&#21517;&#30340;&#26426;&#22120;&#23398;&#20064;&#24211;&#65292;&#21253;&#25324;scikit-learn&#12289;PyTorch&#21644;Hugging Face transformers&#12290;&#21518;&#20004;&#20010;&#38598;&#25104;&#26159;&#21487;&#36873;&#23433;&#35013;&#30340;&#25193;&#23637;&#65292;&#22240;&#27492;&#21487;&#20197;&#20351;&#29992;GPU&#65292;&#20294;&#19981;&#26159;&#24517;&#38656;&#30340;&#12290;&#20351;&#29992;&#36825;&#20010;&#26032;&#24211;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#36817;&#21457;&#24067;&#30340;SetFit&#35757;&#32451;&#33539;&#24335;&#30340;&#24615;&#33021;&#65292;&#23558;&#20854;&#19982;&#26222;&#36890;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce small-text, an easy-to-use active learning library, which offers pool-based active learning for single- and multi-label text classification in Python. It features numerous pre-implemented state-of-the-art query strategies, including some that leverage the GPU. Standardized interfaces allow the combination of a variety of classifiers, query strategies, and stopping criteria, facilitating a quick mix and match, and enabling a rapid and convenient development of both active learning experiments and applications. With the objective of making various classifiers and query strategies accessible for active learning, small-text integrates several well-known machine learning libraries, namely scikit-learn, PyTorch, and Hugging Face transformers. The latter integrations are optionally installable extensions, so GPUs can be used but are not required. Using this new library, we investigate the performance of the recently published SetFit training paradigm, which we compare to vanilla 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24418;&#24335;&#21270;&#30340;&#23450;&#20041;&#65292;&#30740;&#31350;&#20102;&#21518;&#38376;&#25915;&#20987;&#23545;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#35774;&#35745;&#20102;&#38024;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#27979;&#35797;&#29983;&#25104;&#30446;&#26631;&#32473;&#23450;&#28304;&#30340;&#21453;&#21521;&#27010;&#29575;&#21487;&#20197;&#26377;&#25928;&#38450;&#24481;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#31561;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#8220;&#19968;&#23545;&#22810;&#8221;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2106.01810</link><description>&lt;p&gt;
&#38450;&#24481;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending Against Backdoor Attacks in Natural Language Generation. (arXiv:2106.01810v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.01810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24418;&#24335;&#21270;&#30340;&#23450;&#20041;&#65292;&#30740;&#31350;&#20102;&#21518;&#38376;&#25915;&#20987;&#23545;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#35774;&#35745;&#20102;&#38024;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#27979;&#35797;&#29983;&#25104;&#30446;&#26631;&#32473;&#23450;&#28304;&#30340;&#21453;&#21521;&#27010;&#29575;&#21487;&#20197;&#26377;&#25928;&#38450;&#24481;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#31561;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#8220;&#19968;&#23545;&#22810;&#8221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#20351;&#24471;&#24403;&#21069;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#26131;&#21463;&#21518;&#38376;&#25915;&#20987;&#65292;&#29983;&#25104;&#21487;&#33021;&#21253;&#21547;&#24615;&#21035;&#27495;&#35270;&#25110;&#20882;&#29359;&#24615;&#30340;&#24694;&#24847;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#21518;&#38376;&#25915;&#20987;&#22914;&#20309;&#24433;&#21709;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#20197;&#21450;&#22914;&#20309;&#38450;&#24481;&#36825;&#20123;&#25915;&#20987;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32473;&#20986;&#21518;&#38376;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#24418;&#24335;&#21270;&#23450;&#20041;&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#23545;&#35805;&#29983;&#25104;&#36825;&#20004;&#20010;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#22266;&#26377;&#29305;&#24615;&#65288;&#20363;&#22914;&#65292;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#36830;&#36143;&#21333;&#35789;&#24207;&#21015;&#65289;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#38024;&#23545;&#25915;&#20987;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#29983;&#25104;&#30446;&#26631;&#32473;&#23450;&#28304;&#30340;&#21453;&#21521;&#27010;&#29575;&#27979;&#35797;&#20013;&#65292;&#33021;&#22815;&#26377;&#25928;&#38450;&#24481;&#25152;&#26377;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#23545;&#35805;&#29983;&#25104;&#31561;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#8220;&#19968;&#23545;&#22810;&#8221;&#38382;&#39064;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#25552;&#39640;&#23545;&#21518;&#38376;&#25915;&#20987;&#39118;&#38505;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
The frustratingly fragile nature of neural network models make current natural language generation (NLG) systems prone to backdoor attacks and generate malicious sequences that could be sexist or offensive. Unfortunately, little effort has been invested to how backdoor attacks can affect current NLG models and how to defend against these attacks. In this work, by giving a formal definition of backdoor attack and defense, we investigate this problem on two important NLG tasks, machine translation and dialog generation. Tailored to the inherent nature of NLG models (e.g., producing a sequence of coherent words given contexts), we design defending strategies against attacks. We find that testing the backward probability of generating sources given targets yields effective defense performance against all different types of attacks, and is able to handle the {\it one-to-many} issue in many NLG tasks such as dialog generation. We hope that this work can raise the awareness of backdoor risks 
&lt;/p&gt;</description></item><item><title>GraphFormers&#26159;&#19968;&#31181;&#23558;GNN&#23884;&#22871;&#21040;Transformer&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#24335;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#20934;&#30830;&#29702;&#35299;&#25991;&#26412;&#22270;&#20013;&#27599;&#20010;&#33410;&#28857;&#30340;&#35821;&#20041;&#65292;&#21516;&#26102;&#24341;&#20837;&#28176;&#36827;&#24335;&#23398;&#20064;&#21152;&#36895;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2105.02605</link><description>&lt;p&gt;
GraphFormers: GNN&#23884;&#22871;Transformer&#29992;&#20110;&#25991;&#26412;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph. (arXiv:2105.02605v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.02605
&lt;/p&gt;
&lt;p&gt;
GraphFormers&#26159;&#19968;&#31181;&#23558;GNN&#23884;&#22871;&#21040;Transformer&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#24335;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#20934;&#30830;&#29702;&#35299;&#25991;&#26412;&#22270;&#20013;&#27599;&#20010;&#33410;&#28857;&#30340;&#35821;&#20041;&#65292;&#21516;&#26102;&#24341;&#20837;&#28176;&#36827;&#24335;&#23398;&#20064;&#21152;&#36895;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#26159;&#22522;&#20110;&#20010;&#20307;&#25991;&#26412;&#29305;&#24449;&#21644;&#37051;&#22495;&#20449;&#24687;&#29983;&#25104;&#33410;&#28857;&#20302;&#32500;&#23884;&#20837;&#30340;&#36807;&#31243;&#12290;&#26368;&#36817;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31361;&#30772;&#25512;&#21160;&#20102;&#30456;&#24212;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#20381;&#36182;&#32423;&#32852;&#27169;&#22411;&#26550;&#26500;&#65306;&#39318;&#20808;&#65292;&#33410;&#28857;&#30340;&#25991;&#26412;&#29305;&#24449;&#30001;&#35821;&#35328;&#27169;&#22411;&#29420;&#31435;&#32534;&#30721;&#65307;&#28982;&#21518;&#65292;&#25991;&#26412;&#23884;&#20837;&#30001;&#22270;&#31070;&#32463;&#32593;&#32476;&#32858;&#21512;&#12290;&#28982;&#32780;&#65292;&#19978;&#36848;&#26550;&#26500;&#30001;&#20110;&#23545;&#25991;&#26412;&#29305;&#24449;&#30340;&#29420;&#31435;&#24314;&#27169;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GraphFormers&#65292;&#20854;&#20013;GNN&#30340;&#20998;&#23618;&#32452;&#20214;&#23884;&#22871;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;Transformer&#22359;&#26049;&#36793;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26550;&#26500;&#65292;&#25991;&#26412;&#32534;&#30721;&#21644;&#22270;&#32858;&#21512;&#34701;&#21512;&#20026;&#19968;&#20010;&#36845;&#20195;&#24335;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#20174;&#20840;&#23616;&#35270;&#35282;&#20934;&#30830;&#29702;&#35299;&#27599;&#20010;&#33410;&#28857;&#30340;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#28176;&#36827;&#24335;&#23398;&#20064;&#26041;&#27861;&#34987;&#24341;&#20837;&#20197;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The representation learning on textual graph is to generate low-dimensional embeddings for the nodes based on the individual textual features and the neighbourhood information. Recent breakthroughs on pretrained language models and graph neural networks push forward the development of corresponding techniques. The existing works mainly rely on the cascaded model architecture: the textual features of nodes are independently encoded by language models at first; the textual embeddings are aggregated by graph neural networks afterwards. However, the above architecture is limited due to the independent modeling of textual features. In this work, we propose GraphFormers, where layerwise GNN components are nested alongside the transformer blocks of language models. With the proposed architecture, the text encoding and the graph aggregation are fused into an iterative workflow, {making} each node's semantic accurately comprehended from the global perspective. In addition, a {progressive} learn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#38469;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#32452;&#32455;&#30340;&#35745;&#31639;&#31995;&#32479;&#36801;&#31227;&#21040;&#19968;&#20010;&#21487;&#20197;&#27704;&#20037;&#28436;&#36827;&#19988;&#25972;&#21512;&#25972;&#20010;&#32452;&#32455;&#30340;&#26032;&#31995;&#32479;&#65292;&#24378;&#35843;&#27835;&#29702;&#21644;&#25216;&#26415;&#20004;&#20010;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2104.04844</link><description>&lt;p&gt;
&#20851;&#20110;&#36801;&#31227;&#21040;&#27704;&#32493;&#20225;&#19994;&#31995;&#32479;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On migration to Perpetual Enterprise System. (arXiv:2104.04844v4 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.04844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#38469;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#32452;&#32455;&#30340;&#35745;&#31639;&#31995;&#32479;&#36801;&#31227;&#21040;&#19968;&#20010;&#21487;&#20197;&#27704;&#20037;&#28436;&#36827;&#19988;&#25972;&#21512;&#25972;&#20010;&#32452;&#32455;&#30340;&#26032;&#31995;&#32479;&#65292;&#24378;&#35843;&#27835;&#29702;&#21644;&#25216;&#26415;&#20004;&#20010;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#38469;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#32452;&#32455;&#30340;&#35745;&#31639;&#31995;&#32479;&#36801;&#31227;&#21040;&#19968;&#20010;&#21487;&#20197;&#27704;&#20037;&#28436;&#36827;&#19988;&#25972;&#21512;&#25972;&#20010;&#32452;&#32455;&#30340;&#26032;&#31995;&#32479;&#12290;&#27835;&#29702;&#26041;&#38754;&#21644;&#32431;&#25216;&#26415;IT&#26041;&#38754;&#19968;&#26679;&#37325;&#35201;&#65292;&#29978;&#33267;&#26356;&#37325;&#35201;&#65306;&#20154;&#21147;&#36164;&#28304;&#12289;&#25307;&#26631;&#31561;&#12290;&#36801;&#31227;&#24847;&#21619;&#30528;&#19981;&#26159;&#20174;&#38646;&#24320;&#22987;&#12290;
&lt;/p&gt;
&lt;p&gt;
This document describes a pragmatic approach on how to migrate an organisation computer system towards a new system that could evolve forever, addresses the whole organisation and it is integrated.  Governance aspects are as important, if not more, than purely technical IT aspects: human resources, call for tenders, and similar. Migration implies that one is not starting from a green field.
&lt;/p&gt;</description></item></channel></rss>