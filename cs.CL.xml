<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#30740;&#31350;&#21457;&#29616;&#65292;LLaMa&#33021;&#22815;&#20197;&#31454;&#20105;&#24615;&#20934;&#30830;&#24615;&#21644;&#24615;&#21035;&#20559;&#24046;&#32531;&#35299;&#29983;&#25104;&#24615;&#21035;&#29305;&#23450;&#30340;&#32763;&#35793;&#65292;&#24182;&#22312;&#24615;&#21035;&#27169;&#31946;&#30340;&#24773;&#22659;&#20013;&#20445;&#25345;&#31283;&#20581;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03175</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Gender-specific Machine Translation with Large Language Models. (arXiv:2309.03175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03175
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#30740;&#31350;&#21457;&#29616;&#65292;LLaMa&#33021;&#22815;&#20197;&#31454;&#20105;&#24615;&#20934;&#30830;&#24615;&#21644;&#24615;&#21035;&#20559;&#24046;&#32531;&#35299;&#29983;&#25104;&#24615;&#21035;&#29305;&#23450;&#30340;&#32763;&#35793;&#65292;&#24182;&#22312;&#24615;&#21035;&#27169;&#31946;&#30340;&#24773;&#22659;&#20013;&#20445;&#25345;&#31283;&#20581;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#30721;&#22120;&#19987;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#28508;&#21147;&#65292;&#23613;&#31649;&#24615;&#33021;&#30053;&#20302;&#20110;&#20256;&#32479;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;LLM&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#21183;&#65306;&#36890;&#36807;&#25552;&#31034;&#25511;&#21046;&#36755;&#20986;&#30340;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#28789;&#27963;&#24615;&#26469;&#25506;&#32034;LLaMa&#22312;&#20855;&#26377;&#35821;&#27861;&#24615;&#21035;&#30340;&#35821;&#35328;&#20013;&#29983;&#25104;&#24615;&#21035;&#29305;&#23450;&#32763;&#35793;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#31181;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;NLLB&#30456;&#27604;&#65292;LLaMa&#21487;&#20197;&#20197;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#21035;&#20559;&#24046;&#32531;&#35299;&#29983;&#25104;&#24615;&#21035;&#29305;&#23450;&#30340;&#32763;&#35793;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLaMa&#30340;&#32763;&#35793;&#32467;&#26524;&#26159;&#31283;&#20581;&#30340;&#65292;&#22312;&#24615;&#21035;&#27169;&#31946;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#35780;&#20272;&#19982;&#30456;&#21453;&#24615;&#21035;&#21442;&#32771;&#32763;&#35793;&#26102;&#20250;&#20986;&#29616;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#65292;&#20294;&#22312;&#19981;&#22826;&#27169;&#31946;&#30340;&#19978;&#19979;&#25991;&#20013;&#20445;&#25345;&#19968;&#33268;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;LLM&#36827;&#34892;&#24615;&#21035;&#29305;&#23450;&#32763;&#35793;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoder-only Large Language Models (LLMs) have demonstrated potential in machine translation (MT), albeit with performance slightly lagging behind traditional encoder-decoder Neural Machine Translation (NMT) systems. However, LLMs offer a unique advantage: the ability to control the properties of the output through prompts. In this study, we harness this flexibility to explore LLaMa's capability to produce gender-specific translations for languages with grammatical gender. Our results indicate that LLaMa can generate gender-specific translations with competitive accuracy and gender bias mitigation when compared to NLLB, a state-of-the-art multilingual NMT system. Furthermore, our experiments reveal that LLaMa's translations are robust, showing significant performance drops when evaluated against opposite-gender references in gender-ambiguous datasets but maintaining consistency in less ambiguous contexts. This research provides insights into the potential and challenges of using LLMs f
&lt;/p&gt;</description></item><item><title>J-Guard&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;AI&#29983;&#25104;&#26032;&#38395;&#20013;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#38395;&#23646;&#24615;&#21644;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#21487;&#38752;&#24615;&#21644;&#35823;&#25253;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03164</link><description>&lt;p&gt;
J-Guard&#65306;&#26032;&#38395;&#25351;&#23548;&#30340;&#23545;&#25239;&#40065;&#26834;AI&#29983;&#25104;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
J-Guard: Journalism Guided Adversarially Robust Detection of AI-generated News. (arXiv:2309.03164v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03164
&lt;/p&gt;
&lt;p&gt;
J-Guard&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;AI&#29983;&#25104;&#26032;&#38395;&#20013;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#38395;&#23646;&#24615;&#21644;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#21487;&#38752;&#24615;&#21644;&#35823;&#25253;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#29983;&#25104;&#25991;&#26412;&#22312;&#32593;&#32476;&#19978;&#30340;&#36805;&#36895;&#25193;&#24352;&#27491;&#22312;&#28145;&#21051;&#22320;&#25913;&#21464;&#20449;&#24687;&#26684;&#23616;&#12290;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#20013;&#65292;AI&#29983;&#25104;&#26032;&#38395;&#20316;&#20026;&#19968;&#31181;&#26174;&#33879;&#30340;&#26469;&#28304;&#65292;&#23545;&#32593;&#32476;&#19978;&#30340;&#35823;&#23548;&#20449;&#24687;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#34429;&#28982;&#26368;&#36817;&#26377;&#19968;&#20123;&#21162;&#21147;&#33268;&#21147;&#20110;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#20294;&#32771;&#34385;&#21040;&#20854;&#38754;&#20020;&#30340;&#31616;&#21333;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#26356;&#39640;&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#26032;&#38395;&#20889;&#20316;&#30340;&#29305;&#24322;&#24615;&#65292;&#23558;&#36825;&#20123;&#26816;&#27979;&#26041;&#27861;&#24212;&#29992;&#20110;AI&#29983;&#25104;&#26032;&#38395;&#21487;&#33021;&#20250;&#20135;&#29983;&#35823;&#25253;&#65292;&#28508;&#22312;&#22320;&#30772;&#22351;&#26032;&#38395;&#26426;&#26500;&#30340;&#22768;&#35465;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#36328;&#23398;&#31185;&#22242;&#38431;&#30340;&#19987;&#38376;&#30693;&#35782;&#26469;&#24320;&#21457;&#19968;&#20010;&#21517;&#20026;J-Guard&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#24341;&#23548;&#29616;&#26377;&#30340;&#22522;&#20110;&#30417;&#30563;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#20197;&#26816;&#27979;AI&#29983;&#25104;&#26032;&#38395;&#65292;&#24182;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#21463;&#29420;&#29305;&#26032;&#38395;&#23646;&#24615;&#21551;&#21457;&#30340;&#25991;&#20307;&#26263;&#31034;&#65292;J-Guard&#33021;&#26377;&#25928;&#25269;&#21046;&#23545;&#25239;&#25915;&#20987;&#24182;&#38477;&#20302;&#35823;&#25253;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid proliferation of AI-generated text online is profoundly reshaping the information landscape. Among various types of AI-generated text, AI-generated news presents a significant threat as it can be a prominent source of misinformation online. While several recent efforts have focused on detecting AI-generated text in general, these methods require enhanced reliability, given concerns about their vulnerability to simple adversarial attacks. Furthermore, due to the eccentricities of news writing, applying these detection methods for AI-generated news can produce false positives, potentially damaging the reputation of news organizations. To address these challenges, we leverage the expertise of an interdisciplinary team to develop a framework, J-Guard, capable of steering existing supervised AI text detectors for detecting AI-generated news while boosting adversarial robustness. By incorporating stylistic cues inspired by the unique journalistic attributes, J-Guard effectively dis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23450;&#21046;&#21270;&#30340;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#39046;&#22495;&#29305;&#23450;&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#23450;&#21046;&#21270;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#26041;&#26696;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#24573;&#35270;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03126</link><description>&lt;p&gt;
&#27599;&#20010;&#20154;&#37117;&#24212;&#35813;&#24471;&#21040;&#22870;&#21169;&#65306;&#23398;&#20064;&#23450;&#21046;&#30340;&#20154;&#31867;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Everyone Deserves A Reward: Learning Customized Human Preferences. (arXiv:2309.03126v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03126
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23450;&#21046;&#21270;&#30340;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#39046;&#22495;&#29305;&#23450;&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#23450;&#21046;&#21270;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#26041;&#26696;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#24573;&#35270;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#27169;&#22411;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#20132;&#20114;&#36136;&#37327;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#26159;&#22810;&#20803;&#30340;&#65292;&#36825;&#23548;&#33268;&#20102;&#22522;&#20110;&#19981;&#21516;&#23447;&#25945;&#12289;&#25919;&#27835;&#12289;&#25991;&#21270;&#31561;&#30340;&#22810;&#26679;&#21270;&#20154;&#31867;&#20559;&#22909;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#20154;&#23545;&#21508;&#31181;&#20027;&#39064;&#37117;&#21487;&#20197;&#26377;&#33258;&#24049;&#29420;&#29305;&#30340;&#20559;&#22909;&#12290;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#24573;&#35270;&#20102;&#20154;&#31867;&#20559;&#22909;&#30340;&#22810;&#26679;&#24615;&#65292;&#21482;&#20351;&#29992;&#19968;&#20010;&#36890;&#29992;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#23450;&#21046;&#25110;&#20010;&#24615;&#21270;&#24212;&#29992;&#22330;&#26223;&#26469;&#35828;&#26159;&#19981;&#22815;&#28385;&#24847;&#30340;&#12290;&#20026;&#20102;&#25506;&#32034;&#23450;&#21046;&#21270;&#30340;&#20559;&#22909;&#23398;&#20064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25910;&#38598;&#20102;&#26469;&#33258;&#22235;&#20010;&#23454;&#38469;&#39046;&#22495;&#20013;&#23545;&#27599;&#20010;&#32473;&#23450;&#26597;&#35810;&#30340;&#39318;&#36873;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#20174;&#25968;&#25454;&#25928;&#29575;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#23450;&#21046;&#21270;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#26041;&#26696;&#65292;&#24182;&#22312;&#36890;&#29992;&#20559;&#22909;&#25968;&#25454;&#38598;&#21644;&#25105;&#20204;&#30340;&#39046;&#22495;&#29305;&#23450;&#20559;&#22909;&#25968;&#25454;&#38598;&#19978;&#23545;&#20854;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward models (RMs) are crucial in aligning large language models (LLMs) with human preferences for improving interaction quality. However, the real world is pluralistic, which leads to diversified human preferences based on different religions, politics, cultures, etc. Moreover, each individual can have their own unique preferences on various topics. Neglecting the diversity of human preferences, current LLM training processes only use a general reward model, which is below satisfaction for customized or personalized application scenarios. To explore customized preference learning, we collect a domain-specific preference (DSP) dataset, which collects preferred responses to each given query from four practical domains. Besides, from the perspective of data efficiency, we proposed a three-stage customized RM learning scheme, whose effectiveness is empirically verified on both general preference datasets and our DSP set. Furthermore, we test multiple training and data strategies on the t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Knowledge Solver&#65288;KSL&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25945;&#23548;LLMs&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#20013;&#25628;&#32034;&#20851;&#38190;&#30693;&#35782;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLMs&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03118</link><description>&lt;p&gt;
&#30693;&#35782;&#27714;&#35299;&#22120;&#65306;&#25945;&#25480;LLMs&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#25628;&#32034;&#39046;&#22495;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs. (arXiv:2309.03118v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Knowledge Solver&#65288;KSL&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25945;&#23548;LLMs&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#20013;&#25628;&#32034;&#20851;&#38190;&#30693;&#35782;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLMs&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#21644;GPT-4&#30001;&#20110;&#20854;&#26032;&#20852;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#32780;&#20855;&#26377;&#22810;&#21151;&#33021;&#24615;&#65292;&#21487;&#20197;&#35299;&#20915;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;LLMs&#26377;&#26102;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#26469;&#25191;&#34892;&#20219;&#21153;&#65292;&#36825;&#20063;&#20250;&#23548;&#33268;&#25512;&#29702;&#36807;&#31243;&#20013;&#20986;&#29616;&#34394;&#20551;&#20449;&#24687;&#12290;&#22312;&#19968;&#20123;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#39069;&#22806;&#30340;&#27169;&#22359;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34987;&#35757;&#32451;&#29992;&#20110;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#30693;&#35782;&#65292;&#26088;&#22312;&#32531;&#35299;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23558;&#39069;&#22806;&#30340;&#27169;&#22359;&#32435;&#20837;: 1&#65289;&#22312;&#36935;&#21040;&#26032;&#39046;&#22495;&#26102;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#39069;&#22806;&#30340;&#27169;&#22359;; 2&#65289;&#20250;&#25104;&#20026;&#29942;&#39048;&#65292;&#22240;&#20026;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20110;&#26816;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Knowledge Solver&#65288;KSL&#65289;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#33258;&#36523;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#65292;&#25945;&#23548;LLMs&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#20013;&#25628;&#32034;&#20851;&#38190;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25552;&#31034;&#26469;&#23558;&#26816;&#32034;&#36716;&#21270;&#20026;&#22810;&#36339;&#30340;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT and GPT-4, are versatile and can solve different tasks due to their emergent ability and generalizability. However, LLMs sometimes lack domain-specific knowledge to perform tasks, which would also cause hallucination during inference. In some previous works, additional modules like graph neural networks (GNNs) are trained on retrieved knowledge from external knowledge bases, aiming to mitigate the problem of lacking domain-specific knowledge. However, incorporating additional modules: 1) would need retraining additional modules when encountering novel domains; 2) would become a bottleneck since LLMs' strong abilities are not fully utilized for retrieval. In this paper, we propose a paradigm, termed Knowledge Solver (KSL), to teach LLMs to search for essential knowledge from external knowledge bases by harnessing their own strong generalizability. Specifically, we design a simple yet effective prompt to transform retrieval into a multi-hop d
&lt;/p&gt;</description></item><item><title>ContrastWSD&#26159;&#19968;&#31181;&#20351;&#29992;&#20102;&#35789;&#20041;&#28040;&#23696;&#30340;&#38544;&#21947;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#38544;&#21947;&#35782;&#21035;&#36807;&#31243;&#21644;&#35789;&#20041;&#28040;&#23696;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#21462;&#24182;&#23545;&#27604;&#21333;&#35789;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#21644;&#22522;&#26412;&#21547;&#20041;&#65292;&#20197;&#25552;&#39640;&#38544;&#21947;&#26816;&#27979;&#30340;&#25928;&#26524;&#65292;&#36229;&#36807;&#20854;&#20182;&#20165;&#20381;&#36182;&#19978;&#19979;&#25991;&#23884;&#20837;&#25110;&#38598;&#25104;&#22522;&#26412;&#23450;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03103</link><description>&lt;p&gt;
ContrastWSD: &#20351;&#29992;&#35789;&#20041;&#28040;&#23696;&#21152;&#24378;&#38544;&#21947;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ContrastWSD: Enhancing Metaphor Detection with Word Sense Disambiguation Following the Metaphor Identification Procedure. (arXiv:2309.03103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03103
&lt;/p&gt;
&lt;p&gt;
ContrastWSD&#26159;&#19968;&#31181;&#20351;&#29992;&#20102;&#35789;&#20041;&#28040;&#23696;&#30340;&#38544;&#21947;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#38544;&#21947;&#35782;&#21035;&#36807;&#31243;&#21644;&#35789;&#20041;&#28040;&#23696;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#21462;&#24182;&#23545;&#27604;&#21333;&#35789;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#21644;&#22522;&#26412;&#21547;&#20041;&#65292;&#20197;&#25552;&#39640;&#38544;&#21947;&#26816;&#27979;&#30340;&#25928;&#26524;&#65292;&#36229;&#36807;&#20854;&#20182;&#20165;&#20381;&#36182;&#19978;&#19979;&#25991;&#23884;&#20837;&#25110;&#38598;&#25104;&#22522;&#26412;&#23450;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ContrastWSD&#65292;&#19968;&#31181;&#22522;&#20110;RoBERTa&#30340;&#38544;&#21947;&#26816;&#27979;&#27169;&#22411;&#65292;&#23427;&#38598;&#25104;&#20102;&#38544;&#21947;&#35782;&#21035;&#36807;&#31243;(MIP)&#21644;&#35789;&#20041;&#28040;&#23696;(WSD)&#26469;&#25552;&#21462;&#24182;&#23545;&#27604;&#21333;&#35789;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#21644;&#22522;&#26412;&#21547;&#20041;&#65292;&#20197;&#30830;&#23450;&#23427;&#22312;&#21477;&#23376;&#20013;&#26159;&#21542;&#20197;&#38544;&#21947;&#30340;&#26041;&#24335;&#20351;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;WSD&#27169;&#22411;&#24471;&#20986;&#30340;&#21333;&#35789;&#35789;&#20041;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22686;&#24378;&#20102;&#38544;&#21947;&#26816;&#27979;&#36807;&#31243;&#65292;&#24182;&#36229;&#36807;&#20102;&#20165;&#20381;&#36182;&#19978;&#19979;&#25991;&#23884;&#20837;&#25110;&#20165;&#38598;&#25104;&#22522;&#26412;&#23450;&#20041;&#21644;&#20854;&#20182;&#22806;&#37096;&#30693;&#35782;&#30340;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#24378;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#25512;&#36827;&#38544;&#21947;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents ContrastWSD, a RoBERTa-based metaphor detection model that integrates the Metaphor Identification Procedure (MIP) and Word Sense Disambiguation (WSD) to extract and contrast the contextual meaning with the basic meaning of a word to determine whether it is used metaphorically in a sentence. By utilizing the word senses derived from a WSD model, our model enhances the metaphor detection process and outperforms other methods that rely solely on contextual embeddings or integrate only the basic definitions and other external knowledge. We evaluate our approach on various benchmark datasets and compare it with strong baselines, indicating the effectiveness in advancing metaphor detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#32929;&#31080;&#24066;&#22330;&#19978;&#19978;&#24066;&#20844;&#21496;&#30340;&#24180;&#24230;&#25253;&#21578;&#65292;&#29983;&#25104;&#27934;&#23519;&#65292;&#24182;&#36890;&#36807;&#21382;&#21490;&#32929;&#20215;&#25968;&#25454;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21462;&#24471;&#30456;&#23545;&#26631;&#26222;500&#25351;&#25968;&#30340;&#36229;&#39069;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2309.03079</link><description>&lt;p&gt;
GPT-InvestAR: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#32929;&#31080;&#25237;&#36164;&#31574;&#30053;&#36890;&#36807;&#24180;&#24230;&#25253;&#21578;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
GPT-InvestAR: Enhancing Stock Investment Strategies through Annual Report Analysis with Large Language Models. (arXiv:2309.03079v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#32929;&#31080;&#24066;&#22330;&#19978;&#19978;&#24066;&#20844;&#21496;&#30340;&#24180;&#24230;&#25253;&#21578;&#65292;&#29983;&#25104;&#27934;&#23519;&#65292;&#24182;&#36890;&#36807;&#21382;&#21490;&#32929;&#20215;&#25968;&#25454;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21462;&#24471;&#30456;&#23545;&#26631;&#26222;500&#25351;&#25968;&#30340;&#36229;&#39069;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#24066;&#20844;&#21496;&#30340;&#24180;&#24230;&#25253;&#21578;&#21253;&#21547;&#20102;&#20851;&#20110;&#20854;&#36130;&#21153;&#29366;&#20917;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#21487;&#20197;&#24110;&#21161;&#35780;&#20272;&#20854;&#23545;&#32929;&#31080;&#20215;&#26684;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#36825;&#20123;&#25253;&#21578;&#30340;&#20869;&#23481;&#38750;&#24120;&#20840;&#38754;&#65292;&#26377;&#26102;&#29978;&#33267;&#36229;&#36807;100&#39029;&#12290;&#21363;&#20351;&#23545;&#20110;&#19968;&#20010;&#20844;&#21496;&#26469;&#35828;&#65292;&#20998;&#26512;&#36825;&#20123;&#25253;&#21578;&#20063;&#26159;&#19968;&#39033;&#32321;&#29712;&#30340;&#24037;&#20316;&#65292;&#26356;&#19981;&#29992;&#35828;&#25972;&#20010;&#20844;&#21496;&#32676;&#20307;&#20102;&#12290;&#22810;&#24180;&#26469;&#65292;&#37329;&#34701;&#19987;&#23478;&#24050;&#32463;&#33021;&#22815;&#30456;&#23545;&#24555;&#36895;&#22320;&#20174;&#36825;&#20123;&#25991;&#20214;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22810;&#24180;&#30340;&#23454;&#36341;&#21644;&#32463;&#39564;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#31616;&#21270;&#23545;&#25152;&#26377;&#20844;&#21496;&#24180;&#24230;&#25253;&#21578;&#30340;&#35780;&#20272;&#36807;&#31243;&#12290;LLM&#29983;&#25104;&#30340;&#27934;&#23519;&#21147;&#34987;&#27719;&#32534;&#22312;&#19968;&#20010;&#37327;&#21270;&#39118;&#26684;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#24182;&#36890;&#36807;&#21382;&#21490;&#32929;&#20215;&#25968;&#25454;&#36827;&#34892;&#34917;&#20805;&#12290;&#28982;&#21518;&#20351;&#29992;LLM&#36755;&#20986;&#20316;&#20026;&#29305;&#24449;&#35757;&#32451;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#21069;&#21521;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#30456;&#23545;&#20110;&#26631;&#26222;500&#25351;&#25968;&#33719;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#36229;&#39069;&#22238;&#25253;&#12290;&#26412;&#25991;&#26088;&#22312;
&lt;/p&gt;
&lt;p&gt;
Annual Reports of publicly listed companies contain vital information about their financial health which can help assess the potential impact on Stock price of the firm. These reports are comprehensive in nature, going up to, and sometimes exceeding, 100 pages. Analysing these reports is cumbersome even for a single firm, let alone the whole universe of firms that exist. Over the years, financial experts have become proficient in extracting valuable information from these documents relatively quickly. However, this requires years of practice and experience. This paper aims to simplify the process of assessing Annual Reports of all the firms by leveraging the capabilities of Large Language Models (LLMs). The insights generated by the LLM are compiled in a Quant styled dataset and augmented by historical stock price data. A Machine Learning model is then trained with LLM outputs as features. The walkforward test results show promising outperformance wrt S&amp;P500 returns. This paper intends
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;Twitter&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#23454;&#39564;&#20102;&#22810;&#31181;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#33258;&#21160;&#26816;&#27979;&#24433;&#21709;&#32773;&#20869;&#23481;&#20013;&#30340;&#21830;&#19994;&#25512;&#24191;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2309.03064</link><description>&lt;p&gt;
&#22312;Twitter&#19978;&#23545;&#24433;&#21709;&#32773;&#20869;&#23481;&#30340;&#22810;&#27169;&#24577;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Multimodal Analysis of Influencer Content on Twitter. (arXiv:2309.03064v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;Twitter&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#23454;&#39564;&#20102;&#22810;&#31181;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#33258;&#21160;&#26816;&#27979;&#24433;&#21709;&#32773;&#20869;&#23481;&#20013;&#30340;&#21830;&#19994;&#25512;&#24191;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#21709;&#32773;&#33829;&#38144;&#28041;&#21450;&#19968;&#31995;&#21015;&#30340;&#31574;&#30053;&#65292;&#21697;&#29260;&#19982;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#21019;&#20316;&#32773;&#65288;&#21363;&#24433;&#21709;&#32773;&#65289;&#21512;&#20316;&#65292;&#21033;&#29992;&#20182;&#20204;&#30340;&#24433;&#21709;&#21147;&#12289;&#20449;&#20219;&#24230;&#21644;&#23545;&#20182;&#20204;&#30340;&#21463;&#20247;&#30340;&#24433;&#21709;&#21147;&#65292;&#25512;&#24191;&#21644;&#32972;&#20070;&#20135;&#21697;&#25110;&#26381;&#21153;&#12290;&#30001;&#20110;&#24433;&#21709;&#32773;&#30340;&#31881;&#19997;&#22312;&#25509;&#25910;&#21040;&#30495;&#23454;&#30340;&#20135;&#21697;&#35748;&#21487;&#21518;&#26356;&#26377;&#21487;&#33021;&#36141;&#20080;&#20135;&#21697;&#65292;&#32780;&#19981;&#26159;&#26126;&#30830;&#30340;&#30452;&#25509;&#20135;&#21697;&#25512;&#24191;&#65292;&#20010;&#20154;&#35266;&#28857;&#19982;&#21830;&#19994;&#20869;&#23481;&#25512;&#24191;&#20043;&#38388;&#30340;&#30028;&#38480;&#32463;&#24120;&#27169;&#31946;&#12290;&#36825;&#20351;&#24471;&#33258;&#21160;&#26816;&#27979;&#19982;&#24433;&#21709;&#32773;&#24191;&#21578;&#30456;&#20851;&#30340;&#30417;&#31649;&#21512;&#35268;&#36829;&#35268;&#34892;&#20026;&#65288;&#20363;&#22914;&#35823;&#23548;&#24615;&#24191;&#21578;&#25110;&#38544;&#34255;&#36190;&#21161;&#65289;&#23588;&#20026;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#65288;1&#65289;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;Twitter&#65288;&#29616;&#22312;&#26159;X&#65289;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;15,998&#20010;&#24433;&#21709;&#32773;&#30340;&#24086;&#23376;&#65292;&#20998;&#20026;&#21830;&#19994;&#21644;&#38750;&#21830;&#19994;&#31867;&#21035;&#65292;&#20197;&#21327;&#21161;&#33258;&#21160;&#26816;&#27979;&#21830;&#19994;&#24433;&#21709;&#32773;&#20869;&#23481;&#65307;&#65288;2&#65289;&#23581;&#35797;&#20102;&#19968;&#31995;&#21015;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#39044;&#27979;&#27169;&#22411;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influencer marketing involves a wide range of strategies in which brands collaborate with popular content creators (i.e., influencers) to leverage their reach, trust, and impact on their audience to promote and endorse products or services. Because followers of influencers are more likely to buy a product after receiving an authentic product endorsement rather than an explicit direct product promotion, the line between personal opinions and commercial content promotion is frequently blurred. This makes automatic detection of regulatory compliance breaches related to influencer advertising (e.g., misleading advertising or hidden sponsorships) particularly difficult. In this work, we (1) introduce a new Twitter (now X) dataset consisting of 15,998 influencer posts mapped into commercial and non-commercial categories for assisting in the automatic detection of commercial influencer content; (2) experiment with an extensive set of predictive models that combine text and visual information 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#30340;&#20154;&#29289;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;PARADOX&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#20110;&#30495;&#23454;&#20010;&#20307;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#30340;&#25991;&#26412;&#12290;&#35813;&#27169;&#22411;&#20197;&#29992;&#25143;&#30340;&#20154;&#29289;&#24418;&#35937;&#20026;&#26465;&#20214;&#26469;&#32534;&#30721;&#23545;&#35805;&#65292;&#24182;&#29983;&#25104;&#19981;&#24102;&#21333;&#35821;&#21442;&#32771;&#25968;&#25454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#27169;&#22411;&#36824;&#36827;&#34892;&#23545;&#40784;&#65292;&#20351;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#25509;&#36817;&#30495;&#23454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35821;&#20041;&#19978;&#26356;&#26377;&#24847;&#20041;&#65292;&#22312;&#35821;&#35328;&#19978;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2309.02915</link><description>&lt;p&gt;
&#12298;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#30340;&#20154;&#29289;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;&#12299;
&lt;/p&gt;
&lt;p&gt;
Persona-aware Generative Model for Code-mixed Language. (arXiv:2309.02915v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#30340;&#20154;&#29289;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;PARADOX&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#20110;&#30495;&#23454;&#20010;&#20307;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#30340;&#25991;&#26412;&#12290;&#35813;&#27169;&#22411;&#20197;&#29992;&#25143;&#30340;&#20154;&#29289;&#24418;&#35937;&#20026;&#26465;&#20214;&#26469;&#32534;&#30721;&#23545;&#35805;&#65292;&#24182;&#29983;&#25104;&#19981;&#24102;&#21333;&#35821;&#21442;&#32771;&#25968;&#25454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#27169;&#22411;&#36824;&#36827;&#34892;&#23545;&#40784;&#65292;&#20351;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#25509;&#36817;&#30495;&#23454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35821;&#20041;&#19978;&#26356;&#26377;&#24847;&#20041;&#65292;&#22312;&#35821;&#35328;&#19978;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#21644;&#22810;&#35821;&#35328;&#31038;&#20250;&#20013;&#65292;&#20195;&#30721;&#28151;&#21512;&#21644;&#33050;&#26412;&#28151;&#21512;&#38750;&#24120;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#23545;&#20110;&#20195;&#30721;&#28151;&#21512;&#30340;&#20559;&#22909;&#21462;&#20915;&#20110;&#29992;&#25143;&#30340;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#12289;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#21644;&#24403;&#22320;&#29615;&#22659;&#65292;&#32780;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#26102;&#22823;&#22810;&#24573;&#35270;&#20102;&#36825;&#20123;&#22240;&#32032;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#24320;&#21457;&#19968;&#31181;&#20154;&#29289;&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#31867;&#20284;&#20110;&#30495;&#23454;&#20010;&#20307;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20195;&#30721;&#28151;&#21512;&#29983;&#25104;&#30340;&#20154;&#29289;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;&#65288;PARADOX&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#32473;&#23450;&#29992;&#25143;&#30340;&#20154;&#29289;&#24418;&#35937;&#30340;&#26465;&#20214;&#19979;&#23545;&#35805;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#29983;&#25104;&#19981;&#24102;&#21333;&#35821;&#21442;&#32771;&#25968;&#25454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#40784;&#27169;&#22359;&#65292;&#23545;&#29983;&#25104;&#30340;&#24207;&#21015;&#36827;&#34892;&#37325;&#26032;&#26657;&#20934;&#65292;&#20351;&#20854;&#26356;&#25509;&#36817;&#30495;&#23454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;PARADOX&#29983;&#25104;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#22312;&#35821;&#20041;&#19978;&#26356;&#26377;&#24847;&#20041;&#65292;&#22312;&#35821;&#35328;&#19978;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code-mixing and script-mixing are prevalent across online social networks and multilingual societies. However, a user's preference toward code-mixing depends on the socioeconomic status, demographics of the user, and the local context, which existing generative models mostly ignore while generating code-mixed texts. In this work, we make a pioneering attempt to develop a persona-aware generative model to generate texts resembling real-life code-mixed texts of individuals. We propose a Persona-aware Generative Model for Code-mixed Generation, PARADOX, a novel Transformer-based encoder-decoder model that encodes an utterance conditioned on a user's persona and generates code-mixed texts without monolingual reference data. We propose an alignment module that re-calibrates the generated sequence to resemble real-life code-mixed texts. PARADOX generates code-mixed texts that are semantically more meaningful and linguistically more valid. To evaluate the personification capabilities of PARAD
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#32463;&#36807;&#27880;&#37322;&#30340;&#36164;&#28304;&#65292;&#23545;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24037;&#20855;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22320;&#29702;&#32534;&#30721;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#20154;&#36947;&#20027;&#20041;&#25991;&#26723;&#30340;&#22320;&#29702;&#23450;&#20301;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#20998;&#31867;&#22120;&#24615;&#33021;&#65292;&#20943;&#36731;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#20559;&#35265;&#65292;&#23545;&#35199;&#26041;&#20197;&#22806;&#30340;&#36164;&#28304;&#38656;&#27714;&#26356;&#22823;&#12290;</title><link>http://arxiv.org/abs/2309.02914</link><description>&lt;p&gt;
&#26080;&#25152;&#36951;&#28431;&#65306;&#25913;&#36827;&#30340;&#20154;&#36947;&#20027;&#20041;&#25991;&#26723;&#22320;&#29702;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Leave no Place Behind: Improved Geolocation in Humanitarian Documents. (arXiv:2309.02914v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#32463;&#36807;&#27880;&#37322;&#30340;&#36164;&#28304;&#65292;&#23545;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24037;&#20855;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22320;&#29702;&#32534;&#30721;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#20154;&#36947;&#20027;&#20041;&#25991;&#26723;&#30340;&#22320;&#29702;&#23450;&#20301;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#20998;&#31867;&#22120;&#24615;&#33021;&#65292;&#20943;&#36731;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#20559;&#35265;&#65292;&#23545;&#35199;&#26041;&#20197;&#22806;&#30340;&#36164;&#28304;&#38656;&#27714;&#26356;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#20301;&#32622;&#26159;&#20154;&#36947;&#20027;&#20041;&#24212;&#23545;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#25551;&#36848;&#20102;&#33030;&#24369;&#20154;&#21475;&#12289;&#27491;&#22312;&#21457;&#29983;&#30340;&#20107;&#20214;&#21644;&#21487;&#29992;&#36164;&#28304;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21487;&#20197;&#24110;&#21161;&#20174;&#20154;&#36947;&#37096;&#38376;&#20135;&#29983;&#30340;&#25253;&#21578;&#21644;&#25991;&#26723;&#30340;&#27867;&#28389;&#20013;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#20449;&#24687;&#25552;&#21462;&#24037;&#20855;&#30340;&#24615;&#33021;&#21644;&#20559;&#35265;&#26159;&#26410;&#30693;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#32463;&#36807;&#27880;&#37322;&#30340;&#36164;&#28304;&#65292;&#23545;&#27969;&#34892;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#24037;&#20855;Spacy&#21644;roBERTa&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25191;&#34892;&#20154;&#36947;&#20027;&#20041;&#25991;&#26412;&#30340;&#22320;&#29702;&#26631;&#35760;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22320;&#29702;&#32534;&#30721;&#26041;&#27861;FeatureRank&#65292;&#23558;&#20505;&#36873;&#20301;&#32622;&#38142;&#25509;&#21040;GeoNames&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20154;&#36947;&#20027;&#20041;&#39046;&#22495;&#30340;&#25968;&#25454;&#19981;&#20165;&#25913;&#21892;&#20102;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65288;F1 = 0.92&#65289;&#65292;&#32780;&#19988;&#36824;&#20943;&#36731;&#20102;&#29616;&#26377;&#24037;&#20855;&#30340;&#20559;&#35265;&#65292;&#36825;&#20123;&#24037;&#20855;&#38169;&#35823;&#22320;&#20559;&#21521;&#20110;&#35199;&#26041;&#22269;&#23478;&#30340;&#20301;&#32622;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#38656;&#35201;&#26356;&#22810;&#26469;&#33258;&#38750;&#35199;&#26041;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geographical location is a crucial element of humanitarian response, outlining vulnerable populations, ongoing events, and available resources. Latest developments in Natural Language Processing may help in extracting vital information from the deluge of reports and documents produced by the humanitarian sector. However, the performance and biases of existing state-of-the-art information extraction tools are unknown. In this work, we develop annotated resources to fine-tune the popular Named Entity Recognition (NER) tools Spacy and roBERTa to perform geotagging of humanitarian texts. We then propose a geocoding method FeatureRank which links the candidate locations to the GeoNames database. We find that not only does the humanitarian-domain data improves the performance of the classifiers (up to F1 = 0.92), but it also alleviates some of the bias of the existing tools, which erroneously favor locations in the Western countries. Thus, we conclude that more resources from non-Western doc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#26469;&#25351;&#23548;&#26410;&#26469;&#26500;&#24314;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2309.02912</link><description>&lt;p&gt;
&#20851;&#20110;&#26500;&#24314;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
On the Challenges of Building Datasets for Hate Speech Detection. (arXiv:2309.02912v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#26469;&#25351;&#23548;&#26410;&#26469;&#26500;&#24314;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#35328;&#35770;&#30340;&#26816;&#27979;&#34987;&#25552;&#20986;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19968;&#20010;&#29420;&#31435;&#24212;&#29992;&#65292;&#24182;&#37319;&#29992;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#30446;&#26631;&#32676;&#20307;&#12289;&#33719;&#21462;&#21407;&#22987;&#25968;&#25454;&#12289;&#23450;&#20041;&#26631;&#35760;&#36807;&#31243;&#12289;&#36873;&#25321;&#26816;&#27979;&#31639;&#27861;&#20197;&#21450;&#35780;&#20272;&#25152;&#38656;&#29615;&#22659;&#19979;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#19981;&#21516;&#65292;&#30001;&#20110;&#20219;&#21153;&#30340;&#39640;&#24230;&#20027;&#35266;&#24615;&#65292;&#20167;&#24680;&#35328;&#35770;&#30340;&#25968;&#25454;&#38598;&#32570;&#20047;&#22823;&#35268;&#27169;&#12289;&#31934;&#24515;&#31579;&#36873;&#12289;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#29305;&#28857;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#35270;&#35282;&#39318;&#20808;&#20998;&#26512;&#20102;&#22260;&#32469;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#26694;&#26550;&#65292;&#20197;&#20167;&#24680;&#35328;&#35770;&#23545;&#24615;&#23569;&#25968;&#32676;&#20307;&#30340;&#29305;&#23450;&#31034;&#20363;&#20026;&#20363;&#65292;&#27010;&#25324;&#20102;&#28085;&#30422;&#19971;&#20010;&#26041;&#38754;&#30340;&#25968;&#25454;&#21019;&#24314;&#27969;&#31243;&#12290;&#25105;&#20204;&#35748;&#20026;&#20174;&#26410;&#26469;&#26500;&#24314;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#23454;&#36341;&#35282;&#24230;&#20986;&#21457;&#65292;&#20174;&#19994;&#32773;&#23558;&#21463;&#30410;&#20110;&#36981;&#24490;&#36825;&#19968;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection of hate speech has been formulated as a standalone application of NLP and different approaches have been adopted for identifying the target groups, obtaining raw data, defining the labeling process, choosing the detection algorithm, and evaluating the performance in the desired setting. However, unlike other downstream tasks, hate speech suffers from the lack of large-sized, carefully curated, generalizable datasets owing to the highly subjective nature of the task. In this paper, we first analyze the issues surrounding hate speech detection through a data-centric lens. We then outline a holistic framework to encapsulate the data creation pipeline across seven broad dimensions by taking the specific example of hate speech towards sexual minorities. We posit that practitioners would benefit from following this framework as a form of best practice when creating hate speech datasets in the future.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#21270;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#36234;&#21335;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.02902</link><description>&lt;p&gt;
ViCGCN: &#22522;&#20110;&#19978;&#19979;&#25991;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20132;&#23186;&#20307;&#25366;&#25496;&#20013;&#25991;&#26412;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ViCGCN: Graph Convolutional Network with Contextualized Language Models for Social Media Mining in Vietnamese. (arXiv:2309.02902v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#21270;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#36234;&#21335;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#22788;&#29702;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#20855;&#26377;&#20247;&#22810;&#30340;&#24212;&#29992;&#12290;&#38543;&#30528;&#36234;&#21335;&#31038;&#20132;&#23186;&#20307;&#21644;&#20449;&#24687;&#31185;&#23398;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23545;&#36234;&#21335;&#31038;&#20132;&#23186;&#20307;&#36827;&#34892;&#22522;&#20110;&#20449;&#24687;&#30340;&#25366;&#25496;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#38754;&#20020;&#30528;&#19968;&#20123;&#37325;&#35201;&#30340;&#32570;&#28857;&#65292;&#21253;&#25324;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#22122;&#22768;&#25968;&#25454;&#12290;&#22312;&#36234;&#21335;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#65292;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#22122;&#22768;&#26159;&#38656;&#35201;&#35299;&#20915;&#30340;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22270;&#21367;&#31215;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#26469;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#22122;&#22768;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;PhoBERT&#65289;&#21644;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65288;&#22270;&#21367;&#31215;&#32593;&#32476;&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;ViCGCN&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#33021;&#21147;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#33021;&#21147;&#65292;&#25429;&#25417;&#36234;&#21335;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media processing is a fundamental task in natural language processing with numerous applications. As Vietnamese social media and information science have grown rapidly, the necessity of information-based mining on Vietnamese social media has become crucial. However, state-of-the-art research faces several significant drawbacks, including imbalanced data and noisy data on social media platforms. Imbalanced and noisy are two essential issues that need to be addressed in Vietnamese social media texts. Graph Convolutional Networks can address the problems of imbalanced and noisy data in text classification on social media by taking advantage of the graph structure of the data. This study presents a novel approach based on contextualized language model (PhoBERT) and graph-based method (Graph Convolutional Networks). In particular, the proposed approach, ViCGCN, jointly trained the power of Contextualized embeddings with the ability of Graph Convolutional Networks, GCN, to capture mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#30446;&#26631;&#35821;&#35328;&#21477;&#23376;&#23545;&#20043;&#38388;&#25512;&#29702;&#20851;&#31995;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#29305;&#23450;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#36890;&#29992;&#30340;&#32763;&#35793;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23454;&#20363;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#36890;&#29992;&#24615;&#65292;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.02887</link><description>&lt;p&gt;
&#19968;&#31181;&#27809;&#26377;&#35821;&#35328;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#30340;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
A deep Natural Language Inference predictor without language-specific training data. (arXiv:2309.02887v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#30446;&#26631;&#35821;&#35328;&#21477;&#23376;&#23545;&#20043;&#38388;&#25512;&#29702;&#20851;&#31995;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#29305;&#23450;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#36890;&#29992;&#30340;&#32763;&#35793;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23454;&#20363;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#36890;&#29992;&#24615;&#65292;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#30446;&#26631;&#35821;&#35328;&#21477;&#23376;&#23545;&#20043;&#38388;&#25512;&#29702;&#20851;&#31995;&#65288;NLI&#65289;&#38382;&#39064;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#26080;&#38656;&#35821;&#35328;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#36890;&#29992;&#30340;&#25163;&#21160;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#21516;&#19968;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20004;&#20010;&#23454;&#20363;&#8212;&#8212;&#31532;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#28304;&#35821;&#35328;&#30340;&#21477;&#23376;&#23884;&#20837;&#65292;&#31532;&#20108;&#20010;&#22312;&#30446;&#26631;&#35821;&#35328;&#19978;&#36827;&#34892;&#24494;&#35843;&#20197;&#27169;&#20223;&#31532;&#19968;&#20010;&#23454;&#20363;&#12290;&#36825;&#31181;&#25216;&#26415;&#31216;&#20026;&#30693;&#35782;&#33976;&#39311;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#30340;&#26031;&#22374;&#31119;NLI&#27979;&#35797;&#25968;&#25454;&#38598;&#12289;&#26426;&#22120;&#32763;&#35793;&#30340;&#22810;&#31867;&#22411;NLI&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#25163;&#21160;&#32763;&#35793;&#30340;RTE3-ITA&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#23454;&#35777;&#22320;&#23637;&#31034;NLI&#20219;&#21153;&#30340;&#36890;&#29992;&#24615;&#12290;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#26412;&#22320;&#30340;ABSITA&#25968;&#25454;&#38598;&#19978;&#30340;&#24773;&#24863;&#20998;&#26512;&#12289;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#21644;&#20027;&#39064;&#35782;&#21035;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a technique of NLP to tackle the problem of inference relation (NLI) between pairs of sentences in a target language of choice without a language-specific training dataset. We exploit a generic translation dataset, manually translated, along with two instances of the same pre-trained model - the first to generate sentence embeddings for the source language, and the second fine-tuned over the target language to mimic the first. This technique is known as Knowledge Distillation. The model has been evaluated over machine translated Stanford NLI test dataset, machine translated Multi-Genre NLI test dataset, and manually translated RTE3-ITA test dataset. We also test the proposed architecture over different tasks to empirically demonstrate the generality of the NLI task. The model has been evaluated over the native Italian ABSITA dataset, on the tasks of Sentiment Analysis, Aspect-Based Sentiment Analysis, and Topic Recognition. We emphasise the generality and explo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#23545;&#20110;&#20020;&#24202;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"expand-guess-refine"&#30340;&#21307;&#23398;&#38382;&#31572;&#23545;&#40784;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#20351;&#29992;&#25351;&#20196;&#35843;&#20248;&#21644;in-prompt&#31574;&#30053;&#31561;&#25216;&#26415;&#26469;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02884</link><description>&lt;p&gt;
&#23545;&#20110;&#20020;&#24202;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models for Clinical Tasks. (arXiv:2309.02884v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02884
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#23545;&#20110;&#20020;&#24202;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"expand-guess-refine"&#30340;&#21307;&#23398;&#38382;&#31572;&#23545;&#40784;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#20351;&#29992;&#25351;&#20196;&#35843;&#20248;&#21644;in-prompt&#31574;&#30053;&#31561;&#25216;&#26415;&#26469;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36866;&#24212;&#24615;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#27809;&#26377;&#26126;&#30830;&#35757;&#32451;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#33021;&#21147;&#65292;&#20294;&#26377;&#25928;&#22320;&#23545;&#40784;LLM&#20173;&#28982;&#26159;&#22312;&#29305;&#23450;&#20020;&#24202;&#24212;&#29992;&#20013;&#37096;&#32626;&#23427;&#20204;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#29983;&#25104;&#20855;&#26377;&#20107;&#23454;&#20934;&#30830;&#20869;&#23481;&#30340;&#21709;&#24212;&#21644;&#20174;&#20107;&#38750;&#24179;&#20961;&#25512;&#29702;&#27493;&#39588;&#30340;&#33021;&#21147;&#23545;&#20110;LLMs&#33021;&#21542;&#36866;&#29992;&#20110;&#20020;&#24202;&#21307;&#23398;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#21253;&#25324;&#25351;&#20196;&#35843;&#20248;&#21644;&#23569;&#37327;&#31034;&#20363;&#21644;&#24605;&#36335;&#38142;&#25509;&#31561;in-prompt&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21307;&#23398;&#38382;&#31572;&#23545;&#40784;&#31574;&#30053;&#34987;&#31216;&#20026;&#8220; expand-guess-refine&#8221;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21442;&#25968;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;&#21021;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#23427;&#22312;&#38382;&#39064;&#23376;&#38598;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#24471;&#20998;&#20026;70.63&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable adaptability, showcasing their capacity to excel in tasks for which they were not explicitly trained. However, despite their impressive natural language processing (NLP) capabilities, effective alignment of LLMs remains a crucial challenge when deploying them for specific clinical applications. The ability to generate responses with factually accurate content and to engage in non-trivial reasoning steps are crucial for the LLMs to be eligible for applications in clinical medicine. Employing a combination of techniques including instruction-tuning and in-prompt strategies like few-shot and chain of thought prompting has significantly enhanced the performance of LLMs. Our proposed alignment strategy for medical question-answering, known as 'expand-guess-refine', offers a parameter and data-efficient solution. A preliminary analysis of this method demonstrated outstanding performance, achieving a score of 70.63% on a subset of ques
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#19978;&#19979;&#25991;&#21644;&#22238;&#22797;&#20043;&#38388;&#30340;&#38544;&#24335;&#27169;&#24335;&#20449;&#24687;&#25552;&#39640;&#20102;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#37319;&#29992;&#20102;&#25913;&#36827;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35745;&#21010;&#37319;&#26679;&#26041;&#27861;&#65292;&#20351;&#29983;&#25104;&#30340;&#22238;&#22797;&#26356;&#21152;&#29983;&#21160;&#21644;&#20449;&#24687;&#20016;&#23500;&#12290;</title><link>http://arxiv.org/abs/2309.02823</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#19978;&#19979;&#25991;&#21644;&#22238;&#22797;&#20043;&#38388;&#30340;&#27169;&#24335;&#20449;&#24687;&#20419;&#36827;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Promoting Open-domain Dialogue Generation through Learning Pattern Information between Contexts and Responses. (arXiv:2309.02823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#19978;&#19979;&#25991;&#21644;&#22238;&#22797;&#20043;&#38388;&#30340;&#38544;&#24335;&#27169;&#24335;&#20449;&#24687;&#25552;&#39640;&#20102;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#37319;&#29992;&#20102;&#25913;&#36827;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35745;&#21010;&#37319;&#26679;&#26041;&#27861;&#65292;&#20351;&#29983;&#25104;&#30340;&#22238;&#22797;&#26356;&#21152;&#29983;&#21160;&#21644;&#20449;&#24687;&#20016;&#23500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#27169;&#22411;&#24050;&#25104;&#20026;&#28909;&#38376;&#35805;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#22238;&#22797;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#65292;&#22914;&#32570;&#20047;&#19978;&#19979;&#25991;&#21270;&#21644;&#23481;&#26131;&#29983;&#25104;&#32570;&#20047;&#20449;&#24687;&#20869;&#23481;&#30340;&#36890;&#29992;&#22238;&#22797;&#65292;&#20005;&#37325;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#30740;&#31350;&#35797;&#22270;&#24341;&#20837;&#26356;&#22810;&#20449;&#24687;&#21040;&#23545;&#35805;&#27169;&#22411;&#20013;&#65292;&#20351;&#29983;&#25104;&#30340;&#22238;&#22797;&#26356;&#21152;&#29983;&#21160;&#21644;&#20449;&#24687;&#20016;&#23500;&#12290;&#19982;&#23427;&#20204;&#19981;&#21516;&#65292;&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#35757;&#32451;&#26679;&#26412;&#20013;&#19978;&#19979;&#25991;&#21644;&#22238;&#22797;&#20043;&#38388;&#30340;&#38544;&#24335;&#27169;&#24335;&#20449;&#24687;&#26469;&#25552;&#39640;&#29983;&#25104;&#30340;&#22238;&#22797;&#36136;&#37327;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;GPT-2&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35745;&#21010;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#21487;&#20197;&#21033;&#29992;&#29983;&#25104;&#30340;&#22238;&#22797;&#26469;&#25351;&#23548;&#22238;&#22797;&#29983;&#25104;&#65292;&#21516;&#26102;&#36991;&#20813;&#26292;&#38706;&#20559;&#24046;&#38382;&#39064;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, utilizing deep neural networks to build the opendomain dialogue models has become a hot topic. However, the responses generated by these models suffer from many problems such as responses not being contextualized and tend to generate generic responses that lack information content, damaging the user's experience seriously. Therefore, many studies try introducing more information into the dialogue models to make the generated responses more vivid and informative. Unlike them, this paper improves the quality of generated responses by learning the implicit pattern information between contexts and responses in the training samples. In this paper, we first build an open-domain dialogue model based on the pre-trained language model (i.e., GPT-2). And then, an improved scheduled sampling method is proposed for pre-trained models, by which the responses can be used to guide the response generation in the training phase while avoiding the exposure bias problem. More importantly, we de
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#22478;&#24066;&#35268;&#27169;&#30340;&#22320;&#38663;&#26399;&#38388;&#34892;&#20154;&#30095;&#25955;&#65292;&#20197;&#40654;&#24052;&#23273;&#36125;&#40065;&#29305;&#20026;&#20363;&#12290;&#27169;&#22411;&#32508;&#21512;&#32771;&#34385;&#20102;&#22320;&#38663;&#21361;&#38505;&#12289;&#29289;&#29702;&#26131;&#25439;&#24615;&#20197;&#21450;&#20010;&#20307;&#34892;&#20026;&#21644;&#27963;&#21160;&#33021;&#21147;&#12290;&#36825;&#23545;&#20110;&#23450;&#37327;&#39118;&#38505;&#35780;&#20272;&#30740;&#31350;&#26469;&#35828;&#26159;&#37325;&#35201;&#32780;&#21019;&#26032;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.02812</link><description>&lt;p&gt;
&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#34892;&#20154;&#22320;&#38663;&#30095;&#25955;&#20223;&#30495;&#65306;&#20197;&#40654;&#24052;&#23273;&#36125;&#40065;&#29305;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Agent-based simulation of pedestrians' earthquake evacuation; application to Beirut, Lebanon. (arXiv:2309.02812v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#22478;&#24066;&#35268;&#27169;&#30340;&#22320;&#38663;&#26399;&#38388;&#34892;&#20154;&#30095;&#25955;&#65292;&#20197;&#40654;&#24052;&#23273;&#36125;&#40065;&#29305;&#20026;&#20363;&#12290;&#27169;&#22411;&#32508;&#21512;&#32771;&#34385;&#20102;&#22320;&#38663;&#21361;&#38505;&#12289;&#29289;&#29702;&#26131;&#25439;&#24615;&#20197;&#21450;&#20010;&#20307;&#34892;&#20026;&#21644;&#27963;&#21160;&#33021;&#21147;&#12290;&#36825;&#23545;&#20110;&#23450;&#37327;&#39118;&#38505;&#35780;&#20272;&#30740;&#31350;&#26469;&#35828;&#26159;&#37325;&#35201;&#32780;&#21019;&#26032;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#22320;&#38663;&#39118;&#38505;&#35780;&#20272;&#26041;&#27861;&#30528;&#37325;&#20110;&#20272;&#35745;&#24314;&#31569;&#29615;&#22659;&#30340;&#25439;&#22833;&#21644;&#30456;&#24212;&#30340;&#31038;&#20250;&#32463;&#27982;&#25439;&#22833;&#65292;&#32780;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#31038;&#20250;&#39118;&#38505;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#34892;&#20026;&#26159;&#39044;&#27979;&#22320;&#38663;&#23545;&#20154;&#31867;&#24433;&#21709;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#22240;&#27492;&#65292;&#22312;&#23450;&#37327;&#39118;&#38505;&#35780;&#20272;&#30740;&#31350;&#20013;&#21253;&#21547;&#23427;&#26159;&#37325;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#19968;&#31181;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#27169;&#25311;&#22478;&#24066;&#35268;&#27169;&#30340;&#34892;&#20154;&#22320;&#38663;&#30095;&#25955;&#12290;&#35813;&#27169;&#22411;&#25972;&#21512;&#20102;&#22320;&#38663;&#21361;&#38505;&#12289;&#29289;&#29702;&#26131;&#25439;&#24615;&#20197;&#21450;&#20010;&#20307;&#34892;&#20026;&#21644;&#27963;&#21160;&#33021;&#21147;&#12290;&#27169;&#25311;&#22120;&#24212;&#29992;&#20110;&#40654;&#24052;&#23273;&#36125;&#40065;&#29305;&#30340;&#26696;&#20363;&#12290;&#40654;&#24052;&#23273;&#20301;&#20110;&#40654;&#20961;&#29305;&#26029;&#35010;&#24102;&#31995;&#32479;&#30340;&#26680;&#24515;&#22320;&#21306;&#65292;&#35813;&#26029;&#35010;&#24102;&#24050;&#32463;&#21457;&#29983;&#36807;&#22810;&#27425;Mw&gt;7&#30340;&#22320;&#38663;&#65292;&#26368;&#36817;&#19968;&#27425;&#26159;&#22312;1759&#24180;&#12290;&#40654;&#24052;&#23273;&#26159;&#22320;&#20013;&#28023;&#22320;&#21306;&#22320;&#38663;&#39118;&#38505;&#26368;&#39640;&#30340;&#22269;&#23478;&#20043;&#19968;&#65292;&#36825;&#26159;&#30001;&#20110;&#24314;&#31569;&#29289;&#30340;&#39640;&#22320;&#38663;&#26131;&#25439;&#24615;&#20197;&#21450;&#32570;&#20047;&#24378;&#21046;&#24615;&#30340;&#24314;&#31569;&#35268;&#23450;&#25152;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most seismic risk assessment methods focus on estimating the damages to the built environment and the consequent socioeconomic losses without fully taking into account the social aspect of risk. Yet, human behaviour is a key element in predicting the human impact of an earthquake, therefore, it is important to include it in quantitative risk assessment studies. In this study, an interdisciplinary approach simulating pedestrians' evacuation during earthquakes at the city scale is developed using an agent-based model. The model integrates the seismic hazard, the physical vulnerability as well as individuals' behaviours and mobility. The simulator is applied to the case of Beirut, Lebanon. Lebanon is at the heart of the Levant fault system that has generated several Mw&gt;7 earthquakes, the latest being in 1759. It is one of the countries with the highest seismic risk in the Mediterranean region. This is due to the high seismic vulnerability of the buildings due to the absence of mandatory s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;norm tweaking&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35843;&#25972;&#37327;&#21270;&#30340;&#28608;&#27963;&#20998;&#24067;&#26469;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02784</link><description>&lt;p&gt;
Norm&#35843;&#25972;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#24615;&#33021;&#20302;&#27604;&#29305;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Norm Tweaking: High-performance Low-bit Quantization of Large Language Models. (arXiv:2309.02784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;norm tweaking&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35843;&#25972;&#37327;&#21270;&#30340;&#28608;&#27963;&#20998;&#24067;&#26469;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23610;&#23544;&#19981;&#26029;&#22686;&#22823;&#65292;&#22312;&#20445;&#25345;&#31934;&#24230;&#30340;&#21069;&#25552;&#19979;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#24050;&#25104;&#20026;&#37096;&#32626;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#34429;&#28982;&#19968;&#20123;&#37327;&#21270;&#26041;&#27861;&#65292;&#22914;GPTQ&#65292;&#22312;&#23454;&#29616;&#21487;&#25509;&#21463;&#30340;4&#27604;&#29305;&#26435;&#37325;&#37327;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23581;&#35797;&#26356;&#20302;&#20301;&#30340;&#37327;&#21270;&#24448;&#24448;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;norm tweaking&#8221;&#30340;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#24403;&#21069;PTQ&#26041;&#27861;&#30340;&#25554;&#20214;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#25104;&#26412;&#39640;&#25928;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#19968;&#39033;&#35266;&#23519;&#30340;&#21551;&#31034;&#65292;&#21363;&#20351;&#35843;&#25972;&#37327;&#21270;&#30340;&#28608;&#27963;&#20998;&#24067;&#20197;&#19982;&#20854;&#28014;&#28857;&#23545;&#24212;&#29289;&#21305;&#37197;&#65292;&#20063;&#21487;&#20197;&#24674;&#22797;LLMs&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#35843;&#25972;&#31574;&#30053;&#65292;&#21253;&#25324;&#29983;&#25104;&#26657;&#20934;&#25968;&#25454;&#21644;&#36890;&#36947;&#36317;&#31163;&#32422;&#26463;&#65292;&#20197;&#26356;&#26032;&#24402;&#19968;&#21270;&#23618;&#30340;&#26435;&#37325;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#20960;&#20010;&#24320;&#28304;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of large language models (LLMs) continues to grow, model compression without sacrificing accuracy has become a crucial challenge for deployment. While some quantization methods, such as GPTQ, have made progress in achieving acceptable 4-bit weight-only quantization, attempts at lower bit quantization often result in severe performance degradation. In this paper, we introduce a technique called norm tweaking, which can be used as a plugin in current PTQ methods to achieve high precision while being cost-efficient. Our approach is inspired by the observation that rectifying the quantized activation distribution to match its float counterpart can readily restore accuracy for LLMs. To achieve this, we carefully design a tweaking strategy that includes calibration data generation and channel-wise distance constraint to update the weights of normalization layers for better generalization. We conduct extensive experiments on various datasets using several open-sourced LLMs. Our me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#25216;&#26415;&#23454;&#29616;&#20102;&#35821;&#38899;&#35821;&#20041;&#29702;&#35299;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#24494;&#35843;&#19979;&#28216;&#20219;&#21153;&#21518;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02780</link><description>&lt;p&gt;
GRASS: &#35821;&#38899;&#35821;&#20041;&#29702;&#35299;&#32479;&#19968;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GRASS: Unified Generation Model for Speech Semantic Understanding. (arXiv:2309.02780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#25216;&#26415;&#23454;&#29616;&#20102;&#35821;&#38899;&#35821;&#20041;&#29702;&#35299;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#24494;&#35843;&#19979;&#28216;&#20219;&#21153;&#21518;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#25506;&#32034;&#20102;&#35821;&#38899;&#35821;&#20041;&#29702;&#35299;&#30340;&#25351;&#20196;&#24494;&#35843;&#25216;&#26415;&#65292;&#35813;&#26694;&#26550;&#26681;&#25454;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25552;&#31034;&#20026;&#38899;&#39057;&#25968;&#25454;&#29983;&#25104;&#35821;&#20041;&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#37327;&#22810;&#26679;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20854;&#20013;&#25351;&#20196;-&#35821;&#38899;&#23545;&#26159;&#36890;&#36807;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;&#26500;&#24314;&#30340;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#24494;&#35843;&#19979;&#28216;&#20219;&#21153;&#21518;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#22312;&#35821;&#38899;&#21040;&#35821;&#20041;&#20219;&#21153;&#30340;&#25351;&#20196;&#24494;&#35843;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the instruction fine-tuning technique for speech semantic understanding by introducing a unified end-to-end (E2E) framework that generates semantic labels conditioned on a task-related prompt for audio data. We pre-train the model using large and diverse data, where instruction-speech pairs are constructed via a text-to-speech (TTS) system. Extensive experiments demonstrate that our proposed model significantly outperforms state-of-the-art (SOTA) models after fine-tuning downstream tasks. Furthermore, the proposed model achieves competitive performance in zero-shot and few-shot scenarios. To facilitate future work on instruction fine-tuning for speech-to-semantic tasks, we release our instruction dataset and code.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21160;&#24577;&#28201;&#24230;&#37319;&#26679;&#30340;AdapT&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#35843;&#25972;&#28201;&#24230;&#31995;&#25968;&#26469;&#35299;&#20915;&#38590;&#20197;&#39044;&#27979;&#30340;&#20195;&#30721;&#26631;&#35760;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.02772</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#28201;&#24230;&#37319;&#26679;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Code Generation by Dynamic Temperature Sampling. (arXiv:2309.02772v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02772
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#28201;&#24230;&#37319;&#26679;&#30340;AdapT&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#35843;&#25972;&#28201;&#24230;&#31995;&#25968;&#26469;&#35299;&#20915;&#38590;&#20197;&#39044;&#27979;&#30340;&#20195;&#30721;&#26631;&#35760;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#30721;&#31574;&#30053;&#26159;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35774;&#35745;&#30340;&#65292;&#24573;&#35270;&#20102;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#30001;&#20110;&#36825;&#20010;&#30095;&#24573;&#65292;&#22914;&#20309;&#35774;&#35745;&#26356;&#22909;&#30340;&#20195;&#30721;&#29983;&#25104;&#35299;&#30721;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#31995;&#32479;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;&#20195;&#30721;&#26631;&#35760;&#20002;&#22833;&#20998;&#24067;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20195;&#30721;&#26631;&#35760;&#21487;&#20197;&#20998;&#20026;&#20004;&#31867;&#65306;&#38590;&#20197;&#39044;&#27979;&#30340;&#25361;&#25112;&#24615;&#26631;&#35760;&#21644;&#26131;&#20110;&#25512;&#26029;&#30340;&#33258;&#20449;&#26631;&#35760;&#12290;&#20854;&#20013;&#65292;&#25361;&#25112;&#24615;&#26631;&#35760;&#20027;&#35201;&#20986;&#29616;&#22312;&#20195;&#30721;&#22359;&#30340;&#24320;&#22836;&#12290;&#21463;&#21040;&#19978;&#36848;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65306;&#33258;&#36866;&#24212;&#28201;&#24230;&#65288;AdapT&#65289;&#37319;&#26679;&#65292;&#23427;&#22312;&#35299;&#30721;&#19981;&#21516;&#30340;&#26631;&#35760;&#26102;&#21160;&#24577;&#35843;&#25972;&#28201;&#24230;&#31995;&#25968;&#12290;&#25105;&#20204;&#22312;&#37319;&#26679;&#25361;&#25112;&#24615;&#26631;&#35760;&#26102;&#24212;&#29992;&#36739;&#22823;&#30340;&#28201;&#24230;&#20540;&#12290;&#21516;&#26102;&#65292;&#22312;&#37319;&#26679;&#33258;&#20449;&#26631;&#35760;&#26102;&#24212;&#29992;&#36739;&#23567;&#30340;&#28201;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLMs) have shown impressive results in code generation. However, existing decoding strategies are designed for Natural Language (NL) generation, overlooking the differences between NL and programming languages (PL). Due to this oversight, a better decoding strategy for code generation remains an open question. In this paper, we conduct the first systematic study to explore a decoding strategy specialized in code generation. With an analysis of loss distributions of code tokens, we find that code tokens can be divided into two categories: challenging tokens that are difficult to predict and confident tokens that can be easily inferred. Among them, the challenging tokens mainly appear at the beginning of a code block. Inspired by the above findings, we propose a simple yet effective method: Adaptive Temperature (AdapT) sampling, which dynamically adjusts the temperature coefficient when decoding different tokens. We apply a larger temperature when samplin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#35780;&#20998;&#30340;&#32454;&#20998;&#26631;&#20934;&#29305;&#23450;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#22521;&#35757;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#23398;&#20064;&#20043;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#29305;&#24449;&#21644;&#21151;&#33021;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02740</link><description>&lt;p&gt;
&#29992;&#32454;&#20998;&#26631;&#20934;&#29305;&#23450;&#26041;&#27861;&#36827;&#34892;&#22686;&#24378;&#22521;&#35757;&#30340;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Rubric-Specific Approach to Automated Essay Scoring with Augmentation Training. (arXiv:2309.02740v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#35780;&#20998;&#30340;&#32454;&#20998;&#26631;&#20934;&#29305;&#23450;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#22521;&#35757;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#23398;&#20064;&#20043;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#29305;&#24449;&#21644;&#21151;&#33021;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#21644;&#29305;&#24449;&#24037;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20027;&#35266;&#31572;&#26696;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24182;&#27809;&#26377;&#22949;&#21892;&#32771;&#34385;&#21040;&#22312;&#27169;&#22411;&#35757;&#32451;&#21644;&#39564;&#35777;&#36807;&#31243;&#20013;&#23545;&#20110;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#33267;&#20851;&#37325;&#35201;&#30340;&#32454;&#20998;&#26631;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25968;&#25454;&#22686;&#24378;&#25805;&#20316;&#65292;&#36890;&#36807;&#35757;&#32451;&#21644;&#27979;&#35797;&#19968;&#20010;&#33258;&#21160;&#35780;&#20998;&#27169;&#22411;&#65292;&#23398;&#20064;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#29305;&#24449;&#21644;&#21151;&#33021;&#65292;&#21516;&#26102;&#22312;&#33258;&#21160;&#21270;&#23398;&#29983;&#35780;&#20272;&#22870;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural based approaches to automatic evaluation of subjective responses have shown superior performance and efficiency compared to traditional rule-based and feature engineering oriented solutions. However, it remains unclear whether the suggested neural solutions are sufficient replacements of human raters as we find recent works do not properly account for rubric items that are essential for automated essay scoring during model training and validation. In this paper, we propose a series of data augmentation operations that train and test an automated scoring model to learn features and functions overlooked by previous works while still achieving state-of-the-art performance in the Automated Student Assessment Prize dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;HC3 Plus&#65292;&#19968;&#20010;&#35821;&#20041;&#19981;&#21464;&#30340;&#20154;&#31867;ChatGPT&#23545;&#27604;&#35821;&#26009;&#24211;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#35813;&#35821;&#26009;&#24211;&#32771;&#34385;&#20102;&#26356;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#20013;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#21152;&#22256;&#38590;&#12290;&#36890;&#36807;&#22823;&#37327;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#21644;Tk-instruct&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.02731</link><description>&lt;p&gt;
HC3 Plus&#65306;&#19968;&#20010;&#35821;&#20041;&#19981;&#21464;&#30340;&#20154;&#31867;ChatGPT&#23545;&#27604;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
HC3 Plus: A Semantic-Invariant Human ChatGPT Comparison Corpus. (arXiv:2309.02731v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;HC3 Plus&#65292;&#19968;&#20010;&#35821;&#20041;&#19981;&#21464;&#30340;&#20154;&#31867;ChatGPT&#23545;&#27604;&#35821;&#26009;&#24211;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#35813;&#35821;&#26009;&#24211;&#32771;&#34385;&#20102;&#26356;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#20013;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#21152;&#22256;&#38590;&#12290;&#36890;&#36807;&#22823;&#37327;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#21644;Tk-instruct&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22240;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20154;&#20204;&#23545;&#20854;&#28508;&#22312;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#23545;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#30340;&#26816;&#27979;&#36234;&#26469;&#36234;&#20851;&#27880;&#65292;&#36825;&#23545;&#26410;&#32463;&#35757;&#32451;&#30340;&#20154;&#31867;&#26469;&#35828;&#24448;&#24448;&#24456;&#38590;&#35782;&#21035;&#12290;&#30446;&#21069;&#29992;&#20110;&#26816;&#27979;ChatGPT&#29983;&#25104;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#38598;&#20013;&#22312;&#38382;&#31572;&#26041;&#38754;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#20855;&#26377;&#35821;&#20041;&#19981;&#21464;&#24615;&#30340;&#20219;&#21153;&#65292;&#22914;&#25688;&#35201;&#12289;&#32763;&#35793;&#21644;&#25913;&#20889;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#19978;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#12289;&#26356;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#26356;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#32463;&#36807;&#22823;&#37327;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#20197;&#21069;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25351;&#23548;&#24494;&#35843;&#20102;Tk-instruct&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has gained significant interest due to its impressive performance, but people are increasingly concerned about its potential risks, particularly around the detection of AI-generated content (AIGC), which is often difficult for untrained humans to identify. Current datasets utilized for detecting ChatGPT-generated text primarily center around question-answering, yet they tend to disregard tasks that possess semantic-invariant properties, such as summarization, translation, and paraphrasing. Our primary studies demonstrate that detecting model-generated text on semantic-invariant tasks is more difficult. To fill this gap, we introduce a more extensive and comprehensive dataset that considers more types of tasks than previous work, including semantic-invariant tasks. In addition, the model after a large number of task instruction fine-tuning shows a strong powerful performance. Owing to its previous success, we further instruct fine-tuning Tk-instruct and built a more powerful det
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#23398;&#26415;&#20551;&#35774;&#21457;&#29616;&#30340;&#31532;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#22522;&#20110;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#12289;&#26032;&#39062;&#19988;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26377;&#24110;&#21161;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2309.02726</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#21160;&#24320;&#25918;&#39046;&#22495;&#31185;&#23398;&#20551;&#35774;&#21457;&#29616;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Automated Open-domain Scientific Hypotheses Discovery. (arXiv:2309.02726v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02726
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#23398;&#26415;&#20551;&#35774;&#21457;&#29616;&#30340;&#31532;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#22522;&#20110;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#12289;&#26032;&#39062;&#19988;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26377;&#24110;&#21161;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31185;&#23398;&#23478;&#35266;&#23519;&#19990;&#30028;&#24182;&#35797;&#22270;&#25552;&#20986;&#35299;&#37322;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#30340;&#20551;&#35774;&#26102;&#65292;&#20551;&#35774;&#24402;&#32435;&#34987;&#35748;&#20026;&#26159;&#20027;&#35201;&#30340;&#25512;&#29702;&#31867;&#22411;&#12290;&#36807;&#21435;&#20851;&#20110;&#20551;&#35774;&#24402;&#32435;&#30340;&#30740;&#31350;&#23384;&#22312;&#20197;&#19979;&#38480;&#21046;&#65306;&#65288;1&#65289;&#25968;&#25454;&#38598;&#30340;&#35266;&#23519;&#27880;&#37322;&#19981;&#26159;&#21407;&#22987;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#65292;&#32780;&#26159;&#25163;&#21160;&#36873;&#25321;&#30340;&#21477;&#23376;&#65288;&#23548;&#33268;&#20102;&#19968;&#20010;&#23553;&#38381;&#39046;&#22495;&#30340;&#35774;&#32622;&#65289;&#65307;&#65288;2&#65289;&#23454;&#38469;&#30340;&#20551;&#35774;&#27880;&#37322;&#20027;&#35201;&#26159;&#24120;&#35782;&#30693;&#35782;&#65292;&#20351;&#24471;&#20219;&#21153;&#19981;&#22826;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#23398;&#26415;&#20551;&#35774;&#21457;&#29616;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;50&#31687;&#21457;&#34920;&#22312;&#39030;&#32423;&#31038;&#20250;&#31185;&#23398;&#26399;&#21002;&#19978;&#30340;&#26368;&#26032;&#35770;&#25991;&#12290;&#25968;&#25454;&#38598;&#20013;&#36824;&#25910;&#38598;&#20102;&#24320;&#21457;&#35770;&#25991;&#20013;&#30340;&#20551;&#35774;&#25152;&#38656;&#30340;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#31995;&#32479;&#65292;&#20165;&#36890;&#36807;&#19968;&#22534;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#23601;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#12289;&#26032;&#39062;&#19988;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26377;&#24110;&#21161;&#30340;&#20551;&#35774;&#12290;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#21487;&#20197;&#35299;&#20915;&#20197;&#21069;&#20851;&#20110;&#20551;&#35774;&#24402;&#32435;&#30340;&#30740;&#31350;&#25152;&#38754;&#20020;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations. Past research on hypothetical induction has a limited setting that (1) the observation annotations of the dataset are not raw web corpus but are manually selected sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses annotations are mostly commonsense knowledge, making the task less challenging. In this work, we propose the first NLP dataset for social science academic hypotheses discovery, consisting of 50 recent papers published in top social science journals. Raw web corpora that are necessary for developing hypotheses in the published papers are also collected in the dataset, with the final goal of creating a system that automatically generates valid, novel, and helpful (to human researchers) hypotheses, given only a pile of raw web corpora. The new dataset can tackle the previou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24076;&#20271;&#26469;&#35821;&#20398;&#36785;&#24615;&#35821;&#26009;&#24211;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#24076;&#20271;&#26469;&#35821;BERT&#27169;&#22411;&#65288;HeBERT&#21644;AlephBERT&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#32467;&#21512;D_OLaH&#21487;&#20197;&#25552;&#39640;HeBERT&#27169;&#22411;&#30340;&#24615;&#33021;2%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#23545;AlephBERT&#27169;&#22411;&#20063;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02724</link><description>&lt;p&gt;
&#24076;&#20271;&#26469;&#35821;&#20398;&#36785;&#24615;&#35821;&#26009;&#24211;&#21450;BERT&#27169;&#22411;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Offensive Hebrew Corpus and Detection using BERT. (arXiv:2309.02724v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24076;&#20271;&#26469;&#35821;&#20398;&#36785;&#24615;&#35821;&#26009;&#24211;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#24076;&#20271;&#26469;&#35821;BERT&#27169;&#22411;&#65288;HeBERT&#21644;AlephBERT&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#32467;&#21512;D_OLaH&#21487;&#20197;&#25552;&#39640;HeBERT&#27169;&#22411;&#30340;&#24615;&#33021;2%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#23545;AlephBERT&#27169;&#22411;&#20063;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20398;&#36785;&#24615;&#35821;&#35328;&#26816;&#27979;&#22312;&#35768;&#22810;&#35821;&#35328;&#20013;&#24050;&#32463;&#26377;&#24456;&#22810;&#30740;&#31350;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;(&#22914;&#24076;&#20271;&#26469;&#35821;)&#20013;&#20173;&#26377;&#25152;&#28382;&#21518;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#24076;&#20271;&#26469;&#35821;&#20398;&#36785;&#24615;&#35821;&#26009;&#24211;&#65292;&#20174;Twitter&#19978;&#25910;&#38598;&#20102;15881&#26465;&#25512;&#25991;&#12290;&#27599;&#26465;&#25512;&#25991;&#37117;&#30001;&#38463;&#25289;&#20271;-&#24076;&#20271;&#26469;&#21452;&#35821;&#20154;&#22763;&#26631;&#35760;&#20026;&#20116;&#20010;&#31867;&#21035;(&#36785;&#39554;&#12289;&#20167;&#24680;&#12289;&#26292;&#21147;&#12289;&#33394;&#24773;&#25110;&#38750;&#20398;&#36785;&#24615;)&#12290;&#26631;&#27880;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#26631;&#27880;&#32773;&#37117;&#38656;&#35201;&#29087;&#24713;&#20197;&#33394;&#21015;&#30340;&#25991;&#21270;&#12289;&#25919;&#27835;&#21644;&#23454;&#36341;&#65292;&#20197;&#29702;&#35299;&#27599;&#26465;&#25512;&#25991;&#30340;&#32972;&#26223;&#12290;&#25105;&#20204;&#20351;&#29992;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#21644;&#21478;&#19968;&#20010;&#24050;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#23545;&#20004;&#20010;&#24076;&#20271;&#26469;&#35821;BERT&#27169;&#22411;(HeBERT&#21644;AlephBERT)&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#19982;D_OLaH&#32467;&#21512;&#21518;&#65292;&#25552;&#39640;&#20102;HeBERT&#27169;&#22411;2%&#30340;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#19978;&#23545;AlephBERT&#36827;&#34892;&#24494;&#35843;&#24182;&#22312;D_OLaH&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;69%&#65292;&#32780;&#22312;D_OLaH&#19978;&#36827;&#34892;&#24494;&#35843;&#24182;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;&#20934;&#30830;&#29575;&#20026;57%&#65292;&#36825;&#21487;&#33021;&#34920;&#26126;&#25105;&#20204;&#30340;&#25968;&#25454;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offensive language detection has been well studied in many languages, but it is lagging behind in low-resource languages, such as Hebrew. In this paper, we present a new offensive language corpus in Hebrew. A total of 15,881 tweets were retrieved from Twitter. Each was labeled with one or more of five classes (abusive, hate, violence, pornographic, or none offensive) by Arabic-Hebrew bilingual speakers. The annotation process was challenging as each annotator is expected to be familiar with the Israeli culture, politics, and practices to understand the context of each tweet. We fine-tuned two Hebrew BERT models, HeBERT and AlephBERT, using our proposed dataset and another published dataset. We observed that our data boosts HeBERT performance by 2% when combined with D_OLaH. Fine-tuning AlephBERT on our data and testing on D_OLaH yields 69% accuracy, while fine-tuning on D_OLaH and testing on our data yields 57% accuracy, which may be an indication to the generalizability our data offer
&lt;/p&gt;</description></item><item><title>HAE-RAE Bench&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#38889;&#22269;&#30693;&#35782;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20351;&#29992;&#27604;GPT-3.5&#23567;&#30340;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24378;&#35843;&#20102;&#21516;&#36136;&#35821;&#26009;&#24211;&#22312;&#35757;&#32451;&#19987;&#19994;&#32423;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02706</link><description>&lt;p&gt;
HAE-RAE Bench: &#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#38889;&#22269;&#30693;&#35782;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models. (arXiv:2309.02706v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02706
&lt;/p&gt;
&lt;p&gt;
HAE-RAE Bench&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#38889;&#22269;&#30693;&#35782;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20351;&#29992;&#27604;GPT-3.5&#23567;&#30340;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24378;&#35843;&#20102;&#21516;&#36136;&#35821;&#26009;&#24211;&#22312;&#35757;&#32451;&#19987;&#19994;&#32423;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#23545;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#20851;&#27880;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20013;&#26377;&#38480;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#24182;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#38889;&#35821;&#35821;&#35328;&#21644;&#25991;&#21270;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HAE-RAE Bench&#65292;&#22312;&#35789;&#27719;&#12289;&#21382;&#21490;&#21644;&#19968;&#33324;&#30693;&#35782;&#31561;6&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#30340;&#35780;&#20272;&#31361;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;(LLSMs)&#19982;&#20687;GPT-3.5&#36825;&#26679;&#30340;&#20840;&#38754;&#36890;&#29992;&#27169;&#22411;&#30456;&#27604;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#27604;GPT-3.5&#32422;&#23567;13&#20493;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#29305;&#23450;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#23637;&#29616;&#20986;&#31867;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#36825;&#19968;&#35266;&#23519;&#24378;&#35843;&#20102;&#22312;&#35757;&#32451;&#19987;&#19994;&#32423;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#26102;&#21516;&#36136;&#35821;&#26009;&#24211;&#30340;&#37325;&#35201;&#24615;&#12290;&#30456;&#21453;&#65292;&#24403;&#36825;&#20123;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;......
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) pretrained on massive corpora exhibit remarkable capabilities across a wide range of tasks, however, the attention given to non-English languages has been limited in this field of research. To address this gap and assess the proficiency of language models in the Korean language and culture, we present HAE-RAE Bench, covering 6 tasks including vocabulary, history, and general knowledge. Our evaluation of language models on this benchmark highlights the potential advantages of employing Large Language-Specific Models(LLSMs) over a comprehensive, universal model like GPT-3.5. Remarkably, our study reveals that models approximately 13 times smaller than GPT-3.5 can exhibit similar performance levels in terms of language-specific knowledge retrieval. This observation underscores the importance of homogeneous corpora for training professional-level language-specific models. On the contrary, we also observe a perplexing performance dip in these smaller LMs when th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;</title><link>http://arxiv.org/abs/2309.02705</link><description>&lt;p&gt;
&#35777;&#26126;LLM&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certifying LLM Safety against Adversarial Prompting. (arXiv:2309.02705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#23433;&#20840;&#65292;&#20844;&#24320;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#8220;&#27169;&#22411;&#23545;&#40784;&#8221;&#38450;&#25252;&#25514;&#26045;&#12290;&#19968;&#20010;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#25298;&#32477;&#29992;&#25143;&#30340;&#35831;&#27714;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23433;&#20840;&#25514;&#26045;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25552;&#31034;&#30340;&#25915;&#20987;&#65292;&#25932;&#23545;&#25552;&#31034;&#21253;&#21547;&#24694;&#24847;&#35774;&#35745;&#30340;&#26631;&#35760;&#24207;&#21015;&#65292;&#20197;&#35268;&#36991;&#27169;&#22411;&#30340;&#23433;&#20840;&#38450;&#25252;&#24182;&#23548;&#33268;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#31532;&#19968;&#20010;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#12290;&#25105;&#20204;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#12290;&#22914;&#26524;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#27979;&#21040;&#20219;&#20309;&#23376;&#24207;&#21015;&#25110;&#36755;&#20837;&#25552;&#31034;&#26377;&#23475;&#65292;&#25105;&#20204;&#30340;&#36807;&#31243;&#23558;&#23558;&#36755;&#20837;&#25552;&#31034;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#36825;&#20445;&#35777;&#20102;&#23545;&#20110;&#26576;&#20010;&#29305;&#23450;&#22823;&#23567;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#30340;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#20063;&#23558;&#34987;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#25105;&#20204;&#23545;&#25239;&#19977;&#31181;&#25915;&#20987;&#27169;&#24335;&#65306;i)&#25932;&#23545;&#21518;&#32512;&#65292;&#21363;&#38468;&#21152;&#25932;&#23545;&#24207;&#21015;&#8230;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) released for public use incorporate guardrails to ensure their output is safe, often referred to as "model alignment." An aligned language model should decline a user's request to produce harmful content. However, such safety measures are vulnerable to adversarial prompts, which contain maliciously designed token sequences to circumvent the model's safety guards and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with verifiable safety guarantees. We erase tokens individually and inspect the resulting subsequences using a safety filter. Our procedure labels the input prompt as harmful if any subsequences or the input prompt are detected as harmful by the filter. This guarantees that any adversarial modification of a harmful prompt up to a certain size is also labeled harmful. We defend against three attack modes: i) adversarial suffix, which appends an adversarial seq
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#30740;&#31350;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30701;&#35821;&#23450;&#20301;&#21644;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#36890;&#36807;&#39564;&#35777;&#23454;&#39564;&#21457;&#29616;&#20102;&#24403;&#20195;&#27169;&#22411;&#22312;&#30701;&#35821;&#23450;&#20301;&#21644;&#20219;&#21153;&#27714;&#35299;&#26041;&#38754;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02691</link><description>&lt;p&gt;
&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30701;&#35821;&#23450;&#20301;&#21644;&#20219;&#21153;&#34920;&#29616;&#30340;&#32852;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Joint Study of Phrase Grounding and Task Performance in Vision and Language Models. (arXiv:2309.02691v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02691
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#30740;&#31350;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30701;&#35821;&#23450;&#20301;&#21644;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#36890;&#36807;&#39564;&#35777;&#23454;&#39564;&#21457;&#29616;&#20102;&#24403;&#20195;&#27169;&#22411;&#22312;&#30701;&#35821;&#23450;&#20301;&#21644;&#20219;&#21153;&#27714;&#35299;&#26041;&#38754;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38656;&#35201;&#23545;&#35270;&#35273;&#32972;&#26223;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#65292;&#20851;&#38190;&#26159;&#23558;&#21333;&#35789;&#21644;&#30701;&#35821;&#19982;&#22270;&#20687;&#21306;&#22495;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#36890;&#24120;&#39044;&#26399;&#20197;&#26377;&#21161;&#20110;&#27867;&#21270;&#30340;&#26041;&#24335;&#35299;&#20915;&#20219;&#21153;&#65292;&#35266;&#23519;&#21040;&#24403;&#20195;&#27169;&#22411;&#20013;&#30340;&#36825;&#31181;&#23450;&#20301;&#20063;&#26159;&#22797;&#26434;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#20849;&#21516;&#30740;&#31350;&#20219;&#21153;&#25191;&#34892;&#21644;&#30701;&#35821;&#23450;&#20301;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#22522;&#20934;&#26469;&#30740;&#31350;&#20004;&#32773;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20195;&#27169;&#22411;&#22312;&#23450;&#20301;&#30701;&#35821;&#21644;&#35299;&#20915;&#20219;&#21153;&#30340;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23545;&#23450;&#20301;&#26631;&#27880;&#36827;&#34892;&#24378;&#21046;&#24615;&#35757;&#32451;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#25152;&#21019;&#24314;&#30340;&#21160;&#24577;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/lil-lab/phrase_grounding&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Key to tasks that require reasoning about natural language in visual contexts is grounding words and phrases to image regions. However, observing this grounding in contemporary models is complex, even if it is generally expected to take place if the task is addressed in a way that is conductive to generalization. We propose a framework to jointly study task performance and phrase grounding, and propose three benchmarks to study the relation between the two. Our results show that contemporary models demonstrate inconsistency between their ability to ground phrases and solve tasks. We show how this can be addressed through brute-force training on ground phrasing annotations, and analyze the dynamics it creates. Code and at available at https://github.com/lil-lab/phrase_grounding.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#36164;&#28304;&#24187;&#35273;&#39044;&#38450;&#26041;&#27861;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#23545;&#36755;&#20837;&#25351;&#20196;&#20013;&#27010;&#24565;&#30340;&#29087;&#24713;&#31243;&#24230;&#65292;&#22312;&#36935;&#21040;&#19981;&#29087;&#24713;&#30340;&#27010;&#24565;&#26102;&#19981;&#29983;&#25104;&#21709;&#24212;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.02654</link><description>&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#36164;&#28304;&#24187;&#35273;&#39044;&#38450;
&lt;/p&gt;
&lt;p&gt;
Zero-Resource Hallucination Prevention for Large Language Models. (arXiv:2309.02654v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#36164;&#28304;&#24187;&#35273;&#39044;&#38450;&#26041;&#27861;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#23545;&#36755;&#20837;&#25351;&#20196;&#20013;&#27010;&#24565;&#30340;&#29087;&#24713;&#31243;&#24230;&#65292;&#22312;&#36935;&#21040;&#19981;&#29087;&#24713;&#30340;&#27010;&#24565;&#26102;&#19981;&#29983;&#25104;&#21709;&#24212;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24341;&#36215;&#20102;&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#30340;&#20851;&#27880;&#65292;&#36825;&#25351;&#30340;&#26159;LLMs&#29983;&#25104;&#20107;&#23454;&#19981;&#20934;&#30830;&#25110;&#27809;&#26377;&#26681;&#25454;&#30340;&#20449;&#24687;&#30340;&#24773;&#20917;&#12290;&#29616;&#26377;&#30340;&#35821;&#35328;&#21161;&#25163;&#20013;&#24187;&#35273;&#26816;&#27979;&#25216;&#26415;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#27169;&#31946;&#12289;&#22522;&#20110;&#33258;&#30001;&#35821;&#35328;&#30340;&#24605;&#32500;&#38142;&#26465;(CoT)&#25216;&#26415;&#25110;&#22522;&#20110;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#23384;&#22312;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#35782;&#21035;&#29983;&#25104;&#21518;&#24187;&#35273;&#30340;&#26041;&#27861;&#26080;&#27861;&#39044;&#38450;&#20854;&#21457;&#29983;&#65292;&#24182;&#19988;&#30001;&#20110;&#25351;&#20196;&#26684;&#24335;&#21644;&#27169;&#22411;&#39118;&#26684;&#30340;&#24433;&#21709;&#65292;&#24615;&#33021;&#19981;&#19968;&#33268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#26816;&#27979;&#33258;&#25105;&#35780;&#20272;&#25216;&#26415;&#65292;&#31216;&#20026;{\method}&#65292;&#23427;&#19987;&#27880;&#20110;&#35780;&#20272;&#27169;&#22411;&#23545;&#36755;&#20837;&#25351;&#20196;&#20013;&#27010;&#24565;&#30340;&#29087;&#24713;&#31243;&#24230;&#65292;&#24182;&#22312;&#36935;&#21040;&#19981;&#29087;&#24713;&#30340;&#27010;&#24565;&#26102;&#19981;&#29983;&#25104;&#21709;&#24212;&#12290;&#36825;&#31181;&#26041;&#27861;&#27169;&#25311;&#20102;&#20154;&#31867;&#33021;&#22815;&#22312;&#27809;&#26377;&#25226;&#25569;&#26102;&#19981;&#20316;&#22238;&#24212;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalent use of large language models (LLMs) in various domains has drawn attention to the issue of "hallucination," which refers to instances where LLMs generate factually inaccurate or ungrounded information. Existing techniques for hallucination detection in language assistants rely on intricate fuzzy, specific free-language-based chain of thought (CoT) techniques or parameter-based methods that suffer from interpretability issues. Additionally, the methods that identify hallucinations post-generation could not prevent their occurrence and suffer from inconsistent performance due to the influence of the instruction format and model style. In this paper, we introduce a novel pre-detection self-evaluation technique, referred to as {\method}, which focuses on evaluating the model's familiarity with the concepts present in the input instruction and withholding the generation of response in case of unfamiliar concepts. This approach emulates the human ability to refrain from respond
&lt;/p&gt;</description></item><item><title>Epi-Curriculum&#26159;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#20302;&#36164;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#38598;&#35757;&#32451;&#21644;&#21435;&#22122;&#30340;&#35838;&#31243;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#39046;&#22495;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02640</link><description>&lt;p&gt;
Epi-Curriculum: &#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#20302;&#36164;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#20998;&#38598;&#35838;&#31243;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Epi-Curriculum: Episodic Curriculum Learning for Low-Resource Domain Adaptation in Neural Machine Translation. (arXiv:2309.02640v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02640
&lt;/p&gt;
&lt;p&gt;
Epi-Curriculum&#26159;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#20302;&#36164;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#38598;&#35757;&#32451;&#21644;&#21435;&#22122;&#30340;&#35838;&#31243;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#39046;&#22495;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#38750;&#24120;&#25104;&#21151;&#65292;&#20294;&#22312;&#38480;&#23450;&#25968;&#37327;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#26032;&#39046;&#22495;&#30340;&#32763;&#35793;&#26102;&#65292;&#20854;&#24615;&#33021;&#20173;&#28982;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;Epi-Curriculum&#65292;&#29992;&#20110;&#35299;&#20915;&#20302;&#36164;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#65292;&#23427;&#21253;&#21547;&#19968;&#20010;&#26032;&#30340;&#20998;&#38598;&#35757;&#32451;&#26694;&#26550;&#21644;&#21435;&#22122;&#30340;&#35838;&#31243;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20998;&#38598;&#35757;&#32451;&#26694;&#26550;&#36890;&#36807;&#21608;&#26399;&#24615;&#22320;&#23558;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#26292;&#38706;&#32473;&#32463;&#39564;&#19981;&#36275;&#30340;&#35299;&#30721;&#22120;/&#32534;&#30721;&#22120;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#39046;&#22495;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#21435;&#22122;&#30340;&#35838;&#31243;&#23398;&#20064;&#36890;&#36807;&#36880;&#27493;&#24341;&#23548;&#23398;&#20064;&#36807;&#31243;&#20174;&#31616;&#21333;&#21040;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#33521;&#24503;&#21644;&#33521;&#32599;&#39532;&#23612;&#20122;&#32763;&#35793;&#26041;&#21521;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;&#65288;i&#65289;Epi-Curriculum&#22312;&#24050;&#35265;&#21644;&#26410;&#35265;&#39046;&#22495;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#36866;&#24212;&#24615;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#30340;&#20998;&#38598;&#35757;&#32451;&#26694;&#26550;&#22686;&#24378;&#20102;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23545;&#39046;&#22495;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation (NMT) models have become successful, but their performance remains poor when translating on new domains with a limited number of data. In this paper, we present a novel approach Epi-Curriculum to address low-resource domain adaptation (DA), which contains a new episodic training framework along with denoised curriculum learning. Our episodic training framework enhances the model's robustness to domain shift by episodically exposing the encoder/decoder to an inexperienced decoder/encoder. The denoised curriculum learning filters the noised data and further improves the model's adaptability by gradually guiding the learning process from easy to more difficult tasks. Experiments on English-German and English-Romanian translation show that: (i) Epi-Curriculum improves both model's robustness and adaptability in seen and unseen domains; (ii) Our episodic training framework enhances the encoder and decoder's robustness to domain shift.
&lt;/p&gt;</description></item><item><title>CM3Leon&#26159;&#19968;&#20010;&#32553;&#25918;&#33258;&#22238;&#24402;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#29983;&#25104;&#21644;&#22635;&#20805;&#65292;&#36798;&#21040;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#32780;&#35745;&#31639;&#36164;&#28304;&#24320;&#38144;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.02591</link><description>&lt;p&gt;
&#32553;&#25918;&#33258;&#22238;&#24402;&#22810;&#27169;&#24577;&#27169;&#22411;: &#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning. (arXiv:2309.02591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02591
&lt;/p&gt;
&lt;p&gt;
CM3Leon&#26159;&#19968;&#20010;&#32553;&#25918;&#33258;&#22238;&#24402;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#29983;&#25104;&#21644;&#22635;&#20805;&#65292;&#36798;&#21040;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#32780;&#35745;&#31639;&#36164;&#28304;&#24320;&#38144;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CM3Leon&#65288;&#21457;&#38899;&#20026;"Chameleon"&#65289;&#65292;&#19968;&#20010;&#26816;&#32034;&#22686;&#24378;&#30340;&#22522;&#20110;&#20196;&#29260;&#30340;&#35299;&#30721;&#22120;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#21644;&#22635;&#20805;&#25991;&#26412;&#21644;&#22270;&#20687;&#12290;CM3Leon&#20351;&#29992;&#20102;CM3&#22810;&#27169;&#24577;&#26550;&#26500;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#22312;&#26356;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#39118;&#26684;&#25968;&#25454;&#19978;&#30340;&#25193;&#23637;&#21644;&#35843;&#25972;&#30340;&#24040;&#22823;&#20248;&#21183;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#20174;&#32431;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#20013;&#25913;&#32534;&#30340;&#37197;&#26041;&#36827;&#34892;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#30340;&#26816;&#32034;&#22686;&#24378;&#39044;&#35757;&#32451;&#38454;&#27573;&#21644;&#31532;&#20108;&#20010;&#22810;&#20219;&#21153;&#30417;&#30563;&#24494;&#35843;&#38454;&#27573;&#12290;&#23427;&#20063;&#26159;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#24341;&#20837;&#33258;&#21253;&#21547;&#30340;&#23545;&#27604;&#35299;&#30721;&#26041;&#27861;&#65292;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#36755;&#20986;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20010;&#37197;&#26041;&#23545;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#38750;&#24120;&#26377;&#25928;&#12290;CM3Leon&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35757;&#32451;&#35745;&#31639;&#37327;&#27604;&#31867;&#20284;&#26041;&#27861;&#23569;5&#20493;&#65288;&#38646;&#26679;&#26412;MS-COCO FID&#65289;
&lt;/p&gt;
&lt;p&gt;
We present CM3Leon (pronounced "Chameleon"), a retrieval-augmented, token-based, decoder-only multi-modal language model capable of generating and infilling both text and images. CM3Leon uses the CM3 multi-modal architecture but additionally shows the extreme benefits of scaling up and tuning on more diverse instruction-style data. It is the first multi-modal model trained with a recipe adapted from text-only language models, including a large-scale retrieval-augmented pre-training stage and a second multi-task supervised fine-tuning (SFT) stage. It is also a general-purpose model that can do both text-to-image and image-to-text generation, allowing us to introduce self-contained contrastive decoding methods that produce high-quality outputs. Extensive experiments demonstrate that this recipe is highly effective for multi-modal models. CM3Leon achieves state-of-the-art performance in text-to-image generation with 5x less training compute than comparable methods (zero-shot MS-COCO FID o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#28304;&#21477;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#27979;&#35797;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#27979;&#35797;&#32467;&#26524;&#19982;&#20256;&#32479;&#20934;&#30830;&#29575;&#24230;&#37327;&#23384;&#22312;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#21487;&#35266;&#23519;&#21040;&#19968;&#33268;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.02553</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#32763;&#35793;&#30340;&#34892;&#20026;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Automating Behavioral Testing in Machine Translation. (arXiv:2309.02553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#28304;&#21477;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#27979;&#35797;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#27979;&#35797;&#32467;&#26524;&#19982;&#20256;&#32479;&#20934;&#30830;&#29575;&#24230;&#37327;&#23384;&#22312;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#21487;&#35266;&#23519;&#21040;&#19968;&#33268;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#20013;&#30340;&#34892;&#20026;&#27979;&#35797;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;-&#36755;&#20986;&#34892;&#20026;&#26469;&#32454;&#31890;&#24230;&#35780;&#20272;&#31995;&#32479;&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#26426;&#22120;&#32763;&#35793;&#20013;&#34892;&#20026;&#27979;&#35797;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#27979;&#35797;&#33539;&#22260;&#26377;&#38480;&#12289;&#28085;&#30422;&#30340;&#35821;&#35328;&#31181;&#31867;&#20063;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#28304;&#21477;&#23376;&#65292;&#20197;&#27979;&#35797;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22791;&#36873;&#38598;&#65292;&#20197;&#39564;&#35777;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#26159;&#21542;&#34920;&#29616;&#20986;&#39044;&#26399;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#20351;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#34892;&#20026;&#27979;&#35797;&#23454;&#38469;&#21487;&#34892;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;&#26368;&#23569;&#30340;&#20154;&#21147;&#25237;&#20837;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#35780;&#20272;&#26694;&#26550;&#24212;&#29992;&#20110;&#22810;&#20010;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#24635;&#20307;&#19978;&#36890;&#36807;&#29575;&#19982;&#20256;&#32479;&#20934;&#30830;&#29575;&#24230;&#37327;&#21487;&#35266;&#23519;&#21040;&#30340;&#36235;&#21183;&#30456;&#31526;&#65292;&#20294;&#20173;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavioral testing in NLP allows fine-grained evaluation of systems by examining their linguistic capabilities through the analysis of input-output behavior. Unfortunately, existing work on behavioral testing in Machine Translation (MT) is currently restricted to largely handcrafted tests covering a limited range of capabilities and languages. To address this limitation, we propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations. We can then verify whether the MT model exhibits the expected behavior through matching candidate sets that are also generated using LLMs. Our approach aims to make behavioral testing of MT systems practical while requiring only minimal human effort. In our experiments, we apply our proposed evaluation framework to assess multiple available MT systems, revealing that while in general pass-rates follow the trends observable from traditional accuracy-based metri
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#35821;&#38899;&#35268;&#21017;&#35760;&#24518;&#30340;&#26368;&#23567;&#26377;&#25928;&#29702;&#35770;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23616;&#37096;&#36830;&#25509;&#30340;&#24352;&#37327;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#23616;&#37096;&#35821;&#38899;&#30456;&#20851;&#24615;&#26469;&#20419;&#36827;&#21475;&#35821;&#21333;&#35789;&#30340;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.02466</link><description>&lt;p&gt;
&#35821;&#38899;&#35268;&#21017;&#35760;&#24518;&#30340;&#26368;&#23567;&#26377;&#25928;&#29702;&#35770;&#65306;&#25429;&#25417;&#30001;&#35821;&#38899;&#38169;&#35823;&#24341;&#36215;&#30340;&#23616;&#37096;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Minimal Effective Theory for Phonotactic Memory: Capturing Local Correlations due to Errors in Speech. (arXiv:2309.02466v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02466
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#35821;&#38899;&#35268;&#21017;&#35760;&#24518;&#30340;&#26368;&#23567;&#26377;&#25928;&#29702;&#35770;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23616;&#37096;&#36830;&#25509;&#30340;&#24352;&#37327;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#23616;&#37096;&#35821;&#38899;&#30456;&#20851;&#24615;&#26469;&#20419;&#36827;&#21475;&#35821;&#21333;&#35789;&#30340;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#35821;&#35328;&#30340;&#28436;&#21270;&#21463;&#38480;&#20110;&#35821;&#38899;&#32463;&#27982;&#24615;&#65292;&#36825;&#21462;&#20915;&#20110;&#20154;&#31867;&#21475;&#33108;&#30340;&#32467;&#26500;&#31561;&#22240;&#32032;&#12290;&#36825;&#23548;&#33268;&#20102;&#21475;&#35821;&#21333;&#35789;&#20013;&#30340;&#23616;&#37096;&#35821;&#38899;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23616;&#37096;&#36830;&#25509;&#30340;&#24352;&#37327;&#32593;&#32476;&#27169;&#22411;&#26469;&#35777;&#26126;&#65292;&#36825;&#20123;&#23616;&#37096;&#20851;&#32852;&#36890;&#36807;&#20943;&#23569;&#21475;&#35821;&#21333;&#35789;&#30340;&#20449;&#24687;&#20869;&#23481;&#26469;&#20419;&#36827;&#21475;&#35821;&#21333;&#35789;&#30340;&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#21463;&#21040;&#20102;&#35768;&#22810;&#20307;&#29289;&#29702;&#23398;&#20013;&#20351;&#29992;&#30340;&#31867;&#20284;&#21464;&#20998;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#21033;&#29992;&#36825;&#20123;&#23616;&#37096;&#35821;&#38899;&#30456;&#20851;&#24615;&#26469;&#20419;&#36827;&#21475;&#35821;&#21333;&#35789;&#30340;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#35813;&#27169;&#22411;&#26159;&#35821;&#38899;&#35760;&#24518;&#30340;&#26368;&#23567;&#27169;&#22411;&#65292;"&#23398;&#20064;&#21457;&#38899;"&#21644;"&#23398;&#20064;&#19968;&#20010;&#21333;&#35789;"&#26159;&#21516;&#19968;&#22238;&#20107;&#12290;&#20854;&#32467;&#26524;&#26159;&#65292;&#23398;&#20250;&#20135;&#29983;&#23545;&#30446;&#26631;&#35821;&#35328;&#26469;&#35828;&#22312;&#35821;&#38899;&#19978;&#21512;&#29702;&#30340;&#26032;&#21333;&#35789;&#65307;&#24182;&#19988;&#25552;&#20379;&#20102;&#22312;&#35821;&#38899;&#25805;&#20316;&#36807;&#31243;&#20013;&#21487;&#33021;&#20135;&#29983;&#30340;&#26368;&#26377;&#21487;&#33021;&#30340;&#38169;&#35823;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#25105;&#20204;&#23545;&#25289;&#19969;&#35821;&#21644;&#22303;&#32819;&#20854;&#35821;&#21333;&#35789;&#36827;&#34892;&#20102;&#27169;&#22411;&#27979;&#35797;&#12290;&#65288;&#20195;&#30721;&#21487;&#22312;Gi&#19978;&#25214;&#21040;&#65289;
&lt;/p&gt;
&lt;p&gt;
Spoken language evolves constrained by the economy of speech, which depends on factors such as the structure of the human mouth. This gives rise to local phonetic correlations in spoken words. Here we demonstrate that these local correlations facilitate the learning of spoken words by reducing their information content. We do this by constructing a locally-connected tensor-network model, inspired by similar variational models used for many-body physics, which exploits these local phonetic correlations to facilitate the learning of spoken words. The model is therefore a minimal model of phonetic memory, where "learning to pronounce" and "learning a word" are one and the same. A consequence of which is the learned ability to produce new words which are phonetically reasonable for the target language; as well as providing a hierarchy of the most likely errors that could be produced during the action of speech. We test our model against Latin and Turkish words. (The code is available on Gi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#19977;&#32500;&#25171;&#21360;&#30340;G&#20195;&#30721;&#25991;&#20214;&#25552;&#20986;&#20102;&#20845;&#31181;&#22522;&#30784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#35780;&#20272;&#23427;&#20204;&#22312;G&#20195;&#30721;&#35843;&#35797;&#21644;&#25805;&#20316;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#20197;&#21450;&#20960;&#20309;&#21464;&#25442;&#31561;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#24212;&#29992;&#20110;&#22686;&#26448;&#21046;&#36896;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.02465</link><description>&lt;p&gt;
&#38754;&#21521;&#22686;&#26448;&#21046;&#36896;&#30340;&#22522;&#30784;AI&#27169;&#22411;&#65306;&#29992;&#20110;G&#20195;&#30721;&#35843;&#35797;&#12289;&#25805;&#20316;&#21644;&#29702;&#35299;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Foundational AI Models for Additive Manufacturing: Language Models for G-Code Debugging, Manipulation, and Comprehension. (arXiv:2309.02465v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19977;&#32500;&#25171;&#21360;&#30340;G&#20195;&#30721;&#25991;&#20214;&#25552;&#20986;&#20102;&#20845;&#31181;&#22522;&#30784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#35780;&#20272;&#23427;&#20204;&#22312;G&#20195;&#30721;&#35843;&#35797;&#21644;&#25805;&#20316;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#20197;&#21450;&#20960;&#20309;&#21464;&#25442;&#31561;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#24212;&#29992;&#20110;&#22686;&#26448;&#21046;&#36896;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#25171;&#21360;&#25110;&#22686;&#26448;&#21046;&#36896;&#26159;&#19968;&#39033;&#38761;&#21629;&#24615;&#30340;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#20174;&#25968;&#23383;&#27169;&#22411;&#20013;&#21019;&#24314;&#29289;&#29702;&#23545;&#35937;&#12290;&#28982;&#32780;&#65292;&#19977;&#32500;&#25171;&#21360;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#21462;&#20915;&#20110;G&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;G&#20195;&#30721;&#26159;&#19968;&#31181;&#20302;&#32423;&#25968;&#25511;&#32534;&#31243;&#35821;&#35328;&#65292;&#25351;&#23548;&#19977;&#32500;&#25171;&#21360;&#26426;&#22914;&#20309;&#31227;&#21160;&#21644;&#25380;&#20986;&#26448;&#26009;&#12290;&#35843;&#35797;G&#20195;&#30721;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#38656;&#35201;&#23545;G&#20195;&#30721;&#26684;&#24335;&#21644;&#25152;&#25171;&#21360;&#38646;&#20214;&#30340;&#20960;&#20309;&#24418;&#29366;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20845;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#39318;&#27425;&#24191;&#27867;&#35780;&#20272;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35843;&#35797;&#19977;&#32500;&#25171;&#21360;&#30340;G&#20195;&#30721;&#25991;&#20214;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#26377;&#25928;&#30340;&#25552;&#31034;&#65292;&#20351;&#39044;&#35757;&#32451;&#30340;LLMs&#33021;&#22815;&#29702;&#35299;&#21644;&#25805;&#20316;G&#20195;&#30721;&#65292;&#24182;&#22312;G&#20195;&#30721;&#35843;&#35797;&#21644;&#25805;&#20316;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#26816;&#27979;&#21644;&#20462;&#27491;&#24120;&#35265;&#38169;&#35823;&#20197;&#21450;&#25191;&#34892;&#20960;&#20309;&#21464;&#25442;&#26041;&#38754;&#27979;&#35797;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#23427;&#20204;&#30340;&#20248;&#28857;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D printing or additive manufacturing is a revolutionary technology that enables the creation of physical objects from digital models. However, the quality and accuracy of 3D printing depend on the correctness and efficiency of the G-code, a low-level numerical control programming language that instructs 3D printers how to move and extrude material. Debugging G-code is a challenging task that requires a syntactic and semantic understanding of the G-code format and the geometry of the part to be printed. In this paper, we present the first extensive evaluation of six state-of-the-art foundational large language models (LLMs) for comprehending and debugging G-code files for 3D printing. We design effective prompts to enable pre-trained LLMs to understand and manipulate G-code and test their performance on various aspects of G-code debugging and manipulation, including detection and correction of common errors and the ability to perform geometric transformations. We analyze their strength
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19979;&#37319;&#26679;&#22768;&#23398;&#34920;&#31034;&#26469;&#23545;&#40784;&#25991;&#26412;&#27169;&#24577;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#32431;&#25991;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26032;&#39046;&#22495;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.02459</link><description>&lt;p&gt;
&#36890;&#36807;&#19979;&#37319;&#26679;&#30340;&#22768;&#23398;&#34920;&#31034;&#36827;&#34892;&#32431;&#25991;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation. (arXiv:2309.02459v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19979;&#37319;&#26679;&#22768;&#23398;&#34920;&#31034;&#26469;&#23545;&#40784;&#25991;&#26412;&#27169;&#24577;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#32431;&#25991;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26032;&#39046;&#22495;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20004;&#31181;&#24418;&#24335;&#30340;&#36164;&#26009;&#65292;&#22768;&#38899;&#21644;&#25991;&#26412;&#65292;&#26144;&#23556;&#21040;&#20849;&#20139;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#26159;&#19968;&#31181;&#21033;&#29992;&#32431;&#25991;&#26412;&#25968;&#25454;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#24615;&#33021;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#22768;&#38899;&#21644;&#25991;&#26412;&#30340;&#34920;&#31034;&#38271;&#24230;&#19981;&#19968;&#33268;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#19978;&#37319;&#26679;&#25991;&#26412;&#34920;&#31034;&#26469;&#19982;&#38899;&#39057;&#27169;&#24577;&#36827;&#34892;&#23545;&#40784;&#65292;&#20294;&#21487;&#33021;&#19981;&#21305;&#37197;&#39044;&#26399;&#30340;&#23454;&#38469;&#25345;&#32493;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#19979;&#37319;&#26679;&#22768;&#23398;&#34920;&#31034;&#26469;&#19982;&#25991;&#26412;&#27169;&#24577;&#23545;&#40784;&#30340;&#26032;&#22411;&#34920;&#31034;&#21305;&#37197;&#31574;&#30053;&#12290;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#31215;&#20998;-&#28779;&#28846; (CIF) &#27169;&#22359;&#29983;&#25104;&#19982;&#26631;&#35760;&#38271;&#24230;&#19968;&#33268;&#30340;&#22768;&#23398;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;ASR&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#20174;&#20004;&#31181;&#27169;&#24577;&#20013;&#23398;&#20064;&#32479;&#19968;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#33021;&#22815;&#20351;&#29992;&#30446;&#26631;&#39046;&#22495;&#30340;&#32431;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#26032;&#39046;&#22495;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mapping two modalities, speech and text, into a shared representation space, is a research topic of using text-only data to improve end-to-end automatic speech recognition (ASR) performance in new domains. However, the length of speech representation and text representation is inconsistent. Although the previous method up-samples the text representation to align with acoustic modality, it may not match the expected actual duration. In this paper, we proposed novel representations match strategy through down-sampling acoustic representation to align with text modality. By introducing a continuous integrate-and-fire (CIF) module generating acoustic representations consistent with token length, our ASR model can learn unified representations from both modalities better, allowing for domain adaptation using text-only data of the target domain. Experiment results of new domain data demonstrate the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22522;&#20110;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#26367;&#20195;&#35789;&#26469;&#27979;&#37327;&#35821;&#20041;&#21464;&#21270;&#65292;&#19981;&#20165;&#30452;&#35266;&#26131;&#25026;&#65292;&#32780;&#19988;&#23384;&#20648;&#25928;&#29575;&#26356;&#39640;&#65292;&#22312;&#19968;&#20123;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#33021;&#22815;&#36827;&#34892;&#26356;&#32454;&#33268;&#30340;&#21464;&#21270;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.02403</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#26367;&#20195;&#35789;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Substitution-based Semantic Change Detection using Contextual Embeddings. (arXiv:2309.02403v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02403
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#26367;&#20195;&#35789;&#26469;&#27979;&#37327;&#35821;&#20041;&#21464;&#21270;&#65292;&#19981;&#20165;&#30452;&#35266;&#26131;&#25026;&#65292;&#32780;&#19988;&#23384;&#20648;&#25928;&#29575;&#26356;&#39640;&#65292;&#22312;&#19968;&#20123;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#33021;&#22815;&#36827;&#34892;&#26356;&#32454;&#33268;&#30340;&#21464;&#21270;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36804;&#20170;&#20026;&#27490;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#26041;&#27861;&#22312;&#27979;&#37327;&#35821;&#20041;&#21464;&#21270;&#26041;&#38754;&#19968;&#30452;&#38590;&#20197;&#25913;&#36827;&#27604;&#31616;&#21333;&#30340;&#20165;&#20381;&#36182;&#20110;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#20043;&#21069;&#25552;&#20986;&#30340;&#35768;&#22810;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#35299;&#37322;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#23884;&#20837;&#27979;&#37327;&#35821;&#20041;&#21464;&#21270;&#65292;&#20165;&#20381;&#36182;&#20110;&#23545;&#25513;&#30721;&#35789;&#26368;&#21487;&#33021;&#30340;&#26367;&#20195;&#35789;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#30452;&#25509;&#35299;&#37322;&#65292;&#32780;&#19988;&#22312;&#23384;&#20648;&#26041;&#38754;&#26356;&#20026;&#39640;&#25928;&#65292;&#22312;&#26368;&#24120;&#24341;&#29992;&#30340;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#22312;&#21464;&#21270;&#35299;&#37322;&#26041;&#38754;&#27604;&#38745;&#24577;&#35789;&#21521;&#37327;&#26356;&#20026;&#32454;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring semantic change has thus far remained a task where methods using contextual embeddings have struggled to improve upon simpler techniques relying only on static word vectors. Moreover, many of the previously proposed approaches suffer from downsides related to scalability and ease of interpretation. We present a simplified approach to measuring semantic change using contextual embeddings, relying only on the most probable substitutes for masked terms. Not only is this approach directly interpretable, it is also far more efficient in terms of storage, achieves superior average performance across the most frequently cited datasets for this task, and allows for more nuanced investigation of change than is possible with static word vectors.
&lt;/p&gt;</description></item><item><title>CodeApex&#26159;&#19968;&#20010;&#21452;&#35821;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22810;&#20010;&#36873;&#25321;&#39064;&#21644;&#31639;&#27861;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;14&#20010;LLM&#30340;&#32534;&#31243;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.01940</link><description>&lt;p&gt;
CodeApex&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21452;&#35821;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models. (arXiv:2309.01940v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01940
&lt;/p&gt;
&lt;p&gt;
CodeApex&#26159;&#19968;&#20010;&#21452;&#35821;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22810;&#20010;&#36873;&#25321;&#39064;&#21644;&#31639;&#27861;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;14&#20010;LLM&#30340;&#32534;&#31243;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#27169;&#22411;&#30340;&#32534;&#31243;&#33021;&#21147;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#21560;&#24341;&#20102;&#30740;&#31350;&#20154;&#21592;&#26085;&#30410;&#22686;&#38271;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CodeApex&#65292;&#19968;&#31181;&#21452;&#35821;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#19987;&#27880;&#20110;LLM&#30340;&#32534;&#31243;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;CodeApex&#21253;&#25324;&#19977;&#31181;&#31867;&#22411;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#65306;&#27010;&#24565;&#29702;&#35299;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#22810;&#36339;&#25512;&#29702;&#65292;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#32534;&#31243;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;CodeApex&#21033;&#29992;&#31639;&#27861;&#38382;&#39064;&#21644;&#30456;&#24212;&#30340;&#27979;&#35797;&#29992;&#20363;&#26469;&#35780;&#20272;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#36136;&#37327;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;14&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#65292;&#21253;&#25324;&#36890;&#29992;&#21644;&#19987;&#38376;&#21270;&#27169;&#22411;&#12290;GPT&#23637;&#29616;&#20986;&#26368;&#20339;&#30340;&#32534;&#31243;&#33021;&#21147;&#65292;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#36798;&#21040;&#20102;&#32422;50%&#21644;56%&#12290;&#32534;&#31243;&#20219;&#21153;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#25105;&#20204;&#24076;&#26395;CodeApex&#33021;&#22815;&#20026;&#35780;&#20272;&#32534;&#31243;&#33021;&#21147;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of Large Language Models (LLMs), there has been a significant improvement in the programming capabilities of models, attracting growing attention from researchers. We propose CodeApex, a bilingual benchmark dataset focusing on the programming comprehension and code generation abilities of LLMs. CodeApex comprises three types of multiple-choice questions: conceptual understanding, commonsense reasoning, and multi-hop reasoning, designed to evaluate LLMs on programming comprehension tasks. Additionally, CodeApex utilizes algorithmic questions and corresponding test cases to assess the code quality generated by LLMs. We evaluate 14 state-of-the-art LLMs, including both general-purpose and specialized models. GPT exhibits the best programming capabilities, achieving approximate accuracies of 50% and 56% on the two tasks, respectively. There is still significant room for improvement in programming tasks. We hope that CodeApex can serve as a reference for evaluating the co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#20809;&#27969;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#30456;&#32467;&#21512;&#65292;&#20016;&#23500;&#20102;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#27969;&#31243;&#20013;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#22312;&#25163;&#35821;&#35782;&#21035;&#20219;&#21153;&#20013;&#38477;&#20302;&#20102;WER 0.9&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27979;&#35797;&#38598;&#19978;&#22823;&#22810;&#25968;BLEU&#20998;&#25968;&#32422;0.6&#12290;</title><link>http://arxiv.org/abs/2309.01860</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#65306;&#22686;&#24378;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation. (arXiv:2309.01860v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#20809;&#27969;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#30456;&#32467;&#21512;&#65292;&#20016;&#23500;&#20102;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#27969;&#31243;&#20013;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#22312;&#25163;&#35821;&#35782;&#21035;&#20219;&#21153;&#20013;&#38477;&#20302;&#20102;WER 0.9&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27979;&#35797;&#38598;&#19978;&#22823;&#22810;&#25968;BLEU&#20998;&#25968;&#32422;0.6&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#29992;&#20110;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#19982;&#29616;&#26377;&#30340;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#27969;&#31243;&#30456;&#32467;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#20809;&#27969;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#32467;&#21512;&#65292;&#20197;&#20016;&#23500;&#20855;&#26377;&#19982;&#36816;&#21160;&#30456;&#20851;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;&#35813;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30740;&#31350;&#20102;&#36825;&#31181;&#27169;&#24577;&#21253;&#21547;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#30340;&#25554;&#20214;&#38750;&#24120;&#36731;&#37327;&#32423;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20026;&#26032;&#27169;&#24577;&#21253;&#25324;&#19968;&#20010;&#21333;&#29420;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#25105;&#20204;&#22312;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#20013;&#24212;&#29992;&#20102;&#36825;&#20123;&#25913;&#21464;&#65292;&#25913;&#21892;&#20102;&#27599;&#20010;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;RWTH-PHOENIX-2014&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#24615;&#33021;&#65292;&#29992;&#20110;&#25163;&#35821;&#35782;&#21035;&#65292;&#24182;&#22312;RWTH-PHOENIX-2014T&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#32763;&#35793;&#20219;&#21153;&#12290;&#22312;&#35782;&#21035;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;WER&#38477;&#20302;&#20102;0.9&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22823;&#37096;&#20998;BLEU&#20998;&#25968;&#22312;&#27979;&#35797;&#38598;&#19978;&#25552;&#39640;&#20102;&#32422;0.6&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we devise a mechanism for the addition of multi-modal information with an existing pipeline for continuous sign language recognition and translation. In our procedure, we have incorporated optical flow information with RGB images to enrich the features with movement-related information. This work studies the feasibility of such modality inclusion using a cross-modal encoder. The plugin we have used is very lightweight and doesn't need to include a separate feature extractor for the new modality in an end-to-end manner. We have applied the changes in both sign language recognition and translation, improving the result in each case. We have evaluated the performance on the RWTH-PHOENIX-2014 dataset for sign language recognition and the RWTH-PHOENIX-2014T dataset for translation. On the recognition task, our approach reduced the WER by 0.9, and on the translation task, our approach increased most of the BLEU scores by ~0.6 on the test set.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#24615;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#65292;&#22312;COVID-19 CT&#25195;&#25551;&#21644;&#38750;&#26631;&#20934;&#21270;&#25253;&#21578;&#20013;&#24212;&#29992;&#38646;&#26679;&#26412;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#20197;&#21457;&#29616;&#32954;&#26643;&#22622;&#21644;&#32454;&#24494;&#30340;&#32954;&#37096;&#32454;&#33410;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#21457;&#23637;&#26426;&#36935;&#12290;</title><link>http://arxiv.org/abs/2309.01740</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#22810;&#26631;&#31614;&#20998;&#31867; COVID-19 CT&#25195;&#25551;&#21644;&#38750;&#26631;&#20934;&#21270;&#25253;&#21578;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Empirical Analysis for Zero-Shot Multi-Label Classification on COVID-19 CT Scans and Uncurated Reports. (arXiv:2309.01740v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#24615;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#65292;&#22312;COVID-19 CT&#25195;&#25551;&#21644;&#38750;&#26631;&#20934;&#21270;&#25253;&#21578;&#20013;&#24212;&#29992;&#38646;&#26679;&#26412;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#20197;&#21457;&#29616;&#32954;&#26643;&#22622;&#21644;&#32454;&#24494;&#30340;&#32954;&#37096;&#32454;&#33410;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#21457;&#23637;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27969;&#34892;&#23548;&#33268;&#20102;&#21253;&#25324;&#21307;&#23398;&#26816;&#26597;&#22686;&#21152;&#22312;&#20869;&#30340;&#22823;&#37327;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#21253;&#25324;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#23613;&#31649;&#19982;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#30456;&#27604;&#65292;X&#23556;&#32447;&#22270;&#20687;&#30340;&#31934;&#30830;&#24230;&#36739;&#20302;&#65292;&#20294;&#20197;&#24448;&#20851;&#20110; COVID-19 &#30340;&#33258;&#21160;&#35786;&#26029;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312; X&#23556;&#32447;&#22270;&#20687;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21307;&#38498;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#21033;&#29992; CT &#25195;&#25551;&#25552;&#20379;&#30340;&#32454;&#33410;&#36827;&#34892;&#22522;&#20110;&#23545;&#27604;&#24615;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#19982;&#20154;&#31867;&#19987;&#23478;&#21512;&#20316;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22810;&#31181;&#38646;&#26679;&#26412;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#24110;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#26816;&#27979;&#32954;&#26643;&#22622;&#65292;&#24182;&#35782;&#21035;&#35832;&#22914;&#22320;&#29627;&#29827;&#29366;&#27985;&#27978;&#21644;&#23454;&#21464;&#31561;&#32454;&#24494;&#30340;&#32954;&#37096;&#32454;&#33410;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#25552;&#20379;&#20102;&#30446;&#21069;&#22312;&#21307;&#23398;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#25991;&#29486;&#20013;&#34987;&#24573;&#35270;&#30340;&#35299;&#20915;&#36825;&#20123;&#32454;&#31890;&#24230;&#20219;&#21153;&#30340;&#21487;&#33021;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#30340;&#26410;&#26469;&#21457;&#23637;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pandemic resulted in vast repositories of unstructured data, including radiology reports, due to increased medical examinations. Previous research on automated diagnosis of COVID-19 primarily focuses on X-ray images, despite their lower precision compared to computed tomography (CT) scans. In this work, we leverage unstructured data from a hospital and harness the fine-grained details offered by CT scans to perform zero-shot multi-label classification based on contrastive visual language learning. In collaboration with human experts, we investigate the effectiveness of multiple zero-shot models that aid radiologists in detecting pulmonary embolisms and identifying intricate lung details like ground glass opacities and consolidations. Our empirical analysis provides an overview of the possible solutions to target such fine-grained tasks, so far overlooked in the medical multimodal pretraining literature. Our investigation promises future advancements in the medical image analysis co
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.00424</link><description>&lt;p&gt;
CPSP: &#20174;&#38899;&#32032;&#30417;&#30563;&#20013;&#23398;&#20064;&#35821;&#38899;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
CPSP: Learning Speech Concepts From Phoneme Supervision. (arXiv:2309.00424v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00424
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35832;&#22914;&#26368;&#23567;&#30417;&#30563;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#12289;&#35821;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31561;&#32454;&#31890;&#24230;&#29983;&#25104;&#21644;&#35782;&#21035;&#20219;&#21153;&#65292;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#30340;&#20013;&#38388;&#34920;&#31034;&#24212;&#21253;&#21547;&#20171;&#20110;&#25991;&#26412;&#32534;&#30721;&#21644;&#22768;&#23398;&#32534;&#30721;&#20043;&#38388;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#20869;&#23481;&#31361;&#20986;&#65292;&#32780;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#31561;&#35821;&#38899;&#20449;&#24687;&#24212;&#35813;&#34987;&#21435;&#38500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#20013;&#38388;&#34920;&#31034;&#30340;&#26041;&#27861;&#23384;&#22312;&#20887;&#20313;&#24615;&#36807;&#39640;&#21644;&#32500;&#24230;&#29190;&#28856;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#38899;&#39057;&#39046;&#22495;&#20013;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#25552;&#21462;&#29992;&#20110;&#19979;&#28216;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#20840;&#23616;&#25551;&#36848;&#20449;&#24687;&#65292;&#19981;&#36866;&#21512;TTS&#12289;VC&#21644;ASR&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#38899;&#32032;-&#35821;&#38899;&#39044;&#35757;&#32451;&#65288;CPSP&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19977;&#20010;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#35299;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#23558;&#38899;&#32032;&#21644;&#35821;&#38899;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
For fine-grained generation and recognition tasks such as minimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), the intermediate representation extracted from speech should contain information that is between text coding and acoustic coding. The linguistic content is salient, while the paralinguistic information such as speaker identity and acoustic details should be removed. However, existing methods for extracting fine-grained intermediate representations from speech suffer from issues of excessive redundancy and dimension explosion. Additionally, existing contrastive learning methods in the audio field focus on extracting global descriptive information for downstream audio classification tasks, making them unsuitable for TTS, VC, and ASR tasks. To address these issues, we propose a method named Contrastive Phoneme-Speech Pretraining (CPSP), which uses three encoders, one decoder, and contrastive learning to bring phoneme and speech
&lt;/p&gt;</description></item><item><title>BatchPrompt&#26159;&#19968;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#23558;&#22810;&#20010;&#25968;&#25454;&#28857;&#25209;&#37327;&#25171;&#21253;&#21040;&#19968;&#20010;&#25552;&#31034;&#20013;&#26469;&#25552;&#39640;LLM&#30340;&#20196;&#29260;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#20174;&#32780;&#32531;&#35299;&#30001;&#20110;&#20196;&#29260;&#35745;&#25968;&#24046;&#24322;&#23548;&#33268;&#30340;&#25104;&#26412;&#25928;&#29575;&#38382;&#39064;&#65292;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#21644;&#35745;&#31639;&#39044;&#31639;&#30340;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.00384</link><description>&lt;p&gt;
BatchPrompt: &#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#23454;&#29616;&#26356;&#22810;&#20219;&#21153;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
BatchPrompt: Accomplish more with less. (arXiv:2309.00384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00384
&lt;/p&gt;
&lt;p&gt;
BatchPrompt&#26159;&#19968;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#23558;&#22810;&#20010;&#25968;&#25454;&#28857;&#25209;&#37327;&#25171;&#21253;&#21040;&#19968;&#20010;&#25552;&#31034;&#20013;&#26469;&#25552;&#39640;LLM&#30340;&#20196;&#29260;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#20174;&#32780;&#32531;&#35299;&#30001;&#20110;&#20196;&#29260;&#35745;&#25968;&#24046;&#24322;&#23548;&#33268;&#30340;&#25104;&#26412;&#25928;&#29575;&#38382;&#39064;&#65292;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#21644;&#35745;&#31639;&#39044;&#31639;&#30340;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;LLM&#65288;Language Model&#65289;&#34987;&#35757;&#32451;&#26469;&#20351;&#29992;&#22522;&#20110;&#25351;&#20196;&#30340;&#25552;&#31034;&#23454;&#29616;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#25512;&#29702;&#12290;&#20026;&#36825;&#20123;LLM&#21046;&#20316;&#25552;&#31034;&#36890;&#24120;&#38656;&#35201;&#29992;&#25143;&#25552;&#20379;&#35814;&#32454;&#30340;&#20219;&#21153;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#21644;&#23436;&#25104;&#31034;&#20363;&#20197;&#21450;&#25512;&#29702;&#19978;&#19979;&#25991;&#30340;&#21333;&#20010;&#31034;&#20363;&#12290;&#26412;&#25991;&#23558;&#36825;&#31181;&#24120;&#35268;&#25552;&#31034;&#22522;&#20934;&#31216;&#20026;SinglePrompt&#12290;&#28982;&#32780;&#65292;&#22312;&#27599;&#20010;&#25512;&#29702;&#25968;&#25454;&#28857;&#19981;&#19968;&#23450;&#24456;&#38271;&#30340;NLP&#20219;&#21153;&#20013;&#65292;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#21644;&#23569;&#26679;&#26412;&#31034;&#20363;&#30340;&#20196;&#29260;&#35745;&#25968;&#21487;&#33021;&#27604;&#25968;&#25454;&#28857;&#30340;&#20196;&#29260;&#35745;&#25968;&#22823;&#24471;&#22810;&#65292;&#19982;Fine-tuned BERT&#31561;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#23548;&#33268;&#20196;&#29260;&#36164;&#28304;&#21033;&#29992;&#29575;&#38477;&#20302;&#12290;&#36825;&#20010;&#25104;&#26412;&#25928;&#29575;&#38382;&#39064;&#24433;&#21709;&#20102;&#25512;&#29702;&#36895;&#24230;&#21644;&#35745;&#31639;&#39044;&#31639;&#65292;&#25269;&#28040;&#20102;LLM&#25152;&#33021;&#25552;&#20379;&#30340;&#35768;&#22810;&#22909;&#22788;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23558;&#22810;&#20010;&#25968;&#25454;&#28857;&#25209;&#37327;&#25171;&#21253;&#21040;&#19968;&#20010;&#25552;&#31034;&#20013;&#26469;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#25552;&#31034;&#31574;&#30053;&#31216;&#20026;BatchPrompt&#12290;&#36825;&#31181;&#31574;&#30053;&#22686;&#21152;&#20102;&#25968;&#25454;&#28857;&#30340;&#23494;&#24230;&#65292;
&lt;/p&gt;
&lt;p&gt;
Many LLMs are trained to perform zero-shot or few-shot inference using instruction-based prompts. Crafting prompts for these LLMs typically requires the user to provide a detailed task description, examples of context and completion, and single example of context for inference. This regular prompt baseline is referred to as SinglePrompt in this paper. However, for NLP tasks where each data point for inference is not necessarily lengthy, the token count for instructions and few-shot examples in the prompt may be considerably larger than that of the data point, resulting in lower token-resource utilization compared with encoder-based models like fine-tuned BERT. This cost-efficiency issue, affecting inference speed and compute budget, counteracts the many benefits LLMs have to offer. This paper aims to alleviate the preceding problem by batching multiple data points into a single prompt, a prompting strategy we refer to as BatchPrompt. This strategy increases the density of data points, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#29983;&#25104;&#35843;&#30740;&#25991;&#31456;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;GPT-4&#20248;&#20110;GPT-3.5&#65292;&#24182;&#19988;&#25351;&#20986;&#20102;GPT&#22312;&#20449;&#24687;&#23436;&#25972;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#19968;&#20123;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2308.10410</link><description>&lt;p&gt;
&#22522;&#20110;&#32500;&#22522;&#30334;&#31185;&#39118;&#26684;&#30340;&#35843;&#30740;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27010;&#24565;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Large Language Models on Wikipedia-Style Survey Generation: an Evaluation in NLP Concepts. (arXiv:2308.10410v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#29983;&#25104;&#35843;&#30740;&#25991;&#31456;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;GPT-4&#20248;&#20110;GPT-3.5&#65292;&#24182;&#19988;&#25351;&#20986;&#20102;GPT&#22312;&#20449;&#24687;&#23436;&#25972;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#19968;&#20123;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#21151;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#12290;&#34429;&#28982;LLMs&#22312;&#19968;&#33324;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#29305;&#23450;&#39046;&#22495;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#20173;&#22312;&#25506;&#32034;&#20013;&#12290;&#27492;&#22806;&#65292;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26377;&#26102;&#20250;&#20986;&#29616;&#24187;&#35273;&#21644;&#19981;&#23454;&#20449;&#24687;&#31561;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;-NLP&#39046;&#22495;&#20013;&#29983;&#25104;&#31616;&#27905;&#35843;&#30740;&#25991;&#31456;&#30340;&#33021;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;20&#20010;&#36873;&#23450;&#30340;&#20027;&#39064;&#12290;&#33258;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;GPT-4&#22312;&#19982;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26102;&#20248;&#20110;GPT-3.5&#12290;&#27492;&#22806;&#65292;&#22235;&#20301;&#20154;&#31867;&#35780;&#20272;&#32773;&#20174;&#22235;&#20010;&#27169;&#22411;&#37197;&#32622;&#30340;&#20845;&#20010;&#35282;&#24230;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#34429;&#28982;GPT&#36890;&#24120;&#33021;&#20135;&#29983;&#21487;&#31216;&#36190;&#30340;&#32467;&#26524;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#22914;&#20449;&#24687;&#19981;&#23436;&#25972;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved significant success across various natural language processing (NLP) tasks, encompassing question-answering, summarization, and machine translation, among others. While LLMs excel in general tasks, their efficacy in domain-specific applications remains under exploration. Additionally, LLM-generated text sometimes exhibits issues like hallucination and disinformation. In this study, we assess LLMs' capability of producing concise survey articles within the computer science-NLP domain, focusing on 20 chosen topics. Automated evaluations indicate that GPT-4 outperforms GPT-3.5 when benchmarked against the ground truth. Furthermore, four human evaluators provide insights from six perspectives across four model configurations. Through case studies, we demonstrate that while GPT often yields commendable results, there are instances of shortcomings, such as incomplete information and the exhibition of lapses in factual accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#30340;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#21477;&#38405;&#35835;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#27169;&#22411;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21333;&#21477;&#38405;&#35835;&#22120;&#19982;&#20256;&#32479;&#35757;&#32451;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20960;&#20046;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04566</link><description>&lt;p&gt;
&#21333;&#21477;&#38405;&#35835;&#22120;&#65306;&#35299;&#20915;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Single-Sentence Reader: A Novel Approach for Addressing Answer Position Bias. (arXiv:2308.04566v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#30340;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#21477;&#38405;&#35835;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#27169;&#22411;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21333;&#21477;&#38405;&#35835;&#22120;&#19982;&#20256;&#32479;&#35757;&#32451;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20960;&#20046;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#27169;&#22411;&#24448;&#24448;&#21033;&#29992;&#20266;&#30456;&#20851;&#24615;&#65288;&#20063;&#31216;&#20026;&#25968;&#25454;&#38598;&#20559;&#24046;&#25110;&#30740;&#31350;&#30028;&#30340;&#26631;&#27880;&#24037;&#20214;&#65289;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#22312;&#19981;&#23436;&#20840;&#29702;&#35299;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;&#21644;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;MRC&#20219;&#21153;&#65292;&#36825;&#26159;&#19981;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#23548;&#33268;&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#20302;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#35757;&#32451;&#38382;&#39064;&#20013;&#26377;&#30456;&#24403;&#27604;&#20363;&#30340;&#31572;&#26696;&#20165;&#20301;&#20110;&#19978;&#19979;&#25991;&#30340;&#31532;&#19968;&#21477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#21477;&#38405;&#35835;&#22120;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;MRC&#20013;&#30340;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#31181;&#26041;&#27861;&#65292;&#24182;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#21333;&#21477;&#38405;&#35835;&#22120;&#30340;&#32467;&#26524;&#20960;&#20046;&#19982;&#20256;&#32479;&#35757;&#32451;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#21333;&#21477;&#38405;&#35835;&#22120;&#36935;&#21040;&#30340;&#20960;&#20010;&#25361;&#25112;&#21644;&#25552;&#20986;&#30340;&#24212;&#23545;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Reading Comprehension (MRC) models tend to take advantage of spurious correlations (also known as dataset bias or annotation artifacts in the research community). Consequently, these models may perform the MRC task without fully comprehending the given context and question, which is undesirable since it may result in low robustness against distribution shift. This paper delves into the concept of answer-position bias, where a significant percentage of training questions have answers located solely in the first sentence of the context. We propose a Single-Sentence Reader as a new approach for addressing answer position bias in MRC. We implement this approach using six different models and thoroughly analyze their performance. Remarkably, our proposed Single-Sentence Readers achieve results that nearly match those of models trained on conventional training sets, proving their effectiveness. Our study also discusses several challenges our Single-Sentence Readers encounter and prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#38271;&#35270;&#39057;ASR&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#19982;&#26368;&#22823;&#29109;&#22522;&#32447;&#30456;&#27604;&#65292;&#20351;&#29992;LLM&#33021;&#22815;&#26368;&#22810;&#20943;&#23569;8&#65285;&#30340;Word Error Rate&#21644;30&#65285;&#30340;Salient Term Error Rate&#12290;&#32463;&#36807;&#25913;&#36827;&#30340;&#26684;&#22788;&#29702;&#21644;&#25658;&#24102;&#19978;&#19979;&#25991;&#30340;&#32452;&#21512;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.08133</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#24418;&#24335;&#25968;&#25454;&#37325;&#26032;&#35780;&#20998;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large-scale Language Model Rescoring on Long-form Data. (arXiv:2306.08133v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#38271;&#35270;&#39057;ASR&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#19982;&#26368;&#22823;&#29109;&#22522;&#32447;&#30456;&#27604;&#65292;&#20351;&#29992;LLM&#33021;&#22815;&#26368;&#22810;&#20943;&#23569;8&#65285;&#30340;Word Error Rate&#21644;30&#65285;&#30340;Salient Term Error Rate&#12290;&#32463;&#36807;&#25913;&#36827;&#30340;&#26684;&#22788;&#29702;&#21644;&#25658;&#24102;&#19978;&#19979;&#25991;&#30340;&#32452;&#21512;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;YouTube&#35270;&#39057;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#35270;&#39057;&#34987;&#29992;&#20316;&#38271;&#24418;&#24335;ASR&#30340;&#28304;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#32654;&#22269;&#33521;&#35821;&#65288;en-us&#65289;&#21644;&#21360;&#24230;&#33521;&#35821;&#65288;en-in&#65289;&#38271;&#24418;&#24335;ASR&#27979;&#35797;&#38598;&#19978;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#35821;&#35328;&#27169;&#22411;&#24378;&#19968;&#27425;&#36890;&#36807;&#22522;&#32447;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#36798;8&#65285;&#30340;&#30456;&#23545;Word Error Rate&#65288;WER&#65289;&#38477;&#20302;&#21644;&#39640;&#36798;30&#65285;&#30340;&#30456;&#23545;Salient Term Error Rate&#65288;STER&#65289;&#38477;&#20302;&#12290;&#32463;&#36807;&#25913;&#36827;&#30340;&#26684;&#22788;&#29702;&#23548;&#33268;&#24102;&#26377;&#27491;&#30830;&#65288;&#38750;&#26641;&#24418;&#65289;&#26377;&#21521;&#22270;&#25299;&#25169;&#21644;&#25658;&#24102;&#21069;&#19968;&#27573;&#26368;&#20339;&#20551;&#35774;&#30340;&#19978;&#19979;&#25991;&#30340;&#26684;&#30340;&#26174;&#30528;&#33719;&#32988;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22522;&#20110;&#22823;&#37327;&#21487;&#29992;&#25968;&#25454;&#65288;&#22914;C4&#65289;&#30340;LLMs&#21644;&#20256;&#32479;&#31070;&#32463;LMs&#30340;&#32452;&#21512;&#30340;&#24615;&#33021;&#25552;&#21319;&#26159;&#32047;&#21152;&#30340;&#65292;&#24182;&#19988;&#26174;&#30528;&#20248;&#20110;&#20855;&#26377;&#26368;&#22823;&#29109;LM&#30340;&#24378;&#19968;&#27425;&#36890;&#36807;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the impact of Large-scale Language Models (LLM) on Automated Speech Recognition (ASR) of YouTube videos, which we use as a source for long-form ASR. We demonstrate up to 8\% relative reduction in Word Error Eate (WER) on US English (en-us) and code-switched Indian English (en-in) long-form ASR test sets and a reduction of up to 30\% relative on Salient Term Error Rate (STER) over a strong first-pass baseline that uses a maximum-entropy based language model. Improved lattice processing that results in a lattice with a proper (non-tree) digraph topology and carrying context from the 1-best hypothesis of the previous segment(s) results in significant wins in rescoring with LLMs. We also find that the gains in performance from the combination of LLMs trained on vast quantities of available data (such as C4) and conventional neural LMs is additive and significantly outperforms a strong first-pass baseline with a maximum entropy LM.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25351;&#23548;&#25552;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;LATIN-Prompt&#65292;&#36890;&#36807;&#23558;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#23545;&#40784;&#21040;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#24067;&#23616;&#24863;&#30693;&#30340;&#25991;&#26723;&#20869;&#23481;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25551;&#36848;&#65292;&#33021;&#22815;&#24674;&#22797;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#24067;&#23616;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#31526;&#21512;&#20219;&#21153;&#38656;&#27714;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.00526</link><description>&lt;p&gt;
&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#38646;&#26679;&#26412;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#25351;&#23548;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering. (arXiv:2306.00526v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25351;&#23548;&#25552;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;LATIN-Prompt&#65292;&#36890;&#36807;&#23558;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#23545;&#40784;&#21040;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#24067;&#23616;&#24863;&#30693;&#30340;&#25991;&#26723;&#20869;&#23481;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25551;&#36848;&#65292;&#33021;&#22815;&#24674;&#22797;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#24067;&#23616;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#31526;&#21512;&#20219;&#21153;&#38656;&#27714;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24067;&#23616;&#24863;&#30693;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#22312;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#39044;&#35757;&#32451;&#21644;&#20219;&#21153;&#24494;&#35843;&#23545;&#20110;&#39069;&#22806;&#30340;&#35270;&#35273;&#12289;&#24067;&#23616;&#21644;&#20219;&#21153;&#27169;&#22359;&#38459;&#27490;&#20102;&#20854;&#30452;&#25509;&#21033;&#29992;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#26368;&#36817;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#28508;&#21147;&#12290;&#19982;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#39046;&#22495;&#23545;&#40784;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#19982;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;&#65292;&#21033;&#29992;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25351;&#23548;&#25552;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;LATIN-Prompt&#65292;&#23427;&#21253;&#25324;&#24067;&#23616;&#24863;&#30693;&#30340;&#25991;&#26723;&#20869;&#23481;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25551;&#36848;&#12290;&#21069;&#32773;&#36890;&#36807;&#36866;&#24403;&#30340;&#31354;&#26684;&#21644;&#25442;&#34892;&#31526;&#20174;OCR&#24037;&#20855;&#20013;&#24674;&#22797;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#24067;&#23616;&#20449;&#24687;&#12290;&#21518;&#32773;&#30830;&#20445;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#20219;&#21153;&#38656;&#27714;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pre-training-fine-tuning paradigm based on layout-aware multimodal pre-trained models has achieved significant progress on document image question answering. However, domain pre-training and task fine-tuning for additional visual, layout, and task modules prevent them from directly utilizing off-the-shelf instruction-tuning language foundation models, which have recently shown promising potential in zero-shot learning. Contrary to aligning language models to the domain of document image question answering, we align document image question answering to off-the-shell instruction-tuning language foundation models to utilize their zero-shot capability. Specifically, we propose layout and task aware instruction prompt called LATIN-Prompt, which consists of layout-aware document content and task-aware descriptions. The former recovers the layout information among text segments from OCR tools by appropriate spaces and line breaks. The latter ensures that the model generates answers that m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#22312;&#35299;&#20915;&#20132;&#36890;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;LLM&#65292;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20132;&#36890;&#25968;&#25454;&#24182;&#25191;&#34892;&#20132;&#36890;&#36816;&#33829;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#30896;&#25758;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#26694;&#26550;&#20316;&#20026;&#29992;&#20363;&#23637;&#31034;&#20102;&#36825;&#31181;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.05382</link><description>&lt;p&gt;
ChatGPT&#24050;&#22312;&#22320;&#24179;&#32447;&#19978;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23601;&#26159;&#25105;&#20204;&#38656;&#35201;&#30340;&#26234;&#33021;&#20132;&#36890;&#35299;&#20915;&#26041;&#26696;&#65311;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need for Intelligent Transportation?. (arXiv:2303.05382v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#22312;&#35299;&#20915;&#20132;&#36890;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;LLM&#65292;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20132;&#36890;&#25968;&#25454;&#24182;&#25191;&#34892;&#20132;&#36890;&#36816;&#33829;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#30896;&#25758;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#26694;&#26550;&#20316;&#20026;&#29992;&#20363;&#23637;&#31034;&#20102;&#36825;&#31181;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#24320;&#21457;&#30340;&#20855;&#26377;60&#20159;&#21442;&#25968;&#30340;&#37325;&#35201;&#22823;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#12290;ChatGPT&#23637;&#31034;&#20102;LLM&#30340;&#21331;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#23545;&#35805;&#21709;&#24212;&#26041;&#38754;&#12290;&#38543;&#30528;LLM&#22312;&#21508;&#31181;&#30740;&#31350;&#25110;&#24037;&#31243;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#29616;&#22312;&#26159;&#26102;&#20505;&#35774;&#24819;LLM&#22914;&#20309;&#38761;&#26032;&#25105;&#20204;&#22788;&#29702;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#26041;&#24335;&#20102;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#20915;&#20851;&#38190;&#20132;&#36890;&#38382;&#39064;&#26041;&#38754;&#30340;&#26410;&#26469;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;LLM&#65292;&#26234;&#33021;&#31995;&#32479;&#36824;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20132;&#36890;&#25968;&#25454;&#24182;&#36890;&#36807;LLM&#25191;&#34892;&#20132;&#36890;&#36816;&#33829;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;LLM&#35013;&#22791;&#30340;&#36825;&#20123;&#28508;&#22312;&#30340;&#20132;&#36890;&#24212;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35777;&#26126;&#36825;&#31181;&#28508;&#21147;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#30896;&#25758;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#26694;&#26550;&#20316;&#20026;&#29992;&#20363;&#12290;&#23613;&#31649;&#23384;&#22312;&#28508;&#22312;&#30340;&#30410;&#22788;&#65292;&#20294;&#19982;&#25968;&#25454;&#38544;&#31169;&#30456;&#20851;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, developed by OpenAI, is one of the milestone large language models (LLMs) with 6 billion parameters. ChatGPT has demonstrated the impressive language understanding capability of LLM, particularly in generating conversational response. As LLMs start to gain more attention in various research or engineering domains, it is time to envision how LLM may revolutionize the way we approach intelligent transportation systems. This paper explores the future applications of LLM in addressing key transportation problems. By leveraging LLM with cross-modal encoder, an intelligent system can also process traffic data from different modalities and execute transportation operations through an LLM. We present and validate these potential transportation applications equipped by LLM. To further demonstrate this potential, we also provide a concrete smartphone-based crash report auto-generation and analysis framework as a use case. Despite the potential benefits, challenges related to data privac
&lt;/p&gt;</description></item><item><title>NNKGC&#26159;&#19968;&#31181;&#36890;&#36807;&#33410;&#28857;&#37051;&#23621;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#24182;&#24341;&#20837;&#36793;&#36830;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#21487;&#20197;&#39044;&#27979;&#20986;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.06132</link><description>&lt;p&gt;
NNKGC: &#29992;&#33410;&#28857;&#37051;&#23621;&#25913;&#36827;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
NNKGC: Improving Knowledge Graph Completion with Node Neighborhoods. (arXiv:2302.06132v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06132
&lt;/p&gt;
&lt;p&gt;
NNKGC&#26159;&#19968;&#31181;&#36890;&#36807;&#33410;&#28857;&#37051;&#23621;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#24182;&#24341;&#20837;&#36793;&#36830;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#21487;&#20197;&#39044;&#27979;&#20986;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#21457;&#29616;&#26597;&#35810;&#23454;&#20307;&#30340;&#32570;&#22833;&#20851;&#31995;&#12290;&#30446;&#21069;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#21033;&#29992;&#23454;&#20307;&#21517;&#31216;&#21644;&#25551;&#36848;&#25512;&#26029;&#22836;&#23454;&#20307;&#21644;&#29305;&#23450;&#20851;&#31995;&#32473;&#23450;&#30340;&#23614;&#23454;&#20307;&#12290;&#29616;&#26377;&#26041;&#27861;&#36824;&#32771;&#34385;&#20102;&#22836;&#23454;&#20307;&#30340;&#37051;&#23621;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20351;&#29992;&#25153;&#24179;&#32467;&#26500;&#27169;&#25311;&#37051;&#23621;&#65292;&#19988;&#20165;&#38480;&#20110;1&#36339;&#37051;&#23621;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#33410;&#28857;&#37051;&#23621;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#22836;&#23454;&#20307;&#37051;&#23621;&#36827;&#34892;&#22810;&#36339;&#24314;&#27169;&#65292;&#20197;&#20016;&#23500;&#22836;&#33410;&#28857;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#36793;&#36830;&#25509;&#39044;&#27979;&#20219;&#21153;&#26469;&#25913;&#36827;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#31616;&#21333;&#32780;&#26377;&#25928;&#12290;&#26696;&#20363;&#30740;&#31350;&#36824;&#34920;&#26126;&#65292;&#27169;&#22411;&#33021;&#22815;&#39044;&#27979;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion (KGC) aims to discover missing relations of query entities. Current text-based models utilize the entity name and description to infer the tail entity given the head entity and a certain relation. Existing approaches also consider the neighborhood of the head entity. However, these methods tend to model the neighborhood using a flat structure and are only restricted to 1-hop neighbors. In this work, we propose a node neighborhood-enhanced framework for knowledge graph completion. It models the head entity neighborhood from multiple hops using graph neural networks to enrich the head node information. Moreover, we introduce an additional edge link prediction task to improve KGC. Evaluation on two public datasets shows that this framework is simple yet effective. The case study also shows that the model is able to predict explainable predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#21270;&#30340;&#25991;&#26412;&#34164;&#28085;&#65288;TE&#65289;&#27169;&#22411;&#65288;Context-TE&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#20182;&#36873;&#39033;&#20316;&#20026;&#24403;&#21069;&#24314;&#27169;&#30340;&#19978;&#19979;&#25991;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;TE&#26041;&#27861;&#20013;&#30340;&#20004;&#20010;&#38480;&#21046;&#12290;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#21487;&#38752;&#30340;&#36873;&#39033;&#20915;&#31574;&#65292;&#24182;&#19988;&#36890;&#36807;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.00301</link><description>&lt;p&gt;
&#23398;&#20064;&#20174;&#22810;&#20010;&#36873;&#39033;&#20013;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Learning to Select from Multiple Options. (arXiv:2212.00301v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#21270;&#30340;&#25991;&#26412;&#34164;&#28085;&#65288;TE&#65289;&#27169;&#22411;&#65288;Context-TE&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#20182;&#36873;&#39033;&#20316;&#20026;&#24403;&#21069;&#24314;&#27169;&#30340;&#19978;&#19979;&#25991;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;TE&#26041;&#27861;&#20013;&#30340;&#20004;&#20010;&#38480;&#21046;&#12290;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#21487;&#38752;&#30340;&#36873;&#39033;&#20915;&#31574;&#65292;&#24182;&#19988;&#36890;&#36807;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#21487;&#20197;&#30475;&#20316;&#26159;&#20174;&#19968;&#32452;&#36873;&#39033;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#27604;&#22914;&#20998;&#31867;&#20219;&#21153;&#12289;&#22810;&#39033;&#36873;&#25321;&#39064;&#31561;&#12290;&#25991;&#26412;&#34164;&#28085;&#65288;TE&#65289;&#34987;&#35777;&#26126;&#26159;&#22788;&#29702;&#36825;&#20123;&#36873;&#25321;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;TE&#23558;&#36755;&#20837;&#25991;&#26412;&#35270;&#20026;&#21069;&#25552;&#65288;P&#65289;&#65292;&#36873;&#39033;&#35270;&#20026;&#20551;&#35774;&#65288;H&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;&#65288;P&#65292;H&#65289;&#36827;&#34892;&#37197;&#23545;&#24314;&#27169;&#26469;&#22788;&#29702;&#36873;&#25321;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;TE&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;&#39318;&#20808;&#65292;&#37197;&#23545;&#24314;&#27169;&#26080;&#27861;&#24847;&#35782;&#21040;&#20854;&#20182;&#36873;&#39033;&#65292;&#36825;&#19981;&#22815;&#30452;&#35266;&#65292;&#22240;&#20026;&#20154;&#20204;&#24120;&#24120;&#36890;&#36807;&#27604;&#36739;&#31454;&#20105;&#20505;&#36873;&#39033;&#26469;&#30830;&#23450;&#26368;&#20339;&#36873;&#39033;&#65307;&#20854;&#27425;&#65292;&#37197;&#23545;TE&#30340;&#25512;&#29702;&#36807;&#31243;&#32791;&#26102;&#65292;&#29305;&#21035;&#26159;&#24403;&#36873;&#39033;&#31354;&#38388;&#36739;&#22823;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#30340;TE&#27169;&#22411;&#65288;Context-TE&#65289;&#65292;&#36890;&#36807;&#23558;&#20854;&#20182;k&#20010;&#36873;&#39033;&#38468;&#21152;&#20026;&#24403;&#21069;&#65288;P&#65292;H&#65289;&#24314;&#27169;&#30340;&#19978;&#19979;&#25991;&#26469;&#36827;&#34892;&#24314;&#27169;&#12290;Context-TE&#33021;&#22815;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#26469;&#23398;&#20064;&#21040;&#26356;&#21487;&#38752;&#30340;H&#20915;&#31574;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;Pa
&lt;/p&gt;
&lt;p&gt;
Many NLP tasks can be regarded as a selection problem from a set of options, such as classification tasks, multi-choice question answering, etc. Textual entailment (TE) has been shown as the state-of-the-art (SOTA) approach to dealing with those selection problems. TE treats input texts as premises (P), options as hypotheses (H), then handles the selection problem by modeling (P, H) pairwise. Two limitations: first, the pairwise modeling is unaware of other options, which is less intuitive since humans often determine the best options by comparing competing candidates; second, the inference process of pairwise TE is time-consuming, especially when the option space is large. To deal with the two issues, this work first proposes a contextualized TE model (Context-TE) by appending other k options as the context of the current (P, H) modeling. Context-TE is able to learn more reliable decision for the H since it considers various context. Second, we speed up Context-TE by coming up with Pa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#27979;&#37327;&#21644;&#20943;&#36731;&#25512;&#29702;&#25463;&#24452;&#30340;&#25216;&#26415;&#65292;&#24182;&#24378;&#35843;&#20102;&#32570;&#20047;&#20844;&#20849;&#25361;&#25112;&#38598;&#21644;&#20854;&#20182;&#39046;&#22495;&#20943;&#36731;&#25216;&#26415;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.01824</link><description>&lt;p&gt;
&#22312;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#27979;&#37327;&#21644;&#20943;&#36731;&#25512;&#29702;&#25463;&#24452;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Measuring and Mitigating Reasoning Shortcuts in Machine Reading Comprehension. (arXiv:2209.01824v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01824
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#27979;&#37327;&#21644;&#20943;&#36731;&#25512;&#29702;&#25463;&#24452;&#30340;&#25216;&#26415;&#65292;&#24182;&#24378;&#35843;&#20102;&#32570;&#20047;&#20844;&#20849;&#25361;&#25112;&#38598;&#21644;&#20854;&#20182;&#39046;&#22495;&#20943;&#36731;&#25216;&#26415;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#25463;&#23398;&#20064;&#30340;&#38382;&#39064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24191;&#20026;&#20154;&#30693;&#65292;&#24182;&#19988;&#36817;&#24180;&#26469;&#19968;&#30452;&#26159;&#37325;&#35201;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;&#25968;&#25454;&#20013;&#30340;&#26080;&#24847;&#38388;&#30456;&#20851;&#24615;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#36731;&#26494;&#35299;&#20915;&#21407;&#26412;&#24212;&#35813;&#23637;&#31034;&#39640;&#32423;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#39046;&#22495;&#65292;&#36825;&#26159;&#19968;&#20010;&#23637;&#31034;&#39640;&#27700;&#24179;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#21516;&#26102;&#20063;&#21463;&#21040;&#20102;&#21508;&#31181;&#25463;&#24452;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#27979;&#37327;&#21644;&#20943;&#36731;&#25463;&#24452;&#30340;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#22312;&#26368;&#21518;&#25552;&#20986;&#20102;&#23545;&#36827;&#19968;&#27493;&#30340;&#25463;&#24452;&#30740;&#31350;&#30340;&#24314;&#35758;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#20004;&#20010;&#23545;&#20110;&#20943;&#36731;&#25463;&#24452;&#30340;&#20851;&#27880;&#28857;&#65306;(1)&#32570;&#20047;&#20844;&#20849;&#25361;&#25112;&#38598;&#65292;&#36825;&#26159;&#26377;&#25928;&#21644;&#21487;&#37325;&#29992;&#35780;&#20272;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65307;(2)&#32570;&#20047;&#20854;&#20182;&#39046;&#22495;&#31361;&#20986;&#30340;&#26576;&#20123;&#20943;&#36731;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The issue of shortcut learning is widely known in NLP and has been an important research focus in recent years. Unintended correlations in the data enable models to easily solve tasks that were meant to exhibit advanced language understanding and reasoning capabilities. In this survey paper, we focus on the field of machine reading comprehension (MRC), an important task for showcasing high-level language understanding that also suffers from a range of shortcuts. We summarize the available techniques for measuring and mitigating shortcuts and conclude with suggestions for further progress in shortcut research. Importantly, we highlight two concerns for shortcut mitigation in MRC: (1) the lack of public challenge sets, a necessary component for effective and reusable evaluation, and (2) the lack of certain mitigation techniques that are prominent in other areas.
&lt;/p&gt;</description></item></channel></rss>