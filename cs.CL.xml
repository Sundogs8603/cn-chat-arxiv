<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#39044;&#27979;&#25552;&#31034;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#23548;&#23427;&#20204;&#29983;&#25104;&#24178;&#25200;&#39033;&#26469;&#22635;&#34917;&#25945;&#32946;&#32972;&#26223;&#19979;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#29983;&#25104;&#24178;&#25200;&#39033;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#36136;&#37327;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#27169;&#22411;&#65292;&#24182;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#24178;&#25200;&#39033;&#12290;</title><link>http://arxiv.org/abs/2307.16338</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#27979;&#25552;&#31034;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#24178;&#25200;&#39033;
&lt;/p&gt;
&lt;p&gt;
Distractor generation for multiple-choice questions with predictive prompting and large language models. (arXiv:2307.16338v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#39044;&#27979;&#25552;&#31034;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#23548;&#23427;&#20204;&#29983;&#25104;&#24178;&#25200;&#39033;&#26469;&#22635;&#34917;&#25945;&#32946;&#32972;&#26223;&#19979;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#29983;&#25104;&#24178;&#25200;&#39033;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#36136;&#37327;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#27169;&#22411;&#65292;&#24182;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#24178;&#25200;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#21560;&#24341;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#25945;&#32946;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20173;&#28982;&#35266;&#23519;&#21040;&#20351;&#29992;LLM&#20026;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQ&#65289;&#29983;&#25104;&#24178;&#25200;&#39033;&#65288;&#21363;&#21487;&#20449;&#20294;&#19981;&#27491;&#30830;&#30340;&#31572;&#26696;&#65289;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#20174;&#39064;&#24211;&#20013;&#33258;&#21160;&#26816;&#32034;&#30340;&#38382;&#39064;&#39033;&#20316;&#20026;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#24341;&#23548;LLM&#65288;&#22914;ChatGPT&#65289;&#29983;&#25104;&#30456;&#20851;&#30340;&#24178;&#25200;&#39033;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#27979;&#35797;&#38598;&#30340;&#23450;&#37327;&#35780;&#20272;&#21644;&#20154;&#31867;&#19987;&#23478;&#65288;&#25945;&#24072;&#65289;&#30340;&#36136;&#37327;&#27880;&#37322;&#26469;&#35780;&#20272;&#25105;&#20204;&#22522;&#20110;LLM&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24179;&#22343;&#32780;&#35328;&#65292;&#21521;&#25945;&#24072;&#23637;&#31034;&#30340;53&#65285;&#29983;&#25104;&#30340;&#24178;&#25200;&#39033;&#34987;&#35780;&#20026;&#39640;&#36136;&#37327;&#65292;&#21363;&#36866;&#21512;&#31435;&#21363;&#20351;&#29992;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) such as ChatGPT have demonstrated remarkable performance across various tasks and have garnered significant attention from both researchers and practitioners. However, in an educational context, we still observe a performance gap in generating distractors -- i.e., plausible yet incorrect answers -- with LLMs for multiple-choice questions (MCQs). In this study, we propose a strategy for guiding LLMs such as ChatGPT, in generating relevant distractors by prompting them with question items automatically retrieved from a question bank as well-chosen in-context examples. We evaluate our LLM-based solutions using a quantitative assessment on an existing test set, as well as through quality annotations by human experts, i.e., teachers. We found that on average 53% of the generated distractors presented to the teachers were rated as high-quality, i.e., suitable for immediate use as is, outperforming the state-of-the-art model. We also show the gains of our approach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#30340;&#21457;&#38899;&#38169;&#35823;&#26816;&#27979;&#20219;&#21153;&#20013;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#20026;&#30446;&#26631;&#20219;&#21153;&#35757;&#32451;&#30340;&#19979;&#28216;&#27169;&#22411;&#33021;&#22815;&#24471;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#32780;&#22823;&#22810;&#25968;&#19978;&#28216;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#34920;&#29616;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2307.16324</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#36827;&#34892;&#21457;&#38899;&#38169;&#35823;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Mispronunciation detection using self-supervised speech representations. (arXiv:2307.16324v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#30340;&#21457;&#38899;&#38169;&#35823;&#26816;&#27979;&#20219;&#21153;&#20013;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#20026;&#30446;&#26631;&#20219;&#21153;&#35757;&#32451;&#30340;&#19979;&#28216;&#27169;&#22411;&#33021;&#22815;&#24471;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#32780;&#22823;&#22810;&#25968;&#19978;&#28216;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#34920;&#29616;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#30340;&#21457;&#38899;&#38169;&#35823;&#26816;&#27979;&#20219;&#21153;&#20013;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#19979;&#28216;&#26041;&#27861;&#65306;1&#65289;&#20351;&#29992;&#26412;&#22320;&#33521;&#25991;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#38899;&#32032;&#35782;&#21035;&#65288;PR&#65289;&#65307;2&#65289;&#20351;&#29992;&#38750;&#26412;&#22320;&#33521;&#25991;&#25968;&#25454;&#30452;&#25509;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#30446;&#26631;&#20219;&#21153;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#34920;&#31034;&#20197;&#21450;&#20174;&#20256;&#32479;&#30340;&#22522;&#20110;DNN&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#34920;&#31034;&#19979;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#22312;L2Arctic&#21644;EpaDB&#20004;&#20010;&#26631;&#27880;&#20102;&#38899;&#32032;&#32423;&#21457;&#38899;&#26631;&#31614;&#30340;&#38750;&#26412;&#22320;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#20026;&#30446;&#26631;&#20219;&#21153;&#35757;&#32451;&#30340;&#19979;&#28216;&#27169;&#22411;&#33021;&#22815;&#24471;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#32780;&#22823;&#22810;&#25968;&#19978;&#28216;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#34920;&#29616;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, self-supervised learning (SSL) models have produced promising results in a variety of speech-processing tasks, especially in contexts of data scarcity. In this paper, we study the use of SSL models for the task of mispronunciation detection for second language learners. We compare two downstream approaches: 1) training the model for phone recognition (PR) using native English data, and 2) training a model directly for the target task using non-native English data. We compare the performance of these two approaches for various SSL representations as well as a representation extracted from a traditional DNN-based speech recognition model. We evaluate the models on L2Arctic and EpaDB, two datasets of non-native speech annotated with pronunciation labels at the phone level. Overall, we find that using a downstream model trained for the target task gives the best performance and that most upstream models perform similarly for the task.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#20998;&#23618;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#25351;&#26631;&#12289;&#23398;&#20064;&#31574;&#30053;&#21644;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.16265</link><description>&lt;p&gt;
&#20998;&#23618;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#26368;&#26032;&#36827;&#23637;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Hierarchical Multi-label Text Classification: A Survey. (arXiv:2307.16265v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16265
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#20998;&#23618;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#25351;&#26631;&#12289;&#23398;&#20064;&#31574;&#30053;&#21644;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#26088;&#22312;&#23558;&#36755;&#20837;&#30340;&#25991;&#26412;&#20998;&#20026;&#22810;&#20010;&#26631;&#31614;&#65292;&#20854;&#20013;&#26631;&#31614;&#20043;&#38388;&#20855;&#26377;&#32467;&#26500;&#21644;&#23618;&#27425;&#20851;&#31995;&#12290;&#23427;&#26159;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#20363;&#22914;&#31185;&#23398;&#25991;&#29486;&#24402;&#26723;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#20998;&#23618;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#24320;&#28304;&#25968;&#25454;&#38598;&#12289;&#20027;&#35201;&#26041;&#27861;&#12289;&#35780;&#20272;&#25351;&#26631;&#12289;&#23398;&#20064;&#31574;&#30053;&#21644;&#24403;&#21069;&#30340;&#25361;&#25112;&#12290;&#36824;&#21015;&#20030;&#20102;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#20379;&#30740;&#31350;&#32773;&#36827;&#19968;&#27493;&#25913;&#36827;&#36825;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical multi-label text classification aims to classify the input text into multiple labels, among which the labels are structured and hierarchical. It is a vital task in many real world applications, e.g. scientific literature archiving. In this paper, we survey the recent progress of hierarchical multi-label text classification, including the open sourced data sets, the main methods, evaluation metrics, learning strategies and the current challenges. A few future research directions are also listed for community to further improve this field.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31169;&#23494;&#27700;&#21360;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#65292;&#24182;&#20849;&#20139;&#37096;&#20998;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#39640;&#20934;&#30830;&#24615;&#30340;&#26816;&#27979;&#65292;&#21516;&#26102;&#23545;&#29983;&#25104;&#21644;&#26816;&#27979;&#36895;&#24230;&#24433;&#21709;&#26368;&#23567;&#12290;</title><link>http://arxiv.org/abs/2307.16230</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31169;&#23494;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
A Private Watermark for Large Language Models. (arXiv:2307.16230v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16230
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31169;&#23494;&#27700;&#21360;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#65292;&#24182;&#20849;&#20139;&#37096;&#20998;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#39640;&#20934;&#30830;&#24615;&#30340;&#26816;&#27979;&#65292;&#21516;&#26102;&#23545;&#29983;&#25104;&#21644;&#26816;&#27979;&#36895;&#24230;&#24433;&#21709;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#24050;&#32463;&#20943;&#36731;&#20102;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#24102;&#26469;&#30340;&#20266;&#26032;&#38395;&#21644;&#29256;&#26435;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#30340;&#27700;&#21360;&#26816;&#27979;&#38656;&#35201;&#29983;&#25104;&#36807;&#31243;&#30340;&#23494;&#38053;&#65292;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#36829;&#35268;&#21644;&#20266;&#36896;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31169;&#23494;&#27700;&#21360;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#38454;&#27573;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#32780;&#19981;&#26159;&#20351;&#29992;&#30456;&#21516;&#30340;&#23494;&#38053;&#26469;&#25193;&#23637;&#24403;&#21069;&#30340;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#32593;&#32476;&#30340;&#37096;&#20998;&#21442;&#25968;&#26159;&#20849;&#20139;&#30340;&#65292;&#36825;&#20351;&#24471;&#26816;&#27979;&#32593;&#32476;&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30001;&#20110;&#20004;&#20010;&#32593;&#32476;&#30340;&#21442;&#25968;&#35268;&#27169;&#36739;&#23567;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30830;&#20445;&#20102;&#39640;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;&#29983;&#25104;&#21644;&#26816;&#27979;&#36895;&#24230;&#30340;&#24433;&#21709;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, text watermarking algorithms for large language models (LLMs) have been mitigating the potential harms of text generated by the LLMs, including fake news and copyright issues. However, the watermark detection of current text algorithms requires the key from the generation process, making them susceptible to breaches and counterfeiting. In this work, we propose the first private watermarking algorithm, which extends the current text watermarking algorithms by using two different neural networks respectively for watermark generation and detection, rather than using the same key at both stages. Meanwhile, part of the parameters of the watermark generation and detection networks are shared, which makes the detection network achieve a high accuracy very efficiently. Experiments show that our algorithm ensures high detection accuracy with minimal impact on generation and detection speed, due to the small parameter size of both networks. Additionally, our subsequent analysis demonst
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36739;&#23569;&#30340;&#25163;&#21160;&#21019;&#24314;&#25968;&#25454;&#35757;&#32451;&#30340;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#24076;&#20271;&#26469;&#25991;OCR&#21518;&#26657;&#27491;&#12290;&#30740;&#31350;&#30446;&#26631;&#26159;&#24320;&#21457;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#25913;&#21892;OCR&#21518;&#26657;&#27491;&#32467;&#26524;&#65292;&#24182;&#30740;&#31350;&#26368;&#36866;&#29992;&#20110;&#21382;&#21490;&#25991;&#26723;&#30340;OCR&#21518;&#26657;&#27491;&#30340;&#25968;&#25454;&#38598;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.16220</link><description>&lt;p&gt;
&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20197;&#36827;&#34892;&#21382;&#21490;&#24076;&#20271;&#26469;&#25991;OCR&#38169;&#35823;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Optimizing the Neural Network Training for OCR Error Correction of Historical Hebrew Texts. (arXiv:2307.16220v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16220
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36739;&#23569;&#30340;&#25163;&#21160;&#21019;&#24314;&#25968;&#25454;&#35757;&#32451;&#30340;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#24076;&#20271;&#26469;&#25991;OCR&#21518;&#26657;&#27491;&#12290;&#30740;&#31350;&#30446;&#26631;&#26159;&#24320;&#21457;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#25913;&#21892;OCR&#21518;&#26657;&#27491;&#32467;&#26524;&#65292;&#24182;&#30740;&#31350;&#26368;&#36866;&#29992;&#20110;&#21382;&#21490;&#25991;&#26723;&#30340;OCR&#21518;&#26657;&#27491;&#30340;&#25968;&#25454;&#38598;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#22823;&#37327;&#32440;&#36136;&#25991;&#26723;&#22914;&#20070;&#31821;&#21644;&#25253;&#32440;&#24050;&#32463;&#20351;&#29992;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#25216;&#26415;&#36827;&#34892;&#25968;&#23383;&#21270;&#12290;&#36825;&#31181;&#25216;&#26415;&#22312;&#22788;&#29702;&#21382;&#21490;&#25991;&#26723;&#26102;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#12290;&#20026;&#20102;&#32416;&#27491;OCR&#38169;&#35823;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#31561;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#21518;&#22788;&#29702;&#31639;&#27861;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#32570;&#28857;&#26159;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#36825;&#31181;&#25968;&#25454;&#36890;&#24120;&#19981;&#26131;&#33719;&#21462;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36739;&#23569;&#30340;&#25163;&#21160;&#21019;&#24314;&#25968;&#25454;&#26469;&#35757;&#32451;&#36731;&#37327;&#32423;&#30340;&#24076;&#20271;&#26469;&#25991;OCR&#21518;&#26657;&#27491;&#31070;&#32463;&#32593;&#32476;&#12290;&#20027;&#35201;&#30740;&#31350;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#35821;&#35328;&#21644;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#22312;OCR&#21518;&#26657;&#27491;&#20013;&#30340;&#32467;&#26524;&#65292;&#24182;&#30740;&#31350;&#21738;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#21382;&#21490;&#25991;&#26723;&#30340;OCR&#21518;&#26657;&#27491;&#26368;&#26377;&#25928;&#12290;&#20026;&#27492;&#65292;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few decades, large archives of paper-based documents such as books and newspapers have been digitized using Optical Character Recognition. This technology is error-prone, especially for historical documents. To correct OCR errors, post-processing algorithms have been proposed based on natural language analysis and machine learning techniques such as neural networks. Neural network's disadvantage is the vast amount of manually labeled data required for training, which is often unavailable. This paper proposes an innovative method for training a light-weight neural network for Hebrew OCR post-correction using significantly less manually created data. The main research goal is to develop a method for automatically generating language and task-specific training data to improve the neural network results for OCR post-correction, and to investigate which type of dataset is the most effective for OCR post-correction of historical documents. To this end, a series of experiments u
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#23383;&#20154;&#25991;&#30740;&#31350;&#20013;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#39046;&#22495;&#36866;&#24212;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.16217</link><description>&lt;p&gt;
&#25968;&#23383;&#20154;&#25991;&#21644;&#20449;&#24687;&#31185;&#23398;&#20013;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Text Analysis Using Deep Neural Networks in Digital Humanities and Information Science. (arXiv:2307.16217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#23383;&#20154;&#25991;&#30740;&#31350;&#20013;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#39046;&#22495;&#36866;&#24212;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#35745;&#31639;&#25216;&#26415;&#21644;&#20154;&#25991;&#23398;&#31185;&#26159;&#19968;&#39033;&#25345;&#32493;&#36827;&#34892;&#30340;&#21162;&#21147;&#65292;&#26088;&#22312;&#20351;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#20854;&#20182;&#33402;&#26415;&#21697;&#31561;&#36164;&#28304;&#22312;&#25968;&#23383;&#21270;&#26102;&#20195;&#26131;&#20110;&#33719;&#24471;&#12289;&#21487;&#25628;&#32034;&#21644;&#21487;&#20998;&#26512;&#12290;&#22312;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#33258;&#21160;&#25991;&#26412;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#26377;&#26102;&#21576;&#29616;&#20986;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;DNN&#26159;&#35299;&#20915;&#25968;&#23383;&#20154;&#25991;&#30740;&#31350;&#20013;&#19982;NLP&#30456;&#20851;&#30340;&#35768;&#22810;&#20219;&#21153;&#65288;&#20363;&#22914;&#25340;&#20889;&#26816;&#26597;&#12289;&#35821;&#35328;&#26816;&#27979;&#12289;&#23454;&#20307;&#25552;&#21462;&#12289;&#20316;&#32773;&#26816;&#27979;&#12289;&#38382;&#31572;&#31561;&#65289;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#36825;&#20123;&#26377;&#30417;&#30563;&#31639;&#27861;&#20174;&#22823;&#37327;&#30340;&#8220;&#27491;&#30830;&#8221;&#21644;&#8220;&#38169;&#35823;&#8221;&#31034;&#20363;&#20013;&#23398;&#20064;&#27169;&#24335;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26032;&#30340;&#31034;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#23383;&#20154;&#25991;&#30740;&#31350;&#20013;&#20351;&#29992;DNN&#20998;&#26512;&#25991;&#26412;&#36164;&#28304;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;&#19981;&#65289;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#39046;&#22495;&#36866;&#24212;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22810;&#20010;&#20351;&#29992;&#26696;&#20363;&#26469;&#25506;&#35752;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining computational technologies and humanities is an ongoing effort aimed at making resources such as texts, images, audio, video, and other artifacts digitally available, searchable, and analyzable. In recent years, deep neural networks (DNN) dominate the field of automatic text analysis and natural language processing (NLP), in some cases presenting a super-human performance. DNNs are the state-of-the-art machine learning algorithms solving many NLP tasks that are relevant for Digital Humanities (DH) research, such as spell checking, language detection, entity extraction, author detection, question answering, and other tasks. These supervised algorithms learn patterns from a large number of "right" and "wrong" examples and apply them to new examples. However, using DNNs for analyzing the text resources in DH research presents two main challenges: (un)availability of training data and a need for domain adaptation. This paper explores these challenges by analyzing multiple use-cas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#23478;&#26063;&#35889;&#38382;&#31572;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#23478;&#26063;&#35889;&#25968;&#25454;&#34920;&#31034;&#20026;&#30693;&#35782;&#22270;&#24182;&#19982;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#32467;&#21512;&#65292;&#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#23478;&#26063;&#35889;&#39046;&#22495;&#20013;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#22270;&#32467;&#26500;&#21644;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.16214</link><description>&lt;p&gt;
&#29992;&#20110;&#21322;&#32467;&#26500;&#24322;&#26500;&#23478;&#26063;&#35889;&#30693;&#35782;&#22270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Question Answering with Deep Neural Networks for Semi-Structured Heterogeneous Genealogical Knowledge Graphs. (arXiv:2307.16214v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#23478;&#26063;&#35889;&#38382;&#31572;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#23478;&#26063;&#35889;&#25968;&#25454;&#34920;&#31034;&#20026;&#30693;&#35782;&#22270;&#24182;&#19982;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#32467;&#21512;&#65292;&#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#23478;&#26063;&#35889;&#39046;&#22495;&#20013;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#22270;&#32467;&#26500;&#21644;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29992;&#25143;&#29983;&#25104;&#30340;&#23478;&#26063;&#35889;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#26032;&#30340;&#23478;&#26063;&#35889;&#20449;&#24687;&#31995;&#32479;&#24471;&#21040;&#20102;&#24320;&#21457;&#12290;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#31639;&#27861;&#20351;&#29992;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20123;&#27169;&#22411;&#20351;&#29992;&#22522;&#20110;&#24207;&#21015;&#30340;&#36755;&#20837;&#65292;&#19981;&#36866;&#21512;&#22788;&#29702;&#22522;&#20110;&#22270;&#30340;&#32467;&#26500;&#65292;&#32780;&#22522;&#20110;&#22270;&#30340;DNN&#27169;&#22411;&#21017;&#20381;&#36182;&#20110;&#22312;&#23478;&#26063;&#35889;&#39046;&#22495;&#20013;&#19981;&#23384;&#22312;&#30340;&#39640;&#24230;&#20840;&#38754;&#30340;&#30693;&#35782;&#22270;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26377;&#30417;&#30563;&#30340;DNN&#27169;&#22411;&#38656;&#35201;&#22312;&#23478;&#26063;&#35889;&#39046;&#22495;&#20013;&#32570;&#20047;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23478;&#26063;&#35889;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65306;1&#65289;&#23558;&#23478;&#26063;&#35889;&#25968;&#25454;&#34920;&#31034;&#20026;&#30693;&#35782;&#22270;&#65292;2&#65289;&#23558;&#20854;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;3&#65289;&#19982;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#32467;&#21512;&#65292;4&#65289;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#38382;&#31572;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#38656;&#35201;&#19987;&#38376;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#65292;&#23545;&#27604;&#20102;&#24494;&#35843;&#27169;&#24335;&#19979;&#30340;&#27169;&#22411;&#19982;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rising popularity of user-generated genealogical family trees, new genealogical information systems have been developed. State-of-the-art natural question answering algorithms use deep neural network (DNN) architecture based on self-attention networks. However, some of these models use sequence-based inputs and are not suitable to work with graph-based structure, while graph-based DNN models rely on high levels of comprehensiveness of knowledge graphs that is nonexistent in the genealogical domain. Moreover, these supervised DNN models require training datasets that are absent in the genealogical domain. This study proposes an end-to-end approach for question answering using genealogical family trees by: 1) representing genealogical data as knowledge graphs, 2) converting them to texts, 3) combining them with unstructured texts, and 4) training a trans-former-based question answering model. To evaluate the need for a dedicated approach, a comparison between the fine-tuned mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21382;&#21490;&#24076;&#20271;&#26469;&#25991;&#26412;OCR&#38169;&#35823;&#26657;&#27491;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#26102;&#26399;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24076;&#20271;&#26469;&#35821;&#26159;&#24418;&#24577;&#20016;&#23500;&#35821;&#35328;&#65292;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#19981;&#22815;&#20805;&#36275;&#65292;&#19988;&#26368;&#20339;&#30340;&#32593;&#32476;&#32467;&#26500;&#21644;&#36229;&#21442;&#25968;&#20540;&#23578;&#19981;&#26126;&#30830;&#12290;&#21478;&#22806;&#65292;&#19981;&#21516;&#27969;&#27966;&#21644;&#26102;&#26399;&#30340;&#35821;&#35328;&#21464;&#21270;&#21487;&#33021;&#20250;&#24433;&#21709;OCR&#21518;&#26657;&#27491;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16213</link><description>&lt;p&gt;
&#26397;&#30528;&#21382;&#21490;&#24076;&#20271;&#26469;&#25991;&#26412;OCR&#38169;&#35823;&#26657;&#27491;&#30340;&#29305;&#23450;&#26102;&#26399;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Toward a Period-Specific Optimized Neural Network for OCR Error Correction of Historical Hebrew Texts. (arXiv:2307.16213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21382;&#21490;&#24076;&#20271;&#26469;&#25991;&#26412;OCR&#38169;&#35823;&#26657;&#27491;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#26102;&#26399;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24076;&#20271;&#26469;&#35821;&#26159;&#24418;&#24577;&#20016;&#23500;&#35821;&#35328;&#65292;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#19981;&#22815;&#20805;&#36275;&#65292;&#19988;&#26368;&#20339;&#30340;&#32593;&#32476;&#32467;&#26500;&#21644;&#36229;&#21442;&#25968;&#20540;&#23578;&#19981;&#26126;&#30830;&#12290;&#21478;&#22806;&#65292;&#19981;&#21516;&#27969;&#27966;&#21644;&#26102;&#26399;&#30340;&#35821;&#35328;&#21464;&#21270;&#21487;&#33021;&#20250;&#24433;&#21709;OCR&#21518;&#26657;&#27491;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#20351;&#29992;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#25216;&#26415;&#23545;&#22823;&#37327;&#32440;&#36136;&#21382;&#21490;&#25991;&#26723;&#65288;&#22914;&#20070;&#31821;&#21644;&#25253;&#32440;&#65289;&#36827;&#34892;&#20102;&#25968;&#23383;&#21270;&#23384;&#26723;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#22312;&#22788;&#29702;&#20889;&#20110;&#25968;&#30334;&#24180;&#21069;&#30340;OCR&#25991;&#26723;&#26102;&#23481;&#26131;&#20986;&#38169;&#12290;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#21508;&#31181;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;OCR&#21518;&#26657;&#27491;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24076;&#20271;&#26469;&#35821;&#36825;&#26679;&#30340;&#24418;&#24577;&#20016;&#23500;&#35821;&#35328;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#21382;&#21490;&#35821;&#26009;&#24211;&#30340;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#20805;&#36275;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#24076;&#20271;&#26469;&#35821;&#30340;&#29420;&#29305;&#29305;&#28857;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;OCR&#38169;&#35823;&#26657;&#27491;&#26041;&#38754;&#30340;&#26368;&#20339;&#32467;&#26500;&#21644;&#36229;&#21442;&#25968;&#20540;&#65288;&#39044;&#23450;&#20041;&#21442;&#25968;&#65289;&#23578;&#19981;&#28165;&#26970;&#12290;&#27492;&#22806;&#65292;&#35821;&#35328;&#22312;&#19981;&#21516;&#30340;&#27969;&#27966;&#21644;&#26102;&#26399;&#20063;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#36825;&#20123;&#21464;&#21270;&#21487;&#33021;&#20250;&#24433;&#21709;OCR&#21518;&#26657;&#27491;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few decades, large archives of paper-based historical documents, such as books and newspapers, have been digitized using the Optical Character Recognition (OCR) technology. Unfortunately, this broadly used technology is error-prone, especially when an OCRed document was written hundreds of years ago. Neural networks have shown great success in solving various text processing tasks, including OCR post-correction. The main disadvantage of using neural networks for historical corpora is the lack of sufficiently large training datasets they require to learn from, especially for morphologically-rich languages like Hebrew. Moreover, it is not clear what are the optimal structure and values of hyperparameters (predefined parameters) of neural networks for OCR error correction in Hebrew due to its unique features. Furthermore, languages change across genres and periods. These changes may affect the accuracy of OCR post-correction neural network models. To overcome these challenge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#24322;&#26500;&#23478;&#35889;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#25968;&#23383;&#32858;&#21512;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#22312;&#22522;&#22240;&#35889;&#39046;&#22495;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#24182;&#33719;&#24471;&#20934;&#30830;&#31572;&#26696;&#30340;&#33021;&#21147;&#36824;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.16208</link><description>&lt;p&gt;
&#22312;&#24322;&#26500;&#23478;&#35889;&#30693;&#35782;&#22270;&#35889;&#19978;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25968;&#23383;&#32858;&#21512;&#38382;&#31572;&#30340;GLOBE&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Around the GLOBE: Numerical Aggregation Question-Answering on Heterogeneous Genealogical Knowledge Graphs with Deep Neural Networks. (arXiv:2307.16208v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#24322;&#26500;&#23478;&#35889;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#25968;&#23383;&#32858;&#21512;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#22312;&#22522;&#22240;&#35889;&#39046;&#22495;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#24182;&#33719;&#24471;&#20934;&#30830;&#31572;&#26696;&#30340;&#33021;&#21147;&#36824;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#24322;&#26500;&#23478;&#35889;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#25968;&#23383;&#32858;&#21512;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#19982;&#31934;&#30830;&#31572;&#26696;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#30446;&#21069;&#65292;&#22312;&#22522;&#22240;&#35889;&#39046;&#22495;&#65292;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#24182;&#33719;&#24471;&#20934;&#30830;&#31572;&#26696;&#30340;&#33021;&#21147;&#36824;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#65292;&#32780;&#30740;&#31350;&#32773;&#22312;&#20154;&#25991;&#21644;&#31038;&#20250;&#31185;&#23398;&#31561;&#39046;&#22495;&#21487;&#20197;&#20174;&#36825;&#31181;&#33021;&#21147;&#20013;&#21463;&#30410;&#21290;&#27973;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the key AI tools for textual corpora exploration is natural language question-answering (QA). Unlike keyword-based search engines, QA algorithms receive and process natural language questions and produce precise answers to these questions, rather than long lists of documents that need to be manually scanned by the users. State-of-the-art QA algorithms based on DNNs were successfully employed in various domains. However, QA in the genealogical domain is still underexplored, while researchers in this field (and other fields in humanities and social sciences) can highly benefit from the ability to ask questions in natural language, receive concrete answers and gain insights hidden within large corpora. While some research has been recently conducted for factual QA in the genealogical domain, to the best of our knowledge, there is no previous research on the more challenging task of numerical aggregation QA (i.e., answering questions combining aggregation functions, e.g., count, ave
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#20004;&#38454;&#27573;&#29983;&#25104;&#26694;&#26550;&#65288;KTGF&#65289;&#29992;&#20110;&#21307;&#23398;&#23545;&#35805;&#20449;&#24687;&#25552;&#21462;&#12290;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#29983;&#25104;&#65292;&#20998;&#21035;&#29983;&#25104;&#21307;&#23398;&#23545;&#35805;&#20013;&#30340;&#26415;&#35821;&#21644;&#27599;&#20010;&#26415;&#35821;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24314;&#27169;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.16200</link><description>&lt;p&gt;
&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#20004;&#38454;&#27573;&#29983;&#25104;&#26694;&#26550;&#29992;&#20110;&#21307;&#23398;&#23545;&#35805;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue Information Extraction. (arXiv:2307.16200v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#20004;&#38454;&#27573;&#29983;&#25104;&#26694;&#26550;&#65288;KTGF&#65289;&#29992;&#20110;&#21307;&#23398;&#23545;&#35805;&#20449;&#24687;&#25552;&#21462;&#12290;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#29983;&#25104;&#65292;&#20998;&#21035;&#29983;&#25104;&#21307;&#23398;&#23545;&#35805;&#20013;&#30340;&#26415;&#35821;&#21644;&#27599;&#20010;&#26415;&#35821;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24314;&#27169;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#21307;&#23398;&#23545;&#35805;&#20013;&#30340;&#26415;&#35821;-&#29366;&#24577;&#23545;&#25552;&#21462;&#65288;MD-TSPE&#65289;&#65292;&#36825;&#22312;&#35786;&#26029;&#23545;&#35805;&#31995;&#32479;&#21644;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#65288;EMR&#65289;&#30340;&#33258;&#21160;&#25220;&#20889;&#20013;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;MD-TSPE&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#26041;&#27861;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#20043;&#21518;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29983;&#25104;&#26041;&#27861;&#22312;&#19968;&#38454;&#27573;&#36755;&#20986;&#25972;&#20010;&#30001;&#26415;&#35821;-&#29366;&#24577;&#23545;&#32452;&#25104;&#30340;&#24207;&#21015;&#26102;&#24573;&#30053;&#20102;&#38598;&#25104;&#20808;&#21069;&#30693;&#35782;&#30340;&#38656;&#27714;&#65292;&#36825;&#38656;&#35201;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#26469;&#24314;&#27169;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#25512;&#26029;&#27599;&#20010;&#26415;&#35821;&#30340;&#29366;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#20004;&#38454;&#27573;&#29983;&#25104;&#26694;&#26550;&#65288;KTGF&#65289;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#37319;&#29992;&#21333;&#19968;&#27169;&#22411;&#20197;&#32479;&#19968;&#30340;&#29983;&#25104;&#24418;&#24335;&#23436;&#25104;MD-TSPE&#30340;&#20004;&#20010;&#38454;&#27573;&#65306;&#39318;&#20808;&#29983;&#25104;&#25152;&#26377;&#30340;&#26415;&#35821;&#65292;&#28982;&#21518;&#29983;&#25104;&#27599;&#20010;&#29983;&#25104;&#30340;&#26415;&#35821;&#30340;&#29366;&#24577;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on term-status pair extraction from medical dialogues (MD-TSPE), which is essential in diagnosis dialogue systems and the automatic scribe of electronic medical records (EMRs). In the past few years, works on MD-TSPE have attracted increasing research attention, especially after the remarkable progress made by generative methods. However, these generative methods output a whole sequence consisting of term-status pairs in one stage and ignore integrating prior knowledge, which demands a deeper understanding to model the relationship between terms and infer the status of each term. This paper presents a knowledge-enhanced two-stage generative framework (KTGF) to address the above challenges. Using task-specific prompts, we employ a single model to complete the MD-TSPE through two phases in a unified generative form: we generate all terms the first and then generate the status of each generated term. In this way, the relationship between terms can be learned more effect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35789;&#20998;&#21106;&#26041;&#27861;&#25913;&#36827;&#20102;&#19978;&#28023;&#35805;TTS&#27169;&#22411;&#20013;&#22768;&#35843;&#36830;&#35835;&#30340;&#36136;&#37327;&#65292;&#23545;&#20110;&#23558;&#19978;&#28023;&#35805;&#30340;&#24418;&#24335;&#21270;&#35821;&#35328;&#23398;&#35299;&#37322;&#24341;&#20837;&#35745;&#31639;&#39033;&#30446;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.16199</link><description>&lt;p&gt;
&#25913;&#36827;&#19978;&#28023;&#35805;&#30340;TTS&#65306;&#36890;&#36807;&#35789;&#20998;&#21106;&#35299;&#20915;&#22768;&#35843;&#36830;&#35835;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Improving TTS for Shanghainese: Addressing Tone Sandhi via Word Segmentation. (arXiv:2307.16199v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35789;&#20998;&#21106;&#26041;&#27861;&#25913;&#36827;&#20102;&#19978;&#28023;&#35805;TTS&#27169;&#22411;&#20013;&#22768;&#35843;&#36830;&#35835;&#30340;&#36136;&#37327;&#65292;&#23545;&#20110;&#23558;&#19978;&#28023;&#35805;&#30340;&#24418;&#24335;&#21270;&#35821;&#35328;&#23398;&#35299;&#37322;&#24341;&#20837;&#35745;&#31639;&#39033;&#30446;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#35843;&#26159;&#19978;&#28023;&#35805;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#26159;&#19968;&#31181;&#20027;&#35201;&#22312;&#19978;&#28023;&#22478;&#24066;&#20351;&#29992;&#30340;&#21556;&#35821;&#26041;&#35328;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20851;&#20110;&#19978;&#28023;&#35805;TTS&#65288;&#25991;&#26412;&#21040;&#35821;&#38899;&#65289;&#30340;&#30740;&#31350;&#65292;&#22914;&#33529;&#26524;&#30340;VoiceOver&#65292;&#22312;&#22768;&#35843;&#36830;&#35835;&#65288;&#23588;&#20854;&#26159;&#24038;&#20391;&#20248;&#21183;&#36830;&#35835;&#65289;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#25991;&#26412;&#39044;&#22788;&#29702;&#36807;&#31243;&#20013;&#36827;&#34892;&#35789;&#20998;&#21106;&#65292;&#23637;&#31034;&#20102;&#25552;&#39640;TTS&#27169;&#22411;&#20013;&#22768;&#35843;&#36830;&#35835;&#36136;&#37327;&#30340;&#26041;&#27861;&#12290;&#21516;&#19968;&#20010;&#35789;&#20869;&#30340;&#38899;&#33410;&#34987;&#29992;&#29305;&#27530;&#31526;&#21495;&#26631;&#27880;&#65292;&#20316;&#20026;LD&#39046;&#22495;&#35821;&#35843;&#20449;&#24687;&#30340;&#20195;&#29702;&#12290;&#19982;&#36890;&#24120;&#20027;&#35201;&#29992;&#20110;&#38745;&#24577;&#20572;&#39039;&#30340;&#35821;&#35843;&#27880;&#37322;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#35777;&#26126;&#20102;&#35821;&#35843;&#27880;&#37322;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#21160;&#24577;&#35821;&#35843;&#29616;&#35937;&#12290;&#25105;&#26399;&#24453;&#36825;&#20010;&#39033;&#30446;&#25104;&#20026;&#23558;&#19978;&#28023;&#35805;&#30340;&#24418;&#24335;&#21270;&#35821;&#35328;&#23398;&#35299;&#37322;&#24341;&#20837;&#35745;&#31639;&#39033;&#30446;&#30340;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tone is a crucial component of the prosody of Shanghainese, a Wu Chinese variety spoken primarily in urban Shanghai. Tone sandhi, which applies to all multi-syllabic words in Shanghainese, then, is key to natural-sounding speech. Unfortunately, recent work on Shanghainese TTS (text-to-speech) such as Apple's VoiceOver has shown poor performance with tone sandhi, especially LD (left-dominant sandhi). Here I show that word segmentation during text preprocessing can improve the quality of tone sandhi production in TTS models. Syllables within the same word are annotated with a special symbol, which serves as a proxy for prosodic information of the domain of LD. Contrary to the common practice of using prosodic annotation mainly for static pauses, this paper demonstrates that prosodic annotation can also be applied to dynamic tonal phenomena. I anticipate this project to be a starting point for bringing formal linguistic accounts of Shanghainese into computational projects. Too long have w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;MBTI&#20154;&#26684;&#27979;&#35797;&#20316;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#19981;&#21516;LLM&#30340;&#20010;&#24615;&#31867;&#22411;&#12289;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25913;&#21464;&#20010;&#24615;&#31867;&#22411;&#30340;&#21487;&#33021;&#24615;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#20010;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.16180</link><description>&lt;p&gt;
LLM&#26159;&#21542;&#20855;&#26377;&#20010;&#24615;&#65311;MBTI&#20154;&#26684;&#27979;&#35797;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models. (arXiv:2307.16180v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;MBTI&#20154;&#26684;&#27979;&#35797;&#20316;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#19981;&#21516;LLM&#30340;&#20010;&#24615;&#31867;&#22411;&#12289;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25913;&#21464;&#20010;&#24615;&#31867;&#22411;&#30340;&#21487;&#33021;&#24615;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#20010;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20854;&#30693;&#35782;&#23384;&#20648;&#33021;&#21147;&#25509;&#36817;&#20154;&#31867;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#20808;&#36827;&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#20915;LLM&#30340;&#20262;&#29702;&#38382;&#39064;&#21644;&#22916;&#24819;&#38382;&#39064;&#65292;&#20351;&#20854;&#26356;&#21152;&#25509;&#36817;&#20154;&#31867;&#20215;&#20540;&#35266;&#12290;&#36825;&#33258;&#28982;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#33021;&#21147;&#30340;LLM&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#30340;&#20010;&#24615;&#65311;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;Myers-Briggs&#31867;&#22411;&#25351;&#26631;(MBTI)&#65292;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#20154;&#26684;&#35780;&#20272;&#24037;&#20855;&#65292;&#20316;&#20026;LLM&#35780;&#20272;&#25351;&#26631;&#30340;&#21487;&#34892;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23558;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#26469;&#30740;&#31350;&#65306;1&#65289;&#19981;&#21516;LLM&#30340;&#20010;&#24615;&#31867;&#22411;&#65292;2&#65289;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25913;&#21464;&#20010;&#24615;&#31867;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;3&#65289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#20010;&#24615;&#12290;&#23613;&#31649;MBTI&#19981;&#26159;&#20005;&#26684;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#20294;&#20173;&#28982;&#21487;&#20197;&#29992;&#26469;&#35780;&#20272;LLM&#30340;&#20010;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of large language models (LLMs) has made significant progress, and their knowledge storage capacity is approaching that of human beings. Furthermore, advanced techniques, such as prompt learning and reinforcement learning, are being employed to address ethical concerns and hallucination problems associated with LLMs, bringing them closer to aligning with human values. This situation naturally raises the question of whether LLMs with human-like abilities possess a human-like personality? In this paper, we aim to investigate the feasibility of using the Myers-Briggs Type Indicator (MBTI), a widespread human personality assessment tool, as an evaluation metric for LLMs. Specifically, extensive experiments will be conducted to explore: 1) the personality types of different LLMs, 2) the possibility of changing the personality types by prompt engineering, and 3) How does the training dataset affect the model's personality. Although the MBTI is not a rigorous assessment, it can stil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25143;&#21487;&#25511;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#35843;&#33410;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#22238;&#24212;&#26102;&#21019;&#36896;&#21147;&#21644;&#23545;&#22806;&#37096;&#30693;&#35782;&#30340;&#24544;&#35802;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#36825;&#31181;&#26426;&#21046;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#25968;&#20540;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#21270;&#36807;&#31243;&#35745;&#31639;&#26631;&#35760;&#30340;&#31243;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#29992;&#25143;&#23545;&#27169;&#22411;&#30340;&#20381;&#36182;&#31243;&#24230;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.16139</link><description>&lt;p&gt;
&#29992;&#25143;&#21487;&#25511;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#34701;&#21512;&#65306;&#24179;&#34913;&#21019;&#36896;&#21147;&#21644;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination. (arXiv:2307.16139v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25143;&#21487;&#25511;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#35843;&#33410;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#22238;&#24212;&#26102;&#21019;&#36896;&#21147;&#21644;&#23545;&#22806;&#37096;&#30693;&#35782;&#30340;&#24544;&#35802;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#36825;&#31181;&#26426;&#21046;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#25968;&#20540;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#21270;&#36807;&#31243;&#35745;&#31639;&#26631;&#35760;&#30340;&#31243;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#29992;&#25143;&#23545;&#27169;&#22411;&#30340;&#20381;&#36182;&#31243;&#24230;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#29983;&#25104;&#22810;&#26679;&#12289;&#30456;&#20851;&#19988;&#26377;&#21019;&#36896;&#24615;&#30340;&#22238;&#24212;&#33021;&#21147;&#32780;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#23613;&#31649;LLMs&#20855;&#26377;&#36825;&#20123;&#20248;&#28857;&#65292;&#20294;&#22312;&#21019;&#36896;&#21147;&#21644;&#23545;&#22806;&#37096;&#30693;&#35782;&#30340;&#24544;&#35802;&#24230;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#29992;&#25143;&#21487;&#25511;&#26426;&#21046;&#65292;&#29992;&#20110;&#35843;&#33410;LLM&#22312;&#24819;&#35937;&#33021;&#21147;&#21644;&#19982;&#20107;&#23454;&#20449;&#24687;&#30340;&#19968;&#33268;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;LLM&#30340;&#35757;&#32451;&#30340;&#24494;&#35843;&#38454;&#27573;&#20013;&#24341;&#20837;&#19968;&#20010;&#34920;&#31034;&#29983;&#25104;&#22238;&#24212;&#20013;&#23545;&#21442;&#32771;&#30693;&#35782;&#24544;&#35802;&#24230;&#31243;&#24230;&#30340;&#25968;&#20540;&#26631;&#35760;&#12290;&#36825;&#20010;&#31243;&#24230;&#26159;&#36890;&#36807;&#33258;&#21160;&#21270;&#36807;&#31243;&#35745;&#31639;&#30340;&#65292;&#35813;&#36807;&#31243;&#20351;&#29992;ROUGE&#20998;&#25968;&#34913;&#37327;&#35789;&#27719;&#37325;&#21472;&#65292;&#20351;&#29992;Sentence-BERT&#23884;&#20837;&#34913;&#37327;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;LLM&#30340;&#33258;&#25105;&#35780;&#20272;&#20998;&#25968;&#12290;&#22312;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#29992;&#25143;&#21487;&#20197;&#25805;&#20316;&#36825;&#20010;&#25968;&#20540;&#26631;&#35760;&#65292;&#20174;&#32780;&#25511;&#21046;LLM&#23545;&#22806;&#37096;&#30693;&#35782;&#30340;&#20381;&#36182;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern dialogue systems, the use of Large Language Models (LLMs) has grown exponentially due to their capacity to generate diverse, relevant, and creative responses. Despite their strengths, striking a balance between the LLMs' creativity and their faithfulness to external knowledge remains a key challenge. This paper presents an innovative user-controllable mechanism that modulates the balance between an LLM's imaginative capabilities and its adherence to factual information. Our approach incorporates a numerical tag during the fine-tuning phase of the LLM's training, representing the degree of faithfulness to the reference knowledge in the generated responses. This degree is computed through an automated process that measures lexical overlap using ROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's self-evaluation score. During model inference, users can manipulate this numerical tag, thus controlling the degree of the LLM's reliance on external knowledg
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SEED-Bench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;SEED-Bench&#21253;&#25324;19K&#20010;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#28085;&#30422;&#20102;&#22270;&#20687;&#21644;&#35270;&#39057;&#27169;&#24577;&#31561;12&#20010;&#35780;&#20272;&#32500;&#24230;&#12290;&#36890;&#36807;&#20154;&#24037;&#27880;&#37322;&#25552;&#20379;&#30340;&#27491;&#30830;&#36873;&#39033;&#65292;&#33021;&#22815;&#23458;&#35266;&#39640;&#25928;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.16125</link><description>&lt;p&gt;
SEED-Bench: &#29992;&#29983;&#25104;&#24335;&#29702;&#35299;&#23545;&#22810;&#27169;&#24577;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension. (arXiv:2307.16125v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16125
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SEED-Bench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;SEED-Bench&#21253;&#25324;19K&#20010;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#28085;&#30422;&#20102;&#22270;&#20687;&#21644;&#35270;&#39057;&#27169;&#24577;&#31561;12&#20010;&#35780;&#20272;&#32500;&#24230;&#12290;&#36890;&#36807;&#20154;&#24037;&#27880;&#37322;&#25552;&#20379;&#30340;&#27491;&#30830;&#36873;&#39033;&#65292;&#33021;&#22815;&#23458;&#35266;&#39640;&#25928;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22522;&#20110;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23637;&#31034;&#20986;&#20102;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;SEED-Bench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35299;&#20915;&#20102;&#23545;MLLMs&#20013;&#29983;&#25104;&#24335;&#29702;&#35299;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#36825;&#26159;&#23545;&#29983;&#25104;&#24335;&#27169;&#22411;&#20840;&#38754;&#35780;&#20272;&#30340;&#19968;&#20010;&#21021;&#27493;&#27493;&#39588;&#12290;SEED-Bench&#21253;&#25324;19K&#20010;&#20934;&#30830;&#30340;&#20154;&#24037;&#27880;&#37322;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;&#27604;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#22823;6&#20493;&#65289;&#65292;&#28085;&#30422;&#20102;&#21253;&#25324;&#22270;&#20687;&#21644;&#35270;&#39057;&#27169;&#24577;&#22312;&#20869;&#30340;12&#20010;&#35780;&#20272;&#32500;&#24230;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#27969;&#31243;&#26469;&#29983;&#25104;&#38024;&#23545;&#29305;&#23450;&#35780;&#20272;&#32500;&#24230;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#25972;&#21512;&#20102;&#33258;&#21160;&#31579;&#36873;&#21644;&#25163;&#21160;&#39564;&#35777;&#36807;&#31243;&#12290;&#36890;&#36807;&#20154;&#24037;&#27880;&#37322;&#33719;&#24471;&#22320;&#38754;&#23454;&#20917;&#36873;&#39033;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#33021;&#22815;&#23458;&#35266;&#39640;&#25928;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;
Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, elim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35782;&#21035;&#26032;&#20852;&#31038;&#20132;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#31038;&#20132;&#25968;&#25454;&#36827;&#34892;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16082</link><description>&lt;p&gt;
EnrichEvent: &#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#20026;&#26032;&#20986;&#29616;&#30340;&#20107;&#20214;&#25552;&#20379;&#20016;&#23500;&#30340;&#31038;&#20132;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction. (arXiv:2307.16082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35782;&#21035;&#26032;&#20852;&#31038;&#20132;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#31038;&#20132;&#25968;&#25454;&#36827;&#34892;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#24179;&#21488;&#24050;&#25104;&#20026;&#20256;&#25773;&#21644;&#35752;&#35770;&#30495;&#23454;&#20107;&#20214;&#20449;&#24687;&#30340;&#20851;&#38190;&#24179;&#21488;&#65292;&#20026;&#21450;&#26089;&#21457;&#29616;&#26377;&#26032;&#38395;&#20215;&#20540;&#30340;&#20107;&#20214;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#20165;&#21033;&#29992;&#20851;&#38190;&#35789;&#31361;&#21457;&#24615;&#25110;&#32593;&#32476;&#32467;&#26500;&#26469;&#26816;&#27979;&#28909;&#28857;&#20107;&#20214;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#20107;&#20214;&#21644;&#31038;&#20132;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#32780;&#35328;&#65292;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#22312;&#36798;&#21040;&#36235;&#21183;&#29366;&#24577;&#20043;&#21069;&#35782;&#21035;&#20986;&#26032;&#20986;&#29616;&#30340;&#31038;&#20132;&#20107;&#20214;&#12290;&#31038;&#20132;&#25968;&#25454;&#65292;&#20363;&#22914;&#25512;&#25991;&#65292;&#20855;&#26377;&#25340;&#20889;&#38169;&#35823;&#12289;&#19981;&#23436;&#25972;&#24615;&#12289;&#27495;&#20041;&#24615;&#21644;&#35821;&#35328;&#19981;&#35268;&#33539;&#24615;&#65292;&#20197;&#21450;&#24847;&#35265;&#26041;&#38754;&#30340;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#26469;&#23398;&#20064;&#20107;&#20214;&#30340;&#28436;&#21464;&#29305;&#24449;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20960;&#20046;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#27969;&#24335;&#31038;&#20132;&#25968;&#25454;&#30340;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social platforms have emerged as a crucial platform for disseminating and discussing information about real-life events, which offers an excellent opportunity for early detection of newsworthy events. However, most existing approaches for event detection solely exploit keyword burstiness or network structures to detect hot events. Thus, they often fail to identify emerging social events before reaching a trending state regarding the challenging nature of events and social data. Social data, e.g., tweets, is characterized by misspellings, incompleteness, ambiguity, and irregular language, as well as variation in aspects of opinions. Moreover, learning the evolving characteristics of the events utilizing limited contextual knowledge is almost infeasible for machine learning models. To address these problems, in this paper, we propose a framework that exploits the lexical, semantic, and contextual representations of streaming social data. In particular, we leverage contextual knowledge to
&lt;/p&gt;</description></item><item><title>TacoBot&#26159;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#25968;&#23383;&#21161;&#25163;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#22797;&#26434;&#30340;&#23454;&#38469;&#20219;&#21153;&#12290;&#23427;&#20855;&#22791;&#21327;&#20316;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#20307;&#39564;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#35757;&#32451;&#20808;&#36827;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#20379;&#39640;&#25928;&#30340;&#20219;&#21153;&#36741;&#21161;&#12290;&#20316;&#20026;&#24320;&#28304;&#26694;&#26550;&#65292;TacoBot&#21487;&#20197;&#20316;&#20026;&#37096;&#32626;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#23454;&#29992;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.16081</link><description>&lt;p&gt;
&#25506;&#32034;&#21327;&#20316;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#65306;Roll Up Your Sleeves
&lt;/p&gt;
&lt;p&gt;
Roll Up Your Sleeves: Working with a Collaborative and Engaging Task-Oriented Dialogue System. (arXiv:2307.16081v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16081
&lt;/p&gt;
&lt;p&gt;
TacoBot&#26159;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#25968;&#23383;&#21161;&#25163;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#22797;&#26434;&#30340;&#23454;&#38469;&#20219;&#21153;&#12290;&#23427;&#20855;&#22791;&#21327;&#20316;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#20307;&#39564;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#35757;&#32451;&#20808;&#36827;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#20379;&#39640;&#25928;&#30340;&#20219;&#21153;&#36741;&#21161;&#12290;&#20316;&#20026;&#24320;&#28304;&#26694;&#26550;&#65292;TacoBot&#21487;&#20197;&#20316;&#20026;&#37096;&#32626;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#23454;&#29992;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;TacoBot&#65292;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#25968;&#23383;&#21161;&#25163;&#65292;&#26088;&#22312;&#36890;&#36807;&#22810;&#27493;&#39588;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#22797;&#26434;&#30340;&#23454;&#38469;&#20219;&#21153;&#12290;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#28921;&#39274;&#21644;&#25805;&#20316;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#21327;&#20316;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#20307;&#39564;&#12290;TacoBot&#20855;&#22791;&#35821;&#35328;&#29702;&#35299;&#12289;&#23545;&#35805;&#31649;&#29702;&#21644;&#21709;&#24212;&#29983;&#25104;&#32452;&#20214;&#65292;&#25903;&#25345;&#24378;&#22823;&#30340;&#25628;&#32034;&#24341;&#25806;&#65292;&#30830;&#20445;&#39640;&#25928;&#30340;&#20219;&#21153;&#36741;&#21161;&#12290;&#20026;&#20102;&#22686;&#24378;&#23545;&#35805;&#20307;&#39564;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#25506;&#32034;&#20102;&#19968;&#31995;&#21015;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#25345;&#32493;&#35757;&#32451;&#20808;&#36827;&#30340;&#31070;&#32463;&#27169;&#22411;&#12290;TacoBot&#26159;&#25105;&#20204;&#22312;&#39318;&#23626;Alexa Prize TaskBot Challenge&#20013;&#21462;&#24471;&#31532;&#19977;&#21517;&#30340;&#25104;&#21151;&#32463;&#39564;&#30340;&#24310;&#32493;&#12290;&#25105;&#20204;&#23558;TacoBot&#20316;&#20026;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#25552;&#20379;&#65292;&#21487;&#20316;&#20026;&#37096;&#32626;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#23454;&#29992;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce TacoBot, a user-centered task-oriented digital assistant designed to guide users through complex real-world tasks with multiple steps. Covering a wide range of cooking and how-to tasks, we aim to deliver a collaborative and engaging dialogue experience. Equipped with language understanding, dialogue management, and response generation components supported by a robust search engine, TacoBot ensures efficient task assistance. To enhance the dialogue experience, we explore a series of data augmentation strategies using LLMs to train advanced neural models continuously. TacoBot builds upon our successful participation in the inaugural Alexa Prize TaskBot Challenge, where our team secured third place among ten competing teams. We offer TacoBot as an open-source framework that serves as a practical example for deploying task-oriented dialogue systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Iro&#769;yi&#768;nSpeech&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#32422;&#40065;&#24052;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#21487;&#29992;&#20110;TTS&#21644;ASR&#20219;&#21153;&#65292;&#21253;&#21547;38.5&#23567;&#26102;&#30340;&#25968;&#25454;&#65292;&#30001;80&#21517;&#24535;&#24895;&#32773;&#24405;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.16071</link><description>&lt;p&gt;
Iro&#769;yi&#768;nSpeech&#65306;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#32422;&#40065;&#24052;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
\`{I}r\`{o}y\`{i}nSpeech: A multi-purpose Yor\`{u}b\'{a} Speech Corpus. (arXiv:2307.16071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Iro&#769;yi&#768;nSpeech&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#32422;&#40065;&#24052;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#21487;&#29992;&#20110;TTS&#21644;ASR&#20219;&#21153;&#65292;&#21253;&#21547;38.5&#23567;&#26102;&#30340;&#25968;&#25454;&#65292;&#30001;80&#21517;&#24535;&#24895;&#32773;&#24405;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Iro&#769;yi&#768;nSpeech&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#21040;&#24076;&#26395;&#22686;&#21152;&#39640;&#36136;&#37327;&#12289;&#20813;&#36153;&#21487;&#29992;&#30340;&#24403;&#20195;&#32422;&#40065;&#24052;&#35821;&#35821;&#38899;&#30340;&#24433;&#21709;&#32780;&#21019;&#24314;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#22810;&#29992;&#36884;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;TTS&#21644;ASR&#20219;&#21153;&#12290;&#25105;&#20204;&#20174;&#26032;&#38395;&#21644;&#21019;&#24847;&#20889;&#20316;&#39046;&#22495;&#25910;&#38598;&#20102;&#25991;&#23383;&#21477;&#23376;&#65292;&#22312;&#24320;&#25918;&#25480;&#26435;&#65288;CC-BY-4.0&#65289;&#19979;&#36827;&#34892;&#31579;&#36873;&#65292;&#24182;&#30001;&#22810;&#20301;&#21457;&#38899;&#20154;&#24405;&#21046;&#27599;&#20010;&#21477;&#23376;&#12290;&#25105;&#20204;&#23558;5000&#20010;&#21457;&#38899;&#21477;&#25552;&#20379;&#32473;Common Voice&#24179;&#21488;&#65292;&#22312;&#32447;&#20247;&#21253;&#36716;&#24405;&#12290;&#35813;&#25968;&#25454;&#38598;&#24635;&#20849;&#26377;38.5&#23567;&#26102;&#30340;&#25968;&#25454;&#65292;&#30001;80&#21517;&#24535;&#24895;&#32773;&#24405;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the \`{I}r\`{o}y\`{i}nSpeech corpus -- a new dataset influenced by a desire to increase the amount of high quality, freely available, contemporary Yor\`{u}b\'{a} speech. We release a multi-purpose dataset that can be used for both TTS and ASR tasks. We curated text sentences from the news and creative writing domains under an open license i.e., CC-BY-4.0 and had multiple speakers record each sentence. We provide 5000 of our utterances to the Common Voice platform to crowdsource transcriptions online. The dataset has 38.5 hours of data in total, recorded by 80 volunteers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#32599;&#39532;&#23612;&#20122;&#23398;&#26415;&#35789;&#27719;&#34920;&#65288;Ro-AWL&#65289;&#30340;&#33258;&#21160;&#25552;&#21462;&#26041;&#27861;&#21644;&#25968;&#25454;&#65292;&#23558;&#35821;&#26009;&#24211;&#21644;&#35745;&#31639;&#35821;&#35328;&#23398;&#26041;&#27861;&#19982;L2&#23398;&#26415;&#20889;&#20316;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#26681;&#25454;&#19981;&#21516;&#23398;&#31185;&#36827;&#34892;&#20102;&#20998;&#24067;&#65292;Ro-AWL&#21487;&#20379;&#25945;&#23398;&#12289;&#30740;&#31350;&#21644;NLP&#24212;&#29992;&#20813;&#36153;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.16045</link><description>&lt;p&gt;
&#32599;&#39532;&#23612;&#20122;&#23398;&#26415;&#35789;&#27719;&#34920;&#30340;&#33258;&#21160;&#25552;&#21462;&#65306;&#25968;&#25454;&#19982;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Automatic Extraction of the Romanian Academic Word List: Data and Methods. (arXiv:2307.16045v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#32599;&#39532;&#23612;&#20122;&#23398;&#26415;&#35789;&#27719;&#34920;&#65288;Ro-AWL&#65289;&#30340;&#33258;&#21160;&#25552;&#21462;&#26041;&#27861;&#21644;&#25968;&#25454;&#65292;&#23558;&#35821;&#26009;&#24211;&#21644;&#35745;&#31639;&#35821;&#35328;&#23398;&#26041;&#27861;&#19982;L2&#23398;&#26415;&#20889;&#20316;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#26681;&#25454;&#19981;&#21516;&#23398;&#31185;&#36827;&#34892;&#20102;&#20998;&#24067;&#65292;Ro-AWL&#21487;&#20379;&#25945;&#23398;&#12289;&#30740;&#31350;&#21644;NLP&#24212;&#29992;&#20813;&#36153;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#32599;&#39532;&#23612;&#20122;&#23398;&#26415;&#35789;&#27719;&#34920;&#65288;Ro-AWL&#65289;&#33258;&#21160;&#25552;&#21462;&#30340;&#26041;&#27861;&#21644;&#25968;&#25454;&#12290;&#23398;&#26415;&#35789;&#27719;&#34920;&#22312;L2&#21644;L1&#25945;&#23398;&#29615;&#22659;&#20013;&#37117;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#23545;&#20110;&#32599;&#39532;&#23612;&#20122;&#35821;&#26469;&#35828;&#65292;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#36825;&#26679;&#30340;&#36164;&#28304;&#12290;Ro-AWL&#26159;&#36890;&#36807;&#23558;&#35821;&#26009;&#24211;&#21644;&#35745;&#31639;&#35821;&#35328;&#23398;&#26041;&#27861;&#19982;L2&#23398;&#26415;&#20889;&#20316;&#26041;&#27861;&#30456;&#32467;&#21512;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65306;&#65288;a&#65289;&#29616;&#26377;&#25968;&#25454;&#65292;&#20363;&#22914;&#22522;&#20110;ROMBAC&#35821;&#26009;&#24211;&#30340;&#32599;&#39532;&#23612;&#20122;&#39057;&#29575;&#21015;&#34920;&#65292;&#21644;&#65288;b&#65289;&#33258;&#32534;&#25968;&#25454;&#65292;&#20363;&#22914;&#19987;&#23478;&#23398;&#26415;&#20889;&#20316;&#35821;&#26009;&#24211;EXPRES&#12290;&#20026;&#20102;&#26500;&#24314;&#23398;&#26415;&#35789;&#27719;&#34920;&#65292;&#25105;&#20204;&#36981;&#24490;&#20102;&#20026;&#33521;&#35821;&#35821;&#35328;&#24314;&#31435;&#23398;&#26415;&#35789;&#27719;&#34920;&#30340;&#26041;&#27861;&#12290;Ro-AWL&#30340;&#29305;&#24449;&#20998;&#24067;&#65288;&#24635;&#20307;&#20998;&#24067;&#65292;&#35789;&#24615;&#20998;&#24067;&#65289;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#19968;&#33268;&#65292;&#23558;&#20854;&#20998;&#20026;&#22235;&#20010;&#23398;&#31185;&#25968;&#25454;&#38598;&#12290;Ro-AWL&#21487;&#20379;&#25945;&#23398;&#12289;&#30740;&#31350;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20813;&#36153;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the methodology and data used for the automatic extraction of the Romanian Academic Word List (Ro-AWL). Academic Word Lists are useful in both L2 and L1 teaching contexts. For the Romanian language, no such resource exists so far. Ro-AWL has been generated by combining methods from corpus and computational linguistics with L2 academic writing approaches. We use two types of data: (a) existing data, such as the Romanian Frequency List based on the ROMBAC corpus, and (b) self-compiled data, such as the expert academic writing corpus EXPRES. For constructing the academic word list, we follow the methodology for building the Academic Vocabulary List for the English language. The distribution of Ro-AWL features (general distribution, POS distribution) into four disciplinary datasets is in line with previous research. Ro-AWL is freely available and can be used for teaching, research and NLP applications.
&lt;/p&gt;</description></item><item><title>Okapi&#26159;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#20248;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#35299;&#20915;&#20102;&#30446;&#21069;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21482;&#38024;&#23545;&#33521;&#35821;&#21644;&#23569;&#25968;&#27969;&#34892;&#35821;&#35328;&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.16039</link><description>&lt;p&gt;
Okapi: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#20248;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback. (arXiv:2307.16039v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16039
&lt;/p&gt;
&lt;p&gt;
Okapi&#26159;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#20248;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#35299;&#20915;&#20102;&#30446;&#21069;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21482;&#38024;&#23545;&#33521;&#35821;&#21644;&#23569;&#25968;&#27969;&#34892;&#35821;&#35328;&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#26159;&#25351;&#20196;&#35843;&#20248;&#65292;&#23427;&#26377;&#21161;&#20110;&#23558;&#27169;&#22411;&#30340;&#21709;&#24212;&#19982;&#20154;&#31867;&#39044;&#26399;&#23545;&#40784;&#65292;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#20004;&#31181;&#20027;&#35201;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#26159;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#30446;&#21069;&#24050;&#24212;&#29992;&#20110;&#29983;&#20135;&#26368;&#20339;&#30340;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;ChatGPT&#65289;&#12290;&#20026;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#30740;&#31350;&#21644;&#24320;&#21457;&#24037;&#20316;&#20013;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#26368;&#36817;&#36824;&#25512;&#20986;&#20102;&#21508;&#31181;&#32463;&#36807;&#25351;&#20196;&#35843;&#20248;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;Alpaca&#12289;Vicuna&#31561;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#20165;&#23545;&#33521;&#35821;&#21644;&#23569;&#25968;&#27969;&#34892;&#35821;&#35328;&#36827;&#34892;&#20102;&#25351;&#20196;&#35843;&#20248;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20840;&#29699;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#24433;&#21709;&#21147;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;&#26368;&#36817;&#26377;&#19968;&#20123;&#25506;&#32034;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#20248;&#30340;&#24037;&#20316;&#65292;&#20294;&#30446;&#21069;&#21482;&#20351;&#29992;&#20102;SFT&#20316;&#20026;&#25351;&#20196;&#35843;&#20248;&#30340;&#21807;&#19968;&#26041;&#27861;&#12290;&#36825;&#24050;&#32463;&#23384;&#22312;&#20102;&#19968;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has lef
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#23545;&#35805;&#31995;&#32479;&#19982;&#25968;&#25454;&#21487;&#35270;&#21270;&#65292;&#36890;&#36807;&#29992;&#25143;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#20132;&#20114;&#26469;&#29983;&#25104;&#25968;&#25454;&#21487;&#35270;&#21270;&#12290;&#30740;&#31350;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#37325;&#26041;&#27861;&#26469;&#23454;&#29616;&#27492;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.16013</link><description>&lt;p&gt;
&#23558;&#23545;&#35805;&#31995;&#32479;&#19982;&#25968;&#25454;&#21487;&#35270;&#21270;&#32467;&#21512;&#65306;&#20174;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#20013;&#29983;&#25104;&#20132;&#20114;&#24335;&#25968;&#25454;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Marrying Dialogue Systems with Data Visualization: Interactive Data Visualization Generation from Natural Language Conversations. (arXiv:2307.16013v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#23545;&#35805;&#31995;&#32479;&#19982;&#25968;&#25454;&#21487;&#35270;&#21270;&#65292;&#36890;&#36807;&#29992;&#25143;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#20132;&#20114;&#26469;&#29983;&#25104;&#25968;&#25454;&#21487;&#35270;&#21270;&#12290;&#30740;&#31350;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#37325;&#26041;&#27861;&#26469;&#23454;&#29616;&#27492;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21487;&#35270;&#21270;&#24050;&#32463;&#25104;&#20026;&#24066;&#22330;&#19978;&#27969;&#34892;&#30340;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#33021;&#26377;&#25928;&#22320;&#23637;&#31034;&#22823;&#37327;&#30340;&#25968;&#25454;&#35265;&#35299;&#12290;&#20026;&#20102;&#38477;&#20302;&#20351;&#29992;&#25968;&#25454;&#21487;&#35270;&#21270;&#30340;&#38376;&#27099;&#65292;&#30740;&#31350;&#30028;&#24050;&#32463;&#24320;&#22987;&#30740;&#31350;&#33258;&#21160;&#25968;&#25454;&#21487;&#35270;&#21270;&#20219;&#21153;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#21040;&#21487;&#35270;&#21270;&#36716;&#25442;&#65288;&#27491;&#24335;&#31216;&#20026;&#25991;&#26412;&#21040;&#21487;&#35270;&#21270;&#65289;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;&#21040;&#21487;&#35270;&#21270;&#20551;&#35774;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#24050;&#32463;&#26377;&#19968;&#20010;&#33391;&#22909;&#30340;&#32452;&#32455;&#24182;&#19988;&#29992;&#19968;&#20010;&#21477;&#23376;&#26469;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22797;&#26434;&#30340;&#25968;&#25454;&#21487;&#35270;&#21270;&#38656;&#35201;&#36890;&#36807;&#21487;&#35270;&#21270;&#31995;&#32479;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#36830;&#32493;&#20132;&#20114;&#26469;&#23436;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;CoVis&#65292;&#21363;&#23545;&#35805;&#24335;&#25991;&#26412;&#21040;&#21487;&#35270;&#21270;&#65292;&#26088;&#22312;&#36890;&#36807;&#29992;&#25143;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#22810;&#20010;&#20132;&#20114;&#26469;&#26500;&#24314;&#25968;&#25454;&#21487;&#35270;&#21270;&#12290;&#22240;&#20026;&#36825;&#20010;&#20219;&#21153;&#22312;&#25991;&#29486;&#20013;&#36824;&#27809;&#26377;&#34987;&#30740;&#31350;&#65292;&#25152;&#20197;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;Dial-NVBench&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#29992;&#25143;&#30340;&#19968;&#31995;&#21015;&#26597;&#35810;&#21644;&#31995;&#32479;&#30340;&#22238;&#24212;&#30340;&#23545;&#35805;&#20250;&#35805;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#37325;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data visualization (DV) has become the prevailing tool in the market due to its effectiveness into illustrating insights in vast amounts of data. To lower the barrier of using DVs, automatic DV tasks, such as natural language question (NLQ) to visualization translation (formally called text-to-vis), have been investigated in the research community. However, text-to-vis assumes the NLQ to be well-organized and expressed in a single sentence. However, in real-world settings, complex DV is needed through consecutive exchanges between the DV system and the users. In this paper, we propose a new task named CoVis, short for Conversational text-to-Visualization, aiming at constructing DVs through a series of interactions between users and the system. Since it is the task which has not been studied in the literature, we first build a benchmark dataset named Dial-NVBench, including dialogue sessions with a sequence of queries from a user and responses from the system. Then, we propose a multi-m
&lt;/p&gt;</description></item><item><title>RoCar&#26159;&#19968;&#31181;&#21033;&#29992;&#20851;&#31995;&#32593;&#32476;&#26500;&#24314;&#20219;&#21153;&#22270;&#24182;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35780;&#20272;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#35760;&#24518;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26497;&#22823;&#30340;&#38543;&#26426;&#24615;&#30830;&#20445;&#20102;&#35780;&#20272;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15997</link><description>&lt;p&gt;
RoCar:&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#32593;&#32476;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RoCar: A Relationship Network-based Evaluation Method to Large Language Models. (arXiv:2307.15997v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15997
&lt;/p&gt;
&lt;p&gt;
RoCar&#26159;&#19968;&#31181;&#21033;&#29992;&#20851;&#31995;&#32593;&#32476;&#26500;&#24314;&#20219;&#21153;&#22270;&#24182;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35780;&#20272;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#35760;&#24518;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26497;&#22823;&#30340;&#38543;&#26426;&#24615;&#30830;&#20445;&#20102;&#35780;&#20272;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#33021;&#21147;&#30340;&#22797;&#26434;&#24615;&#65292;&#22914;&#20309;&#21512;&#29702;&#35780;&#20272;LLMs&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#38656;&#35201;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RoCar&#26041;&#27861;&#65292;&#21033;&#29992;&#23450;&#20041;&#30340;&#22522;&#26412;&#27169;&#24335;&#38543;&#26426;&#26500;&#24314;&#19968;&#20010;&#20219;&#21153;&#22270;&#65292;&#24182;&#22522;&#20110;&#20219;&#21153;&#22270;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35780;&#20272;&#20219;&#21153;&#65292;&#20998;&#21035;&#35780;&#20272;LLMs&#30340;&#25512;&#29702;&#21644;&#35760;&#24518;&#33021;&#21147;&#12290;&#30001;&#20110;&#20219;&#21153;&#26500;&#24314;&#36807;&#31243;&#30340;&#26497;&#22823;&#38543;&#26426;&#24615;&#65292;&#21487;&#20197;&#30830;&#20445;&#34987;&#27979;&#35797;&#30340;LLMs&#20013;&#27809;&#26377;&#19968;&#20010;&#30452;&#25509;&#23398;&#20064;&#20102;&#35780;&#20272;&#20219;&#21153;&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#35780;&#20272;&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have received increasing attention. However, due to the complexity of its capabilities, how to rationally evaluate the capabilities of LLMs is still a task to be solved. We propose the RoCar method, which utilizes the defined basic schemas to randomly construct a task graph and generates natural language evaluation tasks based on the task graph to evaluate the reasoning and memory abilities of LLMs respectively. Due to the very large randomness of the task construction process, it is possible to ensure that none of the LLMs to be tested has directly learned the evaluation tasks, guaranteeing the fairness of the evaluation method.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#32534;&#35299;&#30721;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20801;&#35768;&#25991;&#26412;&#27700;&#21360;&#25658;&#24102;&#26356;&#22810;&#21487;&#23450;&#21046;&#21270;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27700;&#21360;&#26041;&#27861;&#32534;&#30721;&#25928;&#29575;&#20302;&#12289;&#19981;&#33021;&#28385;&#36275;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#38656;&#27714;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15992</link><description>&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#32534;&#35299;&#30721;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Codable Text Watermarking for Large Language Models. (arXiv:2307.15992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15992
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#32534;&#35299;&#30721;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20801;&#35768;&#25991;&#26412;&#27700;&#21360;&#25658;&#24102;&#26356;&#22810;&#21487;&#23450;&#21046;&#21270;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27700;&#21360;&#26041;&#27861;&#32534;&#30721;&#25928;&#29575;&#20302;&#12289;&#19981;&#33021;&#28385;&#36275;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#38656;&#27714;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#26085;&#30410;&#27969;&#30021;&#21644;&#36924;&#30495;&#65292;&#26377;&#24517;&#35201;&#35782;&#21035;&#25991;&#26412;&#30340;&#26469;&#28304;&#20197;&#38450;&#27490;LLMs&#30340;&#28389;&#29992;&#12290;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#36890;&#36807;&#23558;&#38544;&#34255;&#30340;&#27169;&#24335;&#27880;&#20837;&#21040;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#24050;&#34987;&#35777;&#23454;&#21487;&#20197;&#21487;&#38752;&#22320;&#21306;&#20998;&#26159;&#21542;&#30001;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;LLMs&#27700;&#21360;&#26041;&#27861;&#22312;&#32534;&#30721;&#25928;&#29575;&#19978;&#23384;&#22312;&#38382;&#39064;&#65288;&#21482;&#21253;&#21547;&#19968;&#20010;&#20301;&#30340;&#20449;&#24687;&#65292;&#21363;&#25991;&#26412;&#26159;&#21542;&#30001;LLMs&#29983;&#25104;&#65289;&#65292;&#24182;&#19988;&#19981;&#33021;&#28789;&#27963;&#22320;&#28385;&#36275;&#19981;&#21516;LLMs&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#22810;&#26679;&#21270;&#20449;&#24687;&#32534;&#30721;&#38656;&#27714;&#65288;&#22914;&#32534;&#30721;&#27169;&#22411;&#29256;&#26412;&#12289;&#29983;&#25104;&#26102;&#38388;&#12289;&#29992;&#25143;ID&#31561;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;LLMs&#30340;&#21487;&#32534;&#35299;&#30721;&#25991;&#26412;&#27700;&#21360;&#65288;CTWL&#65289;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#20801;&#35768;&#25991;&#26412;&#27700;&#21360;&#25658;&#24102;&#26356;&#22810;&#21487;&#23450;&#21046;&#21270;&#30340;&#20449;&#24687;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#27700;&#21360;&#25216;&#26415;&#30340;&#20998;&#31867;&#65292;&#20026;CTWL&#25552;&#20379;&#20102;&#25968;&#23398;&#20844;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#30740;&#31350;&#35270;&#22270;&#65292;&#28085;&#30422;&#20102;CTWL&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) generate texts with increasing fluency and realism, there is a growing need to identify the source of texts to prevent the abuse of LLMs. Text watermarking techniques have proven reliable in distinguishing whether a text is generated by LLMs by injecting hidden patterns into the generated texts. However, we argue that existing watermarking methods for LLMs are encoding-inefficient (only contain one bit of information whether it is generated from an LLM or not) and cannot flexibly meet the diverse information encoding needs (such as encoding model version, generation time, user id, etc.) in different LLMs application scenarios. In this work, we conduct the first systematic study on the topic of Codable Text Watermarking for LLMs (CTWL) that allows text watermarks to carry more customizable information. First of all, we study the taxonomy of LLM watermarking technology and give a mathematical formulation for CTWL. Additionally, we provide a comprehensive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19982;&#22522;&#26412;&#35821;&#35328;&#20219;&#21153;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25216;&#33021;&#20135;&#29983;&#30340;&#26426;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#23450;&#24459;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#23398;&#20064;&#65292;&#24182;&#34920;&#29616;&#20986;&#36829;&#21453;&#36890;&#24120;&#27867;&#21270;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.15936</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25216;&#33021;&#20135;&#29983;&#30340;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory for Emergence of Complex Skills in Language Models. (arXiv:2307.15936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19982;&#22522;&#26412;&#35821;&#35328;&#20219;&#21153;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25216;&#33021;&#20135;&#29983;&#30340;&#26426;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#23450;&#24459;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#23398;&#20064;&#65292;&#24182;&#34920;&#29616;&#20986;&#36829;&#21453;&#36890;&#24120;&#27867;&#21270;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#38598;&#21512;&#21644;&#35757;&#32451;&#35821;&#26009;&#24211;&#25193;&#22823;&#26102;&#65292;&#26032;&#30340;&#25216;&#33021;&#23558;&#22312; AI &#20135;&#21697;&#20013;&#20986;&#29616;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;&#36825;&#31181;&#29616;&#35937;&#23578;&#19981;&#20026;&#20154;&#25152;&#29702;&#35299;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#22522;&#20110;&#26799;&#24230;&#35757;&#32451;&#30340;&#25968;&#23398;&#20998;&#26512;&#25552;&#20379;&#26426;&#26800;&#35299;&#37322;&#20284;&#20046;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#37319;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33879;&#21517;&#30340;&#65288;&#21644;&#32463;&#39564;&#24615;&#30340;&#65289;LLM&#25193;&#23637;&#23450;&#24459;&#21644;&#31616;&#21333;&#30340;&#32479;&#35745;&#26694;&#26550;&#26469;&#20998;&#26512;&#20986;&#29616;&#12290;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;a&#65289;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#23558;LLM&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19982;&#35821;&#35328;&#20219;&#21153;&#22522;&#26412;&#25216;&#33021;&#30340;&#33021;&#21147;&#30456;&#20851;&#32852;&#12290;&#65288;b&#65289;&#25968;&#23398;&#20998;&#26512;&#34920;&#26126;&#65292;&#25193;&#23637;&#23450;&#24459;&#24847;&#21619;&#30528;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#24471;&#38750;&#24120;&#39640;&#25928;&#12290;&#25105;&#20204;&#38750;&#27491;&#24335;&#22320;&#31216;&#20043;&#20026;&#8220;&#24377;&#24339;&#27867;&#21270;&#8221;&#65292;&#22240;&#20026;&#34920;&#38754;&#19978;&#30475;&#65292;&#23427;&#20284;&#20046;&#25552;&#20379;&#20102;&#22312;&#25216;&#33021;&#27700;&#24179;&#19978;&#36829;&#21453;&#36890;&#24120;&#27867;&#21270;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;&#65288;c&#65289;&#24377;&#24339;&#27867;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#20363;&#23376;&#65292;&#21363;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major driver of AI products today is the fact that new skills emerge in language models when their parameter set and training corpora are scaled up. This phenomenon is poorly understood, and a mechanistic explanation via mathematical analysis of gradient-based training seems difficult. The current paper takes a different approach, analysing emergence using the famous (and empirical) Scaling Laws of LLMs and a simple statistical framework. Contributions include: (a) A statistical framework that relates cross-entropy loss of LLMs to competence on the basic skills that underlie language tasks. (b) Mathematical analysis showing that the Scaling Laws imply a strong form of inductive bias that allows the pre-trained model to learn very efficiently. We informally call this {\em slingshot generalization} since naively viewed it appears to give competence levels at skills that violate usual generalization theory. (c) A key example of slingshot generalization, that competence at executing task
&lt;/p&gt;</description></item><item><title>GeneMask&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#22240;&#24207;&#21015;&#23631;&#34109;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#26368;&#39640;&#35268;&#19968;&#21270;&#28857;&#23545;&#28857;&#20114;&#20449;&#24687;&#30340;&#36328;&#24230;&#26469;&#20248;&#21270;&#23631;&#34109;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#65292;&#30456;&#27604;&#20110;DNABert&#21644;LOGO&#31561;&#27169;&#22411;&#65292;&#22312;&#32570;&#20047;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35821;&#20041;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.15933</link><description>&lt;p&gt;
GeneMask: &#24555;&#36895;&#39044;&#35757;&#32451;&#22522;&#22240;&#24207;&#21015;&#20197;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GeneMask: Fast Pretraining of Gene Sequences to Enable Few-Shot Learning. (arXiv:2307.15933v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15933
&lt;/p&gt;
&lt;p&gt;
GeneMask&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#22240;&#24207;&#21015;&#23631;&#34109;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#26368;&#39640;&#35268;&#19968;&#21270;&#28857;&#23545;&#28857;&#20114;&#20449;&#24687;&#30340;&#36328;&#24230;&#26469;&#20248;&#21270;&#23631;&#34109;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#65292;&#30456;&#27604;&#20110;DNABert&#21644;LOGO&#31561;&#27169;&#22411;&#65292;&#22312;&#32570;&#20047;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35821;&#20041;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22914;DNABert&#21644;LOGO&#26088;&#22312;&#23398;&#20064;&#26368;&#20339;&#22522;&#22240;&#34920;&#31034;&#65292;&#24182;&#22312;&#25972;&#20010;&#20154;&#31867;&#21442;&#32771;&#22522;&#22240;&#32452;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#20998;&#35789;&#26041;&#26696;&#20165;&#28041;&#21450;&#31867;&#20284;k-mers&#30340;&#31616;&#21333;&#28369;&#21160;&#31383;&#21475;&#65292;&#19981;&#33021;&#21033;&#29992;&#20219;&#20309;&#22522;&#20110;&#22522;&#22240;&#30340;&#35821;&#20041;&#65292;&#22240;&#27492;&#21487;&#33021;&#20250;&#23548;&#33268;&#65288;&#24179;&#20961;&#30340;&#65289;&#23631;&#34109;&#26131;&#20110;&#39044;&#27979;&#24207;&#21015;&#65292;&#20174;&#32780;&#23548;&#33268;&#20302;&#25928;&#30340;&#23631;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23631;&#34109;&#31639;&#27861;GeneMask&#65292;&#29992;&#20110;&#22522;&#22240;&#24207;&#21015;&#30340;MLM&#35757;&#32451;&#65292;&#20854;&#20013;&#25105;&#20204;&#38543;&#26426;&#35782;&#21035;&#22522;&#22240;&#24207;&#21015;&#20013;&#30340;&#23631;&#34109;&#20013;&#24515;&#20301;&#32622;&#65292;&#24182;&#23616;&#37096;&#36873;&#25321;&#20013;&#24515;&#20301;&#32622;&#21608;&#22260;&#20855;&#26377;&#26368;&#39640;&#35268;&#19968;&#21270;&#28857;&#23545;&#28857;&#20114;&#20449;&#24687;&#65288;NPMI&#65289;&#30340;&#36328;&#24230;&#36827;&#34892;&#23631;&#34109;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#32570;&#20047;&#20154;&#31867;&#29702;&#35299;&#30340;&#35821;&#20041;&#30340;&#24773;&#20917;&#19979;&#65288;&#30456;&#27604;&#20043;&#19979;&#65292;NLP&#20013;&#22266;&#26377;&#22320;&#25552;&#20379;&#21333;&#35789;&#21644;&#30701;&#35821;&#31561;&#35821;&#20041;&#21333;&#20301;&#65289;&#65292;&#22522;&#20110;GeneMask&#30340;&#27169;&#22411;&#22312;&#22235;&#20010;&#22522;&#20934;&#22522;&#22240;&#19978;&#26126;&#26174;&#20248;&#20110;SOTA&#27169;&#22411;&#65288;DNABert&#21644;LOGO&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale language models such as DNABert and LOGO aim to learn optimal gene representations and are trained on the entire Human Reference Genome. However, standard tokenization schemes involve a simple sliding window of tokens like k-mers that do not leverage any gene-based semantics and thus may lead to (trivial) masking of easily predictable sequences and subsequently inefficient Masked Language Modeling (MLM) training. Therefore, we propose a novel masking algorithm, GeneMask, for MLM training of gene sequences, where we randomly identify positions in a gene sequence as mask centers and locally select the span around the mask center with the highest Normalized Pointwise Mutual Information (NPMI) to mask. We observe that in the absence of human-understandable semantics in the genomics domain (in contrast, semantic units like words and phrases are inherently available in NLP), GeneMask-based models substantially outperform the SOTA models (DNABert and LOGO) over four benchmark gene
&lt;/p&gt;</description></item><item><title>ATESA-B{\AE}RT&#26159;&#19968;&#20010;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#24322;&#26500;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#20026;&#26041;&#38754;&#35789;&#25552;&#21462;&#21644;&#26041;&#38754;&#35789;&#24773;&#24863;&#20998;&#26512;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;\textit{argmax}&#22810;&#31867;&#21035;&#20998;&#31867;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#22312;&#26041;&#38754;&#32423;&#21035;&#19978;&#30340;&#31890;&#24230;&#65292;&#25913;&#36827;&#20102;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#22312;&#22810;&#26041;&#38754;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15920</link><description>&lt;p&gt;
ATESA-B{\AE}RT: &#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#24322;&#26500;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ATESA-B{\AE}RT: A Heterogeneous Ensemble Learning Model for Aspect-Based Sentiment Analysis. (arXiv:2307.15920v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15920
&lt;/p&gt;
&lt;p&gt;
ATESA-B{\AE}RT&#26159;&#19968;&#20010;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#24322;&#26500;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#20026;&#26041;&#38754;&#35789;&#25552;&#21462;&#21644;&#26041;&#38754;&#35789;&#24773;&#24863;&#20998;&#26512;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;\textit{argmax}&#22810;&#31867;&#21035;&#20998;&#31867;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#22312;&#26041;&#38754;&#32423;&#21035;&#19978;&#30340;&#31890;&#24230;&#65292;&#25913;&#36827;&#20102;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#22312;&#22810;&#26041;&#38754;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#32447;&#35780;&#35770;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#30340;&#21457;&#23637;&#25104;&#20026;&#20102;&#30830;&#23450;&#28040;&#36153;&#32773;&#23545;&#19981;&#21516;&#20135;&#21697;&#21644;&#26381;&#21153;&#30340;&#24847;&#35265;&#30340;&#21487;&#33021;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#24773;&#24863;&#20998;&#26512;&#24050;&#34987;&#35777;&#26126;&#26159;&#30830;&#23450;&#35780;&#35770;&#25972;&#20307;&#26497;&#24615;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#20026;&#20102;&#25552;&#39640;&#22312;&#26041;&#38754;&#32423;&#21035;&#19978;&#30340;&#31890;&#24230;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#26381;&#21153;&#25110;&#20135;&#21697;&#65292;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#39318;&#20808;&#26088;&#22312;&#35782;&#21035;&#26041;&#38754;&#65292;&#28982;&#21518;&#30830;&#23450;&#29992;&#25143;&#23545;&#23427;&#20204;&#30340;&#24847;&#35265;&#12290;&#36825;&#39033;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#22312;&#20110;&#21516;&#19968;&#35780;&#35770;&#21487;&#33021;&#20250;&#21576;&#29616;&#22810;&#20010;&#26041;&#38754;&#65292;&#27599;&#20010;&#26041;&#38754;&#20855;&#26377;&#33258;&#24049;&#30340;&#26497;&#24615;&#12290;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#36825;&#31181;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;ATESA-B{\AE}RT&#65292;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#24322;&#26500;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#21363;&#26041;&#38754;&#35789;&#25552;&#21462;&#21644;&#26041;&#38754;&#35789;&#24773;&#24863;&#20998;&#26512;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;\textit{argmax}&#22810;&#31867;&#21035;&#20998;&#31867;&#31639;&#27861;&#26469;&#23545;&#26041;&#38754;&#35789;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing volume of online reviews has made possible the development of sentiment analysis models for determining the opinion of customers regarding different products and services. Until now, sentiment analysis has proven to be an effective tool for determining the overall polarity of reviews. To improve the granularity at the aspect level for a better understanding of the service or product, the task of aspect-based sentiment analysis aims to first identify aspects and then determine the user's opinion about them. The complexity of this task lies in the fact that the same review can present multiple aspects, each with its own polarity. Current solutions have poor performance on such data. We address this problem by proposing ATESA-B{\AE}RT, a heterogeneous ensemble learning model for Aspect-Based Sentiment Analysis. Firstly, we divide our problem into two sub-tasks, i.e., Aspect Term Extraction and Aspect Term Sentiment Analysis. Secondly, we use the \textit{argmax} multi-class 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#19982;NPC&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#21644;&#23545;&#35805;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#26469;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.15833</link><description>&lt;p&gt;
&#23545;&#35805;&#22609;&#36896;&#65306;&#36890;&#36807;NPC&#20132;&#20114;&#22686;&#24378;&#26234;&#33021;&#20307;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Dialogue Shaping: Empowering Agents through NPC Interaction. (arXiv:2307.15833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#19982;NPC&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#21644;&#23545;&#35805;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#26469;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;(RL)&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;RL&#26234;&#33021;&#20307;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38656;&#35201;&#25910;&#25947;&#24182;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#30340;&#22823;&#37327;&#27493;&#39588;&#65292;&#23588;&#20854;&#26159;&#22312;&#21160;&#20316;&#31354;&#38388;&#24191;&#27867;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#29615;&#22659;&#20013;&#12290;&#28982;&#32780;&#65292;&#38750;&#29609;&#23478;&#35282;&#33394;(NPCs)&#26377;&#26102;&#20250;&#25317;&#26377;&#28216;&#25103;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#26377;&#21161;&#20110;&#26356;&#24555;&#22320;&#35757;&#32451;RL&#26234;&#33021;&#20307;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#19982;NPC&#26234;&#33021;&#20307;&#36827;&#34892;&#20132;&#20114;&#21644;&#23545;&#35805;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33719;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;(KGs)&#21644;&#25925;&#20107;&#22609;&#36896;&#26469;&#21152;&#36895;RL&#26234;&#33021;&#20307;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
One major challenge in reinforcement learning (RL) is the large amount of steps for the RL agent needs to converge in the training process and learn the optimal policy, especially in text-based game environments where the action space is extensive. However, non-player characters (NPCs) sometimes hold some key information about the game, which can potentially help to train RL agents faster. Thus, this paper explores how to interact and converse with NPC agents to get the key information using large language models (LLMs), as well as incorporate this information to speed up RL agent's training using knowledge graphs (KGs) and Story Shaping.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#21319;&#21644;&#26032;&#20852;&#30340;&#35821;&#20041;&#25512;&#29702;&#12290;&#36890;&#36807;&#22312;&#26426;&#22120;&#20154;&#36712;&#36857;&#25968;&#25454;&#21644;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#19978;&#20849;&#21516;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#21333;&#19968;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#20102;&#21516;&#26102;&#23398;&#20064;&#26426;&#22120;&#20154;&#35266;&#27979;&#21040;&#34892;&#20026;&#26144;&#23556;&#21644;&#21033;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#30340;&#30410;&#22788;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.15818</link><description>&lt;p&gt;
RT-2&#65306;&#35270;&#35273;-&#35821;&#35328;-&#34892;&#21160;&#27169;&#22411;&#23558;&#32593;&#32476;&#30693;&#35782;&#36716;&#21270;&#20026;&#26426;&#22120;&#20154;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. (arXiv:2307.15818v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#21319;&#21644;&#26032;&#20852;&#30340;&#35821;&#20041;&#25512;&#29702;&#12290;&#36890;&#36807;&#22312;&#26426;&#22120;&#20154;&#36712;&#36857;&#25968;&#25454;&#21644;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#19978;&#20849;&#21516;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#21333;&#19968;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#20102;&#21516;&#26102;&#23398;&#20064;&#26426;&#22120;&#20154;&#35266;&#27979;&#21040;&#34892;&#20026;&#26144;&#23556;&#21644;&#21033;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#30340;&#30410;&#22788;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#24212;&#29992;&#20110;&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#65292;&#20197;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#24182;&#23454;&#29616;&#26032;&#20852;&#30340;&#35821;&#20041;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35753;&#21333;&#19968;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27169;&#22411;&#26082;&#33021;&#23398;&#20250;&#23558;&#26426;&#22120;&#20154;&#35266;&#27979;&#26144;&#23556;&#21040;&#34892;&#20026;&#65292;&#21448;&#33021;&#20139;&#21463;&#26469;&#33258;&#32593;&#32476;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#30340;&#30410;&#22788;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#26426;&#22120;&#20154;&#36712;&#36857;&#25968;&#25454;&#21644;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#65288;&#22914;&#35270;&#35273;&#38382;&#31572;&#65289;&#19978;&#20849;&#21516;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#36890;&#29992;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65306;&#20026;&#20102;&#20351;&#33258;&#28982;&#35821;&#35328;&#22238;&#31572;&#21644;&#26426;&#22120;&#20154;&#34892;&#20026;&#37117;&#33021;&#20197;&#30456;&#21516;&#30340;&#26684;&#24335;&#36827;&#34892;&#22788;&#29702;&#65292;&#25105;&#20204;&#23558;&#34892;&#20026;&#34920;&#31034;&#20026;&#25991;&#26412;&#26631;&#35760;&#65292;&#24182;&#23558;&#23427;&#20204;&#30452;&#25509;&#32435;&#20837;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#20013;&#65292;&#19982;&#33258;&#28982;&#35821;&#35328;&#26631;&#35760;&#30456;&#21516;&#12290;&#25105;&#20204;&#23558;&#36825;&#31867;&#27169;&#22411;&#31216;&#20026;&#35270;&#35273;-&#35821;&#35328;-&#34892;&#21160;&#27169;&#22411;&#65288;VLA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and inst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#37319;&#29992;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15780</link><description>&lt;p&gt;
LLM-Rec: &#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
LLM-Rec: Personalized Recommendation via Prompting Large Language Models. (arXiv:2307.15780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#37319;&#29992;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;LLM-Rec&#65292;&#21253;&#25324;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65306;&#65288;1&#65289;&#22522;&#30784;&#24341;&#23548;&#65292;&#65288;2&#65289;&#25512;&#33616;&#39537;&#21160;&#24341;&#23548;&#65292;&#65288;3&#65289;&#21442;&#19982;&#24341;&#23548;&#24341;&#23548;&#65292;&#21644;&#65288;4&#65289;&#25512;&#33616;&#39537;&#21160;+&#21442;&#19982;&#24341;&#23548;&#24341;&#23548;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#21407;&#22987;&#20869;&#23481;&#25551;&#36848;&#19982;LLM&#29983;&#25104;&#30340;&#22686;&#24378;&#36755;&#20837;&#25991;&#26412;&#32467;&#21512;&#36215;&#26469;&#65292;&#37319;&#29992;&#36825;&#20123;&#24341;&#23548;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate various prompting strategies for enhancing personalized content recommendation performance with large language models (LLMs) through input augmentation. Our proposed approach, termed LLM-Rec, encompasses four distinct prompting strategies: (1) basic prompting, (2) recommendation-driven prompting, (3) engagement-guided prompting, and (4) recommendation-driven + engagement-guided prompting. Our empirical experiments show that combining the original content description with the augmented input text generated by LLM using these prompting strategies leads to improved recommendation performance. This finding highlights the importance of incorporating diverse prompts and input augmentation techniques to enhance the recommendation capabilities with large language models for personalized content recommendation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#21644;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#25991;&#26412;&#22686;&#24378;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#26694;&#26550;&#36873;&#25321;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#23545;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#25110;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2307.15776</link><description>&lt;p&gt;
&#36873;&#25321;&#21644;&#22686;&#24378;&#65306;&#22686;&#24378;&#31264;&#23494;&#26816;&#32034;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Select and Augment: Enhanced Dense Retrieval Knowledge Graph Augmentation. (arXiv:2307.15776v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#21644;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#25991;&#26412;&#22686;&#24378;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#26694;&#26550;&#36873;&#25321;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#23545;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#25110;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#20013;&#65292;&#23558;&#25991;&#26412;&#20449;&#24687;&#27880;&#20837;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23454;&#20307;&#34920;&#31034;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20540;&#24471;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#20197;&#25552;&#39640;KG&#30456;&#20851;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#24120;&#29992;&#30340;&#22806;&#37096;&#30693;&#35782;&#22686;&#24378;KG&#23884;&#20837;&#30340;&#26041;&#27861;&#21253;&#25324;&#35821;&#20041;&#20016;&#23500;&#30340;&#20381;&#36182;&#35299;&#26512;&#29305;&#24449;&#12289;&#19968;&#32452;&#30456;&#20851;&#20851;&#38190;&#35789;&#65292;&#20197;&#21450;&#26469;&#33258;&#22806;&#37096;&#35821;&#26009;&#24211;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#65289;&#30340;&#23436;&#25972;&#25991;&#26412;&#25551;&#36848;&#12290;&#23613;&#31649;&#36825;&#31181;&#21019;&#26032;&#65288;&#25991;&#26412;&#22686;&#24378;&#30340;KG&#23884;&#20837;&#65289;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#26412;&#25991;&#25552;&#20986;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#25105;&#20204;&#19981;&#20877;&#20351;&#29992;&#21333;&#19968;&#25991;&#26412;&#25551;&#36848;&#65288;&#22240;&#20026;&#25991;&#26412;&#30340;&#22266;&#26377;&#35821;&#20041;&#27495;&#20041;&#26080;&#27861;&#20805;&#20998;&#34920;&#31034;&#19968;&#20010;&#23454;&#20307;&#65289;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#26082;&#33021;&#36873;&#25321;&#19982;KG&#23454;&#20307;&#30456;&#20851;&#30340;&#19968;&#32452;&#25991;&#26412;&#25551;&#36848;&#65292;&#21448;&#33021;&#23558;KG&#23884;&#20837;&#19982;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#23545;&#40784;&#25110;&#22686;&#24378;&#12290;&#19982;&#20043;&#21069;&#23558;&#24418;&#24335;&#21270;&#23454;&#20307;&#25551;&#36848;&#25554;&#20837;&#30693;&#35782;&#24211;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#19968;&#26041;&#27861;&#26159;&#25552;&#20379;&#20102;&#23545;KG&#23884;&#20837;&#36827;&#34892;&#22686;&#24378;&#21644;&#23545;&#40784;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Injecting textual information into knowledge graph (KG) entity representations has been a worthwhile expedition in terms of improving performance in KG oriented tasks within the NLP community. External knowledge often adopted to enhance KG embeddings ranges from semantically rich lexical dependency parsed features to a set of relevant key words to entire text descriptions supplied from an external corpus such as wikipedia and many more. Despite the gains this innovation (Text-enhanced KG embeddings) has made, the proposal in this work suggests that it can be improved even further. Instead of using a single text description (which would not sufficiently represent an entity because of the inherent lexical ambiguity of text), we propose a multi-task framework that jointly selects a set of text descriptions relevant to KG entities as well as align or augment KG embeddings with text descriptions. Different from prior work that plugs formal entity descriptions declared in knowledge bases, th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#25506;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#21457;&#29616;&#20102;Hydra&#25928;&#24212;&#21644;&#26202;&#26399;MLP&#23618;&#30340;&#24179;&#34913;&#21151;&#33021;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.15771</link><description>&lt;p&gt;
Hydra&#25928;&#24212;&#65306;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#20013;&#30340;&#33258;&#36866;&#24212;&#33258;&#20462;&#22797;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
The Hydra Effect: Emergent Self-repair in Language Model Computations. (arXiv:2307.15771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#25506;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#21457;&#29616;&#20102;Hydra&#25928;&#24212;&#21644;&#26202;&#26399;MLP&#23618;&#30340;&#24179;&#34913;&#21151;&#33021;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22240;&#26524;&#20998;&#26512;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#24182;&#23637;&#31034;&#20102;&#20004;&#31181;&#27169;&#24335;&#65306;&#65288;1&#65289;&#19968;&#31181;&#33258;&#36866;&#24212;&#35745;&#31639;&#24418;&#24335;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26576;&#19968;&#33258;&#27880;&#24847;&#23618;&#34987;&#21024;&#20943;&#21518;&#21478;&#19968;&#23618;&#36827;&#34892;&#34917;&#20607;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;Hydra&#25928;&#24212;&#65289;&#65307;&#65288;2&#65289;&#22312;&#21518;&#26399;&#22810;&#23618;&#24863;&#30693;&#26426;&#23618;&#20013;&#23384;&#22312;&#30340;&#24179;&#34913;&#21151;&#33021;&#65292;&#29992;&#20110;&#35843;&#33410;&#26368;&#22823;&#20284;&#28982;&#20196;&#29260;&#12290;&#25105;&#20204;&#30340;&#21024;&#20943;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#23618;&#20043;&#38388;&#36890;&#24120;&#30456;&#23545;&#26494;&#25955;&#32806;&#21512;&#65288;&#23545;&#19968;&#23618;&#30340;&#21024;&#20943;&#21482;&#20250;&#24433;&#21709;&#19968;&#23567;&#37096;&#20998;&#19979;&#28216;&#23618;&#65289;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20219;&#20309;&#24418;&#24335;&#30340;&#38543;&#26426;&#22833;&#27963;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36825;&#20123;&#25928;&#24212;&#20173;&#28982;&#23384;&#22312;&#12290;&#25105;&#20204;&#22312;&#20107;&#23454;&#22238;&#24518;&#30340;&#32972;&#26223;&#19979;&#20998;&#26512;&#20102;&#36825;&#20123;&#25928;&#24212;&#65292;&#24182;&#32771;&#34385;&#20102;&#23427;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#30005;&#36335;&#23618;&#38754;&#24402;&#22240;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the internal structure of language model computations using causal analysis and demonstrate two motifs: (1) a form of adaptive computation where ablations of one attention layer of a language model cause another layer to compensate (which we term the Hydra effect) and (2) a counterbalancing function of late MLP layers that act to downregulate the maximum-likelihood token. Our ablation studies demonstrate that language model layers are typically relatively loosely coupled (ablations to one layer only affect a small number of downstream layers). Surprisingly, these effects occur even in language models trained without any form of dropout. We analyse these effects in the context of factual recall and consider their implications for circuit-level attribution in language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ChatReport&#30340;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#23454;&#29616;&#21487;&#36861;&#28335;&#30340;&#31572;&#26696;&#21644;&#35299;&#20915;&#39046;&#22495;&#19987;&#23478;&#21442;&#19982;&#20302;&#25928;&#24615;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#25259;&#38706;&#20998;&#26512;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.15770</link><description>&lt;p&gt;
CHATREPORT&#65306;&#36890;&#36807;&#22522;&#20110;LLM&#24037;&#20855;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#25259;&#38706;&#20998;&#26512;&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools. (arXiv:2307.15770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ChatReport&#30340;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#23454;&#29616;&#21487;&#36861;&#28335;&#30340;&#31572;&#26696;&#21644;&#35299;&#20915;&#39046;&#22495;&#19987;&#23478;&#21442;&#19982;&#20302;&#25928;&#24615;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#25259;&#38706;&#20998;&#26512;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#27668;&#20505;&#21464;&#21270;&#65292;&#20844;&#21496;&#30495;&#30340;&#22312;&#26397;&#30528;&#26356;&#21487;&#25345;&#32493;&#32463;&#33829;&#36808;&#20986;&#23454;&#36136;&#24615;&#30340;&#27493;&#20240;&#21527;&#65311;&#19968;&#20010;&#20840;&#38754;&#30340;&#31572;&#26696;&#21487;&#20197;&#22312;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#30340;&#23494;&#38598;&#20449;&#24687;&#20013;&#25214;&#21040;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25253;&#21578;&#30340;&#25968;&#37327;&#21644;&#22797;&#26434;&#24615;&#20351;&#20154;&#24037;&#20998;&#26512;&#25104;&#26412;&#38750;&#24120;&#39640;&#26114;&#12290;&#22240;&#27492;&#65292;&#21482;&#26377;&#23569;&#25968;&#30340;&#26426;&#26500;&#25317;&#26377;&#36164;&#28304;&#33021;&#22815;&#22823;&#35268;&#27169;&#20998;&#26512;&#36825;&#20123;&#25253;&#21578;&#65292;&#36825;&#23548;&#33268;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#32570;&#20047;&#36879;&#26126;&#24230;&#12290;&#36890;&#36807;&#22522;&#20110;LLM&#33258;&#21160;&#20998;&#26512;&#24037;&#20855;&#36171;&#33021;&#21033;&#30410;&#30456;&#20851;&#32773;&#21487;&#33021;&#26159;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#20998;&#26512;&#27665;&#20027;&#21270;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#36825;&#26679;&#30340;&#24037;&#20855;&#38754;&#20020;&#25361;&#25112;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;LLM&#30340;&#24187;&#35273;&#38382;&#39064;&#21644;&#23558;&#39046;&#22495;&#19987;&#23478;&#24341;&#20837;AI&#24320;&#21457;&#36807;&#31243;&#30340;&#20302;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatReport&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26032;&#22411;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#36890;&#36807;&#20351;&#31572;&#26696;&#21487;&#36861;&#28335;&#26469;&#20943;&#23569;&#24187;&#35273;&#30340;&#21361;&#23475;&#65292;&#24182;&#35299;&#20915;&#39046;&#22495;&#19987;&#23478;&#21442;&#19982;AI&#24320;&#21457;&#36807;&#31243;&#30340;&#20302;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the face of climate change, are companies really taking substantial steps toward more sustainable operations? A comprehensive answer lies in the dense, information-rich landscape of corporate sustainability reports. However, the sheer volume and complexity of these reports make human analysis very costly. Therefore, only a few entities worldwide have the resources to analyze these reports at scale, which leads to a lack of transparency in sustainability reporting. Empowering stakeholders with LLM-based automatic analysis tools can be a promising way to democratize sustainability report analysis. However, developing such tools is challenging due to (1) the hallucination of LLMs and (2) the inefficiency of bringing domain experts into the AI development loop. In this paper, we ChatReport, a novel LLM-based system to automate the analysis of corporate sustainability reports, addressing existing challenges by (1) making the answers traceable to reduce the harm of hallucination and (2) a
&lt;/p&gt;</description></item><item><title>&#36825;&#20004;&#31687;&#35770;&#25991;&#23545;&#20110;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#30340;NLP&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21487;&#20877;&#29616;&#24615;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#35814;&#23613;&#30340;&#24037;&#20316;&#27969;&#31243;&#12289;&#25972;&#27905;&#30340;&#20195;&#30721;&#24211;&#21644;&#28165;&#26224;&#30340;&#27169;&#22411;&#35780;&#20272;&#25351;&#23548;&#65292;&#20026;&#26410;&#26469;&#30340;&#26448;&#26009;&#31185;&#23398;&#20986;&#29256;&#29289;&#26641;&#31435;&#20102;&#33391;&#22909;&#30340;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2307.15759</link><description>&lt;p&gt;
&#21487;&#20877;&#29616;&#24615;&#25506;&#31350;&#65306;&#20174;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;NLP&#30740;&#31350;&#20013;&#33719;&#24471;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Lessons in Reproducibility: Insights from NLP Studies in Materials Science. (arXiv:2307.15759v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15759
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20004;&#31687;&#35770;&#25991;&#23545;&#20110;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#30340;NLP&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21487;&#20877;&#29616;&#24615;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#35814;&#23613;&#30340;&#24037;&#20316;&#27969;&#31243;&#12289;&#25972;&#27905;&#30340;&#20195;&#30721;&#24211;&#21644;&#28165;&#26224;&#30340;&#27169;&#22411;&#35780;&#20272;&#25351;&#23548;&#65292;&#20026;&#26410;&#26469;&#30340;&#26448;&#26009;&#31185;&#23398;&#20986;&#29256;&#29289;&#26641;&#31435;&#20102;&#33391;&#22909;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22522;&#30707;&#39046;&#22495;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#24212;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#23545;&#35813;&#39046;&#22495;&#20013;&#30340;&#20004;&#39033;&#24320;&#21019;&#24615;&#30740;&#31350;&#65306;&#8220;&#26426;&#22120;&#23398;&#20064;&#21644;&#32534;&#30721;&#30340;&#27687;&#21270;&#29289;&#26448;&#26009;&#21512;&#25104;&#21442;&#25968;&#8221;&#65288;Kim&#31561;&#20154;&#65289;&#21644;&#8220;&#26080;&#30417;&#30563;&#30340;&#35789;&#23884;&#20837;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#28508;&#22312;&#30693;&#35782;&#8221;&#65288;Tshitoyan&#31561;&#20154;&#65289;&#36827;&#34892;&#20102;&#21487;&#20877;&#29616;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#20174;&#21487;&#20877;&#29616;&#24615;&#30340;&#35282;&#24230;&#29702;&#35299;&#36825;&#20123;&#30740;&#31350;&#65292;&#35748;&#35782;&#21040;&#23427;&#20204;&#23545;&#26448;&#26009;&#20449;&#24687;&#23398;&#39046;&#22495;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#32780;&#19981;&#26159;&#25209;&#35780;&#23427;&#20204;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20004;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#35814;&#23613;&#30340;&#24037;&#20316;&#27969;&#31243;&#12289;&#25972;&#27905;&#19988;&#26377;&#33391;&#22909;&#25991;&#26723;&#30340;&#20195;&#30721;&#24211;&#65292;&#20197;&#21450;&#28165;&#26224;&#30340;&#27169;&#22411;&#35780;&#20272;&#25351;&#23548;&#12290;&#36825;&#20351;&#24471;&#25104;&#21151;&#22797;&#21046;&#20182;&#20204;&#30340;&#32467;&#26524;&#24182;&#37096;&#20998;&#37325;&#29616;&#20182;&#20204;&#30340;&#21457;&#29616;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#23427;&#20204;&#20026;&#26410;&#26469;&#30340;&#26448;&#26009;&#31185;&#23398;&#20986;&#29256;&#29289;&#26641;&#31435;&#20102;&#20540;&#24471;&#31216;&#36190;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP), a cornerstone field within artificial intelligence, has been increasingly utilized in the field of materials science literature. Our study conducts a reproducibility analysis of two pioneering works within this domain: "Machine-learned and codified synthesis parameters of oxide materials" by Kim et al., and "Unsupervised word embeddings capture latent knowledge from materials science literature" by Tshitoyan et al. We aim to comprehend these studies from a reproducibility perspective, acknowledging their significant influence on the field of materials informatics, rather than critiquing them. Our study indicates that both papers offered thorough workflows, tidy and well-documented codebases, and clear guidance for model evaluation. This makes it easier to replicate their results successfully and partially reproduce their findings. In doing so, they set commendable standards for future materials science publications to aspire to. However, our analysis 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LDA&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#31616;&#21382;&#20013;&#30340;&#23454;&#20307;&#24182;&#23558;&#20854;&#29992;&#20110;&#35780;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#20505;&#36873;&#20154;&#36873;&#25321;&#12290;&#35813;&#26041;&#27861;&#22312;&#32771;&#34385;&#25152;&#26377;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;82%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.15752</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23545;&#31616;&#21382;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20505;&#36873;&#20154;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Resume Evaluation through Latent Dirichlet Allocation and Natural Language Processing for Effective Candidate Selection. (arXiv:2307.15752v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LDA&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#31616;&#21382;&#20013;&#30340;&#23454;&#20307;&#24182;&#23558;&#20854;&#29992;&#20110;&#35780;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#20505;&#36873;&#20154;&#36873;&#25321;&#12290;&#35813;&#26041;&#27861;&#22312;&#32771;&#34385;&#25152;&#26377;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;82%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#21644;&#20351;&#29992;SpaCy&#36827;&#34892;&#23454;&#20307;&#26816;&#27979;&#30340;&#31616;&#21382;&#35780;&#20998;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;SpaCy&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20174;&#31616;&#21382;&#20013;&#25552;&#21462;&#30456;&#20851;&#23454;&#20307;&#65292;&#22914;&#25945;&#32946;&#32972;&#26223;&#12289;&#24037;&#20316;&#32463;&#21382;&#21644;&#25216;&#33021;&#12290;&#28982;&#21518;&#65292;LDA&#27169;&#22411;&#20351;&#29992;&#36825;&#20123;&#23454;&#20307;&#20026;&#31616;&#21382;&#35780;&#20998;&#65292;&#20026;&#27599;&#20010;&#23454;&#20307;&#20998;&#37197;&#20027;&#39064;&#27010;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;SpaCy&#30340;NER&#36827;&#34892;&#23454;&#20307;&#26816;&#27979;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25253;&#21578;&#20854;&#35780;&#20272;&#25351;&#26631;&#12290;&#20351;&#29992;LDA&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#23558;&#31616;&#21382;&#20998;&#35299;&#20026;&#28508;&#22312;&#20027;&#39064;&#65292;&#24182;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;77%&#30340;&#20934;&#30830;&#29575;&#65292;&#21482;&#32771;&#34385;&#25216;&#33021;&#65292;&#22312;&#32771;&#34385;&#25152;&#26377;&#23646;&#24615;&#65288;&#22914;&#23398;&#38498;&#21517;&#31216;&#12289;&#24037;&#20316;&#32463;&#21382;&#12289;&#23398;&#20301;&#21644;&#25216;&#33021;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#24635;&#20307;&#20934;&#30830;&#29575;&#36798;&#21040;82%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a method for resume rating using Latent Dirichlet Allocation (LDA) and entity detection with SpaCy. The proposed method first extracts relevant entities such as education, experience, and skills from the resume using SpaCy's Named Entity Recognition (NER). The LDA model then uses these entities to rate the resume by assigning topic probabilities to each entity. Furthermore, we conduct a detailed analysis of the entity detection using SpaCy's NER and report its evaluation metrics. Using LDA, our proposed system breaks down resumes into latent topics and extracts meaningful semantic representations. With a vision to define our resume score to be more content-driven rather than a structure and keyword match driven, our model has achieved 77% accuracy with respect to only skills in consideration and an overall 82% accuracy with all attributes in consideration. (like college name, work experience, degree and skills)
&lt;/p&gt;</description></item><item><title>Context-VQA&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#38754;&#28385;&#36275;&#20154;&#20204;&#38656;&#27714;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30340;&#21019;&#26032;&#22312;&#20110;&#23558;&#22270;&#20687;&#19982;&#19981;&#21516;&#19978;&#19979;&#25991;&#37197;&#23545;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#19978;&#19979;&#25991;&#19979;&#38382;&#39064;&#31867;&#22411;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.15745</link><description>&lt;p&gt;
Context-VQA: &#38754;&#21521;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#26377;&#24847;&#20041;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering. (arXiv:2307.15745v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15745
&lt;/p&gt;
&lt;p&gt;
Context-VQA&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#38754;&#28385;&#36275;&#20154;&#20204;&#38656;&#27714;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30340;&#21019;&#26032;&#22312;&#20110;&#23558;&#22270;&#20687;&#19982;&#19981;&#21516;&#19978;&#19979;&#25991;&#37197;&#23545;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#19978;&#19979;&#25991;&#19979;&#38382;&#39064;&#31867;&#22411;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26377;&#28508;&#21147;&#20197;&#19968;&#31181;&#20114;&#21160;&#26041;&#24335;&#20351;&#20114;&#32852;&#32593;&#26356;&#20855;&#21487;&#35775;&#38382;&#24615;&#65292;&#20351;&#19981;&#33021;&#30475;&#21040;&#22270;&#20687;&#30340;&#20154;&#20204;&#33021;&#22815;&#23601;&#22270;&#20687;&#25552;&#20986;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#30450;&#20154;&#25110;&#35270;&#21147;&#20302;&#19979;&#30340;&#20154;&#26356;&#21916;&#27426;&#21253;&#21547;&#22270;&#20687;&#20986;&#29616;&#29615;&#22659;&#30340;&#22270;&#20687;&#35299;&#37322;&#65292;&#32780;&#24403;&#21069;&#30340;VQA&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#23396;&#31435;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#38500;&#38750;&#32771;&#34385;&#19978;&#19979;&#25991;&#65292;&#21542;&#21017;VQA&#27169;&#22411;&#23558;&#26080;&#27861;&#23436;&#20840;&#28385;&#36275;&#20154;&#20204;&#30340;&#38656;&#27714;&#12290;&#20026;&#36827;&#19968;&#27493;&#28608;&#21457;&#21644;&#20998;&#26512;&#19981;&#21516;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Context-VQA&#65292;&#19968;&#31181;&#23558;&#22270;&#20687;&#19982;&#19978;&#19979;&#25991;&#65288;&#22914;&#36141;&#29289;&#32593;&#31449;&#65289;&#37197;&#23545;&#30340;VQA&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#19978;&#19979;&#25991;&#19979;&#30340;&#38382;&#39064;&#31867;&#22411;&#23384;&#22312;&#31995;&#32479;&#24615;&#24046;&#24322;&#12290;&#20363;&#22914;&#65292;&#22312;&#26053;&#34892;&#19978;&#19979;&#25991;&#20013;&#21576;&#29616;&#30340;&#22270;&#20687;&#20135;&#29983;2&#20493;&#20110;&#24179;&#22343;&#25968;&#30340;&#8220;&#22312;&#21738;&#37324;&#65311;&#8221;&#38382;&#39064;&#65292;&#32780;&#31038;&#20132;&#23186;&#20307;&#21644;&#26032;&#38395;&#19978;&#30340;&#22270;&#20687;&#20135;&#29983;&#30340;&#8220;&#35841;&#65311;&#8221;&#38382;&#39064;&#20998;&#21035;&#20026;&#24179;&#22343;&#25968;&#30340;2.8&#20493;&#21644;1.8&#20493;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#19978;&#19979;&#25991;&#23545;&#20110;&#22238;&#31572;&#27491;&#30830;&#30340;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;&#19978;&#19979;&#25991;&#25552;&#20379;&#20102;&#25552;&#31034;&#26102;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;17.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) has the potential to make the Internet more accessible in an interactive way, allowing people who cannot see images to ask questions about them. However, multiple studies have shown that people who are blind or have low-vision prefer image explanations that incorporate the context in which an image appears, yet current VQA datasets focus on images in isolation. We argue that VQA models will not fully succeed at meeting people's needs unless they take context into account. To further motivate and analyze the distinction between different contexts, we introduce Context-VQA, a VQA dataset that pairs images with contexts, specifically types of websites (e.g., a shopping website). We find that the types of questions vary systematically across contexts. For example, images presented in a travel context garner 2 times more "Where?" questions, and images on social media and news garner 2.8 and 1.8 times more "Who?" questions than the average. We also find that c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#65292;&#26088;&#22312;&#19982;&#33647;&#29702;&#23398;&#25968;&#25454;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#36827;&#34892;&#20132;&#20114;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.15717</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33647;&#29702;&#23398;&#25968;&#25454;&#24211;&#30340;&#33258;&#28982;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
Utilizing Large Language Models for Natural Interface to Pharmacology Databases. (arXiv:2307.15717v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15717
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#65292;&#26088;&#22312;&#19982;&#33647;&#29702;&#23398;&#25968;&#25454;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#36827;&#34892;&#20132;&#20114;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#24320;&#21457;&#36807;&#31243;&#38656;&#35201;&#33647;&#29702;&#23398;&#23478;&#36827;&#34892;&#21508;&#31181;&#20219;&#21153;&#65292;&#22914;&#26597;&#38405;&#25991;&#29486;&#65292;&#25552;&#20986;&#20551;&#35774;&#65292;&#35774;&#35745;&#23454;&#39564;&#21644;&#35299;&#37322;&#32467;&#26524;&#12290;&#27599;&#20010;&#38454;&#27573;&#37117;&#38656;&#35201;&#35775;&#38382;&#21644;&#26597;&#35810;&#22823;&#37327;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#65292;&#26088;&#22312;&#19982;&#23384;&#20648;&#22312;&#25968;&#25454;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#25512;&#24191;&#21040;&#26597;&#35810;&#21508;&#31181;&#33647;&#29289;&#25968;&#25454;&#21644;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
The drug development process necessitates that pharmacologists undertake various tasks, such as reviewing literature, formulating hypotheses, designing experiments, and interpreting results. Each stage requires accessing and querying vast amounts of information. In this abstract, we introduce a Large Language Model (LLM)-based Natural Language Interface designed to interact with structured information stored in databases. Our experiments demonstrate the feasibility and effectiveness of the proposed framework. This framework can generalize to query a wide range of pharmaceutical data and knowledge bases.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23545;&#31185;&#23398;&#35770;&#25991;&#30340;&#25688;&#35201;&#36827;&#34892;&#26497;&#31471;&#25688;&#35201;&#21270;&#65292;&#26088;&#22312;&#24110;&#21161;&#21021;&#32423;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#20943;&#36731;&#35748;&#30693;&#36127;&#33655;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#24037;&#20316;&#25928;&#29575;&#21644;&#20943;&#23569;&#24515;&#29702;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2307.15715</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26497;&#31471;&#25688;&#35201;&#21270;&#31185;&#23398;&#25991;&#29486;&#26469;&#25913;&#36827;&#21021;&#32423;&#21307;&#30103;&#20445;&#20581;&#24037;&#20316;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
Improving Primary Healthcare Workflow Using Extreme Summarization of Scientific Literature Based on Generative AI. (arXiv:2307.15715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23545;&#31185;&#23398;&#35770;&#25991;&#30340;&#25688;&#35201;&#36827;&#34892;&#26497;&#31471;&#25688;&#35201;&#21270;&#65292;&#26088;&#22312;&#24110;&#21161;&#21021;&#32423;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#20943;&#36731;&#35748;&#30693;&#36127;&#33655;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#24037;&#20316;&#25928;&#29575;&#21644;&#20943;&#23569;&#24515;&#29702;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21021;&#32423;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#22312;&#25351;&#23548;&#20197;&#35777;&#25454;&#20026;&#22522;&#30784;&#30340;&#23454;&#36341;&#26041;&#38754;&#65292;&#22914;&#20309;&#36319;&#19978;&#26368;&#26032;&#30340;&#31185;&#23398;&#25991;&#29486;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#23545;&#31185;&#23398;&#35770;&#25991;&#30340;&#25688;&#35201;&#36827;&#34892;&#24635;&#32467;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20943;&#23569;&#20174;&#19994;&#32773;&#35748;&#30693;&#36127;&#33655;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#25506;&#32034;&#20854;&#20943;&#36731;&#24515;&#29702;&#21162;&#21147;&#21644;&#36127;&#25285;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21442;&#19982;&#32773;&#25509;&#21463;&#19982;&#39044;&#38450;&#20445;&#20581;&#21644;&#34892;&#20026;&#25913;&#21464;&#30456;&#20851;&#30340;&#20004;&#20010;&#26696;&#20363;&#65292;&#27169;&#25311;&#23545;&#26032;&#30340;&#31185;&#23398;&#25991;&#29486;&#36827;&#34892;&#25628;&#32034;&#12290;&#30740;&#31350;&#21253;&#25324;&#26469;&#33258;&#26031;&#27931;&#25991;&#23612;&#20122;&#21644;&#32654;&#22269;&#30340;113&#21517;&#22823;&#23398;&#29983;&#65292;&#34987;&#38543;&#26426;&#20998;&#25104;&#19977;&#20010;&#19981;&#21516;&#30340;&#30740;&#31350;&#32452;&#12290;&#31532;&#19968;&#32452;&#34987;&#20998;&#37197;&#38405;&#35835;&#23436;&#25972;&#30340;&#25688;&#35201;&#65292;&#31532;&#20108;&#32452;&#34987;&#20998;&#37197;&#38405;&#35835;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#30701;&#25688;&#35201;&#65292;&#31532;&#19977;&#32452;&#26377;&#36873;&#25321;&#24615;&#22320;&#38405;&#35835;&#12290;
&lt;/p&gt;
&lt;p&gt;
Primary care professionals struggle to keep up to date with the latest scientific literature critical in guiding evidence-based practice related to their daily work. To help solve the above-mentioned problem, we employed generative artificial intelligence techniques based on large-scale language models to summarize abstracts of scientific papers. Our objective is to investigate the potential of generative artificial intelligence in diminishing the cognitive load experienced by practitioners, thus exploring its ability to alleviate mental effort and burden. The study participants were provided with two use cases related to preventive care and behavior change, simulating a search for new scientific literature. The study included 113 university students from Slovenia and the United States randomized into three distinct study groups. The first group was assigned to the full abstracts. The second group was assigned to the short abstracts generated by AI. The third group had the option to se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35266;&#24565;&#30340;&#22240;&#26524;&#25506;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#30340;&#23376;&#31354;&#38388;&#19978;&#36827;&#34892;&#21453;&#20107;&#23454;&#24178;&#39044;&#65292;&#20248;&#21270;&#20102;&#22240;&#26524;&#27010;&#24565;&#23376;&#31354;&#38388;&#65292;&#20197;&#23454;&#29616;&#27010;&#24565;&#25511;&#21046;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.15054</link><description>&lt;p&gt;
&#19968;&#31181;&#20960;&#20309;&#35266;&#24565;&#30340;&#22240;&#26524;&#25506;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Geometric Notion of Causal Probing. (arXiv:2307.15054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35266;&#24565;&#30340;&#22240;&#26524;&#25506;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#30340;&#23376;&#31354;&#38388;&#19978;&#36827;&#34892;&#21453;&#20107;&#23454;&#24178;&#39044;&#65292;&#20248;&#21270;&#20102;&#22240;&#26524;&#27010;&#24565;&#23376;&#31354;&#38388;&#65292;&#20197;&#23454;&#29616;&#27010;&#24565;&#25511;&#21046;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20381;&#36182;&#20110;&#25991;&#26412;&#30340;&#23454;&#20540;&#34920;&#31034;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#20123;&#34920;&#31034;&#21253;&#21547;&#20102;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#23398;&#21040;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;&#35821;&#35328;&#23646;&#24615;&#21644;&#22522;&#20110;&#24615;&#21035;&#30340;&#20154;&#21475;&#20559;&#35265;&#31561;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#36890;&#36807;&#22312;&#34920;&#31034;&#31354;&#38388;&#30340;&#23376;&#31354;&#38388;&#19978;&#36827;&#34892;&#27491;&#20132;&#25237;&#24433;&#26469;&#33719;&#24471;&#20851;&#20110;&#36825;&#20123;&#27010;&#24565;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#20449;&#24687;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#20026;&#36825;&#39033;&#30740;&#31350;&#36129;&#29486;&#20102;&#26032;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#26041;&#27861;&#26469;&#36991;&#20813;&#34394;&#20551;&#30456;&#20851;&#30340;&#22833;&#25928;&#27169;&#24335;&#65292;&#36890;&#36807;&#29420;&#31435;&#22788;&#29702;&#23376;&#31354;&#38388;&#20013;&#30340;&#20998;&#37327;&#21644;&#20854;&#27491;&#20132;&#34917;&#31354;&#38388;&#20013;&#30340;&#20998;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23376;&#31354;&#38388;&#20013;&#30340;&#21453;&#20107;&#23454;&#20449;&#24687;&#27010;&#24565;&#26159;&#30001;&#19968;&#20010;&#22240;&#26524;&#27010;&#24565;&#23376;&#31354;&#38388;&#36827;&#34892;&#20248;&#21270;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#24178;&#39044;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#25805;&#20316;&#26469;&#23581;&#35797;&#27010;&#24565;&#25511;&#21046;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models rely on real-valued representations of text to make their predictions. These representations contain information learned from the data that the model has trained on, including knowledge of linguistic properties and forms of demographic bias, e.g., based on gender. A growing body of work has considered information about concepts such as these using orthogonal projections onto subspaces of the representation space. We contribute to this body of work by proposing a formal definition of intrinsic information in a subspace of a language model's representation space. We propose a counterfactual approach that avoids the failure mode of spurious correlations (Kumar et al., 2022) by treating components in the subspace and its orthogonal complement independently. We show that our counterfactual notion of information in a subspace is optimizing by an causal concept subspace. Furthermore, this intervention allows us to attempt concept controlled generation by manipulating the
&lt;/p&gt;</description></item><item><title>Gzip&#19982;KNN&#30456;&#27604;&#36739;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#31616;&#21333;&#30340;&#35789;&#34955;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.15002</link><description>&lt;p&gt;
Gzip&#19982;KNN&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#23545;&#27604;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Gzip versus bag-of-words for text classification with KNN. (arXiv:2307.15002v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15002
&lt;/p&gt;
&lt;p&gt;
Gzip&#19982;KNN&#30456;&#27604;&#36739;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#31616;&#21333;&#30340;&#35789;&#34955;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;KNN&#30340;&#25991;&#26412;&#20998;&#31867;&#20013;&#21387;&#32553;&#36317;&#31163;&#65288;gzip&#65289;&#30340;&#26377;&#25928;&#24615;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#21487;&#33021;&#19981;&#38656;&#35201;&#25991;&#26412;&#21387;&#32553;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#8220;&#35789;&#34955;&#8221;&#21305;&#37197;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of compression distance in KNN-based text classification ('gzip') has recently garnered lots of attention. In this note, we show that similar or better effectiveness can be achieved with simpler means, and text compression may not be necessary. Indeed, we find that a simple 'bag-of-words' matching can achieve similar or better accuracy, and is more efficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#25216;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;&#20154;&#31867;&#33719;&#24471;&#25216;&#33021;&#30340;&#26377;&#24207;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#25216;&#33021;&#26102;&#20063;&#26377;&#19968;&#23450;&#30340;&#39034;&#24207;&#65292;&#24182;&#19988;&#36825;&#31181;&#39034;&#24207;&#21487;&#20197;&#25913;&#21892;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25968;&#25454;&#39640;&#25928;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.14430</link><description>&lt;p&gt;
Skill-it! &#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#25216;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models. (arXiv:2307.14430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#25216;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;&#20154;&#31867;&#33719;&#24471;&#25216;&#33021;&#30340;&#26377;&#24207;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#25216;&#33021;&#26102;&#20063;&#26377;&#19968;&#23450;&#30340;&#39034;&#24207;&#65292;&#24182;&#19988;&#36825;&#31181;&#39034;&#24207;&#21487;&#20197;&#25913;&#21892;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25968;&#25454;&#39640;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#22312;&#22266;&#23450;&#30340;token&#39044;&#31639;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#36873;&#25321;&#33021;&#22815;&#22312;&#21508;&#20010;&#20219;&#21153;&#20013;&#33719;&#24471;&#33391;&#22909;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#20551;&#35774;&#65306;&#20154;&#31867;&#22312;&#26377;&#24847;&#20041;&#30340;&#39034;&#24207;&#20013;&#33719;&#24471;&#30456;&#20114;&#20381;&#36182;&#30340;&#25216;&#33021;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#19968;&#32452;&#25216;&#33021;&#26102;&#20063;&#20250;&#36981;&#24490;&#36825;&#26679;&#30340;&#39034;&#24207;&#12290;&#22914;&#26524;&#23384;&#22312;&#36825;&#26679;&#30340;&#39034;&#24207;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25968;&#25454;&#39640;&#25928;&#35757;&#32451;&#12290;&#21033;&#29992;&#36825;&#31181;&#30452;&#35273;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#25216;&#33021;&#30340;&#27010;&#24565;&#21644;&#26377;&#24207;&#30340;&#25216;&#33021;&#38598;&#21512;&#30340;&#27010;&#24565;&#24418;&#24335;&#21270;&#20026;&#30456;&#20851;&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26377;&#24207;&#30340;&#25216;&#33021;&#38598;&#21512;&#30340;&#23384;&#22312;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#23384;&#22312;&#20351;&#24471;&#22312;&#35757;&#32451;&#20854;&#20808;&#20915;&#26465;&#20214;&#25216;&#33021;&#26102;&#21487;&#20197;&#20351;&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23398;&#20064;&#26356;&#39640;&#32423;&#30340;&#25216;&#33021;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32447;&#25968;&#25454;&#37319;&#26679;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;XDLM&#65292;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#36328;&#35821;&#35328;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#32763;&#35793;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#21644;Transformer&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.13560</link><description>&lt;p&gt;
XDLM: &#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#36328;&#35821;&#35328;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
XDLM: Cross-lingual Diffusion Language Model for Machine Translation. (arXiv:2307.13560v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;XDLM&#65292;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#36328;&#35821;&#35328;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#32763;&#35793;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#21644;Transformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#24050;&#32463;&#24212;&#29992;&#20110;&#31070;&#32463;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#30456;&#23545;&#36739;&#23569;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#24050;&#32463;&#30740;&#31350;&#20102;&#22312;&#21333;&#19968;&#35821;&#35328;&#20013;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20294;&#36328;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#28508;&#21147;&#20173;&#26410;&#34987;&#28145;&#20837;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;XDLM&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#36328;&#35821;&#35328;&#25193;&#25955;&#27169;&#22411;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TLDM&#65292;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#29992;&#20110;&#25484;&#25569;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#65307;&#22312;&#24494;&#35843;&#38454;&#27573;&#65292;&#25105;&#20204;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#26500;&#24314;&#20102;&#32763;&#35793;&#31995;&#32479;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#26426;&#22120;&#32763;&#35793;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#36229;&#36807;&#20102;&#25193;&#25955;&#21644;Transformer&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, diffusion models have excelled in image generation tasks and have also been applied to neural language processing (NLP) for controllable text generation. However, the application of diffusion models in a cross-lingual setting is less unexplored. Additionally, while pretraining with diffusion models has been studied within a single language, the potential of cross-lingual pretraining remains understudied. To address these gaps, we propose XDLM, a novel Cross-lingual diffusion model for machine translation, consisting of pretraining and fine-tuning stages. In the pretraining stage, we propose TLDM, a new training objective for mastering the mapping between different languages; in the fine-tuning stage, we build up the translation system based on the pretrained model. We evaluate the result on several machine translation benchmarks and outperformed both diffusion and Transformer baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24212;&#29992;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#37327;&#23376;&#22686;&#24378;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(QLSTM)&#21487;&#20197;&#26356;&#24555;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#36719;&#20214;&#23454;&#29616;&#26041;&#38754;&#25509;&#36817;&#21476;&#20856;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11788</link><description>&lt;p&gt;
&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24212;&#29992;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Applying QNLP to sentiment analysis in finance. (arXiv:2307.11788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24212;&#29992;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#37327;&#23376;&#22686;&#24378;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(QLSTM)&#21487;&#20197;&#26356;&#24555;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#36719;&#20214;&#23454;&#29616;&#26041;&#38754;&#25509;&#36817;&#21476;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#20010;&#39046;&#22495;&#65292;&#21363;&#20351;&#26159;&#26368;&#24494;&#23567;&#30340;&#36136;&#37327;&#25913;&#36827;&#20063;&#33021;&#20135;&#29983;&#24040;&#22823;&#20215;&#20540;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#37329;&#34701;&#26159;&#26089;&#26399;&#37327;&#23376;&#20248;&#21183;&#30340;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#32773;&#12290;&#22312;&#36805;&#36895;&#21457;&#23637;&#30340;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;DisCoCat&#21644;&#37327;&#23376;&#22686;&#24378;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(QNLP)&#36825;&#20004;&#31181;&#20013;&#24515;&#26041;&#27861;&#22312;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ChatGPT&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#21253;&#21547;1000&#22810;&#20010;&#30495;&#23454;&#21477;&#23376;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;QLSTM&#30340;&#35757;&#32451;&#36895;&#24230;&#27604;DisCoCat&#24555;&#24471;&#22810;&#65292;&#24182;&#19988;&#22312;&#21487;&#29992;&#30340;&#36719;&#20214;&#23454;&#29616;&#20013;&#20063;&#25509;&#36817;&#21476;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an application domain where the slightest qualitative improvements can yield immense value, finance is a promising candidate for early quantum advantage. Focusing on the rapidly advancing field of Quantum Natural Language Processing (QNLP), we explore the practical applicability of the two central approaches DisCoCat and Quantum-Enhanced Long Short-Term Memory (QLSTM) to the problem of sentiment analysis in finance. Utilizing a novel ChatGPT-based data generation approach, we conduct a case study with more than 1000 realistic sentences and find that QLSTMs can be trained substantially faster than DisCoCat while also achieving close to classical results for their available software implementations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;L-Eval&#65292;&#26088;&#22312;&#20026;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#26631;&#20934;&#21270;&#35780;&#20272;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#21253;&#21547;411&#20010;&#38271;&#25991;&#26723;&#21644;2000&#22810;&#20010;&#20154;&#24037;&#26631;&#27880;&#30340;&#26597;&#35810;-&#22238;&#22797;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#25193;&#23637;&#19978;&#19979;&#25991;&#23545;&#20110;&#22788;&#29702;&#38271;&#36755;&#20837;&#30340;&#23454;&#36136;&#24615;&#25910;&#30410;&#21644;&#25913;&#36827;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.11088</link><description>&lt;p&gt;
L-Eval&#65306;&#20026;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#26631;&#20934;&#21270;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
L-Eval: Instituting Standardized Evaluation for Long Context Language Models. (arXiv:2307.11088v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11088
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;L-Eval&#65292;&#26088;&#22312;&#20026;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#26631;&#20934;&#21270;&#35780;&#20272;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#21253;&#21547;411&#20010;&#38271;&#25991;&#26723;&#21644;2000&#22810;&#20010;&#20154;&#24037;&#26631;&#27880;&#30340;&#26597;&#35810;-&#22238;&#22797;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#25193;&#23637;&#19978;&#19979;&#25991;&#23545;&#20110;&#22788;&#29702;&#38271;&#36755;&#20837;&#30340;&#23454;&#36136;&#24615;&#25910;&#30410;&#21644;&#25913;&#36827;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#25193;&#23637;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#20197;&#20415;&#26377;&#25928;&#22788;&#29702;&#21333;&#22238;&#21512;&#30340;&#38271;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#35770;&#25991;&#24635;&#32467;&#65289;&#21644;&#20855;&#26377;&#26356;&#22797;&#26434;&#21382;&#21490;&#30340;&#23545;&#35805;&#65292;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#20687;GPT-4&#21644;Claude&#36825;&#26679;&#30340;&#19987;&#26377;&#27169;&#22411;&#22312;&#22788;&#29702;&#26497;&#38271;&#36755;&#20837;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;&#65292;&#20294;&#24320;&#25918;&#28304;&#20195;&#30721;&#27169;&#22411;&#20173;&#22788;&#20110;&#23581;&#35797;&#38454;&#27573;&#12290;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#25193;&#23637;&#19978;&#19979;&#25991;&#26159;&#21542;&#33021;&#22815;&#27604;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;&#26816;&#32034;&#65289;&#25552;&#20379;&#23454;&#36136;&#24615;&#30340;&#25910;&#30410;&#65292;&#20197;&#21450;&#23427;&#22312;&#23454;&#38469;&#19979;&#28216;&#20219;&#21153;&#20013;&#23545;&#24120;&#35268;&#27169;&#22411;&#30340;&#25913;&#36827;&#31243;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20026;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#26631;&#20934;&#21270;&#35780;&#20272;&#30340;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;L-Eval&#65292;&#20854;&#20013;&#21253;&#21547;411&#20010;&#38271;&#25991;&#26723;&#21644;2000&#22810;&#20010;&#20154;&#24037;&#26631;&#27880;&#30340;&#26597;&#35810;-&#22238;&#22797;&#23545;&#65292;&#28085;&#30422;&#27861;&#24459;&#12289;&#37329;&#34701;&#12289;&#23398;&#26657;&#35762;&#24231;&#12289;&#38271;&#23545;&#35805;&#12289;&#26032;&#38395;&#12289;&#38271;&#31687;&#23567;&#35828;&#21644;&#20250;&#35758;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been growing interest in extending the context length of instruction-following models in order to effectively process single-turn long input (e.g. summarizing a paper) and conversations with more extensive histories. While proprietary models such as GPT-4 and Claude have shown significant strides in handling extremely lengthy input, open-sourced models are still in the early stages of experimentation. It also remains unclear whether extending the context can offer substantial gains over traditional methods such as retrieval, and to what extent it improves upon their regular counterparts in practical downstream tasks. To address this challenge, we propose instituting standardized evaluation for long context language models. Concretely, we develop L-Eval which contains 411 long documents and over 2,000 human-labeled query-response pairs encompassing areas such as law, finance, school lectures, lengthy conversations, news, long-form novels, and meetings. L-Eval also ad
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#21644;Bing Chat&#22312;&#28385;&#36275;&#36234;&#21335;&#23398;&#29983;&#38656;&#27714;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;Bing Chat&#22312;&#38500;&#25991;&#23398;&#22806;&#30340;&#22810;&#20010;&#23398;&#31185;&#34920;&#29616;&#20248;&#20110;ChatGPT&#12290;Bing Chat&#37319;&#29992;&#26356;&#20808;&#36827;&#30340;GPT-4&#25216;&#26415;&#65292;&#33021;&#22815;&#25552;&#39640;&#25991;&#26412;&#30340;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#29983;&#25104;&#21019;&#36896;&#24615;&#12289;&#20449;&#24687;&#20016;&#23500;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2307.08272</link><description>&lt;p&gt;
ChatGPT&#24456;&#22909;&#65292;&#20294;&#23545;&#20110;&#36234;&#21335;&#23398;&#29983;&#26469;&#35828;&#65292;Bing Chat&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is Good but Bing Chat is Better for Vietnamese Students. (arXiv:2307.08272v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#21644;Bing Chat&#22312;&#28385;&#36275;&#36234;&#21335;&#23398;&#29983;&#38656;&#27714;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;Bing Chat&#22312;&#38500;&#25991;&#23398;&#22806;&#30340;&#22810;&#20010;&#23398;&#31185;&#34920;&#29616;&#20248;&#20110;ChatGPT&#12290;Bing Chat&#37319;&#29992;&#26356;&#20808;&#36827;&#30340;GPT-4&#25216;&#26415;&#65292;&#33021;&#22815;&#25552;&#39640;&#25991;&#26412;&#30340;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#29983;&#25104;&#21019;&#36896;&#24615;&#12289;&#20449;&#24687;&#20016;&#23500;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21363;ChatGPT&#21644;&#24494;&#36719;Bing Chat&#65288;BingChat&#65289;&#65292;&#22312;&#28385;&#36275;&#36234;&#21335;&#23398;&#29983;&#38656;&#27714;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#23613;&#31649;ChatGPT&#22312;&#22810;&#20010;&#23398;&#31185;&#20013;&#23637;&#29616;&#20102;&#39640;&#27700;&#20934;&#30340;&#33021;&#21147;&#65292;&#20294;Bing Chat&#34987;&#35748;&#20026;&#26159;&#26356;&#26377;&#20248;&#21183;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#23545;&#23427;&#20204;&#22312;&#25968;&#23398;&#12289;&#25991;&#23398;&#12289;&#33521;&#35821;&#12289;&#29289;&#29702;&#12289;&#21270;&#23398;&#12289;&#29983;&#29289;&#12289;&#21382;&#21490;&#12289;&#22320;&#29702;&#21644;&#20844;&#27665;&#25945;&#32946;&#31561;&#21508;&#20010;&#23398;&#31185;&#30340;&#23398;&#19994;&#25104;&#23601;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;BingChat&#22312;&#22810;&#20010;&#23398;&#31185;&#19978;&#26174;&#31034;&#20986;&#27604;ChatGPT&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#21807;&#29420;&#22312;&#25991;&#23398;&#26041;&#38754;&#65292;ChatGPT&#30340;&#34920;&#29616;&#26356;&#22909;&#19968;&#20123;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;GPT-3.5&#26500;&#24314;&#30340;ChatGPT&#30456;&#27604;&#65292;BingChat&#37319;&#29992;&#20102;&#26356;&#20808;&#36827;&#30340;GPT-4&#25216;&#26415;&#65292;&#36825;&#20351;&#20854;&#33021;&#22815;&#25552;&#39640;&#25991;&#26412;&#30340;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#29983;&#25104;&#21019;&#36896;&#24615;&#12289;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;BingChat&#22312;&#36234;&#21335;&#22320;&#21306;&#21487;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study examines the efficacy of two SOTA large language models (LLMs), namely ChatGPT and Microsoft Bing Chat (BingChat), in catering to the needs of Vietnamese students. Although ChatGPT exhibits proficiency in multiple disciplines, Bing Chat emerges as the more advantageous option. We conduct a comparative analysis of their academic achievements in various disciplines, encompassing mathematics, literature, English language, physics, chemistry, biology, history, geography, and civic education. The results of our study suggest that BingChat demonstrates superior performance compared to ChatGPT across a wide range of subjects, with the exception of literature, where ChatGPT exhibits better performance. Additionally, BingChat utilizes the more advanced GPT-4 technology in contrast to ChatGPT, which is built upon GPT-3.5. This allows BingChat to improve to comprehension, reasoning and generation of creative and informative text. Moreover, the fact that BingChat is accessible in Vietna
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GLORY&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20840;&#23616;&#22270;&#19982;&#26412;&#22320;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26500;&#24314;&#20840;&#23616;&#24863;&#30693;&#21382;&#21490;&#26032;&#38395;&#32534;&#30721;&#22120;&#26469;&#34701;&#21512;&#21382;&#21490;&#26032;&#38395;&#34920;&#31034;&#65292;&#24182;&#32771;&#34385;&#20102;&#29992;&#25143;&#38544;&#34255;&#30340;&#21160;&#26426;&#21644;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.06576</link><description>&lt;p&gt;
&#36229;&#36234;&#26412;&#22320;&#33539;&#22260;&#65306;&#20840;&#29699;&#22270;&#22686;&#24378;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Going Beyond Local: Global Graph-Enhanced Personalized News Recommendations. (arXiv:2307.06576v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GLORY&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20840;&#23616;&#22270;&#19982;&#26412;&#22320;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26500;&#24314;&#20840;&#23616;&#24863;&#30693;&#21382;&#21490;&#26032;&#38395;&#32534;&#30721;&#22120;&#26469;&#34701;&#21512;&#21382;&#21490;&#26032;&#38395;&#34920;&#31034;&#65292;&#24182;&#32771;&#34385;&#20102;&#29992;&#25143;&#38544;&#34255;&#30340;&#21160;&#26426;&#21644;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#22320;&#21521;&#29992;&#25143;&#25512;&#33616;&#20505;&#36873;&#26032;&#38395;&#25991;&#31456;&#19968;&#30452;&#26159;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#30340;&#26680;&#24515;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#36817;&#26399;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20174;&#20016;&#23500;&#30340;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#29992;&#20174;&#26412;&#22320;&#21382;&#21490;&#26032;&#38395;&#27966;&#29983;&#30340;&#22522;&#20110;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#20840;&#23616;&#35270;&#35282;&#65292;&#26410;&#33021;&#32771;&#34385;&#29992;&#25143;&#38544;&#34255;&#30340;&#21160;&#26426;&#21644;&#34892;&#20026;&#65292;&#36229;&#36234;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411; GLORY&#65288;Global-LOcal news Recommendation sYstem&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#20174;&#20854;&#20182;&#29992;&#25143;&#23398;&#21040;&#30340;&#20840;&#23616;&#34920;&#31034;&#21644;&#26412;&#22320;&#34920;&#31034;&#65292;&#26469;&#22686;&#24378;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20840;&#23616;&#24863;&#30693;&#21382;&#21490;&#26032;&#38395;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#20840;&#23616;&#26032;&#38395;&#22270;&#65292;&#24182;&#20351;&#29992;&#38376;&#25511;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20016;&#23500;&#26032;&#38395;&#34920;&#31034;&#65292;&#20174;&#32780;&#36890;&#36807;&#21382;&#21490;&#26032;&#38395;&#32858;&#21512;&#22120;&#34701;&#21512;&#21382;&#21490;&#26032;&#38395;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precisely recommending candidate news articles to users has always been a core challenge for personalized news recommendation systems. Most recent works primarily focus on using advanced natural language processing techniques to extract semantic information from rich textual data, employing content-based methods derived from local historical news. However, this approach lacks a global perspective, failing to account for users' hidden motivations and behaviors beyond semantic information. To address this challenge, we propose a novel model called GLORY (Global-LOcal news Recommendation sYstem), which combines global representations learned from other users with local representations to enhance personalized recommendation systems. We accomplish this by constructing a Global-aware Historical News Encoder, which includes a global news graph and employs gated graph neural networks to enrich news representations, thereby fusing historical news representations by a historical news aggregator.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30142;&#30149;&#36827;&#23637;&#32858;&#31867;&#20013;&#35299;&#35835;&#28145;&#24230;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;2&#22411;&#31958;&#23615;&#30149;&#21442;&#19982;&#32773;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#23545;&#30142;&#30149;&#36827;&#23637;&#27169;&#24335;&#30340;&#20020;&#24202;&#24847;&#20041;&#24615;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.06060</link><description>&lt;p&gt;
&#35299;&#35835;&#30142;&#30149;&#36827;&#23637;&#32858;&#31867;&#20013;&#30340;&#28145;&#24230;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Interpreting deep embeddings for disease progression clustering. (arXiv:2307.06060v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30142;&#30149;&#36827;&#23637;&#32858;&#31867;&#20013;&#35299;&#35835;&#28145;&#24230;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;2&#22411;&#31958;&#23615;&#30149;&#21442;&#19982;&#32773;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#23545;&#30142;&#30149;&#36827;&#23637;&#27169;&#24335;&#30340;&#20020;&#24202;&#24847;&#20041;&#24615;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24739;&#32773;&#32858;&#31867;&#30340;&#32972;&#26223;&#19979;&#35299;&#35835;&#28145;&#24230;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#33521;&#22269;&#29983;&#29289;&#24211;&#30340;2&#22411;&#31958;&#23615;&#30149;&#21442;&#19982;&#32773;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20986;&#23545;&#30142;&#30149;&#36827;&#23637;&#27169;&#24335;&#30340;&#20020;&#24202;&#24847;&#20041;&#24615;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel approach for interpreting deep embeddings in the context of patient clustering. We evaluate our approach on a dataset of participants with type 2 diabetes from the UK Biobank, and demonstrate clinically meaningful insights into disease progression patterns.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20851;&#27880;&#20102;&#25991;&#26412;&#38271;&#24230;&#23545;&#35789;&#27719;&#22810;&#26679;&#24615;&#20272;&#35745;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25351;&#25968;&#34429;&#28982;&#33021;&#35299;&#20915;&#38271;&#24230;&#20381;&#36182;&#38382;&#39064;&#65292;&#21364;&#26410;&#33021;&#35299;&#20915;&#23545;&#25991;&#26412;&#32553;&#20943;&#38271;&#24230;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#35770;&#25991;&#32473;&#20986;&#20102;&#20248;&#21270;&#35789;&#27719;&#22810;&#26679;&#24615;&#20998;&#26512;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2307.04626</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#20013;&#27979;&#37327;&#35789;&#27719;&#22810;&#26679;&#24615;&#65306;&#8220;&#20004;&#20493;&#38271;&#24230;&#38382;&#39064;&#8221;
&lt;/p&gt;
&lt;p&gt;
Measuring Lexical Diversity in Texts: The Twofold Length Problem. (arXiv:2307.04626v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04626
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20851;&#27880;&#20102;&#25991;&#26412;&#38271;&#24230;&#23545;&#35789;&#27719;&#22810;&#26679;&#24615;&#20272;&#35745;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25351;&#25968;&#34429;&#28982;&#33021;&#35299;&#20915;&#38271;&#24230;&#20381;&#36182;&#38382;&#39064;&#65292;&#21364;&#26410;&#33021;&#35299;&#20915;&#23545;&#25991;&#26412;&#32553;&#20943;&#38271;&#24230;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#35770;&#25991;&#32473;&#20986;&#20102;&#20248;&#21270;&#35789;&#27719;&#22810;&#26679;&#24615;&#20998;&#26512;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#25991;&#26412;&#38271;&#24230;&#23545;&#35789;&#27719;&#22810;&#26679;&#24615;&#20272;&#35745;&#30340;&#24433;&#21709;&#24050;&#32463;&#24341;&#36215;&#31185;&#23398;&#30028;&#30340;&#38271;&#26399;&#20851;&#27880;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25351;&#25968;&#65292;&#24182;&#36827;&#34892;&#20102;&#35768;&#22810;&#30740;&#31350;&#26469;&#35780;&#20272;&#23427;&#20204;&#65292;&#20294;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#26412;&#26041;&#27861;&#23398;&#32508;&#36848;&#19981;&#20165;&#23545;&#35821;&#35328;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#24120;&#29992;&#30340;&#25351;&#25968;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#36824;&#23545;&#38271;&#24230;&#38382;&#39064;&#26412;&#36523;&#20197;&#21450;&#35780;&#20272;&#25152;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#23545;&#19977;&#32452;&#33521;&#35821;&#23398;&#20064;&#32773;&#25991;&#26412;&#30340;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#25152;&#26377;&#25991;&#26412;&#20351;&#29992;&#27010;&#29575;&#25110;&#31639;&#27861;&#26041;&#27861;&#32553;&#20943;&#20026;&#30456;&#21516;&#38271;&#24230;&#30340;&#25351;&#25968;&#21487;&#20197;&#35299;&#20915;&#38271;&#24230;&#20381;&#36182;&#38382;&#39064;&#65307;&#28982;&#32780;&#65292;&#25152;&#26377;&#36825;&#20123;&#25351;&#25968;&#37117;&#26410;&#35299;&#20915;&#31532;&#20108;&#20010;&#38382;&#39064;&#65292;&#21363;&#23427;&#20204;&#23545;&#20915;&#23450;&#32553;&#20943;&#25991;&#26412;&#38271;&#24230;&#30340;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#12290;&#35770;&#25991;&#26368;&#21518;&#32473;&#20986;&#20102;&#20248;&#21270;&#35789;&#27719;&#22810;&#26679;&#24615;&#20998;&#26512;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impact of text length on the estimation of lexical diversity has captured the attention of the scientific community for more than a century. Numerous indices have been proposed, and many studies have been conducted to evaluate them, but the problem remains. This methodological review provides a critical analysis not only of the most commonly used indices in language learning studies, but also of the length problem itself, as well as of the methodology for evaluating the proposed solutions. The analysis of three datasets of English language-learners' texts revealed that indices that reduce all texts to the same length using a probabilistic or an algorithmic approach solve the length dependency problem; however, all these indices failed to address the second problem, which is their sensitivity to the parameter that determines the length to which the texts are reduced. The paper concludes with recommendations for optimizing lexical diversity analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#21644;&#38190;&#20540;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#24403;&#30456;&#20851;&#20449;&#24687;&#20301;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#24320;&#22836;&#25110;&#32467;&#23614;&#26102;&#24615;&#33021;&#26368;&#20339;&#65292;&#32780;&#24403;&#27169;&#22411;&#38656;&#35201;&#22312;&#38271;&#25991;&#26412;&#30340;&#20013;&#38388;&#35775;&#38382;&#30456;&#20851;&#20449;&#24687;&#26102;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#19987;&#38376;&#22788;&#29702;&#38271;&#25991;&#26412;&#30340;&#27169;&#22411;&#65292;&#36755;&#20837;&#25991;&#26412;&#36234;&#38271;&#24615;&#33021;&#20063;&#20250;&#22823;&#24133;&#38477;&#20302;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#36755;&#20837;&#25991;&#26412;&#30340;&#19978;&#19979;&#25991;&#25552;&#20379;&#20102;&#26032;&#30340;&#35748;&#35782;&#65292;&#24182;&#19988;&#20026;&#26410;&#26469;&#30340;&#38271;&#25991;&#26412;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.03172</link><description>&lt;p&gt;
&#36855;&#22833;&#22312;&#20013;&#38388;&#65306;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#38271;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Lost in the Middle: How Language Models Use Long Contexts. (arXiv:2307.03172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#21644;&#38190;&#20540;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#24403;&#30456;&#20851;&#20449;&#24687;&#20301;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#24320;&#22836;&#25110;&#32467;&#23614;&#26102;&#24615;&#33021;&#26368;&#20339;&#65292;&#32780;&#24403;&#27169;&#22411;&#38656;&#35201;&#22312;&#38271;&#25991;&#26412;&#30340;&#20013;&#38388;&#35775;&#38382;&#30456;&#20851;&#20449;&#24687;&#26102;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#19987;&#38376;&#22788;&#29702;&#38271;&#25991;&#26412;&#30340;&#27169;&#22411;&#65292;&#36755;&#20837;&#25991;&#26412;&#36234;&#38271;&#24615;&#33021;&#20063;&#20250;&#22823;&#24133;&#38477;&#20302;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#36755;&#20837;&#25991;&#26412;&#30340;&#19978;&#19979;&#25991;&#25552;&#20379;&#20102;&#26032;&#30340;&#35748;&#35782;&#65292;&#24182;&#19988;&#20026;&#26410;&#26469;&#30340;&#38271;&#25991;&#26412;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23558;&#38271;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#65292;&#20294;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#36739;&#38271;&#30340;&#25991;&#26412;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20004;&#20010;&#38656;&#35201;&#22312;&#36755;&#20837;&#25991;&#26412;&#20013;&#35782;&#21035;&#30456;&#20851;&#20449;&#24687;&#30340;&#20219;&#21153;&#65288;&#22810;&#25991;&#26723;&#38382;&#31572;&#21644;&#38190;&#20540;&#26816;&#32034;&#65289;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#30456;&#20851;&#20449;&#24687;&#20986;&#29616;&#22312;&#36755;&#20837;&#25991;&#26412;&#30340;&#24320;&#22836;&#25110;&#32467;&#23614;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#36890;&#24120;&#26368;&#20339;&#65307;&#32780;&#24403;&#27169;&#22411;&#38656;&#35201;&#35775;&#38382;&#38271;&#25991;&#26412;&#20013;&#30340;&#20013;&#38388;&#30456;&#20851;&#20449;&#24687;&#26102;&#65292;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#19987;&#38376;&#22788;&#29702;&#38271;&#25991;&#26412;&#30340;&#27169;&#22411;&#65292;&#24403;&#36755;&#20837;&#25991;&#26412;&#21464;&#24471;&#36234;&#26469;&#36234;&#38271;&#26102;&#65292;&#24615;&#33021;&#20063;&#20250;&#22823;&#24133;&#38477;&#20302;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20026;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#36755;&#20837;&#25991;&#26412;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#38271;&#25991;&#26412;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent language models have the ability to take long contexts as input, relatively little is known about how well the language models use longer context. We analyze language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. We find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. Furthermore, performance substantially decreases as the input context grows longer, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#35757;&#32451;GPT4&#39118;&#26684;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#21512;&#30740;&#31350;&#65292;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#20102;&#32593;&#32476;&#32467;&#26500;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#35757;&#32451;&#31574;&#30053;&#31561;&#35774;&#35745;&#36873;&#25321;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2307.02469</link><description>&lt;p&gt;
&#29992;&#22810;&#27169;&#24577;&#36755;&#20837;&#35757;&#32451;GPT4&#39118;&#26684;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#21738;&#20123;&#37325;&#35201;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?. (arXiv:2307.02469v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#35757;&#32451;GPT4&#39118;&#26684;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#21512;&#30740;&#31350;&#65292;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#20102;&#32593;&#32476;&#32467;&#26500;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#35757;&#32451;&#31574;&#30053;&#31561;&#35774;&#35745;&#36873;&#25321;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT4&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#22312;&#26681;&#25454;&#22270;&#20687;&#36981;&#24490;&#24320;&#25918;&#24335;&#25351;&#20196;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#32593;&#32476;&#32467;&#26500;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#35757;&#32451;&#31574;&#30053;&#31561;&#35774;&#35745;&#36873;&#25321;&#65292;&#24182;&#19988;&#36825;&#20123;&#36873;&#25321;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#35752;&#35770;&#65292;&#36825;&#20351;&#24471;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#24456;&#38590;&#37327;&#21270;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#23545;&#35757;&#32451;&#36825;&#31181;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#20840;&#38754;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;20&#22810;&#20010;&#24102;&#26377;&#25511;&#21046;&#35774;&#32622;&#30340;&#21464;&#20307;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#32593;&#32476;&#32467;&#26500;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;LLM&#39592;&#24178;&#21644;&#27169;&#22411;&#35774;&#35745;&#12290;&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#21644;&#37319;&#26679;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#25351;&#20196;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22810;&#26679;&#21270;&#25552;&#31034;&#23545;&#25152;&#35757;&#32451;&#27169;&#22411;&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;&#31532;&#19968;&#20010;&#22312;&#25105;&#20204;&#30340;&#26368;&#20339;&#30693;&#35782;&#33539;&#22260;&#20869;&#23545;&#27492;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs) such as GPT4 have displayed exceptional multi-modal capabilities in following open-ended instructions given images. However, the performance of these models heavily relies on design choices such as network structures, training data, and training strategies, and these choices have not been extensively discussed in the literature, making it difficult to quantify progress in this field. To address this issue, this paper presents a systematic and comprehensive study, quantitatively and qualitatively, on training such models. We implement over 20 variants with controlled settings. Concretely, for network structures, we compare different LLM backbones and model designs. For training data, we investigate the impact of data and sampling strategies. For instructions, we explore the influence of diversified prompts on the instruction-following ability of the trained models. For benchmarks, we contribute the first, to our best knowledge, compreh
&lt;/p&gt;</description></item><item><title>&#20845;&#36793;&#24418;&#26631;&#27880;&#22120;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20381;&#23384;&#20998;&#26512;&#22120;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#26102;&#23454;&#29616;&#23436;&#20840;&#24182;&#34892;&#21270;&#65292;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312; Penn Treebank &#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05477</link><description>&lt;p&gt;
&#20845;&#36793;&#24418;&#26631;&#27880;&#65306;&#23558;&#25237;&#24433;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#20316;&#20026;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Hexatagging: Projective Dependency Parsing as Tagging. (arXiv:2306.05477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05477
&lt;/p&gt;
&lt;p&gt;
&#20845;&#36793;&#24418;&#26631;&#27880;&#22120;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20381;&#23384;&#20998;&#26512;&#22120;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#26102;&#23454;&#29616;&#23436;&#20840;&#24182;&#34892;&#21270;&#65292;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312; Penn Treebank &#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20381;&#23384;&#20998;&#26512;&#22120;&#8212;&#8212;&#20845;&#36793;&#24418;&#26631;&#27880;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#21477;&#23376;&#20013;&#30340;&#21333;&#35789;&#26631;&#35760;&#20026;&#26469;&#33258;&#21487;&#33021;&#26631;&#35760;&#26377;&#38480;&#38598;&#21512;&#20013;&#30340;&#20803;&#32032;&#26469;&#26500;&#24314;&#20381;&#23384;&#26641;&#12290;&#19982;&#35768;&#22810;&#22788;&#29702;&#20381;&#23384;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#26102;&#26159;&#23436;&#20840;&#21487;&#24182;&#34892;&#21270;&#30340;&#65292;&#21363;&#29992;&#20110;&#26500;&#24314;&#20381;&#23384;&#20998;&#26512;&#25152;&#38656;&#30340;&#32467;&#26500;&#26500;&#24314;&#25805;&#20316;&#21487;&#20197;&#30456;&#20114;&#24182;&#34892;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#30830;&#20999;&#35299;&#30721;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#37117;&#26159;&#32447;&#24615;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#20381;&#23384;&#20998;&#26512;&#22120;&#65292;&#23427;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#24449;&#26469;&#39044;&#27979;&#20845;&#36793;&#26631;&#35760;&#65292;&#32780;&#19981;&#38656;&#35201;&#19987;&#20026;&#27492;&#20219;&#21153;&#26126;&#30830;&#35774;&#35745;&#30340;&#23450;&#21046;&#20307;&#31995;&#32467;&#26500;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#24615;&#21644;&#31616;&#21333;&#24615;&#65292;&#20294;&#22312; Penn Treebank &#27979;&#35797;&#38598;&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102; 96.4 LAS &#21644; 97.4 UAS &#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#22120;&#30340;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#24182;&#34892;&#24615;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;&#22823;&#32422;&#21313;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel dependency parser, the hexatagger, that constructs dependency trees by tagging the words in a sentence with elements from a finite set of possible tags. In contrast to many approaches to dependency parsing, our approach is fully parallelizable at training time, i.e., the structure-building actions needed to build a dependency parse can be predicted in parallel to each other. Additionally, exact decoding is linear in time and space complexity. Furthermore, we derive a probabilistic dependency parser that predicts hexatags using no more than a linear model with features from a pretrained language model, i.e., we forsake a bespoke architecture explicitly designed for the task. Despite the generality and simplicity of our approach, we achieve state-of-the-art performance of 96.4 LAS and 97.4 UAS on the Penn Treebank test set. Additionally, our parser's linear time complexity and parallelism significantly improve computational efficiency, with a roughly 10-times speed-u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;2SDiac&#30340;&#22810;&#28304;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#20013;&#20351;&#29992;&#21487;&#36873;&#38899;&#26631;&#26469;&#30830;&#23450;&#25152;&#26377;&#39044;&#27979;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#36890;&#36807;&#24341;&#20837;Guided Learning&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#38543;&#26426;&#25513;&#34109;&#21644;&#32473;&#23450;&#30340;&#36755;&#20837;&#38899;&#26631;&#25552;&#21319;&#26631;&#35760;&#30340;&#27491;&#30830;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#26631;&#35760;&#25991;&#26412;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03557</link><description>&lt;p&gt;
&#21033;&#29992;&#37096;&#20998;&#26631;&#27880;&#30340;&#25991;&#26412;&#25552;&#21319;&#38463;&#25289;&#20271;&#35821;&#38899;&#26631;&#27880;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Take the Hint: Improving Arabic Diacritization with Partially-Diacritized Text. (arXiv:2306.03557v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;2SDiac&#30340;&#22810;&#28304;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#20013;&#20351;&#29992;&#21487;&#36873;&#38899;&#26631;&#26469;&#30830;&#23450;&#25152;&#26377;&#39044;&#27979;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#36890;&#36807;&#24341;&#20837;Guided Learning&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#38543;&#26426;&#25513;&#34109;&#21644;&#32473;&#23450;&#30340;&#36755;&#20837;&#38899;&#26631;&#25552;&#21319;&#26631;&#35760;&#30340;&#27491;&#30830;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#26631;&#35760;&#25991;&#26412;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;&#38463;&#25289;&#20271;&#35821;&#38899;&#26631;&#27880;&#22312;&#24456;&#22810;&#24212;&#29992;&#22330;&#26223;&#20013;&#37117;&#38750;&#24120;&#26377;&#29992;&#65292;&#27604;&#22914;&#23545;&#20110;&#35821;&#35328;&#23398;&#20064;&#32773;&#26469;&#35828;&#65292;&#26631;&#27880;&#21487;&#20197;&#25552;&#20379;&#38405;&#35835;&#25903;&#25345;&#65292;&#32780;&#23545;&#20110;&#35821;&#38899;&#21512;&#25104;&#36825;&#26679;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#26631;&#27880;&#20934;&#30830;&#24615;&#23545;&#20110;&#21457;&#38899;&#39044;&#27979;&#20063;&#38750;&#24120;&#37325;&#35201;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#25968;&#19987;&#27880;&#20110;&#22788;&#29702;&#27809;&#26377;&#38899;&#26631;&#30340;&#21407;&#22987;&#25991;&#26412;&#30340;&#27169;&#22411;&#65292;&#20294;&#26159;&#36890;&#36807;&#32473;&#20154;&#31867;&#25552;&#20379;&#36873;&#23450;&#30340;&#25110;&#37096;&#20998;&#26631;&#27880;&#30340;&#25935;&#24863;&#35789;&#27719;&#65292;&#21487;&#20197;&#20351;&#24471;&#29983;&#20135;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;2SDiac&#30340;&#22810;&#28304;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25903;&#25345;&#36755;&#20837;&#20013;&#30340;&#21487;&#36873;&#38899;&#26631;&#20197;&#30830;&#23450;&#25152;&#26377;&#39044;&#27979;&#30340;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Guided Learning&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#21033;&#29992;&#32473;&#23450;&#30340;&#36755;&#20837;&#38899;&#26631;&#21644;&#19981;&#21516;&#31561;&#32423;&#30340;&#38543;&#26426;&#25513;&#34109;&#26469;&#25552;&#21319;&#26631;&#27880;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27979;&#35797;&#26399;&#38388;&#25552;&#20379;&#30340;&#26631;&#27880;&#33021;&#22815;&#24433;&#21709;&#26356;&#22810;&#30340;&#36755;&#20986;&#20301;&#32622;&#65292;&#23454;&#39564;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#38750;&#26631;&#35760;&#25991;&#26412;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#20943;&#23569;60%&#30340;&#21442;&#25968;&#25968;&#30446;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Arabic diacritization is useful in many applications, ranging from reading support for language learners to accurate pronunciation predictor for downstream tasks like speech synthesis. While most of the previous works focused on models that operate on raw non-diacritized text, production systems can gain accuracy by first letting humans partly annotate ambiguous words. In this paper, we propose 2SDiac, a multi-source model that can effectively support optional diacritics in input to inform all predictions. We also introduce Guided Learning, a training scheme to leverage given diacritics in input with different levels of random masking. We show that the provided hints during test affect more output positions than those annotated. Moreover, experiments on two common benchmarks show that our approach i) greatly outperforms the baseline also when evaluated on non-diacritized text; and ii) achieves state-of-the-art results while reducing the parameter count by over 60%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#27861;&#24863;&#30693;&#30340;&#28151;&#21512;&#25552;&#31034;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#34701;&#21512;&#25163;&#24037;&#25552;&#31034;&#21644;&#21487;&#23398;&#20064;&#25552;&#31034;&#65292;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20248;&#21270;&#25552;&#31034;&#32534;&#30721;&#22120;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#26512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01312</link><description>&lt;p&gt;
&#35821;&#27861;&#24863;&#30693;&#30340;&#28151;&#21512;&#25552;&#31034;&#27169;&#22411;&#29992;&#20110;&#23567;&#26679;&#26412;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Syntax-aware Hybrid prompt model for Few-shot multi-modal sentiment analysis. (arXiv:2306.01312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#27861;&#24863;&#30693;&#30340;&#28151;&#21512;&#25552;&#31034;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#34701;&#21512;&#25163;&#24037;&#25552;&#31034;&#21644;&#21487;&#23398;&#20064;&#25552;&#31034;&#65292;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20248;&#21270;&#25552;&#31034;&#32534;&#30721;&#22120;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#26512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26159;&#24403;&#21069;&#19968;&#20010;&#28909;&#38376;&#35805;&#39064;&#65292;&#20027;&#35201;&#38024;&#23545;&#21477;&#23376;&#21644;&#26041;&#38754;&#32423;&#21035;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20960;&#20046;&#37117;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#36825;&#20250;&#24102;&#26469;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#36328;&#27169;&#24577;&#30340;&#23569;&#26679;&#26412;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#26159;&#24456;&#23454;&#29992;&#30340;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#25191;&#34892;&#22312;&#25991;&#26412;&#27169;&#24335;&#65292;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20027;&#35201;&#26377;&#20004;&#31181;&#31867;&#22411;&#65306;&#25163;&#24037;&#25552;&#31034;&#21644;&#21487;&#23398;&#20064;&#25552;&#31034;&#12290;&#22312;&#23567;&#26679;&#26412;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#29616;&#26377;&#26041;&#27861;&#24050;&#20998;&#21035;&#20351;&#29992;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#24335;&#65292;&#21487;&#20197;&#32467;&#21512;&#19968;&#20010;&#25110;&#22810;&#20010;&#22266;&#23450;&#30340;&#25163;&#24037;&#25552;&#31034;&#21644;&#21487;&#23398;&#20064;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#20248;&#21270;&#25552;&#31034;&#32534;&#30721;&#22120;&#12290;&#22312;&#21477;&#23376;&#32423;&#21644;&#26041;&#38754;&#32423;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sentiment Analysis (MSA) has been a popular topic in natural language processing nowadays, at both sentence and aspect level. However, the existing approaches almost require large-size labeled datasets, which bring about large consumption of time and resources. Therefore, it is practical to explore the method for few-shot sentiment analysis in cross-modalities. Previous works generally execute on textual modality, using the prompt-based methods, mainly two types: hand-crafted prompts and learnable prompts. The existing approach in few-shot multi-modality sentiment analysis task has utilized both methods, separately. We further design a hybrid pattern that can combine one or more fixed hand-crafted prompts and learnable prompts and utilize the attention mechanisms to optimize the prompt encoder. The experiments on both sentence-level and aspect-level datasets prove that we get a significant outperformance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#36827;&#34892;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35828;&#26126;&#20316;&#20026;&#25552;&#31034;&#30340;ChatGPT&#22312;&#21015;&#31867;&#22411;&#27880;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20219;&#21153;&#23450;&#20041;&#36827;&#34892;&#28789;&#27963;&#30340;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.00745</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#36827;&#34892;&#21015;&#31867;&#22411;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
Column Type Annotation using ChatGPT. (arXiv:2306.00745v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#36827;&#34892;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35828;&#26126;&#20316;&#20026;&#25552;&#31034;&#30340;ChatGPT&#22312;&#21015;&#31867;&#22411;&#27880;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20219;&#21153;&#23450;&#20041;&#36827;&#34892;&#28789;&#27963;&#30340;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21015;&#31867;&#22411;&#27880;&#37322;&#26159;&#23558;&#20851;&#31995;&#34920;&#30340;&#21015;&#26631;&#27880;&#20026;&#27599;&#21015;&#20013;&#21253;&#21547;&#30340;&#20540;&#30340;&#35821;&#20041;&#31867;&#22411;&#30340;&#20219;&#21153;&#12290;&#21015;&#31867;&#22411;&#27880;&#37322;&#26159;&#25968;&#25454;&#28246;&#29615;&#22659;&#20013;&#25968;&#25454;&#26816;&#32034;&#21644;&#25968;&#25454;&#38598;&#25104;&#30340;&#37325;&#35201;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#21015;&#31867;&#22411;&#27880;&#37322;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#23558;&#34920;&#21015;&#19982;&#30693;&#35782;&#22270;&#30340;&#23646;&#24615;&#36827;&#34892;&#21305;&#37197;&#65292;&#35201;&#20040;&#23545;&#21015;&#31867;&#22411;&#27880;&#37322;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#25506;&#31350;&#20351;&#29992;ChatGPT&#36827;&#34892;&#21015;&#31867;&#22411;&#27880;&#37322;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#24182;&#23581;&#35797;&#21521;&#27169;&#22411;&#25552;&#20379;&#20219;&#21153;&#23450;&#20041;&#21644;&#35814;&#32454;&#35828;&#26126;&#12290;&#25105;&#20204;&#36824;&#23454;&#29616;&#20102;&#19968;&#20010;&#20004;&#27493;&#30340;&#34920;&#26684;&#27880;&#37322;&#27969;&#31243;&#65292;&#39318;&#20808;&#30830;&#23450;&#34920;&#20013;&#25551;&#36848;&#30340;&#23454;&#20307;&#30340;&#31867;&#21035;&#65292;&#28982;&#21518;&#26681;&#25454;&#36825;&#20010;&#31867;&#21035;&#65292;&#20351;&#29992;ChatGPT&#23545;&#21015;&#36827;&#34892;&#27880;&#37322;&#65292;&#20165;&#20351;&#29992;&#25972;&#20307;&#35789;&#27719;&#34920;&#30340;&#30456;&#20851;&#23376;&#38598;&#12290;&#20351;&#29992;&#35828;&#26126;&#20316;&#20026;&#25552;&#31034;&#30340;ChatGPT&#22312;&#21015;&#31867;&#22411;&#27880;&#37322;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#33021;&#22815;&#26681;&#25454;&#20219;&#21153;&#23450;&#20041;&#36827;&#34892;&#28789;&#27963;&#30340;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Column type annotation is the task of annotating the columns of a relational table with the semantic type of the values contained in each column. Column type annotation is an important pre-processing step for data search and data integration in the context of data lakes. State-of-the-art column type annotation methods either rely on matching table columns to properties of a knowledge graph or fine-tune pre-trained language models such as BERT for column type annotation. In this work, we take a different approach and explore using ChatGPT for column type annotation. We evaluate different prompt designs in zero- and few-shot settings and experiment with providing task definitions and detailed instructions to the model. We further implement a two-step table annotation pipeline which first determines the class of the entities described in the table and depending on this class asks ChatGPT to annotate columns using only the relevant subset of the overall vocabulary. Using instructions as we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#21644;&#23454;&#35777;&#22320;&#32771;&#23519;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#20869;&#30340;&#24341;&#25991;&#27169;&#24335;&#65292;&#35777;&#26126;&#20102;&#22823;&#32422;62%&#30340;&#24341;&#29992;&#35770;&#25991;&#23646;&#20110;&#20986;&#29256;&#21069;&#20116;&#24180;&#65292;&#21482;&#26377;&#32422;17%&#30340;&#35770;&#25991;&#36229;&#36807;&#21313;&#24180;&#12290;&#30446;&#21069;&#65292;NLP&#35770;&#25991;&#30340;&#24341;&#29992;&#24180;&#40836;&#36235;&#20110;&#21382;&#21490;&#26368;&#20302;&#27700;&#24179;&#65292;&#36825;&#20010;&#36235;&#21183;&#19982;&#27492;&#21069;&#30456;&#21453;&#12290;</title><link>http://arxiv.org/abs/2305.18554</link><description>&lt;p&gt;
&#36951;&#24536;&#30340;&#30693;&#35782;&#65306;&#23457;&#35270;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24341;&#25991;&#20581;&#24536;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Forgotten Knowledge: Examining the Citational Amnesia in NLP. (arXiv:2305.18554v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#21644;&#23454;&#35777;&#22320;&#32771;&#23519;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#20869;&#30340;&#24341;&#25991;&#27169;&#24335;&#65292;&#35777;&#26126;&#20102;&#22823;&#32422;62%&#30340;&#24341;&#29992;&#35770;&#25991;&#23646;&#20110;&#20986;&#29256;&#21069;&#20116;&#24180;&#65292;&#21482;&#26377;&#32422;17%&#30340;&#35770;&#25991;&#36229;&#36807;&#21313;&#24180;&#12290;&#30446;&#21069;&#65292;NLP&#35770;&#25991;&#30340;&#24341;&#29992;&#24180;&#40836;&#36235;&#20110;&#21382;&#21490;&#26368;&#20302;&#27700;&#24179;&#65292;&#36825;&#20010;&#36235;&#21183;&#19982;&#27492;&#21069;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#29992;&#35770;&#25991;&#26159;&#29616;&#20195;&#31185;&#23398;&#20889;&#20316;&#35752;&#35770;&#21644;&#24314;&#31435;&#20808;&#21069;&#24037;&#20316;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#20840;&#38754;&#22320;&#24341;&#29992;&#19968;&#32452;&#22810;&#26679;&#21270;(&#26102;&#38388;&#21644;&#30740;&#31350;&#39046;&#22495;)&#30340;&#35770;&#25991;&#26159;&#34913;&#37327;&#31038;&#21306;&#38405;&#35835;&#24191;&#27867;&#31243;&#24230;&#30340;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#40092;&#26377;&#30740;&#31350;&#25506;&#35752;&#24341;&#25991;&#30340;&#24191;&#27867;&#26102;&#38388;&#27169;&#24335;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#21644;&#23454;&#35777;&#22320;&#32771;&#23519;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20869;&#30340;&#24341;&#25991;&#37325;&#35201;&#24615;&#65292;&#25506;&#35752;&#20102;&#25105;&#20204;&#24341;&#29992;&#35770;&#25991;&#26102;&#24448;&#22238;&#36861;&#28335;&#22810;&#23569;&#24180;&#30340;&#38382;&#39064;&#65292;&#24341;&#29992;&#26102;&#38388;&#30340;&#21464;&#21270;&#36235;&#21183;&#65292;&#20197;&#21450;&#26377;&#21738;&#20123;&#22240;&#32032;&#19982;&#36825;&#31181;&#24341;&#25991;&#27880;&#24847;&#21147;/&#20581;&#24536;&#29366;&#24577;&#30456;&#20851;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#32422;71.5K&#31687;&#35770;&#25991;&#65292;&#36873;&#25321;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20316;&#20026;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#39046;&#22495;&#65292;&#24182;&#35777;&#26126;&#24182;&#37327;&#21270;&#20102;&#24341;&#29992;&#20013;&#30340;&#20960;&#20010;&#20851;&#38190;&#36235;&#21183;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#32422;62%&#30340;&#24341;&#29992;&#35770;&#25991;&#23646;&#20110;&#22312;&#20986;&#29256;&#21069;&#20116;&#24180;&#30340;&#35770;&#25991;&#65292;&#32780;&#21482;&#26377;&#32422;17%&#30340;&#35770;&#25991;&#36229;&#36807;&#21313;&#24180;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#24341;&#29992;&#35770;&#25991;&#30340;&#24180;&#40836;&#20013;&#20301;&#25968;&#21644;&#24180;&#40836;&#22810;&#26679;&#24615;&#20174;1990&#24180;&#21040;2014&#24180;&#25345;&#32493;&#22686;&#21152;&#65292;&#20294;&#33258;&#37027;&#26102;&#36215;&#65292;&#36825;&#20010;&#36235;&#21183;&#24050;&#32463;&#36870;&#36716;&#65292;&#30446;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35770;&#25991;&#30340;&#24341;&#25991;&#24180;&#40836;&#36824;&#21019;&#21382;&#21490;&#26368;&#20302;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Citing papers is the primary method through which modern scientific writing discusses and builds on past work. Collectively, citing a diverse set of papers (in time and area of study) is an indicator of how widely the community is reading. Yet, there is little work looking at broad temporal patterns of citation. This work systematically and empirically examines: How far back in time do we tend to go to cite papers? How has that changed over time, and what factors correlate with this citational attention/amnesia? We chose NLP as our domain of interest and analyzed approximately 71.5K papers to show and quantify several key trends in citation. Notably, around 62% of cited papers are from the immediate five years prior to publication, whereas only about 17% are more than ten years old. Furthermore, we show that the median age and age diversity of cited papers were steadily increasing from 1990 to 2014, but since then, the trend has reversed, and current NLP papers have an all-time low tem
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#24037;&#20316;&#21407;&#29702;&#12289;&#23433;&#20840;&#19982;&#38544;&#31169;&#23041;&#32961;&#12289;&#29616;&#29366;&#21644;&#26410;&#26469;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.18339</link><description>&lt;p&gt;
ChatGPT&#65306;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#25361;&#25112;&#19982;&#35299;&#20915;&#26041;&#26696;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions. (arXiv:2305.18339v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#24037;&#20316;&#21407;&#29702;&#12289;&#23433;&#20840;&#19982;&#38544;&#31169;&#23041;&#32961;&#12289;&#29616;&#29366;&#21644;&#26410;&#26469;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#27604;&#22914;ChatGPT&#30340;&#26222;&#21450;&#20351;&#29992;&#65292;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#65292;&#27491;&#22312;&#24341;&#39046;&#20869;&#23481;&#21019;&#20316;&#21644;&#30693;&#35782;&#34920;&#31034;&#26041;&#24335;&#23454;&#29616;&#33539;&#24335;&#36716;&#21464;&#12290;AIGC&#21033;&#29992;&#29983;&#25104;&#24335;&#22823;&#22411;AI&#31639;&#27861;&#26469;&#36741;&#21161;&#25110;&#26367;&#20195;&#20154;&#31867;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#25552;&#31034;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#21644;&#26356;&#20302;&#30340;&#25104;&#26412;&#21019;&#24314;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#21644;&#31867;&#20154;&#30340;&#20869;&#23481;&#12290;&#23613;&#31649;&#22312;AIGC&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23433;&#20840;&#12289;&#38544;&#31169;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#25361;&#25112;&#20173;&#38656;&#35299;&#20915;&#12290;&#26412;&#25991;&#28145;&#20837;&#35843;&#26597;&#20102;AIGC&#33539;&#24335;&#30340;&#24037;&#20316;&#21407;&#29702;&#12289;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#12289;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#21644;&#26410;&#26469;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#35752;&#20102;AIGC&#30340;&#25216;&#26415;&#23454;&#29616;&#12289;&#24635;&#20307;&#26550;&#26500;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24037;&#20316;&#27169;&#24335;&#21644;&#20851;&#38190;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#38024;&#23545;AIGC&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#20998;&#31867;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;GPT&#21644;AIGC&#25216;&#26415;&#30340;&#20262;&#29702;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#24050;&#30830;&#23450;&#30340;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;AIGC&#39046;&#22495;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#21457;&#23637;AIGC&#30340;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread use of large artificial intelligence (AI) models such as ChatGPT, AI-generated content (AIGC) has garnered increasing attention and is leading a paradigm shift in content creation and knowledge representation. AIGC uses generative large AI algorithms to assist or replace humans in creating massive, high-quality, and human-like content at a faster pace and lower cost, based on user-provided prompts. Despite the recent significant progress in AIGC, security, privacy, ethical, and legal challenges still need to be addressed. This paper presents an in-depth survey of working principles, security and privacy threats, state-of-the-art solutions, and future challenges of the AIGC paradigm. Specifically, we first explore the enabling technologies, general architecture of AIGC, and discuss its working modes and key characteristics. Then, we investigate the taxonomy of security and privacy threats to AIGC and highlight the ethical and societal implications of GPT and AIGC tec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.17760</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;
&lt;/p&gt;
&lt;p&gt;
Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Ouyang&#31561;&#20154;&#65292;2022&#65289;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#65288;Kahneman&#65292;2011&#65289;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24515;&#29702;&#23398;&#23478;&#20204;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25193;&#23637;&#36825;&#20010;&#26694;&#26550;&#30340;&#36884;&#24452;&#12290;&#26412;&#30740;&#31350;&#23454;&#36136;&#19978;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#26469;&#33719;&#24471;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#26041;&#38754;&#30340;&#28145;&#21051;&#35265;&#35299;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do language models "think"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;AI&#23545;&#31185;&#23398;&#30740;&#31350;&#25152;&#24102;&#26469;&#30340;&#35748;&#35782;&#35770;&#25361;&#25112;&#12289;&#20262;&#29702;&#21644;&#35802;&#20449;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#21313;&#39033;&#24314;&#35758;&#65292;&#20197;&#22312;AI&#26102;&#20195;&#20419;&#36827;&#26356;&#36127;&#36131;&#20219;&#30340;&#30740;&#31350;&#36827;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.15299</link><description>&lt;p&gt;
&#22312;ChatGPT&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;AI&#26102;&#20195;&#30340;&#31185;&#23398;&#65306;&#30740;&#31350;&#20262;&#29702;&#30340;&#25361;&#25112;&#21450;&#24212;&#23545;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond. (arXiv:2305.15299v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;AI&#23545;&#31185;&#23398;&#30740;&#31350;&#25152;&#24102;&#26469;&#30340;&#35748;&#35782;&#35770;&#25361;&#25112;&#12289;&#20262;&#29702;&#21644;&#35802;&#20449;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#21313;&#39033;&#24314;&#35758;&#65292;&#20197;&#22312;AI&#26102;&#20195;&#20419;&#36827;&#26356;&#36127;&#36131;&#20219;&#30340;&#30740;&#31350;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#20855;&#26377;&#26174;&#33879;&#20294;&#26377;&#20105;&#35758;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;AI&#26102;&#20195;&#31185;&#23398;&#30740;&#31350;&#30340;&#35748;&#35782;&#35770;&#25361;&#25112;&#12289;&#20262;&#29702;&#21644;&#35802;&#20449;&#39118;&#38505;&#65292;&#24182;&#26088;&#22312;&#20026;&#39640;&#36136;&#37327;&#30340;&#30740;&#31350;&#20262;&#29702;&#23457;&#26597;&#22880;&#23450;&#26032;&#30340;&#21450;&#26102;&#22522;&#30784;&#12290;&#23545;AI&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30740;&#31350;&#24037;&#20855;&#21644;&#30740;&#31350;&#23545;&#35937;&#30340;&#35282;&#33394;&#36827;&#34892;&#20102;&#35814;&#32454;&#23457;&#26597;&#65292;&#24182;&#35752;&#35770;&#20102;&#23545;&#31185;&#23398;&#23478;&#12289;&#21442;&#19982;&#32773;&#21644;&#35780;&#23457;&#20154;&#21592;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;&#35752;&#35770;&#20102;&#30740;&#31350;&#20262;&#29702;&#23457;&#26597;&#30340;&#26032;&#20852;&#23454;&#36341;&#65292;&#24182;&#32473;&#20986;&#20102;&#21313;&#39033;&#24314;&#35758;&#65292;&#20026;&#22312;AI&#26102;&#20195;&#26356;&#36127;&#36131;&#20219;&#30340;&#30740;&#31350;&#36827;&#34892;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models of artificial intelligence (AI), such as ChatGPT, find remarkable but controversial applicability in science and research. This paper reviews epistemological challenges, ethical and integrity risks in science conduct in the advent of generative AI. This is with the aim to lay new timely foundations for a high-quality research ethics review. The role of AI language models as a research instrument and subject is scrutinized along with ethical implications for scientists, participants and reviewers. New emerging practices for research ethics review are discussed, concluding with ten recommendations that shape a response for a more responsible research conduct in the era of AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;Survey&#20102;&#22522;&#20110;DTL&#30340;ASR&#26694;&#26550;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14535</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65306;&#36808;&#21521;&#26356;&#22909;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Deep Transfer Learning for Automatic Speech Recognition: Towards Better Generalization. (arXiv:2304.14535v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;Survey&#20102;&#22522;&#20110;DTL&#30340;ASR&#26694;&#26550;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26041;&#38754;&#38754;&#20020;&#30528;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#36825;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36890;&#24120;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#30456;&#21516;&#30340;&#22495;&#65292;&#20855;&#26377;&#30456;&#21516;&#30340;&#36755;&#20837;&#29305;&#24449;&#31354;&#38388;&#21644;&#25968;&#25454;&#20998;&#24067;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20123;&#29616;&#23454;&#19990;&#30028;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#20551;&#35774;&#26159;&#26080;&#27861;&#36866;&#29992;&#30340;&#12290;DTL&#34987;&#24341;&#20837;&#26469;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#23427;&#26377;&#21161;&#20110;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#38598;&#24320;&#21457;&#39640;&#24615;&#33021;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#23454;&#38469;&#25968;&#25454;&#38598;&#21363;&#20351;&#24456;&#23567;&#25110;&#31245;&#26377;&#19981;&#21516;&#65292;&#20294;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;DTL&#30340;ASR&#26694;&#26550;&#30340;&#20840;&#38754;&#35843;&#26597;&#65292;&#20197;&#38416;&#26126;&#26368;&#26032;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) has recently become an important challenge when using deep learning (DL). It requires large-scale training datasets and high computational and storage resources. Moreover, DL techniques and machine learning (ML) approaches in general, hypothesize that training and testing data come from the same domain, with the same input feature space and data distribution characteristics. This assumption, however, is not applicable in some real-world artificial intelligence (AI) applications. Moreover, there are situations where gathering real data is challenging, expensive, or rarely occurring, which can not meet the data requirements of DL models. deep transfer learning (DTL) has been introduced to overcome these issues, which helps develop high-performing models using real datasets that are small or slightly different but related to the training data. This paper presents a comprehensive survey of DTL-based ASR frameworks to shed light on the latest developments 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#33258;&#38382;&#33258;&#31572;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05368</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#20934;&#22791;&#23601;&#32490;&#20102;&#21527;&#65311;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding. (arXiv:2304.05368v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#33258;&#38382;&#33258;&#31572;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#19987;&#19994;&#24615;&#36136;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#26368;&#20808;&#36827;&#30340;LLMs&#8212;&#8212;GPT-3.5&#12289;GPT-4&#21644;Bard&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#35813;&#35780;&#20272;&#33539;&#22260;&#28085;&#30422;&#20102;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#12289;&#25991;&#26723;&#20998;&#31867;&#21644;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#8212;&#8212;&#33258;&#38382;&#33258;&#31572;&#25552;&#31034;&#65288;SQP&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#21457;&#19982;&#30456;&#20851;&#20020;&#24202;&#22330;&#26223;&#30456;&#20851;&#30340;&#20449;&#24687;&#24615;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#23450;&#21046;&#21270;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#24378;&#35843;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#23398;&#20064;&#31574;&#30053;&#21644;&#25552;&#31034;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant progress in various domains, including healthcare. However, the specialized nature of clinical language understanding tasks presents unique challenges and limitations that warrant further investigation. In this study, we conduct a comprehensive evaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within the realm of clinical language understanding tasks. These tasks span a diverse range, including named entity recognition, relation extraction, natural language inference, semantic textual similarity, document classification, and question-answering. We also introduce a novel prompting strategy, self-questioning prompting (SQP), tailored to enhance LLMs' performance by eliciting informative questions and answers pertinent to the clinical scenarios at hand. Our evaluation underscores the significance of task-specific learning strategies and prompting techniques for improving LLMs' effectiveness in healthcare-related tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#21033;&#29992;LLMs&#22312;&#29273;&#31185;&#20020;&#24202;&#39046;&#22495;&#23454;&#29616;&#33258;&#21160;&#21270;&#21644;&#36328;&#27169;&#24577;&#35786;&#26029;&#30340;&#21487;&#33021;&#24615;&#65292;&#20171;&#32461;&#20102;&#21033;&#29992;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#36827;&#34892;&#39640;&#32423;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;LLM AI&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#29273;&#31185;&#20020;&#24202;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.03086</link><description>&lt;p&gt;
ChatGPT&#22609;&#36896;&#29273;&#31185;&#26410;&#26469;&#65306;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Shaping the Future of Dentistry: The Potential of Multi-Modal Large Language Model. (arXiv:2304.03086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#21033;&#29992;LLMs&#22312;&#29273;&#31185;&#20020;&#24202;&#39046;&#22495;&#23454;&#29616;&#33258;&#21160;&#21270;&#21644;&#36328;&#27169;&#24577;&#35786;&#26029;&#30340;&#21487;&#33021;&#24615;&#65292;&#20171;&#32461;&#20102;&#21033;&#29992;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#36827;&#34892;&#39640;&#32423;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;LLM AI&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#29273;&#31185;&#20020;&#24202;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;OpenAI&#24320;&#21457;&#30340;Generative Pretrained Transformer 4&#65288;GPT-4&#65289;&#30340;&#31934;&#31616;&#21644;&#23545;&#35805;&#21464;&#20307;&#65292;&#20855;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#37324;&#31243;&#30865;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#12290;&#20107;&#23454;&#19978;&#65292;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#30340;&#21360;&#35937;&#28145;&#21051;&#33021;&#21147;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#23545;&#21508;&#20010;&#39046;&#22495;&#20135;&#29983;&#20102;&#28145;&#36828;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20027;&#35201;&#35752;&#35770;LLMs&#22312;&#29273;&#31185;&#39046;&#22495;&#30340;&#26410;&#26469;&#24212;&#29992;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;LLM&#37096;&#32626;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#21160;&#29273;&#31185;&#35786;&#26029;&#21644;&#36328;&#27169;&#24577;&#29273;&#31185;&#35786;&#26029;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#37197;&#22791;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#65292;&#21333;&#20010;LLM&#21487;&#20197;&#31649;&#29702;&#22810;&#28304;&#25968;&#25454;&#24182;&#36827;&#34892;&#39640;&#32423;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#20197;&#25191;&#34892;&#22797;&#26434;&#30340;&#20020;&#24202;&#25805;&#20316;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#26469;&#23637;&#31034;&#38024;&#23545;&#29273;&#31185;&#20020;&#24202;&#24212;&#29992;&#30340;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#22810;&#27169;&#24577;LLM AI&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;LLMs&#22312;&#25552;&#20379;&#24040;&#22823;&#30340;&#28508;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;
&lt;/p&gt;
&lt;p&gt;
The ChatGPT, as a lite and conversational variant of Generative Pretrained Transformer 4 (GPT-4) developed by OpenAI, is one of the milestone Large Language Models (LLMs) with billions of parameters. LLMs, in fact, have stirred up a lot of interest among researchers and practitioners by their impressive skills in natural language processing tasks, which have a profound impact on a wide range of fields. This paper mainly discusses the future applications of LLMs in dentistry. We introduce two primary LLM deployment methods in dentistry, including automated dental diagnosis and cross-modal dental diagnosis, and examine their potential applications. Especially, equipped with a cross-modal encoder, a single LLM can manage multi-source data and conduct advanced natural language reasoning to perform complex clinical operations. A use case is presented to demonstrate the potential of a fully automatic Multi-Modal LLM AI system for dentistry clinical application. While LLMs offer significant p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SPDF&#31639;&#27861;&#26469;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#23494;&#38598;&#24494;&#35843;&#21017;&#21487;&#20197;&#20445;&#35777;&#39640;&#24615;&#33021;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.10464</link><description>&lt;p&gt;
SPDF&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#31232;&#30095;&#39044;&#35757;&#32451;&#21644;&#23494;&#38598;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models. (arXiv:2303.10464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SPDF&#31639;&#27861;&#26469;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#23494;&#38598;&#24494;&#35843;&#21017;&#21487;&#20197;&#20445;&#35777;&#39640;&#24615;&#33021;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#22810;&#39033;&#31361;&#30772;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#35821;&#35328;&#27169;&#22411;&#39318;&#20808;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#36328;&#22495;&#30693;&#35782;&#30340;&#39044;&#35757;&#32451;&#65288;&#20363;&#22914;&#65292;Pile&#12289;MassiveText&#31561;&#65289;&#65292;&#28982;&#21518;&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65288;&#20363;&#22914;&#65292;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12289;&#25991;&#26412;&#25688;&#35201;&#31561;&#65289;&#12290;&#34429;&#28982;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#26377;&#21161;&#20110;&#25552;&#39640;LLM&#24615;&#33021;&#65292;&#20294;&#36825;&#20063;&#24102;&#26469;&#20102;&#26497;&#20026;&#31105;&#27490;&#24615;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#39044;&#35757;&#32451;LLMs&#36890;&#24120;&#38656;&#35201;&#27604;&#24494;&#35843;&#28436;&#20064;&#26356;&#22810;&#30340;FLOPs&#65292;&#20004;&#20010;&#38454;&#27573;&#20043;&#38388;&#30340;&#27169;&#22411;&#23481;&#37327;&#36890;&#24120;&#20445;&#25345;&#19981;&#21464;&#12290;&#20026;&#20102;&#23454;&#29616;&#30456;&#23545;&#20110;&#35757;&#32451;FLOPs&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#20004;&#20010;&#38454;&#27573;&#20043;&#38388;&#35299;&#32806;&#27169;&#22411;&#23481;&#37327;&#65292;&#24182;&#24341;&#20837;&#31232;&#30095;&#39044;&#35757;&#32451;&#21644;&#23494;&#38598;&#24494;&#35843;&#65288;SPDF&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#20165;&#35757;&#32451;&#23376;&#38598;&#26435;&#37325;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pre-training and fine-tuning paradigm has contributed to a number of breakthroughs in Natural Language Processing (NLP). Instead of directly training on a downstream task, language models are first pre-trained on large datasets with cross-domain knowledge (e.g., Pile, MassiveText, etc.) and then fine-tuned on task-specific data (e.g., natural language generation, text summarization, etc.). Scaling the model and dataset size has helped improve the performance of LLMs, but unfortunately, this also leads to highly prohibitive computational costs. Pre-training LLMs often require orders of magnitude more FLOPs than fine-tuning and the model capacity often remains the same between the two phases. To achieve training efficiency w.r.t training FLOPs, we propose to decouple the model capacity between the two phases and introduce Sparse Pre-training and Dense Fine-tuning (SPDF). In this work, we show the benefits of using unstructured weight sparsity to train only a subset of weights during 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26550;&#26500;&#26469;&#35299;&#20915;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#39046;&#22495;&#22806;&#24847;&#22270;&#26816;&#27979;&#21644;&#24847;&#22270;&#21457;&#29616;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#21644;&#21306;&#20998;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#24847;&#22270;&#65292;&#24182;&#21457;&#29616;&#28508;&#34255;&#22312;&#39046;&#22495;&#22806;&#36755;&#20837;&#20013;&#30340;&#19981;&#21516;&#26410;&#30693;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2303.04134</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#39046;&#22495;&#22806;&#24847;&#22270;&#26816;&#27979;&#21644;&#24847;&#22270;&#21457;&#29616;&#30340;&#28151;&#21512;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Architecture for Out of Domain Intent Detection and Intent Discovery. (arXiv:2303.04134v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26550;&#26500;&#26469;&#35299;&#20915;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#39046;&#22495;&#22806;&#24847;&#22270;&#26816;&#27979;&#21644;&#24847;&#22270;&#21457;&#29616;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#21644;&#21306;&#20998;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#24847;&#22270;&#65292;&#24182;&#21457;&#29616;&#28508;&#34255;&#22312;&#39046;&#22495;&#22806;&#36755;&#20837;&#20013;&#30340;&#19981;&#21516;&#26410;&#30693;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22270;&#26816;&#27979;&#26159;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#27169;&#22359;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#39046;&#22495;&#22806;&#65288;OOS&#65289;&#21644;&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#30340;&#36755;&#20837;&#21487;&#33021;&#20250;&#32473;&#36825;&#20123;&#31995;&#32479;&#24102;&#26469;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35757;&#32451;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#24847;&#22270;&#26816;&#27979;&#27169;&#22411;&#38656;&#35201;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;&#21019;&#24314;&#26631;&#35760;&#25968;&#25454;&#38598;&#32791;&#26102;&#19988;&#38656;&#35201;&#20154;&#21147;&#36164;&#28304;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#23558;&#35782;&#21035;OOD/OOS&#36755;&#20837;&#30340;&#20219;&#21153;&#21629;&#21517;&#20026;OOD/OOS&#24847;&#22270;&#26816;&#27979;&#12290;&#21516;&#26102;&#65292;&#21457;&#29616;&#26032;&#30340;&#24847;&#22270;&#24182;&#23545;OOD&#36755;&#20837;&#36827;&#34892;&#20266;&#26631;&#35760;&#65292;&#21017;&#34987;&#31216;&#20026;&#24847;&#22270;&#21457;&#29616;&#12290;&#22312;OOD&#24847;&#22270;&#26816;&#27979;&#37096;&#20998;&#65292;&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#21306;&#20998;&#24050;&#30693;&#24847;&#22270;&#21644;&#26410;&#30693;&#24847;&#22270;&#65292;&#29420;&#31435;&#20110;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#12290;&#20043;&#21518;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#26469;&#21457;&#29616;OOD/OOS&#36755;&#20837;&#20013;&#19981;&#21516;&#30340;&#26410;&#30693;&#24847;&#22270;&#12290;&#25105;&#20204;&#36824;&#23545;OOD/OOS&#34920;&#31034;&#24212;&#29992;&#38750;&#32447;&#24615;&#38477;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intent Detection is one of the tasks of the Natural Language Understanding (NLU) unit in task-oriented dialogue systems. Out of Scope (OOS) and Out of Domain (OOD) inputs may run these systems into a problem. On the other side, a labeled dataset is needed to train a model for Intent Detection in task-oriented dialogue systems. The creation of a labeled dataset is time-consuming and needs human resources. The purpose of this article is to address mentioned problems. The task of identifying OOD/OOS inputs is named OOD/OOS Intent Detection. Also, discovering new intents and pseudo-labeling of OOD inputs is well known by Intent Discovery. In OOD intent detection part, we make use of a Variational Autoencoder to distinguish between known and unknown intents independent of input data distribution. After that, an unsupervised clustering method is used to discover different unknown intents underlying OOD/OOS inputs. We also apply a non-linear dimensionality reduction on OOD/OOS representations
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#26694;&#26550;CALM&#65292;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#26469;&#30772;&#22351;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#35780;&#20272;&#20854;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#23545;&#19981;&#21516;&#34920;&#31034;&#30340;&#20351;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20851;&#31995;&#23646;&#24615;&#30340;&#21033;&#29992;&#23384;&#22312;&#19968;&#23450;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.00333</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Competence-Based Analysis of Language Models. (arXiv:2303.00333v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00333
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#26694;&#26550;CALM&#65292;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#26469;&#30772;&#22351;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#35780;&#20272;&#20854;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#23545;&#19981;&#21516;&#34920;&#31034;&#30340;&#20351;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20851;&#31995;&#23646;&#24615;&#30340;&#21033;&#29992;&#23384;&#22312;&#19968;&#23450;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;&#25552;&#31034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#23545;&#36755;&#20837;&#25110;&#24212;&#29992;&#29615;&#22659;&#20013;&#30340;&#24494;&#23567;&#21464;&#21270;&#21364;&#24322;&#24120;&#33030;&#24369;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31181;&#34892;&#20026;&#24182;&#28608;&#21169;&#35774;&#35745;&#26356;&#20581;&#22766;&#30340;LMs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23454;&#39564;&#26694;&#26550;CALM&#65288;&#22522;&#20110;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#65289;&#65292;&#20854;&#20013;&#21033;&#29992;&#26377;&#38024;&#23545;&#24615;&#30340;&#22240;&#26524;&#24178;&#39044;&#26469;&#30772;&#22351;LM&#22312;&#21508;&#31181;&#35821;&#35328;&#23646;&#24615;&#19978;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#20197;&#35780;&#20272;&#23427;&#22312;&#25191;&#34892;&#32473;&#23450;&#20219;&#21153;&#26102;&#23545;&#27599;&#20010;&#34920;&#31034;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#24178;&#39044;&#23454;&#29616;&#20026;&#22522;&#20110;&#26799;&#24230;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#19982;&#20808;&#21069;&#30340;&#22240;&#26524;&#25506;&#26597;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#20204;&#33021;&#22815;&#38024;&#23545;&#20219;&#24847;&#32534;&#30721;&#30340;&#20851;&#31995;&#23646;&#24615;&#36827;&#34892;&#25915;&#20987;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;BERT-like LMs&#22312;&#25191;&#34892;&#30456;&#20851;&#20851;&#31995;&#25552;&#31034;&#20219;&#21153;&#26102;&#22914;&#20309;&#20351;&#29992;&#22810;&#31181;&#20851;&#31995;&#23646;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#34920;&#31034;&#30340;&#36873;&#25321;&#23545;LM&#30340;&#24615;&#33021;&#20135;&#29983;&#20102;&#24433;&#21709;&#65292;&#20294;&#27169;&#22411;&#23545;&#26576;&#20123;&#29305;&#23450;&#20851;&#31995;&#23646;&#24615;&#30340;&#21033;&#29992;&#24182;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent success of large pretrained language models (LMs) on a variety of prompting tasks, these models can be alarmingly brittle to small changes in inputs or application contexts. To better understand such behavior and motivate the design of more robust LMs, we propose a general experimental framework, CALM (Competence-based Analysis of Language Models), where targeted causal interventions are utilized to damage an LM's internal representation of various linguistic properties in order to evaluate its use of each representation in performing a given task. We implement these interventions as gradient-based adversarial attacks, which (in contrast to prior causal probing methodologies) are able to target arbitrarily-encoded representations of relational properties, and carry out a case study of this approach to analyze how BERT-like LMs use representations of several relational properties in performing associated relation prompting tasks. We find that, while the representation
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20803;&#27169;&#24577;&#28151;&#21512;&#30340;&#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#39044;&#27979;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#31995;&#25968;&#20197;&#36827;&#34892;&#23454;&#20307;&#32423;&#29305;&#24449;&#32858;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;&#30828;&#23454;&#20307;&#37325;&#25773;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#31946;&#23454;&#20307;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#35757;&#32451;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#24182;&#26377;&#25928;&#25552;&#39640;&#20102;MMEA&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.14454</link><description>&lt;p&gt;
MEAformer: &#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#21464;&#21387;&#22120;&#29992;&#20110;&#20803;&#27169;&#24577;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality Hybrid. (arXiv:2212.14454v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14454
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20803;&#27169;&#24577;&#28151;&#21512;&#30340;&#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#39044;&#27979;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#31995;&#25968;&#20197;&#36827;&#34892;&#23454;&#20307;&#32423;&#29305;&#24449;&#32858;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;&#30828;&#23454;&#20307;&#37325;&#25773;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#31946;&#23454;&#20307;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#35757;&#32451;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#24182;&#26377;&#25928;&#25552;&#39640;&#20102;MMEA&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#30340;&#19968;&#20010;&#37325;&#35201;&#21464;&#20307;&#65292;&#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#65288;MMEA&#65289;&#26088;&#22312;&#21457;&#29616;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#20855;&#26377;&#30456;&#20851;&#22270;&#20687;&#30340;&#30456;&#21516;&#23454;&#20307;&#12290; &#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#24403;&#21069;&#30340;MMEA&#31639;&#27861;&#37117;&#20840;&#23616;&#37319;&#29992;KG&#32423;&#27169;&#24577;&#34701;&#21512;&#31574;&#30053;&#36827;&#34892;&#22810;&#27169;&#24335;&#23454;&#20307;&#34920;&#31034;&#65292;&#20294;&#24573;&#30053;&#20102;&#20010;&#20307;&#23454;&#20307;&#30340;&#27169;&#24577;&#20559;&#22909;&#21464;&#21270;&#65292;&#20174;&#32780;&#21066;&#24369;&#20102;&#23545;&#27169;&#24577;&#65288;&#20363;&#22914;&#27169;&#31946;&#22270;&#20687;&#21644;&#20851;&#31995;&#65289;&#20013;&#28508;&#22312;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MEAformer&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#20803;&#27169;&#24577;&#28151;&#21512;&#30340;&#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21160;&#24577;&#39044;&#27979;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#31995;&#25968;&#20197;&#36827;&#34892;&#23454;&#20307;&#32423;&#29305;&#24449;&#32858;&#21512;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;&#30828;&#23454;&#20307;&#37325;&#25773;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#31946;&#23454;&#20307;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#22810;&#20010;&#35757;&#32451;&#22330;&#26223;&#65288;&#21253;&#25324;&#26377;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#12289;&#36845;&#20195;&#21644;&#20302;&#36164;&#28304;&#35774;&#32622;&#65289;&#20013;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#65292;&#32780;&#19988;&#36890;&#36807;&#21033;&#29992;&#27169;&#24577;&#20559;&#22909;&#21464;&#21270;&#26377;&#25928;&#25552;&#39640;&#20102;MMEA&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an important variant of entity alignment (EA), multi-modal entity alignment (MMEA) aims to discover identical entities across different knowledge graphs (KGs) with relevant images attached. We noticed that current MMEA algorithms all globally adopt the KG-level modality fusion strategies for multi-modal entity representation but ignore the variation in modality preferences for individual entities, hurting the robustness to potential noise involved in modalities (e.g., blurry images and relations). In this paper, we present MEAformer, a multi-modal entity alignment transformer approach for meta modality hybrid, which dynamically predicts the mutual correlation coefficients among modalities for entity-level feature aggregation. A modal-aware hard entity replay strategy is further proposed for addressing vague entity details. Experimental results show that our model not only achieves SOTA performance on multiple training scenarios including supervised, unsupervised, iterative, and low 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21512;&#25104;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#26469;&#22686;&#24378;&#20219;&#21153;&#26426;&#22120;&#20154;&#30340;&#21442;&#19982;&#24230;&#30340;&#26694;&#26550;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#32479;&#19968;&#27169;&#22411;PivotBot&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#20999;&#25442;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#21644;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#65292;&#22312;&#22788;&#29702;&#34701;&#21512;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.10008</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#26469;&#22686;&#24378;&#20219;&#21153;&#26426;&#22120;&#20154;&#30340;&#21442;&#19982;&#24230;
&lt;/p&gt;
&lt;p&gt;
Enhancing Task Bot Engagement with Synthesized Open-Domain Dialog. (arXiv:2212.10008v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21512;&#25104;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#26469;&#22686;&#24378;&#20219;&#21153;&#26426;&#22120;&#20154;&#30340;&#21442;&#19982;&#24230;&#30340;&#26694;&#26550;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#32479;&#19968;&#27169;&#22411;PivotBot&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#20999;&#25442;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#21644;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#65292;&#22312;&#22788;&#29702;&#34701;&#21512;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#36827;&#34892;&#20102;&#35768;&#22810;&#21162;&#21147;&#26469;&#26500;&#24314;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#23545;&#35805;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#22914;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#21644;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#65288;ODD&#65289;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#27169;&#20223;&#20154;&#31867;&#32423;&#21035;&#30340;&#23545;&#35805;&#65292;&#36890;&#24120;&#38656;&#35201;&#34701;&#21512;&#21508;&#31181;&#23545;&#35805;&#27169;&#24335;&#65292;&#24182;&#24314;&#31435;&#19968;&#20010;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;TOD&#21644;ODD&#65292;&#24182;&#35775;&#38382;&#19981;&#21516;&#30693;&#35782;&#28304;&#30340;&#31995;&#32479;&#12290;&#20026;&#20102;&#35299;&#20915;&#34701;&#21512;&#20219;&#21153;&#32570;&#20047;&#21487;&#29992;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#32467;&#21512;&#20102;&#30693;&#35782;&#20026;&#22522;&#30784;&#30340;ODD&#21644;TOD&#30340;&#23545;&#35805;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;PivotBot&#65292;&#33021;&#22815;&#36866;&#24403;&#22320;&#37319;&#29992;TOD&#21644;ODD&#27169;&#24335;&#65292;&#24182;&#35775;&#38382;&#19981;&#21516;&#30340;&#30693;&#35782;&#28304;&#65292;&#20197;&#20415;&#26377;&#25928;&#22320;&#22788;&#29702;&#34701;&#21512;&#20219;&#21153;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;TOD&#21644;ODD&#20219;&#21153;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#30340;&#33021;&#21147;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many efforts have been made to construct dialog systems for different types of conversations, such as task-oriented dialog (TOD) and open-domain dialog (ODD). To better mimic human-level conversations that usually fuse various dialog modes, it is essential to build a system that can effectively handle both TOD and ODD and access different knowledge sources. To address the lack of available data for the fused task, we propose a framework for automatically generating dialogues that combine knowledge-grounded ODDs and TODs in various settings. Additionally, we introduce a unified model PivotBot that is capable of appropriately adopting TOD and ODD modes and accessing different knowledge sources in order to effectively tackle the fused task. Evaluation results demonstrate the superior ability of the proposed model to switch seamlessly between TOD and ODD tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#32500;&#24230;&#21270;&#35780;&#20272;&#20154;&#26426;&#32842;&#22825;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#19968;&#32452;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20026;&#26410;&#26469;&#32842;&#22825;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2212.09180</link><description>&lt;p&gt;
&#21035;&#24536;&#20102;&#20320;&#30340;ABC&#65306;&#35780;&#20272;&#32842;&#22825;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems. (arXiv:2212.09180v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#32500;&#24230;&#21270;&#35780;&#20272;&#20154;&#26426;&#32842;&#22825;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#19968;&#32452;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20026;&#26410;&#26469;&#32842;&#22825;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#32842;&#22825;&#20132;&#20114;&#39046;&#22495;&#36817;&#26469;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#36866;&#24403;&#30340;&#35780;&#20272;&#20173;&#38656;&#35201;&#20154;&#31867;&#20027;&#35266;&#21028;&#26029;&#65292;&#22240;&#27492;&#35780;&#27979;&#25351;&#26631;&#26131;&#20986;&#29616;&#39640;&#26041;&#24046;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#35780;&#20272;&#26041;&#27861;&#21644;&#26631;&#20934;&#32570;&#20047;&#35268;&#33539;&#24615;&#65292;&#32570;&#20047;&#29992;&#20110;&#35780;&#20272;&#26377;&#25928;&#24615;&#30340;&#24037;&#20316;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#32467;&#26524;&#21487;&#33021;&#26080;&#27861;&#23436;&#25972;&#21453;&#26144;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20248;&#28857;&#21644;&#32570;&#38519;&#12290;&#25105;&#20204;&#26088;&#22312;&#23454;&#29616;&#23545;&#20154;&#26426;&#32842;&#22825;&#30340;&#32500;&#24230;&#21270;&#35780;&#20272;&#65292;&#21487;&#21487;&#38752;&#22320;&#27979;&#37327;&#32842;&#22825;&#36136;&#37327;&#30340;&#20960;&#20010;&#19981;&#21516;&#26041;&#38754;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#65292;&#37327;&#21270;&#20102;&#20960;&#31181;&#19982;&#36136;&#37327;&#30456;&#20851;&#30340;&#26426;&#22120;&#20154;&#32842;&#22825;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#26367;&#20195;&#30340;Likert-style&#25110;&#27604;&#36739;&#26041;&#27861;&#26356;&#36866;&#21512;&#35780;&#20272;&#32500;&#24230;&#21270;&#32842;&#22825;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#39564;&#35777;&#30340;&#26041;&#27861;&#21644;&#29616;&#26377;&#26041;&#27861;&#26469;&#35780;&#20272;&#19968;&#32452;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#24182;&#20840;&#38754;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;&#20960;&#20010;&#36136;&#37327;&#32500;&#24230;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#29616;&#26377;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#25345;&#32493;&#20248;&#21183;&#21644;&#38480;&#21046;&#65292;&#24182;&#20026;&#32842;&#22825;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been great recent advancement in human-computer chat. However, proper evaluation currently requires human judgements that produce notoriously high-variance metrics due to their inherent subjectivity. Furthermore, there is little standardization in the methods and labels used for evaluation, with an overall lack of work to compare and assess the validity of various evaluation approaches. As a consequence, existing evaluation results likely leave an incomplete picture of the strengths and weaknesses of open-domain chatbots. We aim towards a dimensional evaluation of human-computer chat that can reliably measure several distinct aspects of chat quality. To this end, we present our novel human evaluation method that quantifies the rate of several quality-related chatbot behaviors. Our results demonstrate our method to be more suitable for dimensional chat evaluation than alternative likert-style or comparative methods. We then use our validated method and existing methods to eval
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26500;&#24314;&#20102;&#23545;&#35805;&#31995;&#32479;&#22122;&#38899;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#19981;&#21516;&#22122;&#38899;&#19979;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#26174;&#31034;&#27169;&#22411;&#23545;&#26631;&#31614;&#38169;&#35823;&#40065;&#26834;&#24615;&#36739;&#39640;&#65292;&#20294;&#22312;&#23545;&#35805;&#29305;&#23450;&#22122;&#38899;&#19979;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#20250;&#35805;&#29615;&#22659;&#30340;&#25968;&#25454;&#28165;&#27927;&#31639;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#38024;&#23545;&#24615;&#30340;&#23545;&#35805;&#21435;&#22122;&#27010;&#24565;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2212.02745</link><description>&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#22122;&#38899;&#26469;&#28304;&#21450;&#20854;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sources of Noise in Dialogue and How to Deal with Them. (arXiv:2212.02745v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#23545;&#35805;&#31995;&#32479;&#22122;&#38899;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#19981;&#21516;&#22122;&#38899;&#19979;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#26174;&#31034;&#27169;&#22411;&#23545;&#26631;&#31614;&#38169;&#35823;&#40065;&#26834;&#24615;&#36739;&#39640;&#65292;&#20294;&#22312;&#23545;&#35805;&#29305;&#23450;&#22122;&#38899;&#19979;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#20250;&#35805;&#29615;&#22659;&#30340;&#25968;&#25454;&#28165;&#27927;&#31639;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#38024;&#23545;&#24615;&#30340;&#23545;&#35805;&#21435;&#22122;&#27010;&#24565;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#23545;&#35805;&#31995;&#32479;&#24120;&#24120;&#38656;&#35201;&#22788;&#29702;&#22122;&#38899;&#24178;&#25200;&#21644;&#24847;&#22806;&#29992;&#25143;&#36755;&#20837;&#12290;&#23613;&#31649;&#36825;&#20123;&#38382;&#39064;&#24456;&#26222;&#36941;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#23545;&#23545;&#35805;&#22122;&#38899;&#30340;&#20934;&#30830;&#35843;&#26597;&#65292;&#20063;&#27809;&#26377;&#23545;&#27599;&#31181;&#22122;&#38899;&#31867;&#22411;&#23545;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#26377;&#28165;&#26224;&#30340;&#35748;&#35782;&#12290;&#26412;&#25991;&#39318;&#20808;&#26500;&#24314;&#20102;&#23545;&#35805;&#31995;&#32479;&#36935;&#21040;&#30340;&#22122;&#38899;&#20998;&#31867;&#20307;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#19981;&#21516;&#31243;&#24230;&#21644;&#31867;&#22411;&#30340;&#22122;&#38899;&#19979;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#23545;&#24120;&#35265;&#26631;&#31614;&#38169;&#35823;&#38750;&#24120;&#40065;&#26834;&#65292;&#20294;&#22312;&#23545;&#35805;&#29305;&#23450;&#22122;&#38899;&#19979;&#24615;&#33021;&#19979;&#38477;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#20250;&#35805;&#29615;&#22659;&#30340;&#25968;&#25454;&#28165;&#27927;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#26377;&#38024;&#23545;&#24615;&#30340;&#23545;&#35805;&#21435;&#22122;&#27010;&#24565;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training dialogue systems often entails dealing with noisy training examples and unexpected user inputs. Despite their prevalence, there currently lacks an accurate survey of dialogue noise, nor is there a clear sense of the impact of each noise type on task performance. This paper addresses this gap by first constructing a taxonomy of noise encountered by dialogue systems. In addition, we run a series of experiments to show how different models behave when subjected to varying levels of noise and types of noise. Our results reveal that models are quite robust to label errors commonly tackled by existing denoising algorithms, but that performance suffers from dialogue-specific noise. Driven by these observations, we design a data cleaning algorithm specialized for conversational settings and apply it as a proof-of-concept for targeted dialogue denoising.
&lt;/p&gt;</description></item><item><title>X$^2$-VLM&#26159;&#19968;&#20010;&#20840;&#33021;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#32479;&#19968;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#22810;&#31890;&#24230;&#30340;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#21644;&#23450;&#20301;&#65292;&#24182;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#32479;&#19968;&#20102;&#22270;&#20687;-&#25991;&#26412;&#21644;&#35270;&#39057;-&#25991;&#26412;&#39044;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;X$^2$-VLM&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#22909;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2211.12402</link><description>&lt;p&gt;
X$^2$-VLM: &#20840;&#33021;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks. (arXiv:2211.12402v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12402
&lt;/p&gt;
&lt;p&gt;
X$^2$-VLM&#26159;&#19968;&#20010;&#20840;&#33021;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#32479;&#19968;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#22810;&#31890;&#24230;&#30340;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#21644;&#23450;&#20301;&#65292;&#24182;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#32479;&#19968;&#20102;&#22270;&#20687;-&#25991;&#26412;&#21644;&#35270;&#39057;-&#25991;&#26412;&#39044;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;X$^2$-VLM&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#22909;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#26088;&#22312;&#20174;&#22823;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#23398;&#20064;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#12290;&#20854;&#20182;&#19968;&#20123;&#26041;&#27861;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#22312;&#23545;&#35937;&#32423;&#21035;&#19978;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21516;&#26102;&#23398;&#20064;&#22810;&#31890;&#24230;&#30340;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#21644;&#22810;&#31890;&#24230;&#30340;&#23450;&#20301;&#65292;&#20174;&#32780;&#23454;&#29616;&#22810;&#31890;&#24230;&#30340;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#33021;&#27169;&#22411;X$^2$-VLM&#65292;&#20855;&#26377;&#28789;&#27963;&#30340;&#27169;&#22359;&#21270;&#26550;&#26500;&#65292;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#36827;&#19968;&#27493;&#32479;&#19968;&#20102;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#21644;&#35270;&#39057;-&#25991;&#26412;&#39044;&#35757;&#32451;&#12290;X$^2$-VLM&#33021;&#22815;&#23398;&#20064;&#19982;&#22810;&#26679;&#30340;&#25991;&#26412;&#25551;&#36848;&#30456;&#20851;&#30340;&#26080;&#38480;&#35270;&#35273;&#27010;&#24565;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;X$^2$-VLM&#22312;&#22270;&#20687;-&#25991;&#26412;&#21644;&#35270;&#39057;-&#25991;&#26412;&#20219;&#21153;&#30340;&#22522;&#30784;&#21644;&#22823;&#35268;&#27169;&#19978;&#34920;&#29616;&#26368;&#22909;&#65292;&#22312;&#24615;&#33021;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;X$^2$-VLM&#30340;&#27169;&#22359;&#21270;&#35774;&#35745;&#23548;&#33268;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Vision language pre-training aims to learn alignments between vision and language from a large amount of data. Most existing methods only learn image-text alignments. Some others utilize pre-trained object detectors to leverage vision language alignments at the object level. In this paper, we propose to learn multi-grained vision language alignments by a unified pre-training framework that learns multi-grained aligning and multi-grained localization simultaneously. Based on it, we present X$^2$-VLM, an all-in-one model with a flexible modular architecture, in which we further unify image-text pre-training and video-text pre-training in one model. X$^2$-VLM is able to learn unlimited visual concepts associated with diverse text descriptions. Experiment results show that X$^2$-VLM performs the best on base and large scale for both image-text and video-text tasks, making a good trade-off between performance and model scale. Moreover, we show that the modular design of X$^2$-VLM results in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;BERT-based DR&#36981;&#24490;&#38750;&#22343;&#21248;&#20998;&#24067;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27491;&#21017;&#21270;&#27969;&#21644;&#30333;&#21270;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#21333;&#35789;&#32423;&#26041;&#27861;&#26469;&#24212;&#29992;&#36825;&#20123;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22686;&#24378;&#34920;&#31034;&#30340;&#21516;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2209.00218</link><description>&lt;p&gt;
&#21516;&#24615;&#36136;&#30340;&#34920;&#31034;&#21487;&#20197;&#25913;&#21892;&#23494;&#38598;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Isotropic Representation Can Improve Dense Retrieval. (arXiv:2209.00218v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;BERT-based DR&#36981;&#24490;&#38750;&#22343;&#21248;&#20998;&#24067;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27491;&#21017;&#21270;&#27969;&#21644;&#30333;&#21270;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#21333;&#35789;&#32423;&#26041;&#27861;&#26469;&#24212;&#29992;&#36825;&#20123;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22686;&#24378;&#34920;&#31034;&#30340;&#21516;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#34920;&#31034;&#24314;&#27169;&#30340;&#36827;&#23637;&#24191;&#27867;&#24433;&#21709;&#20102;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#35774;&#35745;&#12290;&#29305;&#21035;&#26159;&#65292;&#35768;&#22810;&#39640;&#24615;&#33021;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#20351;&#29992;BERT&#35780;&#20272;&#26597;&#35810;&#21644;&#25991;&#26723;&#30340;&#34920;&#31034;&#65292;&#24182;&#38543;&#21518;&#24212;&#29992;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#35780;&#20998;&#26469;&#30830;&#23450;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;BERT&#34920;&#31034;&#36981;&#24490;&#19968;&#20010;&#29421;&#31364;&#38181;&#24418;&#30340;&#38750;&#22343;&#21248;&#20998;&#24067;&#65292;&#36825;&#31181;&#38750;&#22343;&#21248;&#20998;&#24067;&#23545;&#20110;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#35780;&#20998;&#21487;&#33021;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22522;&#20110;BERT&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#20063;&#36981;&#24490;&#38750;&#22343;&#21248;&#20998;&#24067;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26080;&#30417;&#30563;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65306;&#27491;&#21017;&#21270;&#27969;&#21644;&#30333;&#21270;&#65292;&#24182;&#24320;&#21457;&#20102;&#21333;&#35789;&#32423;&#26041;&#27861;&#26469;&#23558;&#36825;&#20123;&#21518;&#22788;&#29702;&#26041;&#27861;&#24212;&#29992;&#21040;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#34920;&#31034;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22686;&#24378;&#34920;&#31034;&#30340;&#21516;&#24615;&#36136;&#65292;&#28982;&#21518;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancement in language representation modeling has broadly affected the design of dense retrieval models. In particular, many of the high-performing dense retrieval models evaluate representations of query and document using BERT, and subsequently apply a cosine-similarity based scoring to determine the relevance. BERT representations, however, are known to follow an anisotropic distribution of a narrow cone shape and such an anisotropic distribution can be undesirable for the cosine-similarity based scoring. In this work, we first show that BERT-based DR also follows an anisotropic distribution. To cope with the problem, we introduce unsupervised post-processing methods of Normalizing Flow and whitening, and develop token-wise method in addition to the sequence-wise method for applying the post-processing methods to the representations of dense retrieval models. We show that the proposed methods can effectively enhance the representations to be isotropic, then we perform e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#21644;&#20266;&#26631;&#31614;&#26041;&#27861;&#26469;&#25552;&#21319;&#36328;&#39046;&#22495;&#35821;&#38899;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.09783</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#25552;&#21319;&#36328;&#39046;&#22495;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Boosting Cross-Domain Speech Recognition with Self-Supervision. (arXiv:2206.09783v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#21644;&#20266;&#26631;&#31614;&#26041;&#27861;&#26469;&#25552;&#21319;&#36328;&#39046;&#22495;&#35821;&#38899;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#36328;&#39046;&#22495;&#24615;&#33021;&#21487;&#33021;&#21463;&#21040;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#30340;&#20005;&#37325;&#38459;&#30861;&#12290;&#30001;&#20110;&#30446;&#26631;&#39046;&#22495;&#36890;&#24120;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#19988;&#22768;&#23398;&#21644;&#35821;&#35328;&#23618;&#38754;&#23384;&#22312;&#39046;&#22495;&#36716;&#31227;&#65292;&#22240;&#27492;&#23545;ASR&#36827;&#34892;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#25110;&#20266;&#26631;&#31614;&#65288;PL&#65289;&#65292;&#21487;&#20197;&#22312;UDA&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#33258;&#30417;&#30563;&#20063;&#38754;&#20020;&#30528;&#19981;&#21305;&#37197;&#39046;&#22495;&#20998;&#24067;&#30340;&#24615;&#33021;&#38477;&#20302;&#38382;&#39064;&#65292;&#36825;&#26159;&#20043;&#21069;&#30340;&#30740;&#31350;&#27809;&#26377;&#35299;&#20915;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;UDA&#26694;&#26550;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#20013;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#37325;&#25918;&#25216;&#26415;&#26469;&#20943;&#36731;SSL&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39046;&#22495;&#19981;&#21305;&#37197;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cross-domain performance of automatic speech recognition (ASR) could be severely hampered due to the mismatch between training and testing distributions. Since the target domain usually lacks labeled data, and domain shifts exist at acoustic and linguistic levels, it is challenging to perform unsupervised domain adaptation (UDA) for ASR. Previous work has shown that self-supervised learning (SSL) or pseudo-labeling (PL) is effective in UDA by exploiting the self-supervisions of unlabeled data. However, these self-supervisions also face performance degradation in mismatched domain distributions, which previous work fails to address. This work presents a systematic UDA framework to fully utilize the unlabeled data with self-supervision in the pre-training and fine-tuning paradigm. On the one hand, we apply continued pre-training and data replay techniques to mitigate the domain mismatch of the SSL pre-trained model. On the other hand, we propose a domain-adaptive fine-tuning approach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20351;&#29992;&#30340;&#25506;&#27979;&#35774;&#32622;&#65292;&#36890;&#36807;&#24178;&#39044;&#27169;&#22411;&#30340;&#34920;&#31034;&#26469;&#21435;&#38500;&#23646;&#24615;&#65292;&#20174;&#32780;&#21457;&#29616;&#27169;&#22411;&#23454;&#38469;&#20351;&#29992;&#30340;&#32534;&#30721;&#12290;&#20197;BERT&#22914;&#20309;&#32534;&#30721;&#35821;&#27861;&#25968;&#20026;&#20363;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;BERT&#20381;&#36182;&#20110;&#35821;&#27861;&#25968;&#30340;&#32447;&#24615;&#32534;&#30721;&#26469;&#20135;&#29983;&#27491;&#30830;&#30340;&#34892;&#20026;&#36755;&#20986;&#65292;&#24182;&#23545;&#21517;&#35789;&#21644;&#21160;&#35789;&#30340;&#35821;&#27861;&#25968;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2204.08831</link><description>&lt;p&gt;
&#25506;&#31350;&#35821;&#27861;&#25968;&#30340;&#20351;&#29992;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Probing for the Usage of Grammatical Number. (arXiv:2204.08831v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20351;&#29992;&#30340;&#25506;&#27979;&#35774;&#32622;&#65292;&#36890;&#36807;&#24178;&#39044;&#27169;&#22411;&#30340;&#34920;&#31034;&#26469;&#21435;&#38500;&#23646;&#24615;&#65292;&#20174;&#32780;&#21457;&#29616;&#27169;&#22411;&#23454;&#38469;&#20351;&#29992;&#30340;&#32534;&#30721;&#12290;&#20197;BERT&#22914;&#20309;&#32534;&#30721;&#35821;&#27861;&#25968;&#20026;&#20363;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;BERT&#20381;&#36182;&#20110;&#35821;&#27861;&#25968;&#30340;&#32447;&#24615;&#32534;&#30721;&#26469;&#20135;&#29983;&#27491;&#30830;&#30340;&#34892;&#20026;&#36755;&#20986;&#65292;&#24182;&#23545;&#21517;&#35789;&#21644;&#21160;&#35789;&#30340;&#35821;&#27861;&#25968;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#30340;&#26680;&#24515;&#38382;&#39064;&#26159;&#25581;&#31034;&#39044;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#22312;&#20854;&#34920;&#31034;&#20013;&#32534;&#30721;&#35821;&#35328;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#32534;&#30721;&#21487;&#33021;&#26159;&#34394;&#20551;&#30340;&#65292;&#21363;&#27169;&#22411;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#21487;&#33021;&#19981;&#20381;&#36182;&#20110;&#23427;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#23547;&#25214;&#27169;&#22411;&#23454;&#38469;&#20351;&#29992;&#30340;&#32534;&#30721;&#65292;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#20351;&#29992;&#30340;&#25506;&#27979;&#35774;&#32622;&#12290;&#25105;&#20204;&#39318;&#20808;&#36873;&#25321;&#19968;&#20010;&#34892;&#20026;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#22312;&#19981;&#20351;&#29992;&#35821;&#35328;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#35299;&#20915;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#24178;&#39044;&#27169;&#22411;&#30340;&#34920;&#31034;&#26469;&#21435;&#38500;&#23646;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#27169;&#22411;&#20351;&#29992;&#20102;&#26576;&#31181;&#32534;&#30721;&#65292;&#21435;&#38500;&#35813;&#32534;&#30721;&#24212;&#35813;&#20250;&#25439;&#23475;&#25152;&#36873;&#25321;&#30340;&#34892;&#20026;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20197;BERT&#22914;&#20309;&#32534;&#30721;&#35821;&#27861;&#25968;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#35813;&#32534;&#30721;&#35299;&#20915;&#25968;&#30340;&#19968;&#33268;&#24615;&#20219;&#21153;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;BERT&#20381;&#36182;&#20110;&#35821;&#27861;&#25968;&#30340;&#32447;&#24615;&#32534;&#30721;&#26469;&#20135;&#29983;&#27491;&#30830;&#30340;&#34892;&#20026;&#36755;&#20986;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;BERT&#23545;&#21517;&#35789;&#21644;&#21160;&#35789;&#30340;&#35821;&#27861;&#25968;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central quest of probing is to uncover how pre-trained models encode a linguistic property within their representations. An encoding, however, might be spurious-i.e., the model might not rely on it when making predictions. In this paper, we try to find encodings that the model actually uses, introducing a usage-based probing setup. We first choose a behavioral task which cannot be solved without using the linguistic property. Then, we attempt to remove the property by intervening on the model's representations. We contend that, if an encoding is used by the model, its removal should harm the performance on the chosen behavioral task. As a case study, we focus on how BERT encodes grammatical number, and on how it uses this encoding to solve the number agreement task. Experimentally, we find that BERT relies on a linear encoding of grammatical number to produce the correct behavioral output. We also find that BERT uses a separate encoding of grammatical number for nouns and verbs. Fina
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#25226;&#20351;&#29992;&#19981;&#21516;&#20070;&#20889;&#31995;&#32479;&#30340;&#30456;&#36817;&#35821;&#35328;&#38899;&#35793;&#25104;&#21516;&#19968;&#31181;&#20070;&#20889;&#31995;&#32479;&#65292;&#23545;&#20110;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21319;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#38899;&#35793;&#21487;&#20197;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#34920;&#29616;&#65292;&#32780;&#19981;&#20250;&#23545;&#36164;&#28304;&#30456;&#23545;&#36739;&#39640;&#30340;&#35821;&#35328;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2201.12501</link><description>&lt;p&gt;
&#32763;&#35793;&#65306;&#27721;&#35821;&#25340;&#38899;&#26159;&#21542;&#26377;&#21161;&#20110;&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Transliteration Help Multilingual Language Modeling?. (arXiv:2201.12501v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#25226;&#20351;&#29992;&#19981;&#21516;&#20070;&#20889;&#31995;&#32479;&#30340;&#30456;&#36817;&#35821;&#35328;&#38899;&#35793;&#25104;&#21516;&#19968;&#31181;&#20070;&#20889;&#31995;&#32479;&#65292;&#23545;&#20110;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21319;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#38899;&#35793;&#21487;&#20197;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#34920;&#29616;&#65292;&#32780;&#19981;&#20250;&#23545;&#36164;&#28304;&#30456;&#23545;&#36739;&#39640;&#30340;&#35821;&#35328;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#37096;&#20998;&#35821;&#35328;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;&#20195;&#34920;&#24615;&#35821;&#26009;&#24211;&#65292;&#23545;&#20110;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26469;&#35828;&#65292;&#20174;&#29616;&#26377;&#30340;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#19981;&#21516;&#35821;&#35328;&#30340;&#25991;&#26412;&#34920;&#29616;&#24418;&#24335;&#30340;&#22810;&#26679;&#24615;&#20351;&#24471;MLLM&#38754;&#20020;&#22256;&#38590;&#65292;&#22240;&#20026;&#30456;&#36817;&#30340;&#35821;&#35328;&#20043;&#38388;&#35789;&#27719;&#37325;&#21472;&#36739;&#23569;&#12290;&#22240;&#27492;&#65292;&#25226;&#20351;&#29992;&#19981;&#21516;&#20070;&#20889;&#31995;&#32479;&#30340;&#30456;&#36817;&#30340;&#35821;&#35328;&#38899;&#35793;&#25104;&#21516;&#19968;&#31181;&#20070;&#20889;&#31995;&#32479;&#21487;&#20197;&#25552;&#39640;MLLM&#30340;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39044;&#35757;&#32451;&#20004;&#20010;ALBERT&#27169;&#22411;&#65292;&#20197;&#23454;&#35777;&#30340;&#26041;&#24335;&#27979;&#37327;&#38899;&#35793;&#23545;MLLM&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#21360;&#24230;&#35821;-&#38597;&#21033;&#23433;&#35821;&#31995;&#65292;&#35813;&#31995;&#22312;&#19990;&#30028;&#19978;&#25317;&#26377;&#26368;&#39640;&#30340;&#20070;&#20889;&#31995;&#32479;&#22810;&#26679;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;IndicGLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#26364;&#65293;&#24800;&#29305;&#23612;U&#26816;&#39564;&#65292;&#20197;&#20005;&#26684;&#39564;&#35777;&#38899;&#35793;&#30340;&#25928;&#26524;&#26159;&#21542;&#26174;&#33879;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38899;&#35793;&#26377;&#21033;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#32780;&#19981;&#20250;&#23545;&#36164;&#28304;&#30456;&#23545;&#36739;&#39640;&#30340;&#35821;&#35328;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
As there is a scarcity of large representative corpora for most languages, it is important for Multilingual Language Models (MLLM) to extract the most out of existing corpora. In this regard, script diversity presents a challenge to MLLMs by reducing lexical overlap among closely related languages. Therefore, transliterating closely related languages that use different writing scripts to a common script may improve the downstream task performance of MLLMs. In this paper, we pretrain two ALBERT models to empirically measure the effect of transliteration on MLLMs. We specifically focus on the Indo-Aryan language family, which has the highest script diversity in the world. Afterward, we evaluate our models on the IndicGLUE benchmark. We perform Mann-Whitney U test to rigorously verify whether the effect of transliteration is significant or not. We find that transliteration benefits the low-resource languages without negatively affecting the comparatively high-resource languages. We also m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;kNN-KGE&#65292;&#23427;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26368;&#36817;&#37051;&#30340;&#32447;&#24615;&#25554;&#20540;&#65292;&#20801;&#35768;&#32597;&#35265;&#25110;&#26032;&#20986;&#29616;&#30340;&#23454;&#20307;&#34987;&#26126;&#30830;&#22320;&#35760;&#24518;&#65292;&#32780;&#19981;&#26159;&#38544;&#34255;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#38142;&#25509;&#39044;&#27979;&#32467;&#26524;&#24182;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2201.05575</link><description>&lt;p&gt;
&#36890;&#36807;&#35760;&#24518;&#25512;&#29702;&#65306;&#26368;&#36817;&#37051;&#30693;&#35782;&#22270;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings. (arXiv:2201.05575v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;kNN-KGE&#65292;&#23427;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26368;&#36817;&#37051;&#30340;&#32447;&#24615;&#25554;&#20540;&#65292;&#20801;&#35768;&#32597;&#35265;&#25110;&#26032;&#20986;&#29616;&#30340;&#23454;&#20307;&#34987;&#26126;&#30830;&#22320;&#35760;&#24518;&#65292;&#32780;&#19981;&#26159;&#38544;&#34255;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#38142;&#25509;&#39044;&#27979;&#32467;&#26524;&#24182;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#36890;&#24120;&#23558;&#23454;&#20307;&#26144;&#23556;&#21040;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#35780;&#20998;&#20989;&#25968;&#39044;&#27979;&#30446;&#26631;&#23454;&#20307;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38590;&#20197;&#25512;&#29702;&#20986;&#32597;&#35265;&#25110;&#26032;&#20986;&#29616;&#30340;&#26410;&#30693;&#23454;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;kNN-KGE&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#23558;&#20854;&#23454;&#20307;&#20998;&#24067;&#19982;k&#20010;&#26368;&#36817;&#37051;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#26681;&#25454;&#30693;&#35782;&#23384;&#20648;&#20013;&#23454;&#20307;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#36317;&#31163;&#35745;&#31639;&#26368;&#36817;&#37051;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26126;&#30830;&#22320;&#35760;&#24518;&#32597;&#35265;&#25110;&#26032;&#20986;&#29616;&#30340;&#23454;&#20307;&#65292;&#32780;&#19981;&#26159;&#38544;&#34255;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#24402;&#32435;&#21644;&#20256;&#36882;&#24335;&#38142;&#25509;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#22312;&#21482;&#26377;&#23569;&#37327;&#19977;&#20803;&#32452;&#30340;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#36825;&#21487;&#33021;&#26356;&#23481;&#26131;&#36890;&#36807;&#26126;&#30830;&#30340;&#35760;&#24518;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous knowledge graph embedding approaches usually map entities to representations and utilize score functions to predict the target entities, yet they typically struggle to reason rare or emerging unseen entities. In this paper, we propose kNN-KGE, a new knowledge graph embedding approach with pre-trained language models, by linearly interpolating its entity distribution with k-nearest neighbors. We compute the nearest neighbors based on the distance in the entity embedding space from the knowledge store. Our approach can allow rare or emerging entities to be memorized explicitly rather than implicitly in model parameters. Experimental results demonstrate that our approach can improve inductive and transductive link prediction results and yield better performance for low-resource settings with only a few triples, which might be easier to reason via explicit memory. Code is available at https://github.com/zjunlp/KNN-KG.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#26597;&#35810;&#32858;&#28966;&#25688;&#35201;&#29983;&#25104;&#20219;&#21153;&#20316;&#20026;&#19968;&#39033;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;(KI-QFS)&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26080;&#27861;&#30452;&#25509;&#35775;&#38382;&#30456;&#20851;&#25991;&#26723;&#30340;&#24773;&#20917;&#19979;&#65292;QFS&#27169;&#22411;&#22312;KI-QFS&#19978;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;</title><link>http://arxiv.org/abs/2112.07536</link><description>&lt;p&gt;
&#23558;&#38754;&#21521;&#26597;&#35810;&#30340;&#25688;&#35201;&#29983;&#25104;&#20316;&#20026;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#36827;&#34892;&#22788;&#29702;&#65306;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tackling Query-Focused Summarization as A Knowledge-Intensive Task: A Pilot Study. (arXiv:2112.07536v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.07536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#26597;&#35810;&#32858;&#28966;&#25688;&#35201;&#29983;&#25104;&#20219;&#21153;&#20316;&#20026;&#19968;&#39033;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;(KI-QFS)&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26080;&#27861;&#30452;&#25509;&#35775;&#38382;&#30456;&#20851;&#25991;&#26723;&#30340;&#24773;&#20917;&#19979;&#65292;QFS&#27169;&#22411;&#22312;KI-QFS&#19978;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#32858;&#28966;&#25688;&#35201;(QFS)&#38656;&#35201;&#22312;&#32473;&#23450;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#30456;&#20851;&#25991;&#26723;&#38598;&#21512;&#29983;&#25104;&#25688;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#36825;&#20123;&#30456;&#20851;&#25991;&#26723;&#38656;&#35201;&#25163;&#21160;&#36827;&#34892;&#27880;&#37322;&#65292;&#22240;&#27492;&#19981;&#19968;&#23450;&#33021;&#21363;&#26102;&#33719;&#24471;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#23558;QFS&#20219;&#21153;&#20316;&#20026;&#19968;&#39033;&#30693;&#35782;&#23494;&#38598;&#22411;(KI)&#20219;&#21153;&#36827;&#34892;&#22788;&#29702;&#65292;&#19981;&#35201;&#27714;&#30452;&#25509;&#33719;&#21462;&#30456;&#20851;&#25991;&#26723;&#65292;&#32780;&#26159;&#20551;&#23450;&#36825;&#20123;&#25991;&#26723;&#23384;&#22312;&#20110;&#22823;&#35268;&#27169;&#30693;&#35782;&#35821;&#26009;&#24211;&#20013;&#65292;&#24182;&#38656;&#35201;&#39318;&#20808;&#36827;&#34892;&#26816;&#32034;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#20010;&#26032;&#30340;&#35774;&#23450;&#65292;&#25105;&#20204;&#36890;&#36807;&#25913;&#32534;&#29616;&#26377;&#30340;QFS&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;(KI-QFS)&#12290;&#22312;&#35813;&#25968;&#25454;&#38598;&#20013;&#65292;&#22238;&#31572;&#26597;&#35810;&#38656;&#35201;&#20174;&#30693;&#35782;&#35821;&#26009;&#24211;&#26816;&#32034;&#25991;&#26723;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#30693;&#35782;&#35821;&#26009;&#24211;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20379;&#30456;&#20851;&#24615;&#27880;&#37322;&#20197;&#36827;&#34892;&#26816;&#32034;&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;QFS&#27169;&#22411;&#21644;&#22686;&#24378;&#30340;&#26816;&#32034;&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#21407;&#22987;QFS&#30456;&#27604;&#65292;QFS&#27169;&#22411;&#22312;KI-QFS&#19978;&#34920;&#29616;&#26174;&#33879;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query-focused summarization (QFS) requires generating a summary given a query using a set of relevant documents. However, such relevant documents should be annotated manually and thus are not readily available in realistic scenarios. To address this limitation, we tackle the QFS task as a knowledge-intensive (KI) task without access to any relevant documents. Instead, we assume that these documents are present in a large-scale knowledge corpus and should be retrieved first. To explore this new setting, we build a new dataset (KI-QFS) by adapting existing QFS datasets. In this dataset, answering the query requires document retrieval from a knowledge corpus. We construct three different knowledge corpora, and we further provide relevance annotations to enable retrieval evaluation. Finally, we benchmark the dataset with state-of-the-art QFS models and retrieval-enhanced models. The experimental results demonstrate that QFS models perform significantly worse on KI-QFS compared to the origi
&lt;/p&gt;</description></item></channel></rss>