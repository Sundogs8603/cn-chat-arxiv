<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411; successfully &#21306;&#20998;&#20102;&#21271;&#33832;&#31859;&#35821;&#35328;&#30340;&#22235;&#31181;&#26041;&#35328;&#21464;&#20307;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#26041;&#35328;&#21463;&#21040;&#20102;&#22269;&#23478;&#35821;&#35328;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.11864</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#21271;&#33832;&#31859;&#35821;&#35328;&#26041;&#35328;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
North S\'{a}mi Dialect Identification with Self-supervised Speech Models. (arXiv:2305.11864v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11864
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411; successfully &#21306;&#20998;&#20102;&#21271;&#33832;&#31859;&#35821;&#35328;&#30340;&#22235;&#31181;&#26041;&#35328;&#21464;&#20307;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#26041;&#35328;&#21463;&#21040;&#20102;&#22269;&#23478;&#35821;&#35328;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21271;&#33832;&#31859;&#35821;&#35328;&#21253;&#25324;&#22235;&#31181;&#20027;&#35201;&#30340;&#26041;&#35328;&#21464;&#20307;&#65292;&#23427;&#20204;&#22312;&#35821;&#38899;&#12289;&#24418;&#24577;&#21644;&#35789;&#27719;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#21271;&#33832;&#31859;&#35821;&#35328;&#20351;&#29992;&#32773;&#30340;&#29305;&#27530;&#22320;&#32536;&#25919;&#27835;&#20301;&#32622;&#24847;&#21619;&#30528;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20182;&#20204;&#26082;&#26159;&#33832;&#31859;&#35821;&#20351;&#29992;&#32773;&#65292;&#20063;&#26159;&#20248;&#21183;&#22269;&#23478;&#35821;&#35328;&#65288;&#25386;&#23041;&#35821;&#12289;&#29790;&#20856;&#35821;&#25110;&#33452;&#20848;&#35821;&#65289;&#30340;&#21452;&#35821;&#32773;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#32452;&#24191;&#27867;&#30340;&#22768;&#23398;&#29305;&#24449;&#65292;&#21253;&#25324;MFCC&#21644;&#38901;&#24459;&#29305;&#24449;&#65292;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#26041;&#27861;&#65292;&#21363;XLS-R&#12289;WavLM&#21644;HuBERT&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#22235;&#31181;&#21271;&#33832;&#31859;&#26041;&#35328;&#21464;&#20307;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20248;&#21183;&#22269;&#23478;&#35821;&#35328;&#22914;&#20309;&#21453;&#26144;&#22312;&#26041;&#35328;&#21464;&#20307;&#20013;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21271;&#33832;&#31859;&#26041;&#35328;&#21463;&#21040;&#22269;&#23478;&#35821;&#35328;&#30340;&#24433;&#21709;&#65292;&#22235;&#31181;&#21271;&#33832;&#31859;&#26041;&#35328;&#21464;&#20307;&#26159;&#21487;&#20197;&#20998;&#31163;&#30340;&#65292;&#24182;&#19988;&#20351;&#29992;XLS-R&#27169;&#22411;&#21487;&#33719;&#24471;&#24456;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The North S\'{a}mi (NS) language encapsulates four primary dialectal variants that are related but that also have differences in their phonology, morphology, and vocabulary. The unique geopolitical location of NS speakers means that in many cases they are bilingual in S\'{a}mi as well as in the dominant state language: Norwegian, Swedish, or Finnish. This enables us to study the NS variants both with respect to the spoken state language and their acoustic characteristics. In this paper, we investigate an extensive set of acoustic features, including MFCCs and prosodic features, as well as state-of-the-art self-supervised representations, namely, XLS-R, WavLM, and HuBERT, for the automatic detection of the four NS variants. In addition, we examine how the majority state language is reflected in the dialects. Our results show that NS dialects are influenced by the state language and that the four dialects are separable, reaching high classification accuracy, especially with the XLS-R mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#22312;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.11863</link><description>&lt;p&gt;
&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#30340;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling laws for language encoding models in fMRI. (arXiv:2305.11863v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#22312;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#21333;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#22823;&#33041;&#23545;&#33258;&#28982;&#35821;&#35328;&#30340;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#19982;&#22823;&#33041;&#30340;&#30740;&#31350;&#37117;&#20351;&#29992;&#20102;&#31867;&#20284;GPT-2&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#26159;&#21542;&#26356;&#22823;&#30340;&#24320;&#28304;&#27169;&#22411;&#65288;&#22914;OPT&#21644;LLaMA&#31995;&#21015;&#65289;&#26356;&#36866;&#29992;&#20110;&#39044;&#27979;&#20351;&#29992;fMRI&#35760;&#24405;&#30340;&#22823;&#33041;&#21453;&#24212;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20174;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#22823;&#33041;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#36328;3&#20010;&#21463;&#35797;&#32773;&#30340;&#20445;&#30041;&#27979;&#35797;&#38598;&#30456;&#20851;&#24615;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;&#24403;&#25193;&#23637;fMRI&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#26102;&#65292;&#25105;&#20204;&#20063;&#35266;&#23519;&#21040;&#20102;&#31867;&#20284;&#30340;&#23545;&#25968;&#32447;&#24615;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#23545;&#20351;&#29992;HuBERT&#65292;WavLM&#21644;Whisper&#30340;&#22768;&#23398;&#32534;&#30721;&#27169;&#22411;&#36827;&#34892;&#20102;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;&#65292;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#24102;&#26469;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#22122;&#38899;&#22825;&#33457;&#26495;&#20998;&#26512;&#20102;&#36825;&#20123;&#22823;&#35268;&#27169;&#19988;&#39640;&#24615;&#33021;&#30340;&#32534;&#30721;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations from transformer-based unidirectional language models are known to be effective at predicting brain responses to natural language. However, most studies comparing language models to brains have used GPT-2 or similarly sized language models. Here we tested whether larger open-source models such as those from the OPT and LLaMA families are better at predicting brain responses recorded using fMRI. Mirroring scaling results from other contexts, we found that brain prediction performance scales log-linearly with model size from 125M to 30B parameter models, with ~15% increased encoding performance as measured by correlation with a held-out test set across 3 subjects. Similar log-linear behavior was observed when scaling the size of the fMRI training set. We also characterized scaling for acoustic encoding models that use HuBERT, WavLM, and Whisper, and we found comparable improvements with model size. A noise ceiling analysis of these large, high-performance encoding models 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#28304;&#25991;&#26412;&#20013;&#30340;&#32534;&#36753;&#25805;&#20316;&#26469;&#20943;&#23569;&#24207;&#21015;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11862</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#32534;&#36753;&#25805;&#20316;&#26469;&#20943;&#23569;&#24207;&#21015;&#38271;&#24230;
&lt;/p&gt;
&lt;p&gt;
Reducing Sequence Length by Predicting Edit Operations with Large Language Models. (arXiv:2305.11862v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#28304;&#25991;&#26412;&#20013;&#30340;&#32534;&#36753;&#25805;&#20316;&#26469;&#20943;&#23569;&#24207;&#21015;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#20851;&#27880;&#12290; LLMs&#20063;&#34987;&#29992;&#20110;&#26412;&#22320;&#24207;&#21015;&#36716;&#25442;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;GEC&#65289;&#21644;&#24418;&#24335;&#39118;&#26684;&#36716;&#25442;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#28304;&#25991;&#26412;&#20013;&#30340;&#22823;&#37096;&#20998;&#35760;&#21495;&#20445;&#25345;&#19981;&#21464;&#12290; &#28982;&#32780;&#65292;&#29983;&#25104;&#25152;&#26377;&#30446;&#26631;&#35760;&#21495;&#26159;&#20302;&#25928;&#29575;&#30340;&#65292;&#22240;&#20026;&#30446;&#26631;&#35760;&#21495;&#30340;&#39044;&#27979;&#38169;&#35823;&#21487;&#33021;&#23548;&#33268;&#22312;&#39044;&#27979;&#21518;&#32493;&#35760;&#21495;&#26102;&#20986;&#29616;&#28798;&#38590;&#65292;&#24182;&#19988;&#38543;&#30528;&#30446;&#26631;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#35745;&#31639;&#25104;&#26412;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#26412;&#22320;&#24207;&#21015;&#36716;&#25442;&#20219;&#21153;&#30340;&#28304;&#25991;&#26412;&#30340;&#19968;&#32452;&#32534;&#36753;&#25805;&#20316;&#30340;&#26041;&#27861;&#12290; &#36890;&#36807;&#34920;&#31034;&#19968;&#20010;&#32534;&#36753;&#25805;&#20316;&#30340;&#28304;&#25991;&#26412;&#36328;&#24230;&#21644;&#26356;&#25913;&#30340;&#35760;&#21495;&#65292;&#25105;&#20204;&#21487;&#20197;&#20943;&#23569;&#30446;&#26631;&#24207;&#21015;&#30340;&#38271;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#25512;&#26029;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290; &#25105;&#20204;&#22312;&#32534;&#36753;&#25805;&#20316;&#30340;&#30417;&#30563;&#25968;&#25454;&#19978;&#24212;&#29992;&#25351;&#23548;&#35843;&#20248;LLMs&#12290; &#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable performance in various tasks and gained significant attention. LLMs are also used for local sequence transduction tasks, including grammatical error correction (GEC) and formality style transfer, where most tokens in a source text are kept unchanged. However, it is inefficient to generate all target tokens because a prediction error of a target token may cause a catastrophe in predicting subsequent tokens and because the computational cost grows quadratically with the target sequence length. This paper proposes to predict a set of edit operations for the source text for local sequence transduction tasks. Representing an edit operation with a span of the source text and changed tokens, we can reduce the length of the target sequence and thus the computational cost for inference. We apply instruction tuning for LLMs on the supervision data of edit operations. Experiments show that the proposed method achieves comparable performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#19968;&#33268;&#24615; (Adaptive-Consistency) &#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#27599;&#20010;&#38382;&#39064;&#30340;&#26679;&#26412;&#25968;&#37327;&#65292;&#20174;&#32780;&#26377;&#25928;&#20943;&#23569;&#26679;&#26412;&#39044;&#31639;&#65292;&#24182;&#36739;&#23567;&#31243;&#24230;&#22320;&#38477;&#20302;&#20102;&#24179;&#22343;&#20934;&#30830;&#24230;</title><link>http://arxiv.org/abs/2305.11860</link><description>&lt;p&gt;
&#20998;&#27493;&#37319;&#26679;&#65306;&#29992;&#33258;&#36866;&#24212;&#19968;&#33268;&#24615;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs. (arXiv:2305.11860v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#19968;&#33268;&#24615; (Adaptive-Consistency) &#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#27599;&#20010;&#38382;&#39064;&#30340;&#26679;&#26412;&#25968;&#37327;&#65292;&#20174;&#32780;&#26377;&#25928;&#20943;&#23569;&#26679;&#26412;&#39044;&#31639;&#65292;&#24182;&#36739;&#23567;&#31243;&#24230;&#22320;&#38477;&#20302;&#20102;&#24179;&#22343;&#20934;&#30830;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#33391;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36755;&#20986;&#27491;&#30830;&#24615;&#30340;&#19968;&#20010;&#27969;&#34892;&#26041;&#27861;&#26159;&#20351;&#29992;&#33258;&#19968;&#33268;&#24615;&#8212;&#8212;&#23545; LLM &#36827;&#34892;&#22810;&#27425;&#25237;&#31080;&#24182;&#36755;&#20986;&#26368;&#39057;&#32321;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#33258;&#19968;&#33268;&#24615;&#25216;&#26415;&#37117;&#26159;&#27599;&#20010;&#38382;&#39064;&#37117;&#20250;&#22266;&#23450;&#37319;&#38598;&#19968;&#23450;&#25968;&#37327;&#30340;&#26679;&#26412;&#65292;&#32780;&#26356;&#22909;&#30340;&#26041;&#27861;&#26159;&#26681;&#25454;&#24050;&#32463;&#37319;&#38598;&#21040;&#30340;&#26679;&#26412;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#38750;&#22343;&#21248;&#22320;&#20998;&#37197;&#39044;&#31639;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#19968;&#33268;&#24615; (Adaptive-Consistency) &#30340;&#25104;&#26412;&#26377;&#25928;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#36731;&#37327;&#32423;&#20572;&#27490;&#20934;&#21017;&#21160;&#24577;&#35843;&#25972;&#27599;&#20010;&#38382;&#39064;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312; 13 &#20010;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010; LLM &#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#33258;&#36866;&#24212;&#19968;&#33268;&#24615;&#21487;&#20197;&#23558;&#26679;&#26412;&#39044;&#31639;&#38477;&#20302;&#22810;&#36798; 6 &#20493;&#65292;&#24182;&#19988;&#24179;&#22343;&#20934;&#30830;&#24230;&#38477;&#20302;&#19981;&#21040; 0.1%&#12290;
&lt;/p&gt;
&lt;p&gt;
A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always draw a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples drawn so far. In response, we introduce Adaptive-Consistency, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 13 datasets and two LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 6.0 times with an average accuracy drop of less than 0.1%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#27969;&#31243;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#32593;&#32476;&#20013;&#26816;&#32034;&#21407;&#22987;&#35777;&#25454;&#26469;&#26816;&#26597;&#23454;&#38469;&#20027;&#24352;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#21253;&#25324;&#20116;&#20010;&#37096;&#20998;&#65306;&#20027;&#24352;&#20998;&#35299;&#65292;&#21407;&#22987;&#25991;&#26723;&#26816;&#32034;&#65292;&#32454;&#31890;&#24230;&#35777;&#25454;&#26816;&#32034;&#65292;&#20027;&#24352;&#28966;&#28857;&#25688;&#35201;&#21644;&#21028;&#26029;&#30495;&#23454;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27969;&#31243;&#21487;&#20197;&#25913;&#21892;&#30495;&#23454;&#24615;&#21028;&#26029;&#65292;&#25105;&#20204;&#25552;&#20379;&#30340;&#35777;&#25454;&#25688;&#35201;&#26356;&#26131;&#20110;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.11859</link><description>&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#22797;&#26434;&#20027;&#24352;&#39564;&#35777;&#20013;&#30340;&#21462;&#35777;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Complex Claim Verification with Evidence Retrieved in the Wild. (arXiv:2305.11859v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#27969;&#31243;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#32593;&#32476;&#20013;&#26816;&#32034;&#21407;&#22987;&#35777;&#25454;&#26469;&#26816;&#26597;&#23454;&#38469;&#20027;&#24352;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#21253;&#25324;&#20116;&#20010;&#37096;&#20998;&#65306;&#20027;&#24352;&#20998;&#35299;&#65292;&#21407;&#22987;&#25991;&#26723;&#26816;&#32034;&#65292;&#32454;&#31890;&#24230;&#35777;&#25454;&#26816;&#32034;&#65292;&#20027;&#24352;&#28966;&#28857;&#25688;&#35201;&#21644;&#21028;&#26029;&#30495;&#23454;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27969;&#31243;&#21487;&#20197;&#25913;&#21892;&#30495;&#23454;&#24615;&#21028;&#26029;&#65292;&#25105;&#20204;&#25552;&#20379;&#30340;&#35777;&#25454;&#25688;&#35201;&#26356;&#26131;&#20110;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21462;&#35777;&#26159;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#30340;&#26680;&#24515;&#37096;&#20998;&#12290; &#38024;&#23545;&#33719;&#21462;&#21462;&#35777;&#30340;&#20551;&#35774;&#19981;&#31526;&#21512;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#20570;&#20986;&#20102;&#31616;&#21270;&#30340;&#20551;&#35774;&#65292;&#21363;&#27809;&#26377;&#33719;&#21462;&#35777;&#25454;&#30340;&#26435;&#38480;&#65292;&#35775;&#38382;&#30001;&#20154;&#24037;&#20107;&#23454;&#26680;&#26597;&#21592;&#31934;&#31616;&#30340;&#35777;&#25454;&#65292;&#25110;&#22312;&#23545;&#20027;&#24352;&#30830;&#35748;&#20043;&#21518;&#24456;&#38271;&#26102;&#38388;&#25165;&#33021;&#33719;&#21462;&#35777;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#20174;&#32593;&#32476;&#20013;&#26816;&#32034;&#21407;&#22987;&#35777;&#25454;&#26469;&#26816;&#26597;&#23454;&#38469;&#20027;&#24352;&#12290;&#25105;&#20204;&#38480;&#21046;&#25105;&#20204;&#30340;&#26816;&#32034;&#22120;&#20165;&#25628;&#32034;&#22312;&#25552;&#20986;&#20027;&#24352;&#20043;&#21069;&#23601;&#24050;&#21487;&#25552;&#20379;&#30340;&#25991;&#26723;&#65292;&#27169;&#25311;&#26032;&#20986;&#29616;&#30340;&#20027;&#24352;&#38656;&#35201;&#26816;&#26597;&#30340;&#30495;&#23454;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#21253;&#25324;&#20116;&#20010;&#37096;&#20998;&#65306;&#20027;&#24352;&#20998;&#35299;&#65292;&#21407;&#22987;&#25991;&#26723;&#26816;&#32034;&#65292;&#32454;&#31890;&#24230;&#35777;&#25454;&#26816;&#32034;&#65292;&#20027;&#24352;&#28966;&#28857;&#25688;&#35201;&#21644;&#21028;&#26029;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#22312;ClaimDecomp&#25968;&#25454;&#38598;&#20013;&#23545;&#22797;&#26434;&#25919;&#27835;&#20027;&#24352;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#27969;&#31243;&#20135;&#29983;&#30340;&#32858;&#21512;&#35777;&#25454;&#21487;&#20197;&#25913;&#21892;&#30495;&#23454;&#24615;&#21028;&#26029;&#12290;&#20154;&#31867;&#35780;&#20272;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#25688;&#35201;&#20135;&#29983;&#30340;&#35777;&#25454;&#35201;&#27604;&#30001;&#29616;&#26377;&#31995;&#32479;&#20135;&#29983;&#30340;&#35777;&#25454;&#26356;&#26131;&#20110;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evidence retrieval is a core part of automatic fact-checking. Prior work makes simplifying assumptions in retrieval that depart from real-world use cases: either no access to evidence, access to evidence curated by a human fact-checker, or access to evidence available long after the claim has been made. In this work, we present the first fully automated pipeline to check real-world claims by retrieving raw evidence from the web. We restrict our retriever to only search documents available prior to the claim's making, modeling the realistic scenario where an emerging claim needs to be checked. Our pipeline includes five components: claim decomposition, raw document retrieval, fine-grained evidence retrieval, claim-focused summarization, and veracity judgment. We conduct experiments on complex political claims in the ClaimDecomp dataset and show that the aggregated evidence produced by our pipeline improves veracity judgments. Human evaluation finds the evidence summary produced by our s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#24341;&#23548;LLMs&#36827;&#34892;&#25991;&#26412;&#21040;SQL&#30340;&#20219;&#21153;&#20013;&#25552;&#31034;&#25991;&#26412;&#26500;&#24314;&#38382;&#39064;&#23637;&#24320;&#20102;&#32508;&#21512;&#25506;&#31350;&#65292;&#20174;&#32780;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.11853</link><description>&lt;p&gt;
&#22914;&#20309;&#24341;&#23548;LLMs&#36827;&#34892;&#25991;&#26412;&#21040;SQL&#30340;&#23398;&#20064;: &#20174;&#38646;&#26679;&#26412;&#21040;&#21333;&#39046;&#22495;&#21040;&#36328;&#39046;&#22495;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings. (arXiv:2305.11853v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24341;&#23548;LLMs&#36827;&#34892;&#25991;&#26412;&#21040;SQL&#30340;&#20219;&#21153;&#20013;&#25552;&#31034;&#25991;&#26412;&#26500;&#24314;&#38382;&#39064;&#23637;&#24320;&#20102;&#32508;&#21512;&#25506;&#31350;&#65292;&#20174;&#32780;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#33021;&#21147;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#21508;&#31181;&#28436;&#31034;-&#26816;&#32034;&#31574;&#30053;&#21644;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26469;&#20419;&#20351;LLMs&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#22312;&#26500;&#24314;&#25991;&#26412;&#21040;SQL&#36755;&#20837;&#30340;&#25552;&#31034;&#25991;&#26412;(&#22914;&#25968;&#25454;&#24211;&#21644;&#28436;&#31034;&#31034;&#20363;)&#26102;&#24120;&#37319;&#29992;&#19981;&#21516;&#30340;&#31574;&#30053;&#12290;&#36825;&#23548;&#33268;&#25552;&#31034;&#25991;&#26412;&#30340;&#26500;&#24314;&#21644;&#20854;&#20027;&#35201;&#36129;&#29486;&#30340;&#21487;&#27604;&#24615;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#36873;&#25321;&#26377;&#25928;&#30340;&#25552;&#31034;&#25991;&#26412;&#24314;&#35774;&#24050;&#25104;&#20026;&#26410;&#26469;&#30740;&#31350;&#20013;&#30340;&#25345;&#20037;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#20840;&#38754;&#35843;&#26597;&#20102;&#19981;&#21516;&#35774;&#32622;&#19979;&#25552;&#31034;&#25991;&#26412;&#32467;&#26500;&#30340;&#24433;&#21709;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with in-context learning have demonstrated remarkable capability in the text-to-SQL task. Previous research has prompted LLMs with various demonstration-retrieval strategies and intermediate reasoning steps to enhance the performance of LLMs. However, those works often employ varied strategies when constructing the prompt text for text-to-SQL inputs, such as databases and demonstration examples. This leads to a lack of comparability in both the prompt constructions and their primary contributions. Furthermore, selecting an effective prompt construction has emerged as a persistent problem for future research. To address this limitation, we comprehensively investigate the impact of prompt constructions across various settings and provide insights for future work.
&lt;/p&gt;</description></item><item><title>CoDi&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#21644;&#36755;&#20986;&#31354;&#38388;&#20013;&#36827;&#34892;&#27169;&#24577;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#21487;&#32452;&#21512;&#29983;&#25104;&#31574;&#30053;&#65292;&#20174;&#32780;&#21487;&#20197;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#36755;&#20986;&#27169;&#24577;&#12290; CoDi &#38750;&#24120;&#28789;&#27963;&#65292;&#33021;&#22815;&#29983;&#25104;&#22810;&#20010;&#27169;&#24577;&#65292;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#35821;&#35328;&#21644;&#38899;&#39057;&#65292;&#29978;&#33267;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#23384;&#22312;&#30340;&#27169;&#24577;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.11846</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#32452;&#21512;&#25193;&#25955;&#23454;&#29616;&#20219;&#24847;&#36755;&#20837;&#19982;&#36755;&#20986;&#20043;&#38388;&#30340;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Any-to-Any Generation via Composable Diffusion. (arXiv:2305.11846v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11846
&lt;/p&gt;
&lt;p&gt;
CoDi&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#21644;&#36755;&#20986;&#31354;&#38388;&#20013;&#36827;&#34892;&#27169;&#24577;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#21487;&#32452;&#21512;&#29983;&#25104;&#31574;&#30053;&#65292;&#20174;&#32780;&#21487;&#20197;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#36755;&#20986;&#27169;&#24577;&#12290; CoDi &#38750;&#24120;&#28789;&#27963;&#65292;&#33021;&#22815;&#29983;&#25104;&#22810;&#20010;&#27169;&#24577;&#65292;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#35821;&#35328;&#21644;&#38899;&#39057;&#65292;&#29978;&#33267;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#23384;&#22312;&#30340;&#27169;&#24577;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20043;&#20026;&#21487;&#32452;&#21512;&#25193;&#25955;&#65288;CoDi&#65289;&#12290;&#36825;&#20010;&#27169;&#22411;&#33021;&#22815;&#20174;&#20219;&#24847;&#36755;&#20837;&#27169;&#24577;&#21644;&#20219;&#24847;&#32452;&#21512;&#20013;&#29983;&#25104;&#21508;&#31181;&#36755;&#20986;&#27169;&#24577;&#65292;&#22914;&#35821;&#35328;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#38899;&#39057;&#12290;&#19982;&#29616;&#26377;&#30340;&#29983;&#25104; AI &#31995;&#32479;&#19981;&#21516;&#65292;CoDi &#21487;&#20197;&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#27169;&#24577;&#65292;&#24182;&#19988;&#20854;&#36755;&#20837;&#19981;&#23616;&#38480;&#20110;&#25991;&#26412;&#25110;&#22270;&#20687;&#30340;&#23376;&#38598;&#12290;&#23613;&#31649;&#24456;&#22810;&#27169;&#24577;&#30340;&#32452;&#21512;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#36755;&#20837;&#31354;&#38388;&#21644;&#36755;&#20986;&#31354;&#38388;&#20013;&#36827;&#34892;&#27169;&#24577;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20351; CoDi &#21487;&#20197;&#33258;&#30001;&#22320;&#23545;&#20219;&#20309;&#36755;&#20837;&#32452;&#21512;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#65292;&#24182;&#29983;&#25104;&#20219;&#20309;&#27169;&#24577;&#32452;&#21512;&#65292;&#21363;&#20351;&#36825;&#20123;&#32452;&#21512;&#19981;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;CoDi&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#32452;&#21512;&#29983;&#25104;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#26500;&#24314;&#20849;&#20139;&#30340;&#22810;&#27169;&#24577;&#31354;&#38388;&#26469;&#23454;&#29616;&#23545;&#40784;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20132;&#32455;&#27169;&#24577;&#30340;&#21516;&#27493;&#29983;&#25104;&#65292;&#20363;&#22914;&#26102;&#38388;&#19978;&#23545;&#40784;&#30340;&#35270;&#39057;&#21644;&#38899;&#39057;&#12290;&#39640;&#24230;&#21487;&#23450;&#21046;&#21644;&#28789;&#27963;&#65292;CoDi &#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#32852;&#21512;&#27169;&#24577;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality gen
&lt;/p&gt;</description></item><item><title>RxnScribe&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#26512;&#21270;&#23398;&#25991;&#29486;&#20013;&#22797;&#26434;&#30340;&#21453;&#24212;&#22270;&#65292;&#24182;&#22312;&#20132;&#21449;&#39564;&#35777;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.11845</link><description>&lt;p&gt;
RxnScribe&#65306;&#21453;&#24212;&#22270;&#35299;&#26512;&#30340;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing. (arXiv:2305.11845v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11845
&lt;/p&gt;
&lt;p&gt;
RxnScribe&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#26512;&#21270;&#23398;&#25991;&#29486;&#20013;&#22797;&#26434;&#30340;&#21453;&#24212;&#22270;&#65292;&#24182;&#22312;&#20132;&#21449;&#39564;&#35777;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#24212;&#22270;&#35299;&#26512;&#26159;&#20174;&#21270;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#21453;&#24212;&#26041;&#26696;&#30340;&#20219;&#21153;&#12290;&#21453;&#24212;&#22270;&#21487;&#20197;&#26497;&#20854;&#22797;&#26434;&#65292;&#22240;&#27492;&#23558;&#20854;&#31283;&#20581;&#22320;&#35299;&#26512;&#25104;&#32467;&#26500;&#21270;&#25968;&#25454;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RxnScribe&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35299;&#26512;&#19981;&#21516;&#39118;&#26684;&#21453;&#24212;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#37319;&#29992;&#24207;&#21015;&#29983;&#25104;&#26041;&#27861;&#26469;&#21046;&#23450;&#36825;&#20010;&#32467;&#26500;&#21270;&#39044;&#27979;&#20219;&#21153;&#65292;&#23558;&#20256;&#32479;&#30340;&#31649;&#36947;&#27969;&#31243;&#21387;&#32553;&#20026;&#31471;&#21040;&#31471;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;1,378&#20010;&#22270;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;RxnScribe&#65292;&#24182;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;80.0%&#30340;&#36719;&#21305;&#37197;F1&#20998;&#25968;&#65292;&#19982;&#20197;&#21069;&#30340;&#27169;&#22411;&#30456;&#27604;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#20197;&#22312;https://github.com/thomas0809/RxnScribe &#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reaction diagram parsing is the task of extracting reaction schemes from a diagram in the chemistry literature. The reaction diagrams can be arbitrarily complex, thus robustly parsing them into structured data is an open challenge. In this paper, we present RxnScribe, a machine learning model for parsing reaction diagrams of varying styles. We formulate this structured prediction task with a sequence generation approach, which condenses the traditional pipeline into an end-to-end model. We train RxnScribe on a dataset of 1,378 diagrams and evaluate it with cross validation, achieving an 80.0% soft match F1 score, with significant improvements over previous models. Our code and data are publicly available at https://github.com/thomas0809/RxnScribe.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#35821;&#26009;&#35268;&#27169;&#19979;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#25216;&#26415;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#25193;&#23637;&#21040;&#21253;&#21547;8.8M&#31687;&#31456;&#30340;MS MARCO&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#35780;&#20272;&#20102;&#39640;&#36798;11B&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#25581;&#31034;&#20986;&#20102;&#22312;&#32034;&#24341;&#26399;&#38388;&#20351;&#29992;&#21512;&#25104;&#26597;&#35810;&#20316;&#20026;&#25991;&#26723;&#34920;&#31034;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11841</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#26816;&#32034;&#22914;&#20309;&#25193;&#23637;&#21040;&#25968;&#30334;&#19975;&#31687;&#25991;&#31456;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Does Generative Retrieval Scale to Millions of Passages?. (arXiv:2305.11841v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#35821;&#26009;&#35268;&#27169;&#19979;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#25216;&#26415;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#25193;&#23637;&#21040;&#21253;&#21547;8.8M&#31687;&#31456;&#30340;MS MARCO&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#35780;&#20272;&#20102;&#39640;&#36798;11B&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#25581;&#31034;&#20986;&#20102;&#22312;&#32034;&#24341;&#26399;&#38388;&#20351;&#29992;&#21512;&#25104;&#26597;&#35810;&#20316;&#20026;&#25991;&#26723;&#34920;&#31034;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#21487;&#24494;&#25628;&#32034;&#32034;&#24341;(Differentiable Search Index)&#25512;&#24191;&#32780;&#26469;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#30340;&#26032;&#20852;&#33539;&#24335;,&#23558;&#32463;&#20856;&#30340;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#37325;&#26032;&#26500;&#36896;&#20026;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#24314;&#27169;&#20219;&#21153;&#65292;&#25918;&#24323;&#20102;&#22806;&#37096;&#32034;&#24341;&#65292;&#24182;&#23558;&#25972;&#20010;&#25991;&#26723;&#35821;&#26009;&#24211;&#32534;&#30721;&#22312;&#21333;&#20010;Transformer&#20013;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#29983;&#25104;&#24335;&#26816;&#32034;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#23427;&#20204;&#20165;&#22312;&#32422;100k&#30340;&#25991;&#26723;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#26681;&#25454;&#19981;&#21516;&#30340;&#35821;&#26009;&#35268;&#27169;&#36827;&#34892;&#29983;&#25104;&#24335;&#26816;&#32034;&#25216;&#26415;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#26368;&#32456;&#25193;&#23637;&#21040;&#25972;&#20010;8.8M&#31687;&#31456;&#30340;MS MARCO&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#35780;&#20272;&#20102;&#39640;&#36798;11B&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#22823;&#23567;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#20851;&#20110;&#23558;&#29983;&#25104;&#24335;&#26816;&#32034;&#25193;&#23637;&#21040;&#25968;&#30334;&#19975;&#31687;&#25991;&#31456;&#30340;&#20960;&#20010;&#21457;&#29616;&#65292;&#29305;&#21035;&#26159;&#32034;&#24341;&#26399;&#38388;&#20351;&#29992;&#21512;&#25104;&#26597;&#35810;&#20316;&#20026;&#25991;&#26723;&#34920;&#31034;&#30340;&#26680;&#24515;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#32771;&#34385;&#24182;&#38750;&#29616;&#26377;&#30340;&#24314;&#35758;&#26550;&#26500;&#20462;&#25913;&#26102;&#30340;&#26080;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Popularized by the Differentiable Search Index, the emerging paradigm of generative retrieval re-frames the classic information retrieval problem into a sequence-to-sequence modeling task, forgoing external indices and encoding an entire document corpus within a single Transformer. Although many different approaches have been proposed to improve the effectiveness of generative retrieval, they have only been evaluated on document corpora on the order of 100k in size. We conduct the first empirical study of generative retrieval techniques across various corpus scales, ultimately scaling up to the entire MS MARCO passage ranking task with a corpus of 8.8M passages and evaluating model sizes up to 11B parameters. We uncover several findings about scaling generative retrieval to millions of passages; notably, the central importance of using synthetic queries as document representations during indexing, the ineffectiveness of existing proposed architecture modifications when accounting for c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#27867;&#35206;&#30422;&#19981;&#21516;&#22320;&#29702;&#21644;&#25991;&#21270;&#32972;&#26223;&#19979;&#30340;&#21051;&#26495;&#21360;&#35937;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;SeeGULL&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21051;&#26495;&#21360;&#35937;&#65292;&#38024;&#23545;178&#20010;&#22269;&#23478;&#12289;8&#20010;&#19981;&#21516;&#22320;&#32536;&#25919;&#27835;&#22320;&#21306;&#21644;&#32654;&#22269;&#19982;&#21360;&#24230;&#30340;&#24030;&#32423;&#36523;&#20221;&#32676;&#20307;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20197;&#25552;&#39640;NLP&#27169;&#22411;&#23545;&#20840;&#29699;&#22810;&#20803;&#21270;&#20154;&#32676;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11840</link><description>&lt;p&gt;
SeeGULL&#65306;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#24191;&#27867;&#22320;&#29702;&#25991;&#21270;&#35206;&#30422;&#30340;&#21051;&#26495;&#21360;&#35937;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models. (arXiv:2305.11840v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11840
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#27867;&#35206;&#30422;&#19981;&#21516;&#22320;&#29702;&#21644;&#25991;&#21270;&#32972;&#26223;&#19979;&#30340;&#21051;&#26495;&#21360;&#35937;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;SeeGULL&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21051;&#26495;&#21360;&#35937;&#65292;&#38024;&#23545;178&#20010;&#22269;&#23478;&#12289;8&#20010;&#19981;&#21516;&#22320;&#32536;&#25919;&#27835;&#22320;&#21306;&#21644;&#32654;&#22269;&#19982;&#21360;&#24230;&#30340;&#24030;&#32423;&#36523;&#20221;&#32676;&#20307;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20197;&#25552;&#39640;NLP&#27169;&#22411;&#23545;&#20840;&#29699;&#22810;&#20803;&#21270;&#20154;&#32676;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21051;&#26495;&#21360;&#35937;&#22522;&#20934;&#25968;&#25454;&#38598;&#23545;&#20110;&#26816;&#27979;&#21644;&#32531;&#35299;NLP&#27169;&#22411;&#20013;&#26377;&#20851;&#20154;&#32676;&#30340;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#21644;&#35206;&#30422;&#33539;&#22260;&#19978;&#23384;&#22312;&#38480;&#21046;&#65292;&#24182;&#19988;&#20027;&#35201;&#38480;&#20110;&#35199;&#26041;&#31038;&#20250;&#26222;&#36941;&#23384;&#22312;&#30340;&#21051;&#26495;&#21360;&#35937;&#12290;&#38543;&#30528;&#35821;&#35328;&#25216;&#26415;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#26222;&#21450;&#65292;&#36825;&#19968;&#38382;&#39064;&#23588;&#20026;&#20005;&#37325;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SeeGULL&#65292;&#36825;&#26159;&#19968;&#20010;&#24191;&#27867;&#35206;&#30422;&#30340;&#21051;&#26495;&#21360;&#35937;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;PaLM&#21644;GPT-3&#65289;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#21033;&#29992;&#20840;&#29699;&#24615;&#30340;&#22810;&#26679;&#21270;&#35780;&#20998;&#27744;&#39564;&#35777;&#31038;&#20250;&#20013;&#36825;&#20123;&#21051;&#26495;&#21360;&#35937;&#30340;&#26222;&#36941;&#31243;&#24230;&#12290;SeeGULL&#20351;&#29992;&#33521;&#35821;&#65292;&#24182;&#21253;&#21547;&#28085;&#30422;178&#20010;&#22269;&#23478;&#12289;6&#20010;&#22823;&#27954;&#21644;8&#20010;&#19981;&#21516;&#22320;&#32536;&#25919;&#27835;&#22320;&#21306;&#30340;&#36523;&#20221;&#32676;&#20307;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#20197;&#21450;&#32654;&#22269;&#21644;&#21360;&#24230;&#30340;&#24030;&#32423;&#36523;&#20221;&#12290;&#25105;&#20204;&#36824;&#21253;&#25324;&#19981;&#21516;&#21051;&#26495;&#21360;&#35937;&#30340;&#31934;&#32454;&#20882;&#29359;&#20998;&#25968;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#20043;&#38388;&#30340;&#20840;&#29699;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21253;&#25324;&#27604;&#36739;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stereotype benchmark datasets are crucial to detect and mitigate social stereotypes about groups of people in NLP models. However, existing datasets are limited in size and coverage, and are largely restricted to stereotypes prevalent in the Western society. This is especially problematic as language technologies gain hold across the globe. To address this gap, we present SeeGULL, a broad-coverage stereotype dataset, built by utilizing generative capabilities of large language models such as PaLM, and GPT-3, and leveraging a globally diverse rater pool to validate the prevalence of those stereotypes in society. SeeGULL is in English, and contains stereotypes about identity groups spanning 178 countries across 8 different geo-political regions across 6 continents, as well as state-level identities within the US and India. We also include fine-grained offensiveness scores for different stereotypes and demonstrate their global disparities. Furthermore, we include comparative annotations a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLM&#21327;&#21161;&#21046;&#20316;&#21307;&#23398;&#35777;&#25454;&#32508;&#36848;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#26377;&#21487;&#33021;&#33258;&#21160;&#29983;&#25104;&#25991;&#29486;&#32508;&#36848;&#65292;&#20294;&#30001;&#20110;&#21487;&#33021;&#20986;&#29616;&#34394;&#26500;&#25110;&#36951;&#28431;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;LLM&#30340;&#20351;&#29992;&#38656;&#35201;&#35880;&#24910;&#12290;</title><link>http://arxiv.org/abs/2305.11828</link><description>&lt;p&gt;
LLM&#22312;&#21307;&#23398;&#31995;&#32479;&#32508;&#36848;&#20013;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews. (arXiv:2305.11828v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLM&#21327;&#21161;&#21046;&#20316;&#21307;&#23398;&#35777;&#25454;&#32508;&#36848;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#26377;&#21487;&#33021;&#33258;&#21160;&#29983;&#25104;&#25991;&#29486;&#32508;&#36848;&#65292;&#20294;&#30001;&#20110;&#21487;&#33021;&#20986;&#29616;&#34394;&#26500;&#25110;&#36951;&#28431;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;LLM&#30340;&#20351;&#29992;&#38656;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#31995;&#32479;&#32508;&#36848;&#23545;&#20110;&#21046;&#23450;&#20020;&#24202;&#20915;&#31574;&#21644;&#21307;&#30103;&#25919;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20294;&#26159;&#21046;&#20316;&#36825;&#26679;&#30340;&#32508;&#36848;&#24456;&#36153;&#21147;&#19988;&#32791;&#26102;&#12290;&#22240;&#27492;&#65292;&#24456;&#22810;&#38382;&#39064;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#35777;&#25454;&#32508;&#36848;&#65292;&#21363;&#20351;&#36825;&#20123;&#32508;&#36848;&#21487;&#29992;&#65292;&#22312;&#23457;&#26597;&#36807;&#31243;&#20013;&#21487;&#33021;&#24050;&#32463;&#36807;&#26102;&#12290;&#29616;&#22312;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#33021;&#22815;&#29983;&#25104;&#38271;&#31687;&#25991;&#26412;&#65292;&#36825;&#24847;&#21619;&#30528;&#33258;&#21160;&#29983;&#25104;&#25991;&#29486;&#32508;&#36848;&#30340;&#35825;&#20154;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34394;&#26500;&#25110;&#36951;&#28431;&#37325;&#35201;&#20449;&#24687;&#65292;LLM&#26377;&#26102;&#20250;&#20135;&#29983;&#19981;&#20934;&#30830;&#65288;&#29978;&#33267;&#21487;&#33021;&#20855;&#26377;&#35823;&#23548;&#24615;&#65289;&#30340;&#25991;&#26412;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#65292;&#36825;&#21487;&#33021;&#20351;LLM&#22312;&#26368;&#22909;&#24773;&#20917;&#19979;&#26080;&#27861;&#20351;&#29992;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#20250;&#24102;&#26469;&#21361;&#38505;&#12290;&#23545;&#20110;LLM&#30340;&#30410;&#22788;&#21644;&#39118;&#38505;&#30340;&#22823;&#22810;&#25968;&#35752;&#35770;&#19982;&#20855;&#20307;&#24212;&#29992;&#33073;&#31163;&#20102;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#23450;&#24615;&#25551;&#36848;LLM&#22312;&#21327;&#21161;&#21046;&#20316;&#21307;&#23398;&#35777;&#25454;&#32508;&#36848;&#26041;&#38754;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#12290;&#25105;&#20204;&#23545;16&#20301;&#22269;&#38469;&#19987;&#23478;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical systematic reviews are crucial for informing clinical decision making and healthcare policy. But producing such reviews is onerous and time-consuming. Thus, high-quality evidence synopses are not available for many questions and may be outdated even when they are available. Large language models (LLMs) are now capable of generating long-form texts, suggesting the tantalizing possibility of automatically generating literature reviews on demand. However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucinating or omitting important information. In the healthcare context, this may render LLMs unusable at best and dangerous at worst. Most discussion surrounding the benefits and risks of LLMs have been divorced from specific applications. In this work, we seek to qualitatively characterize the potential utility and risks of LLMs for assisting in production of medical evidence reviews. We conducted 16 semi-structured interviews with international experts
&lt;/p&gt;</description></item><item><title>STOAT&#27169;&#22411;&#26159;&#34920;&#26684;&#21644;&#25512;&#29702;&#24847;&#35782;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#25968;&#23383;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#26102;&#38388;&#25512;&#29702;&#12289;&#34920;&#26684;&#30693;&#35782;&#21644;&#23454;&#20307;&#30693;&#35782;&#26041;&#38754;&#26377;&#36739;&#22909;&#30340;&#25511;&#21046;&#65292;&#25552;&#39640;&#20102;&#20998;&#26512;&#21477;&#23376;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.11826</link><description>&lt;p&gt;
STOAT: &#32467;&#26500;&#21270;&#25968;&#25454;&#25511;&#21046;&#24615;&#20998;&#26512;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
STOAT: Structured Data to Analytical Text With Controls. (arXiv:2305.11826v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11826
&lt;/p&gt;
&lt;p&gt;
STOAT&#27169;&#22411;&#26159;&#34920;&#26684;&#21644;&#25512;&#29702;&#24847;&#35782;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#25968;&#23383;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#26102;&#38388;&#25512;&#29702;&#12289;&#34920;&#26684;&#30693;&#35782;&#21644;&#23454;&#20307;&#30693;&#35782;&#26041;&#38754;&#26377;&#36739;&#22909;&#30340;&#25511;&#21046;&#65292;&#25552;&#39640;&#20102;&#20998;&#26512;&#21477;&#23376;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#38656;&#35201;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#20197;&#29983;&#25104;&#25551;&#36848;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#34920;&#29616;&#20986;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#20363;&#22914;&#34920;&#26684;&#65289;&#29983;&#25104;&#20998;&#26512;&#25991;&#26412;&#12290;&#22312;&#65288;Gupta et al.,2020&#65289;&#25552;&#20986;&#30340;&#20998;&#31867;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#20197;&#19979;&#25512;&#29702;&#31867;&#21035;&#30340;&#21487;&#25511;&#21046;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;&#65306;&#25968;&#23383;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#26102;&#38388;&#25512;&#29702;&#12289;&#34920;&#26684;&#30693;&#35782;&#21644;&#23454;&#20307;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STOAT&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#34920;&#26684;&#21644;&#25512;&#29702;&#24847;&#35782;&#65292;&#24182;&#36890;&#36807;&#30690;&#37327;&#37327;&#21270;&#23558;&#32473;&#23450;&#30340;&#25512;&#29702;&#31867;&#21035;&#27880;&#20837;&#36755;&#20986;&#20013;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#20998;&#26512;&#21477;&#23376;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;iToTTo&#21644;Infotabs&#30340;PARENT&#25351;&#26631;&#19978;&#20998;&#21035;&#25552;&#20379;&#20102;10.19&#65285;&#21644;1.13&#65285;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#25551;&#36848;&#26356;&#21152;&#20934;&#30830;&#21644;&#20998;&#26512;&#65292;&#20154;&#31867;&#35780;&#20272;&#20013;&#22686;&#21152;&#20102;15.3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent language models have made tremendous progress in the structured data to text generation task. However, these models still give sub-optimal performance where logical inference is required to generate the descriptions. In this work, we specifically focus on analytical text generation from structured data such as tables. Building on the taxonomy proposed in (Gupta et al., 2020) we focus on controllable table to text generation for the following reasoning categories: numerical reasoning, commonsense reasoning, temporal reasoning, table knowledge, and entity knowledge. We propose STOAT model, which is table and reasoning aware, with vector-quantization to infuse the given reasoning categories in the output. We observe that our model provides 10.19%, 1.13% improvement on the PARENT metric in iToTTo and Infotabs for the analytical sentence task. We also found that our model generates 15.3% more faithful and analytical descriptions as compared to the baseline models in human evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30528;&#37325;&#30740;&#31350;&#20102;NMT&#20013;&#32463;&#24120;&#20351;&#29992;&#30340;&#20266;&#26631;&#31614;&#35757;&#32451;&#65288;PLT&#65289;&#65292;&#21457;&#29616;PLT&#19981;&#20165;&#20250;&#24433;&#21709;&#36136;&#37327;&#65292;&#32780;&#19988;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#27169;&#22411;&#26356;&#26032;&#21644;&#36755;&#20837;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#65292;&#24418;&#25104;&#27169;&#22411;&#24815;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11808</link><description>&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#20266;&#26631;&#31614;&#35757;&#32451;&#21644;&#27169;&#22411;&#24815;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Label Training and Model Inertia in Neural Machine Translation. (arXiv:2305.11808v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30528;&#37325;&#30740;&#31350;&#20102;NMT&#20013;&#32463;&#24120;&#20351;&#29992;&#30340;&#20266;&#26631;&#31614;&#35757;&#32451;&#65288;PLT&#65289;&#65292;&#21457;&#29616;PLT&#19981;&#20165;&#20250;&#24433;&#21709;&#36136;&#37327;&#65292;&#32780;&#19988;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#27169;&#22411;&#26356;&#26032;&#21644;&#36755;&#20837;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#65292;&#24418;&#25104;&#27169;&#22411;&#24815;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;&#35768;&#22810;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#19968;&#26679;&#65292;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#21463;&#30410;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#34987;&#35266;&#23519;&#21040;&#26159;&#33030;&#24369;&#30340;&#65306;NMT&#27169;&#22411;&#39044;&#27979;&#23545;&#23567;&#30340;&#36755;&#20837;&#21464;&#21270;&#25935;&#24863;&#65292;&#24182;&#19988;&#22312;&#37325;&#26032;&#35757;&#32451;&#25110;&#22686;&#37327;&#27169;&#22411;&#26356;&#26032;&#26102;&#20250;&#26174;&#31034;&#26174;&#30528;&#21464;&#21270;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;NMT&#20013;&#32463;&#24120;&#20351;&#29992;&#30340;&#20266;&#26631;&#31614;&#35757;&#32451;&#65288;PLT&#65289;&#65292;&#23427;&#26159;&#21069;&#21521;&#32763;&#35793;&#65288;&#25110;&#33258;&#25105;&#35757;&#32451;&#65289;&#21644;&#24207;&#21015;&#32423;&#30693;&#35782;&#33976;&#39311;&#31561;&#30456;&#20851;&#25216;&#26415;&#30340;&#24120;&#35265;&#35757;&#32451;&#26041;&#27861;&#12290;&#23613;&#31649;PLT&#23545;&#36136;&#37327;&#30340;&#24433;&#21709;&#26377;&#20805;&#20998;&#30340;&#25991;&#29486;&#35760;&#36733;&#65292;&#20294;&#25105;&#20204;&#24378;&#35843;&#19968;&#20010;&#36739;&#23569;&#34987;&#20154;&#20102;&#35299;&#30340;&#24433;&#21709;&#65306;PLT&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#27169;&#22411;&#26356;&#26032;&#21644;&#36755;&#20837;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#65292;&#24418;&#25104;&#19968;&#32452;&#25105;&#20204;&#31216;&#20043;&#20026;&#27169;&#22411;&#24815;&#24615;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#35757;&#32451;&#35774;&#32622;&#19979;&#30340;&#24815;&#24615;&#25928;&#24212;&#65292;&#24182;&#30830;&#23450;&#20102;&#20998;&#24067;&#31616;&#21270;&#26159;&#35266;&#23519;&#32467;&#26524;&#32972;&#21518;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like many other machine learning applications, neural machine translation (NMT) benefits from over-parameterized deep neural models. However, these models have been observed to be brittle: NMT model predictions are sensitive to small input changes and can show significant variation across re-training or incremental model updates. This work studies a frequently used method in NMT, pseudo-label training (PLT), which is common to the related techniques of forward-translation (or self-training) and sequence-level knowledge distillation. While the effect of PLT on quality is well-documented, we highlight a lesser-known effect: PLT can enhance a model's stability to model updates and input perturbations, a set of properties we call model inertia. We study inertia effects under different training settings and we identify distribution simplification as a mechanism behind the observed results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#22810;&#31181;&#31070;&#32463;&#21487;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#29992;&#20110;&#35299;&#37322;&#26368;&#20808;&#36827;&#24494;&#35843;&#31070;&#32463;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#24230;&#37327;&#29992;&#26469;&#21033;&#29992;&#20196;&#20154;&#30452;&#25509;&#24402;&#22240;&#20110;&#32763;&#35793;&#38169;&#35823;&#30340;&#20196;&#29260;&#32423;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.11806</link><description>&lt;p&gt;
&#20869;&#24149;&#25581;&#31192;&#65306;&#26356;&#22909;&#29702;&#35299;&#26426;&#22120;&#32763;&#35793;&#31070;&#32463;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics. (arXiv:2305.11806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#22810;&#31181;&#31070;&#32463;&#21487;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#29992;&#20110;&#35299;&#37322;&#26368;&#20808;&#36827;&#24494;&#35843;&#31070;&#32463;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#24230;&#37327;&#29992;&#26469;&#21033;&#29992;&#20196;&#20154;&#30452;&#25509;&#24402;&#22240;&#20110;&#32763;&#35793;&#38169;&#35823;&#30340;&#20196;&#29260;&#32423;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#30340;&#31070;&#32463;&#24230;&#37327;&#65288;&#22914; COMET&#65289;&#22312;&#19982;&#20256;&#32479;&#22522;&#20110;&#35789;&#27719;&#37325;&#21472;&#30340;&#24230;&#37327;&#65288;&#22914; BLEU&#65289;&#30456;&#27604;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#35780;&#20272;&#25351;&#26631;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#8220;&#40657;&#30418;&#23376;&#8221;&#65292;&#21482;&#36820;&#22238;&#21333;&#20010;&#21477;&#23376;&#32423;&#21035;&#24471;&#20998;&#65292;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#21644;&#27604;&#36739;&#20102;&#22810;&#31181;&#31070;&#32463;&#21487;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#29992;&#20110;&#35299;&#37322;&#26368;&#20808;&#36827;&#30340;&#24494;&#35843;&#31070;&#32463;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#24230;&#37327;&#21033;&#29992;&#30340;&#20196;&#20154;&#30452;&#25509;&#24402;&#22240;&#20110;&#32763;&#35793;&#38169;&#35823;&#30340;&#20196;&#29260;&#32423;&#20449;&#24687;&#65292;&#36890;&#36807;&#27604;&#36739;&#20196;&#29260;&#32423;&#31070;&#32463;&#26174;&#33879;&#24615;&#22270;&#19982;&#22810;&#32500;&#36136;&#37327;&#24230;&#37327;&#65288;MQM&#65289;&#27880;&#37322;&#21644;&#21512;&#25104;&#20005;&#37325;&#32763;&#35793;&#38169;&#35823;&#12290;&#20026;&#20102;&#26041;&#20415;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#22312;&#27492;&#21457;&#24067;&#25105;&#20204;&#30340;&#20195;&#30721;&#65306; https://github.com/Unbabel/COMET/tree/explainable-metrics&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural metrics for machine translation evaluation, such as COMET, exhibit significant improvements in their correlation with human judgments, as compared to traditional metrics based on lexical overlap, such as BLEU. Yet, neural metrics are, to a great extent, "black boxes" returning a single sentence-level score without transparency about the decision-making process. In this work, we develop and compare several neural explainability methods and demonstrate their effectiveness for interpreting state-of-the-art fine-tuned neural metrics. Our study reveals that these metrics leverage token-level information that can be directly attributed to translation errors, as assessed through comparison of token-level neural saliency maps with Multidimensional Quality Metrics (MQM) annotations and with synthetically-generated critical translation errors. To ease future research, we release our code at: https://github.com/Unbabel/COMET/tree/explainable-metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24605;&#36335;&#38142;&#32034;&#24341;&#30340;&#26041;&#24335;&#26469;&#21709;&#24212;&#29992;&#25143;&#29366;&#24577;&#65292;&#20197;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#21644;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;&#32780;&#38750;&#27979;&#35797;&#26597;&#35810;&#20570;&#20013;&#38388;&#25512;&#29702;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.11792</link><description>&lt;p&gt;
LLM&#30340;&#24605;&#36335;&#38142;&#32034;&#24341;&#29992;&#20110;&#22238;&#31572;&#28145;&#20837;&#23545;&#35805;&#38382;&#39064;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought prompting for responding to in-depth dialogue questions with LLM. (arXiv:2305.11792v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24605;&#36335;&#38142;&#32034;&#24341;&#30340;&#26041;&#24335;&#26469;&#21709;&#24212;&#29992;&#25143;&#29366;&#24577;&#65292;&#20197;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#21644;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;&#32780;&#38750;&#27979;&#35797;&#26597;&#35810;&#20570;&#20013;&#38388;&#25512;&#29702;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#25552;&#38382;&#30340;&#26041;&#24335;&#21644;&#20869;&#23481;&#21487;&#20197;&#27934;&#23519;&#20182;&#20204;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#21253;&#25324;&#20154;&#26684;&#12289;&#24773;&#24863;&#21644;&#24515;&#29702;&#29366;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24605;&#36335;&#38142;&#32034;&#24341;&#30340;&#26041;&#24335;&#26469;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#65292;&#20197;&#21709;&#24212;&#29992;&#25143;&#29366;&#24577;&#65292;&#20197;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#21644;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;6&#20010;&#33521;&#35821;&#21644;&#20013;&#25991;&#30340;&#23545;&#35805;&#25110;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#29992;&#25143;&#29366;&#24577;&#30340;3&#20010;&#19981;&#21516;&#26041;&#38754;&#65288;&#21253;&#25324;&#20154;&#26684;&#12289;&#24773;&#24863;&#21644;&#24515;&#29702;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20851;&#20110;&#29992;&#25143;&#29366;&#24577;&#30340;&#21709;&#24212;&#20316;&#20026;&#20013;&#38388;&#25512;&#29702;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20013;&#38388;&#25512;&#29702;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#32780;&#38750;&#27979;&#35797;&#26597;&#35810;&#30340;&#26032;&#39062;&#28436;&#31034;&#36873;&#25321;&#31574;&#30053;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The way and content in which users ask questions can provide insight into their current status, including their personality, emotions, and psychology. Instead of directly prompting the large language models (LLMs), we explore how chain-of-thought prompting helps in this scenario to perform reasoning and planning according to user status, aiming to provide a more personalized and engaging experience for the user query. To this end, we first construct a benchmark of 6 dialogue or question-answering datasets in both English and Chinese, covering 3 different aspects of user status (\textit{including} \textit{personality}, \textit{emotion}, and \textit{psychology}). Then we prompt the LLMs to generate the response regarding the user status as intermediate reasoning processing. We propose a novel demonstration selection strategy using the semantic similarity of intermediate reasoning instead of test queries. To evaluate the effectiveness and robustness of our approach, we conduct extensive e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt Ordering&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#25552;&#39640;Few-shot NER&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#19981;&#21516;&#20294;&#21512;&#29702;&#30340;&#30446;&#26631;&#24207;&#21015;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#25193;&#23637;&#24615;&#22909;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.11791</link><description>&lt;p&gt;
&#22522;&#20110;Prompt Ordering&#30340;&#25968;&#25454;&#22686;&#24378;&#22686;&#24378;Few-shot NER&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Enhancing Few-shot NER with Prompt Ordering based Data Augmentation. (arXiv:2305.11791v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt Ordering&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#25552;&#39640;Few-shot NER&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#19981;&#21516;&#20294;&#21512;&#29702;&#30340;&#30446;&#26631;&#24207;&#21015;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#25193;&#23637;&#24615;&#22909;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;, &#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;Few-shot&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#20219;&#21153;&#26377;&#25928;.&#28982;&#32780;,&#20256;&#32479;&#30340;NER&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#22823;&#22810;&#38024;&#23545;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#21363;&#26631;&#35760;&#32423;&#20998;&#31867;,&#24182;&#19988;&#20854;&#36866;&#29992;&#24615;&#26377;&#38480;,&#38590;&#20197;&#22788;&#29702;&#23884;&#22871;&#30340;NER&#31561;&#20219;&#21153; .&#26412;&#25991;&#25552;&#20986;,&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#36890;&#36807;&#20026;&#27169;&#22411;&#25552;&#20379;&#26356;&#22810;&#26679;&#21270;&#20294;&#21512;&#29702;&#30340;&#30446;&#26631;&#23454;&#20307;&#24207;&#21015;&#30340;&#26041;&#24335;,&#26469;&#25552;&#39640;NER&#27169;&#22411;&#22312;Few-shot&#20219;&#21153;&#19979;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;,&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Prompt Ordering&#30340;&#31616;&#21333;&#23454;&#29992;&#30340;&#25216;&#26415;,&#23558;&#29983;&#25104;&#30340;&#19981;&#21516;&#30446;&#26631;&#23454;&#20307;&#24207;&#21015;&#25353;&#21487;&#29702;&#35299;&#30340;&#39034;&#24207;&#32452;&#21512;,&#20197;&#26356;&#22909;&#22320;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;NER&#20219;&#21153;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#26412;&#26041;&#27861;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, data augmentation (DA) methods have been proven to be effective for pre-trained language models (PLMs) in low-resource settings, including few-shot named entity recognition (NER). However, conventional NER DA methods are mostly aimed at sequence labeling models, i.e., token-level classification, and few are compatible with unified autoregressive generation frameworks, which can handle a wider range of NER tasks, such as nested NER. Furthermore, these generation frameworks have a strong assumption that the entities will appear in the target sequence with the same left-to-right order as the source sequence. In this paper, we claim that there is no need to keep this strict order, and more diversified but reasonable target entity sequences can be provided during the training stage as a novel DA method. Nevertheless, a naive mixture of augmented data can confuse the model since one source sequence will then be paired with different target sequences. Therefore, we propose a simple 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;&#33021;&#21542;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;&#20266;&#20195;&#30721;&#25552;&#31034;&#21487;&#20197;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#39640;7-16&#20998;&#65292;&#24182;&#30456;&#23545;&#25913;&#21892;12-38%&#12290;</title><link>http://arxiv.org/abs/2305.11790</link><description>&lt;p&gt;
&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Prompting with Pseudo-Code Instructions. (arXiv:2305.11790v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;&#33021;&#21542;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;&#20266;&#20195;&#30721;&#25552;&#31034;&#21487;&#20197;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#39640;7-16&#20998;&#65292;&#24182;&#30456;&#23545;&#25913;&#21892;12-38%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25552;&#31034;&#24050;&#25104;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#37492;&#20110;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#65292;&#22240;&#27492;&#32771;&#34385;&#20351;&#29992;&#26356;&#23569;&#27495;&#20041;&#30340;&#25552;&#31034;&#26679;&#24335;&#65292;&#22914;&#20266;&#20195;&#30721;&#25552;&#31034;&#65292;&#21487;&#33021;&#20855;&#26377;&#20248;&#21183;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;&#26159;&#21542;&#26377;&#21161;&#20110;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25163;&#21160;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;Super-NaturalInstructions&#25968;&#25454;&#38598;&#30340;132&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#20266;&#20195;&#30721;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20998;&#31867;&#12289;QA&#21644;&#29983;&#25104;&#35821;&#35328;&#20219;&#21153;&#12290;&#20351;&#29992;&#36825;&#20123;&#20266;&#20195;&#30721;&#25552;&#31034;&#20197;&#21450;&#23427;&#20204;&#30340;&#33258;&#28982;&#35821;&#35328;&#23545;&#24212;&#29289;&#65292;&#22312;&#20004;&#20010;LLM&#23478;&#26063;-BLOOM&#21644;CodeGen&#19978;&#30740;&#31350;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;&#20250;&#24102;&#26469;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#23545;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;F1&#20998;&#25968;&#24179;&#22343;&#22686;&#21152;&#65288;&#32477;&#23545;&#20540;&#65289;7-16&#20998;&#65292;&#30456;&#23545;&#25913;&#21892;12-38%&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting with natural language instructions has recently emerged as a popular method of harnessing the capabilities of large language models. Given the inherent ambiguity present in natural language, it is intuitive to consider the possible advantages of prompting with less ambiguous prompt styles, such as the use of pseudo-code.  In this paper we explore if prompting via pseudo-code instructions helps improve the performance of pre-trained language models. We manually create a dataset of pseudo-code prompts for 132 different tasks spanning classification, QA and generative language tasks, sourced from the Super-NaturalInstructions dataset. Using these prompts along with their counterparts in natural language, we study their performance on two LLM families - BLOOM and CodeGen. Our experiments show that using pseudo-code instructions leads to better results, with an average increase (absolute) of 7-16 points in F1 scores for classification tasks and an improvement (relative) of 12-38% 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35752;&#35770;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20154;&#26426;&#21327;&#20316;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38590;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#36827;&#34892;&#23545;&#35805;&#24182;&#20462;&#27491;&#39044;&#27979;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#19982;&#20154;&#31867;&#30340;&#35752;&#35770;&#25552;&#39640;&#20934;&#30830;&#24615;&#39640;&#36798;25%&#12290;</title><link>http://arxiv.org/abs/2305.11789</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#26426;&#21327;&#20316;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38590;&#39064;&#65306;&#19968;&#31181;&#22522;&#20110;&#35752;&#35770;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach. (arXiv:2305.11789v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35752;&#35770;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20154;&#26426;&#21327;&#20316;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38590;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#36827;&#34892;&#23545;&#35805;&#24182;&#20462;&#27491;&#39044;&#27979;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#19982;&#20154;&#31867;&#30340;&#35752;&#35770;&#25552;&#39640;&#20934;&#30830;&#24615;&#39640;&#36798;25%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#35752;&#35770;&#12289;&#35299;&#37322;&#24182;&#30456;&#20114;&#36190;&#21516;&#25110;&#21453;&#23545;&#31561;&#26041;&#24335;&#20849;&#21516;&#35299;&#20915;&#20849;&#21516;&#38382;&#39064;&#12290;&#21516;&#26679;&#65292;&#22914;&#26524;&#31995;&#32479;&#22312;&#35299;&#20915;&#20219;&#21153;&#26102;&#33021;&#19982;&#20154;&#31867;&#36827;&#34892;&#35752;&#35770;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;&#22312;&#20043;&#21069;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20013;&#65292;&#31995;&#32479;&#21482;&#33021;&#20570;&#20986;&#39044;&#27979;&#65292;&#20154;&#31867;&#21482;&#33021;&#23601;&#36825;&#20123;&#39044;&#27979;&#25552;&#38382;&#65292;&#32780;&#27809;&#26377;&#24444;&#27492;&#38388;&#30340;&#24847;&#35265;&#20132;&#25442;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#20351;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#23545;&#35805;&#36827;&#34892;&#35752;&#35770;&#21644;&#20462;&#27491;&#39044;&#27979;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#19982;&#20154;&#31867;&#36827;&#34892;&#26377;&#30410;&#30340;&#35752;&#35770;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#39640;&#36798;25&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans work together to solve common problems by having discussions, explaining, and agreeing or disagreeing with each other. Similarly, if a system can have discussions with humans when solving tasks, it can improve the system's performance and reliability. In previous research on explainability, it has only been possible for the system to make predictions and for humans to ask questions about them rather than having a mutual exchange of opinions. This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue. Through experiments, we show that the proposed system can have beneficial discussions with humans improving the accuracy by up to 25 points in the natural language inference task.
&lt;/p&gt;</description></item><item><title>DMDD&#26159;&#19968;&#20010;&#20844;&#24320;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25968;&#25454;&#38598;&#25552;&#21450;&#26816;&#27979;&#20219;&#21153;&#65292;&#21253;&#21547;31,219&#31687;&#31185;&#23398;&#25991;&#31456;&#21644;&#36229;&#36807;449,000&#20010;&#24369;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#25552;&#21450;&#12290;&#35813;&#25968;&#25454;&#38598;&#20026;&#35813;&#20219;&#21153;&#24314;&#31435;&#20102;&#22522;&#20934;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#30830;&#23450;&#20102;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11779</link><description>&lt;p&gt;
DMDD&#65306;&#38754;&#21521;&#25968;&#25454;&#38598;&#25552;&#21450;&#26816;&#27979;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DMDD: A Large-Scale Dataset for Dataset Mentions Detection. (arXiv:2305.11779v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11779
&lt;/p&gt;
&lt;p&gt;
DMDD&#26159;&#19968;&#20010;&#20844;&#24320;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25968;&#25454;&#38598;&#25552;&#21450;&#26816;&#27979;&#20219;&#21153;&#65292;&#21253;&#21547;31,219&#31687;&#31185;&#23398;&#25991;&#31456;&#21644;&#36229;&#36807;449,000&#20010;&#24369;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#25552;&#21450;&#12290;&#35813;&#25968;&#25454;&#38598;&#20026;&#35813;&#20219;&#21153;&#24314;&#31435;&#20102;&#22522;&#20934;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#30830;&#23450;&#20102;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#21517;&#31216;&#35782;&#21035;&#26159;&#31185;&#23398;&#25991;&#29486;&#33258;&#21160;&#20449;&#24687;&#25552;&#21462;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;&#21644;&#35782;&#21035;&#30740;&#31350;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#25968;&#25454;&#38598;&#25552;&#21450;&#26816;&#27979;&#30340;&#35821;&#26009;&#24211;&#22312;&#22823;&#23567;&#21644;&#21629;&#21517;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#25552;&#21450;&#26816;&#27979;&#25968;&#25454;&#38598;&#65288;DMDD&#65289;&#65292;&#36825;&#26159;&#24403;&#21069;&#26368;&#22823;&#30340;&#20844;&#24320;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#22788;&#29702;&#36825;&#39033;&#20219;&#21153;&#12290;DMDD&#30001;DMDD&#20027;&#35201;&#35821;&#26009;&#24211;&#21644;&#35780;&#20272;&#38598;&#32452;&#25104;&#65292;&#20854;&#20013;DMDD&#20027;&#35201;&#35821;&#26009;&#24211;&#21253;&#25324;31,219&#31687;&#31185;&#23398;&#25991;&#31456;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;449,000&#20010;&#25968;&#25454;&#38598;&#25552;&#21450;&#24369;&#27880;&#37322;&#26684;&#24335;&#30340;&#25991;&#26412;&#27573;&#65292;&#35780;&#20272;&#38598;&#21253;&#25324;450&#31687;&#25163;&#21160;&#27880;&#37322;&#30340;&#31185;&#23398;&#25991;&#31456;&#65292;&#29992;&#20110;&#35780;&#20272;&#30446;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;DMDD&#24314;&#31435;&#20102;&#25968;&#25454;&#38598;&#25552;&#21450;&#26816;&#27979;&#21644;&#38142;&#25509;&#30340;&#22522;&#20934;&#24615;&#33021;&#12290;&#36890;&#36807;&#20998;&#26512;&#21508;&#31181;&#27169;&#22411;&#22312;DMDD&#19978;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#25968;&#25454;&#38598;&#25552;&#21450;&#26816;&#27979;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36992;&#35831;&#31038;&#21306;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20316;&#20026;&#25361;&#25112;&#65292;&#24320;&#21457;&#26032;&#30340;&#25968;&#25454;&#38598;&#25552;&#21450;&#26816;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recognition of dataset names is a critical task for automatic information extraction in scientific literature, enabling researchers to understand and identify research opportunities. However, existing corpora for dataset mention detection are limited in size and naming diversity. In this paper, we introduce the Dataset Mentions Detection Dataset (DMDD), the largest publicly available corpus for this task. DMDD consists of the DMDD main corpus, comprising 31,219 scientific articles with over 449,000 dataset mentions weakly annotated in the format of in-text spans, and an evaluation set, which comprises of 450 scientific articles manually annotated for evaluation purposes. We use DMDD to establish baseline performance for dataset mention detection and linking. By analyzing the performance of various models on DMDD, we are able to identify open problems in dataset mention detection. We invite the community to use our dataset as a challenge to develop novel dataset mention detection mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#20013;&#20351;&#29992;&#36328;&#35821;&#31181;&#24182;&#34892;&#25968;&#25454;&#33021;&#25552;&#39640;&#20854;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65307;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#23398;&#20064;&#20004;&#20010;&#30446;&#26631;&#20043;&#38388;&#30340;&#26368;&#20339;&#28151;&#21512;&#27604;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.11778</link><description>&lt;p&gt;
&#12298;&#36328;&#35821;&#31181;&#30417;&#30563;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#36136;&#37327;&#12299;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Supervision improves Large Language Models Pre-training. (arXiv:2305.11778v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#20013;&#20351;&#29992;&#36328;&#35821;&#31181;&#24182;&#34892;&#25968;&#25454;&#33021;&#25552;&#39640;&#20854;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65307;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#23398;&#20064;&#20004;&#20010;&#30446;&#26631;&#20043;&#38388;&#30340;&#26368;&#20339;&#28151;&#21512;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36805;&#36895;&#36827;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;&#20351;&#29992;&#33258;&#30417;&#30563;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#65288;&#22914;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#25110;&#36328;&#24230;&#25439;&#22351;&#65289;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20027;&#35201;&#20351;&#29992;&#38656;&#35201;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#23545;&#40784;&#25968;&#25454;&#30340;&#36328;&#35821;&#31181;&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#33258;&#30417;&#30563;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#21644;&#21463;&#30417;&#30563;&#30340;&#26426;&#22120;&#32763;&#35793;&#30446;&#26631;&#28151;&#21512;&#65292;&#22240;&#27492;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#21253;&#21547;&#36328;&#35821;&#31181;&#24182;&#34892;&#25968;&#25454;&#65292;&#21487;&#20135;&#29983;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#26159;&#38750;&#24120;&#36164;&#28304;&#23494;&#38598;&#30340;&#36807;&#31243;&#65292;&#32780;&#22312;&#20004;&#20010;&#30446;&#26631;&#20043;&#38388;&#36827;&#34892;&#26368;&#20339;&#28151;&#21512;&#27604;&#20363;&#30340;&#32593;&#26684;&#25628;&#32034;&#26159;&#38750;&#24120;&#26114;&#36149;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#22312;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#36825;&#20123;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent rapid progress in pre-training Large Language Models has relied on using self-supervised language modeling objectives like next token prediction or span corruption. On the other hand, Machine Translation Systems are mostly trained using cross-lingual supervision that requires aligned data between source and target languages. We demonstrate that pre-training Large Language Models on a mixture of a self-supervised Language Modeling objective and the supervised Machine Translation objective, therefore including cross-lingual parallel data during pre-training, yields models with better in-context learning abilities. As pre-training is a very resource-intensive process and a grid search on the best mixing ratio between the two objectives is prohibitively expensive, we propose a simple yet effective strategy to learn it during pre-training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JADE&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#20197;&#25552;&#21319;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#23545;&#40784;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#36827;&#34892;&#35270;&#35273;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.11769</link><description>&lt;p&gt;
&#25552;&#21319;&#32852;&#21512;&#23398;&#20064;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65306;&#22522;&#20110;&#32852;&#21512;&#23398;&#20064;&#30340;&#38382;&#31572;&#19982;&#23494;&#38598;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner. (arXiv:2305.11769v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JADE&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#20197;&#25552;&#21319;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#23545;&#40784;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#36827;&#34892;&#35270;&#35273;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#21253;&#25324;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12289;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#21644;&#35270;&#35273;&#38382;&#31572;&#31561;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#20174;&#32593;&#32476;&#19978;&#25910;&#38598;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20316;&#20026;&#39044;&#20808;&#35757;&#32451;&#30340;&#25968;&#25454;&#65292;&#24573;&#35270;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#38656;&#35201;&#32454;&#31890;&#24230;&#29305;&#24449;&#23545;&#40784;&#30340;&#38656;&#27714;&#65292;&#36825;&#38656;&#35201;&#23545;&#22270;&#20687;&#21644;&#35821;&#35328;&#34920;&#36798;&#36827;&#34892;&#35814;&#32454;&#30340;&#29702;&#35299;&#12290;&#23558;&#35270;&#35273;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#38598;&#25104;&#21040;&#39044;&#20808;&#35757;&#32451;&#20013;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#33719;&#21462;&#22270;&#20687;-&#38382;&#39064;-&#31572;&#26696;&#20197;&#21450;&#22270;&#20687;-&#20301;&#32622;-&#23383;&#24149;&#19977;&#20803;&#32452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#12290;&#27492;&#22806;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#35270;&#35273;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#25968;&#25454;&#38598;&#36890;&#24120;&#30001;&#20110;&#25163;&#21160;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#27880;&#32780;&#35268;&#27169;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#32852;&#21512;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#29983;&#25104;&#65288;JADE&#65289;&#65292;&#23427;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#26131;&#20110;&#33719;&#21462;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained multimodal models have demonstrated significant success in a range of downstream tasks, including image captioning, image-text retrieval, visual question answering (VQA), etc. However, many of these methods rely on image-text pairs collected from the web as pre-training data and unfortunately overlook the need for fine-grained feature alignment between vision and language modalities, which requires detailed understanding of images and language expressions. While integrating VQA and dense captioning (DC) into pre-training can address this issue, acquiring image-question-answer as well as image-location-caption triplets is challenging and time-consuming. Additionally, publicly available datasets for VQA and dense captioning are typically limited in scale due to manual data collection and labeling efforts. In this paper, we propose a novel method called Joint QA and DC GEneration (JADE), which utilizes a pre-trained multimodal model and easily-crawled image-text pairs to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;3D&#22330;&#26223;&#29305;&#24449;&#32435;&#20837;VSD&#26041;&#27861;&#65292;&#26500;&#24314;&#30446;&#26631;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;3D&#31354;&#38388;&#22330;&#26223;&#22270;(Go3D-S2G)&#65292;&#25552;&#20986;&#22810;&#26679;&#21270;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11768</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#20307;3D&#22330;&#26223;&#29702;&#35299;&#29983;&#25104;&#35270;&#35273;&#31354;&#38388;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Generating Visual Spatial Description via Holistic 3D Scene Understanding. (arXiv:2305.11768v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;3D&#22330;&#26223;&#29305;&#24449;&#32435;&#20837;VSD&#26041;&#27861;&#65292;&#26500;&#24314;&#30446;&#26631;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;3D&#31354;&#38388;&#22330;&#26223;&#22270;(Go3D-S2G)&#65292;&#25552;&#20986;&#22810;&#26679;&#21270;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#31354;&#38388;&#25551;&#36848;(VSD)&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#25551;&#36848;&#22270;&#20687;&#20013;&#32473;&#23450;&#23545;&#35937;&#31354;&#38388;&#20851;&#31995;&#30340;&#25991;&#26412;&#12290;&#29616;&#26377;&#30340;VSD&#24037;&#20316;&#20165;&#27169;&#25311;2D&#20960;&#20309;&#35270;&#35273;&#29305;&#24449;&#65292;&#22240;&#27492;&#19981;&#21487;&#36991;&#20813;&#22320;&#38519;&#20837;&#30446;&#26631;&#23545;&#35937;&#31354;&#38388;&#29702;&#35299;&#20542;&#26012;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;3D&#22330;&#26223;&#29305;&#24449;&#32435;&#20837;VSD&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22806;&#37096;3D&#22330;&#26223;&#25552;&#21462;&#22120;&#65292;&#25105;&#20204;&#33719;&#21462;&#36755;&#20837;&#22270;&#20687;&#30340;3D&#23545;&#35937;&#21644;&#22330;&#26223;&#29305;&#24449;&#65292;&#22522;&#20110;&#27492;&#26500;&#24314;&#30446;&#26631;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;3D&#31354;&#38388;&#22330;&#26223;&#22270;(Go3D-S2G)&#65292;&#20174;&#32780;&#27169;&#25311;&#30446;&#26631;&#23545;&#35937;&#22312;&#25972;&#20307;3D&#22330;&#26223;&#20013;&#30340;&#31354;&#38388;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22330;&#26223;&#23376;&#22270;&#36873;&#25321;&#26426;&#21046;&#65292;&#20174;Go3D-S2G&#20013;&#37319;&#26679;&#25299;&#25169;&#22810;&#26679;&#30340;&#23376;&#22270;&#65292;&#23548;&#33322;&#19981;&#21516;&#30340;&#23616;&#37096;&#32467;&#26500;&#29305;&#24449;&#20197;&#20135;&#29983;&#31354;&#38388;&#22810;&#26679;&#21270;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#23545;&#20004;&#20010;VSD&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#65292;&#29305;&#21035;&#26159;&#22312;one-split&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual spatial description (VSD) aims to generate texts that describe the spatial relations of the given objects within images. Existing VSD work merely models the 2D geometrical vision features, thus inevitably falling prey to the problem of skewed spatial understanding of target objects. In this work, we investigate the incorporation of 3D scene features for VSD. With an external 3D scene extractor, we obtain the 3D objects and scene features for input images, based on which we construct a target object-centered 3D spatial scene graph (Go3D-S2G), such that we model the spatial semantics of target objects within the holistic 3D scenes. Besides, we propose a scene subgraph selecting mechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the diverse local structure features are navigated to yield spatially-diversified text generation. Experimental results on two VSD datasets demonstrate that our framework outperforms the baselines significantly, especially improving on
&lt;/p&gt;</description></item><item><title>ReSeTOX&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#23398;&#20064;&#20851;&#27880;&#26435;&#37325;&#65292;&#33021;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#20943;&#23569;&#26426;&#22120;&#32763;&#35793;&#20013;&#26377;&#27602;&#20869;&#23481;&#30340;&#20135;&#29983;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;164&#31181;&#35821;&#35328;&#19978;&#25104;&#21151;&#23558;&#26377;&#27602;&#20869;&#23481;&#21387;&#32553;57%&#12290;</title><link>http://arxiv.org/abs/2305.11761</link><description>&lt;p&gt;
ReSeTOX&#65306;&#37325;&#26032;&#23398;&#20064;&#20851;&#27880;&#26435;&#37325;&#26469;&#20943;&#36731;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#27602;&#24615;
&lt;/p&gt;
&lt;p&gt;
ReSeTOX: Re-learning attention weights for toxicity mitigation in machine translation. (arXiv:2305.11761v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11761
&lt;/p&gt;
&lt;p&gt;
ReSeTOX&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#23398;&#20064;&#20851;&#27880;&#26435;&#37325;&#65292;&#33021;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#20943;&#23569;&#26426;&#22120;&#32763;&#35793;&#20013;&#26377;&#27602;&#20869;&#23481;&#30340;&#20135;&#29983;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;164&#31181;&#35821;&#35328;&#19978;&#25104;&#21151;&#23558;&#26377;&#27602;&#20869;&#23481;&#21387;&#32553;57%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861; ReSeTOX&#65288;REdo SEarch if TOXic&#65289;&#35299;&#20915;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#29983;&#25104;&#21253;&#21547;&#36755;&#20837;&#20013;&#19981;&#23384;&#22312;&#30340;&#26377;&#27602;&#21333;&#35789;&#30340;&#32763;&#35793;&#36755;&#20986;&#30340;&#38382;&#39064;&#12290;&#20854;&#30446;&#30340;&#26159;&#36890;&#36807;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#24335;&#32531;&#35299;&#27602;&#24615;&#35821;&#35328;&#30340;&#24341;&#20837;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#21457;&#29616;&#26377;&#27602;&#65292;ReSeTOX &#20250;&#21160;&#24577;&#35843;&#25972;&#20851;&#38190;&#20540;&#33258;&#27880;&#24847;&#21147;&#26435;&#37325;&#24182;&#37325;&#26032;&#35780;&#20272;&#27874;&#26463;&#25628;&#32034;&#20551;&#35774;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312; 164 &#31181;&#35821;&#35328;&#20013;&#65292;ReSeTOX &#23454;&#29616;&#20102;&#26174;&#33879;&#30340; 57% &#20943;&#23569;&#26377;&#27602;&#20869;&#23481;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#24179;&#22343; 99.5% &#30340;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our proposed method, ReSeTOX (REdo SEarch if TOXic), addresses the issue of Neural Machine Translation (NMT) generating translation outputs that contain toxic words not present in the input. The objective is to mitigate the introduction of toxic language without the need for re-training. In the case of identified added toxicity during the inference process, ReSeTOX dynamically adjusts the key-value self-attention weights and re-evaluates the beam search hypotheses. Experimental results demonstrate that ReSeTOX achieves a remarkable 57% reduction in added toxicity while maintaining an average translation quality of 99.5% across 164 languages.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;Prompt-Tuning&#31574;&#30053;&#26469;&#25511;&#21046;&#20174;&#20013;&#25552;&#21462;&#35760;&#24518;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#25915;&#20987;&#21644;&#38450;&#24481;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#12290;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;GPT-Neo&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25915;&#20987;&#31574;&#30053;&#30456;&#23545;&#20110;&#22522;&#32447;&#20135;&#29983;&#20102;9.3%&#30340;&#25552;&#21462;&#29575;&#22686;&#21152;&#65292;&#32780;&#38450;&#24481;&#31574;&#30053;&#21487;&#20197;&#35843;&#25972;&#20197;&#23454;&#29616;&#19981;&#21516;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#22810;97.7%&#30340;&#25552;&#21462;&#29575;&#20943;&#23569;&#65292;&#20294;&#22256;&#24785;&#24230;&#22686;&#21152;&#20102;16.9%&#12290;</title><link>http://arxiv.org/abs/2305.11759</link><description>&lt;p&gt;
&#36890;&#36807;Prompt-Tuning&#25511;&#21046;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#35760;&#24518;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning. (arXiv:2305.11759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11759
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;Prompt-Tuning&#31574;&#30053;&#26469;&#25511;&#21046;&#20174;&#20013;&#25552;&#21462;&#35760;&#24518;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#25915;&#20987;&#21644;&#38450;&#24481;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#12290;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;GPT-Neo&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25915;&#20987;&#31574;&#30053;&#30456;&#23545;&#20110;&#22522;&#32447;&#20135;&#29983;&#20102;9.3%&#30340;&#25552;&#21462;&#29575;&#22686;&#21152;&#65292;&#32780;&#38450;&#24481;&#31574;&#30053;&#21487;&#20197;&#35843;&#25972;&#20197;&#23454;&#29616;&#19981;&#21516;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#22810;97.7%&#30340;&#25552;&#21462;&#29575;&#20943;&#23569;&#65292;&#20294;&#22256;&#24785;&#24230;&#22686;&#21152;&#20102;16.9%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#30693;&#20250;&#35760;&#24518;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#24182;&#19988;&#31616;&#21333;&#26597;&#35810;&#27169;&#22411;&#21363;&#21487;&#25552;&#21462;&#35760;&#24518;&#30340;&#20869;&#23481;&#65292;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;Prompt-Tuning&#26469;&#25511;&#21046;LLMs&#20013;&#35760;&#24518;&#20869;&#23481;&#30340;&#25552;&#21462;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;Prompt&#35757;&#32451;&#31574;&#30053;&#26469;&#22686;&#21152;&#21644;&#20943;&#23569;&#25552;&#21462;&#29575;&#65292;&#20998;&#21035;&#23545;&#24212;&#25915;&#20987;&#21644;&#38450;&#24481;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;GPT-Neo&#31995;&#21015;&#30340;&#27169;&#22411;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;&#23545;&#20110;1.3B&#21442;&#25968;&#30340;GPT-Neo&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#30456;&#23545;&#20110;&#22522;&#32447;&#20135;&#29983;&#20102;9.3&#20010;&#30334;&#20998;&#28857;&#30340;&#25552;&#21462;&#29575;&#22686;&#21152;&#12290;&#36890;&#36807;&#29992;&#25143;&#25351;&#23450;&#30340;&#36229;&#21442;&#25968;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#21487;&#20197;&#35843;&#25972;&#20197;&#23454;&#29616;&#19981;&#21516;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#12290;&#25105;&#20204;&#30456;&#23545;&#20110;&#22522;&#32447;&#23454;&#29616;&#20102;&#26368;&#22810;97.7%&#30340;&#25552;&#21462;&#29575;&#20943;&#23569;&#65292;&#21516;&#26102;&#22256;&#24785;&#24230;&#22686;&#21152;&#20102;16.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk. We present a novel approach which uses prompt-tuning to control the extraction rates of memorized content in LLMs. We present two prompt training strategies to increase and decrease extraction rates, which correspond to an attack and a defense, respectively. We demonstrate the effectiveness of our techniques by using models from the GPT-Neo family on a public benchmark. For the 1.3B parameter GPT-Neo model, our attack yields a 9.3 percentage point increase in extraction rate compared to our baseline. Our defense can be tuned to achieve different privacy-utility trade-offs by a user-specified hyperparameter. We achieve an extraction rate reduction of up to 97.7% relative to our baseline, with a perplexity increase of 16.9%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65288;HELMA&#65289;&#65292;&#20854;&#20026;&#26631;&#20934;&#21270;&#21644;&#21487;&#38752;&#30340;&#20272;&#31639;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;ChatGPT&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#34920;&#26126;&#20854;&#23384;&#22312;&#24187;&#35273;&#30340;&#39118;&#38505;&#24182;&#20026;&#37492;&#21035;&#21644;&#20943;&#36731;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11747</link><description>&lt;p&gt;
HELMA&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. (arXiv:2305.11747v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65288;HELMA&#65289;&#65292;&#20854;&#20026;&#26631;&#20934;&#21270;&#21644;&#21487;&#38752;&#30340;&#20272;&#31639;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;ChatGPT&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#34920;&#26126;&#20854;&#23384;&#22312;&#24187;&#35273;&#30340;&#39118;&#38505;&#24182;&#20026;&#37492;&#21035;&#21644;&#20943;&#36731;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#65292;&#23481;&#26131;&#29983;&#25104;&#24187;&#35273;&#65292;&#21363;&#19982;&#28304;&#20869;&#23481;&#20914;&#31361;&#25110;&#26080;&#27861;&#36890;&#36807;&#20107;&#23454;&#30693;&#35782;&#36827;&#34892;&#39564;&#35777;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#20102;&#35299;LLMs&#20250;&#20135;&#29983;&#21738;&#31181;&#31867;&#22411;&#30340;&#20869;&#23481;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20250;&#20135;&#29983;&#24187;&#35273;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Hallucination Evaluation for Large Language Models&#65288;HELMA&#65289;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#29983;&#25104;&#30340;&#21644;&#20154;&#24037;&#27880;&#37322;&#30340;&#24187;&#35273;&#26679;&#26412;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#21644;&#20943;&#36731;&#24187;&#35273;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#29983;&#25104;&#36825;&#20123;&#26679;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;ChatGPT&#30340;&#20004;&#27493;&#26694;&#26550;&#65292;&#21363;&#37319;&#26679;-&#36807;&#28388;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#37319;&#26679;&#26041;&#27861;&#22522;&#20110;&#25351;&#20196;&#29983;&#25104;&#24187;&#35273;&#26679;&#26412;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#20010;&#31034;&#20363;&#22686;&#24378;&#36807;&#28388;&#26041;&#27861;&#36873;&#25321;&#26368;&#22909;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32856;&#35831;&#19968;&#20123;&#20154;&#24037;&#26631;&#27880;&#21592;&#26469;&#27880;&#37322;ChatGPT&#21709;&#24212;&#20013;&#30340;&#24187;&#35273;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;ChatGPT&#26377;&#19968;&#23450;&#30340;&#27010;&#29575;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#23384;&#22312;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;HELMA&#22522;&#20934;&#21487;&#20316;&#20026;&#35782;&#21035;&#21644;&#20943;&#36731;LLMs&#24187;&#35273;&#38382;&#39064;&#30340;&#26631;&#20934;&#21270;&#21487;&#38752;&#35780;&#20272;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, \ie content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation for Large Language Models (HELMA) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing and alleviating hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, \ie sampling-then-filtering. Specifically, we first adopt two different sampling methods to generate hallucinated samples based on instructions, and then use an example-enhanced filtering method to select the best one. Furthermore, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT has some probabilities to generate hallucinations and exist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#28085;&#30422;18&#20010;&#32763;&#35793;&#26041;&#21521;&#65292;&#21253;&#25324;&#22810;&#31181;&#36164;&#28304;&#27700;&#24179;&#21644;&#33050;&#26412;&#30340;&#26426;&#22120;&#32763;&#35793;&#20013;&#8220;&#24187;&#35273;&#8221;&#21644;&#36951;&#28431;&#29616;&#35937;&#30340;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#35821;&#35328;&#23545;&#30340;&#34920;&#29616;&#65292;&#20026;&#35813;&#39046;&#22495;&#30740;&#31350;&#25552;&#20379;&#21487;&#38752;&#30340;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.11746</link><description>&lt;p&gt;
HalOmi&#65306;&#26426;&#22120;&#32763;&#35793;&#20013;&#22810;&#35821;&#35328;&#8220;&#24187;&#35273;&#8221;&#21644;&#36951;&#28431;&#26816;&#27979;&#30340;&#25163;&#21160;&#27880;&#37322;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
HalOmi: A Manually Annotated Benchmark for Multilingual Hallucination and Omission Detection in Machine Translation. (arXiv:2305.11746v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#28085;&#30422;18&#20010;&#32763;&#35793;&#26041;&#21521;&#65292;&#21253;&#25324;&#22810;&#31181;&#36164;&#28304;&#27700;&#24179;&#21644;&#33050;&#26412;&#30340;&#26426;&#22120;&#32763;&#35793;&#20013;&#8220;&#24187;&#35273;&#8221;&#21644;&#36951;&#28431;&#29616;&#35937;&#30340;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#35821;&#35328;&#23545;&#30340;&#34920;&#29616;&#65292;&#20026;&#35813;&#39046;&#22495;&#30740;&#31350;&#25552;&#20379;&#21487;&#38752;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#8220;&#24187;&#35273;&#8221;&#25351;&#30340;&#26159;&#23436;&#20840;&#19982;&#36755;&#20837;&#20449;&#24687;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#32780;&#8220;&#36951;&#28431;&#8221;&#26159;&#25351;&#26410;&#21253;&#25324;&#26576;&#20123;&#36755;&#20837;&#20449;&#24687;&#30340;&#32763;&#35793;&#12290;&#23613;&#31649;&#36825;&#20004;&#31181;&#24773;&#20917;&#24448;&#24448;&#26159;&#30772;&#22351;&#29992;&#25143;&#20449;&#20219;&#30340;&#28798;&#38590;&#24615;&#38169;&#35823;&#65292;&#20294;&#36825;&#20123;&#31867;&#22411;&#30340;&#24102;&#27880;&#37322;&#25968;&#25454;&#38750;&#24120;&#31232;&#32570;&#65292;&#24182;&#19988;&#20165;&#38480;&#20110;&#23569;&#25968;&#39640;&#36164;&#28304;&#35821;&#35328;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#28085;&#30422;18&#31181;&#32763;&#35793;&#26041;&#21521;&#30340;&#24187;&#35273;&#21644;&#36951;&#28431;&#29616;&#35937;&#65292;&#20854;&#36164;&#28304;&#27700;&#24179;&#21644;&#33050;&#26412;&#21508;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#30340;&#27880;&#37322;&#28085;&#30422;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#23436;&#20840;&#24187;&#35273;&#12289;&#37096;&#20998;&#24187;&#35273;&#20197;&#21450;&#21477;&#23376;&#21644;&#21333;&#35789;&#19968;&#32423;&#30340;&#36951;&#28431;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37325;&#35775;&#20102;&#20197;&#21069;&#30340;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;&#21333;&#20010;&#35821;&#35328;&#23545;&#24471;&#20986;&#30340;&#32467;&#35770;&#22312;&#22823;&#35268;&#27169;&#35780;&#20272;&#20013;&#24456;&#38590;&#25104;&#31435;&#65292;&#24182;&#24314;&#31435;&#20102;&#26032;&#30340;&#21487;&#38752;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucinations in machine translation are translations that contain information completely unrelated to the input. Omissions are translations that do not include some of the input information. While both cases tend to be catastrophic errors undermining user trust, annotated data with these types of pathologies is extremely scarce and is limited to a few high-resource languages. In this work, we release an annotated dataset for the hallucination and omission phenomena covering 18 translation directions with varying resource levels and scripts. Our annotation covers different levels of partial and full hallucinations as well as omissions both at the sentence and at the word level. Additionally, we revisit previous methods for hallucination and omission detection, show that conclusions made based on a single language pair largely do not hold for a large-scale evaluation, and establish new solid baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#37325;&#25490;&#22120;&#25552;&#20379;&#25512;&#29702;&#26102;&#38388;&#21453;&#39304;&#26469;&#25913;&#36827;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20302;&#21484;&#22238;&#29575;@ K&#19979;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11744</link><description>&lt;p&gt;
&#38754;&#21521;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#30340;&#25512;&#29702;&#26102;&#38388;&#37325;&#25490;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Inference-time Re-ranker Relevance Feedback for Neural Information Retrieval. (arXiv:2305.11744v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#37325;&#25490;&#22120;&#25552;&#20379;&#25512;&#29702;&#26102;&#38388;&#21453;&#39304;&#26469;&#25913;&#36827;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20302;&#21484;&#22238;&#29575;@ K&#19979;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#36890;&#24120;&#37319;&#29992;&#26816;&#32034;&#21644;&#37325;&#25490;&#26694;&#26550;&#65306;&#20808;&#20351;&#29992;&#21452;&#32534;&#30721;&#22120;&#32593;&#32476;&#26816;&#32034;K&#65288;&#20363;&#22914;100&#65289;&#20010;&#20505;&#36873;&#39033;&#65292;&#28982;&#21518;&#20877;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;&#20132;&#21449;&#32534;&#30721;&#22120;&#27169;&#22411;&#23545;&#36825;&#20123;&#20505;&#36873;&#39033;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#20197;&#20351;&#26356;&#22909;&#30340;&#20505;&#36873;&#39033;&#25490;&#21517;&#26356;&#39640;&#12290;&#37325;&#25490;&#22120;&#36890;&#24120;&#20135;&#29983;&#27604;&#26816;&#32034;&#22120;&#26356;&#22909;&#30340;&#20505;&#36873;&#20998;&#25968;&#65292;&#20294;&#20165;&#38480;&#20110;&#26597;&#30475;&#21069;K&#20010;&#26816;&#32034;&#21040;&#30340;&#20505;&#36873;&#39033;&#65292;&#22240;&#27492;&#26080;&#27861;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#65288;&#20197;Recall @ K&#20026;&#24230;&#37327;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#37325;&#25490;&#22120;&#36890;&#36807;&#25552;&#20379;&#25512;&#29702;&#26102;&#38388;&#30456;&#20851;&#21453;&#39304;&#26469;&#25913;&#36827;&#26816;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#37325;&#25490;&#22120;&#30340;&#39044;&#27979;&#23545;&#27979;&#35797;&#23454;&#20363;&#30340;&#37325;&#35201;&#20449;&#24687;&#36827;&#34892;&#20102;&#26816;&#32034;&#22120;&#26597;&#35810;&#34920;&#31034;&#30340;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25512;&#29702;&#26102;&#38388;&#33976;&#39311;&#26469;&#23454;&#29616;&#65292;&#30446;&#30340;&#26159;&#20351;&#26816;&#32034;&#22120;&#30340;&#20505;&#36873;&#20998;&#25968;&#26356;&#25509;&#36817;&#20110;&#37325;&#25490;&#22120;&#30340;&#20998;&#25968;&#12290;&#28982;&#21518;&#20351;&#29992;&#26356;&#26032;&#21518;&#30340;&#26597;&#35810;&#21521;&#37327;&#25191;&#34892;&#31532;&#20108;&#20010;&#26816;&#32034;&#27493;&#39588;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#21484;&#22238;&#29575;@ K&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural information retrieval often adopts a retrieve-and-rerank framework: a bi-encoder network first retrieves K (e.g., 100) candidates that are then re-ranked using a more powerful cross-encoder model to rank the better candidates higher. The re-ranker generally produces better candidate scores than the retriever, but is limited to seeing only the top K retrieved candidates, thus providing no improvements in retrieval performance as measured by Recall@K. In this work, we leverage the re-ranker to also improve retrieval by providing inference-time relevance feedback to the retriever. Concretely, we update the retriever's query representation for a test instance using a lightweight inference-time distillation of the re-ranker's prediction for that instance. The distillation loss is designed to bring the retriever's candidate scores closer to those of the re-ranker. A second retrieval step is then performed with the updated query vector. We empirically show that our approach, which can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19982;&#24037;&#20855;&#30340;&#20132;&#20114;&#26657;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11738</link><description>&lt;p&gt;
CRITIC&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24037;&#20855;&#20132;&#20114;&#25209;&#35780;&#36827;&#34892;&#33258;&#25105;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. (arXiv:2305.11738v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19982;&#24037;&#20855;&#30340;&#20132;&#20114;&#26657;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#38750;&#24120;&#24341;&#20154;&#27880;&#30446;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#65292;&#20363;&#22914;&#20986;&#29616;&#24187;&#35273;&#20107;&#23454;&#65292;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#20195;&#30721;&#25110;&#21019;&#24314;&#20882;&#29359;&#21644;&#26377;&#23475;&#30340;&#20869;&#23481;&#12290;&#19982;&#36825;&#20123;&#27169;&#22411;&#19981;&#21516;&#65292;&#20154;&#31867;&#36890;&#24120;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#26469;&#20132;&#21449;&#26816;&#26597;&#21644;&#31934;&#28860;&#20182;&#20204;&#30340;&#21021;&#27493;&#20869;&#23481;&#65292;&#20363;&#22914;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20107;&#23454;&#26816;&#26597;&#25110;&#20351;&#29992;&#20195;&#30721;&#35299;&#37322;&#22120;&#36827;&#34892;&#35843;&#35797;&#12290;&#21463;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;LLMs&#65288;&#23454;&#36136;&#19978;&#26159;&#8220;&#40657;&#30418;&#23376;&#8221;&#65289;&#20197;&#31867;&#20284;&#20110;&#20154;&#31867;&#19982;&#24037;&#20855;&#20132;&#20114;&#30340;&#26041;&#24335;&#39564;&#35777;&#21644;&#36880;&#27493;&#20462;&#27491;&#33258;&#24049;&#30340;&#36755;&#20986;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#20174;&#21021;&#22987;&#36755;&#20986;&#24320;&#22987;&#65292;CRITIC&#19982;&#36866;&#24403;&#30340;&#24037;&#20855;&#20132;&#20114;&#20197;&#35780;&#20272;&#25991;&#26412;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#28982;&#21518;&#26681;&#25454;&#22312;&#27492;&#39564;&#35777;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#21453;&#39304;&#20462;&#25913;&#36755;&#20986;&#12290;&#28041;&#21450;&#33258;&#30001;&#24418;&#24335;&#38382;&#31572;&#12289;&#25968;&#23398;&#31243;&#24207;&#32508;&#21512;&#21644;&#27602;&#24615;&#26816;&#27979;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;LLMs&#33021;&#22815;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#24182;&#32416;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially "black boxes" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#25991;&#26412;&#34920;&#26684;&#38382;&#31572;&#26694;&#26550;S3HQA&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#35757;&#32451;&#31934;&#32454;&#30340;&#26816;&#32034;&#22120;&#26469;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#37319;&#29992;&#28151;&#21512;&#36873;&#25321;&#22120;&#26469;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#23454;&#38469;&#30693;&#35782;&#24182;&#37319;&#29992;&#22522;&#20110;&#29983;&#25104;&#30340;&#25512;&#29702;&#22120;&#26469;&#33719;&#21462;&#31572;&#26696;&#12290;&#22312;WikiTableQuestions&#21644;ComplexWebQuestions&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11725</link><description>&lt;p&gt;
S3HQA&#65306;&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;-&#34920;&#26684;&#28151;&#21512;&#38382;&#31572;&#30340;&#19977;&#38454;&#27573;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
S$^3$HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering. (arXiv:2305.11725v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#25991;&#26412;&#34920;&#26684;&#38382;&#31572;&#26694;&#26550;S3HQA&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#35757;&#32451;&#31934;&#32454;&#30340;&#26816;&#32034;&#22120;&#26469;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#37319;&#29992;&#28151;&#21512;&#36873;&#25321;&#22120;&#26469;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#23454;&#38469;&#30693;&#35782;&#24182;&#37319;&#29992;&#22522;&#20110;&#29983;&#25104;&#30340;&#25512;&#29702;&#22120;&#26469;&#33719;&#21462;&#31572;&#26696;&#12290;&#22312;WikiTableQuestions&#21644;ComplexWebQuestions&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#31572;&#28041;&#21450;&#25991;&#26412;&#21644;&#34920;&#26684;&#28151;&#21512;&#20107;&#23454;&#30693;&#35782;&#30340;&#22810;&#36339;&#38382;&#39064;(TextTableQA)&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#27169;&#22411;&#20027;&#35201;&#37319;&#29992;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#26694;&#26550;&#65292;&#23384;&#22312;&#20960;&#20010;&#38382;&#39064;&#65292;&#22914;&#35757;&#32451;&#26816;&#32034;&#22120;&#30340;&#22024;&#26434;&#26631;&#31614;&#12289;&#23545;&#25991;&#26412;&#21644;&#34920;&#26684;&#30340;&#24322;&#26500;&#20449;&#24687;&#21033;&#29992;&#19981;&#36275;&#20197;&#21450;&#19981;&#21516;&#25512;&#29702;&#25805;&#20316;&#33021;&#21147;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#25991;&#26412;&#34920;&#26684;&#38382;&#31572;&#26694;&#26550;S3HQA&#65292;&#21253;&#25324;&#26816;&#32034;&#22120;&#12289;&#36873;&#25321;&#22120;&#21644;&#25512;&#29702;&#22120;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#35757;&#32451;&#31934;&#32454;&#30340;&#26816;&#32034;&#22120;&#26469;&#35299;&#20915;&#22024;&#26434;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#19968;&#20010;&#28151;&#21512;&#36873;&#25321;&#22120;&#26469;&#32771;&#34385;&#24322;&#26500;&#25968;&#25454;&#20043;&#38388;&#30340;&#38142;&#25509;&#20851;&#31995;&#65292;&#20197;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#23454;&#38469;&#30693;&#35782;&#12290;&#22312;&#26368;&#21518;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#30340;&#25512;&#29702;&#22120;&#26469;&#33719;&#21462;&#31572;&#26696;&#65292;&#32780;&#19981;&#26159;&#20687;&#20043;&#21069;&#30340;&#26041;&#27861;&#19968;&#26679;&#20351;&#29992;&#38405;&#35835;&#29702;&#35299;&#27169;&#22359;&#12290;&#20854;&#20013;&#21253;&#25324;&#20004;&#31181;&#26041;&#27861;&#65306;&#19968;&#31181;&#26159;&#25353;&#34892;&#29983;&#25104;&#22120;&#65292;&#19968;&#31181;&#26159;LLM&#25552;&#31034;&#29983;&#25104;&#22120;&#65288;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#31532;&#19968;&#27425;&#20351;&#29992;&#65289;&#12290;&#22312;WikiTableQuestions&#21644;ComplexWebQuestions&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;S3HQA&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering multi-hop questions over hybrid factual knowledge from the given text and table (TextTableQA) is a challenging task. Existing models mainly adopt a retriever-reader framework, which have several deficiencies, such as noisy labeling in training retriever, insufficient utilization of heterogeneous information over text and table, and deficient ability for different reasoning operations. In this paper, we propose a three-stage TextTableQA framework S3HQA, which comprises of retriever, selector, and reasoner. We use a retriever with refinement training to solve the noisy labeling problem. Then, a hybrid selector considers the linked relationships between heterogeneous data to select the most relevant factual knowledge. For the final stage, instead of adapting a reading comprehension module like in previous methods, we employ a generation-based reasoner to obtain answers. This includes two approaches: a row-wise generator and an LLM prompting generator~(first time used in this tas
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20869;&#37096;&#20449;&#24687;&#31579;&#36873;&#21644;&#22806;&#37096;&#20449;&#24687;&#21033;&#29992;&#30340;&#24605;&#24819;&#12290;&#36890;&#36807;&#35270;&#35273;&#21644;&#25991;&#26412;&#22330;&#26223;&#22270;&#34920;&#31034;&#36755;&#20837;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#22270;&#24418;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#36827;&#34892;&#32467;&#26500;&#32454;&#21270;&#21644;&#29305;&#24449;&#21435;&#22122;&#65292;&#21516;&#26102;&#36816;&#29992;&#20027;&#39064;&#24314;&#27169;&#20016;&#23500;&#19978;&#19979;&#25991;&#65292;&#35813;&#31995;&#32479;&#22312;&#22522;&#20934;MRE&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11719</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#24449;&#21435;&#22122;&#21644;&#22810;&#27169;&#24577;&#20027;&#39064;&#24314;&#27169;&#30340;&#22810;&#27169;&#24577;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#20449;&#24687;&#31579;&#36873;
&lt;/p&gt;
&lt;p&gt;
Information Screening whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling. (arXiv:2305.11719v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11719
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20869;&#37096;&#20449;&#24687;&#31579;&#36873;&#21644;&#22806;&#37096;&#20449;&#24687;&#21033;&#29992;&#30340;&#24605;&#24819;&#12290;&#36890;&#36807;&#35270;&#35273;&#21644;&#25991;&#26412;&#22330;&#26223;&#22270;&#34920;&#31034;&#36755;&#20837;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#22270;&#24418;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#36827;&#34892;&#32467;&#26500;&#32454;&#21270;&#21644;&#29305;&#24449;&#21435;&#22122;&#65292;&#21516;&#26102;&#36816;&#29992;&#20027;&#39064;&#24314;&#27169;&#20016;&#23500;&#19978;&#19979;&#25991;&#65292;&#35813;&#31995;&#32479;&#22312;&#22522;&#20934;MRE&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#20851;&#31995;&#25277;&#21462;(MRE)&#30740;&#31350;&#38754;&#20020;&#30528;&#20004;&#20010;&#20849;&#23384;&#30340;&#25361;&#25112;&#65292;&#21363;&#20869;&#37096;&#20449;&#24687;&#36807;&#24230;&#21033;&#29992;&#21644;&#22806;&#37096;&#20449;&#24687;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#20869;&#37096;&#20449;&#24687;&#31579;&#36873;&#21644;&#22806;&#37096;&#20449;&#24687;&#21033;&#29992;&#30340;&#24605;&#24819;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#22330;&#26223;&#22270;&#34920;&#31034;&#36755;&#20837;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#32467;&#26500;&#65292;&#23558;&#20854;&#36827;&#19968;&#27493;&#34701;&#21512;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#36328;&#27169;&#24577;&#22270;(CMG)&#12290;&#22522;&#20110;CMG&#65292;&#25105;&#20204;&#21033;&#29992;&#22270;&#24418;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#36827;&#34892;&#32467;&#26500;&#32454;&#21270;&#65292;&#20027;&#21160;&#21435;&#38500;&#19981;&#22826;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#29305;&#24449;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23545;&#36755;&#20837;&#22270;&#20687;&#21644;&#25991;&#26412;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;&#65292;&#23558;&#28508;&#22312;&#30340;&#22810;&#27169;&#24577;&#20027;&#39064;&#29305;&#24449;&#34701;&#20837;&#20854;&#20013;&#20197;&#20016;&#23500;&#19978;&#19979;&#25991;&#12290;&#22312;&#22522;&#20934;MRE&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#26368;&#20339;&#27169;&#22411;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MRE&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research on multimodal relation extraction (MRE) faces two co-existing challenges, internal-information over-utilization and external-information under-exploitation. To combat that, we propose a novel framework that simultaneously implements the idea of internal-information screening and external-information exploiting. First, we represent the fine-grained semantic structures of the input image and text with the visual and textual scene graphs, which are further fused into a unified cross-modal graph (CMG). Based on CMG, we perform structure refinement with the guidance of the graph information bottleneck principle, actively denoising the less-informative features. Next, we perform topic modeling over the input image and text, incorporating latent multimodal topic features to enrich the contexts. On the benchmark MRE dataset, our system outperforms the current best model significantly. With further in-depth analyses, we reveal the great potential of our method for the MRE task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#22120;&#19982;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#25506;&#27979;&#29983;&#25104;&#22120;&#30340;&#36755;&#20986;&#31354;&#38388;&#26469;&#27979;&#37327;&#20854;&#23545;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#30340;&#26657;&#20934;&#31243;&#24230;&#65292;&#24182;&#35777;&#26126;&#29992;&#22810;&#20010;&#26679;&#26412;&#21644;&#22810;&#20010;&#21442;&#32771;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.11707</link><description>&lt;p&gt;
&#19979;&#19968;&#20010;&#20250;&#26159;&#20160;&#20040;&#65311;&#35780;&#20272;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#19982;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability. (arXiv:2305.11707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#22120;&#19982;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#25506;&#27979;&#29983;&#25104;&#22120;&#30340;&#36755;&#20986;&#31354;&#38388;&#26469;&#27979;&#37327;&#20854;&#23545;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#30340;&#26657;&#20934;&#31243;&#24230;&#65292;&#24182;&#35777;&#26126;&#29992;&#22810;&#20010;&#26679;&#26412;&#21644;&#22810;&#20010;&#21442;&#32771;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#20013;&#65292;&#38024;&#23545;&#20219;&#20309;&#36755;&#20837;&#65292;&#23384;&#22312;&#22810;&#20010;&#21487;&#34892;&#30340;&#20132;&#38469;&#30446;&#26631;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#22810;&#31181;&#26041;&#24335;&#23558;&#20219;&#20309;&#30446;&#26631;&#29992;&#35821;&#35328;&#34920;&#36798;&#20986;&#26469;&#25110;&#36827;&#34892;&#29983;&#20135;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#20154;&#31867;&#29983;&#20135;&#22312;&#22235;&#20010;NLG&#20219;&#21153;&#20013;&#35789;&#27719;&#12289;&#21477;&#27861;&#21644;&#35821;&#20041;&#26041;&#38754;&#30340;&#21464;&#24322;&#31243;&#24230;&#65292;&#24182;&#23558;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#19982;&#19981;&#30830;&#23450;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#29983;&#25104;&#31995;&#32479;&#39044;&#27979;&#30340;&#27010;&#29575;&#20998;&#24067;&#21644;&#35299;&#30721;&#31639;&#27861;&#25152;&#24418;&#25104;&#30340;&#36755;&#20986;&#23383;&#31526;&#20018;&#31354;&#38388;&#65292;&#20197;&#25506;&#31350;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;&#38024;&#23545;&#27599;&#20010;&#27979;&#35797;&#36755;&#20837;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#29983;&#25104;&#22120;&#23545;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#30340;&#26657;&#20934;&#31243;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#22522;&#20110;&#23454;&#20363;&#32423;&#21035;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;NLG&#27169;&#22411;&#21644;&#35299;&#30721;&#31574;&#30053;&#65292;&#35777;&#26126;&#29992;&#22810;&#20010;&#26679;&#26412;&#21644;&#22810;&#20010;&#21442;&#32771;&#23545;&#29983;&#25104;&#22120;&#36827;&#34892;&#25506;&#27979;&#65292;&#25552;&#20379;&#20102;&#29702;&#35299;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#25152;&#24517;&#38656;&#30340;&#35814;&#32454;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Natural Language Generation (NLG) tasks, for any input, multiple communicative goals are plausible, and any goal can be put into words, or produced, in multiple ways. We characterise the extent to which human production varies lexically, syntactically, and semantically across four NLG tasks, connecting human production variability to aleatoric or data uncertainty. We then inspect the space of output strings shaped by a generation system's predicted probability distribution and decoding algorithm to probe its uncertainty. For each test input, we measure the generator's calibration to human production variability. Following this instance-level approach, we analyse NLG models and decoding strategies, demonstrating that probing a generator with multiple samples and, when possible, multiple references, provides the level of detail necessary to gain understanding of a model's representation of uncertainty.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;QUEST&#30340;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21547;&#26377;3357&#20010;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#36825;&#20123;&#26597;&#35810;&#20351;&#29992;&#38544;&#24335;&#38598;&#21512;&#25805;&#20316;&#26469;&#28385;&#36275;&#26377;&#36873;&#25321;&#24615;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#35201;&#27714;&#27169;&#22411;&#21305;&#37197;&#26597;&#35810;&#20013;&#25552;&#21040;&#30340;&#26465;&#20214;&#65292;&#24182;&#27491;&#30830;&#25191;&#34892;&#38598;&#21512;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2305.11694</link><description>&lt;p&gt;
QUEST:&#19968;&#31181;&#21033;&#29992;&#38544;&#24335;&#38598;&#21512;&#25805;&#20316;&#30340;&#23454;&#20307;&#25628;&#32034;&#26597;&#35810;&#26816;&#32034;&#25968;&#25454;&#38598;( arXiv:2305.11694v1[cs.CL])
&lt;/p&gt;
&lt;p&gt;
QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations. (arXiv:2305.11694v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11694
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;QUEST&#30340;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21547;&#26377;3357&#20010;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#36825;&#20123;&#26597;&#35810;&#20351;&#29992;&#38544;&#24335;&#38598;&#21512;&#25805;&#20316;&#26469;&#28385;&#36275;&#26377;&#36873;&#25321;&#24615;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#35201;&#27714;&#27169;&#22411;&#21305;&#37197;&#26597;&#35810;&#20013;&#25552;&#21040;&#30340;&#26465;&#20214;&#65292;&#24182;&#27491;&#30830;&#25191;&#34892;&#38598;&#21512;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#28385;&#36275;&#26377;&#36873;&#25321;&#24615;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#38544;&#24335;&#38598;&#21512;&#25805;&#20316;&#65292;&#20363;&#22914;&#20132;&#38598;&#12289;&#24182;&#38598;&#21644;&#24046;&#38598;&#12290;&#30740;&#31350;&#26816;&#32034;&#31995;&#32479;&#28385;&#36275;&#36825;&#31181;&#20449;&#24687;&#38656;&#27714;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;QUEST&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;3357&#20010;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#36825;&#20123;&#26597;&#35810;&#20855;&#26377;&#38544;&#24335;&#38598;&#21512;&#25805;&#20316;&#65292;&#21516;&#26102;&#23545;&#24212;&#20110;&#32500;&#22522;&#30334;&#31185;&#25991;&#26723;&#20013;&#30340;&#23454;&#20307;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#35201;&#27714;&#27169;&#22411;&#23558;&#26597;&#35810;&#20013;&#25552;&#21040;&#30340;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#19982;&#25991;&#26723;&#20013;&#30456;&#24212;&#30340;&#35777;&#25454;&#30456;&#21305;&#37197;&#65292;&#24182;&#27491;&#30830;&#25191;&#34892;&#21508;&#31181;&#38598;&#21512;&#25805;&#20316;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#21322;&#33258;&#21160;&#26500;&#24314;&#30340;&#65292;&#20351;&#29992;&#32500;&#22522;&#30334;&#31185;&#31867;&#21035;&#21517;&#31216;&#33258;&#21160;&#32452;&#21512;&#26597;&#35810;&#65292;&#28982;&#21518;&#30001;&#20247;&#21253;&#24037;&#20316;&#32773;&#23545;&#20854;&#36827;&#34892;&#37322;&#20041;&#21644;&#33258;&#28982;&#24230;&#39564;&#35777;&#12290;&#20247;&#21253;&#24037;&#20316;&#32773;&#36824;&#26681;&#25454;&#25991;&#26723;&#35780;&#20272;&#23454;&#20307;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#31361;&#20986;&#26174;&#31034;&#26597;&#35810;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Formulating selective information needs results in queries that implicitly specify set operations, such as intersection, union, and difference. For instance, one might search for "shorebirds that are not sandpipers" or "science-fiction films shot in England". To study the ability of retrieval systems to meet such information needs, we construct QUEST, a dataset of 3357 natural language queries with implicit set operations, that map to a set of entities corresponding to Wikipedia documents. The dataset challenges models to match multiple constraints mentioned in queries with corresponding evidence in documents and correctly perform various set operations. The dataset is constructed semi-automatically using Wikipedia category names. Queries are automatically composed from individual categories, then paraphrased and further validated for naturalness and fluency by crowdworkers. Crowdworkers also assess the relevance of entities based on their documents and highlight attribution of query c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Surgical-VQLA&#26041;&#27861;&#65292;&#32467;&#21512;Transformer&#27169;&#22411;&#21644;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#25163;&#26415;VQA&#20013;&#23545;&#35937;&#26816;&#27979;&#31232;&#32570;&#12289;&#24322;&#26500;&#27169;&#24577;&#34701;&#21512;&#31574;&#30053;&#19981;&#36275;&#12289;&#23450;&#20301;&#31572;&#26696;&#32570;&#22833;&#31561;&#38382;&#39064;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11692</link><description>&lt;p&gt;
&#24102;&#26377;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#30340;Transformer&#29992;&#20110;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Surgical-VQLA: Transformer with Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery. (arXiv:2305.11692v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Surgical-VQLA&#26041;&#27861;&#65292;&#32467;&#21512;Transformer&#27169;&#22411;&#21644;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#25163;&#26415;VQA&#20013;&#23545;&#35937;&#26816;&#27979;&#31232;&#32570;&#12289;&#24322;&#26500;&#27169;&#24577;&#34701;&#21512;&#31574;&#30053;&#19981;&#36275;&#12289;&#23450;&#20301;&#31572;&#26696;&#32570;&#22833;&#31561;&#38382;&#39064;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23384;&#22312;&#30528;&#35745;&#31639;&#26426;&#36741;&#21161;&#27169;&#25311;&#22120;&#21644;&#25163;&#26415;&#36807;&#31243;&#30340;&#24405;&#21046;&#35270;&#39057;&#65292;&#20294;&#21021;&#32423;&#20303;&#38498;&#21307;&#24072;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#19987;&#23478;&#26469;&#22238;&#31572;&#20182;&#20204;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19987;&#23478;&#22806;&#31185;&#21307;&#29983;&#36890;&#24120;&#25215;&#25285;&#30528;&#20020;&#24202;&#21644;&#23398;&#26415;&#24037;&#20316;&#65292;&#38480;&#21046;&#20102;&#20182;&#20204;&#22238;&#31572;&#38382;&#39064;&#30340;&#26102;&#38388;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25163;&#26415;&#38382;&#31572;&#31995;&#32479;&#65292;&#20197;&#20415;&#20174;&#24405;&#21046;&#30340;&#35270;&#39057;&#20013;&#20419;&#36827;&#26426;&#22120;&#20154;&#36741;&#21161;&#25163;&#26415;&#22330;&#26223;&#21644;&#27963;&#21160;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;Transformer&#27169;&#22411;&#19982;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#30456;&#32467;&#21512;&#30340;&#26426;&#22120;&#20154;&#25163;&#26415;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25163;&#26415;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#31232;&#32570;&#12289;&#24322;&#26500;&#27169;&#24577;&#34701;&#21512;&#31574;&#30053;&#19981;&#36275;&#12289;&#32570;&#22833;&#23450;&#20301;&#31572;&#26696;&#31561;&#38382;&#39064;&#65292;&#24182;&#22312;&#22522;&#20934;&#25163;&#26415;VQA&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the availability of computer-aided simulators and recorded videos of surgical procedures, junior residents still heavily rely on experts to answer their queries. However, expert surgeons are often overloaded with clinical and academic workloads and limit their time in answering. For this purpose, we develop a surgical question-answering system to facilitate robot-assisted surgical scene and activity understanding from recorded videos. Most of the existing VQA methods require an object detector and regions based feature extractor to extract visual features and fuse them with the embedded text of the question for answer generation. However, (1) surgical object detection model is scarce due to smaller datasets and lack of bounding box annotation; (2) current fusion strategy of heterogeneous modalities like text and image is naive; (3) the localized answering is missing, which is crucial in complex surgical scenarios. In this paper, we propose Visual Question Localized-Answering in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#36890;&#29992;&#21387;&#32553;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#21644;&#33976;&#39311;&#23631;&#34109;&#26469;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#36739;&#20302;&#30340;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.11685</link><description>&lt;p&gt;
&#22238;&#25910;&#21644;&#31934;&#39311;&#65306;&#24102;&#26377;&#27880;&#24847;&#21147;&#26144;&#23556;&#37325;&#29992;&#21644;&#33976;&#39311;&#23631;&#34109;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#36890;&#29992;&#21387;&#32553;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation. (arXiv:2305.11685v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#36890;&#29992;&#21387;&#32553;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#21644;&#33976;&#39311;&#23631;&#34109;&#26469;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#36739;&#20302;&#30340;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35821;&#38899; SSL &#27169;&#22411;&#20013;&#24222;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#38656;&#35201;&#21387;&#32553;&#25104;&#26356;&#32039;&#20945;&#30340;&#27169;&#22411;&#65292;&#20197;&#20415;&#22312;&#23398;&#26415;&#30028;&#25110;&#23567;&#20844;&#21496;&#20013;&#26356;&#24191;&#27867;&#22320;&#20351;&#29992;&#12290;&#26412;&#30740;&#31350;&#24314;&#35758;&#37325;&#29992;Transformer&#23618;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#26144;&#23556;&#65292;&#22240;&#27492;&#21487;&#20197;&#21024;&#38500;&#38190;&#21644;&#26597;&#35810;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#23618;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33976;&#39311;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#36136;&#37327;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#33976;&#39311;&#25439;&#22833;&#65292;&#21033;&#29992;&#36974;&#32617;&#21644;&#26410;&#36974;&#32617;&#30340;&#35821;&#38899;&#24103;&#65292;&#20805;&#20998;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#21387;&#32553;&#31574;&#30053;&#20135;&#29983;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;SUPERB&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;7.72%&#30340;&#38899;&#32032;&#35823;&#24046;&#29575;&#65288;PER&#65289;&#21644;9.96%&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based speech self-supervised learning (SSL) models, such as HuBERT, show surprising performance in various speech processing tasks. However, huge number of parameters in speech SSL models necessitate the compression to a more compact model for wider usage in academia or small companies. In this study, we suggest to reuse attention maps across the Transformer layers, so as to remove key and query parameters while retaining the number of layers. Furthermore, we propose a novel masking distillation strategy to improve the student model's speech representation quality. We extend the distillation loss to utilize both masked and unmasked speech frames to fully leverage the teacher model's high-quality representation. Our universal compression strategy yields the student model that achieves phoneme error rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;&#35821;&#35328;&#30340;&#26041;&#27861;&#26469;&#24863;&#30693;&#33016;&#24102;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#21560;&#27668;&#20107;&#20214;&#65292;&#32467;&#26524;&#34920;&#26126;VRB&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#21516;&#26102;&#35813;&#30740;&#31350;&#36824;&#21457;&#29616;&#26391;&#35835;&#21644;&#33258;&#21457;&#35821;&#35328;&#20869;&#23481;&#37117;&#20855;&#26377;&#26174;&#30528;&#30340;&#38750;&#35821;&#27861;&#24615;&#21628;&#21560;&#65292;&#20026;&#24320;&#21457;VRB&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.11683</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#38899;&#30340;&#28789;&#24863;&#20107;&#20214;&#24863;&#30693;&#65306;&#28145;&#24230;&#23398;&#20064;&#19982;&#35821;&#35328;&#26041;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Sensing of inspiration events from speech: comparison of deep learning and linguistic methods. (arXiv:2305.11683v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;&#35821;&#35328;&#30340;&#26041;&#27861;&#26469;&#24863;&#30693;&#33016;&#24102;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#21560;&#27668;&#20107;&#20214;&#65292;&#32467;&#26524;&#34920;&#26126;VRB&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#21516;&#26102;&#35813;&#30740;&#31350;&#36824;&#21457;&#29616;&#26391;&#35835;&#21644;&#33258;&#21457;&#35821;&#35328;&#20869;&#23481;&#37117;&#20855;&#26377;&#26174;&#30528;&#30340;&#38750;&#35821;&#27861;&#24615;&#21628;&#21560;&#65292;&#20026;&#24320;&#21457;VRB&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21628;&#21560;&#33016;&#24102;&#20256;&#24863;&#22120;&#21487;&#20197;&#29992;&#20110;&#27979;&#37327;&#21628;&#21560;&#29575;&#21644;&#20854;&#20182;&#21628;&#21560;&#20581;&#24247;&#21442;&#25968;&#12290;&#34394;&#25311;&#21628;&#21560;&#33016;&#24102;&#65288;VRB&#65289;&#31639;&#27861;&#21487;&#20197;&#20174;&#35821;&#38899;&#38899;&#39057;&#20272;&#31639;&#20986;&#33016;&#24102;&#20256;&#24863;&#22120;&#27874;&#24418;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;VRB&#31639;&#27861;&#21644;&#22522;&#20110;&#26102;&#38388;&#23545;&#40784;&#35821;&#35328;&#20869;&#23481;&#30340;&#26816;&#27979;&#26469;&#33258;&#33016;&#24102;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#21560;&#27668;&#20107;&#20214;&#65288;IE&#65289;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;VRB&#26041;&#27861;&#20248;&#20110;&#21333;&#35789;&#26242;&#20572;&#26816;&#27979;&#25110;&#35821;&#27861;&#20869;&#23481;&#20998;&#21106;&#12290;&#26041;&#27861;&#30340;&#27604;&#36739;&#26174;&#31034;&#65292;&#26391;&#35835;&#21644;&#33258;&#21457;&#35821;&#35328;&#20869;&#23481;&#37117;&#20855;&#26377;&#26174;&#30528;&#30340;&#38750;&#35821;&#27861;&#24615;&#21628;&#21560;&#65292;&#21363;&#21628;&#21560;&#20107;&#20214;&#19981;&#19982;&#35821;&#35328;&#20013;&#30340;&#35821;&#27861;&#27491;&#30830;&#30340;&#20301;&#32622;&#23545;&#40784;&#12290;&#35813;&#30740;&#31350;&#20026;VRB&#26041;&#27861;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#22686;&#21152;&#20102;&#23545;&#35821;&#38899;&#21628;&#21560;&#34892;&#20026;&#30340;&#26222;&#36941;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#28436;&#31034;&#20102;&#19968;&#31181;&#36830;&#32493;&#21628;&#21560;&#27874;&#24418;&#30340;&#37325;&#24314;&#26032;VRB&#26041;&#27861;VRBOLA&#12290;
&lt;/p&gt;
&lt;p&gt;
Respiratory chest belt sensor can be used to measure the respiratory rate and other respiratory health parameters. Virtual Respiratory Belt, VRB, algorithms estimate the belt sensor waveform from speech audio. In this paper we compare the detection of inspiration events (IE) from respiratory belt sensor data using a novel neural VRB algorithm and the detections based on time-aligned linguistic content. The results show the superiority of the VRB method over word pause detection or grammatical content segmentation. The comparison of the methods show that both read and spontaneous speech content has a significant amount of ungrammatical breathing, that is, breathing events that are not aligned with grammatically appropriate places in language. This study gives new insights into the development of VRB methods and adds to the general understanding of speech breathing behavior. Moreover, a new VRB method, VRBOLA, for the reconstruction of the continuous breathing waveform is demonstrated.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22235;&#31181;&#35821;&#35328;&#24314;&#31435;&#20102;&#19968;&#20010;&#23545;&#24615;&#21035;&#21644;&#31181;&#26063;/&#31227;&#27665;&#20559;&#35265;&#30340;&#21453;&#20107;&#23454;&#35780;&#20272;&#35821;&#26009;&#24211;&#65292;&#24182;&#36890;&#36807;&#22238;&#31572;&#20851;&#38190;&#38382;&#39064;&#25581;&#31034;&#20102;&#31995;&#32479;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23548;&#20837;&#30340;&#20559;&#24046;&#34892;&#20026;&#21464;&#21270;&#65292;&#20026;&#26356;&#26377;&#38024;&#23545;&#24615;&#22320;&#20943;&#36731;&#31574;&#30053;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11673</link><description>&lt;p&gt;
&#36229;&#36234;&#33521;&#35821;&#65306;&#22235;&#31181;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#20559;&#24046;&#21453;&#20107;&#23454;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Bias Beyond English: Counterfactual Tests for Bias in Sentiment Analysis in Four Languages. (arXiv:2305.11673v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22235;&#31181;&#35821;&#35328;&#24314;&#31435;&#20102;&#19968;&#20010;&#23545;&#24615;&#21035;&#21644;&#31181;&#26063;/&#31227;&#27665;&#20559;&#35265;&#30340;&#21453;&#20107;&#23454;&#35780;&#20272;&#35821;&#26009;&#24211;&#65292;&#24182;&#36890;&#36807;&#22238;&#31572;&#20851;&#38190;&#38382;&#39064;&#25581;&#31034;&#20102;&#31995;&#32479;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23548;&#20837;&#30340;&#20559;&#24046;&#34892;&#20026;&#21464;&#21270;&#65292;&#20026;&#26356;&#26377;&#38024;&#23545;&#24615;&#22320;&#20943;&#36731;&#31574;&#30053;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#65288;SA&#65289;&#31995;&#32479;&#22312;&#35768;&#22810;&#20135;&#21697;&#21644;&#25968;&#30334;&#31181;&#35821;&#35328;&#20013;&#20351;&#29992;&#12290;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#22312;&#33521;&#35821;SA&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#20294;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#21364;&#40092;&#26377;&#30740;&#31350;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#38024;&#23545;&#22235;&#31181;&#35821;&#35328;&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;/&#31227;&#27665;&#20559;&#35265;&#30340;&#21453;&#20107;&#23454;&#35780;&#20272;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#36890;&#36807;&#22238;&#31572;&#24037;&#31243;&#24072;&#22312;&#37096;&#32626;&#31995;&#32479;&#26102;&#21487;&#33021;&#38656;&#35201;&#22238;&#31572;&#30340;&#19968;&#20010;&#31616;&#21333;&#20294;&#37325;&#35201;&#30340;&#38382;&#39064;&#26469;&#35777;&#26126;&#20854;&#26377;&#29992;&#24615;&#65306;&#19982;&#27809;&#26377;&#39044;&#35757;&#32451;&#30340;&#22522;&#20934;&#32447;&#30456;&#27604;&#65292;&#31995;&#32479;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23548;&#20837;&#20102;&#21738;&#20123;&#20559;&#24046;&#65311;&#30001;&#20110;&#23427;&#26159;&#21453;&#20107;&#23454;&#30340;&#35780;&#20272;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#35821;&#26009;&#24211;&#19981;&#20165;&#25581;&#31034;&#20102;&#21738;&#20123;&#27169;&#22411;&#20855;&#26377;&#26356;&#23569;&#30340;&#20559;&#24046;&#65292;&#32780;&#19988;&#36824;&#30830;&#23450;&#20102;&#27169;&#22411;&#20559;&#24046;&#34892;&#20026;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#20351;&#26356;&#38024;&#23545;&#24615;&#30340;&#20943;&#36731;&#31574;&#30053;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#35780;&#20272;&#35821;&#26009;&#24211;&#20197;&#20415;&#20110;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis (SA) systems are used in many products and hundreds of languages. Gender and racial biases are well-studied in English SA systems, but understudied in other languages, with few resources for such studies. To remedy this, we build a counterfactual evaluation corpus for gender and racial/migrant bias in four languages. We demonstrate its usefulness by answering a simple but important question that an engineer might need to answer when deploying a system: What biases do systems import from pre-trained models when compared to a baseline with no pre-training? Our evaluation corpus, by virtue of being counterfactual, not only reveals which models have less bias, but also pinpoints changes in model bias behaviour, which enables more targeted mitigation strategies. We release our code and evaluation corpora to facilitate future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#33021;&#21147;&#30340;&#33539;&#20363;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#33258;&#36523;&#29983;&#25104;&#30340;&#19981;&#21516;&#24847;&#20041;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#33258;&#25105;&#19968;&#33268;&#24615;&#20316;&#20026;&#27169;&#22411;&#29702;&#35299;&#30340;&#26816;&#39564;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;ChatGPT&#22312;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11662</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#35780;&#20272;&#20219;&#21153;&#29702;&#35299;&#65306;ChatGPT&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating task understanding through multilingual consistency: A ChatGPT case study. (arXiv:2305.11662v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#33021;&#21147;&#30340;&#33539;&#20363;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#33258;&#36523;&#29983;&#25104;&#30340;&#19981;&#21516;&#24847;&#20041;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#33258;&#25105;&#19968;&#33268;&#24615;&#20316;&#20026;&#27169;&#22411;&#29702;&#35299;&#30340;&#26816;&#39564;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;ChatGPT&#22312;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21151;&#33021;&#30340;&#24778;&#20154;&#25552;&#21319;&#65292;&#21019;&#24314;&#26410;&#26469;&#21487;&#25345;&#32493;&#30340;&#35780;&#20272;&#38598;&#20197;&#35780;&#20272;&#23427;&#20204;&#30340;&#29702;&#35299;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;LLM&#30340;&#33539;&#20363;&#65292;&#35813;&#33539;&#20363;&#21033;&#29992;&#20102;&#27491;&#30830;&#30340;&#19990;&#30028;&#29702;&#35299;&#24212;&#35813;&#22312;&#30456;&#21516;&#21547;&#20041;&#30340;&#19981;&#21516;&#65288;&#24343;&#38647;&#26684;&#65289;&#24847;&#20041;&#19978;&#20445;&#25345;&#19968;&#33268;&#30340;&#24605;&#24819;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#26159;&#36890;&#36807;&#27491;&#30830;&#24615;&#26469;&#34913;&#37327;&#29702;&#35299;&#65292;&#32780;&#26159;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#33258;&#36523;&#29983;&#25104;&#30340;&#22810;&#20010;&#24847;&#20041;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#20363;&#21270;&#19968;&#20010;&#27979;&#35797;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19981;&#21516;&#30340;&#24847;&#20041;&#26159;&#19981;&#21516;&#30340;&#35821;&#35328;&#65292;&#22240;&#27492;&#23558;&#22810;&#35821;&#35328;&#33258;&#25105;&#19968;&#33268;&#24615;&#20316;&#20026;&#27169;&#22411;&#29702;&#35299;&#30340;&#26816;&#39564;&#24182;&#21516;&#26102;&#35299;&#20915;&#22810;&#35821;&#35328;&#30340;&#37325;&#35201;&#20027;&#39064;&#12290;&#25105;&#20204;&#20197;&#26368;&#26032;&#29256;&#26412;&#30340;ChatGPT&#20026;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#35937;&#65292;&#22312;&#19977;&#31181;&#19981;&#21516;&#35821;&#35328;&#20013;&#35780;&#20272;&#20004;&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;ChatGPT&#22312;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
At the staggering pace with which the capabilities of large language models (LLMs) are increasing, creating future-proof evaluation sets to assess their understanding becomes more and more challenging. In this paper, we propose a novel paradigm for evaluating LLMs which leverages the idea that correct world understanding should be consistent across different (Fregean) senses of the same meaning. Accordingly, we measure understanding not in terms of correctness but by evaluating consistency across multiple senses that are generated by the model itself. We showcase our approach by instantiating a test where the different senses are different languages, hence using multilingual self-consistency as a litmus test for the model's understanding and simultaneously addressing the important topic of multilingualism. Taking one of the latest versions of ChatGPT as our object of study, we evaluate multilingual consistency for two different tasks across three different languages. We show that its m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21517;&#20026;LLM-Pruner&#65292;&#37319;&#29992;&#32467;&#26500;&#20462;&#21098;&#30340;&#26041;&#24335;&#22312;&#20445;&#30041;&#22823;&#22810;&#25968;&#21151;&#33021;&#30340;&#21516;&#26102;&#65292;&#21387;&#32553;LLM&#30340;&#32467;&#26500;&#65292;&#20197;&#20943;&#23569;LLM&#22312;&#37096;&#32626;&#12289;&#25512;&#29702;&#21644;&#35757;&#32451;&#38454;&#27573;&#20013;&#30340;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.11627</link><description>&lt;p&gt;
LLM-Pruner: &#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#20462;&#21098;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LLM-Pruner: On the Structural Pruning of Large Language Models. (arXiv:2305.11627v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21517;&#20026;LLM-Pruner&#65292;&#37319;&#29992;&#32467;&#26500;&#20462;&#21098;&#30340;&#26041;&#24335;&#22312;&#20445;&#30041;&#22823;&#22810;&#25968;&#21151;&#33021;&#30340;&#21516;&#26102;&#65292;&#21387;&#32553;LLM&#30340;&#32467;&#26500;&#65292;&#20197;&#20943;&#23569;LLM&#22312;&#37096;&#32626;&#12289;&#25512;&#29702;&#21644;&#35757;&#32451;&#38454;&#27573;&#20013;&#30340;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20986;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#20196;&#20154;&#24778;&#35766;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#36890;&#24120;&#20276;&#38543;&#30528;&#24040;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#36825;&#22312;&#37096;&#32626;&#12289;&#25512;&#29702;&#21644;&#35757;&#32451;&#38454;&#27573;&#37117;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;&#25105;&#20204;&#20197;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#25506;&#32034;&#20102;LLM&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#26088;&#22312;&#20445;&#30041;&#21407;&#22987;LLM&#30340;&#22810;&#20219;&#21153;&#35299;&#20915;&#21644;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#32467;&#26500;&#20462;&#21098;&#65292;&#22312;&#26799;&#24230;&#20449;&#24687;&#30340;&#25903;&#25345;&#19979;&#36873;&#25321;&#24615;&#22320;&#31227;&#38500;&#38750;&#20851;&#38190;&#30340;&#32806;&#21512;&#32467;&#26500;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#20445;&#30041;&#20102;&#22823;&#22810;&#25968;LLM&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionalit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#20811;&#38534;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#20174;CodeForces&#25968;&#25454;&#38598;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;XCD&#12290;&#25105;&#20204;&#20351;&#29992;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#35757;&#32451;&#65288;CCT&#65289;&#26041;&#27861;&#35757;&#32451;&#20102;&#35821;&#35328;&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#20855;&#26377;&#26032;&#39062;&#24615;&#33021;&#30340;CCT-LM&#27169;&#22411;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11626</link><description>&lt;p&gt;
CCT-Code&#65306;&#38754;&#21521;&#22810;&#35821;&#35328;&#20811;&#38534;&#26816;&#27979;&#21644;&#20195;&#30721;&#25628;&#32034;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CCT-Code: Cross-Consistency Training for Multilingual Clone Detection and Code Search. (arXiv:2305.11626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#20811;&#38534;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#20174;CodeForces&#25968;&#25454;&#38598;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;XCD&#12290;&#25105;&#20204;&#20351;&#29992;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#35757;&#32451;&#65288;CCT&#65289;&#26041;&#27861;&#35757;&#32451;&#20102;&#35821;&#35328;&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#20855;&#26377;&#26032;&#39062;&#24615;&#33021;&#30340;CCT-LM&#27169;&#22411;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#28304;&#20195;&#30721;&#30340;&#20811;&#38534;&#26816;&#27979;&#21644;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#65292;&#36825;&#20004;&#20010;&#38382;&#39064;&#23545;&#20110;&#20219;&#20309;&#32534;&#31243;&#35821;&#35328;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#20811;&#38534;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#20174;CodeForces&#25552;&#20132;&#25968;&#25454;&#38598;&#20135;&#29983;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;XCD&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#35757;&#32451;&#65288;CCT&#65289;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#30340;&#32534;&#31243;&#35821;&#35328;&#20013;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36827;&#32780;&#24471;&#21040;&#22522;&#20110;CCT-LM &#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32487;&#25215;&#20102;GraphCodeBERT&#24182;&#29992;CCT&#24494;&#35843;&#65292;&#36798;&#21040;&#20102;95.67\% MAP&#21644;47.18\% MRR&#30340;&#24615;&#33021;&#65292;&#25104;&#21151;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the clone detection and information retrieval problems for source code, well-known tasks important for any programming language. Although it is also an important and interesting problem to find code snippets that operate identically but are written in different programming languages, to the best of our knowledge multilingual clone detection has not been studied in literature. In this work, we formulate the multilingual clone detection problem and present XCD, a new benchmark dataset produced from the CodeForces submissions dataset. Moreover, we present a novel training procedure, called cross-consistency training (CCT), that we apply to train language models on source code in different programming languages. The resulting CCT-LM model, initialized with GraphCodeBERT and fine-tuned with CCT, achieves new state of the art, outperforming existing approaches on the POJ-104 clone detection benchmark with 95.67\% MAP and AdvTest code search benchmark with 47.18\% MRR; it also sho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#25353;&#20195;&#30721;&#29255;&#27573;&#25628;&#32034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;StackOverflow&#25968;&#25454;&#30340;SearchBySnippet&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#21333;&#32534;&#30721;&#22120;&#27169;&#22411;SnippeR&#65292;&#23427;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11625</link><description>&lt;p&gt;
&#20195;&#30721;&#25628;&#32034;&#65306;&#19968;&#20010;&#26032;&#30340;SearchBySnippet&#25968;&#25454;&#38598;&#21644;SnippeR&#26816;&#32034;&#27169;&#22411;&#29992;&#20110;&#25353;&#20195;&#30721;&#29255;&#27573;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Searching by Code: a New SearchBySnippet Dataset and SnippeR Retrieval Model for Searching by Code Snippets. (arXiv:2305.11625v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#25353;&#20195;&#30721;&#29255;&#27573;&#25628;&#32034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;StackOverflow&#25968;&#25454;&#30340;SearchBySnippet&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#21333;&#32534;&#30721;&#22120;&#27169;&#22411;SnippeR&#65292;&#23427;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#25628;&#32034;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#22312;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#24456;&#22810;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#23581;&#35797;&#20027;&#35201;&#32771;&#34385;&#36890;&#36807;&#25991;&#26412;&#26597;&#35810;&#25628;&#32034;&#20195;&#30721;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20351;&#29992;&#20195;&#30721;&#29255;&#27573;&#65288;&#21487;&#33021;&#20276;&#38543;&#30528;&#22238;&#28335;&#65289;&#20316;&#20026;&#26597;&#35810;&#65292;&#24182;&#23547;&#25214;&#20855;&#26377;&#20462;&#22797;&#38169;&#35823;&#25351;&#20196;&#21644;&#20195;&#30721;&#31034;&#20363;&#30340;&#31572;&#26696;&#26159;&#19968;&#20010;&#33258;&#28982;&#30340;&#29992;&#20363;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#24182;&#26410;&#28085;&#30422;&#27492;&#31867;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#20351;&#29992;&#20174;&#20195;&#30721;&#20013;&#25552;&#21462;&#30340;&#27880;&#37322;&#32780;&#19981;&#26159;&#20840;&#25991;&#25551;&#36848;&#20316;&#20026;&#25991;&#26412;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#36825;&#31181;&#29992;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;StackOverflow&#25968;&#25454;&#23454;&#29616;&#25628;&#32034;BySnippet&#29992;&#20363;&#30340;&#26032;SearchBySnippet&#25968;&#25454;&#38598;; &#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#20307;&#31995;&#32467;&#26500;&#21363;&#20351;&#22312;&#24494;&#35843;&#20043;&#21518;&#20063;&#19981;&#33021;&#19982;&#26368;&#31616;&#21333;&#30340;BM25&#22522;&#32447;&#30456;&#25552;&#24182;&#35770;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21333;&#32534;&#30721;&#22120;&#27169;&#22411;SnippeR&#65292;&#23427;&#22312;SearchBySnippet&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#32447;&#65292;&#32467;&#26524;&#20026;0.451 Recall@10; &#25105;&#20204;&#24314;&#35758;&#23558;SearchBySnippet&#25968;&#25454;&#38598;&#21644;SnippeR&#20316;&#20026;&#26032;&#30340;&#37325;&#35201;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26469;&#25512;&#24191;&#20195;&#30721;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code search is an important task that has seen many developments in recent years. However, previous attempts have mostly considered the problem of searching for code by a text query. We argue that using a code snippet (and possibly an associated traceback) as a query and looking for answers with bugfixing instructions and code samples is a natural use case that is not covered by existing approaches. Moreover, existing datasets use comments extracted from code rather than full-text descriptions as text, making them unsuitable for this use case. We present a new SearchBySnippet dataset implementing the search-by-code use case based on StackOverflow data; it turns out that in this setting, existing architectures fall short of the simplest BM25 baseline even after fine-tuning. We present a new single encoder model SnippeR that outperforms several strong baselines on the SearchBySnippet dataset with a result of 0.451 Recall@10; we propose the SearchBySnippet dataset and SnippeR as a new imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#24402;&#22240;&#19988;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#24847;&#35265;&#25688;&#35201;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#32534;&#30721;&#35780;&#35770;&#21477;&#23376;&#20026;&#20998;&#23618;&#28508;&#31354;&#38388;&#26469;&#30830;&#23450;&#24120;&#35265;&#24847;&#35265;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#25277;&#35937;&#25110;&#25552;&#21462;&#30340;&#25688;&#35201;&#12290;&#35813;&#26041;&#27861;&#22312;&#20960;&#31181;&#22522;&#32447;&#26041;&#27861;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#20801;&#35768;&#29983;&#25104;&#29305;&#23450;&#26041;&#38754;&#30340;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.11603</link><description>&lt;p&gt;
&#21487;&#24402;&#22240;&#19988;&#21487;&#25193;&#23637;&#30340;&#24847;&#35265;&#25688;&#35201;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Attributable and Scalable Opinion Summarization. (arXiv:2305.11603v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11603
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#24402;&#22240;&#19988;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#24847;&#35265;&#25688;&#35201;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#32534;&#30721;&#35780;&#35770;&#21477;&#23376;&#20026;&#20998;&#23618;&#28508;&#31354;&#38388;&#26469;&#30830;&#23450;&#24120;&#35265;&#24847;&#35265;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#25277;&#35937;&#25110;&#25552;&#21462;&#30340;&#25688;&#35201;&#12290;&#35813;&#26041;&#27861;&#22312;&#20960;&#31181;&#22522;&#32447;&#26041;&#27861;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#20801;&#35768;&#29983;&#25104;&#29305;&#23450;&#26041;&#38754;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#24847;&#35265;&#25688;&#35201;&#26041;&#27861;&#65292;&#23558;&#39038;&#23458;&#35780;&#35770;&#20013;&#30340;&#21477;&#23376;&#32534;&#30721;&#20026;&#20998;&#23618;&#31163;&#25955;&#28508;&#31354;&#38388;&#65292;&#28982;&#21518;&#26681;&#25454;&#20854;&#32534;&#30721;&#30340;&#39057;&#29575;&#30830;&#23450;&#24120;&#35265;&#24847;&#35265;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#35299;&#30721;&#36825;&#20123;&#39057;&#32321;&#30340;&#32534;&#30721;&#26469;&#29983;&#25104;&#25277;&#35937;&#24635;&#32467;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#20998;&#37197;&#32473;&#30456;&#21516;&#39057;&#32321;&#32534;&#30721;&#30340;&#21477;&#23376;&#26469;&#29983;&#25104;&#25552;&#21462;&#24635;&#32467;&#12290;&#30001;&#20110;&#27169;&#22411;&#23558;&#29983;&#25104;&#24635;&#32467;&#25152;&#29992;&#30340;&#21477;&#23376;&#35782;&#21035;&#20026;&#25688;&#35201;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#24402;&#22240;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28508;&#31354;&#38388;&#20013;&#25191;&#34892;&#32858;&#21512;&#32780;&#19981;&#26159;&#22312;&#38271;&#24207;&#21015;&#20013;&#25191;&#34892;&#32858;&#21512;&#65292;&#22240;&#27492;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#35768;&#22810;&#36755;&#20837;&#35780;&#35770;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23558;&#27169;&#22411;&#38480;&#21046;&#20026;&#19982;&#25152;&#38656;&#26041;&#38754;&#65288;&#20363;&#22914;&#20301;&#32622;&#25110;&#39135;&#21697;&#65289;&#30456;&#23545;&#24212;&#30340;&#32534;&#30721;&#31354;&#38388;&#37096;&#20998;&#26469;&#29983;&#25104;&#29305;&#23450;&#26041;&#38754;&#30340;&#25688;&#35201;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#25511;&#21046;&#24230;&#12290;&#23545;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20960;&#31181;&#31454;&#20105;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method for unsupervised opinion summarization that encodes sentences from customer reviews into a hierarchical discrete latent space, then identifies common opinions based on the frequency of their encodings. We are able to generate both abstractive summaries by decoding these frequent encodings, and extractive summaries by selecting the sentences assigned to the same frequent encodings. Our method is attributable, because the model identifies sentences used to generate the summary as part of the summarization process. It scales easily to many hundreds of input reviews, because aggregation is performed in the latent space rather than over long sequences of tokens. We also demonstrate that our appraoch enables a degree of control, generating aspect-specific summaries by restricting the model to parts of the encoding space that correspond to desired aspects (e.g., location or food). Automatic and human evaluation on two datasets from different domains demonstrates that our m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#25216;&#24039;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20869;&#30465;&#36712;&#36857;&#29983;&#25104;&#31616;&#27905;&#26377;&#20215;&#20540;&#30340;&#25552;&#31034;&#65292;&#19981;&#35843;&#25972;LLM&#21442;&#25968;&#23601;&#33021;&#25552;&#39640;&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11598</link><description>&lt;p&gt;
&#20869;&#30465;&#25216;&#24039;&#65306;&#19978;&#19979;&#25991;&#20915;&#31574;&#21046;&#23450;&#19979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Introspective Tips: Large Language Model for In-Context Decision Making. (arXiv:2305.11598v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#25216;&#24039;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20869;&#30465;&#36712;&#36857;&#29983;&#25104;&#31616;&#27905;&#26377;&#20215;&#20540;&#30340;&#25552;&#31034;&#65292;&#19981;&#35843;&#25972;LLM&#21442;&#25968;&#23601;&#33021;&#25552;&#39640;&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#8220;&#20869;&#30465;&#25216;&#24039;&#8221;&#26469;&#20419;&#36827;LLMs&#22312;&#33258;&#25105;&#20248;&#21270;&#20915;&#31574;&#21046;&#23450;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;&#36890;&#36807;&#20869;&#30465;&#22320;&#26816;&#26597;&#36712;&#36857;&#65292;LLM&#29983;&#25104;&#31616;&#27905;&#32780;&#26377;&#20215;&#20540;&#30340;&#25552;&#31034;&#26469;&#25913;&#21892;&#20854;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#19977;&#31181;&#37325;&#35201;&#24773;&#22659;&#26469;&#25552;&#39640;&#20195;&#29702;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65306;&#20174;&#20195;&#29702;&#36807;&#21435;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#12289;&#25972;&#21512;&#19987;&#23478;&#28436;&#31034;&#21644;&#22312;&#19981;&#21516;&#28216;&#25103;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#19981;&#35843;&#25972;LLM&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#35843;&#25972;&#25552;&#31034;&#26469;&#20174;&#36825;&#19977;&#31181;&#24773;&#20917;&#20013;&#27010;&#25324;&#35265;&#35299;&#26469;&#23454;&#29616;&#36825;&#20123;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#20165;&#25903;&#25345;&#32780;&#19988;&#24378;&#35843;&#20102;&#22312;&#19978;&#19979;&#25991;&#20915;&#31574;&#21046;&#23450;&#20013;&#37319;&#29992;LLM&#30340;&#20248;&#21183;&#12290;&#22312;TextWorld&#20013;&#28041;&#21450;&#36229;&#36807;100&#20010;&#28216;&#25103;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large language models (LLMs) has substantially influenced natural language processing, demonstrating exceptional results across various tasks. In this study, we employ ``Introspective Tips" to facilitate LLMs in self-optimizing their decision-making. By introspectively examining trajectories, LLM refines its policy by generating succinct and valuable tips. Our method enhances the agent's performance in both few-shot and zero-shot learning situations by considering three essential scenarios: learning from the agent's past experiences, integrating expert demonstrations, and generalizing across diverse games. Importantly, we accomplish these improvements without fine-tuning the LLM parameters; rather, we adjust the prompt to generalize insights from the three aforementioned situations. Our framework not only supports but also emphasizes the advantage of employing LLM in in-contxt decision-making. Experiments involving over 100 games in TextWorld illustrate the superior pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#21518;&#38376;&#27745;&#26579;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#36731;&#25991;&#26412;&#29305;&#24449;&#21644;&#20998;&#31867;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#26469;&#38450;&#24481;&#25915;&#20987;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25152;&#26377;&#21518;&#38376;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;&#25554;&#20837;&#24335;&#25915;&#20987;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#20960;&#20046;&#23436;&#32654;&#30340;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2305.11596</link><description>&lt;p&gt;
&#36879;&#36807;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#35270;&#35282;&#32531;&#35299;&#21518;&#38376;&#27745;&#26579;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation. (arXiv:2305.11596v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#21518;&#38376;&#27745;&#26579;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#36731;&#25991;&#26412;&#29305;&#24449;&#21644;&#20998;&#31867;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#26469;&#38450;&#24481;&#25915;&#20987;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25152;&#26377;&#21518;&#38376;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;&#25554;&#20837;&#24335;&#25915;&#20987;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#20960;&#20046;&#23436;&#32654;&#30340;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;NLP&#27169;&#22411;&#36890;&#24120;&#22312;&#22823;&#22411;&#19981;&#21487;&#20449;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#25552;&#39640;&#20102;&#24694;&#24847;&#23545;&#25163;&#30772;&#22351;&#27169;&#22411;&#34892;&#20026;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#21518;&#38376;&#27745;&#26579;&#25915;&#20987;&#23637;&#31034;&#20102;&#31616;&#21333;&#25991;&#26412;&#29305;&#24449;&#21644;&#20998;&#31867;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#20943;&#36731;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#20316;&#20026;&#38450;&#24481;&#25163;&#27573;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#24694;&#24847;&#35302;&#21457;&#22120;&#19982;&#20854;&#30446;&#26631;&#26631;&#31614;&#39640;&#24230;&#30456;&#20851;&#65292;&#22240;&#27492;&#36825;&#20123;&#30456;&#20851;&#24615;&#19982;&#33391;&#24615;&#29305;&#24449;&#24471;&#20998;&#30456;&#27604;&#26497;&#26131;&#21306;&#20998;&#65292;&#24182;&#21487;&#29992;&#20110;&#36807;&#28388;&#21487;&#33021;&#26377;&#38382;&#39064;&#30340;&#23454;&#20363;&#12290;&#19982;&#20960;&#31181;&#29616;&#26377;&#30340;&#38450;&#24481;&#25514;&#26045;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#25152;&#26377;&#21518;&#38376;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#22312;&#25554;&#20837;&#24335;&#25915;&#20987;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#20960;&#20046;&#23436;&#32654;&#30340;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern NLP models are often trained over large untrusted datasets, raising the potential for a malicious adversary to compromise model behaviour. For instance, backdoors can be implanted through crafting training instances with a specific textual trigger and a target label. This paper posits that backdoor poisoning attacks exhibit spurious correlation between simple text features and classification labels, and accordingly, proposes methods for mitigating spurious correlation as means of defence. Our empirical study reveals that the malicious triggers are highly correlated to their target labels; therefore such correlations are extremely distinguishable compared to those scores of benign features, and can be used to filter out potentially problematic instances. Compared with several existing defences, our defence method significantly reduces attack success rates across backdoor attacks, and in the case of insertion based attacks, our method provides a near-perfect defence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#36777;&#35770;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#36890;&#36807;&#20005;&#26684;&#30340;&#36777;&#35770;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.11595</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#30740;&#31350;&#65306;&#36890;&#36807;&#36777;&#35770;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate. (arXiv:2305.11595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#36777;&#35770;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#36890;&#36807;&#20005;&#26684;&#30340;&#36777;&#35770;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#37327;&#26679;&#26412;&#36890;&#35782;&#25512;&#29702;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#25317;&#26377;&#24378;&#22823;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#21508;&#31181;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#25506;&#32034;&#20004;&#20010;&#25110;&#22810;&#20010;LLMs&#20043;&#38388;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#19981;&#21516;&#21644;&#31934;&#30830;&#30340;&#20915;&#31574;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#36777;&#35770;&#26694;&#26550;&#65292;&#22312;7&#20010;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;LLMs&#19981;&#20165;&#36890;&#36807;&#22949;&#21327;&#21644;&#21453;&#39539;&#21464;&#24471;&#26356;&#20855;&#20869;&#37096;&#19968;&#33268;&#24615;&#65292;&#32780;&#19988;&#36824;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive zero-shot or few-shot commonsense reasoning performance on various natural language processing (NLP) tasks. However, despite their strong commonsense reasoning abilities, LLMs still exhibit various kinds of inconsistency problems. While previous researches mainly focused on the self-consistency within a single LLM, we propose to explore the inter-consistency issue between two or more LLMs, which is critical for diverse and precise decision-making processes. Since the LLMs possess human-like intelligence after instruction tuning and reinforcement learning with human feedback (RLHF), we design a formal debate framework to delve into the inter-consistency problem among LLMs with three-stage debate: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on 7 commonsense reasoning datasets, LLMs not only become more inter-consistent by compromising and refuting but also achieve higher performance and str
&lt;/p&gt;</description></item><item><title>IKDSumm&#26159;&#19968;&#20010;&#29305;&#23450;&#20110;&#28798;&#38590;&#20107;&#20214;&#30340;&#25512;&#25991;&#25688;&#35201;&#26694;&#26550;&#65292;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#23558;&#20851;&#38190;&#35789;&#34701;&#21512;&#21040;BERT&#20013;&#65292;&#20174;&#32780;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25688;&#35201;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.11592</link><description>&lt;p&gt;
IKDSumm&#65306;&#23558;&#20851;&#38190;&#35789;&#34701;&#20837;BERT&#29992;&#20110;&#28798;&#38590;&#25512;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
IKDSumm: Incorporating Key-phrases into BERT for extractive Disaster Tweet Summarization. (arXiv:2305.11592v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11592
&lt;/p&gt;
&lt;p&gt;
IKDSumm&#26159;&#19968;&#20010;&#29305;&#23450;&#20110;&#28798;&#38590;&#20107;&#20214;&#30340;&#25512;&#25991;&#25688;&#35201;&#26694;&#26550;&#65292;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#23558;&#20851;&#38190;&#35789;&#34701;&#21512;&#21040;BERT&#20013;&#65292;&#20174;&#32780;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25688;&#35201;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28798;&#38590;&#20107;&#20214;&#20013;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65288;&#22914;Twitter&#65289;&#26159;&#26368;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#26469;&#28304;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#20154;&#36947;&#20027;&#20041;&#32452;&#32455;&#12289;&#25919;&#24220;&#26426;&#26500;&#21644;&#24535;&#24895;&#32773;&#20381;&#38752;&#36825;&#20123;&#20449;&#24687;&#65288;&#21363;&#25512;&#25991;&#65289;&#30340;&#25688;&#35201;&#26469;&#36827;&#34892;&#26377;&#25928;&#30340;&#28798;&#38590;&#31649;&#29702;&#12290;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#29616;&#26377;&#30340;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#26041;&#27861;&#26469;&#23454;&#29616;&#33258;&#21160;&#25512;&#25991;&#25688;&#35201;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#20449;&#24687;&#65292;&#35201;&#20040;&#19981;&#21253;&#21547;&#26377;&#20851;&#28798;&#38590;&#30340;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#28798;&#38590;&#25688;&#35201;&#26041;&#27861;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#26469;&#25552;&#39640;&#25688;&#35201;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#21033;&#29992;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20154;&#21147;&#21162;&#21147;&#26469;&#29702;&#35299;&#25512;&#25991;&#30340;&#37325;&#35201;&#24615;&#65288;&#31361;&#20986;&#24615;&#65289;&#65292;&#36825;&#36827;&#19968;&#27493;&#26377;&#21161;&#20110;&#25688;&#35201;&#30340;&#21019;&#24314;&#21644;&#25552;&#39640;&#25688;&#35201;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29305;&#23450;&#20110;&#28798;&#38590;&#30340;&#25512;&#25991;&#25688;&#35201;&#26694;&#26550;IKDSumm&#65292;&#23427;&#29992;&#20110;&#65306;
&lt;/p&gt;
&lt;p&gt;
Online social media platforms, such as Twitter, are one of the most valuable sources of information during disaster events. Therefore, humanitarian organizations, government agencies, and volunteers rely on a summary of this information, i.e., tweets, for effective disaster management. Although there are several existing supervised and unsupervised approaches for automated tweet summary approaches, these approaches either require extensive labeled information or do not incorporate specific domain knowledge of disasters. Additionally, the most recent approaches to disaster summarization have proposed BERT-based models to enhance the summary quality. However, for further improved performance, we introduce the utilization of domain-specific knowledge without any human efforts to understand the importance (salience) of a tweet which further aids in summary creation and improves summary quality. In this paper, we propose a disaster-specific tweet summarization framework, IKDSumm, which init
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SPECTRA&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24212;&#29992;&#20102;&#26032;&#30340;&#26102;&#38388;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#26469;&#25429;&#25417;&#35821;&#38899;&#25991;&#26412;&#23545;&#40784;&#65292;&#21516;&#26102;&#23558;&#22238;&#31572;&#36873;&#25321;&#20219;&#21153;&#25512;&#24191;&#21040;&#26381;&#21153;&#20110;&#21475;&#35821;&#23545;&#35805;&#29702;&#35299;&#65292;&#29992;&#20110;&#20016;&#23500;&#35805;&#35821;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.11579</link><description>&lt;p&gt;
&#24102;&#26377;&#26174;&#24335;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#29992;&#20110;&#21475;&#35821;&#23545;&#35805;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Speech-Text Dialog Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment. (arXiv:2305.11579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SPECTRA&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24212;&#29992;&#20102;&#26032;&#30340;&#26102;&#38388;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#26469;&#25429;&#25417;&#35821;&#38899;&#25991;&#26412;&#23545;&#40784;&#65292;&#21516;&#26102;&#23558;&#22238;&#31572;&#36873;&#25321;&#20219;&#21153;&#25512;&#24191;&#21040;&#26381;&#21153;&#20110;&#21475;&#35821;&#23545;&#35805;&#29702;&#35299;&#65292;&#29992;&#20110;&#20016;&#23500;&#35805;&#35821;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#35768;&#22810;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#24120;&#38024;&#23545;&#19968;&#20010;&#25110;&#20004;&#20010;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#20102;&#23450;&#21046;&#65292;&#20294;&#26410;&#33021;&#24449;&#26381;&#21508;&#31181;&#35821;&#38899;&#25991;&#26412;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#26410;&#33021;&#25506;&#32034;&#23545;&#35805;&#20013;&#30340;&#24773;&#22659;&#20449;&#24687;&#20197;&#20016;&#23500;&#35805;&#35821;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#26174;&#24335;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;SPECTRA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#32771;&#34385;&#35821;&#38899;&#27169;&#24577;&#30340;&#26102;&#38388;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#39033;&#26032;&#30340;&#26102;&#38388;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#26469;&#25429;&#25417;&#35821;&#38899;&#25991;&#26412;&#23545;&#40784;&#12290;&#36825;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#26088;&#22312;&#39044;&#27979;&#30456;&#24212;&#35821;&#38899;&#27874;&#24418;&#20013;&#27599;&#20010;&#25991;&#26412;&#21333;&#35789;&#30340;&#24320;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23398;&#20064;&#21475;&#35821;&#23545;&#35805;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#23558;&#22238;&#31572;&#36873;&#25321;&#20219;&#21153;&#25512;&#24191;&#21040;&#26381;&#21153;&#20110;&#21475;&#35821;&#23545;&#35805;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, speech-text pre-training methods have shown remarkable success in many speech and natural language processing tasks. However, most previous pre-trained models are usually tailored for one or two specific tasks, but fail to conquer a wide range of speech-text tasks. In addition, existing speech-text pre-training methods fail to explore the contextual information within a dialogue to enrich utterance representations. In this paper, we propose Speech-text dialog Pre-training for spoken dialog understanding with ExpliCiT cRoss-Modal Alignment (SPECTRA), which is the first-ever speech-text dialog pre-training model. Concretely, to consider the temporality of speech modality, we design a novel temporal position prediction task to capture the speech-text alignment. This pre-training task aims to predict the start and end time of each textual word in the corresponding speech waveform. In addition, to learn the characteristics of spoken dialogs, we generalize a response selection task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#20197;&#22269;&#38469;&#38899;&#26631;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#36890;&#29992;&#38899;&#32032;&#27169;&#22411;&#65292;&#20026;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#21333;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2305.11576</link><description>&lt;p&gt;
&#35821;&#35328;&#36890;&#29992;&#38899;&#32032;&#32534;&#30721;&#22120;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Language-universal phonetic encoder for low-resource speech recognition. (arXiv:2305.11576v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#20197;&#22269;&#38469;&#38899;&#26631;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#36890;&#29992;&#38899;&#32032;&#27169;&#22411;&#65292;&#20026;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#21333;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35757;&#32451;&#23545;&#20110;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#26377;&#25928;&#65292;&#37096;&#20998;&#21407;&#22240;&#22312;&#20110;&#35821;&#38899;&#34920;&#31034;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#20849;&#20139;&#12290;&#22312;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#65292;&#23383;&#32032;&#36890;&#24120;&#34987;&#29992;&#20316;&#22522;&#26412;&#24314;&#27169;&#21333;&#20803;&#65292;&#28982;&#32780;&#23383;&#32032;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#22810;&#35821;&#35328;&#38899;&#32032;&#20849;&#20139;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20197;&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#36890;&#29992;&#38899;&#32032;&#27169;&#22411;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#65292;&#36825;&#26159;&#39318;&#27425;&#24212;&#29992;&#20110;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#20307;&#31995;&#26550;&#26500;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#26041;&#27861;&#65292;&#37319;&#29992;&#38899;&#26631;IPA&#27169;&#22411;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#26497;&#24230;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#22312;&#24320;&#28304;MLS&#35821;&#26009;&#24211;&#21644;&#20869;&#37096;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#32447;&#21333;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#26041;&#27861;&#21644;&#36866;&#24212;&#24615;&#26041;&#27861;&#36866;&#29992;&#20110;&#26497;&#24230;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#29978;&#33267;&#21253;&#25324;&#39046;&#22495;&#21644;&#35821;&#35328;&#19981;&#21305;&#37197;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual training is effective in improving low-resource ASR, which may partially be explained by phonetic representation sharing between languages. In end-to-end (E2E) ASR systems, graphemes are often used as basic modeling units, however graphemes may not be ideal for multilingual phonetic sharing. In this paper, we leverage International Phonetic Alphabet (IPA) based language-universal phonetic model to improve low-resource ASR performances, for the first time within the attention encoder-decoder architecture. We propose an adaptation method on the phonetic IPA model to further improve the proposed approach on extreme low-resource languages. Experiments carried out on the open-source MLS corpus and our internal databases show our approach outperforms baseline monolingual models and most state-of-the-art works. Our main approach and adaptation are effective on extremely low-resource languages, even within domain- and language-mismatched scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22269;&#38469;&#38899;&#26631;&#22810;&#35821;&#31181;&#27169;&#22411;&#20026;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#21019;&#36896;&#24103;&#32423;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#27492;&#25351;&#23548;&#38544;&#34255;&#21333;&#20803;BERT&#65288;HuBERT&#65289;&#30340;&#35821;&#38899;&#39044;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#35821;&#35328;&#35821;&#38899;&#65288;MLS&#65289;&#35821;&#26009;&#24211;&#20013;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#26631;&#20934;HuBERT&#65292;&#22312;&#25152;&#26377;&#30446;&#26631;&#35821;&#35328;&#19978;&#24615;&#33021;&#25552;&#39640;&#65292;&#19988;&#33021;&#33410;&#30465;&#22823;&#37327;&#21463;&#30417;&#30563;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.11569</link><description>&lt;p&gt;
&#35821;&#35328;&#36890;&#29992;&#30340;&#22269;&#38469;&#38899;&#26631;&#34920;&#31034;&#22312;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#22810;&#35821;&#31181;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Language-Universal Phonetic Representation in Multilingual Speech Pretraining for Low-Resource Speech Recognition. (arXiv:2305.11569v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22269;&#38469;&#38899;&#26631;&#22810;&#35821;&#31181;&#27169;&#22411;&#20026;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#21019;&#36896;&#24103;&#32423;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#27492;&#25351;&#23548;&#38544;&#34255;&#21333;&#20803;BERT&#65288;HuBERT&#65289;&#30340;&#35821;&#38899;&#39044;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#35821;&#35328;&#35821;&#38899;&#65288;MLS&#65289;&#35821;&#26009;&#24211;&#20013;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#26631;&#20934;HuBERT&#65292;&#22312;&#25152;&#26377;&#30446;&#26631;&#35821;&#35328;&#19978;&#24615;&#33021;&#25552;&#39640;&#65292;&#19988;&#33021;&#33410;&#30465;&#22823;&#37327;&#21463;&#30417;&#30563;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22810;&#35821;&#35328;&#35757;&#32451;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#24605;&#24819;&#34701;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#21033;&#29992;&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#22810;&#35821;&#31181;&#27169;&#22411;&#20026;&#26080;&#26631;&#35760;&#35821;&#38899;&#21019;&#24314;&#24103;&#32423;&#20266;&#26631;&#31614;&#65292;&#24182;&#20197;&#35821;&#38899;&#39044;&#35757;&#32451;&#20026;&#22522;&#30784;&#65292;&#20197;&#22269;&#38469;&#38899;&#26631;&#20026;&#20381;&#25454;&#25351;&#23548;&#38544;&#34255;&#21333;&#20803;BERT&#65288;HuBERT&#65289;&#30340;&#35821;&#38899;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20302;&#36164;&#28304;ASR&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25152;&#26377;&#30446;&#26631;&#35821;&#35328;&#19978;&#22343;&#27604;&#26631;&#20934;HuBERT&#34920;&#29616;&#26356;&#22909;&#12290;&#19982;&#26631;&#20934;HuBERT&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;4&#31181;&#35821;&#35328;&#20013;&#26377;3&#31181;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#33021;&#22815;&#26368;&#22810;&#33410;&#30465;1.5k&#23567;&#26102;&#65288;75%&#65289;&#30340;&#21463;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#12290; &#30456;&#23545;&#20110;XLSR-53&#21644;&#22522;&#20110;&#37325;&#26032;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#26041;&#27861;&#65292;&#26412;&#26041;&#27861;&#22312;&#23436;&#20840;&#21644;&#38480;&#21046;&#24494;&#35843;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We improve low-resource ASR by integrating the ideas of multilingual training and self-supervised learning. Concretely, we leverage an International Phonetic Alphabet (IPA) multilingual model to create frame-level pseudo labels for unlabeled speech, and use these pseudo labels to guide hidden-unit BERT (HuBERT) based speech pretraining in a phonetically-informed manner. The experiments on the Multilingual Speech (MLS) Corpus show that the proposed approach consistently outperforms the standard HuBERT on all the target languages. Moreover, on 3 of the 4 languages, comparing to the standard HuBERT, the approach performs better, meanwhile is able to save supervised training data by 1.5k hours (75%) at most. Our approach outperforms most of the state of the arts, with much less pretraining data in terms of hours and language diversity. Compared to XLSR-53 and a retraining based multilingual method, our approach performs better with full and limited finetuning data scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20214;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#19982;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;&#30693;&#35782;&#23384;&#20648;&#20998;&#31163;&#65292;&#37319;&#29992;&#21487;&#32534;&#36753;&#21644;&#21487;&#25193;&#23637;&#30340;&#38190;&#20540;&#23384;&#20648;&#22120;&#65292;&#36890;&#36807;DPM&#20013;&#30340;&#30693;&#35782;&#26816;&#32034;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#21033;&#29992;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.11564</link><description>&lt;p&gt;
&#35299;&#32806;&#21442;&#25968;&#20013;&#30340;&#30693;&#35782;&#65306;&#21487;&#25554;&#25300;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decouple knowledge from paramters for plug-and-play language modeling. (arXiv:2305.11564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20214;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#19982;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;&#30693;&#35782;&#23384;&#20648;&#20998;&#31163;&#65292;&#37319;&#29992;&#21487;&#32534;&#36753;&#21644;&#21487;&#25193;&#23637;&#30340;&#38190;&#20540;&#23384;&#20648;&#22120;&#65292;&#36890;&#36807;DPM&#20013;&#30340;&#30693;&#35782;&#26816;&#32034;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#21033;&#29992;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290; &#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#26159;&#36825;&#20123;&#27169;&#22411;&#30340;&#21442;&#25968;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#38544;&#21547;&#22320;&#23398;&#20064;&#20102;&#21508;&#31181;&#30693;&#35782;&#12290; &#28982;&#32780;&#65292;&#23558;&#30693;&#35782;&#38544;&#21547;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#20855;&#26377;&#20004;&#20010;&#22522;&#26412;&#32570;&#28857;&#12290; &#39318;&#20808;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#21518;&#65292;&#26080;&#27861;&#32534;&#36753;&#25110;&#25193;&#23637;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#22312;&#30693;&#35782;&#19981;&#26029;&#21457;&#23637;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#38382;&#39064;&#12290; &#20854;&#27425;&#65292;&#23427;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#24182;&#38459;&#27490;&#20154;&#20204;&#20102;&#35299;PLM&#22312;&#26576;&#20010;&#38382;&#39064;&#19978;&#25152;&#38656;&#30340;&#21738;&#20123;&#30693;&#35782;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;PlugLM&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#21487;&#24494;&#20998;&#25554;&#20214;&#23384;&#20648;&#22120;&#65288;DPM&#65289;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290; &#20851;&#38190;&#30340;&#30452;&#35273;&#26159;&#20351;&#29992;&#21487;&#32534;&#36753;&#21644;&#21487;&#25193;&#23637;&#30340;&#38190;&#20540;&#23384;&#20648;&#22120;&#23558;&#30693;&#35782;&#23384;&#20648;&#19982;&#27169;&#22411;&#21442;&#25968;&#20998;&#31163;&#65292;&#24182;&#36890;&#36807;DPM&#20013;&#30340;&#30693;&#35782;&#26816;&#32034;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#21033;&#29992;&#30693;&#35782;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#30340;&#21512;&#29702;&#24615;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#35774;&#32622;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#20123;&#35774;&#32622;&#38656;&#35201;&#21508;&#31181;&#24418;&#24335;&#30340;&#30693;&#35782;&#65306;&#65288;1&#65289;&#39046;&#22495;&#36866;&#24212;&#65292;&#65288;2&#65289;&#26410;&#35265;&#23454;&#20307;&#21512;&#24182;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22312;&#19981;&#36951;&#24536;&#26087;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models(PLM) have made impressive results in various NLP tasks. It has been revealed that one of the key factors to their success is the parameters of these models implicitly learn all kinds of knowledge during pre-training. However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks. First, the knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. Second, it lacks interpretability and prevents humans from understanding which knowledge PLM requires for a certain problem. In this paper, we introduce PlugLM, a pre-training model with differentiable plug-in memory(DPM). The key intuition is to decouple the knowledge storage from model parameters with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the DPM. To justify this design choice, we conduct evaluations in three settings in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;CTC&#20013;&#30340;&#38750;&#31354;&#30333;&#31526;&#21495;&#30340;&#33258;&#29615;&#65292;&#26126;&#30830;&#22320;&#40723;&#21169;&#26356;&#22810;&#30340;&#31354;&#30333;&#31526;&#21495;&#65292;&#24182;&#25104;&#21151;&#23558;&#31070;&#32463;&#36716;&#24405;&#22120;&#30340;&#25512;&#29702;&#21152;&#36895;4&#20493;&#65292;&#32780;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11558</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#36716;&#24405;&#22120;&#20013;&#20351;&#29992;&#31354;&#30333;&#27491;&#21017;&#21270;CTC&#35299;&#20915;&#24103;&#36339;&#36807;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Blank-regularized CTC for Frame Skipping in Neural Transducer. (arXiv:2305.11558v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;CTC&#20013;&#30340;&#38750;&#31354;&#30333;&#31526;&#21495;&#30340;&#33258;&#29615;&#65292;&#26126;&#30830;&#22320;&#40723;&#21169;&#26356;&#22810;&#30340;&#31354;&#30333;&#31526;&#21495;&#65292;&#24182;&#25104;&#21151;&#23558;&#31070;&#32463;&#36716;&#24405;&#22120;&#30340;&#25512;&#29702;&#21152;&#36895;4&#20493;&#65292;&#32780;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36716;&#24405;&#22120;&#21644;&#36830;&#25509;&#26102;&#24207;&#20998;&#31867;&#65288;CTC&#65289;&#37117;&#26159;&#24120;&#35265;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;&#30001;&#20110;&#20854;&#22522;&#20110;&#24103;&#21516;&#27493;&#35774;&#35745;&#65292;&#24341;&#20837;&#20102;&#31354;&#30333;&#31526;&#21495;&#20197;&#35299;&#20915;&#22768;&#23398;&#24103;&#21644;&#36755;&#20986;&#20196;&#29260;&#20043;&#38388;&#30340;&#38271;&#24230;&#19981;&#21305;&#37197;&#65292;&#36825;&#21487;&#33021;&#24102;&#26469;&#20887;&#20313;&#35745;&#31639;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#20002;&#24323;&#30001;&#32852;&#21512;&#35757;&#32451;&#30340;CTC&#39044;&#27979;&#30340;&#31354;&#30333;&#31526;&#21495;&#65292;&#26469;&#21152;&#36895;&#31070;&#32463;&#36716;&#24405;&#22120;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#32852;&#21512;&#35757;&#32451;&#30340;CTC&#19981;&#33021;&#20445;&#35777;&#26368;&#22823;&#21270;&#31354;&#30333;&#31526;&#21495;&#30340;&#27604;&#20363;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;CTC&#20013;&#30340;&#38750;&#31354;&#30333;&#31526;&#21495;&#30340;&#33258;&#29615;&#65292;&#26126;&#30830;&#22320;&#40723;&#21169;&#26356;&#22810;&#30340;&#31354;&#30333;&#31526;&#21495;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#31070;&#32463;&#36716;&#24405;&#22120;&#30340;&#24103;&#38477;&#20302;&#29575;&#21487;&#20197;&#25509;&#36817;&#29702;&#35770;&#36793;&#30028;&#12290;&#22312;LibriSpeech&#35821;&#26009;&#24211;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23558;&#31070;&#32463;&#36716;&#24405;&#22120;&#30340;&#25512;&#29702;&#21152;&#36895;4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Transducer and connectionist temporal classification (CTC) are popular end-to-end automatic speech recognition systems. Due to their frame-synchronous design, blank symbols are introduced to address the length mismatch between acoustic frames and output tokens, which might bring redundant computation. Previous studies managed to accelerate the training and inference of neural Transducers by discarding frames based on the blank symbols predicted by a co-trained CTC. However, there is no guarantee that the co-trained CTC can maximize the ratio of blank symbols. This paper proposes two novel regularization methods to explicitly encourage more blanks by constraining the self-loop of non-blank symbols in the CTC. It is interesting to find that the frame reduction ratio of the neural Transducer can approach the theoretical boundary. Experiments on LibriSpeech corpus show that our proposed method accelerates the inference of neural Transducer by 4 times without sacrificing performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolkenGPT&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11554</link><description>&lt;p&gt;
ToolkenGPT&#65306;&#36890;&#36807;&#24037;&#20855;&#23884;&#20837;&#25193;&#20805;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolkenGPT&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22806;&#37096;&#24037;&#20855;&#32467;&#21512;&#36215;&#26469;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#29992;&#24037;&#20855;&#28436;&#31034;&#25968;&#25454;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#26082;&#36153;&#26102;&#21448;&#21463;&#38480;&#20110;&#39044;&#23450;&#20041;&#30340;&#24037;&#20855;&#38598;&#12290;&#26368;&#36817;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#20363;&#32531;&#35299;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#26159;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21482;&#20801;&#35768;&#28436;&#31034;&#20960;&#27425;&#65292;&#23548;&#33268;&#23545;&#24037;&#20855;&#30340;&#29702;&#35299;&#19981;&#22815;&#20805;&#20998;&#12290;&#27492;&#22806;&#65292;&#24403;&#26377;&#22823;&#37327;&#24037;&#20855;&#21487;&#20379;&#36873;&#25321;&#26102;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#33021;&#23436;&#20840;&#26080;&#27861;&#27491;&#24120;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;$\textbf{ToolkenGPT}$&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23558;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;$\underline{&#24037;&#20855;}$&#34920;&#31034;&#20026;&#19968;&#20010;$\underline{token}$&#65288;$\textit{toolken}$&#65289;&#65292;&#24182;&#20026;&#20854;&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;&#65292;&#20351;&#24471;&#24037;&#20855;&#35843;&#29992;&#19982;&#29983;&#25104;&#24120;&#35268;&#21333;&#35789;&#26631;&#35760;&#30340;&#26041;&#24335;&#30456;&#21516;&#12290;&#19968;&#26086;&#35302;&#21457;&#20102;toolken&#65292;LLM&#34987;&#25552;&#31034;&#23436;&#25104;&#24037;&#20855;&#25191;&#34892;&#25152;&#38656;&#30340;&#21442;&#25968;&#12290;ToolkenGPT&#25552;&#20379;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;1&#65289;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#20197;&#25193;&#20805;LLM&#19982;&#22806;&#37096;&#24037;&#20855;&#30340;&#20132;&#20114;&#65292;2&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#20363;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;3&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our approach represents each $\underline{tool}$ as a to$\underline{ken}$ ($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#20934;&#21270;&#20114;&#20449;&#24687;&#30340;&#26080;&#30417;&#30563;&#31185;&#25216;&#35770;&#25991;&#25688;&#35201;&#20998;&#21106;&#26041;&#27861;&#65292;&#20854;&#20013;GreedyCAS&#22312;&#38750;&#32467;&#26500;&#21270;&#25688;&#35201;&#21644;&#32467;&#26500;&#21270;&#25688;&#35201;&#20013;&#22343;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.11553</link><description>&lt;p&gt;
&#22522;&#20110;&#26631;&#20934;&#21270;&#20114;&#20449;&#24687;&#30340;&#26080;&#30417;&#30563;&#31185;&#25216;&#35770;&#25991;&#25688;&#35201;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Scientific Abstract Segmentation with Normalized Mutual Information. (arXiv:2305.11553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#20934;&#21270;&#20114;&#20449;&#24687;&#30340;&#26080;&#30417;&#30563;&#31185;&#25216;&#35770;&#25991;&#25688;&#35201;&#20998;&#21106;&#26041;&#27861;&#65292;&#20854;&#20013;GreedyCAS&#22312;&#38750;&#32467;&#26500;&#21270;&#25688;&#35201;&#21644;&#32467;&#26500;&#21270;&#25688;&#35201;&#20013;&#22343;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#25216;&#35770;&#25991;&#25688;&#35201;&#30001;&#21069;&#25552;&#21644;&#32467;&#35770;&#32452;&#25104;&#12290;&#32467;&#26500;&#21270;&#25688;&#35201;&#26126;&#30830;&#31361;&#20986;&#32467;&#35770;&#21477;&#23376;&#65292;&#32780;&#38750;&#32467;&#26500;&#21270;&#25688;&#35201;&#21017;&#21487;&#33021;&#22312;&#19981;&#30830;&#23450;&#20301;&#32622;&#26377;&#32467;&#35770;&#21477;&#23376;&#65292;&#36825;&#31181;&#38544;&#21547;&#30340;&#32467;&#35770;&#20301;&#32622;&#20351;&#24471;&#31185;&#25216;&#35770;&#25991;&#25688;&#35201;&#30340;&#33258;&#21160;&#20998;&#21106;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#25506;&#32034;&#20351;&#29992;&#26631;&#20934;&#21270;&#20114;&#20449;&#24687;&#65288;NMI&#65289;&#36827;&#34892;&#25688;&#35201;&#20998;&#21106;&#12290;&#25105;&#20204;&#23558;&#27599;&#20010;&#25688;&#35201;&#35270;&#20026;&#19968;&#20010;&#24490;&#29615;&#30340;&#21477;&#23376;&#65292;&#24182;&#36890;&#36807;&#36138;&#23146;&#22320;&#20248;&#21270;&#21069;&#25552;&#21644;&#32467;&#35770;&#20043;&#38388;&#30340;NMI&#20998;&#25968;&#26469;&#25918;&#32622;&#20998;&#21106;&#36793;&#30028;&#12290;&#22312;&#38750;&#32467;&#26500;&#21270;&#25688;&#35201;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;GreedyCAS&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#22312;&#32467;&#26500;&#21270;&#25688;&#35201;&#20013;&#65292;GreedyCAS&#36890;&#36807;$P_k$&#30340;&#21508;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;NMI&#19982;&#25105;&#20204;&#30340;&#35780;&#20272;&#25351;&#26631;&#30340;&#24378;&#30456;&#20851;&#24615;&#25581;&#31034;&#20102;NMI&#22312;&#25688;&#35201;&#20998;&#21106;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abstracts of scientific papers consist of premises and conclusions. Structured abstracts explicitly highlight the conclusion sentences, whereas non-structured abstracts may have conclusion sentences at uncertain positions. This implicit nature of conclusion positions makes the automatic segmentation of scientific abstracts into premises and conclusions a challenging task. In this work, we empirically explore using Normalized Mutual Information (NMI) for abstract segmentation. We consider each abstract as a recurrent cycle of sentences and place segmentation boundaries by greedily optimizing the NMI score between premises and conclusions. On non-structured abstracts, our proposed unsupervised approach GreedyCAS achieves the best performance across all evaluation metrics; on structured abstracts, GreedyCAS outperforms all baseline methods measured by $P_k$. The strong correlation of NMI to our evaluation metrics reveals the effectiveness of NMI for abstract segmentation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#34920;&#24449;&#36716;&#31227;&#28508;&#21147;&#65288;RTP&#65289;&#26469;&#34913;&#37327;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#21457;&#29616;&#22810;&#36335;&#24182;&#34892;&#37325;&#21472;&#26159;&#20851;&#38190;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#40723;&#21169;&#34920;&#24449;&#22312;&#35821;&#35328;&#20043;&#38388;&#26356;&#20855;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#21644;&#27169;&#22411;&#35774;&#32622;&#20013;&#25552;&#39640;&#20102;&#20302;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.11550</link><description>&lt;p&gt;
&#36879;&#36807;&#34920;&#24449;&#38236;&#22836;&#30475;&#24453;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens. (arXiv:2305.11550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11550
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#34920;&#24449;&#36716;&#31227;&#28508;&#21147;&#65288;RTP&#65289;&#26469;&#34913;&#37327;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#21457;&#29616;&#22810;&#36335;&#24182;&#34892;&#37325;&#21472;&#26159;&#20851;&#38190;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#40723;&#21169;&#34920;&#24449;&#22312;&#35821;&#35328;&#20043;&#38388;&#26356;&#20855;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#21644;&#27169;&#22411;&#35774;&#32622;&#20013;&#25552;&#39640;&#20102;&#20302;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;&#65292;&#20165;&#20165;&#20381;&#38752;&#32763;&#35793;&#36136;&#37327;&#24230;&#37327;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#26159;&#19981;&#22815;&#30340;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#34920;&#24449;&#36716;&#31227;&#28508;&#21147;&#65288;RTP&#65289;&#65292;&#23427;&#27979;&#37327;&#35821;&#35328;&#20043;&#38388;&#30340;&#34920;&#24449;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RTP&#21487;&#20197;&#27979;&#37327;&#27491;&#21521;&#21644;&#36127;&#21521;&#36716;&#31227;&#65288;&#24178;&#25200;&#65289;&#65292;&#24182;&#21457;&#29616;RTP&#19982;&#32763;&#35793;&#36136;&#37327;&#21464;&#21270;&#24378;&#30456;&#20851;&#65292;&#34920;&#26126;&#30830;&#23454;&#23384;&#22312;&#36716;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#36716;&#31227;&#30456;&#20851;&#30340;&#25968;&#25454;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#24182;&#21457;&#29616;&#22810;&#36335;&#24182;&#34892;&#37325;&#21472;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#26377;&#25506;&#32034;&#30340;&#29305;&#24449;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20351;&#29992;&#36741;&#21161;&#30456;&#20284;&#24615;&#25439;&#22833;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#36335;&#24182;&#34892;&#25968;&#25454;&#65292;&#40723;&#21169;&#34920;&#24449;&#22312;&#35821;&#35328;&#20043;&#38388;&#26356;&#20855;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#21644;&#27169;&#22411;&#35774;&#32622;&#20013;&#25552;&#39640;&#20102;&#20302;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We argue that translation quality alone is not a sufficient metric for measuring knowledge transfer in multilingual neural machine translation. To support this claim, we introduce Representational Transfer Potential (RTP), which measures representational similarities between languages. We show that RTP can measure both positive and negative transfer (interference), and find that RTP is strongly correlated with changes in translation quality, indicating that transfer does occur. Furthermore, we investigate data and language characteristics that are relevant for transfer, and find that multi-parallel overlap is an important yet under-explored feature. Based on this, we develop a novel training scheme, which uses an auxiliary similarity loss that encourages representations to be more invariant across languages by taking advantage of multi-parallel data. We show that our method yields increased translation quality for low- and mid-resource languages across multiple data and model setups.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#35789;-&#19978;&#19979;&#25991;&#32806;&#21512;&#31354;&#38388;&#65292;&#24182;&#24341;&#20837;&#32852;&#24819;&#30693;&#35782;&#32593;&#32476;&#21644;&#19978;&#19979;&#25991;&#30456;&#23545;&#36317;&#31163;&#20316;&#20026;&#35821;&#20041;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#24314;&#27169;&#21487;&#35299;&#37322;&#24615;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.11543</link><description>&lt;p&gt;
&#26500;&#24314;&#22522;&#20110;&#32852;&#24819;&#30693;&#35782;&#20851;&#31995;&#30340;&#35789;-&#19978;&#19979;&#25991;&#32806;&#21512;&#31354;&#38388;&#29992;&#20110;&#21487;&#35299;&#37322;&#21270;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Constructing Word-Context-Coupled Space Aligned with Associative Knowledge Relations for Interpretable Language Modeling. (arXiv:2305.11543v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#35789;-&#19978;&#19979;&#25991;&#32806;&#21512;&#31354;&#38388;&#65292;&#24182;&#24341;&#20837;&#32852;&#24819;&#30693;&#35782;&#32593;&#32476;&#21644;&#19978;&#19979;&#25991;&#30456;&#23545;&#36317;&#31163;&#20316;&#20026;&#35821;&#20041;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#24314;&#27169;&#21487;&#35299;&#37322;&#24615;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#30340;&#22522;&#30784;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40657;&#31665;&#32467;&#26500;&#20005;&#37325;&#38480;&#21046;&#20102;&#35821;&#35328;&#24314;&#27169;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#28145;&#24230;&#31070;&#32463;&#34920;&#31034;&#21644;&#35821;&#35328;&#24314;&#27169;&#30340;&#35821;&#20041;&#36923;&#36753;&#20043;&#38388;&#30340;&#32806;&#21512;&#35201;&#27714;&#65292;&#24341;&#20837;&#20102;&#35789;-&#19978;&#19979;&#25991;&#32806;&#21512;&#31354;&#38388;(W2CSpace)&#65292;&#36890;&#36807;&#20171;&#32461;&#21487;&#35299;&#37322;&#30340;&#32479;&#35745;&#36923;&#36753;&#21644;&#19981;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#40784;&#22788;&#29702;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#32858;&#31867;&#36807;&#31243;&#26469;&#36830;&#25509;&#35789;&#32423;&#21644;&#19978;&#19979;&#25991;&#32423;&#35821;&#20041;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#23545;&#40784;&#35789;&#32423;&#35821;&#20041;&#30340;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#32479;&#35745;&#36923;&#36753;&#30340;&#32852;&#24819;&#30693;&#35782;&#32593;&#32476;(AKN)&#12290;&#27492;&#22806;&#65292;&#19978;&#19979;&#25991;&#30456;&#23545;&#36317;&#31163;&#34987;&#29992;&#20316;&#19979;&#28216;&#20998;&#31867;&#22120;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#36825;&#19982;&#24403;&#21069;&#30340;&#38750;&#35299;&#37322;&#24615;&#26041;&#27861;&#38750;&#24120;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the foundation of current natural language processing methods, pre-trained language model has achieved excellent performance. However, the black-box structure of the deep neural network in pre-trained language models seriously limits the interpretability of the language modeling process. After revisiting the coupled requirement of deep neural representation and semantics logic of language modeling, a Word-Context-Coupled Space (W2CSpace) is proposed by introducing the alignment processing between uninterpretable neural representation and interpretable statistical logic. Moreover, a clustering process is also designed to connect the word- and context-level semantics. Specifically, an associative knowledge network (AKN), considered interpretable statistical logic, is introduced in the alignment process for word-level semantics. Furthermore, the context-relative distance is employed as the semantic feature for the downstream classifier, which is greatly different from the current unint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#34892;&#19994;&#20113;&#29305;&#23450;QA&#25968;&#25454;&#38598; MSQA&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29305;&#23450;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20132;&#20114;&#33539;&#24335;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11541</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#19994;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering. (arXiv:2305.11541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#34892;&#19994;&#20113;&#29305;&#23450;QA&#25968;&#25454;&#38598; MSQA&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29305;&#23450;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20132;&#20114;&#33539;&#24335;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#24320;&#25918;&#39046;&#22495;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#24191;&#27867;&#24212;&#29992;&#21644;&#21331;&#36234;&#30340;&#25104;&#26524;&#65292;&#20294;&#20854;&#22312;&#30495;&#23454;&#30340;&#24037;&#19994;&#29305;&#23450;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#36890;&#24120;&#24456;&#24179;&#24248;&#65292;&#22240;&#20026;&#23427;&#32570;&#20047;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#36825;&#20010;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#24456;&#23569;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;MSQA&#30340;&#22522;&#20934;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#28041;&#21450;Microsoft&#20135;&#21697;&#21644;&#23458;&#25143;&#36935;&#21040;&#30340;IT&#25216;&#26415;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#34892;&#19994;&#20113;&#30340;&#29305;&#23450;QA&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#19968;&#33324;&#30340;LLM&#26469;&#35828;&#26159;&#19981;&#21487;&#29992;&#30340;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#21512;&#35780;&#20272;&#26088;&#22312;&#25552;&#39640;LLM&#29305;&#23450;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20132;&#20114;&#33539;&#24335;&#65292;&#21487;&#20197;&#20351;LLM&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36981;&#24490;&#25105;&#20204;&#30340;&#27169;&#22411;&#34701;&#21512;&#26694;&#26550;&#30340;&#26041;&#27861;&#27604;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#30340;&#24120;&#29992;LLM&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) has gained popularity and achieved remarkable results in open-domain tasks, but its performance in real industrial domain-specific scenarios is average since there is no specific knowledge in it. This issue has attracted widespread attention, but there are few relevant benchmarks available. In this paper, we provide a benchmark Question Answering (QA) dataset named MSQA, which is about Microsoft products and IT technical problems encountered by customers. This dataset contains industry cloud-specific QA knowledge, which is not available for general LLM, so it is well suited for evaluating methods aimed at improving domain-specific capabilities of LLM. In addition, we propose a new model interaction paradigm that can empower LLM to achieve better performance on domain-specific tasks where it is not proficient. Extensive experiments demonstrate that the approach following our model fusion framework outperforms the commonly used LLM with retrieval methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#36807;&#20248;&#21270;&#30340;&#36328;&#35821;&#35328;&#36716;&#25442;&#26041;&#27861;IAP&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22270;&#20687;&#20316;&#20026;&#26530;&#32445;&#65292;&#22312;&#20013;&#33521;&#25991;&#20043;&#38388;&#24314;&#31435;&#35821;&#20041;&#36830;&#25509;&#65292;&#23558;&#33521;&#25991;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#36716;&#21270;&#20026;&#20013;&#25991;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#27604;&#22810;&#31181;&#24378;&#22823;&#30340;&#20013;&#25991;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#29255;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2305.11540</link><description>&lt;p&gt;
&#20197;&#22270;&#20687;&#20026;&#26530;&#32445;&#30340;&#20013;&#25991;&#31283;&#23450;&#25193;&#25955;&#30340;&#39640;&#25928;&#36328;&#35821;&#35328;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Efficient Cross-Lingual Transfer for Chinese Stable Diffusion with Images as Pivots. (arXiv:2305.11540v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#36807;&#20248;&#21270;&#30340;&#36328;&#35821;&#35328;&#36716;&#25442;&#26041;&#27861;IAP&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22270;&#20687;&#20316;&#20026;&#26530;&#32445;&#65292;&#22312;&#20013;&#33521;&#25991;&#20043;&#38388;&#24314;&#31435;&#35821;&#20041;&#36830;&#25509;&#65292;&#23558;&#33521;&#25991;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#36716;&#21270;&#20026;&#20013;&#25991;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#27604;&#22810;&#31181;&#24378;&#22823;&#30340;&#20013;&#25991;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#29255;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#65288;&#20363;&#22914;&#31283;&#23450;&#25193;&#25955;&#65289;&#38656;&#35201;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#28023;&#37327;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;-&#22270;&#20687;&#37197;&#23545;&#65292;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#21017;&#26080;&#27861;&#25215;&#21463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IAP&#65292;&#19968;&#31181;&#23558;&#33521;&#25991;&#31283;&#23450;&#25193;&#25955;&#36716;&#31227;&#21040;&#20013;&#25991;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;IAP &#20165;&#20248;&#21270;&#19968;&#20010;&#21333;&#29420;&#30340;&#20013;&#25991;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#25152;&#26377;&#20854;&#20182;&#21442;&#25968;&#37117;&#20445;&#25345;&#19981;&#21464;&#65292;&#20197;&#23558;&#20013;&#25991;&#35821;&#20041;&#31354;&#38388;&#19982; CLIP &#20013;&#30340;&#33521;&#25991;&#35821;&#20041;&#31354;&#38388;&#23545;&#40784;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#23558;&#22270;&#20687;&#35270;&#20026;&#26530;&#32445;&#65292;&#24182;&#26368;&#23567;&#21270;&#36890;&#36807;&#22270;&#20687;&#21644;&#27599;&#31181;&#35821;&#35328;&#20043;&#38388;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#20135;&#29983;&#30340;&#27880;&#24847;&#29305;&#24449;&#30340;&#36317;&#31163;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;IAP &#22312; CLIP &#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#24314;&#31435;&#20102;&#20013;&#25991;&#12289;&#33521;&#35821;&#21644;&#35270;&#35273;&#35821;&#20041;&#30340;&#36830;&#25509;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20351;&#29992;&#30452;&#25509;&#20013;&#25991;&#25552;&#31034;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#22270;&#29255;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#24378;&#22823;&#30340;&#20013;&#25991;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have made impressive progress in text-to-image synthesis. However, training such large-scale models (e.g. Stable Diffusion), from scratch requires high computational costs and massive high-quality text-image pairs, which becomes unaffordable in other languages. To handle this challenge, we propose IAP, a simple but effective method to transfer English Stable Diffusion into Chinese. IAP optimizes only a separate Chinese text encoder with all other parameters fixed to align Chinese semantics space to the English one in CLIP. To achieve this, we innovatively treat images as pivots and minimize the distance of attentive features produced from cross-attention between images and each language respectively. In this way, IAP establishes connections of Chinese, English and visual semantics in CLIP's embedding space efficiently, advancing the quality of the generated image with direct Chinese prompts. Experimental results show that our method outperforms several strong Chinese d
&lt;/p&gt;</description></item><item><title>PORTRAIT&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#21019;&#24314;&#28798;&#38590;&#20107;&#20214;&#30340;&#25552;&#21462;&#24335;&#22522;&#20934;&#25688;&#35201;&#65292;&#26082;&#20351;&#29992;&#26234;&#33021;&#31639;&#27861;&#65292;&#21448;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26631;&#27880;&#32773;&#30340;&#20154;&#21147;&#25104;&#26412;&#24182;&#23454;&#29616;&#26356;&#21487;&#38752;&#30340;&#22522;&#20934;&#25688;&#35201;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.11536</link><description>&lt;p&gt;
PORTRAIT: &#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#21019;&#24314;&#28798;&#38590;&#20107;&#20214;&#30340;&#25552;&#21462;&#24335;&#22522;&#20934;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
PORTRAIT: a hybrid aPproach tO cReate extractive ground-TRuth summAry for dIsaster evenT. (arXiv:2305.11536v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11536
&lt;/p&gt;
&lt;p&gt;
PORTRAIT&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#21019;&#24314;&#28798;&#38590;&#20107;&#20214;&#30340;&#25552;&#21462;&#24335;&#22522;&#20934;&#25688;&#35201;&#65292;&#26082;&#20351;&#29992;&#26234;&#33021;&#31639;&#27861;&#65292;&#21448;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26631;&#27880;&#32773;&#30340;&#20154;&#21147;&#25104;&#26412;&#24182;&#23454;&#29616;&#26356;&#21487;&#38752;&#30340;&#22522;&#20934;&#25688;&#35201;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28798;&#38590;&#25688;&#35201;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#20851;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#21457;&#29983;&#30340;&#28798;&#38590;&#20107;&#20214;&#30340;&#37325;&#35201;&#20449;&#24687;&#30340;&#27010;&#36848;&#65292;&#20363;&#22914; Twitter&#12290;&#28982;&#32780;&#65292;&#21457;&#24067;&#30340;&#20449;&#24687;&#31867;&#22411;&#26681;&#25454;&#22320;&#28857;&#12289;&#31867;&#22411;&#12289;&#20005;&#37325;&#31243;&#24230;&#31561;&#22810;&#31181;&#22240;&#32032;&#22312;&#28798;&#38590;&#20013;&#26174;&#33879;&#21464;&#21270;&#12290;&#30001;&#20110;&#32570;&#20047;&#33391;&#22909;&#30340;&#25968;&#25454;&#38598;&#35889;&#65292;&#21152;&#20043;&#32570;&#20047;&#22522;&#20934;&#25688;&#35201;&#30340;&#21487;&#29992;&#24615;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#22522;&#20934;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65288;&#25552;&#21462;&#24335;&#25688;&#35201;&#30340;&#22522;&#20934;&#25688;&#35201;&#65289;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;&#20381;&#36182;&#20110;&#26631;&#27880;&#32773;&#30340;&#26234;&#24935;&#21644;&#30452;&#35273;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#21644;&#26102;&#38388;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PORTRAIT&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#26234;&#33021;&#31639;&#27861;&#21644;&#20154;&#24037;&#26234;&#33021;&#26469;&#21019;&#24314;&#28798;&#38590;&#20107;&#20214;&#30340;&#22522;&#20934;&#25688;&#35201;&#12290;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;&#26174;&#33879;&#38477;&#20302;&#20102;&#26631;&#27880;&#32773;&#30340;&#20154;&#21147;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#21487;&#38752;&#30340;&#22522;&#20934;&#25688;&#35201;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disaster summarization approaches provide an overview of the important information posted during disaster events on social media platforms, such as, Twitter. However, the type of information posted significantly varies across disasters depending on several factors like the location, type, severity, etc. Verification of the effectiveness of disaster summarization approaches still suffer due to the lack of availability of good spectrum of datasets along with the ground-truth summary. Existing approaches for ground-truth summary generation (ground-truth for extractive summarization) relies on the wisdom and intuition of the annotators. Annotators are provided with a complete set of input tweets from which a subset of tweets is selected by the annotators for the summary. This process requires immense human effort and significant time. Additionally, this intuition-based selection of the tweets might lead to a high variance in summaries generated across annotators. Therefore, to handle these
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38463;&#25289;&#20271;&#35821;&#20195;&#35789;&#28040;&#35299;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;AnATAr&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25163;&#24037;&#29305;&#24449;&#27169;&#22411;&#12290;&#30740;&#31350;&#32773;&#36824;&#25506;&#35752;&#20102;&#19968;&#20123;&#23545;&#27169;&#22411;&#30340;&#20462;&#25913;&#65292;&#36825;&#20123;&#20462;&#25913;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11529</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#38463;&#25289;&#20271;&#35821;&#20195;&#35789;&#28040;&#35299;&#38382;&#39064;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Sequence-to-Sequence Approach for Arabic Pronoun Resolution. (arXiv:2305.11529v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38463;&#25289;&#20271;&#35821;&#20195;&#35789;&#28040;&#35299;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;AnATAr&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25163;&#24037;&#29305;&#24449;&#27169;&#22411;&#12290;&#30740;&#31350;&#32773;&#36824;&#25506;&#35752;&#20102;&#19968;&#20123;&#23545;&#27169;&#22411;&#30340;&#20462;&#25913;&#65292;&#36825;&#20123;&#20462;&#25913;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;Bi-LSTM&#21644;BERT&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35299;&#20915;&#38463;&#25289;&#20271;&#35821;&#20195;&#35789;&#28040;&#35299;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;AnATAr&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#19982;&#20960;&#31181;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25163;&#24037;&#29305;&#24449;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#37117;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#21253;&#25324;KNN&#12289;&#36923;&#36753;&#22238;&#24402;&#21644;SVM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23545;&#27169;&#22411;&#30340;&#21508;&#31181;&#20462;&#25913;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#23558;&#25351;&#20195;&#35789;&#25991;&#26412;&#19982;&#27573;&#33853;&#25991;&#26412;&#36830;&#25509;&#20316;&#20026;&#36755;&#20837;&#12289;&#28155;&#21152;&#25513;&#30721;&#20197;&#20851;&#27880;&#20505;&#36873;&#20998;&#25968;&#20197;&#21450;&#22522;&#20110;&#25351;&#20195;&#35789;&#30340;&#24615;&#21035;&#21644;&#25968;&#37327;&#21327;&#35758;&#26469;&#36807;&#28388;&#20505;&#36873;&#39033;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#20462;&#25913;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a sequence-to-sequence learning approach for Arabic pronoun resolution, which explores the effectiveness of using advanced natural language processing (NLP) techniques, specifically Bi-LSTM and the BERT pre-trained Language Model, in solving the pronoun resolution problem in Arabic. The proposed approach is evaluated on the AnATAr dataset, and its performance is compared to several baseline models, including traditional machine learning models and handcrafted feature-based models. Our results demonstrate that the proposed model outperforms the baseline models, which include KNN, logistic regression, and SVM, across all metrics. In addition, we explore the effectiveness of various modifications to the model, including concatenating the anaphor text beside the paragraph text as input, adding a mask to focus on candidate scores, and filtering candidates based on gender and number agreement with the anaphor. Our results show that these modifications significantly improv
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.11527</link><description>&lt;p&gt;
InstructIE: &#19968;&#20221;&#22522;&#20110;&#25351;&#20196;&#30340;&#20013;&#25991;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
InstructIE: A Chinese Instruction-based Information Extraction Dataset. (arXiv:2305.11527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11527
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#31216;&#20026;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462; (Instruction-based IE)&#65292;&#23427;&#26088;&#22312;&#35201;&#27714;&#31995;&#32479;&#36981;&#24490;&#29305;&#23450;&#30340;&#25351;&#20196;&#25110;&#25351;&#21335;&#26469;&#25552;&#21462;&#20449;&#24687;&#12290;&#20026;&#20102;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#20013;&#25991;&#32500;&#22522;&#30334;&#31185;&#30340; 270,000 &#20010;&#24369;&#30417;&#30563;&#25968;&#25454;&#21644; 1,000 &#20010;&#39640;&#36136;&#37327;&#20247;&#21253;&#27880;&#37322;&#23454;&#20363;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#22312;InstructIE&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#24456;&#26377;&#24076;&#26395;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312; https://github.com/zjunlp/DeepKE/tree/main/example/llm &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new Information Extraction (IE) task dubbed Instruction-based IE, which aims to ask the system to follow specific instructions or guidelines to extract information. To facilitate research in this area, we construct a dataset called InstructIE, consisting of 270,000 weakly supervised data from Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We further evaluate the performance of various baseline models on the InstructIE dataset. The results reveal that although current models exhibit promising performance, there is still room for improvement. Furthermore, we conduct a comprehensive case study analysis, underlining the challenges inherent in the Instruction-based IE task. Code and dataset are available at https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DiffuSIA&#30340;&#34746;&#26059;&#20132;&#20114;&#26550;&#26500;&#65292;&#29992;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25991;&#26412;&#25193;&#25955;&#12290;&#22312;&#36825;&#20010;&#26550;&#26500;&#20013;&#65292;&#26465;&#20214;&#20449;&#24687;&#21644;&#30446;&#26631;&#20449;&#24687;&#20250;&#20132;&#20114;&#25429;&#33719;&#65292;&#20197;&#25552;&#39640;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11517</link><description>&lt;p&gt;
DiffuSIA&#65306;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25991;&#26412;&#25193;&#25955;&#30340;&#34746;&#26059;&#20132;&#20114;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
DiffuSIA: A Spiral Interaction Architecture for Encoder-Decoder Text Diffusion. (arXiv:2305.11517v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DiffuSIA&#30340;&#34746;&#26059;&#20132;&#20114;&#26550;&#26500;&#65292;&#29992;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25991;&#26412;&#25193;&#25955;&#12290;&#22312;&#36825;&#20010;&#26550;&#26500;&#20013;&#65292;&#26465;&#20214;&#20449;&#24687;&#21644;&#30446;&#26631;&#20449;&#24687;&#20250;&#20132;&#20114;&#25429;&#33719;&#65292;&#20197;&#25552;&#39640;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#26032;&#19968;&#20195;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#20195;&#34920;&#65292;&#24182;&#19988;&#36817;&#24180;&#26469;&#23427;&#20204;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#30340;&#28508;&#21147;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#22823;&#22810;&#37319;&#29992;&#21333;&#20010;&#32534;&#30721;&#22120;&#32467;&#26500;&#21644;&#37096;&#20998;&#22122;&#22768;&#36807;&#31243;&#29992;&#20110;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#65292;&#20294;&#26159;&#20854;&#23545;&#20110;&#26465;&#20214;&#24314;&#27169;&#30340;&#28789;&#27963;&#24615;&#26377;&#38480;&#12290;&#23454;&#38469;&#19978;&#65292;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#23545;&#20110;&#23427;&#30340;&#21487;&#20998;&#31163;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22359;&#22825;&#28982;&#26356;&#21152;&#28789;&#27963;&#65292;&#21487;&#25193;&#23637;&#21040;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26377;&#26465;&#20214;&#30340;&#25991;&#26412;&#32534;&#30721;&#36807;&#31243;&#32570;&#20047;&#23545;&#30446;&#26631;&#25991;&#26412;&#30340;&#29702;&#35299;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25991;&#26412;&#25193;&#25955;&#30340;&#34746;&#26059;&#20132;&#20114;&#26550;&#26500;&#65288;DiffuSIA&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35774;&#35745;&#20102;&#20174;&#32534;&#30721;&#22120;&#25429;&#33719;&#26465;&#20214;&#20449;&#24687;&#24182;&#34987;&#25193;&#25955;&#35299;&#30721;&#22120;&#25429;&#33719;&#65292;&#20197;&#21450;&#20174;&#35299;&#30721;&#22120;&#25429;&#33719;&#30446;&#26631;&#20449;&#24687;&#24182;&#34987;&#26465;&#20214;&#32534;&#30721;&#22120;&#25429;&#33719;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as the new state-of-the-art family of deep generative models, and their promising potentials for text generation have recently attracted increasing attention. Existing studies mostly adopt a single encoder architecture with partially noising processes for conditional text generation, but its degree of flexibility for conditional modeling is limited. In fact, the encoder-decoder architecture is naturally more flexible for its detachable encoder and decoder modules, which is extensible to multilingual and multimodal generation tasks for conditions and target texts. However, the encoding process of conditional texts lacks the understanding of target texts. To this end, a spiral interaction architecture for encoder-decoder text diffusion (DiffuSIA) is proposed. Concretely, the conditional information from encoder is designed to be captured by the diffusion decoder, while the target information from decoder is designed to be captured by the conditional encoder.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#21644;&#35789;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20004;&#20010;&#35821;&#26009;&#24211;&#20013;&#21457;&#29616;&#21333;&#35789;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#22312;&#22788;&#29702;&#35821;&#26009;&#24211;&#22823;&#23567;&#30340;&#20559;&#26012;&#26041;&#38754;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#19981;&#24120;&#35265;&#21333;&#35789;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#25351;&#20986;&#22312;&#27604;&#36739;&#30340;&#20004;&#20010;&#35821;&#26009;&#24211;&#20013;&#32570;&#22833;&#26576;&#20010;&#21547;&#20041;&#30340;&#21333;&#35789;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.11516</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#22659;&#21270;&#35789;&#21521;&#37327;&#30340;&#26080;&#38656;&#35757;&#32451;&#21644;&#35789;&#23545;&#40784;&#30340;&#21457;&#29616;&#35821;&#20041;&#24046;&#24322;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Contextualized Word Vector-based Methods for Discovering Semantic Differences with No Training nor Word Alignment. (arXiv:2305.11516v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#21644;&#35789;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20004;&#20010;&#35821;&#26009;&#24211;&#20013;&#21457;&#29616;&#21333;&#35789;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#22312;&#22788;&#29702;&#35821;&#26009;&#24211;&#22823;&#23567;&#30340;&#20559;&#26012;&#26041;&#38754;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#19981;&#24120;&#35265;&#21333;&#35789;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#25351;&#20986;&#22312;&#27604;&#36739;&#30340;&#20004;&#20010;&#35821;&#26009;&#24211;&#20013;&#32570;&#22833;&#26576;&#20010;&#21547;&#20041;&#30340;&#21333;&#35789;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#22659;&#21270;&#35789;&#21521;&#37327;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#35821;&#26009;&#24211;&#20013;&#21457;&#29616;&#20986;&#29616;&#30340;&#21333;&#35789;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#20854;&#24179;&#22343;&#35789;&#21521;&#37327;&#30340;&#35268;&#33539;&#26469;&#21453;&#26144;&#20854;&#24847;&#20041;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#25152;&#38656;&#30340;&#20851;&#20110;&#23545;&#27604;&#21333;&#35789;&#21644;&#35821;&#26009;&#24211;&#30340;&#20551;&#35774;&#12290;&#23427;&#20204;&#25152;&#38656;&#35201;&#30340;&#26159;&#35745;&#31639;&#35821;&#22659;&#21270;&#35789;&#21521;&#37327;&#30340;&#24179;&#22343;&#21521;&#37327;&#21450;&#20854;&#23545;&#20110;&#27599;&#20010;&#21333;&#35789;&#31867;&#22411;&#30340;&#35268;&#33539;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26159;(i) &#23545;&#20110;&#35821;&#26009;&#24211;&#22823;&#23567;&#30340;&#20559;&#26012;&#20855;&#26377;&#40065;&#26834;&#24615;; (ii) &#33021;&#22815;&#26816;&#27979;&#20999;&#20837;&#21333;&#35789;&#30340;&#35821;&#20041;&#24046;&#24322;; (iii) &#26377;&#25928;&#22320;&#25351;&#20986;&#22312;&#23545;&#27604;&#30340;&#20004;&#20010;&#35821;&#26009;&#24211;&#20013;&#32570;&#22833;&#26576;&#20010;&#21547;&#20041;&#30340;&#21333;&#35789;&#23454;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#26412;&#22303;&#21644;&#38750;&#26412;&#22303;&#33521;&#35821;&#35821;&#26009;&#24211;&#20197;&#21450;&#21382;&#21490;&#35821;&#26009;&#24211;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose methods for discovering semantic differences in words appearing in two corpora based on the norms of contextualized word vectors. The key idea is that the coverage of meanings is reflected in the norm of its mean word vector. The proposed methods do not require the assumptions concerning words and corpora for comparison that the previous methods do. All they require are to compute the mean vector of contextualized word vectors and its norm for each word type. Nevertheless, they are (i) robust for the skew in corpus size; (ii) capable of detecting semantic differences in infrequent words; and (iii) effective in pinpointing word instances that have a meaning missing in one of the two corpora for comparison. We show these advantages for native and non-native English corpora and also for historical corpora.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21307;&#30103;&#38382;&#31572;&#21450;&#35786;&#26029;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26114;&#36149;&#30340;LLMs&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2305.11508</link><description>&lt;p&gt;
&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play Medical Dialogue System. (arXiv:2305.11508v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11508
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21307;&#30103;&#38382;&#31572;&#21450;&#35786;&#26029;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26114;&#36149;&#30340;LLMs&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#20026;&#24739;&#32773;&#25552;&#20379;&#20934;&#30830;&#30340;&#31572;&#26696;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#36827;&#23637;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22312;&#21307;&#30103;&#38382;&#31572;&#39046;&#22495;&#20855;&#26377;&#26480;&#20986;&#30340;&#33021;&#21147;&#65292;&#34920;&#26126;&#20855;&#22791;&#20102;&#23545;&#24120;&#35782;&#30340;&#20016;&#23500;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#35786;&#26029;&#31574;&#30053;&#65292;LLMs&#26080;&#27861;&#30452;&#25509;&#29992;&#20110;&#35786;&#26029;&#12290;&#20256;&#32479;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#26114;&#36149;&#30340;LLMs&#24494;&#35843;&#12290;&#21478;&#19968;&#31181;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#24320;&#21457;&#19968;&#20010;&#25554;&#20214;&#65292;&#36171;&#20104;LLMs&#25191;&#34892;&#21307;&#30103;&#23545;&#35805;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PlugMed&#65292;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#20004;&#20010;&#27169;&#22359;&#20419;&#36827;&#20102;LLMs&#30340;&#24688;&#24403;&#23545;&#35805;&#21160;&#20316;&#65306;&#25552;&#31034;&#29983;&#25104;&#65288;PG&#65289;&#27169;&#22359;&#21644;&#22238;&#22797;&#25490;&#21517;&#65288;RR&#65289;&#27169;&#22359;&#12290;PG&#27169;&#22359;&#26088;&#22312;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#35282;&#24230;&#25429;&#33719;&#23545;&#35805;&#20449;&#24687;&#12290;&#23427;&#36890;&#36807;&#35780;&#20272;&#21305;&#37197;&#24230;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical dialogue systems aim to provide accurate answers to patients, necessitating specific domain knowledge. Recent advancements in Large Language Models (LLMs) have demonstrated their exceptional capabilities in the medical Q&amp;A domain, indicating a rich understanding of common sense. However, LLMs are insufficient for direct diagnosis due to the absence of diagnostic strategies. The conventional approach to address this challenge involves expensive fine-tuning of LLMs. Alternatively, a more appealing solution is the development of a plugin that empowers LLMs to perform medical conversation tasks. Drawing inspiration from in-context learning, we propose PlugMed, a Plug-and-Play Medical Dialogue System that facilitates appropriate dialogue actions by LLMs through two modules: the prompt generation (PG) module and the response ranking (RR) module. The PG module is designed to capture dialogue information from both global and local perspectives. It selects suitable prompts by assessing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28789;&#27963;&#24615;&#30340;&#36890;&#29992;&#25688;&#35201;&#26694;&#26550;&#65292;&#21487;&#20197;&#25972;&#21512;&#21508;&#31181;&#27169;&#24577;&#30340;&#20391;&#38754;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25991;&#31456;&#25688;&#35201;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11503</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#19981;&#21516;&#27169;&#24577;&#20391;&#38754;&#20449;&#24687;&#30340;&#20027;&#39064;&#24863;&#30693;&#25688;&#35201;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Topic-aware Summarization Framework with Different Modal Side Information. (arXiv:2305.11503v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28789;&#27963;&#24615;&#30340;&#36890;&#29992;&#25688;&#35201;&#26694;&#26550;&#65292;&#21487;&#20197;&#25972;&#21512;&#21508;&#31181;&#27169;&#24577;&#30340;&#20391;&#38754;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25991;&#31456;&#25688;&#35201;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25688;&#35201;&#22312;&#32593;&#32476;&#25991;&#26723;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#22312;&#20687;CNN.com&#21644;WikiHow.com&#36825;&#26679;&#30340;&#20869;&#23481;&#32593;&#31449;&#19978;&#65292;&#32463;&#24120;&#23384;&#22312;&#21508;&#31181;&#19982;&#20027;&#35201;&#25991;&#26723;&#19968;&#36215;&#29992;&#20110;&#21560;&#24341;&#27880;&#24847;&#21147;&#21644;&#26356;&#26131;&#29702;&#35299;&#30340;&#20391;&#38754;&#20449;&#24687;&#65292;&#20363;&#22914;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#26597;&#35810;&#12290;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#29992;&#20110;&#26356;&#22909;&#30340;&#25688;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#24120;&#24120;&#26126;&#31034;&#25110;&#26263;&#31034;&#25991;&#31456;&#30340;&#26412;&#36136;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20391;&#38754;&#24863;&#30693;&#25688;&#35201;&#26041;&#27861;&#37117;&#26159;&#35774;&#35745;&#20026;&#38598;&#25104;&#21333;&#27169;&#25110;&#22810;&#27169;&#20391;&#38754;&#20449;&#24687; &#65292;&#24182;&#19981;&#33021;&#26377;&#25928;&#22320;&#20114;&#30456;&#36866;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25688;&#35201;&#26694;&#26550;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#25972;&#21512;&#21508;&#31181;&#27169;&#24577;&#30340;&#20391;&#38754;&#20449;&#24687;&#12290;&#35774;&#35745;&#20855;&#26377;&#20391;&#38754;&#20449;&#24687;&#30340;&#28789;&#27963;&#25688;&#35201;&#27169;&#22411;&#30340;&#20027;&#35201;&#25361;&#25112;&#21253;&#25324;&#65306;&#65288;1&#65289;&#20391;&#38754;&#20449;&#24687;&#21487;&#20197;&#26159;&#25991;&#26412;&#25110;&#35270;&#35273;&#26684;&#24335;&#65292;&#24182;&#19988;&#27169;&#22411;&#38656;&#35201;&#23558;&#20854;&#19982;&#25991;&#26723;&#23545;&#40784;&#24182;&#32479;&#19968;&#20026;&#30456;&#21516;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic summarization plays an important role in the exponential document growth on the Web. On content websites such as CNN.com and WikiHow.com, there often exist various kinds of side information along with the main document for attention attraction and easier understanding, such as videos, images, and queries. Such information can be used for better summarization, as they often explicitly or implicitly mention the essence of the article. However, most of the existing side-aware summarization methods are designed to incorporate either single-modal or multi-modal side information, and cannot effectively adapt to each other. In this paper, we propose a general summarization framework, which can flexibly incorporate various modalities of side information. The main challenges in designing a flexible summarization model with side information include: (1) the side information can be in textual or visual format, and the model needs to align and unify it with the document into the same sem
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#34164;&#28085;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#23454;&#20307;&#30340;&#19977;&#20803;&#32452;&#36716;&#21270;&#20026;&#32479;&#19968;&#30340;&#25991;&#26412;&#24207;&#21015;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#23454;&#20307;&#20043;&#38388;&#30340;&#34164;&#28085;&#27010;&#29575;&#36827;&#34892;&#23454;&#20307;&#23545;&#40784;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#25991;&#26412;&#35299;&#37322;&#35828;&#26126;&#12290;</title><link>http://arxiv.org/abs/2305.11501</link><description>&lt;p&gt;
&#20174;&#23545;&#40784;&#21040;&#34164;&#28085;&#65306;&#38754;&#21521;&#23454;&#20307;&#23545;&#40784;&#30340;&#32479;&#19968;&#25991;&#26412;&#34164;&#28085;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
From Alignment to Entailment: A Unified Textual Entailment Framework for Entity Alignment. (arXiv:2305.11501v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11501
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#34164;&#28085;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#23454;&#20307;&#30340;&#19977;&#20803;&#32452;&#36716;&#21270;&#20026;&#32479;&#19968;&#30340;&#25991;&#26412;&#24207;&#21015;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#23454;&#20307;&#20043;&#38388;&#30340;&#34164;&#28085;&#27010;&#29575;&#36827;&#34892;&#23454;&#20307;&#23545;&#40784;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#25991;&#26412;&#35299;&#37322;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#26088;&#22312;&#21457;&#29616;&#20004;&#20010;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#30340;&#31561;&#25928;&#23454;&#20307;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#23558;&#23454;&#20307;&#30340;&#19977;&#20803;&#32452;&#32534;&#30721;&#20026;&#23884;&#20837;&#24182;&#23398;&#20064;&#23545;&#40784;&#23884;&#20837;&#65292;&#36825;&#23548;&#33268;&#36328;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#30340;&#21407;&#22987;&#20449;&#24687;&#26080;&#27861;&#30452;&#25509;&#20132;&#20114;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#23558;&#23454;&#20307;&#30340;&#20851;&#31995;&#19977;&#20803;&#32452;&#21644;&#23646;&#24615;&#19977;&#20803;&#32452;&#32534;&#30721;&#20026;&#24322;&#26500;&#23884;&#20837;&#31354;&#38388;&#65292;&#36825;&#38459;&#27490;&#20102;&#23427;&#20204;&#20114;&#30456;&#24110;&#21161;&#12290;&#26412;&#25991;&#23558;&#20004;&#20010;&#19977;&#20803;&#32452;&#36716;&#25442;&#20026;&#32479;&#19968;&#30340;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#23558;EA&#20219;&#21153;&#24314;&#27169;&#20026;&#20004;&#20010;&#23454;&#20307;&#24207;&#21015;&#20043;&#38388;&#30340;&#21452;&#21521;&#25991;&#26412;&#34164;&#28085;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21516;&#26102;&#23558;&#20004;&#20010;&#23454;&#20307;&#30340;&#24207;&#21015;&#39304;&#36865;&#21040;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;PLM&#30340;&#23454;&#20307;&#23545;&#40784;&#22120;&#65292;&#23558;&#24207;&#21015;&#20043;&#38388;&#30340;&#34164;&#28085;&#27010;&#29575;&#24314;&#27169;&#20026;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25429;&#33719;&#20102;&#20004;&#31181;&#23454;&#20307;&#20449;&#24687;&#20043;&#38388;&#30340;&#32479;&#19968;&#30456;&#20851;&#27169;&#24335;&#65292;&#24182;&#29992;&#25991;&#26412;&#35299;&#37322;&#35828;&#26126;&#20102;&#23545;&#40784;&#30340;&#23454;&#20307;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25991;&#26412;&#34164;&#28085;&#26694;&#26550;&#23545;EA&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity Alignment (EA) aims to find the equivalent entities between two Knowledge Graphs (KGs). Existing methods usually encode the triples of entities as embeddings and learn to align the embeddings, which prevents the direct interaction between the original information of the cross-KG entities. Moreover, they encode the relational triples and attribute triples of an entity in heterogeneous embedding spaces, which prevents them from helping each other. In this paper, we transform both triples into unified textual sequences, and model the EA task as a bi-directional textual entailment task between the sequences of cross-KG entities. Specifically, we feed the sequences of two entities simultaneously into a pre-trained language model (PLM) and propose two kinds of PLM-based entity aligners that model the entailment probability between sequences as the similarity between entities. Our approach captures the unified correlation pattern of two kinds of information between entities, and explic
&lt;/p&gt;</description></item><item><title>RCOT &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#32416;&#27491; LLM &#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#65292;&#20197;&#25552;&#39640; LLM &#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11499</link><description>&lt;p&gt;
RCOT&#65306;&#36890;&#36807;&#21453;&#36716;&#24605;&#32500;&#38142;&#26465;&#26816;&#27979;&#21644;&#32416;&#27491;&#25512;&#29702;&#20013;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought. (arXiv:2305.11499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11499
&lt;/p&gt;
&lt;p&gt;
RCOT &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#32416;&#27491; LLM &#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#65292;&#20197;&#25552;&#39640; LLM &#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#36880;&#27493;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#22312;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25104;&#32489;&#12290;&#28982;&#32780;&#65292;LLM&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#38754;&#20020;&#30528;&#32500;&#25252;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#25361;&#25112;&#65292;&#34920;&#29616;&#20986;&#22312;&#32473;&#23450;&#38382;&#39064;&#19978;&#30830;&#23450;&#36807;&#24230;&#12289;&#38382;&#39064;&#35823;&#35299;&#21644;&#26465;&#20214;&#24187;&#35273;&#30340;&#36235;&#21183;&#12290;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#31895;&#31890;&#24230;&#21453;&#39304;&#65288;&#20363;&#22914;&#65292;&#31572;&#26696;&#26159;&#21542;&#27491;&#30830;&#65289;&#26469;&#25552;&#39640;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;RCoT&#65288;&#21453;&#36716;CoT&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#26816;&#27979;&#21644;&#32416;&#27491;LLM&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#26469;&#25552;&#39640;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#26816;&#27979;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#65292;RCoT&#39318;&#20808;&#35201;&#27714;LLMs&#22522;&#20110;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#37325;&#26500;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#38382;&#39064;&#21644;&#37325;&#26500;&#38382;&#39064;&#65292;&#36739;&#20026;&#35814;&#32454;&#22320;&#25581;&#31034;&#20102;&#21407;&#22987;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#32416;&#27491;&#35299;&#20915;&#26041;&#26696;&#65292;RCoT&#21046;&#23450;&#20102;&#26816;&#27979;&#21040;&#30340;fa
&lt;/p&gt;
&lt;p&gt;
Large language Models (LLMs) have achieved promising performance on arithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting. However, LLMs face challenges in maintaining factual consistency during reasoning, exhibiting tendencies to condition overlooking, question misinterpretation, and condition hallucination over given problems. Existing methods use coarse-grained feedback (e.g., whether the answer is correct) to improve factual consistency. In this work, we propose RCoT (Reversing Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by automatically detecting and rectifying factual inconsistency in LLMs' generated solutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct the problem based on generated solutions. Then fine-grained comparisons between the original problem and the reconstructed problem expose the factual inconsistency in the original solutions. To rectify the solution, RCoT formulates detected fa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#20559;&#32622;&#30340;&#37325;&#26032;&#32806;&#21512;&#20107;&#20214;&#22330;&#27169;&#22411;&#65288;ProCE&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#20107;&#20214;&#25552;&#21462;&#26694;&#26550;&#65292;&#20197;&#28548;&#28165;&#26469;&#33258;&#27169;&#31946;&#32416;&#32544;&#30340;&#20107;&#20214;&#23383;&#27573;&#65292;&#24182;&#37325;&#26032;&#32806;&#21512;&#30456;&#24212;&#30340;&#28548;&#28165;&#20998;&#24067;&#20197;&#25429;&#33719;&#26356;&#22810;&#28508;&#22312;&#20449;&#24687;&#23383;&#27573;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#20855;&#26377;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11498</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#20559;&#32622;&#30340;&#20107;&#20214;&#25277;&#21462;&#20013;&#37325;&#26032;&#32806;&#21512;&#20107;&#20214;&#22330;
&lt;/p&gt;
&lt;p&gt;
Recouple Event Field via Probabilistic Bias for Event Extraction. (arXiv:2305.11498v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11498
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#20559;&#32622;&#30340;&#37325;&#26032;&#32806;&#21512;&#20107;&#20214;&#22330;&#27169;&#22411;&#65288;ProCE&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#20107;&#20214;&#25552;&#21462;&#26694;&#26550;&#65292;&#20197;&#28548;&#28165;&#26469;&#33258;&#27169;&#31946;&#32416;&#32544;&#30340;&#20107;&#20214;&#23383;&#27573;&#65292;&#24182;&#37325;&#26032;&#32806;&#21512;&#30456;&#24212;&#30340;&#28548;&#28165;&#20998;&#24067;&#20197;&#25429;&#33719;&#26356;&#22810;&#28508;&#22312;&#20449;&#24687;&#23383;&#27573;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#20855;&#26377;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25277;&#21462;&#65288;EE&#65289;&#26088;&#22312;&#20174;&#20107;&#20214;&#25552;&#21450;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#20107;&#20214;&#35302;&#21457;&#22120;&#21644;&#21442;&#25968;&#65292;&#24050;&#32463;&#21463;&#30410;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22522;&#20110;PLM&#30340;&#26041;&#27861;&#24573;&#30053;&#20102;&#35302;&#21457;/&#21442;&#25968;&#23383;&#27573;&#30340;&#20449;&#24687;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#20107;&#20214;&#27169;&#24335;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#37325;&#26032;&#32806;&#21512;&#27169;&#22411;&#22686;&#24378;&#20107;&#20214;&#25552;&#21462;&#26694;&#26550;&#65288;ProCE&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#35821;&#27861;&#30456;&#20851;&#30340;&#20107;&#20214;&#23383;&#27573;&#24314;&#27169;&#20026;&#27010;&#29575;&#20559;&#32622;&#65292;&#20197;&#28548;&#28165;&#26469;&#33258;&#27169;&#31946;&#32416;&#32544;&#30340;&#20107;&#20214;&#23383;&#27573;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;EE&#20013;&#21516;&#19968;&#35302;&#21457;&#22120;/&#21442;&#25968;&#30340;&#22810;&#27425;&#20986;&#29616;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21516;&#19968;&#35302;&#21457;&#22120;/&#21442;&#25968;&#30340;&#22810;&#20010;&#23383;&#27573;&#20043;&#38388;&#30340;&#27010;&#29575;&#20132;&#20114;&#31574;&#30053;&#65292;&#20197;&#37325;&#26032;&#32806;&#21512;&#30456;&#24212;&#30340;&#28548;&#28165;&#20998;&#24067;&#24182;&#25429;&#33719;&#26356;&#22810;&#28508;&#22312;&#20449;&#24687;&#23383;&#27573;&#12290;&#22312;EE&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event Extraction (EE), aiming to identify and classify event triggers and arguments from event mentions, has benefited from pre-trained language models (PLMs). However, existing PLM-based methods ignore the information of trigger/argument fields, which is crucial for understanding event schemas. To this end, we propose a Probabilistic reCoupling model enhanced Event extraction framework (ProCE). Specifically, we first model the syntactic-related event fields as probabilistic biases, to clarify the event fields from ambiguous entanglement. Furthermore, considering multiple occurrences of the same triggers/arguments in EE, we explore probabilistic interaction strategies among multiple fields of the same triggers/arguments, to recouple the corresponding clarified distributions and capture more latent information fields. Experiments on EE datasets demonstrate the effectiveness and generalization of our proposed approach.
&lt;/p&gt;</description></item><item><title>&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;&#24050;&#32463;&#22312;&#35270;&#35273;&#23450;&#20301;&#39046;&#22495;&#21331;&#26377;&#25104;&#25928;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#22810;&#25968;&#21487;&#35299;&#37322;&#24615;&#19981;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26500;&#24314;&#26041;&#27861;&#65292;&#21517;&#20026;TreePrompt&#65292;&#36890;&#36807;&#23558;&#21477;&#23376;&#20998;&#35299;&#25104;&#26641;&#29366;&#32467;&#26500;&#36827;&#34892;&#36880;&#27493;&#25552;&#31034;&#26500;&#24314;&#65292;&#25552;&#39640;&#20102;&#25552;&#31034;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11497</link><description>&lt;p&gt;
TreePrompt&#65306;&#23398;&#20064;&#29983;&#25104;&#26641;&#24418;&#25552;&#31034;&#20197;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding. (arXiv:2305.11497v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11497
&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;&#24050;&#32463;&#22312;&#35270;&#35273;&#23450;&#20301;&#39046;&#22495;&#21331;&#26377;&#25104;&#25928;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#22810;&#25968;&#21487;&#35299;&#37322;&#24615;&#19981;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26500;&#24314;&#26041;&#27861;&#65292;&#21517;&#20026;TreePrompt&#65292;&#36890;&#36807;&#23558;&#21477;&#23376;&#20998;&#35299;&#25104;&#26641;&#29366;&#32467;&#26500;&#36827;&#34892;&#36880;&#27493;&#25552;&#31034;&#26500;&#24314;&#65292;&#25552;&#39640;&#20102;&#25552;&#31034;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#24050;&#32463;&#22312;&#23558;&#20174;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#24182;&#19988;&#24050;&#32463;&#25903;&#37197;&#20102;&#35270;&#35273;&#23450;&#20301;&#65288;VG&#65289;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#30340;&#25552;&#31034;&#35843;&#25972;&#33539;&#20363;&#37117;&#36973;&#21463;&#30528;&#21487;&#35299;&#37322;&#24615;&#24046;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#20854;&#21487;&#35299;&#37322;&#24615;&#24046;&#26159;&#30001;&#20110;&#20840;&#23616;&#25552;&#31034;&#29983;&#25104;&#21644;&#25512;&#29702;&#36807;&#31243;&#36896;&#25104;&#30340;&#12290;&#36890;&#36807;&#8220;&#20840;&#23616;&#8221;&#65292;&#25105;&#20204;&#26159;&#25351;&#23427;&#20204;&#36890;&#24120;&#30452;&#25509;&#23398;&#20064;&#19968;&#32452;&#21521;&#37327;&#20316;&#20026;&#25552;&#31034;&#65288;&#21363;&#25552;&#31034;&#29983;&#25104;&#65289;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#20840;&#23616;&#25552;&#31034;&#22686;&#24378;VG&#27169;&#22411;&#30340;&#25991;&#26412;&#36755;&#20837;&#65288;&#21363;&#25552;&#31034;&#25512;&#29702;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26174;&#24335;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#22411;&#25552;&#31034;&#26500;&#24314;&#33539;&#20363;&#65292;&#31216;&#20026;TreePrompt&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#22797;&#26434;&#30340;&#21477;&#23376;&#20998;&#35299;&#25104;&#19968;&#26869;&#19982;&#20154;&#31867;&#25512;&#29702;&#19968;&#33268;&#30340;&#26641;&#12290;&#28982;&#21518;&#65292;&#25353;&#29031;&#35821;&#27861;&#26641;&#65292;&#25105;&#20204;&#20174;&#24213;&#21521;&#19978;&#20197;&#32467;&#26500;&#21270;&#25552;&#31034;&#30340;&#26041;&#24335;&#26500;&#25104;&#19968;&#20010;&#25552;&#31034;&#12290;&#30001;&#20110;&#36825;&#19968;&#27493;&#39588;&#19968;&#27493;&#19968;&#27493;&#30340;&#25552;&#31034;&#26500;&#24314;&#36807;&#31243;&#65292;e
&lt;/p&gt;
&lt;p&gt;
Prompt tuning has achieved great success in transferring the knowledge from large pretrained vision-language models into downstream tasks, and has dominated the performance on visual grounding (VG). However, almost all existing prompt tuning paradigms suffer from poor interpretability. In this paper, we argue that their poor interpretability is attributed to the holistic prompt generation and inference process. By "holistic", we mean that they usually directly learn a set of vectors as the prompt (i.e., prompt generation), and use the learned global prompt to augment the textual input for the VG model (i.e., prompt inference). To this end, we propose a new prompt construction paradigm with explicit explainable ability, named TreePrompt. Specifically, we first deconstruct a complex sentence into a tree, that is consistent with human reasoning. Then, following the syntax tree, we compose a structured prompt in a bottom-up manner. Thanks to this step-by-step prompt construction process, e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.11490</link><description>&lt;p&gt;
LLM&#33258;&#36523;&#21487;&#35835;&#21462;&#21644;&#29983;&#25104;CXR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11490
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#20110;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#65292;&#20154;&#20204;&#27491;&#31215;&#26497;&#23581;&#35797;&#23558;LLMs&#30340;&#23454;&#29992;&#24615;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#24050;&#32463;&#26377;&#20154;&#23581;&#35797;&#36830;&#25509;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#19988;&#20063;&#22312;&#19981;&#26029;&#23581;&#35797;&#20026;LLMs&#28155;&#21152;&#35270;&#35273;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#21482;&#20351;&#29992;LLMs&#20316;&#20026;&#22270;&#20687;&#35299;&#30721;&#22120;&#65292;&#27809;&#26377;&#23581;&#35797;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#36890;&#36807;&#37319;&#29992;VQ-GAN&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#30340;&#28508;&#22312;&#34920;&#31034;&#35270;&#20026;&#19968;&#31181;&#25991;&#26412;&#26631;&#35760;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#65292;&#20197;&#20687;&#25991;&#26412;&#19968;&#26679;&#35835;&#21462;&#21644;&#29983;&#25104;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25110;&#35757;&#32451;&#19987;&#38376;&#30340;&#32593;&#32476;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#20449;&#24687;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#23545;&#27604;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#65288;CLV&#65289;&#32467;&#21512;&#31232;&#30095;&#21644;&#23494;&#38598;&#20154;&#26684;&#25551;&#36848;&#20197;&#21450;&#23545;&#35805;&#21382;&#21490;&#35760;&#24405;&#26469;&#23454;&#29616;&#26356;&#20016;&#23500;&#21644;&#20934;&#30830;&#30340;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20010;&#24615;&#21270;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11482</link><description>&lt;p&gt;
&#29992;&#23545;&#27604;&#28508;&#22312;&#21464;&#37327;&#22686;&#24378;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#65306;&#32467;&#21512;&#31232;&#30095;&#21644;&#23494;&#38598;&#20154;&#26684;
&lt;/p&gt;
&lt;p&gt;
Enhancing Personalized Dialogue Generation with Contrastive Latent Variables: Combining Sparse and Dense Persona. (arXiv:2305.11482v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#23545;&#27604;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#65288;CLV&#65289;&#32467;&#21512;&#31232;&#30095;&#21644;&#23494;&#38598;&#20154;&#26684;&#25551;&#36848;&#20197;&#21450;&#23545;&#35805;&#21382;&#21490;&#35760;&#24405;&#26469;&#23454;&#29616;&#26356;&#20016;&#23500;&#21644;&#20934;&#30830;&#30340;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20010;&#24615;&#21270;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#23545;&#35805;&#25506;&#32034;&#20102;&#23545;&#35805;&#29983;&#25104;&#19982;&#20010;&#24615;&#20043;&#38388;&#30340;&#19968;&#33268;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#20010;&#24615;&#21270;&#23545;&#35805;&#27169;&#22411;&#20174;&#19977;&#20010;&#36164;&#28304;&#65288;&#31232;&#30095;&#25110;&#23494;&#38598;&#30340;&#20010;&#24615;&#25551;&#36848;&#20197;&#21450;&#23545;&#35805;&#21382;&#21490;&#35760;&#24405;&#65289;&#20013;&#24314;&#27169;&#20010;&#20154;&#36164;&#26009;&#12290;&#28982;&#32780;&#65292;&#31232;&#30095;&#30340;&#32467;&#26500;&#21270;&#20010;&#24615;&#23646;&#24615;&#26159;&#26174;&#24615;&#20294;&#26080;&#20449;&#24687;&#65307;&#23494;&#38598;&#30340;&#20010;&#24615;&#25991;&#26412;&#21253;&#21547;&#30528;&#20016;&#23500;&#30340;&#20010;&#24615;&#25551;&#36848;&#19982;&#36739;&#22810;&#30340;&#22122;&#22768;&#65292;&#32780;&#23545;&#35805;&#21382;&#21490;&#26597;&#35810;&#26082;&#22024;&#26434;&#21448;&#26080;&#20449;&#24687;&#20197;&#20379;&#20010;&#24615;&#24314;&#27169;&#12290;&#26412;&#25991;&#23558;&#19977;&#20010;&#36164;&#28304;&#30340;&#20248;&#21183;&#36827;&#34892;&#32467;&#21512;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#27604;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#65288;CLV&#65289;&#30340;&#26694;&#26550;&#65292;&#23558;&#23494;&#38598;&#30340;&#20010;&#24615;&#25551;&#36848;&#32858;&#31867;&#25104;&#31232;&#30095;&#30340;&#31867;&#21035;&#65292;&#28982;&#21518;&#21644;&#26597;&#35810;&#21382;&#21490;&#20849;&#21516;&#29983;&#25104;&#20010;&#24615;&#21270;&#22238;&#24212;&#12290;&#20013;&#33521;&#25991;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20010;&#24615;&#21270;&#19978;&#21331;&#26377;&#25104;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The personalized dialogue explores the consistent relationship between dialogue generation and personality. Existing personalized dialogue agents model persona profiles from three resources: sparse or dense persona descriptions and dialogue histories. However, sparse structured persona attributes are explicit but uninformative, dense persona texts contain rich persona descriptions with much noise, and dialogue history query is both noisy and uninformative for persona modeling. In this work, we combine the advantages of the three resources to obtain a richer and more accurate persona. We design a Contrastive Latent Variable-based model (CLV) that clusters the dense persona descriptions into sparse categories, which are combined with the history query to generate personalized responses. Experimental results on Chinese and English datasets demonstrate our model's superiority in personalization.
&lt;/p&gt;</description></item><item><title>CCGen&#26159;&#19968;&#20010;&#30005;&#23376;&#21830;&#21153;&#20013;&#21487;&#35299;&#37322;&#30340;&#20114;&#34917;&#27010;&#24565;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20114;&#34917;&#27010;&#24565;&#25490;&#21517;&#21015;&#34920;&#65292;&#24182;&#29983;&#25104;&#35299;&#37322;&#20197;&#35777;&#26126;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11480</link><description>&lt;p&gt;
CCGen&#65306;&#30005;&#23376;&#21830;&#21153;&#20013;&#21487;&#35299;&#37322;&#30340;&#20114;&#34917;&#27010;&#24565;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CCGen: Explainable Complementary Concept Generation in E-Commerce. (arXiv:2305.11480v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11480
&lt;/p&gt;
&lt;p&gt;
CCGen&#26159;&#19968;&#20010;&#30005;&#23376;&#21830;&#21153;&#20013;&#21487;&#35299;&#37322;&#30340;&#20114;&#34917;&#27010;&#24565;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20114;&#34917;&#27010;&#24565;&#25490;&#21517;&#21015;&#34920;&#65292;&#24182;&#29983;&#25104;&#35299;&#37322;&#20197;&#35777;&#26126;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#20114;&#34917;&#27010;&#24565;&#29983;&#25104;&#65288;CCGen&#65289;&#65306;&#32473;&#23450;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#27010;&#24565;&#65292;&#20363;&#22914;&#8220;&#25968;&#30721;&#30456;&#26426;&#8221;&#65292;&#29983;&#25104;&#19968;&#31995;&#21015;&#20114;&#34917;&#30340;&#27010;&#24565;&#65292;&#20363;&#22914;1&#65289;&#30456;&#26426;&#38236;&#22836;2&#65289;&#30005;&#27744;3&#65289;&#30456;&#26426;&#30418;&#23376;4&#65289;&#23384;&#20648;&#21345;5&#65289;&#30005;&#27744;&#20805;&#30005;&#22120;&#12290;CCGen&#23545;&#20110;&#35832;&#22914;&#26597;&#35810;&#24314;&#35758;&#21644;&#29289;&#21697;&#25512;&#33616;&#31561;&#21508;&#31181;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;&#38750;&#24120;&#26377;&#30410;&#12290;&#20026;&#20102;&#35299;&#20915;CCGen&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25490;&#21517;&#21015;&#34920;&#12290;&#25105;&#20204;&#36824;&#25945;&#25480;&#27169;&#22411;&#29983;&#25104;&#35299;&#37322;&#65292;&#36890;&#36807;&#21152;&#20837;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#35299;&#37322;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20114;&#34917;&#27010;&#24565;&#65292;&#21516;&#26102;&#29983;&#25104;&#35299;&#37322;&#20197;&#35777;&#26126;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose and study Complementary Concept Generation (CCGen): given a concept of interest, e.g., "Digital Cameras", generating a list of complementary concepts, e.g., 1) Camera Lenses 2) Batteries 3) Camera Cases 4) Memory Cards 5) Battery Chargers. CCGen is beneficial for various applications like query suggestion and item recommendation, especially in the e-commerce domain. To solve CCGen, we propose to train language models to generate ranked lists of concepts with a two-step training strategy. We also teach the models to generate explanations by incorporating explanations distilled from large teacher models. Extensive experiments and analysis demonstrate that our model can generate high-quality concepts complementary to the input concept while producing explanations to justify the predictions.
&lt;/p&gt;</description></item><item><title>Graphologue&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21709;&#24212;&#36716;&#25442;&#20026;&#22270;&#24418;&#21270;&#22270;&#34920;&#20197;&#22686;&#24378;&#20854;&#21487;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#21644;&#31361;&#20986;&#26174;&#31034;&#29305;&#23450;&#33410;&#28857;&#21644;&#38142;&#25509;&#26469;&#19982;&#36825;&#20123;&#22270;&#34920;&#36827;&#34892;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2305.11473</link><description>&lt;p&gt;
Graphologue&#65306;&#29992;&#20132;&#20114;&#24335;&#22270;&#34920;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Graphologue: Exploring Large Language Model Responses with Interactive Diagrams. (arXiv:2305.11473v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11473
&lt;/p&gt;
&lt;p&gt;
Graphologue&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21709;&#24212;&#36716;&#25442;&#20026;&#22270;&#24418;&#21270;&#22270;&#34920;&#20197;&#22686;&#24378;&#20854;&#21487;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#21644;&#31361;&#20986;&#26174;&#31034;&#29305;&#23450;&#33410;&#28857;&#21644;&#38142;&#25509;&#26469;&#19982;&#36825;&#20123;&#22270;&#34920;&#36827;&#34892;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30001;&#20110;&#26131;&#20110;&#33719;&#21462;&#21644;&#22312;&#22810;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#26234;&#33021;&#32780;&#36817;&#26469;&#39118;&#38753;&#19968;&#26102;&#12290;&#28982;&#32780;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;LLM&#22312;&#25903;&#25345;&#22797;&#26434;&#20449;&#24687;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#26174;&#30528;&#30340;&#38480;&#21046;&#65292;&#21407;&#22240;&#26159;&#22522;&#20110;&#25991;&#26412;&#30340;&#23186;&#20171;&#21644;&#32447;&#24615;&#23545;&#35805;&#32467;&#26500;&#25552;&#20379;&#30340;&#21151;&#33021;&#19981;&#36275;&#12290;&#36890;&#36807;&#19982;&#21313;&#21517;&#21442;&#19982;&#32773;&#30340;&#24418;&#24335;&#21270;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#30028;&#38754;&#36890;&#24120;&#20250;&#21576;&#29616;&#20887;&#38271;&#30340;&#21709;&#24212;&#65292;&#20351;&#20154;&#20204;&#38590;&#20197;&#24555;&#36895;&#29702;&#35299;&#21644;&#28789;&#27963;&#22320;&#19982;&#21508;&#31181;&#20449;&#24687;&#36827;&#34892;&#20132;&#20114;&#65292;&#29305;&#21035;&#26159;&#22312;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Graphologue&#65292;&#36825;&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#23558;LLM&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21709;&#24212;&#36716;&#25442;&#20026;&#22270;&#24418;&#21270;&#22270;&#34920;&#65292;&#20197;&#20415;&#20110;&#20449;&#24687;&#26597;&#25214;&#21644;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#12290;Graphologue&#37319;&#29992;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#30028;&#38754;&#35774;&#35745;&#65292;&#20174;LLM&#21709;&#24212;&#20013;&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#24182;&#23454;&#26102;&#26500;&#24314;&#33410;&#28857;&#38142;&#25509;&#22270;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#21644;&#31361;&#20986;&#26174;&#31034;&#29305;&#23450;&#33410;&#28857;&#21644;&#38142;&#25509;&#26469;&#19982;&#36825;&#20123;&#22270;&#34920;&#36827;&#34892;&#20132;&#20114;&#65292;&#20197;&#25506;&#32034;&#30456;&#20851;&#20449;&#24687;&#21644;&#36319;&#36827;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#30028;&#38754;&#30456;&#27604;&#65292;Graphologue&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#22312;&#22797;&#26434;&#20449;&#24687;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#21644;&#28385;&#24847;&#24230;&#12290;Graphologue&#20026;&#22686;&#24378;LLM&#22312;&#21508;&#31181;&#24212;&#29992;&#21644;&#39046;&#22495;&#20013;&#30340;&#21487;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently soared in popularity due to their ease of access and the unprecedented intelligence exhibited on diverse applications. However, LLMs like ChatGPT present significant limitations in supporting complex information tasks due to the insufficient affordances of the text-based medium and linear conversational structure. Through a formative study with ten participants, we found that LLM interfaces often present long-winded responses, making it difficult for people to quickly comprehend and interact flexibly with various pieces of information, particularly during more complex tasks. We present Graphologue, an interactive system that converts text-based responses from LLMs into graphical diagrams to facilitate information-seeking and question-answering tasks. Graphologue employs novel prompting strategies and interface designs to extract entities and relationships from LLM responses and constructs node-link diagrams in real-time. Further, users can int
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#38271;&#26399;&#35760;&#24518;&#32593;&#32476;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20174;&#26080;&#38480;&#38271;&#24207;&#21015;&#20013;&#23398;&#20064;&#65292;&#20197;&#25193;&#23637;&#24402;&#32435;&#35821;&#35328;&#24314;&#27169;&#30340;&#35760;&#24518;&#23481;&#37327;&#65292;&#24182;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.11462</link><description>&lt;p&gt;
&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Extending Memory for Language Modelling. (arXiv:2305.11462v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#38271;&#26399;&#35760;&#24518;&#32593;&#32476;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20174;&#26080;&#38480;&#38271;&#24207;&#21015;&#20013;&#23398;&#20064;&#65292;&#20197;&#25193;&#23637;&#24402;&#32435;&#35821;&#35328;&#24314;&#27169;&#30340;&#35760;&#24518;&#23481;&#37327;&#65292;&#24182;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21644;&#35760;&#24518;&#32593;&#32476;&#30340;&#31361;&#30772;&#20351;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#35821;&#35328;&#26159;&#36830;&#32493;&#30340;&#65292;&#36890;&#36807;&#24207;&#21015;&#20256;&#36882;&#30340;&#20449;&#24687;&#21487;&#20197;&#36890;&#36807;&#35760;&#24518;&#32593;&#32476;&#25429;&#33719;&#12290;&#23398;&#20064;&#24207;&#21015;&#26159;&#23398;&#20064;&#35821;&#35328;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#35760;&#24518;&#32593;&#32476;&#19981;&#33021;&#22312;&#20854;&#20869;&#23384;&#20013;&#25345;&#26377;&#26080;&#38480;&#38271;&#30340;&#24207;&#21015;&#65292;&#24182;&#21463;&#21040;&#21508;&#31181;&#32422;&#26463;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#28040;&#22833;&#25110;&#29190;&#28856;&#26799;&#24230;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24403;&#38754;&#23545;&#38271;&#24207;&#21015;&#25991;&#26412;&#26102;&#65292;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#38271;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LTM&#65289;&#65292;&#20197;&#20174;&#26080;&#38480;&#38271;&#30340;&#24207;&#21015;&#20013;&#23398;&#20064;&#12290; LTM &#37325;&#35270;&#24403;&#21069;&#36755;&#20837;&#65292;&#20197;&#20351;&#20854;&#20855;&#26377;&#39640;&#24433;&#21709;&#21147;&#12290;&#35821;&#35328;&#24314;&#27169;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290; LTM &#22312;&#38656;&#35201;&#38271;&#26399;&#35760;&#24518;&#30340;&#35821;&#35328;&#24314;&#27169;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290; LTM &#22312; Penn Tree Bank &#25968;&#25454;&#38598;&#65292;Google Billion&#23383;&#25968;&#25454;&#38598;&#21644;WikiText-2&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#23558; LTM &#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126; LTM &#22312;&#25152;&#26377;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#22256;&#24785;&#24230;&#37117;&#27604;&#23427;&#20204;&#20302;&#65292;&#19988;&#21442;&#25968;&#26356;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102; LTM &#21487;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Breakthroughs in deep learning and memory networks have made major advances in natural language understanding. Language is sequential and information carried through the sequence can be captured through memory networks. Learning the sequence is one of the key aspects in learning the language. However, memory networks are not capable of holding infinitely long sequences in their memories and are limited by various constraints such as the vanishing or exploding gradient problem. Therefore, natural language understanding models are affected when presented with long sequential text. We introduce Long Term Memory network (LTM) to learn from infinitely long sequences. LTM gives priority to the current inputs to allow it to have a high impact. Language modeling is an important factor in natural language understanding. LTM was tested in language modeling, which requires long term memory. LTM is tested on Penn Tree bank dataset, Google Billion Word dataset and WikiText-2 dataset. We compare LTM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#21327;&#35758;(Self-Agreement)&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#35843;LLMs&#20197;&#33258;&#20027;&#22320;&#25214;&#21040;&#20849;&#35782;&#65292;&#24182;&#20351;&#29992;LLM&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.11460</link><description>&lt;p&gt;
&#33258;&#25105;&#21327;&#35758;&#65306;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20197;&#22312;&#19981;&#21516;&#24847;&#35265;&#20043;&#38388;&#25214;&#21040;&#20849;&#35782;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Self-Agreement: A Framework for Fine-tuning Language Models to Find Agreement among Diverse Opinions. (arXiv:2305.11460v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#21327;&#35758;(Self-Agreement)&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#35843;LLMs&#20197;&#33258;&#20027;&#22320;&#25214;&#21040;&#20849;&#35782;&#65292;&#24182;&#20351;&#29992;LLM&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#25214;&#21040;&#19981;&#21516;&#24847;&#35265;&#20043;&#38388;&#30340;&#20849;&#35782;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35805;&#39064;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#29702;&#35299;&#20154;&#31867;&#35266;&#28857;&#21644;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#21327;&#35758;(Self-Agreement)&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#35843;LLMs&#20197;&#33258;&#20027;&#22320;&#25214;&#21040;&#20849;&#35782;&#65292;&#24182;&#20351;&#29992;LLM&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;3(GPT-3)&#20026;&#38382;&#39064;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#38382;&#39064;&#29983;&#25104;&#22810;&#20010;&#24847;&#35265;&#65292;&#24182;&#20026;&#36825;&#20123;&#24847;&#35265;&#21019;&#24314;&#22810;&#20010;&#20849;&#35782;&#20505;&#36873;&#39033;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#21387;&#22120;(BERT)&#30340;&#27169;&#22411;&#35780;&#20272;&#27599;&#20010;&#20849;&#35782;&#20505;&#36873;&#39033;&#30340;&#19968;&#33268;&#24615;&#24471;&#20998;&#65292;&#24182;&#36873;&#25321;&#24471;&#20998;&#26368;&#39640;&#30340;&#20849;&#35782;&#20505;&#36873;&#39033;&#12290;&#36825;&#20010;&#36807;&#31243;&#20135;&#29983;&#20102;&#19968;&#20010;&#38382;&#39064;-&#24847;&#35265;-&#20849;&#35782;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#24494;&#35843;&#19968;&#20010;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding an agreement among diverse opinions is a challenging topic in multiagent systems. Recently, large language models (LLMs) have shown great potential in addressing this challenge due to their remarkable capabilities in comprehending human opinions and generating human-like text. However, they typically rely on extensive human-annotated data. In this paper, we propose Self-Agreement, a novel framework for fine-tuning LLMs to autonomously find agreement using data generated by LLM itself. Specifically, our approach employs the generative pre-trained transformer-3 (GPT-3) to generate multiple opinions for each question in a question dataset and create several agreement candidates among these opinions. Then, a bidirectional encoder representations from transformers (BERT)-based model evaluates the agreement score of each agreement candidate and selects the one with the highest agreement score. This process yields a dataset of question-opinion-agreements, which we use to fine-tune a p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24605;&#36335;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#21516;&#26102;&#20316;&#20026;&#31574;&#30053;&#12289;&#22870;&#21169;&#20989;&#25968;&#21644;&#36716;&#31227;&#20989;&#25968;&#65292;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#22870;&#21169;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#21487;&#20197;&#24102;&#26469;&#24040;&#22823;&#30340;&#32479;&#35745;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2305.11455</link><description>&lt;p&gt;
&#31361;&#30772;&#26234;&#33021;&#20307;-&#29615;&#22659;&#30028;&#38754;&#65292;&#20248;&#21270;&#20855;&#26377;&#21253;&#23481;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Shattering the Agent-Environment Interface for Fine-Tuning Inclusive Language Models. (arXiv:2305.11455v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11455
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24605;&#36335;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#21516;&#26102;&#20316;&#20026;&#31574;&#30053;&#12289;&#22870;&#21169;&#20989;&#25968;&#21644;&#36716;&#31227;&#20989;&#25968;&#65292;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#22870;&#21169;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#21487;&#20197;&#24102;&#26469;&#24040;&#22823;&#30340;&#32479;&#35745;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#35843;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#20154;&#31867;&#21453;&#39304;&#30340;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65288;RLHF&#65289;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#26174;&#24335;&#35757;&#32451;&#19968;&#20010;&#22870;&#21169;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#32780;&#19981;&#26159;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20010;&#22870;&#21169;&#27169;&#22411;&#19982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#32806;&#21512;&#36215;&#26469;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#19982;&#26399;&#26395;&#21709;&#24212;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35266;&#28857;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#21516;&#26102;&#26159;&#31574;&#30053;&#12289;&#22870;&#21169;&#20989;&#25968;&#21644;&#36716;&#31227;&#20989;&#25968;&#12290;&#36825;&#20010;&#35266;&#28857;&#30340;&#19968;&#20010;&#30452;&#25509;&#32467;&#26524;&#26159;&#65292;&#21487;&#20197;&#21516;&#26102;&#30452;&#25509;&#36827;&#34892;&#22870;&#21169;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#30340;&#19979;&#28216;&#31574;&#30053;&#20248;&#21270;&#12290;&#34429;&#28982;&#36825;&#20010;&#35266;&#28857;&#30830;&#23454;&#25171;&#30772;&#20102;&#20256;&#32479;&#26234;&#33021;&#20307;-&#29615;&#22659;&#30028;&#38754;&#65292;&#20294;&#25105;&#20204;&#20173;&#28982;&#35748;&#20026;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#30340;&#20256;&#32479;&#31639;&#27861;&#27010;&#24565;&#36816;&#29992;&#20110;&#36825;&#31181;&#26041;&#27861;&#20013;&#21487;&#20197;&#24102;&#26469;&#24040;&#22823;&#30340;&#32479;&#35745;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
A centerpiece of the ever-popular reinforcement learning from human feedback (RLHF) approach to fine-tuning autoregressive language models is the explicit training of a reward model to emulate human feedback, distinct from the language model itself. This reward model is then coupled with policy-gradient methods to dramatically improve the alignment between language model outputs and desired responses. In this work, we adopt a novel perspective wherein a pre-trained language model is itself simultaneously a policy, reward function, and transition function. An immediate consequence of this is that reward learning and language model fine-tuning can be performed jointly and directly, without requiring any further downstream policy optimization. While this perspective does indeed break the traditional agent-environment interface, we nevertheless maintain that there can be enormous statistical benefits afforded by bringing to bear traditional algorithmic concepts from reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#24494;&#35843;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#32531;&#24930;&#21644;&#24555;&#36895;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#36801;&#31227;&#20013;&#30340;&#24615;&#33021;&#24046;&#36317;&#38382;&#39064;&#65292;&#36890;&#36807;&#20943;&#23569;&#36951;&#24536;&#26469;&#24357;&#34917;&#24615;&#33021;&#24046;&#36317;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#27604;&#22522;&#32447;&#26041;&#27861;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.11449</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#20943;&#23569;&#36328;&#35821;&#35328;&#36801;&#31227;&#20013;&#30340;&#24615;&#33021;&#24046;&#36317;&#65306;&#32531;&#24930;&#21644;&#24555;&#36895;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast. (arXiv:2305.11449v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#24494;&#35843;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#32531;&#24930;&#21644;&#24555;&#36895;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#36801;&#31227;&#20013;&#30340;&#24615;&#33021;&#24046;&#36317;&#38382;&#39064;&#65292;&#36890;&#36807;&#20943;&#23569;&#36951;&#24536;&#26469;&#24357;&#34917;&#24615;&#33021;&#24046;&#36317;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#27604;&#22522;&#32447;&#26041;&#27861;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#20010;&#65288;&#28304;&#65289;&#35821;&#35328;&#19978;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#22312;&#38750;&#28304;&#35821;&#35328;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#20063;&#34920;&#29616;&#33391;&#22909;&#65292;&#23613;&#31649;&#36825;&#20123;&#35821;&#35328;&#27809;&#26377;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#28304;&#35821;&#35328;&#21644;&#38750;&#28304;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#24494;&#35843;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#24615;&#33021;&#24046;&#36317;&#20309;&#26102;&#25913;&#21464;&#65292;&#24182;&#30830;&#23450;&#21738;&#20123;&#32593;&#32476;&#26435;&#37325;&#23545;&#25972;&#20307;&#24615;&#33021;&#24433;&#21709;&#26368;&#22823;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#35797;&#22270;&#22238;&#31572;&#36890;&#36807;&#20943;&#23569;&#36951;&#24536;&#26469;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#32553;&#23567;&#24046;&#36317;&#12290;&#22522;&#20110;&#20998;&#26512;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32531;&#24930;&#21644;&#24555;&#36895;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22235;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#26377;&#26126;&#26174;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research has shown that a multilingual pre-trained language model fine-tuned with one (source) language also performs well on downstream tasks for non-source languages, even though no fine-tuning is done on these languages. However, there is a clear gap between the performance of the source language and that of the non-source languages. This paper analyzes the fine-tuning process, discovers when the performance gap changes and identifies which network weights affect the overall performance most. Additionally, the paper seeks to answer to what extent the gap can be reduced by reducing forgetting. Based on the analysis results, a method named Fine-tuning slow and fast with four training policies is proposed to address these issues. Experimental results show the proposed method outperforms baselines by a clear margin.
&lt;/p&gt;</description></item><item><title>Arukikata&#26053;&#28216;&#28216;&#35760;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;3100&#19975;&#20010;&#26085;&#25991;&#21333;&#35789;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;4672&#20010;&#26085;&#26412;&#22269;&#20869;&#28216;&#35760;&#21644;9607&#20010;&#28023;&#22806;&#28216;&#35760;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#36879;&#26126;&#30340;&#30740;&#31350;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.11444</link><description>&lt;p&gt;
Arukikata&#26053;&#28216;&#28216;&#35760;&#25968;&#25454;&#38598; (arXiv:2305.11444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Arukikata Travelogue Dataset. (arXiv:2305.11444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11444
&lt;/p&gt;
&lt;p&gt;
Arukikata&#26053;&#28216;&#28216;&#35760;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;3100&#19975;&#20010;&#26085;&#25991;&#21333;&#35789;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;4672&#20010;&#26085;&#26412;&#22269;&#20869;&#28216;&#35760;&#21644;9607&#20010;&#28023;&#22806;&#28216;&#35760;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#36879;&#26126;&#30340;&#30740;&#31350;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21019;&#24314;&#20102;Arukikata&#26053;&#28216;&#28216;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#20813;&#36153;&#25552;&#20379;&#32473;&#23398;&#26415;&#30740;&#31350;&#20351;&#29992;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#36229;&#36807;3100&#19975;&#20010;&#26085;&#25991;&#21333;&#35789;&#65292;&#21253;&#25324;4672&#20010;&#26085;&#26412;&#22269;&#20869;&#28216;&#35760;&#21644;9607&#20010;&#28023;&#22806;&#28216;&#35760;&#12290;&#22312;&#25105;&#20204;&#25552;&#20379;&#25968;&#25454;&#38598;&#20043;&#21069;&#65292;&#24456;&#38590;&#33719;&#24471;&#21487;&#29992;&#20110;&#30740;&#31350;&#30340;&#24191;&#27867;&#26053;&#28216;&#28216;&#35760;&#25968;&#25454;&#65292;&#27599;&#20010;&#30740;&#31350;&#20154;&#21592;&#37117;&#24517;&#39035;&#20934;&#22791;&#33258;&#24049;&#30340;&#25968;&#25454;&#12290;&#36825;&#38459;&#30861;&#20102;&#23545;&#29616;&#26377;&#30740;&#31350;&#30340;&#22797;&#21046;&#20197;&#21450;&#23545;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20844;&#27491;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20351;&#24471;&#20219;&#20309;&#30740;&#31350;&#20154;&#21592;&#37117;&#21487;&#20197;&#23545;&#30456;&#21516;&#30340;&#25968;&#25454;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#30830;&#20445;&#30740;&#31350;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30340;&#23398;&#26415;&#24847;&#20041;&#12289;&#29305;&#28857;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
We have constructed Arukikata Travelogue Dataset and released it free of charge for academic research. This dataset is a Japanese text dataset with a total of over 31 million words, comprising 4,672 Japanese domestic travelogues and 9,607 overseas travelogues. Before providing our dataset, there was a scarcity of widely available travelogue data for research purposes, and each researcher had to prepare their own data. This hinders the replication of existing studies and fair comparative analysis of experimental results. Our dataset enables any researchers to conduct investigation on the same data and to ensure transparency and reproducibility in research. In this paper, we describe the academic significance, characteristics, and prospects of our dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#35843;&#25972;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#27573;&#33853;&#20013;&#30340;&#31532;&#19968;&#21477;&#35805;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#25512;&#26029;&#65292;&#27169;&#22411;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#65292;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#19981;&#25935;&#24863;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11442</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#35843;&#25972;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Text Classification via Self-Supervised Tuning. (arXiv:2305.11442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#35843;&#25972;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#27573;&#33853;&#20013;&#30340;&#31532;&#19968;&#21477;&#35805;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#25512;&#26029;&#65292;&#27169;&#22411;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#65292;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#19981;&#25935;&#24863;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#65307;&#35201;&#20040;&#20381;&#36182;&#20110;&#22823;&#37327;&#30456;&#20851;&#20219;&#21153;&#30340;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#31216;&#20026;&#33258;&#30417;&#30563;&#35843;&#25972;&#12290;&#36890;&#36807;&#25506;&#32034;&#33258;&#30001;&#25991;&#26412;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#31216;&#20026;&#39318;&#21477;&#39044;&#27979;&#65292;&#20197;&#24357;&#21512;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35843;&#25972;&#27169;&#22411;&#20197;&#23398;&#20064;&#26681;&#25454;&#21097;&#20313;&#25991;&#26412;&#26469;&#39044;&#27979;&#27573;&#33853;&#20013;&#30340;&#31532;&#19968;&#21477;&#35805;&#21518;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#20986;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#22914;&#20027;&#39064;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;10&#20010;&#20219;&#21153;&#20013;&#30340;7&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#32447;&#12290;&#27492;&#22806;&#65292;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#19981;&#25935;&#24863;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates, or rely on large-scale annotated data of relevant tasks for meta-tuning. In this work, we propose a new paradigm based on self-supervised learning to solve zero-shot text classification tasks by tuning the language models with unlabeled data, called self-supervised tuning. By exploring the inherent structure of free texts, we propose a new learning objective called first sentence prediction to bridge the gap between unlabeled data and text classification tasks. After tuning the model to learn to predict the first sentence in a paragraph based on the rest, the model is able to conduct zero-shot inference on unseen tasks such as topic classification and sentiment analysis. Experimental results show that our model outperforms the state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals that our model is less s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38899;&#38901;&#21644;&#38901;&#24459;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#27597;&#35821;&#27969;&#30021;&#24230;&#35780;&#20998;&#12290;&#36890;&#36807;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#25552;&#31034;&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30340;&#35780;&#20998;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#26041;&#27861;&#22312;Pearson&#30456;&#20851;&#31995;&#25968;&#65288;PCC&#65289;&#26041;&#38754;&#20248;&#20110;&#22522;&#32447;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2305.11438</link><description>&lt;p&gt;
&#22522;&#20110;&#38899;&#38901;&#21644;&#38901;&#24459;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#38750;&#27597;&#35821;&#27969;&#30021;&#24230;&#35780;&#20998;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Phonetic and Prosody-aware Self-supervised Learning Approach for Non-native Fluency Scoring. (arXiv:2305.11438v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38899;&#38901;&#21644;&#38901;&#24459;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#27597;&#35821;&#27969;&#30021;&#24230;&#35780;&#20998;&#12290;&#36890;&#36807;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#25552;&#31034;&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30340;&#35780;&#20998;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#26041;&#27861;&#22312;Pearson&#30456;&#20851;&#31995;&#25968;&#65288;PCC&#65289;&#26041;&#38754;&#20248;&#20110;&#22522;&#32447;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#19968;&#31995;&#21015;&#30340;&#38899;&#38901;&#21644;&#38901;&#24459;&#29305;&#24449;&#65292;&#21487;&#20197;&#35780;&#20272;&#35828;&#35805;&#30340;&#27969;&#30021;&#24230;/&#19981;&#27969;&#30021;&#24230;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#29992;&#20110;&#23558;&#19982;&#27969;&#30021;&#24230;&#30456;&#20851;&#30340;&#29305;&#24449;&#26144;&#23556;&#21040;&#20154;&#31867;&#35780;&#20998;&#20013;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#26631;&#35760;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064; (SSL) &#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#23545;&#20110;&#27969;&#30021;&#24230;&#35780;&#20998;&#32780;&#35328;&#30340;&#38899;&#38901;&#21644;&#38901;&#24459;&#24863;&#30693;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#19968;&#20010;&#37325;&#24314;&#25439;&#22833;&#20989;&#25968;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#25552;&#31034;&#19978;&#32852;&#21512;&#23631;&#34109;&#38899;&#32032;&#21450;&#20854;&#25345;&#32493;&#26102;&#38388;&#26469;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30340;&#35780;&#20998;&#25968;&#25454;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#22312;Speechocean762&#31561;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#65292;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Pearson&#30456;&#20851;&#31995;&#25968;&#65288;PCC&#65289;&#26041;&#38754;&#20248;&#20110;&#22522;&#32447;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#25105;&#20204;&#26041;&#27861;&#30340;&#20316;&#29992;&#21450;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech fluency/disfluency can be evaluated by analyzing a range of phonetic and prosodic features. Deep neural networks are commonly trained to map fluency-related features into the human scores. However, the effectiveness of deep learning-based models is constrained by the limited amount of labeled training samples. To address this, we introduce a self-supervised learning (SSL) approach that takes into account phonetic and prosody awareness for fluency scoring. Specifically, we first pre-train the model using a reconstruction loss function, by masking phones and their durations jointly on a large amount of unlabeled speech and text prompts. We then fine-tune the pre-trained model using human-annotated scoring data. Our experimental results, conducted on datasets such as Speechocean762 and our non-native datasets, show that our proposed method outperforms the baseline systems in terms of Pearson correlation coefficients (PCC). Moreover, we also conduct an ablation study to better under
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#37319;&#29992;&#22522;&#20110;&#35270;&#35273;&#24341;&#23548;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38899;&#33410;&#21457;&#29616;&#21644;&#36328;&#35821;&#35328;&#27867;&#21270;&#12290;&#20351;&#29992;&#26368;&#23567;&#21106;&#31639;&#27861;&#21644;2&#38454;&#27573;&#32858;&#31867;&#26041;&#27861;&#33258;&#21160;&#39044;&#27979;&#35821;&#38899;&#20013;&#30340;&#38899;&#33410;&#36793;&#30028;&#12290;&#22312;&#33521;&#35821;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38899;&#33410;&#20998;&#21106;&#26041;&#27861;&#65292;&#24182;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#22312;&#29233;&#27801;&#23612;&#20122;&#35821;&#19978;&#27867;&#21270;&#12290;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#20063;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2305.11435</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#24341;&#23548;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#20013;&#30340;&#38899;&#33410;&#21457;&#29616;&#21644;&#36328;&#35821;&#35328;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Mode. (arXiv:2305.11435v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#37319;&#29992;&#22522;&#20110;&#35270;&#35273;&#24341;&#23548;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38899;&#33410;&#21457;&#29616;&#21644;&#36328;&#35821;&#35328;&#27867;&#21270;&#12290;&#20351;&#29992;&#26368;&#23567;&#21106;&#31639;&#27861;&#21644;2&#38454;&#27573;&#32858;&#31867;&#26041;&#27861;&#33258;&#21160;&#39044;&#27979;&#35821;&#38899;&#20013;&#30340;&#38899;&#33410;&#36793;&#30028;&#12290;&#22312;&#33521;&#35821;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38899;&#33410;&#20998;&#21106;&#26041;&#27861;&#65292;&#24182;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#22312;&#29233;&#27801;&#23612;&#20122;&#35821;&#19978;&#27867;&#21270;&#12290;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#20063;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#22522;&#20110;&#35270;&#35273;&#24341;&#23548;&#30340;&#35757;&#32451;&#30446;&#26631;&#35757;&#32451;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#26102;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#34920;&#31034;&#38899;&#33410;&#30340;&#21333;&#20803;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20960;&#20046;&#30456;&#21516;&#30340;&#27169;&#22411;&#32467;&#26500;&#65288;HuBERT&#65289;&#65292;&#22312;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#26102;&#27809;&#26377;&#34920;&#29616;&#20986;&#36825;&#31181;&#33021;&#21147;&#65292;&#36825;&#34920;&#26126;&#35270;&#35273;&#24341;&#23548;&#30446;&#26631;&#23548;&#33268;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26368;&#23567;&#21106;&#31639;&#27861;&#33258;&#21160;&#39044;&#27979;&#35821;&#38899;&#20013;&#30340;&#38899;&#33410;&#36793;&#30028;&#65292;&#28982;&#21518;&#20351;&#29992;&#20004;&#38454;&#27573;&#32858;&#31867;&#26041;&#27861;&#23558;&#30456;&#21516;&#30340;&#38899;&#33410;&#32452;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#35757;&#32451;&#30340;&#35821;&#35328;&#65288;&#33521;&#35821;&#65289;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38899;&#33410;&#20998;&#21106;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#29233;&#27801;&#23612;&#20122;&#35821;&#19978;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#36827;&#34892;&#27867;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#21516;&#30340;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;4&#31181;&#20854;&#20182;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#21333;&#35789;&#20998;&#21106;&#20219;&#21153;&#27867;&#21270;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20987;&#36133;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we show that representations capturing syllabic units emerge when training a self-supervised speech model with a visually-grounded training objective. We demonstrate that a nearly identical model architecture (HuBERT) trained with a masked language modeling loss does not exhibit this same ability, suggesting that the visual grounding objective is responsible for the emergence of this phenomenon. We propose the use of a minimum cut algorithm to automatically predict syllable boundaries in speech, followed by a 2-stage clustering method to group identical syllables together. We show that our model not only outperforms a state-of-the-art syllabic segmentation method on the language it was trained on (English), but also generalizes in a zero-shot fashion to Estonian. Finally, we show that the same model is capable of zero-shot generalization for a word segmentation task on 4 other languages from the Zerospeech Challenge, in some cases beating the previous state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#26469;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#21464;&#24322;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11430</link><description>&lt;p&gt;
TELeR&#65306;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#22797;&#26434;&#20219;&#21153;&#30340;LLM&#25552;&#31034;&#30340;&#36890;&#29992;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks. (arXiv:2305.11430v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#26469;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#21464;&#24322;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;LLM&#22312;&#20256;&#32479;&#23545;&#35805;&#29615;&#22659;&#20013;&#29702;&#35299;&#21644;&#29983;&#25104;&#25991;&#26412;&#26102;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#25191;&#34892;&#19981;&#26126;&#30830;&#30340;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#28982;&#21463;&#21040;&#24456;&#23569;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#65292;&#20197;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31867;&#22411;/&#39118;&#26684;&#21644;&#25552;&#31034;&#25552;&#20379;&#30340;&#19981;&#21516;&#35814;&#32454;&#31243;&#24230;&#26102;LLM&#24615;&#33021;&#21464;&#21270;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#20998;&#31867;&#27861;&#23558;&#20351;&#26410;&#26469;&#30340;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#33021;&#22815;&#25253;&#21578;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#29305;&#23450;&#25552;&#31034;&#31867;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#36328;&#19981;&#21516;&#30740;&#31350;&#30340;&#26377;&#24847;&#20041;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
While LLMs have shown great success in understanding and generating text in traditional conversational settings, their potential for performing ill-defined complex tasks is largely under-studied. Indeed, we are yet to conduct comprehensive benchmarking studies with multiple LLMs that are exclusively focused on a complex task. However, conducting such benchmarking studies is challenging because of the large variations in LLMs' performance when different prompt types/styles are used and different degrees of detail are provided in the prompts. To address this issue, the paper proposes a general taxonomy that can be used to design prompts with specific properties in order to perform a wide range of complex tasks. This taxonomy will allow future benchmarking studies to report the specific categories of prompts used as part of the study, enabling meaningful comparisons across different studies. Also, by establishing a common standard through this taxonomy, researchers will be able to draw mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;AMPLIFY&#65292;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#33258;&#21160;&#21270;&#29983;&#25104;&#21407;&#22240;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11426</link><description>&lt;p&gt;
&#21518;&#39564;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Post Hoc Explanations of Language Models Can Improve Language Models. (arXiv:2305.11426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;AMPLIFY&#65292;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#33258;&#21160;&#21270;&#29983;&#25104;&#21407;&#22240;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#20837;&#20154;&#31867;&#27880;&#37322;&#30340;&#21407;&#29702;&#65288;&#20363;&#22914;&#65292;&#24605;&#32500;&#38142;&#25552;&#31034;&#65289;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#25512;&#29702;&#33021;&#21147;&#30340;&#20219;&#21153;&#19978;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#21407;&#29702;&#21152;&#20837;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#39640;&#24230;&#30340;&#20154;&#24037;&#21442;&#19982;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#21363;&#36890;&#36807;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25918;&#22823;&#27169;&#22411;&#24615;&#33021;&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#36755;&#20986;&#31216;&#20026;&#23646;&#24615;&#20998;&#25968;&#65288;&#35299;&#37322;&#65289;&#30340;&#20540;&#65292;&#29992;&#20110;&#25429;&#33719;&#27599;&#20010;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#33258;&#21160;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#21407;&#29702;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;&#23646;&#24615;&#20998;&#25968;&#20013;&#33719;&#24471;&#30340;&#20449;&#24687;&#65292;&#20197;&#20415;&#29992;&#25143;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AMPLIFY&#21487;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of- Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insi
&lt;/p&gt;</description></item><item><title>&#31163;&#25955;&#21333;&#20803;&#21453;&#21521;&#32763;&#35793;&#26041;&#27861;&#25104;&#21151;&#23558;&#26377;&#29992;&#30340;MT&#25216;&#26415;&#24212;&#29992;&#22312;&#30452;&#25509;ST&#19978;&#65292;&#24179;&#22343;&#25552;&#21319;&#20102;5.5 BLEU&#65292;&#21487;&#20197;&#32531;&#35299;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11411</link><description>&lt;p&gt;
DUB: &#31163;&#25955;&#21333;&#20803;&#21453;&#21521;&#32763;&#35793;&#29992;&#20110;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
DUB: Discrete Unit Back-translation for Speech Translation. (arXiv:2305.11411v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11411
&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#21333;&#20803;&#21453;&#21521;&#32763;&#35793;&#26041;&#27861;&#25104;&#21151;&#23558;&#26377;&#29992;&#30340;MT&#25216;&#26415;&#24212;&#29992;&#22312;&#30452;&#25509;ST&#19978;&#65292;&#24179;&#22343;&#25552;&#21319;&#20102;5.5 BLEU&#65292;&#21487;&#20197;&#32531;&#35299;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#20351;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#65288;ST&#65289;&#19982;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#30456;&#24403;&#65311;&#20851;&#38190;&#22312;&#20110;&#24357;&#21512;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#65292;&#20351;&#24471;&#26377;&#29992;&#30340;MT&#25216;&#26415;&#21487;&#20197;&#24212;&#29992;&#20110;ST&#12290;&#26368;&#36817;&#65292;&#29992;&#26080;&#30417;&#30563;&#31163;&#25955;&#21333;&#20803;&#34920;&#31034;&#35821;&#38899;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#27169;&#24577;&#38382;&#39064;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#25552;&#20986;&#20102;&#31163;&#25955;&#21333;&#20803;&#21453;&#21521;&#32763;&#35793;&#65288;DUB&#65289;&#20197;&#22238;&#31572;&#20004;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#22312;&#30452;&#25509;ST&#20013;&#65292;&#29992;&#31163;&#25955;&#21333;&#20803;&#34920;&#31034;&#35821;&#38899;&#26159;&#21542;&#27604;&#20351;&#29992;&#36830;&#32493;&#29305;&#24449;&#26356;&#22909;&#65311;&#65288;2&#65289;&#26377;&#29992;&#30340;MT&#25216;&#26415;&#21487;&#20197;&#20026;ST&#24102;&#26469;&#22810;&#23569;&#25910;&#30410;&#65311;&#36890;&#36807;DUB&#65292;&#21453;&#21521;&#32763;&#35793;&#25216;&#26415;&#21487;&#20197;&#25104;&#21151;&#24212;&#29992;&#20110;&#30452;&#25509;ST&#65292;&#24182;&#22312;MuST-C En-De / Fr / Es&#19978;&#24179;&#22343;&#25552;&#21319;&#20102;5.5 BLEU&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35821;&#35328;&#29615;&#22659;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#19982;&#20381;&#36182;&#22823;&#35268;&#27169;&#22806;&#37096;&#25968;&#25454;&#30340;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;/models&#21487;&#22312;https://github.com/0nutation/DUB&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can speech-to-text translation (ST) perform as well as machine translation (MT)? The key point is to bridge the modality gap between speech and text so that useful MT techniques can be applied to ST. Recently, the approach of representing speech with unsupervised discrete units yields a new way to ease the modality problem. This motivates us to propose Discrete Unit Back-translation (DUB) to answer two questions: (1) Is it better to represent speech with discrete units than with continuous features in direct ST? (2) How much benefit can useful MT techniques bring to ST? With DUB, the back-translation technique can successfully be applied on direct ST and obtains an average boost of 5.5 BLEU on MuST-C En-De/Fr/Es. In the low-resource language scenario, our method achieves comparable performance to existing methods that rely on large-scale external data. Code and models are available at https://github.com/0nutation/DUB.
&lt;/p&gt;</description></item><item><title>AlignAtt&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;SimulST&#31574;&#30053;&#65292;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32763;&#35793;&#23545;&#40784;&#26469;&#25351;&#23548;&#27169;&#22411;&#65292;&#22312;BLEU&#21644;&#24310;&#36831;&#26041;&#38754;&#22343;&#20248;&#20110;&#20043;&#21069;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.11408</link><description>&lt;p&gt;
AlignAtt&#65306;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32763;&#35793;&#23545;&#40784;&#20316;&#20026;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#30340;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation. (arXiv:2305.11408v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11408
&lt;/p&gt;
&lt;p&gt;
AlignAtt&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;SimulST&#31574;&#30053;&#65292;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32763;&#35793;&#23545;&#40784;&#26469;&#25351;&#23548;&#27169;&#22411;&#65292;&#22312;BLEU&#21644;&#24310;&#36831;&#26041;&#38754;&#22343;&#20248;&#20110;&#20043;&#21069;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26159;&#24403;&#20170;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#24120;&#29992;&#30340;&#26550;&#26500;&#30340;&#26680;&#24515;&#26426;&#21046;&#65292;&#24182;&#24050;&#20174;&#35768;&#22810;&#35282;&#24230;&#36827;&#34892;&#20998;&#26512;&#65292;&#21253;&#25324;&#20854;&#22312;&#26426;&#22120;&#32763;&#35793;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#20123;&#30740;&#31350;&#20013;&#65292;&#27880;&#24847;&#21147;&#22312;&#36755;&#20837;&#25991;&#26412;&#34987;&#26367;&#25442;&#20026;&#38899;&#39057;&#29255;&#27573;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#26159;&#33719;&#21462;&#26377;&#20851;&#21333;&#35789;&#23545;&#40784;&#30340;&#26377;&#29992;&#20449;&#24687;&#30340;&#19968;&#31181;&#26041;&#24335;&#65292;&#20363;&#22914;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AlignAtt&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#26102;ST&#65288;SimulST&#65289;&#31574;&#30053;&#65292;&#23427;&#21033;&#29992;&#27880;&#24847;&#21147;&#20449;&#24687;&#26469;&#29983;&#25104;&#28304;-&#30446;&#26631;&#23545;&#40784;&#65292;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25351;&#23548;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;MuST-C v1.0&#30340;8&#31181;&#35821;&#35328;&#23545;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#32447;&#19979;&#35757;&#32451;&#30340;&#27169;&#22411;&#19978;&#24212;&#29992;&#20808;&#21069;&#30340;&#26368;&#26032;SimulST&#31574;&#30053;&#65292;AlignAtt&#22312;BLEU&#26041;&#38754;&#33719;&#24471;&#20102;2&#20010;&#20998;&#25968;&#30340;&#25552;&#39640;&#65292;&#24182;&#19988;8&#31181;&#35821;&#35328;&#30340;&#24310;&#36831;&#32553;&#20943;&#22312;0.5&#31186;&#21040;0.8&#31186;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention is the core mechanism of today's most used architectures for natural language processing and has been analyzed from many perspectives, including its effectiveness for machine translation-related tasks. Among these studies, attention resulted to be a useful source of information to get insights about word alignment also when the input text is substituted with audio segments, as in the case of the speech translation (ST) task. In this paper, we propose AlignAtt, a novel policy for simultaneous ST (SimulST) that exploits the attention information to generate source-target alignments that guide the model during inference. Through experiments on the 8 language pairs of MuST-C v1.0, we show that AlignAtt outperforms previous state-of-the-art SimulST policies applied to offline-trained models with gains in terms of BLEU of 2 points and latency reductions ranging from 0.5s to 0.8s across the 8 languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22312;Twitter&#19978;&#30340;YouTube&#35270;&#39057;&#35843;&#26597;&#20102;COVID-19&#26399;&#38388;&#30340;&#39278;&#39135;&#21464;&#21270;&#65292;&#21457;&#29616;&#25910;&#20837;&#36739;&#20302;&#22320;&#21306;&#30340;&#33021;&#37327;&#12289;&#33026;&#32938;&#21644;&#39281;&#21644;&#33026;&#32938;&#25668;&#20837;&#37327;&#19979;&#38477;&#65292;&#39640;&#31958;&#12289;&#39640;&#34507;&#30333;&#21644;&#39640;&#38048;&#30340;&#39135;&#21697;&#25104;&#20026;&#31361;&#20986;&#30340;&#35805;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11398</link><description>&lt;p&gt;
&#33298;&#36866;&#39135;&#21697;&#19982;&#31038;&#21306;&#36830;&#32467;&#65306;&#21033;&#29992;Twitter&#19978;&#30340;YouTube&#35270;&#39057;&#35843;&#26597;COVID-19&#26399;&#38388;&#30340;&#39278;&#39135;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Comfort Foods and Community Connectedness: Investigating Diet Change during COVID-19 Using YouTube Videos on Twitter. (arXiv:2305.11398v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22312;Twitter&#19978;&#30340;YouTube&#35270;&#39057;&#35843;&#26597;&#20102;COVID-19&#26399;&#38388;&#30340;&#39278;&#39135;&#21464;&#21270;&#65292;&#21457;&#29616;&#25910;&#20837;&#36739;&#20302;&#22320;&#21306;&#30340;&#33021;&#37327;&#12289;&#33026;&#32938;&#21644;&#39281;&#21644;&#33026;&#32938;&#25668;&#20837;&#37327;&#19979;&#38477;&#65292;&#39640;&#31958;&#12289;&#39640;&#34507;&#30333;&#21644;&#39640;&#38048;&#30340;&#39135;&#21697;&#25104;&#20026;&#31361;&#20986;&#30340;&#35805;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20896;&#30123;&#24773;&#24320;&#22987;&#26102;&#31354;&#21069;&#30340;&#23553;&#38145;&#25514;&#26045;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#25968;&#30334;&#19975;&#20154;&#30340;&#26085;&#24120;&#29983;&#27963;&#26041;&#24335;&#65292;&#21487;&#33021;&#24433;&#21709;&#21040;&#37325;&#35201;&#30340;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22312;COVID-19&#30123;&#24773;&#21069;&#21518;&#21457;&#24067;&#30340;&#20851;&#20110;&#39278;&#39135;&#12289;&#38203;&#28860;&#21644;&#20581;&#36523;&#30340;Twitter&#19978;&#30340;YouTube&#35270;&#39057;&#26469;&#35843;&#26597;&#30123;&#24773;&#23553;&#38145;&#23545;&#39278;&#39135;&#21644;&#33829;&#20859;&#24433;&#21709;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#20998;&#21035;&#20174;&#35270;&#39057;&#30340;&#25991;&#23383;&#31295;&#12289;&#25551;&#36848;&#21644;&#26631;&#39064;&#20013;&#32771;&#23519;&#25152;&#25552;&#21040;&#39135;&#29289;&#30340;&#20845;&#31181;&#33829;&#20859;&#29289;&#36136;&#65288;&#34507;&#30333;&#36136;&#12289;&#33021;&#37327;&#12289;&#33026;&#32938;&#12289;&#38048;&#12289;&#31958;&#21644;&#39281;&#21644;&#33026;&#32938;&#65289;&#30340;&#28909;&#37327;&#20540;&#12290;&#36825;&#20123;&#33829;&#20859;&#29289;&#36136;&#20540;&#36827;&#19968;&#27493;&#19982;&#21508;&#31181;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#20851;&#32852;&#36215;&#26469;&#65292;&#20197;&#35780;&#20272;&#26159;&#21542;&#20250;&#23545;&#37027;&#20123;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#20581;&#24247;&#39135;&#21697;&#25552;&#20379;&#29305;&#23450;&#24433;&#21709;&#12290;&#20013;&#26029;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26174;&#31034;&#65292;&#22312;COVID-19&#26399;&#38388;&#65292;&#24635;&#20307;&#33829;&#20859;&#32032;&#24471;&#20998;&#20986;&#29616;&#20102;&#30456;&#24403;&#22823;&#30340;&#21464;&#21270;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#25910;&#20837;&#36739;&#20302;&#30340;&#22320;&#21306;&#65292;&#33021;&#37327;&#12289;&#33026;&#32938;&#21644;&#39281;&#21644;&#33026;&#32938;&#30340;&#25668;&#20837;&#37327;&#19979;&#38477;&#65292;&#32780;&#39640;&#31958;&#12289;&#39640;&#34507;&#30333;&#21644;&#39640;&#38048;&#30340;&#39135;&#21697;&#25104;&#20026;&#31038;&#20132;&#23186;&#20307;&#35752;&#35770;&#20013;&#26356;&#21152;&#31361;&#20986;&#65292;&#36825;&#21487;&#33021;&#21453;&#26144;&#20102;&#23545;&#39135;&#21697;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unprecedented lockdowns at the start of the COVID-19 pandemic have drastically changed the routines of millions of people, potentially impacting important health-related behaviors. In this study, we use YouTube videos embedded in tweets about diet, exercise and fitness posted before and during COVID-19 to investigate the influence of the pandemic lockdowns on diet and nutrition. In particular, we examine the nutritional profile of the foods mentioned in the transcript, description and title of each video in terms of six macronutrients (protein, energy, fat, sodium, sugar, and saturated fat). These macronutrient values were further linked to demographics to assess if there are specific effects on those potentially having insufficient access to healthy sources of food. Interrupted time series analysis revealed a considerable shift in the aggregated macronutrient scores before and during COVID-19. In particular, whereas areas with lower incomes showed decrease in energy, fat, and saturate
&lt;/p&gt;</description></item><item><title>Fast-StrucTexT&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#25991;&#26723;&#29702;&#35299;&#26694;&#26550;&#65292;&#37319;&#29992;&#27801;&#28431;&#21464;&#21387;&#22120;&#32467;&#26500;&#21644; Symmetry Cross Attention &#31561;&#26041;&#27861;&#23454;&#29616;&#20102;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#27169;&#24577;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.11392</link><description>&lt;p&gt;
Fast-StrucTexT&#65306;&#19968;&#31181;&#24102;&#26377;&#27169;&#24577;&#24341;&#23548;&#21160;&#24577;&#26631;&#35760;&#21512;&#24182;&#30340;&#39640;&#25928;&#27801;&#28431;&#21464;&#21387;&#22120;&#65292;&#29992;&#20110;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Fast-StrucTexT: An Efficient Hourglass Transformer with Modality-guided Dynamic Token Merge for Document Understanding. (arXiv:2305.11392v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11392
&lt;/p&gt;
&lt;p&gt;
Fast-StrucTexT&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#25991;&#26723;&#29702;&#35299;&#26694;&#26550;&#65292;&#37319;&#29992;&#27801;&#28431;&#21464;&#21387;&#22120;&#32467;&#26500;&#21644; Symmetry Cross Attention &#31561;&#26041;&#27861;&#23454;&#29616;&#20102;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#27169;&#24577;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#22312;&#25991;&#26723;&#29702;&#35299;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#26524;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#23545;&#24207;&#21015;&#38271;&#24230;&#30340;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24230;&#20381;&#36182;&#65292;&#22240;&#27492;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;StrucTexT&#31639;&#27861;&#30340;&#39640;&#25928;&#22810;&#27169;&#24577;&#26694;&#26550;Fast-StrucTexT&#65292;&#37319;&#29992;&#27801;&#28431;&#21464;&#21387;&#22120;&#20307;&#31995;&#32467;&#26500;&#29992;&#20110;&#25991;&#26723;&#24067;&#23616;&#34920;&#31034;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#27169;&#24577;&#24341;&#23548;&#30340;&#21160;&#24577;&#26631;&#35760;&#21512;&#24182;&#22359;&#65292;&#20197;&#23398;&#20064;&#22810;&#31890;&#24230;&#34920;&#31034;&#24182;&#20462;&#21098;&#20887;&#20313;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#20132;&#20114;&#27169;&#22359;Symmetry Cross Attention&#65288;SCA&#65289;&#20197;&#32771;&#34385;&#22810;&#27169;&#24577;&#34701;&#21512;&#24182;&#26377;&#25928;&#22320;&#24341;&#23548;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers achieve promising performance in document understanding because of their high effectiveness and still suffer from quadratic computational complexity dependency on the sequence length. General efficient transformers are challenging to be directly adapted to model document. They are unable to handle the layout representation in documents, e.g. word, line and paragraph, on different granularity levels and seem hard to achieve a good trade-off between efficiency and performance. To tackle the concerns, we propose Fast-StrucTexT, an efficient multi-modal framework based on the StrucTexT algorithm with an hourglass transformer architecture, for visual document understanding. Specifically, we design a modality-guided dynamic token merging block to make the model learn multi-granularity representation and prunes redundant tokens. Additionally, we present a multi-modal interaction module called Symmetry Cross Attention (SCA) to consider multi-modal fusion and efficiently guide the 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#35821;&#35328;&#21644;&#28436;&#31034;&#20132;&#27969;&#30340;&#25945;&#23398;&#27169;&#24335;&#30340;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20219;&#21153;&#36739;&#20026;&#31616;&#21333;&#26102;&#65292;&#28436;&#31034;&#25945;&#23398;&#26356;&#20026;&#26377;&#25928;&#65307;&#32780;&#24403;&#20219;&#21153;&#38590;&#24230;&#22686;&#21152;&#26102;&#65292;&#35821;&#35328;&#25945;&#23398;&#26356;&#23481;&#26131;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#24773;&#22659;&#20013;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#35821;&#35328;&#21644;&#28436;&#31034;&#20316;&#20026;&#20154;&#31867;&#25945;&#23398;&#26041;&#24335;&#30340;&#26435;&#34913;&#35777;&#25454;&#65292;&#21516;&#26102;&#23545;&#20110;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#24212;&#35813;&#36873;&#25321;&#20309;&#31181;&#25945;&#23398;&#26041;&#24335;&#25552;&#20986;&#20102;&#26032;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.11374</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#36890;&#36807;&#35821;&#35328;&#21644;&#28436;&#31034;&#25945;&#23398;&#30340;&#26435;&#34913;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Characterizing tradeoffs between teaching via language and demonstrations in multi-agent systems. (arXiv:2305.11374v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11374
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#35821;&#35328;&#21644;&#28436;&#31034;&#20132;&#27969;&#30340;&#25945;&#23398;&#27169;&#24335;&#30340;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20219;&#21153;&#36739;&#20026;&#31616;&#21333;&#26102;&#65292;&#28436;&#31034;&#25945;&#23398;&#26356;&#20026;&#26377;&#25928;&#65307;&#32780;&#24403;&#20219;&#21153;&#38590;&#24230;&#22686;&#21152;&#26102;&#65292;&#35821;&#35328;&#25945;&#23398;&#26356;&#23481;&#26131;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#24773;&#22659;&#20013;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#35821;&#35328;&#21644;&#28436;&#31034;&#20316;&#20026;&#20154;&#31867;&#25945;&#23398;&#26041;&#24335;&#30340;&#26435;&#34913;&#35777;&#25454;&#65292;&#21516;&#26102;&#23545;&#20110;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#24212;&#35813;&#36873;&#25321;&#20309;&#31181;&#25945;&#23398;&#26041;&#24335;&#25552;&#20986;&#20102;&#26032;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#35821;&#35328;&#21644;&#28436;&#31034;&#26469;&#21521;&#20182;&#20154;&#25945;&#25480;&#20851;&#20110;&#19990;&#30028;&#30340;&#30693;&#35782;&#12290;&#20309;&#26102;&#20854;&#20013;&#19968;&#31181;&#26041;&#24335;&#27604;&#21478;&#19968;&#31181;&#26041;&#24335;&#26356;&#26377;&#25928;&#65311;&#26412;&#25991;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#26469;&#27169;&#25311;&#20154;&#31867;&#20132;&#27969;&#65292;&#30740;&#31350;&#20102;&#35821;&#35328;&#21644;&#28436;&#31034;&#30340;&#26377;&#25928;&#24615;&#35843;&#33410;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#36890;&#36807;&#35821;&#35328;&#25110;&#28436;&#31034;&#36827;&#34892;&#22522;&#20110;&#22330;&#26223;&#30340;&#36890;&#20449;&#20219;&#21153;&#65292;&#25805;&#32437;1&#65289;&#20219;&#21153;&#30340;&#20869;&#22312;&#38590;&#24230;&#21644;2&#65289;&#25945;&#24072;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26368;&#31616;&#21333;&#30340;&#35774;&#32622;&#20013;&#65292;&#28436;&#31034;&#25945;&#23398;&#26356;&#26377;&#25928;&#65292;&#20294;&#38543;&#30528;&#20219;&#21153;&#38590;&#24230;&#30340;&#22686;&#21152;&#65292;&#35821;&#35328;&#26356;&#26377;&#25928;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#24773;&#22659;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#32467;&#26524;&#25552;&#20379;&#20102;&#35821;&#35328;&#21644;&#28436;&#31034;&#20316;&#20026;&#20154;&#31867;&#25945;&#23398;&#26041;&#24335;&#20043;&#38388;&#26435;&#34913;&#30340;&#35777;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#27979;&#65292;&#21363;&#28436;&#31034;&#21487;&#33021;&#26159;&#31616;&#21333;&#20219;&#21153;&#30340;&#26368;&#20339;&#36873;&#25321;&#65292;&#32780;&#35821;&#35328;&#21017;&#33021;&#22815;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans teach others about the world through language and demonstration. When might one of these modalities be more effective than the other? In this work, we study the factors that modulate the effectiveness of language vs. demonstration using multi-agent systems to model human communication. Specifically, we train neural network agents to teach via language or demonstration in a grounded communication task, manipulating 1) the inherent difficulty of the task and 2) the competence of the teacher. We find that teaching by demonstration is more effective in the simplest settings, but language is more effective as task difficulty increases, due to its ability to generalize more effectively to unseen scenarios. Overall, these results provide converging evidence for a tradeoff between language and demonstration as teaching modalities in humans, and make the novel predictions that demonstration may be optimal for easy tasks, while language enables generalization in more challenging settings.
&lt;/p&gt;</description></item><item><title>AutoTrial&#26159;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20020;&#24202;&#35797;&#39564;&#32435;&#20837;/&#25490;&#38500;&#26631;&#20934;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21487;&#25511;&#29983;&#25104;&#12289;&#21487;&#25193;&#23637;&#23398;&#20064;&#12289;&#25552;&#20379;&#25512;&#29702;&#38142;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#27969;&#30021;&#20934;&#30830;&#30340;&#26631;&#20934;&#25991;&#26412;&#65292;&#19982;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#65292;&#20294;&#36164;&#28304;&#21344;&#29992;&#26356;&#23569;&#12290;</title><link>http://arxiv.org/abs/2305.11366</link><description>&lt;p&gt;
AutoTrial&#65306;&#29992;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20020;&#24202;&#35797;&#39564;&#35774;&#35745;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
AutoTrial: Prompting Language Models for Clinical Trial Design. (arXiv:2305.11366v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11366
&lt;/p&gt;
&lt;p&gt;
AutoTrial&#26159;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20020;&#24202;&#35797;&#39564;&#32435;&#20837;/&#25490;&#38500;&#26631;&#20934;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21487;&#25511;&#29983;&#25104;&#12289;&#21487;&#25193;&#23637;&#23398;&#20064;&#12289;&#25552;&#20379;&#25512;&#29702;&#38142;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#27969;&#30021;&#20934;&#30830;&#30340;&#26631;&#20934;&#25991;&#26412;&#65292;&#19982;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#65292;&#20294;&#36164;&#28304;&#21344;&#29992;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#23545;&#20110;&#33647;&#29289;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#24739;&#32773;&#25311;&#23450;&#21512;&#36866;&#30340;&#32435;&#20837;/&#25490;&#38500;&#26631;&#20934;&#26159;&#35797;&#39564;&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;&#20020;&#24202;&#35797;&#39564;&#26041;&#26696;&#30340;&#27491;&#30830;&#35774;&#35745;&#24212;&#32771;&#34385;&#21040;&#31867;&#20284;&#30340;&#20808;&#20363;&#35797;&#39564;&#21450;&#20854;&#32435;&#20837;/&#25490;&#38500;&#26631;&#20934;&#65292;&#20197;&#30830;&#20445;&#24739;&#32773;&#30340;&#20805;&#20998;&#35206;&#30422;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoTrial&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#26469;&#24110;&#21161;&#35774;&#35745;&#20020;&#24202;&#32435;&#20837;/&#25490;&#38500;&#26631;&#20934;&#12290;&#23427;&#20801;&#35768;&#65288;1&#65289;&#36890;&#36807;&#31163;&#25955;&#21644;&#31070;&#32463;&#25552;&#31034;&#30340;&#28151;&#21512;&#36827;&#34892;&#21487;&#25511;&#30340;&#25351;&#23548;&#29983;&#25104;&#65292;&#65288;2&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#34701;&#21512;&#65292;&#20197;&#21450;&#65288;3&#65289;&#25552;&#20379;&#26126;&#30830;&#30340;&#25512;&#29702;&#38142;&#20197;&#29702;&#35299;&#36755;&#20986;&#30340;&#21512;&#29702;&#24615;&#12290;&#23545;&#36229;&#36807;70,000&#39033;&#20020;&#24202;&#35797;&#39564;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;AutoTrial&#29983;&#25104;&#30340;&#26631;&#20934;&#25991;&#26412;&#20855;&#26377;&#27969;&#30021;&#24615;&#21644;&#36830;&#36143;&#24615;&#65292;&#24182;&#19988;&#22312;&#25429;&#25417;&#30446;&#26631;&#35797;&#39564;&#30456;&#20851;&#20020;&#24202;&#27010;&#24565;&#26041;&#38754;&#20934;&#30830;&#24230;&#39640;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#36164;&#28304;&#21344;&#29992;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#21487;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical trials are critical for drug development. Constructing the appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for patient recruitment) is essential for the trial's success. Proper design of clinical trial protocols should consider similar precedent trials and their eligibility criteria to ensure sufficient patient coverage. In this paper, we present a method named AutoTrial to aid the design of clinical eligibility criteria using language models. It allows (1) controllable generation under instructions via a hybrid of discrete and neural prompting, (2) scalable knowledge incorporation via in-context learning, and (3) explicit reasoning chains to provide rationales for understanding the outputs. Experiments on over 70K clinical trials verify that AutoTrial generates high-quality criteria texts that are fluent and coherent and with high accuracy in capturing the relevant clinical concepts to the target trial. It is noteworthy that our method, with a much sm
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#30340;&#21477;&#27861;&#22810;&#26679;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#23618;&#21487;&#35270;&#21270;&#26469;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#27983;&#35272;&#27010;&#36848;&#21644;&#26816;&#26597;&#21508;&#20010;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.11364</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#22810;&#26679;&#24615;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models. (arXiv:2305.11364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#30340;&#21477;&#27861;&#22810;&#26679;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#23618;&#21487;&#35270;&#21270;&#26469;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#27983;&#35272;&#27010;&#36848;&#21644;&#26816;&#26597;&#21508;&#20010;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#29983;&#25104;&#26356;&#31934;&#32454;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#12289;&#24494;&#35843;&#25110;&#20854;&#20182;&#29992;&#20363;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#21644;&#35780;&#20272;&#36825;&#20123;&#25968;&#25454;&#38598;&#24456;&#22256;&#38590;&#65292;&#32780;LLM&#29983;&#25104;&#25968;&#25454;&#30340;&#22833;&#36133;&#27169;&#24335;&#20173;&#19981;&#20026;&#20154;&#20204;&#25152;&#29702;&#35299;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25968;&#25454;&#21487;&#33021;&#20197;&#24847;&#22806;&#30340;&#26041;&#24335;&#21464;&#24471;&#37325;&#22797;&#65292;&#19981;&#20165;&#35821;&#20041;&#19978;&#22914;&#27492;&#65292;&#32780;&#19988;&#22312;&#21477;&#27861;&#12289;&#35789;&#27719;&#26041;&#38754;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LinguisticLens&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#20998;&#26512;LLM&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#21477;&#27861;&#22810;&#26679;&#24615;&#12290; LinguisticLens&#27839;&#30528;&#21477;&#27861;&#12289;&#35789;&#27719;&#21644;&#35821;&#20041;&#36724;&#23558;&#25991;&#26412;&#32858;&#31867;&#12290;&#23427;&#25903;&#25345;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#20998;&#23618;&#21487;&#35270;&#21270;&#65292;&#20801;&#35768;&#29992;&#25143;&#24555;&#36895;&#27983;&#35272;&#27010;&#36848;&#21644;&#26816;&#26597;&#21508;&#20010;&#31034;&#20363;&#12290;&#23454;&#26102;&#28436;&#31034;&#21487;&#22312;shorturl.at/zHOUV&#26597;&#30475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can be used to generate smaller, more refined datasets via few-shot prompting for benchmarking, fine-tuning or other use cases. However, understanding and evaluating these datasets is difficult, and the failure modes of LLM-generated data are still not well understood. Specifically, the data can be repetitive in surprising ways, not only semantically but also syntactically and lexically. We present LinguisticLens, a novel inter-active visualization tool for making sense of and analyzing syntactic diversity of LLM-generated datasets. LinguisticLens clusters text along syntactic, lexical, and semantic axes. It supports hierarchical visualization of a text dataset, allowing users to quickly scan for an overview and inspect individual examples. The live demo is available at shorturl.at/zHOUV.
&lt;/p&gt;</description></item><item><title>MD3&#26159;&#19968;&#20010;&#26032;&#30340;&#22810;&#26041;&#35328;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#21360;&#24230;&#12289;&#23612;&#26085;&#21033;&#20122;&#21644;&#32654;&#22269;&#30340;&#33521;&#35821;&#26041;&#35328;&#65292;&#36890;&#36807;&#25552;&#31034;&#21442;&#19982;&#32773;&#25191;&#34892;&#20449;&#24687;&#20849;&#20139;&#20219;&#21153;&#23454;&#29616;&#20102;&#24320;&#25918;&#23545;&#35805;&#35821;&#38899;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#30740;&#31350;&#21457;&#29616;&#20102;&#19981;&#21516;&#26041;&#35328;&#22312;&#21477;&#27861;&#21644;&#35821;&#31687;&#26631;&#35760;&#30340;&#20351;&#29992;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.11355</link><description>&lt;p&gt;
MD3: &#19968;&#20010;&#22810;&#26041;&#35328;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MD3: The Multi-Dialect Dataset of Dialogues. (arXiv:2305.11355v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11355
&lt;/p&gt;
&lt;p&gt;
MD3&#26159;&#19968;&#20010;&#26032;&#30340;&#22810;&#26041;&#35328;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#21360;&#24230;&#12289;&#23612;&#26085;&#21033;&#20122;&#21644;&#32654;&#22269;&#30340;&#33521;&#35821;&#26041;&#35328;&#65292;&#36890;&#36807;&#25552;&#31034;&#21442;&#19982;&#32773;&#25191;&#34892;&#20449;&#24687;&#20849;&#20139;&#20219;&#21153;&#23454;&#29616;&#20102;&#24320;&#25918;&#23545;&#35805;&#35821;&#38899;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#30740;&#31350;&#21457;&#29616;&#20102;&#19981;&#21516;&#26041;&#35328;&#22312;&#21477;&#27861;&#21644;&#35821;&#31687;&#26631;&#35760;&#30340;&#20351;&#29992;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#35805;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#21360;&#24230;&#12289;&#23612;&#26085;&#21033;&#20122;&#21644;&#32654;&#22269;&#30340;&#33521;&#35821;&#26041;&#35328;&#12290;&#22810;&#26041;&#35328;&#23545;&#35805;&#25968;&#25454;&#38598;&#65288;MD3&#65289;&#36890;&#36807;&#25552;&#31034;&#21442;&#19982;&#32773;&#25191;&#34892;&#19968;&#31995;&#21015;&#30701;&#20449;&#24687;&#20849;&#20139;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#24320;&#25918;&#23545;&#35805;&#35821;&#38899;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#20043;&#38388;&#30340;&#26032;&#24179;&#34913;&#12290;&#36825;&#26377;&#21161;&#20110;&#23450;&#37327;&#30340;&#36328;&#26041;&#35328;&#27604;&#36739;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#23545;&#25233;&#21046;&#26041;&#35328;&#29305;&#24449;&#34920;&#36798;&#30340;&#38480;&#21046;&#24615;&#20219;&#21153;&#32467;&#26500;&#30340;&#24378;&#21152;&#12290;&#23545;&#25968;&#25454;&#38598;&#30340;&#21021;&#27493;&#20998;&#26512;&#26174;&#31034;&#65292;&#19981;&#21516;&#26041;&#35328;&#22312;&#21477;&#27861;&#21644;&#35821;&#31687;&#26631;&#35760;&#30340;&#20351;&#29992;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25968;&#25454;&#38598;&#23558;&#38543;&#30528;&#26412;&#25991;&#30340;&#21457;&#34920;&#32780;&#20844;&#24320;&#21457;&#24067;&#65292;&#21253;&#25324;&#36229;&#36807;20&#20010;&#23567;&#26102;&#30340;&#38899;&#39057;&#21644;&#36229;&#36807;200,000&#20010;&#25340;&#20889;&#36716;&#24405;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new dataset of conversational speech representing English from India, Nigeria, and the United States. The Multi-Dialect Dataset of Dialogues (MD3) strikes a new balance between open-ended conversational speech and task-oriented dialogue by prompting participants to perform a series of short information-sharing tasks. This facilitates quantitative cross-dialectal comparison, while avoiding the imposition of a restrictive task structure that might inhibit the expression of dialect features. Preliminary analysis of the dataset reveals significant differences in syntax and in the use of discourse markers. The dataset, which will be made publicly available with the publication of this paper, includes more than 20 hours of audio and more than 200,000 orthographically-transcribed tokens.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23545;&#24050;&#35757;&#32451;&#22909;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#21518;&#26399;&#32534;&#36753;&#65292;&#20197;&#20415;&#32534;&#36753;&#25481;&#26576;&#20123;&#26465;&#20214;&#20998;&#25903;&#65292;&#36825;&#20123;&#26465;&#20214;&#20998;&#25903;&#24456;&#21487;&#33021;&#20250;&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#12290;&#36890;&#36807;&#31934;&#31616;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#32593;&#32476;&#23454;&#29616;&#65292;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26377;&#25928;&#12289;&#39640;&#25928;&#12289;&#20855;&#26377;&#21487;&#25511;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11351</link><description>&lt;p&gt;
&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Data Redaction from Conditional Generative Models. (arXiv:2305.11351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23545;&#24050;&#35757;&#32451;&#22909;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#21518;&#26399;&#32534;&#36753;&#65292;&#20197;&#20415;&#32534;&#36753;&#25481;&#26576;&#20123;&#26465;&#20214;&#20998;&#25903;&#65292;&#36825;&#20123;&#26465;&#20214;&#20998;&#25903;&#24456;&#21487;&#33021;&#20250;&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#12290;&#36890;&#36807;&#31934;&#31616;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#32593;&#32476;&#23454;&#29616;&#65292;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26377;&#25928;&#12289;&#39640;&#25928;&#12289;&#20855;&#26377;&#21487;&#25511;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22240;&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#32780;&#21463;&#21040;&#25209;&#35780;&#12290;&#20256;&#32479;&#30340;&#32531;&#35299;&#26041;&#27861;&#21253;&#25324;&#37325;&#26032;&#35757;&#32451;&#12289;&#36807;&#28388;&#25110;&#32534;&#36753;&#65307;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#35201;&#20040;&#20250;&#34987;&#31532;&#19977;&#26041;&#22238;&#36991;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#22914;&#20309;&#21518;&#26399;&#32534;&#36753;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#20854;&#32534;&#36753;&#25481;&#26576;&#20123;&#26465;&#20214;&#20998;&#25903;&#65292;&#36825;&#20123;&#26465;&#20214;&#20998;&#25903;&#24456;&#21487;&#33021;&#20250;&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#12290;&#36825;&#26159;&#36890;&#36807;&#31934;&#31616;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#32593;&#32476;&#26469;&#23454;&#29616;&#30340;&#65292;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#12289;&#20855;&#26377;&#21487;&#25511;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#33021;&#29992;&#20110;&#19968;&#31867;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#25968;&#25454;&#32534;&#36753;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#65292;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#32534;&#36753;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26080;&#38656;&#30417;&#30563;&#12289;&#36328;&#39046;&#22495;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#23884;&#20837;&#22810;&#27169;&#24577;&#20449;&#24687;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#26500;&#24314;&#25216;&#26415;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.11349</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#36328;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#24369;&#20449;&#21495;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain-agnostic Fake News Detection using Multi-modal Weak Signals. (arXiv:2305.11349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26080;&#38656;&#30417;&#30563;&#12289;&#36328;&#39046;&#22495;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#23884;&#20837;&#22810;&#27169;&#24577;&#20449;&#24687;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#26500;&#24314;&#25216;&#26415;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#20316;&#20026;&#20154;&#20204;&#33719;&#21462;&#26032;&#38395;&#30340;&#20027;&#35201;&#24179;&#21488;&#20043;&#19968;&#30340;&#20986;&#29616;&#65292;&#20419;&#36827;&#20102;&#34394;&#20551;&#26032;&#38395;&#30340;&#24191;&#27867;&#20256;&#25773;&#12290;&#36825;&#28608;&#21457;&#20102;&#22823;&#37327;&#30340;&#33258;&#21160;&#21270;&#35782;&#21035;&#34394;&#20551;&#26032;&#38395;&#30340;&#30740;&#31350;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#20102;&#19968;&#20123;&#26410;&#32463;&#30417;&#30563;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#23581;&#35797;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#26410;&#21033;&#29992;&#19982;&#26032;&#38395;&#35760;&#24405;&#30456;&#20851;&#30340;&#21508;&#31181;&#27169;&#24577;&#30693;&#35782;&#21644;&#29616;&#26377;&#26032;&#38395;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#21508;&#31181;&#28508;&#22312;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;&#65292;&#39318;&#20808;&#23558;&#26032;&#38395;&#35760;&#24405;&#20013;&#22235;&#31181;&#27169;&#24577;&#30340;&#30693;&#35782;&#23884;&#20837;&#65292;&#28982;&#21518;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22122;&#22768;&#40065;&#26834;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#35782;&#21035;&#26032;&#38395;&#35760;&#24405;&#30340;&#30495;&#23454;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26500;&#24314;&#26032;&#38395;&#25968;&#25454;&#38598;&#30340;&#25216;&#26415;&#65292;&#26368;&#23567;&#21270;&#29616;&#26377;&#26032;&#38395;&#25968;&#25454;&#38598;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;&#25353;&#29031;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#26500;&#24314;&#26041;&#27861;&#65292;&#25105;&#20204;&#21046;&#20316;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#30340;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of social media as one of the main platforms for people to access news has enabled the wide dissemination of fake news. This has motivated numerous studies on automating fake news detection. Although there have been limited attempts at unsupervised fake news detection, their performance suffers due to not exploiting the knowledge from various modalities related to news records and due to the presence of various latent biases in the existing news datasets. To address these limitations, this work proposes an effective framework for unsupervised fake news detection, which first embeds the knowledge available in four modalities in news records and then proposes a novel noise-robust self-supervised learning technique to identify the veracity of news records from the multi-modal embeddings. Also, we propose a novel technique to construct news datasets minimizing the latent biases in existing news datasets. Following the proposed approach for dataset construction, we produce a L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20020;&#24202;&#35760;&#24405;&#21435;&#35782;&#21035;&#31995;&#32479;&#22312;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#21517;&#31216;&#21435;&#35782;&#21035;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.11348</link><description>&lt;p&gt;
&#20197;&#20844;&#24179;&#21517;&#20041;&#65306;&#35780;&#20272;&#20020;&#24202;&#35760;&#24405;&#21435;&#35782;&#21035;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
In the Name of Fairness: Assessing the Bias in Clinical Record De-identification. (arXiv:2305.11348v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20020;&#24202;&#35760;&#24405;&#21435;&#35782;&#21035;&#31995;&#32479;&#22312;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#21517;&#31216;&#21435;&#35782;&#21035;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20849;&#20139;&#23545;&#20110;&#24320;&#25918;&#31185;&#23398;&#21644;&#21487;&#37325;&#22797;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#21512;&#27861;&#20849;&#20139;&#20020;&#24202;&#25968;&#25454;&#38656;&#35201;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#21024;&#38500;&#21463;&#20445;&#25252;&#30340;&#20581;&#24247;&#20449;&#24687;&#12290;&#36825;&#20010;&#36807;&#31243;&#65292;&#31216;&#20026;&#21435;&#35782;&#21035;&#65292;&#36890;&#24120;&#36890;&#36807;&#35768;&#22810;&#21830;&#19994;&#21644;&#24320;&#28304;&#31995;&#32479;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#12290;&#34429;&#28982;&#36825;&#20123;&#31995;&#32479;&#22312;&#24179;&#22343;&#27700;&#24179;&#19978;&#24050;&#32463;&#26174;&#31034;&#20986;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#20154;&#21475;&#32676;&#20307;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#36824;&#27809;&#26377;&#24471;&#21040;&#24443;&#24213;&#30340;&#26816;&#26597;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#20020;&#24202;&#31508;&#35760;&#20013;&#30340;&#21517;&#31216;&#21435;&#35782;&#21035;&#31995;&#32479;&#30340;&#20559;&#35265;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;16&#20010;&#21517;&#31216;&#38598;&#65292;&#28085;&#30422;&#20102;&#22235;&#20010;&#20154;&#21475;&#32479;&#35745;&#23398;&#32500;&#24230;&#65306;&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;&#21517;&#31216;&#27969;&#34892;&#24230;&#21644;&#27969;&#34892;&#30340;&#21313;&#24180;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#21517;&#31216;&#25554;&#20837;&#21040;100&#20010;&#25163;&#21160;&#31579;&#36873;&#30340;&#20020;&#24202;&#27169;&#26495;&#20013;&#65292;&#24182;&#35780;&#20272;&#20102;&#20061;&#31181;&#20844;&#20849;&#21644;&#31169;&#20154;&#21435;&#35782;&#21035;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#20020;&#24202;&#35760;&#24405;&#21435;&#35782;&#21035;&#31995;&#32479;&#30340;&#21517;&#31216;&#26041;&#38754;&#23384;&#22312;&#32479;&#35745;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data sharing is crucial for open science and reproducible research, but the legal sharing of clinical data requires the removal of protected health information from electronic health records. This process, known as de-identification, is often achieved through the use of machine learning algorithms by many commercial and open-source systems. While these systems have shown compelling results on average, the variation in their performance across different demographic groups has not been thoroughly examined. In this work, we investigate the bias of de-identification systems on names in clinical notes via a large-scale empirical analysis. To achieve this, we create 16 name sets that vary along four demographic dimensions: gender, race, name popularity, and the decade of popularity. We insert these names into 100 manually curated clinical templates and evaluate the performance of nine public and private de-identification methods. Our findings reveal that there are statistically significant p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;Tree-Search&#21644;&#33258;&#25105;&#19978;&#19979;&#25991;QA&#65292;&#21487;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;Tree-Search&#37319;&#26679;&#25216;&#26415;&#26377;&#21161;&#20110;&#20174;&#25552;&#31034;&#20013;&#25552;&#21462;&#22810;&#26679;&#21270;&#20449;&#24687;&#65292;&#32780;&#33258;&#25105;&#19978;&#19979;&#25991;QA&#21487;&#20351;&#27169;&#22411;&#21019;&#24314;&#33258;&#24049;&#30340;&#19978;&#19979;&#25991;&#65292;&#29983;&#25104;&#26356;&#22909;&#30340;&#24320;&#25918;&#24335;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#25552;&#39640;&#20581;&#22766;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11334</link><description>&lt;p&gt;
&#32534;&#20889;&#33258;&#24049;&#30340;&#20070;&#65306;&#19968;&#31181;&#20174;&#38381;&#21512;&#21040;&#24320;&#25918;&#24335;&#20070;&#26412;QA&#30340;&#26041;&#27861;&#65292;&#25913;&#21892;&#36739;&#23567;LLM&#30340;&#20581;&#22766;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Writing your own book: A method for going from closed to open book QA to improve robustness and performance of smaller LLMs. (arXiv:2305.11334v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;Tree-Search&#21644;&#33258;&#25105;&#19978;&#19979;&#25991;QA&#65292;&#21487;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;Tree-Search&#37319;&#26679;&#25216;&#26415;&#26377;&#21161;&#20110;&#20174;&#25552;&#31034;&#20013;&#25552;&#21462;&#22810;&#26679;&#21270;&#20449;&#24687;&#65292;&#32780;&#33258;&#25105;&#19978;&#19979;&#25991;QA&#21487;&#20351;&#27169;&#22411;&#21019;&#24314;&#33258;&#24049;&#30340;&#19978;&#19979;&#25991;&#65292;&#29983;&#25104;&#26356;&#22909;&#30340;&#24320;&#25918;&#24335;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#25552;&#39640;&#20581;&#22766;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;Tree-Search&#21644;&#33258;&#25105;&#19978;&#19979;&#25991;QA&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290; Tree-Search&#26159;&#19968;&#31181;&#37319;&#26679;&#25216;&#26415;&#65292;&#19987;&#38376;&#29992;&#20110;&#20174;&#32473;&#23450;&#25552;&#31034;&#30340;LLM&#20013;&#25552;&#21462;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#12290;&#33258;&#25105;&#19978;&#19979;&#25991;QA&#21033;&#29992;Tree-Search&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20351;&#29992;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#21508;&#31181;&#20449;&#24687;&#21019;&#24314;&#33258;&#24049;&#30340;&#19978;&#19979;&#25991;&#65292;&#26126;&#30830;&#35780;&#20272;&#24182;&#36820;&#22238;&#21021;&#22987;&#25552;&#31034;&#30340;&#24320;&#25918;&#24335;&#31572;&#26696;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25353;&#29031;&#21508;&#31181;&#25351;&#26631;&#65288;&#21253;&#25324;GPT3.5&#65288;text-davinci-003&#65289;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12289;&#20449;&#24687;&#37327;&#12289;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#65289;&#35780;&#20272;&#30340;&#29983;&#25104;&#31572;&#26696;&#36136;&#37327;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#20102;&#22686;&#21152;&#30340;&#20581;&#22766;&#24615;&#65292;&#24182;&#19988;&#24615;&#33021;&#19982;&#26641;&#22823;&#23567;&#21576;&#27491;&#30456;&#20851;&#65292;&#20174;&#32780;&#26377;&#30410;&#20110;&#31572;&#26696;&#36136;&#37327;&#21644;&#20581;&#22766;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;Tree-Search&#30340;&#20854;&#20182;&#26377; promising &#24212;&#29992;&#65292;&#31361;&#20986;&#20102;&#20854;&#25552;&#39640;&#36739;&#23567;LLM&#20581;&#22766;&#24615;&#21644;&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce two novel methods, Tree-Search and Self-contextualizing QA, designed to enhance the performance of large language models (LLMs) in question-answering tasks. Tree-Search is a sampling technique specifically created to extract diverse information from an LLM for a given prompt. Self-contextualizing QA leverages Tree-Search to enable the model to create its own context using a wide range of information relevant to the prompt, evaluate it explicitly and return a open book answer to the initial prompt . We demonstrate that the quality of generated answers improves according to various metrics, including accuracy, informativeness, coherence, and consistency, as evaluated by GPT3.5(text-davinci-003). Furthermore, we show that our methods result in increased robustness and that performance is positively correlated with tree size, benefiting both answer quality and robustness. Finally, we discuss other promising applications of Tree-Search, highlighting its potential to enhance a b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#33258;&#21160;&#21270;&#21019;&#36896;&#30340;&#20250;&#35805;&#25509;&#21475;&#26469;&#26041;&#20415;&#22823;&#20247;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11326</link><description>&lt;p&gt;
&#33258;&#21160;&#29983;&#25104;&#20250;&#35805;&#25509;&#21475;&#20197;&#20415;&#20110;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data. (arXiv:2305.11326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#33258;&#21160;&#21270;&#21019;&#36896;&#30340;&#20250;&#35805;&#25509;&#21475;&#26469;&#26041;&#20415;&#22823;&#20247;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#26159;&#22312;&#32447;&#21457;&#24067;&#21644;&#20132;&#25442;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#26368;&#24120;&#35265;&#26684;&#24335;&#12290;&#19968;&#20010;&#26126;&#30830;&#30340;&#20363;&#23376;&#26159;&#21508;&#31181;&#31867;&#22411;&#30340;&#20844;&#20849;&#34892;&#25919;&#26426;&#26500;&#21457;&#24067;&#30340;&#24320;&#25918;&#25968;&#25454;&#38376;&#25143;&#25968;&#37327;&#30340;&#22686;&#38271;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#25968;&#25454;&#28304;&#30340;&#21033;&#29992;&#30446;&#21069;&#20165;&#38480;&#20110;&#33021;&#22815;&#20197;&#31243;&#24207;&#26041;&#24335;&#22788;&#29702;&#21644;&#28040;&#21270;&#27492;&#31867;&#25968;&#25454;&#30340;&#25216;&#26415;&#20154;&#21592;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20250;&#35805;&#25509;&#21475;&#65292;&#20197;&#20415;&#20110;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;&#28304;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20219;&#20309;&#26222;&#36890;&#20844;&#27665;&#37117;&#21487;&#20197;&#20174;&#20013;&#21463;&#30410;&#24182;&#21033;&#29992;&#23427;&#20204;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#19981;&#26159;&#25163;&#21160;&#21019;&#24314;&#30340;&#65306;&#30456;&#21453;&#65292;&#23427;&#20204;&#26159;&#36890;&#36807;&#23454;&#20363;&#21270;&#21487;&#37197;&#32622;&#30340;&#23545;&#35805;&#27169;&#24335;&#38598;&#20174;&#25968;&#25454;&#28304;&#26412;&#36523;&#33258;&#21160;&#29983;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data is the most common format to publish and exchange structured data online. A clear example is the growing number of open data portals published by all types of public administrations. However, exploitation of these data sources is currently limited to technical people able to programmatically manipulate and digest such data. As an alternative, we propose the use of chatbots to offer a conversational interface to facilitate the exploration of tabular data sources. With our approach, any regular citizen can benefit and leverage them. Moreover, our chatbots are not manually created: instead, they are automatically generated from the data source itself thanks to the instantiation of a configurable collection of conversation patterns.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#38598;&#25104;GPT-k&#26469;&#25552;&#39640;T2I&#29983;&#25104;&#20013;&#30340;&#32534;&#36753;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26356;&#25797;&#38271;&#35843;&#25972;&#65288;&#20462;&#25913;&#65289;&#25991;&#26412;&#20013;&#30340;&#20462;&#39280;&#35821;&#65292;&#32780;&#20154;&#31867;&#20542;&#21521;&#20110;&#26367;&#25442;&#21333;&#35789;&#21644;&#30701;&#35821;&#12290;</title><link>http://arxiv.org/abs/2305.11317</link><description>&lt;p&gt;
&#21512;&#20316;&#29983;&#25104;AI&#65306;&#38598;&#25104;GPT-k&#20197;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#25552;&#39640;&#32534;&#36753;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation. (arXiv:2305.11317v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#38598;&#25104;GPT-k&#26469;&#25552;&#39640;T2I&#29983;&#25104;&#20013;&#30340;&#32534;&#36753;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26356;&#25797;&#38271;&#35843;&#25972;&#65288;&#20462;&#25913;&#65289;&#25991;&#26412;&#20013;&#30340;&#20462;&#39280;&#35821;&#65292;&#32780;&#20154;&#31867;&#20542;&#21521;&#20110;&#26367;&#25442;&#21333;&#35789;&#21644;&#30701;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#39046;&#22495;&#22312;&#30740;&#31350;&#30028;&#21644;&#29992;&#25143;&#20013;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#34429;&#28982;T2I&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#29992;&#25143;&#24120;&#36935;&#21040;&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#38656;&#35201;&#37325;&#22797;&#32534;&#36753;&#36755;&#20837;&#25552;&#31034;&#25165;&#33021;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#22270;&#20687;&#65292;&#36825;&#26159;&#32791;&#26102;&#19988;&#21171;&#21160;&#24378;&#24230;&#22823;&#30340;&#12290;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-k&#65289;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#25913;&#36827;T2I&#29983;&#25104;&#20013;&#25552;&#31034;&#32534;&#36753;&#36807;&#31243;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;GPT-k&#24120;&#35265;&#30340;&#32534;&#36753;&#26041;&#24335;&#65292;&#35780;&#20272;GPT-k&#22312;&#25512;&#21160;T2I&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#26816;&#26597;&#21487;&#33021;&#24433;&#21709;&#27492;&#36807;&#31243;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-k&#27169;&#22411;&#26356;&#27880;&#37325;&#25554;&#20837;&#20462;&#25913;&#22120;&#65292;&#32780;&#20154;&#31867;&#20542;&#21521;&#20110;&#26367;&#25442;&#21333;&#35789;&#21644;&#30701;&#35821;&#65292;&#21253;&#25324;&#23545;&#20027;&#39064;&#30340;&#26356;&#25913;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-k&#22312;&#35843;&#25972;&#20462;&#25913;&#22120;&#26041;&#38754;&#27604;&#36739;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of text-to-image (T2I) generation has garnered significant attention both within the research community and among everyday users. Despite the advancements of T2I models, a common issue encountered by users is the need for repetitive editing of input prompts in order to receive a satisfactory image, which is time-consuming and labor-intensive. Given the demonstrated text generation power of large-scale language models, such as GPT-k, we investigate the potential of utilizing such models to improve the prompt editing process for T2I generation. We conduct a series of experiments to compare the common edits made by humans and GPT-k, evaluate the performance of GPT-k in prompting T2I, and examine factors that may influence this process. We found that GPT-k models focus more on inserting modifiers while humans tend to replace words and phrases, which includes changes to the subject matter. Experimental results show that GPT-k are more effective in adjusting modifiers rather than p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GeoNorm&#30340;&#22320;&#29702;&#32534;&#30721;&#26694;&#26550;&#12290;&#36890;&#36807;&#29983;&#25104;&#21644;&#37325;&#26032;&#25490;&#24207;&#20004;&#27425;&#35299;&#26512;&#22320;&#21517;&#65292;GeoNorm&#22312;&#35299;&#26512;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;&#20854;&#20013;&#25913;&#36827;&#30340;&#20505;&#36873;&#29983;&#25104;&#21644;&#22522;&#20110;Transformer&#30340;&#37325;&#26032;&#25490;&#24207;&#26159;&#21019;&#26032;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2305.11315</link><description>&lt;p&gt;
&#36890;&#36807;&#25913;&#36827;&#20505;&#36873;&#29983;&#25104;&#65292;&#22522;&#20110;Transformer&#30340;&#37325;&#26032;&#25490;&#24207;&#21644;&#20004;&#38454;&#27573;&#35299;&#26512;&#26041;&#27861;&#26469;&#25552;&#39640;&#22320;&#21517;&#35299;&#26512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Toponym Resolution with Better Candidate Generation, Transformer-based Reranking, and Two-Stage Resolution. (arXiv:2305.11315v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GeoNorm&#30340;&#22320;&#29702;&#32534;&#30721;&#26694;&#26550;&#12290;&#36890;&#36807;&#29983;&#25104;&#21644;&#37325;&#26032;&#25490;&#24207;&#20004;&#27425;&#35299;&#26512;&#22320;&#21517;&#65292;GeoNorm&#22312;&#35299;&#26512;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;&#20854;&#20013;&#25913;&#36827;&#30340;&#20505;&#36873;&#29983;&#25104;&#21644;&#22522;&#20110;Transformer&#30340;&#37325;&#26032;&#25490;&#24207;&#26159;&#21019;&#26032;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#32534;&#30721;&#26159;&#23558;&#25991;&#26412;&#20013;&#30340;&#20301;&#32622;&#25552;&#21450;&#36716;&#25442;&#20026;&#32534;&#30721;&#22320;&#29702;&#31354;&#38388;&#35821;&#20041;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22320;&#29702;&#32534;&#30721;&#26550;&#26500;GeoNorm&#12290;GeoNorm&#39318;&#20808;&#20351;&#29992;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#20174;&#22320;&#29702;&#26412;&#20307;&#20013;&#29983;&#25104;&#20505;&#36873;&#26465;&#30446;&#21015;&#34920;&#65292;&#28982;&#21518;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#20174;&#26412;&#20307;&#20013;&#33719;&#21462;&#20449;&#24687;(&#22914;&#26465;&#30446;&#20154;&#21475;)&#23545;&#20505;&#36873;&#26465;&#30446;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;&#36825;&#20010;&#29983;&#25104;&#21644;&#37325;&#26032;&#25490;&#24207;&#36807;&#31243;&#24212;&#29992;&#20102;&#20004;&#27425;:&#39318;&#20808;&#35299;&#20915;&#19981;&#22826;&#21547;&#31946;&#30340;&#22269;&#23478;&#65292;&#24030;&#21644;&#21439;&#65292;&#28982;&#21518;&#35299;&#20915;&#20854;&#20313;&#30340;&#20301;&#32622;&#25552;&#21450;&#65292;&#20351;&#29992;&#24050;&#30830;&#23450;&#30340;&#22269;&#23478;&#12289;&#24030;&#21644;&#21439;&#32423;&#20449;&#24687;&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22320;&#21517;&#35299;&#26512;&#26694;&#26550;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21644;&#27169;&#22411;&#21487;&#22312; \url{https://github.com/clulab/geonorm}&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geocoding is the task of converting location mentions in text into structured data that encodes the geospatial semantics. We propose a new architecture for geocoding, GeoNorm. GeoNorm first uses information retrieval techniques to generate a list of candidate entries from the geospatial ontology. Then it reranks the candidate entries using a transformer-based neural network that incorporates information from the ontology such as the entry's population. This generate-and-rerank process is applied twice: first to resolve the less ambiguous countries, states, and counties, and second to resolve the remaining location mentions, using the identified countries, states, and counties as context. Our proposed toponym resolution framework achieves state-of-the-art performance on multiple datasets. Code and models are available at \url{https://github.com/clulab/geonorm}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#35745;&#21010;&#33719;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#20016;&#23500;&#30340;&#24863;&#30693;&#21644;&#23545;&#35805;&#21382;&#21490;&#65292;&#35753;&#20195;&#29702;&#20154;&#39044;&#27979;&#20182;&#20204;&#33258;&#24049;&#21644;&#21512;&#20316;&#20249;&#20276;&#32570;&#22833;&#30340;&#20219;&#21153;&#30693;&#35782;&#65292;&#23454;&#29616;&#32852;&#21512;&#20219;&#21153;&#30340;&#23436;&#25972;&#35745;&#21010;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2305.11271</link><description>&lt;p&gt;
&#38754;&#21521;&#24773;&#22659;&#23545;&#35805;&#20013;&#30340;&#24515;&#26234;&#24314;&#27169;&#65292;&#23454;&#29616;&#21327;&#21516;&#35745;&#21010;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Towards Collaborative Plan Acquisition through Theory of Mind Modeling in Situated Dialogue. (arXiv:2305.11271v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#35745;&#21010;&#33719;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#20016;&#23500;&#30340;&#24863;&#30693;&#21644;&#23545;&#35805;&#21382;&#21490;&#65292;&#35753;&#20195;&#29702;&#20154;&#39044;&#27979;&#20182;&#20204;&#33258;&#24049;&#21644;&#21512;&#20316;&#20249;&#20276;&#32570;&#22833;&#30340;&#20219;&#21153;&#30693;&#35782;&#65292;&#23454;&#29616;&#32852;&#21512;&#20219;&#21153;&#30340;&#23436;&#25972;&#35745;&#21010;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#20219;&#21153;&#36890;&#24120;&#22987;&#20110;&#21452;&#26041;&#25317;&#26377;&#19981;&#23436;&#20840;&#30340;&#20219;&#21153;&#30693;&#35782;&#21644;&#19981;&#23436;&#25972;&#30340;&#21021;&#22987;&#35745;&#21010;&#12290;&#20026;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#65292;&#20195;&#29702;&#20154;&#38656;&#35201;&#19982;&#21512;&#20316;&#20249;&#20276;&#36827;&#34892;&#23454;&#22320;&#20132;&#27969;&#65292;&#24182;&#21327;&#35843;&#20182;&#20204;&#30340;&#37096;&#20998;&#35745;&#21010;&#20197;&#23454;&#29616;&#32852;&#21512;&#20219;&#21153;&#30446;&#26631;&#12290;&#34429;&#28982;&#36825;&#31181;&#21327;&#20316;&#22312;&#20154;&#19982;&#20154;&#30340;&#22242;&#38431;&#20013;&#20284;&#20046;&#36731;&#32780;&#26131;&#20030;&#65292;&#20294;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21327;&#20316;&#26469;&#35828;&#21364;&#20855;&#26377;&#24456;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#35745;&#21010;&#33719;&#21462;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20154;&#31867;&#21644;&#20195;&#29702;&#20154;&#21162;&#21147;&#23398;&#20064;&#24182;&#30456;&#20114;&#20132;&#27969;&#65292;&#20197;&#33719;&#21462;&#32852;&#21512;&#20219;&#21153;&#30340;&#23436;&#25972;&#35745;&#21010;&#12290;&#20855;&#20307;&#22320;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#35753;&#20195;&#29702;&#20154;&#22522;&#20110;&#20016;&#23500;&#30340;&#24863;&#30693;&#21644;&#23545;&#35805;&#21382;&#21490;&#65292;&#39044;&#27979;&#20182;&#20204;&#33258;&#24049;&#21644;&#21512;&#20316;&#20249;&#20276;&#32570;&#22833;&#30340;&#20219;&#21153;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#19977;&#32500;&#26041;&#22359;&#19990;&#30028;&#30340;&#23545;&#31216;&#21327;&#20316;&#20219;&#21153;&#20013;&#25193;&#23637;&#20102;&#19968;&#20010;&#24773;&#22659;&#23545;&#35805;&#22522;&#20934;&#65292;&#24182;&#30740;&#31350;&#20102;&#35745;&#21010;&#33719;&#21462;&#30340;&#35745;&#31639;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#27979;&#20219;&#21153;&#30693;&#35782;&#26159;&#35745;&#21010;&#33719;&#21462;&#36807;&#31243;&#20013;&#30340;&#37325;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative tasks often begin with partial task knowledge and incomplete initial plans from each partner. To complete these tasks, agents need to engage in situated communication with their partners and coordinate their partial plans towards a complete plan to achieve a joint task goal. While such collaboration seems effortless in a human-human team, it is highly challenging for human-AI collaboration. To address this limitation, this paper takes a step towards collaborative plan acquisition, where humans and agents strive to learn and communicate with each other to acquire a complete plan for joint tasks. Specifically, we formulate a novel problem for agents to predict the missing task knowledge for themselves and for their partners based on rich perceptual and dialogue history. We extend a situated dialogue benchmark for symmetric collaborative tasks in a 3D blocks world and investigate computational strategies for plan acquisition. Our empirical results suggest that predicting the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20013;&#25991;&#25968;&#25454;&#38598;&#65292;CHBias&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#32531;&#35299;&#20013;&#25991;&#23545;&#35805;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#20013;&#25991;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#21547;&#26377;&#20559;&#35265;&#30340;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.11262</link><description>&lt;p&gt;
CHBias: &#20013;&#25991;&#23545;&#35805;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#24046;&#35780;&#20272;&#19982;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models. (arXiv:2305.11262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20013;&#25991;&#25968;&#25454;&#38598;&#65292;CHBias&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#32531;&#35299;&#20013;&#25991;&#23545;&#35805;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#20013;&#25991;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#21547;&#26377;&#20559;&#35265;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#23545;&#35805;&#20195;&#29702;&#24050;&#34987;&#26292;&#38706;&#20986;&#23433;&#20840;&#38382;&#39064;&#65292;&#34920;&#29616;&#20986;&#19968;&#31995;&#21015;&#21051;&#26495;&#21270;&#30340;&#20154;&#31867;&#20559;&#35265;&#65292;&#20363;&#22914;&#24615;&#21035;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#22312;&#24403;&#21069;&#30740;&#31350;&#20013;&#20173;&#23384;&#22312;&#26377;&#38480;&#30340;&#20559;&#35265;&#31867;&#21035;&#65292;&#32780;&#19988;&#22823;&#37096;&#20998;&#21482;&#38024;&#23545;&#33521;&#25991;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20013;&#25991;&#25968;&#25454;&#38598; CHBias&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#32531;&#35299;&#20013;&#25991;&#23545;&#35805;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#38500;&#20102;&#20043;&#21069;&#24050;&#32463;&#28145;&#20837;&#30740;&#31350;&#30340;&#20559;&#35265;&#31867;&#21035;&#22806;&#65292;CHBias&#21253;&#25324;&#20102;&#19968;&#20123;&#19981;&#22826;&#21463;&#20851;&#27880;&#30340;&#20559;&#35265;&#31867;&#21035;&#65292;&#20363;&#22914;&#24180;&#40836;&#27495;&#35270;&#21644;&#22806;&#35980;&#20559;&#35265;&#12290;&#25105;&#20204;&#21033;&#29992; CHBias &#35780;&#20272;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;&#20013;&#25991;&#39044;&#35757;&#32451;&#23545;&#35805;&#27169;&#22411; CDial-GPT &#21644; EVA2.0&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#32531;&#35299;&#19981;&#21516;&#30340;&#20559;&#35265;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20960;&#31181;&#21435;&#20559;&#26041;&#27861;&#26469;&#22788;&#29702;&#20013;&#25991;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#20013;&#25991;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#21547;&#26377;&#20559;&#35265;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{\textbf{\textcolor{red}{Warning}:} This paper contains content that may be offensive or upsetting.} Pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias. However, there are still limited bias categories in current research, and most of them only focus on English. In this paper, we introduce a new Chinese dataset, CHBias, for bias evaluation and mitigation of Chinese conversational language models. Apart from those previous well-explored bias categories, CHBias includes under-explored bias categories, such as ageism and appearance biases, which received less attention. We evaluate two popular pretrained Chinese conversational models, CDial-GPT and EVA2.0, using CHBias. Furthermore, to mitigate different biases, we apply several debiasing methods to the Chinese pretrained models. Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#32034;&#24341;&#30340;&#38544;&#24335;&#24773;&#24863;&#25512;&#26029;&#26694;&#26550;&#65288;THOR&#65289;&#65292;&#36890;&#36807;&#19977;&#27425;&#36339;&#25512;&#29702;&#27169;&#20223;&#31867;&#20154;&#25512;&#29702;&#36807;&#31243;&#65292;&#25903;&#25345;&#24120;&#35782;&#21644;&#22810;&#36339;&#25512;&#29702;&#20197;&#25512;&#26029;&#24847;&#35265;&#30340;&#28508;&#22312;&#24847;&#22270;&#65292;&#24182;&#36880;&#27493;&#35825;&#23548;&#38544;&#24335;&#26041;&#38754;&#12289;&#24847;&#35265;&#21644;&#26368;&#32456;&#24773;&#24863;&#26497;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#19978;&#22823;&#24133;&#25552;&#39640;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.11255</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#38142;&#32034;&#24341;&#30340;&#38544;&#24335;&#24773;&#24863;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Reasoning Implicit Sentiment with Chain-of-Thought Prompting. (arXiv:2305.11255v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#32034;&#24341;&#30340;&#38544;&#24335;&#24773;&#24863;&#25512;&#26029;&#26694;&#26550;&#65288;THOR&#65289;&#65292;&#36890;&#36807;&#19977;&#27425;&#36339;&#25512;&#29702;&#27169;&#20223;&#31867;&#20154;&#25512;&#29702;&#36807;&#31243;&#65292;&#25903;&#25345;&#24120;&#35782;&#21644;&#22810;&#36339;&#25512;&#29702;&#20197;&#25512;&#26029;&#24847;&#35265;&#30340;&#28508;&#22312;&#24847;&#22270;&#65292;&#24182;&#36880;&#27493;&#35825;&#23548;&#38544;&#24335;&#26041;&#38754;&#12289;&#24847;&#35265;&#21644;&#26368;&#32456;&#24773;&#24863;&#26497;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#19978;&#22823;&#24133;&#25552;&#39640;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#31995;&#32479;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#20851;&#38190;&#35266;&#28857;&#34920;&#36798;&#26469;&#30830;&#23450;&#32473;&#23450;&#30446;&#26631;&#30340;&#24773;&#24863;&#26497;&#24615;&#65292;&#32780;&#22312;&#38544;&#24335;&#24773;&#24863;&#20998;&#26512;&#65288;ISA&#65289;&#20013;&#65292;&#35266;&#28857;&#25552;&#31034;&#20197;&#19968;&#31181;&#38544;&#21547;&#21644;&#27169;&#31946;&#30340;&#26041;&#24335;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#38544;&#24335;&#24773;&#24863;&#38656;&#35201;&#24120;&#35782;&#21644;&#22810;&#36339;&#25512;&#29702;&#33021;&#21147;&#26469;&#25512;&#26029;&#24847;&#35265;&#30340;&#28508;&#22312;&#24847;&#22270;&#12290;&#21463;&#26368;&#36817;&#24605;&#32500;&#38142;&#32034;&#24341;&#65288;CoT&#65289;&#24605;&#24819;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#19977;&#27425;&#36339;&#25512;&#29702;&#65288;THOR&#65289;CoT&#26694;&#26550;&#65292;&#27169;&#20223;ISA&#30340;&#31867;&#20154;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#20026;THOR&#35774;&#35745;&#20102;&#19968;&#20010;&#19977;&#27493;&#25552;&#31034;&#21407;&#21017;&#65292;&#20197;&#36880;&#27493;&#35825;&#23548;&#38544;&#24335;&#26041;&#38754;&#12289;&#24847;&#35265;&#21644;&#26368;&#32456;&#24773;&#24863;&#26497;&#24615;&#12290;&#25105;&#20204;&#30340;THOR+Flan-T5&#65288;11B&#65289;&#22312;&#30417;&#30563;&#35774;&#32622;&#19978;&#23558;&#25216;&#26415;&#27700;&#24179;&#25512;&#36827;&#20102;&#36229;&#36807;6&#65285;&#30340;F1&#20540;&#12290;&#26356;&#20026;&#26174;&#33879;&#30340;&#26159;&#65292;THOR+GPT3&#65288;175B&#65289;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19978;&#23558;&#25216;&#26415;&#27700;&#24179;&#25552;&#21319;&#20102;&#36229;&#36807;50&#65285;&#30340;F1&#20540;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20301;&#20110;https://github.com/scofield7419/THOR-ISA &#12290;
&lt;/p&gt;
&lt;p&gt;
While sentiment analysis systems try to determine the sentiment polarities of given targets based on the key opinion expressions in input texts, in implicit sentiment analysis (ISA) the opinion cues come in an implicit and obscure manner. Thus detecting implicit sentiment requires the common-sense and multi-hop reasoning ability to infer the latent intent of opinion. Inspired by the recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop Reasoning (THOR) CoT framework to mimic the human-like reasoning process for ISA. We design a three-step prompting principle for THOR to step-by-step induce the implicit aspect, opinion, and finally the sentiment polarity. Our THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on supervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50% F1 on zero-shot setting. Our code is at https://github.com/scofield7419/THOR-ISA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#22810;&#31181;&#31639;&#27861;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#25991;&#26412;&#20043;&#38388;&#30340;&#20027;&#39064;&#30456;&#20284;&#24615;&#65292;&#21457;&#29616;&#22312;&#37319;&#29992;&#8220;&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#8221;&#25991;&#26412;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#8220;&#20313;&#24358;&#30456;&#20284;&#24230;&#8221;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#30340;&#32452;&#21512;&#19979;&#65292;&#25991;&#26412;&#32858;&#31867;&#30340;&#25928;&#26524;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.11251</link><description>&lt;p&gt;
&#35745;&#31639;&#20027;&#39064;&#23398;&#65306;&#27604;&#36739;&#25991;&#23398;&#23567;&#35828;&#26679;&#26412;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Computational thematics: Comparing algorithms for clustering the genres of literary fiction. (arXiv:2305.11251v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#22810;&#31181;&#31639;&#27861;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#25991;&#26412;&#20043;&#38388;&#30340;&#20027;&#39064;&#30456;&#20284;&#24615;&#65292;&#21457;&#29616;&#22312;&#37319;&#29992;&#8220;&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#8221;&#25991;&#26412;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#8220;&#20313;&#24358;&#30456;&#20284;&#24230;&#8221;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#30340;&#32452;&#21512;&#19979;&#65292;&#25991;&#26412;&#32858;&#31867;&#30340;&#25928;&#26524;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#25429;&#25417;&#25991;&#23398;&#25991;&#26412;&#20043;&#38388;&#30340;&#20027;&#39064;&#30456;&#20284;&#24615;&#65311; &#26412;&#25991;&#23545;&#25991;&#26412;&#30340;&#21069;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#29305;&#24449;&#21015;&#34920;&#20043;&#38388;&#30340;&#36317;&#31163;&#27979;&#37327;&#31561;&#19977;&#20010;&#27493;&#39588;&#30340;&#21508;&#31181;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#23398;&#20064;&#35745;&#31639;&#20027;&#39064;&#20998;&#26512;&#30340;&#26368;&#20339;&#32452;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;&#8220;&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#8221;&#65288;TF-IDF&#65289;&#25991;&#26412;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#8220;&#20313;&#24358;&#30456;&#20284;&#24230;&#8221;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#30340;&#32452;&#21512;&#19979;&#65292;&#25991;&#26412;&#32858;&#31867;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#24471;&#20197;&#23454;&#29616;&#12290;&#26412;&#25991;&#30340;&#30740;&#31350;&#32467;&#26524;&#20026;&#20174;&#20107;&#35745;&#31639;&#20027;&#39064;&#20998;&#26512;&#30340;&#20154;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#21442;&#32771;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
What are the best methods of capturing thematic similarity between literary texts? Knowing the answer to this question would be useful for automatic clustering of book genres, or any other thematic grouping. This paper compares a variety of algorithms for unsupervised learning of thematic similarities between texts, which we call "computational thematics". These algorithms belong to three steps of analysis: text preprocessing, extraction of text features, and measuring distances between the lists of features. Each of these steps includes a variety of options. We test all the possible combinations of these options: every combination of algorithms is given a task to cluster a corpus of books belonging to four pre-tagged genres of fiction. This clustering is then validated against the "ground truth" genre labels. Such comparison of algorithms allows us to learn the best and the worst combinations for computational thematic analysis. To illustrate the sharp difference between the best and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27531;&#24046;&#36866;&#37197;&#22120;&#21644;&#27169;&#22411;&#37325;&#32534;&#31243;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35760;&#21495;&#30340;&#26631;&#31614;&#26144;&#23556;&#65292;&#24182;&#22312;ADI-17&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;PEL&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.11244</link><description>&lt;p&gt;
&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24102;&#26377;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#30340;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model. (arXiv:2305.11244v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27531;&#24046;&#36866;&#37197;&#22120;&#21644;&#27169;&#22411;&#37325;&#32534;&#31243;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35760;&#21495;&#30340;&#26631;&#31614;&#26144;&#23556;&#65292;&#24182;&#22312;ADI-17&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;PEL&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#65288;PEL&#65289;&#25216;&#26415;&#65292;&#20197;&#37325;&#26032;&#21033;&#29992;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#65288;GSM&#65289;&#36827;&#34892;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#65288;ADI&#65289;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35760;&#21495;&#30340;&#26631;&#31614;&#26144;&#23556;&#65292;&#23558;GSM&#36866;&#24212;&#20110;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#65292;&#36890;&#36807;&#27531;&#24046;&#36866;&#37197;&#22120;&#21644;&#27169;&#22411;&#37325;&#32534;&#31243;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;vanilla fine-tuning&#22312;ADI-17&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#39640;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;PEL&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#65292;&#20351;&#29992;&#39069;&#22806;2.5&#65285;&#30340;&#32593;&#32476;&#21487;&#35757;&#32451;&#21442;&#25968;&#21363;&#21487;&#36798;&#21040;fine-tuning&#31934;&#24230;&#30340;1.86&#65285;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#21644;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#35782;&#21035;&#38463;&#25289;&#20271;&#26041;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore Parameter-Efficient-Learning (PEL) techniques to repurpose a General-Purpose-Speech (GSM) model for Arabic dialect identification (ADI). Specifically, we investigate different setups to incorporate trainable features into a multi-layer encoder-decoder GSM formulation under frozen pre-trained settings. Our architecture includes residual adapter and model reprogramming (input-prompting). We design a token-level label mapping to condition the GSM for Arabic Dialect Identification (ADI). This is challenging due to the high variation in vocabulary and pronunciation among the numerous regional dialects. We achieve new state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We further reduce the training budgets with the PEL method, which performs within 1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable parameters. Our study demonstrates how to identify Arabic dialects using a small dataset and limited computation with open sou
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20799;&#31461;&#21457;&#23637;&#23454;&#39564;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#21516;&#26102;&#27604;&#36739;LLMs&#21644;&#20799;&#31461;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#24320;&#21457;&#26356;&#20855;&#20154;&#31867;&#29305;&#24449;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11243</link><description>&lt;p&gt;
&#27604;&#36739;&#26426;&#22120;&#21644;&#20799;&#31461;&#65306;&#20351;&#29992;&#21457;&#23637;&#24515;&#29702;&#23398;&#23454;&#39564;&#35780;&#20272;LaMDA&#21709;&#24212;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;
&lt;/p&gt;
&lt;p&gt;
Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses. (arXiv:2305.11243v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11243
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20799;&#31461;&#21457;&#23637;&#23454;&#39564;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#21516;&#26102;&#27604;&#36739;LLMs&#21644;&#20799;&#31461;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#24320;&#21457;&#26356;&#20855;&#20154;&#31867;&#29305;&#24449;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#24515;&#29702;&#23398;&#23478;&#33457;&#36153;&#20102;&#20960;&#21313;&#24180;&#30340;&#26102;&#38388;&#35774;&#35745;&#23454;&#39564;&#26469;&#27979;&#35797;&#23156;&#20799;&#21644;&#20799;&#31461;&#30340;&#26234;&#21147;&#21644;&#30693;&#35782;&#65292;&#36861;&#28335;&#37325;&#35201;&#27010;&#24565;&#21644;&#33021;&#21147;&#30340;&#36215;&#28304;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20351;&#29992;&#20799;&#31461;&#21457;&#23637;&#30340;&#32463;&#20856;&#23454;&#39564;&#26159;&#25506;&#31350;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;LLM&#27169;&#22411;&#30340;&#26368;&#26377;&#25928;&#30340;&#26041;&#24335;&#20043;&#19968;&#12290;&#20854;&#27425;&#65292;&#23558;LLM&#19982;&#20799;&#31461;&#36827;&#34892;&#27604;&#36739;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#24320;&#21457;&#26356;&#20855;&#26377;&#20154;&#31867;&#29305;&#28857;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#23884;&#20837;&#21040;&#38656;&#35201;&#19982;&#20154;&#20132;&#20114;&#30340;&#23454;&#38469;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developmental psychologists have spent decades devising experiments to test the intelligence and knowledge of infants and children, tracing the origin of crucial concepts and capacities. Moreover, experimental techniques in developmental psychology have been carefully designed to discriminate the cognitive capacities that underlie particular behaviors. We propose that using classical experiments from child development is a particularly effective way to probe the computational abilities of AI models, in general, and LLMs in particular. First, the methodological techniques of developmental psychology, such as the use of novel stimuli to control for past experience or control conditions to determine whether children are using simple associations, can be equally helpful for assessing the capacities of LLMs. In parallel, testing LLMs in this way can tell us whether the information that is encoded in text is sufficient to enable particular responses, or whether those responses depend on othe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#24847;&#22823;&#21033;&#35821;&#12289;&#20013;&#25991;&#12289;&#33521;&#35821;&#12289;&#24076;&#20271;&#26469;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#20559;&#35265;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#65292;&#21516;&#26102;&#35843;&#26597;&#20102;&#22810;&#35821;&#35328;&#19982;&#21333;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.11242</link><description>&lt;p&gt;
&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#27604;&#36739;&#20559;&#35265;&#21450;&#22810;&#35821;&#35328;&#35757;&#32451;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Comparing Biases and the Impact of Multilingual Training across Multiple Languages. (arXiv:2305.11242v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#24847;&#22823;&#21033;&#35821;&#12289;&#20013;&#25991;&#12289;&#33521;&#35821;&#12289;&#24076;&#20271;&#26469;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#20559;&#35265;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#65292;&#21516;&#26102;&#35843;&#26597;&#20102;&#22810;&#35821;&#35328;&#19982;&#21333;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26377;&#20851;&#20559;&#35265;&#21644;&#20844;&#24179;&#30340;&#30740;&#31350;&#20027;&#35201;&#32771;&#23519;&#21333;&#20010;&#35821;&#35328;&#20869;&#21644;/&#25110;&#36328;&#23569;&#25968;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#65292;&#31181;&#26063;&#65289;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21333;&#20010;&#23646;&#24615;&#65292;&#20559;&#35265;&#21487;&#33021;&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#26041;&#24335;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#26816;&#26597;&#27599;&#31181;&#35821;&#35328;&#21644;&#23646;&#24615;&#20869;&#37096;&#30340;&#20559;&#35265;&#12290;&#21516;&#26679;&#37325;&#35201;&#30340;&#26159;&#30740;&#31350;&#36825;&#20123;&#20559;&#35265;&#22312;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#24322;&#20197;&#21450;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#21644;&#21333;&#35821;&#35328;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#26102;&#36825;&#20123;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#24847;&#22823;&#21033;&#35821;&#12289;&#20013;&#25991;&#12289;&#33521;&#35821;&#12289;&#24076;&#20271;&#26469;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20043;&#38388;&#23545;&#19979;&#28216;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#36827;&#34892;&#20102;&#20559;&#35265;&#20998;&#26512;&#65292;&#20197;&#35266;&#23519;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#26159;&#21542;&#34987;&#26356;&#31215;&#26497;&#22320;&#30475;&#24453;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#35821;&#35328;&#20043;&#38388;&#30340;&#20559;&#35265;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#65292;&#24182;&#35843;&#26597;&#20102;&#22810;&#35821;&#35328;&#19982;&#21333;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#33521;&#25991;&#24773;&#24863;&#20559;&#35265;&#27169;&#26495;&#36866;&#24212;&#21040;&#24847;&#22823;&#21033;&#35821;&#12289;&#20013;&#25991;&#12289;&#24076;&#20271;&#26469;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20013;&#65292;&#21253;&#25324;&#22235;&#20010;&#23646;&#24615;&#65306;&#31181;&#26063;&#12289;&#23447;&#25945;&#12289;&#22269;&#31821;&#21644;&#24615;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studies in bias and fairness in natural language processing have primarily examined social biases within a single language and/or across few attributes (e.g. gender, race). However, biases can manifest differently across various languages for individual attributes. As a result, it is critical to examine biases within each language and attribute. Of equal importance is to study how these biases compare across languages and how the biases are affected when training a model on multilingual data versus monolingual data. We present a bias analysis across Italian, Chinese, English, Hebrew, and Spanish on the downstream sentiment analysis task to observe whether specific demographics are viewed more positively. We study bias similarities and differences across these languages and investigate the impact of multilingual vs. monolingual training data. We adapt existing sentiment bias templates in English to Italian, Chinese, Hebrew, and Spanish for four attributes: race, religion, nationality, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26080;&#30417;&#30563;&#25688;&#35201;&#30340;&#26368;&#26032;&#25216;&#26415;&#21644;&#27169;&#22411;&#65292;&#21253;&#25324;&#25277;&#21462;&#24335;&#12289;&#29983;&#25104;&#24335;&#21644;&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#19968;&#20123;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11231</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#25688;&#35201;&#30340;&#26368;&#26032;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Recent Trends in Unsupervised Summarization. (arXiv:2305.11231v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26080;&#30417;&#30563;&#25688;&#35201;&#30340;&#26368;&#26032;&#25216;&#26415;&#21644;&#27169;&#22411;&#65292;&#21253;&#25324;&#25277;&#21462;&#24335;&#12289;&#29983;&#25104;&#24335;&#21644;&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#19968;&#20123;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#25688;&#35201;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#25688;&#35201;&#27169;&#22411;&#12290;&#26412;&#32508;&#36848;&#28085;&#30422;&#20102;&#29992;&#20110;&#26080;&#30417;&#30563;&#25688;&#35201;&#30340;&#19981;&#21516;&#25216;&#26415;&#21644;&#27169;&#22411;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#25277;&#21462;&#24335;&#12289;&#29983;&#25104;&#24335;&#21644;&#28151;&#21512;&#27169;&#22411;&#20197;&#21450;&#29992;&#20110;&#23454;&#29616;&#26080;&#30417;&#30563;&#25688;&#35201;&#30340;&#31574;&#30053;&#12290;&#23613;&#31649;&#26412;&#32508;&#36848;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#26368;&#26032;&#30740;&#31350;&#65292;&#20294;&#25105;&#20204;&#20063;&#20171;&#32461;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#26681;&#25454;&#30740;&#31350;&#23545;&#26080;&#30417;&#30563;&#35757;&#32451;&#30340;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#21040;&#20102;&#19968;&#20123;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised summarization is a powerful technique that enables training summarizing models without requiring labeled datasets. This survey covers different recent techniques and models used for unsupervised summarization. We cover extractive, abstractive, and hybrid models and strategies used to achieve unsupervised summarization. While the main focus of this survey is on recent research, we also cover some of the important previous research. We additionally introduce a taxonomy, classifying different research based on their approach to unsupervised training. Finally, we discuss the current approaches and mention some datasets and evaluation methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#22768;&#35843;&#23398;&#20064;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26631;&#20934;&#30417;&#30563;&#25439;&#22833;&#24494;&#35843;&#30340;&#26041;&#27861;&#65288;&#19981;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25110;&#20154;&#31867;&#27169;&#22411;&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#20063;&#26377;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11206</link><description>&lt;p&gt;
LIMA: &#23545;&#40784;&#30340;&#26356;&#23569;&#21363;&#20026;&#26356;&#20248;&#65288;Less Is More for Alignment&#65289;
&lt;/p&gt;
&lt;p&gt;
LIMA: Less Is More for Alignment. (arXiv:2305.11206v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11206
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#22768;&#35843;&#23398;&#20064;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26631;&#20934;&#30417;&#30563;&#25439;&#22833;&#24494;&#35843;&#30340;&#26041;&#27861;&#65288;&#19981;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25110;&#20154;&#31867;&#27169;&#22411;&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#20063;&#26377;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36890;&#24120;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;(1)&#26080;&#30417;&#30563;&#30340;&#21407;&#22987;&#25991;&#26412;&#39044;&#35757;&#32451;&#65292;&#20197;&#23398;&#20064;&#36890;&#29992;&#34920;&#31034;&#65307;(2)&#22823;&#35268;&#27169;&#30340;&#25351;&#20196;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#26368;&#32456;&#20219;&#21153;&#21644;&#29992;&#25143;&#20559;&#22909;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;LIMA&#65292;&#19968;&#20010;&#20351;&#29992;&#26631;&#20934;&#30417;&#30563;&#25439;&#22833;&#20540;&#36827;&#34892;&#30340;65B&#21442;&#25968;LLaMa&#35821;&#35328;&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;1000&#20010;&#32463;&#36807;&#31579;&#36873;&#30340;&#25552;&#31034;&#21644;&#22238;&#22797;&#36827;&#34892;&#24494;&#35843;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#24378;&#21270;&#23398;&#20064;&#25110;&#20154;&#31867;&#20559;&#22909;&#24314;&#27169;&#65292;&#34913;&#37327;&#20102;&#36825;&#20004;&#20010;&#38454;&#27573;&#20043;&#38388;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290; LIMA&#34920;&#29616;&#20986;&#20102;&#26497;&#24378;&#30340;&#24615;&#33021;&#65292;&#20165;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#21040;&#22914;&#20309;&#36981;&#24490;&#29305;&#23450;&#30340;&#21709;&#24212;&#26684;&#24335;&#65292;&#21253;&#25324;&#20174;&#35268;&#21010;&#26053;&#34892;&#34892;&#31243;&#21040;&#25512;&#27979;&#26367;&#20195;&#21382;&#21490;&#30340;&#22797;&#26434;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#20542;&#21521;&#20110;&#33391;&#22909;&#22320;&#25512;&#24191;&#21040;&#26410;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#20219;&#21153;&#20013;&#12290;&#22312;&#19968;&#39033;&#25511;&#21046;&#30340;&#20154;&#31867;&#30740;&#31350;&#20013;&#65292;&#19982;GPT-4&#30456;&#27604;&#65292;LIMA&#30340;&#21709;&#24212;&#22312;43%&#30340;&#24773;&#20917;&#19979;&#31561;&#25928;&#25110;&#20005;&#26684;&#20248;&#20808;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#36716;&#31227;&#25552;&#31034;&#26469;&#20248;&#21270;&#21387;&#32553;&#30340;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#31934;&#24230;&#26356;&#39640;&#30340;&#25552;&#31034;&#26174;&#33879;&#25552;&#39640;&#20102;&#21387;&#32553;&#30340;LLM&#22312;&#29305;&#23450;&#26597;&#35810;&#26041;&#38754;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#23454;&#29616;&#20102;4&#20493;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2305.11186</link><description>&lt;p&gt;
&#21387;&#32553;&#65292;&#28982;&#21518;&#25552;&#31034;&#65306;&#20351;&#29992;&#21487;&#36716;&#31227;&#25552;&#31034;&#26469;&#25913;&#21892;LLM&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt. (arXiv:2305.11186v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#36716;&#31227;&#25552;&#31034;&#26469;&#20248;&#21270;&#21387;&#32553;&#30340;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#31934;&#24230;&#26356;&#39640;&#30340;&#25552;&#31034;&#26174;&#33879;&#25552;&#39640;&#20102;&#21387;&#32553;&#30340;LLM&#22312;&#29305;&#23450;&#26597;&#35810;&#26041;&#38754;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#23454;&#29616;&#20102;4&#20493;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#25968;&#21313;&#20159;&#30340;&#21442;&#25968;&#65292;&#34920;&#29616;&#20986;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20250;&#24102;&#26469;&#26174;&#30528;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#24120;&#35265;&#30340;&#30828;&#20214;&#19978;&#37096;&#32626;&#65288;&#20363;&#22914;&#21333;&#20010;GPU&#65289;&#26102;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#21387;&#32553;&#26469;&#26368;&#23567;&#21270;LLM&#25512;&#29702;&#30340;&#24310;&#36831;&#65292;&#21363;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20294;&#26159;&#65292;&#27492;&#36807;&#31243;&#24517;&#28982;&#24341;&#21457;&#25928;&#29575;&#21644;&#31934;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#22240;&#20026;&#21387;&#32553;&#30340;LLMs&#36890;&#24120;&#20250;&#32463;&#21382;&#39044;&#27979;&#31934;&#24230;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#35270;&#35282;&#65306;&#20026;&#20102;&#20248;&#21270;&#36825;&#31181;&#24179;&#34913;&#65292;&#21387;&#32553;&#30340;LLMs&#38656;&#35201;&#19968;&#31181;&#19981;&#21516;&#20110;&#21407;&#22987;&#27169;&#22411;&#30340;&#29420;&#29305;&#36755;&#20837;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#31934;&#24230;&#30340;&#25552;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#21387;&#32553;&#30340;LLM&#22312;&#29305;&#23450;&#26597;&#35810;&#26041;&#38754;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#36716;&#31227;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#26377;&#25928;&#25552;&#31034;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36755;&#20986;&#65292;&#24182;&#23454;&#29616;&#20102;4&#20493;&#36895;&#24230;&#30340;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), armed with billions of parameters, exhibit exceptional performance across a wide range of Natural Language Processing (NLP) tasks. However, they present a significant computational challenge during inference, especially when deploying on common hardware such as single GPUs. As such, minimizing the latency of LLM inference by curtailing computational and memory requirements, though achieved through compression, becomes critically important. However, this process inevitably instigates a trade-off between efficiency and accuracy, as compressed LLMs typically experience a reduction in predictive precision. In this research, we introduce an innovative perspective: to optimize this trade-off, compressed LLMs require a unique input format that varies from that of the original models. Our findings indicate that the generation quality in a compressed LLM can be markedly improved for specific queries by selecting prompts with precision. Capitalizing on this insight,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;TaxBox&#65292;&#35813;&#26041;&#27861;&#23558;&#20998;&#31867;&#27861;&#27010;&#24565;&#26144;&#23556;&#21040;&#26694;&#23884;&#20837;&#20013;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#27010;&#29575;&#35780;&#20998;&#22120;&#26469;&#36827;&#34892;&#27010;&#24565;&#38468;&#21152;&#21644;&#25554;&#20837;&#65292;&#36991;&#20813;&#20351;&#29992;&#20266;&#21494;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20108;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;TaxBox&#22312;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11004</link><description>&lt;p&gt;
&#36890;&#36807;&#26694;&#23884;&#20837;&#21644;&#27010;&#29575;&#35780;&#20998;&#22120;&#23436;&#25104;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Taxonomy Completion with Probabilistic Scorer via Box Embedding. (arXiv:2305.11004v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;TaxBox&#65292;&#35813;&#26041;&#27861;&#23558;&#20998;&#31867;&#27861;&#27010;&#24565;&#26144;&#23556;&#21040;&#26694;&#23884;&#20837;&#20013;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#27010;&#29575;&#35780;&#20998;&#22120;&#26469;&#36827;&#34892;&#27010;&#24565;&#38468;&#21152;&#21644;&#25554;&#20837;&#65292;&#36991;&#20813;&#20351;&#29992;&#20266;&#21494;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20108;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;TaxBox&#22312;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20998;&#31867;&#27861;&#30340;&#23436;&#21892;&#20219;&#21153;--&#33258;&#21160;&#21033;&#29992;&#26032;&#30340;&#27010;&#24565;&#20016;&#23500;&#29616;&#26377;&#20998;&#31867;&#27861;--&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#22797;&#26434;&#27169;&#22359;&#12289;&#22806;&#37096;&#20449;&#24687;&#21644;&#20266;&#21494;&#26469;&#20016;&#23500;&#34920;&#31034;&#24182;&#32479;&#19968;&#38468;&#21152;&#21644;&#25554;&#20837;&#30340;&#21305;&#37197;&#36807;&#31243;&#12290;&#34429;&#28982;&#23427;&#20204;&#24050;&#32463;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#20171;&#32461;&#21487;&#33021;&#20250;&#22312;&#35757;&#32451;&#21644;&#35780;&#20998;&#36807;&#31243;&#20013;&#24102;&#26469;&#22122;&#38899;&#21644;&#19981;&#20844;&#24179;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TaxBox&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#23436;&#25104;&#20998;&#31867;&#27861;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#20998;&#31867;&#27861;&#27010;&#24565;&#26144;&#23556;&#21040;&#26694;&#23884;&#20837;&#20013;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#27010;&#29575;&#35780;&#20998;&#22120;&#26469;&#36827;&#34892;&#27010;&#24565;&#38468;&#21152;&#21644;&#25554;&#20837;&#65292;&#36991;&#20813;&#20351;&#29992;&#20266;&#21494;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TaxBox&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#65288;1&#65289;&#22270;&#32858;&#21512;&#27169;&#22359;&#65292;&#20197;&#21033;&#29992;&#20998;&#31867;&#27861;&#30340;&#32467;&#26500;&#20449;&#24687;&#21644;&#20004;&#20010;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#65292;&#23558;&#29305;&#24449;&#26144;&#23556;&#21040;&#26694;&#23884;&#20837;&#65292;&#24182;&#25429;&#25417;&#27010;&#24565;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65307;&#65288;2&#65289;&#20004;&#20010;&#27010;&#29575;&#35780;&#20998;&#22120;&#65292;&#20998;&#21035;&#23545;&#24212;&#38468;&#21152;&#21644;&#25554;&#20837;&#20219;&#21153;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#26041;&#27861;&#26469;&#20943;&#36731;&#35823;&#23548;&#20449;&#24687;&#30340;&#24433;&#21709;&#65307;&#65288;3&#65289;&#19968;&#31181;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#21644;&#20248;&#21270;&#20004;&#20010;&#35780;&#20998;&#22120;&#30340;&#35757;&#32451;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TaxBox&#22312;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#29366;&#24577;-of-the-art&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Taxonomy completion, a task aimed at automatically enriching an existing taxonomy with new concepts, has gained significant interest in recent years. Previous works have introduced complex modules, external information, and pseudo-leaves to enrich the representation and unify the matching process of attachment and insertion. While they have achieved good performance, these introductions may have brought noise and unfairness during training and scoring. In this paper, we present TaxBox, a novel framework for taxonomy completion that maps taxonomy concepts to box embeddings and employs two probabilistic scorers for concept attachment and insertion, avoiding the need for pseudo-leaves. Specifically, TaxBox consists of three components: (1) a graph aggregation module to leverage the structural information of the taxonomy and two lightweight decoders that map features to box embedding and capture complex relationships between concepts; (2) two probabilistic scorers that correspond to attach
&lt;/p&gt;</description></item><item><title>SpeechGPT&#26159;&#19968;&#20010;&#20855;&#26377;&#26412;&#36136;&#36328;&#27169;&#24577;&#20250;&#35805;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#24863;&#30693;&#21644;&#29983;&#25104;&#22810;&#27169;&#24577;&#20869;&#23481;&#65292;&#21487;&#25353;&#29031;&#22810;&#27169;&#24577;&#20154;&#31867;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#24182;&#31361;&#26174;&#20102;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#22788;&#29702;&#22810;&#20010;&#27169;&#24577;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11000</link><description>&lt;p&gt;
SpeechGPT: &#29992;&#26412;&#36136;&#36328;&#27169;&#24577;&#20250;&#35805;&#33021;&#21147;&#36171;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities. (arXiv:2305.11000v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11000
&lt;/p&gt;
&lt;p&gt;
SpeechGPT&#26159;&#19968;&#20010;&#20855;&#26377;&#26412;&#36136;&#36328;&#27169;&#24577;&#20250;&#35805;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#24863;&#30693;&#21644;&#29983;&#25104;&#22810;&#27169;&#24577;&#20869;&#23481;&#65292;&#21487;&#25353;&#29031;&#22810;&#27169;&#24577;&#20154;&#31867;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#24182;&#31361;&#26174;&#20102;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#22788;&#29702;&#22810;&#20010;&#27169;&#24577;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#36808;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#37325;&#35201;&#19968;&#27493;&#65292;&#38543;&#30528;ChatGPT&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#35821;&#38899;-&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#37319;&#29992;&#32423;&#32852;&#33539;&#24335;&#65292;&#38459;&#27490;&#20102;&#36328;&#27169;&#24577;&#30693;&#35782;&#30340;&#36716;&#31227;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SpeechGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#26412;&#36136;&#36328;&#27169;&#24577;&#20250;&#35805;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#24863;&#30693;&#21644;&#29983;&#25104;&#22810;&#27169;&#24577;&#20869;&#23481;&#12290;&#36890;&#36807;&#31163;&#25955;&#21270;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;SpeechInstruct&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#36328;&#27169;&#24577;&#35821;&#38899;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21253;&#25324;&#27169;&#24577;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#12289;&#36328;&#27169;&#24577;&#25351;&#20196;&#24494;&#35843;&#21644;&#27169;&#24577;&#38142;&#25351;&#20196;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SpeechGPT&#20855;&#26377;&#25353;&#29031;&#22810;&#27169;&#24577;&#20154;&#31867;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#24182;&#31361;&#26174;&#20102;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#22788;&#29702;&#22810;&#20010;&#27169;&#24577;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content. With discrete speech representations, we first construct SpeechInstruct, a large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model. Demos are shown 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#35777;&#26126;&#36825;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10951</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Making More of Little Data: Improving Low-Resource Automatic Speech Recognition Using Data Augmentation. (arXiv:2305.10951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#35777;&#26126;&#36825;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#24615;&#33021;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#22823;&#37327;&#36716;&#24405;&#35821;&#38899;&#30340;&#35821;&#35328;&#32780;&#35328;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#22914;&#23569;&#25968;&#27665;&#26063;&#35821;&#35328;&#12289;&#22320;&#26041;&#35821;&#35328;&#25110;&#26041;&#35328;&#65292;ASR&#24615;&#33021;&#36890;&#24120;&#20173;&#28982;&#36739;&#20302;&#12290;&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#20302;&#36164;&#28304;ASR&#24615;&#33021;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22235;&#31181;&#35821;&#35328;&#25110;&#35821;&#35328;&#21464;&#20307;&#65288;&#26085;&#32819;&#26364;&#35821;&#31995;&#65306;&#26684;&#32599;&#23425;&#26681;&#35821;&#12289;&#35199;&#24343;&#37324;&#35199;&#20122;&#35821;&#65307;&#39532;&#26469;-&#27874;&#21033;&#23612;&#35199;&#20122;&#35821;&#31995;&#65306;&#36125;&#29791;&#29595;&#35821;&#12289;&#32435;&#33832;&#23572;&#35821;&#65289;&#12290;&#23545;&#20110;&#36825;&#22235;&#31181;&#35821;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#35757;&#32451;&#30340;&#20351;&#29992;&#65292;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;&#35757;&#32451;&#38598;&#25968;&#25454;&#35757;&#32451;ASR&#31995;&#32479;&#65292;&#21033;&#29992;&#35813;&#31995;&#32479;&#29983;&#25104;&#30340;&#36716;&#24405;&#25991;&#26412;&#19982;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#32467;&#21512;&#65292;&#26469;&#35757;&#32451;&#26032;&#30340;ASR&#31995;&#32479;&#12290;&#23545;&#20110;&#24050;&#26377;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#65288;TTS&#65289;&#30340;&#26684;&#32599;&#23425;&#26681;&#35821;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;TTS&#26469;&#29983;&#25104;ASR&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of automatic speech recognition (ASR) systems has advanced substantially in recent years, particularly for languages for which a large amount of transcribed speech is available. Unfortunately, for low-resource languages, such as minority languages, regional languages or dialects, ASR performance generally remains much lower. In this study, we investigate whether data augmentation techniques could help improve low-resource ASR performance, focusing on four typologically diverse minority languages or language variants (West Germanic: Gronings, West-Frisian; Malayo-Polynesian: Besemah, Nasal). For all four languages, we examine the use of self-training, where an ASR system trained with the available human-transcribed data is used to generate transcriptions, which are then combined with the original data to train a new ASR system. For Gronings, for which there was a pre-existing text-to-speech (TTS) system available, we also examined the use of TTS to generate ASR training 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#26469;&#26377;&#25928;&#35268;&#36991;&#29616;&#26377;&#30340;&#25991;&#26412;&#26816;&#27979;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10847</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#24341;&#23548;&#26469;&#35268;&#36991;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#26469;&#26377;&#25928;&#35268;&#36991;&#29616;&#26377;&#30340;&#25991;&#26412;&#26816;&#27979;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21253;&#25324;&#35770;&#25991;&#20889;&#20316;&#21644;&#38382;&#31572;&#31561;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24517;&#39035;&#35299;&#20915;&#36825;&#20123;&#27169;&#22411;&#28508;&#22312;&#30340;&#35823;&#29992;&#38382;&#39064;&#65292;&#21542;&#21017;&#21487;&#33021;&#23548;&#33268;&#25220;&#34989;&#21644;&#22403;&#22334;&#20449;&#24687;&#31561;&#19981;&#33391;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26367;&#25442;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#20248;&#21270;&#26041;&#27861;&#65288;SICO&#65289;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#36825;&#31181;&#25552;&#31034;&#35821;&#12290;&#22312;&#19977;&#20010;&#29616;&#23454;&#20219;&#21153;&#20013;&#65292;LLMs&#21487;&#33021;&#34987;&#35823;&#29992;&#65292;&#22312;SICO&#30340;&#24110;&#21161;&#19979;&#65292;ChatGPT&#25104;&#21151;&#22320;&#35268;&#36991;&#20102;&#20845;&#39033;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#65292;&#24179;&#22343;&#23548;&#33268;0.54&#30340;AUC&#19979;&#38477;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#34920;&#29616;&#29978;&#33267;&#27604;&#38543;&#26426;&#20998;&#31867;&#22120;&#36824;&#35201;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#22362;&#23450;&#22320;&#25581;&#31034;&#20102;&#29616;&#26377;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional performance in a variety of tasks, including essay writing and question answering. However, it is crucial to address the potential misuse of these models, which can lead to detrimental outcomes such as plagiarism and spamming. Recently, several detectors have been proposed, including fine-tuned classifiers and various statistical methods. In this study, we reveal that with the aid of carefully crafted prompts, LLMs can effectively evade these detection systems. We propose a novel Substitution-based In-Context example Optimization method (SICO) to automatically generate such prompts. On three real-world tasks where LLMs can be misused, SICO successfully enables ChatGPT to evade six existing detectors, causing a significant 0.54 AUC drop on average. Surprisingly, in most cases these detectors perform even worse than random classifiers. These results firmly reveal the vulnerability of existing detectors. Finally, the strong perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25351;&#23548;&#38405;&#35835;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26377;&#24847;&#20041;&#38382;&#39064;&#65292;&#20855;&#26377;&#22810;&#26679;&#24615;&#19988;&#28085;&#30422;&#36755;&#20837;&#25991;&#26412;&#20013;&#22823;&#22810;&#25968;&#20027;&#39064;&#65292;&#21516;&#26102;&#33021;&#22815;&#26377;&#25928;&#22320;&#24635;&#32467;&#22238;&#31572;&#21644;&#25512;&#33616;&#37325;&#26032;&#38405;&#35835;&#30340;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.10645</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#21512;&#25351;&#23548;&#38405;&#35835;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Fit For Guided Reading?. (arXiv:2305.10645v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25351;&#23548;&#38405;&#35835;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26377;&#24847;&#20041;&#38382;&#39064;&#65292;&#20855;&#26377;&#22810;&#26679;&#24615;&#19988;&#28085;&#30422;&#36755;&#20837;&#25991;&#26412;&#20013;&#22823;&#22810;&#25968;&#20027;&#39064;&#65292;&#21516;&#26102;&#33021;&#22815;&#26377;&#25928;&#22320;&#24635;&#32467;&#22238;&#31572;&#21644;&#25512;&#33616;&#37325;&#26032;&#38405;&#35835;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#25351;&#23548;&#38405;&#35835;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;&#25105;&#20204;&#20855;&#20307;&#35780;&#20272;&#20102;&#23427;&#20204;&#20174;&#36755;&#20837;&#25991;&#26412;&#20013;&#29983;&#25104;&#26377;&#24847;&#20041;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#29983;&#25104;&#20869;&#23481;&#28085;&#30422;&#21644;&#38382;&#39064;&#38590;&#24230;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#26681;&#25454;&#23398;&#29983;&#23545;&#38382;&#39064;&#30340;&#22238;&#31572;&#25512;&#33616;&#24212;&#35813;&#37325;&#26032;&#38405;&#35835;&#30340;&#25991;&#26412;&#37096;&#20998;&#30340;&#33021;&#21147;&#12290;&#22312;&#23545;ChatGPT&#21644;Bard&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#22914;&#19979;&#32467;&#26524;&#65306;1&#65289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#19982;&#36755;&#20837;&#25991;&#26412;&#39640;&#30456;&#20851;&#30340;&#39640;&#36136;&#37327;&#26377;&#24847;&#20041;&#30340;&#38382;&#39064;&#65292;2&#65289;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#28085;&#30422;&#36755;&#20837;&#25991;&#26412;&#20013;&#22823;&#22810;&#25968;&#20027;&#39064;&#30340;&#22810;&#26679;&#21270;&#38382;&#39064;&#65292;&#23613;&#31649;&#38543;&#30528;&#36755;&#20837;&#25991;&#26412;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#33021;&#21147;&#26174;&#33879;&#38477;&#20302;&#65292;3&#65289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#20302;&#21644;&#39640;&#35748;&#30693;&#38590;&#24230;&#30340;&#38382;&#39064;&#65292;&#23613;&#31649;&#23427;&#20204;&#26174;&#33879;&#20559;&#21521;&#20110;&#20302;&#35748;&#30693;&#38590;&#24230;&#30340;&#38382;&#39064;&#65292;4&#65289;&#23427;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#24635;&#32467;&#22238;&#31572;&#24182;&#25552;&#21462;&#24212;&#35813;&#37325;&#26032;&#38405;&#35835;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper looks at the ability of large language models to participate in educational guided reading. We specifically, evaluate their ability to generate meaningful questions from the input text, generate diverse questions both in terms of content coverage and difficulty of the questions and evaluate their ability to recommend part of the text that a student should re-read based on the student's responses to the questions. Based on our evaluation of ChatGPT and Bard, we report that,  1) Large language models are able to generate high quality meaningful questions that have high correlation with the input text, 2) They generate diverse question that cover most topics in the input text even though this ability is significantly degraded as the input text increases, 3)The large language models are able to generate both low and high cognitive questions even though they are significantly biased toward low cognitive question, 4) They are able to effectively summarize responses and extract a p
&lt;/p&gt;</description></item><item><title>UniEX&#26159;&#19968;&#31181;&#33021;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24335;&#26684;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#26694;&#26550;&#65292;&#24182;&#33021;&#21516;&#26102;&#35299;&#20915;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#65292;&#22312;&#24615;&#33021;&#21644;&#25512;&#29702;&#36895;&#24230;&#19978;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10306</link><description>&lt;p&gt;
UniEX&#65306;&#19968;&#31181;&#22522;&#20110;&#36328;&#24230;&#25552;&#21462;&#30340;&#32479;&#19968;&#20449;&#24687;&#25277;&#21462;&#30340;&#26377;&#25928;&#39640;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective. (arXiv:2305.10306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10306
&lt;/p&gt;
&lt;p&gt;
UniEX&#26159;&#19968;&#31181;&#33021;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24335;&#26684;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#26694;&#26550;&#65292;&#24182;&#33021;&#21516;&#26102;&#35299;&#20915;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#65292;&#22312;&#24615;&#33021;&#21644;&#25512;&#29702;&#36895;&#24230;&#19978;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#33539;&#24335;&#65292;&#23427;&#19982;&#20219;&#20309;&#27169;&#24335;&#26684;&#24335;&#20860;&#23481;&#65292;&#24182;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#36716;&#21270;&#20026; token-pair &#38382;&#39064;&#65292;&#20351;&#29992;&#19968;&#31181;&#32479;&#19968;&#30340;&#25552;&#21462;&#26694;&#26550; UniEX&#65292;&#23558;&#25152;&#26377;&#25552;&#21462;&#30446;&#26631;&#37117;&#32479;&#19968;&#20998;&#35299;&#20026;&#32852;&#21512;&#36328;&#24230;&#26816;&#27979;&#12289;&#20998;&#31867;&#21644;&#20851;&#32852;&#38382;&#39064;&#12290;UniEX &#21487;&#20197;&#21516;&#26102;&#32534;&#30721;&#22522;&#20110;&#27169;&#24335;&#30340;&#25552;&#31034;&#21644;&#25991;&#26412;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#23398;&#20064;&#39044;&#23450;&#20041;&#20449;&#24687;&#30340;&#24191;&#20041;&#30693;&#35782;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102; traffine &#27880;&#24847;&#26426;&#21046;&#65292;&#23558;&#21253;&#25324;&#20219;&#21153;&#12289;&#26631;&#31614;&#21644;&#20869;&#37096; token &#22312;&#20869;&#30340;&#24322;&#26500;&#22240;&#32032;&#38598;&#25104;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#35780;&#20998;&#30697;&#38453;&#33719;&#24471;&#25552;&#21462;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniEX &#22312; $14$&#20010;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#21644;&#25512;&#29702;&#36895;&#24230;&#37117;&#20248;&#20110;&#22522;&#20110;&#29983;&#25104;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new paradigm for universal information extraction (IE) that is compatible with any schema format and applicable to a list of IE tasks, such as named entity recognition, relation extraction, event extraction and sentiment analysis. Our approach converts the text-based IE tasks as the token-pair problem, which uniformly disassembles all extraction targets into joint span detection, classification and association problems with a unified extractive framework, namely UniEX. UniEX can synchronously encode schema-based prompt and textual information, and collaboratively learn the generalized knowledge from pre-defined information using the auto-encoder language models. We develop a traffine attention mechanism to integrate heterogeneous factors including tasks, labels and inside tokens, and obtain the extraction target via a scoring matrix. Experiment results show that UniEX can outperform generative universal IE models in terms of performance and inference-speed on $14$ benchmar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65288;CPL-NoViD&#65289;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23558;&#19978;&#19979;&#25991;&#34701;&#20837;&#21040;&#27169;&#22411;&#20013;&#65292;&#29992;&#20110;&#22312;&#32447;&#31038;&#21306;&#20013;&#30340;&#36829;&#35268;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#31038;&#21306;&#20013;&#30340;&#21508;&#31181;&#35268;&#21017;&#21644;&#35299;&#37322;&#30340;&#24046;&#24322;&#65292;&#22312;&#36328;&#35268;&#21017;&#31867;&#22411;&#21644;&#36328;&#31038;&#21306;&#30340;&#36829;&#35268;&#34892;&#20026;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09846</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25552;&#31034;&#24335;&#23398;&#20064;&#29992;&#20110;&#22312;&#32447;&#31038;&#21306;&#36829;&#35268;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CPL-NoViD: Context-Aware Prompt-based Learning for Norm Violation Detection in Online Communities. (arXiv:2305.09846v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65288;CPL-NoViD&#65289;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23558;&#19978;&#19979;&#25991;&#34701;&#20837;&#21040;&#27169;&#22411;&#20013;&#65292;&#29992;&#20110;&#22312;&#32447;&#31038;&#21306;&#20013;&#30340;&#36829;&#35268;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#31038;&#21306;&#20013;&#30340;&#21508;&#31181;&#35268;&#21017;&#21644;&#35299;&#37322;&#30340;&#24046;&#24322;&#65292;&#22312;&#36328;&#35268;&#21017;&#31867;&#22411;&#21644;&#36328;&#31038;&#21306;&#30340;&#36829;&#35268;&#34892;&#20026;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#21306;&#20013;&#26816;&#27979;&#36829;&#35268;&#34892;&#20026;&#23545;&#20110;&#32500;&#25252;&#20581;&#24247;&#21644;&#23433;&#20840;&#30340;&#22312;&#32447;&#35752;&#35770;&#31354;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#36866;&#24212;&#19981;&#21516;&#31038;&#21306;&#20043;&#38388;&#21508;&#31181;&#35268;&#21017;&#21644;&#35299;&#37322;&#30340;&#24046;&#24322;&#65292;&#22240;&#20026;&#20026;&#36825;&#31181;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#20219;&#21153;&#24494;&#35843;&#27169;&#22411;&#20855;&#26377;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#23398;&#20064;&#29992;&#20110;&#26816;&#27979;&#19981;&#21516;&#31867;&#22411;&#35268;&#21017;&#19979;&#30340;&#36829;&#35268;&#34892;&#20026;&#65288;CPL-NoViD&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;CPL-NoViD&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#23558;&#19978;&#19979;&#25991;&#34701;&#20837;&#21040;&#27169;&#22411;&#20013;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#35268;&#21017;&#30340;&#34920;&#29616;&#20063;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#19981;&#20165;&#22312;&#36328;&#35268;&#21017;&#31867;&#22411;&#21644;&#36328;&#31038;&#21306;&#30340;&#36829;&#35268;&#34892;&#20026;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#36866;&#24212;&#24615;&#12290;&#23588;&#20854;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#36829;&#35268;&#26816;&#27979;&#26032;&#30340;&#26368;&#39640;&#27700;&#24179;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting norm violations in online communities is critical to maintaining healthy and safe spaces for online discussions. Existing machine learning approaches often struggle to adapt to the diverse rules and interpretations across different communities due to the inherent challenges of fine-tuning models for such context-specific tasks. In this paper, we introduce Context-aware Prompt-based Learning for Norm Violation Detection (CPL-NoViD), a novel method that employs prompt-based learning to detect norm violations across various types of rules. CPL-NoViD outperforms the baseline by incorporating context through natural language prompts and demonstrates improved performance across different rule types. Significantly, it not only excels in cross-rule-type and cross-community norm violation detection but also exhibits adaptability in few-shot learning scenarios. Most notably, it establishes a new state-of-the-art in norm violation detection, surpassing existing benchmarks. Our work high
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65288;AR-Diffusion&#65289;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#21160;&#24577;&#25968;&#37327;&#30340;&#38477;&#22122;&#27493;&#39588;&#65292;&#30830;&#20445;&#24038;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#24433;&#21709;&#21491;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.09515</link><description>&lt;p&gt;
AR-Diffusion&#65306;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation. (arXiv:2305.09515v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65288;AR-Diffusion&#65289;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#21160;&#24577;&#25968;&#37327;&#30340;&#38477;&#22122;&#27493;&#39588;&#65292;&#30830;&#20445;&#24038;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#24433;&#21709;&#21491;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#36825;&#31181;&#25104;&#21151;&#24050;&#32463;&#25193;&#23637;&#21040;&#20102;&#36890;&#36807;&#21516;&#26102;&#29983;&#25104;&#24207;&#21015;&#20013;&#30340;&#25152;&#26377;&#26631;&#35760;&#26469;&#23454;&#29616;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#30456;&#23545;&#20110;&#22270;&#20687;&#20855;&#26377;&#26356;&#20026;&#26126;&#26174;&#30340;&#24207;&#21015;&#20381;&#36182;&#24615;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#37117;&#26159;&#20351;&#29992;&#33258;&#24038;&#21521;&#21491;&#30340;&#33258;&#22238;&#24402;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22266;&#26377;&#30340;&#24207;&#21015;&#29305;&#24449;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#22238;&#24402;&#25193;&#25955;&#65288;AR-Diffusion&#65289;&#27169;&#22411;&#12290;AR-Diffusion&#30830;&#20445;&#21491;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#21462;&#20915;&#20110;&#24038;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#65292;&#36825;&#31181;&#26426;&#21046;&#26159;&#36890;&#36807;&#37319;&#29992;&#21160;&#24577;&#25968;&#37327;&#30340;&#38477;&#22122;&#27493;&#39588;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#20123;&#27493;&#39588;&#26681;&#25454;&#26631;&#35760;&#20301;&#32622;&#32780;&#21464;&#21270;&#12290;&#36825;&#23548;&#33268;&#24038;&#20391;&#30340;&#26631;&#35760;&#32463;&#21382;&#30340;&#38477;&#22122;&#27493;&#39588;&#27604;&#21491;&#20391;&#30340;&#26631;&#35760;&#23569;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#33021;&#22815;&#26356;&#26089;&#22320;&#29983;&#25104;&#24182;&#38543;&#21518;&#24433;&#21709;&#21491;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have gained significant attention in the realm of image generation due to their exceptional performance. Their success has been recently expanded to text generation via generating all tokens within a sequence concurrently. However, natural language exhibits a far more pronounced sequential dependency in comparison to images, and the majority of existing language models are trained utilizing a left-to-right auto-regressive approach. To account for the inherent sequential characteristic of natural language, we introduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that the generation of tokens on the right depends on the generated ones on the left, a mechanism achieved through employing a dynamic number of denoising steps that vary based on token position. This results in tokens on the left undergoing fewer denoising steps than those on the right, thereby enabling them to generate earlier and subsequently influence the generation of tokens on the right.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#23398;&#38590;&#23398;&#30340;&#20449;&#24687;&#25277;&#21462;&#23398;&#20064;&#26694;&#26550;&#65292;&#20998;&#20026;&#20837;&#38376;&#12289;&#22256;&#38590;&#21644;&#20027;&#38454;&#27573;&#65292;&#27169;&#20223;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#65292;&#36890;&#36807;&#20998;&#38454;&#27573;&#23398;&#20064;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.09193</link><description>&lt;p&gt;
&#20449;&#24687;&#25277;&#21462;&#30340;&#26131;&#23398;&#38590;&#23398;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Easy-to-Hard Learning for Information Extraction. (arXiv:2305.09193v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#23398;&#38590;&#23398;&#30340;&#20449;&#24687;&#25277;&#21462;&#23398;&#20064;&#26694;&#26550;&#65292;&#20998;&#20026;&#20837;&#38376;&#12289;&#22256;&#38590;&#21644;&#20027;&#38454;&#27573;&#65292;&#27169;&#20223;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#65292;&#36890;&#36807;&#20998;&#38454;&#27573;&#23398;&#20064;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25277;&#21462;&#26159;&#25351;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#20986;&#21629;&#21517;&#23454;&#20307;&#12289;&#23454;&#20307;&#20851;&#31995;&#21644;&#20107;&#20214;&#31561;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#30340;&#26131;&#23398;&#38590;&#23398;&#26694;&#26550;&#65292;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;&#20837;&#38376;&#38454;&#27573;&#12289;&#22256;&#38590;&#38454;&#27573;&#21644;&#20027;&#38454;&#27573;&#12290;&#36890;&#36807;&#20998;&#38454;&#27573;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20419;&#36827;&#20102;&#27169;&#22411;&#33719;&#24471;&#26356;&#24191;&#27867;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#25552;&#39640;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#22235;&#20010;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information extraction (IE) systems aim to automatically extract structured information, such as named entities, relations between entities, and events, from unstructured texts. While most existing work addresses a particular IE task, universally modeling various IE tasks with one model has achieved great success recently. Despite their success, they employ a one-stage learning strategy, i.e., directly learning to extract the target structure given the input text, which contradicts the human learning process. In this paper, we propose a unified easy-to-hard learning framework consisting of three stages, i.e., the easy stage, the hard stage, and the main stage, for IE by mimicking the human learning process. By breaking down the learning process into multiple stages, our framework facilitates the model to acquire general IE task knowledge and improve its generalization ability. Extensive experiments across four IE tasks demonstrate the effectiveness of our framework. We achieve new stat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;LoRA&#21644;&#32467;&#26500;&#21270;&#23618;&#21098;&#26525;&#26041;&#27861;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#22312;&#20445;&#25345;&#36229;&#36807;92%&#29983;&#25104;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35843;&#25972;&#20165;0.6%&#30340;&#21442;&#25968;&#24182;&#21098;&#26525;&#36229;&#36807;30%&#30340;Transformer&#23618;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;50%&#30340;GPU&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#21319;&#20102;100%&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.08285</link><description>&lt;p&gt;
&#26080;&#38656;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65306;&#22522;&#20110;&#32467;&#26500;&#21270;&#21098;&#26525;&#30340;LoRA&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning with Layer Pruning on Free-Text Sequence-to-Sequence modeling. (arXiv:2305.08285v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;LoRA&#21644;&#32467;&#26500;&#21270;&#23618;&#21098;&#26525;&#26041;&#27861;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#22312;&#20445;&#25345;&#36229;&#36807;92%&#29983;&#25104;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35843;&#25972;&#20165;0.6%&#30340;&#21442;&#25968;&#24182;&#21098;&#26525;&#36229;&#36807;30%&#30340;Transformer&#23618;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;50%&#30340;GPU&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#21319;&#20102;100%&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23610;&#23544;&#30340;&#19981;&#26029;&#22686;&#38271;&#24341;&#36215;&#20102;&#23545;&#20110;&#21442;&#25968;&#25928;&#29575;&#30340;&#24494;&#35843;&#26041;&#27861;&#30340;&#30740;&#31350;&#20852;&#36259;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;LoRA&#21644;&#32467;&#26500;&#21270;&#23618;&#21098;&#26525;&#26041;&#27861;&#32467;&#21512;&#30340;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312; MIMIC-IV-Note&#19978;&#30340;&#20004;&#20010;&#21307;&#30103;&#25253;&#21578;&#27010;&#36848;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#20844;&#20849;&#21307;&#30103;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#36890;&#36807;&#35843;&#25972;&#21407;&#22987;&#27169;&#22411;&#30340;0.6%&#30340;&#21442;&#25968;&#24182;&#21098;&#26525;&#36229;&#36807;30%&#30340;Transformer&#23618;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20943;&#23569;50%&#30340;GPU&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#21319;100%&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#33258;&#30001;&#25991;&#26412;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#19978;&#36229;&#36807;92%&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing size of language models raises great research interests in parameter-efficient fine-tuning such as LoRA that freezes the pre-trained model, and injects small-scale trainable parameters for multiple downstream tasks (e.g., summarization, question answering and translation). To further enhance the efficiency of fine-tuning, we propose a framework that integrates LoRA and structured layer pruning. The integrated framework is validated on two created deidentified medical report summarization datasets based on MIMIC-IV-Note and two public medical dialogue datasets. By tuning 0.6% parameters of the original model and pruning over 30% Transformer-layers, our framework can reduce 50% of GPU memory usage and speed up 100% of the training phase, while preserving over 92% generation qualities on free-text sequence-to-sequence tasks.
&lt;/p&gt;</description></item><item><title>FactKG&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#65292;&#21253;&#21547;108k&#20010;&#33258;&#28982;&#35821;&#35328;&#22768;&#26126;&#21644;&#20116;&#31181;&#25512;&#29702;&#31867;&#22411;&#65292;&#21487;&#24110;&#21161;&#31038;&#21306;&#26356;&#22909;&#22320;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.06590</link><description>&lt;p&gt;
FactKG: &#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
FactKG: Fact Verification via Reasoning on Knowledge Graphs. (arXiv:2305.06590v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06590
&lt;/p&gt;
&lt;p&gt;
FactKG&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#65292;&#21253;&#21547;108k&#20010;&#33258;&#28982;&#35821;&#35328;&#22768;&#26126;&#21644;&#20116;&#31181;&#25512;&#29702;&#31867;&#22411;&#65292;&#21487;&#24110;&#21161;&#31038;&#21306;&#26356;&#22909;&#22320;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#22312;&#21508;&#31181;&#39046;&#22495;&#65288;&#22914;&#21307;&#30103;&#24212;&#29992;&#21644;&#23545;&#35805;&#20195;&#29702;&#65289;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#20107;&#23454;&#39564;&#35777;&#26041;&#38754;&#65292;KG&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#20316;&#20026;&#30693;&#35782;&#28304;&#12290;KG&#21487;&#20197;&#25104;&#20026;&#20107;&#23454;&#39564;&#35777;&#30340;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#26469;&#28304;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#21487;&#38752;&#24615;&#21644;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;KG&#30001;&#33410;&#28857;&#21644;&#36793;&#32452;&#25104;&#65292;&#28165;&#26224;&#22320;&#23637;&#31034;&#20102;&#27010;&#24565;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20351;&#24471;&#26426;&#22120;&#21487;&#20197;&#25512;&#29702;&#20986;&#19968;&#31995;&#21015;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#36825;&#20123;&#26426;&#22120;&#21487;&#35835;&#30340;&#27010;&#24565;&#22914;&#20309;&#26144;&#23556;&#21040;&#25991;&#26412;&#20013;&#30340;&#20449;&#24687;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;&#20026;&#20102;&#20351;&#31038;&#21306;&#26356;&#22909;&#22320;&#21033;&#29992;KG&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;FactKG:&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#65292;&#23427;&#21253;&#21547;108k&#20010;&#33258;&#28982;&#35821;&#35328;&#22768;&#26126;&#20197;&#21450;&#20116;&#31181;&#25512;&#29702;&#31867;&#22411;&#65306;&#21333;&#36339;&#12289;&#21512;&#21462;&#12289;&#23384;&#22312;&#12289;&#22810;&#36339;&#21644;&#21542;&#23450;&#12290;&#27492;&#22806;&#65292;FactKG&#21253;&#21547;&#21508;&#31181;&#35821;&#35328;&#27169;&#24335;&#65292;&#21253;&#25324;&#21475;&#35821;&#39118;&#26684;&#30340;&#22768;&#26126;&#21644;&#20070;&#38754;&#39118;&#26684;&#30340;&#22768;&#26126;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real world applications, knowledge graphs (KG) are widely used in various domains (e.g. medical applications and dialogue agents). However, for fact verification, KGs have not been adequately utilized as a knowledge source. KGs can be a valuable knowledge source in fact verification due to their reliability and broad applicability. A KG consists of nodes and edges which makes it clear how concepts are linked together, allowing machines to reason over chains of topics. However, there are many challenges in understanding how these machine-readable concepts map to information in text. To enable the community to better use KGs, we introduce a new dataset, FactKG: Fact Verification via Reasoning on Knowledge Graphs. It consists of 108k natural language claims with five types of reasoning: One-hop, Conjunction, Existence, Multi-hop, and Negation. Furthermore, FactKG contains various linguistic patterns, including colloquial style claims as well as written style claims to increase practica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLMs&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM Personas&#65292;&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2305.02547</link><description>&lt;p&gt;
PersonaLLM: &#25506;&#31350;GPT-3.5&#34920;&#36798;&#20010;&#24615;&#29305;&#24449;&#21644;&#24615;&#21035;&#24046;&#24322;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences. (arXiv:2305.02547v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLMs&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM Personas&#65292;&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#34892;&#19994;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#26377;&#35768;&#22810;&#29992;&#36884;&#65292;&#24182;&#19988;&#30740;&#31350;&#34920;&#26126;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#28385;&#36275;&#19981;&#21516;&#20154;&#26684;&#29305;&#24449;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#20010;&#24615;&#21270;LLM&#30340;&#34892;&#20026;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#12289;&#19968;&#33268;&#22320;&#21453;&#26144;&#26576;&#20123;&#20154;&#26684;&#29305;&#24449;&#12290;&#25105;&#20204;&#32771;&#34385;&#30740;&#31350;&#22522;&#20110;LLM&#30340;&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM personas&#65292;&#24182;&#20351;&#29992;GPT-3.5&#65288;text-davinci-003&#65289;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;LLM&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;320&#20010;LLM personas&#65288;&#27599;&#31181;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#26377;5&#20010;&#22899;&#24615;&#21644;5&#20010;&#30007;&#24615;&#65289;&#65292;&#24182;&#25552;&#31034;&#20182;&#20204;&#23436;&#25104;&#32463;&#20856;&#30340;44&#39033;&#22823;&#20116;&#20154;&#26684;&#38382;&#21367;&#65288;BFI&#65289;&#65292;&#28982;&#21518;&#25776;&#20889;&#19968;&#20010;&#20851;&#20110;&#20182;&#20204;&#31461;&#24180;&#30340;800&#23383;&#25925;&#20107;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM personas&#30340;&#33258;&#25105;&#25253;&#21578;&#30340;BFI&#20998;&#25968;&#19982;&#20182;&#20204;&#20998;&#37197;&#30340;&#20154;&#26684;&#31867;&#22411;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the many use cases for large language models (LLMs) in the design of chatbots in various industries and the research showing the importance of personalizing chatbots to cater to different personality traits, little work has been done to evaluate whether the behaviors of personalized LLMs can reflect certain personality traits accurately and consistently. We consider studying the behavior of LLM-based simulated agents which refer to as LLM personas and present a case study with GPT-3.5 (text-davinci-003) to investigate whether LLMs can generate content with consistent, personalized traits when assigned Big Five personality types and gender roles. We created 320 LLM personas (5 females and 5 males for each of the 32 Big Five personality types) and prompted them to complete the classic 44-item Big Five Inventory (BFI) and then write an 800-word story about their childhood. Results showed that LLM personas' self-reported BFI scores are consistent with their assigned personality typ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.15714</link><description>&lt;p&gt;
&#26174;&#24335;&#35268;&#21010;&#26377;&#21161;&#20110;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;&#26174;&#24335;&#35268;&#21010;&#32435;&#20837;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#23637;&#26395;&#26410;&#26469;&#30340;&#25928;&#26524;&#26469;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#20840;&#22871;&#31995;&#32479;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#39064;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#65292;&#23613;&#31649;&#21482;&#26377;&#32422;15&#20159;&#20010;&#21442;&#25968;&#65292;&#20294;&#19982;GPT-3-davinci&#34920;&#29616;&#30456;&#24403;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#28040;&#34701;&#30740;&#31350;&#20197;&#35777;&#26126;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose a novel system that uses language models to perform multi-step logical reasoning. Our system incorporates explicit planning into its inference procedure, thus able to make more informed reasoning decisions at each step by looking ahead into their future effects. In our experiments, our full system significantly outperforms other competing systems. On a multiple-choice question answering task, our system performs competitively compared to GPT-3-davinci despite having only around 1.5B parameters. We conduct several ablation studies to demonstrate that explicit planning plays a crucial role in the system's performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#25110;&#35821;&#38899;&#39537;&#21160;&#30340;UniFLG&#31995;&#32479;&#65292;&#23558;&#25991;&#26412;&#19982;&#35821;&#38899;&#32467;&#21512;&#36215;&#26469;&#29983;&#25104;&#38754;&#37096;&#29305;&#24449;&#28857;&#12290;&#35813;&#31995;&#32479;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#22522;&#20110;&#25991;&#26412;&#39537;&#21160;&#26041;&#27861;&#26356;&#39640;&#30340;&#35821;&#38899;&#21644;&#38754;&#37096;&#34920;&#29616;&#33258;&#28982;&#24230;&#65292;&#21487;&#20197;&#20174;&#27809;&#26377;&#38754;&#37096;&#35270;&#39057;&#25968;&#25454;&#25110;&#35821;&#38899;&#25968;&#25454;&#30340;&#35762;&#35805;&#32773;&#20013;&#29983;&#25104;&#38754;&#37096;&#29305;&#24449;&#28857;&#12290;</title><link>http://arxiv.org/abs/2302.14337</link><description>&lt;p&gt;
UniFLG: &#25991;&#26412;&#25110;&#35821;&#38899;&#39537;&#21160;&#30340;&#32479;&#19968;&#38754;&#37096;&#29305;&#24449;&#28857;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
UniFLG: Unified Facial Landmark Generator from Text or Speech. (arXiv:2302.14337v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14337
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#25110;&#35821;&#38899;&#39537;&#21160;&#30340;UniFLG&#31995;&#32479;&#65292;&#23558;&#25991;&#26412;&#19982;&#35821;&#38899;&#32467;&#21512;&#36215;&#26469;&#29983;&#25104;&#38754;&#37096;&#29305;&#24449;&#28857;&#12290;&#35813;&#31995;&#32479;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#22522;&#20110;&#25991;&#26412;&#39537;&#21160;&#26041;&#27861;&#26356;&#39640;&#30340;&#35821;&#38899;&#21644;&#38754;&#37096;&#34920;&#29616;&#33258;&#28982;&#24230;&#65292;&#21487;&#20197;&#20174;&#27809;&#26377;&#38754;&#37096;&#35270;&#39057;&#25968;&#25454;&#25110;&#35821;&#38899;&#25968;&#25454;&#30340;&#35762;&#35805;&#32773;&#20013;&#29983;&#25104;&#38754;&#37096;&#29305;&#24449;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#35805;&#30340;&#38754;&#37096;&#29983;&#25104;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#20102;&#12290;&#29992;&#20110;&#35828;&#35805;&#30340;&#20004;&#31181;&#20027;&#35201;&#26694;&#26550;&#21253;&#25324;&#39537;&#21160;&#20110;&#25991;&#26412;&#30340;&#26694;&#26550;&#65292;&#20174;&#25991;&#26412;&#29983;&#25104;&#21516;&#27493;&#30340;&#35821;&#38899;&#21644;&#35828;&#35805;&#30340;&#38754;&#23380;&#65292;&#20197;&#21450;&#39537;&#21160;&#20110;&#35821;&#38899;&#30340;&#26694;&#26550;&#65292;&#20174;&#35821;&#38899;&#29983;&#25104;&#35828;&#35805;&#30340;&#38754;&#23380;&#12290;&#20026;&#20102;&#25972;&#21512;&#36825;&#20123;&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#38754;&#37096;&#29305;&#24449;&#28857;&#29983;&#25104;&#22120;&#65288;UniFLG&#65289;&#12290;&#35813;&#31995;&#32479;&#19981;&#20165;&#21033;&#29992;&#31471;&#21040;&#31471;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#26469;&#21512;&#25104;&#35821;&#38899;&#65292;&#36824;&#21033;&#29992;&#20854;&#20013;&#30340;&#19968;&#31995;&#21015;&#28508;&#22312;&#34920;&#31034;&#26469;&#20174;&#25991;&#26412;&#21644;&#35821;&#38899;&#20013;&#25552;&#21462;&#20849;&#21516;&#30340;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#29305;&#24449;&#28857;&#35299;&#30721;&#22120;&#20013;&#29983;&#25104;&#38754;&#37096;&#29305;&#24449;&#28857;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#35821;&#38899;&#21512;&#25104;&#21644;&#38754;&#37096;&#29305;&#24449;&#28857;&#29983;&#25104;&#26041;&#38754;&#37117;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#33258;&#28982;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20174;&#27809;&#26377;&#38754;&#37096;&#35270;&#39057;&#25968;&#25454;&#25110;&#29978;&#33267;&#35821;&#38899;&#25968;&#25454;&#30340;&#35762;&#35805;&#32773;&#30340;&#35821;&#38899;&#20013;&#29983;&#25104;&#38754;&#37096;&#29305;&#24449;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Talking face generation has been extensively investigated owing to its wide applicability. The two primary frameworks used for talking face generation comprise a text-driven framework, which generates synchronized speech and talking faces from text, and a speech-driven framework, which generates talking faces from speech. To integrate these frameworks, this paper proposes a unified facial landmark generator (UniFLG). The proposed system exploits end-to-end text-to-speech not only for synthesizing speech but also for extracting a series of latent representations that are common to text and speech, and feeds it to a landmark decoder to generate facial landmarks. We demonstrate that our system achieves higher naturalness in both speech synthesis and facial landmark generation compared to the state-of-the-art text-driven method. We further demonstrate that our system can generate facial landmarks from speech of speakers without facial video data or even speech data.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21442;&#21152;SemEval-2023&#20219;&#21153;4&#30340;&#23454;&#39564;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#23450;&#20041;&#26469;&#25552;&#39640;&#23545;&#35770;&#25454;&#32972;&#21518;&#30340;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.13925</link><description>&lt;p&gt;
Epicurus&#21442;&#21152;SemEval-2023&#20219;&#21153;4&#65306;&#36890;&#36807;&#21033;&#29992;&#23450;&#20041;&#26469;&#25913;&#21892;&#39044;&#27979;&#35770;&#25454;&#32972;&#21518;&#30340;&#20154;&#31867;&#20215;&#20540;&#35266;
&lt;/p&gt;
&lt;p&gt;
Epicurus at SemEval-2023 Task 4: Improving Prediction of Human Values behind Arguments by Leveraging Their Definitions. (arXiv:2302.13925v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21442;&#21152;SemEval-2023&#20219;&#21153;4&#30340;&#23454;&#39564;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#23450;&#20041;&#26469;&#25552;&#39640;&#23545;&#35770;&#25454;&#32972;&#21518;&#30340;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#38024;&#23545;SemEval-2023&#20219;&#21153;4&#65288;ValueEval&#65289;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#20854;&#20013;&#28041;&#21450;&#22914;&#20309;&#30830;&#23450;&#35770;&#25454;&#32972;&#21518;&#30340;&#20154;&#31867;&#20215;&#20540;&#35266;&#12290;&#30001;&#20110;&#20154;&#31867;&#20215;&#20540;&#35266;&#26159;&#20027;&#35266;&#27010;&#24565;&#65292;&#38656;&#35201;&#31934;&#30830;&#23450;&#20041;&#65292;&#22240;&#27492;&#25105;&#20204;&#20551;&#35774;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#20837;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23450;&#20041;&#65288;&#20197;&#27880;&#37322;&#35828;&#26126;&#21644;&#24050;&#39564;&#35777;&#30340;&#35843;&#26597;&#26465;&#30446;&#30340;&#24418;&#24335;&#65289;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20010;&#24819;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#27604;&#25361;&#25112;&#32452;&#32455;&#32773;&#30340;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#23439;F1&#24471;&#20998;&#25552;&#39640;&#20102;&#39640;&#36798;18%&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe our experiments for SemEval-2023 Task 4 on the identification of human values behind arguments (ValueEval). Because human values are subjective concepts which require precise definitions, we hypothesize that incorporating the definitions of human values (in the form of annotation instructions and validated survey items) during model training can yield better prediction performance. We explore this idea and show that our proposed models perform better than the challenge organizers' baselines, with improvements in macro F1 scores of up to 18%.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiLD&#30340;&#26694;&#26550;&#65292;&#23427;&#30001;&#22823;&#23567;&#19981;&#21516;&#30340;&#20004;&#20010;&#27169;&#22411;&#21327;&#20316;&#29983;&#25104;&#25991;&#26412;&#12290;&#20854;&#20013;&#23567;&#22411;&#27169;&#22411;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;&#25991;&#26412;&#65292;&#32780;&#22823;&#22411;&#27169;&#22411;&#21017;&#22312;&#24517;&#35201;&#26102;&#20197;&#38750;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#23545;&#23567;&#22411;&#27169;&#22411;&#30340;&#39044;&#27979;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#25512;&#29702;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2302.07863</link><description>&lt;p&gt;
&#22823;&#23567;&#19981;&#21516;&#30340;Transformer&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Big Little Transformer Decoder. (arXiv:2302.07863v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07863
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiLD&#30340;&#26694;&#26550;&#65292;&#23427;&#30001;&#22823;&#23567;&#19981;&#21516;&#30340;&#20004;&#20010;&#27169;&#22411;&#21327;&#20316;&#29983;&#25104;&#25991;&#26412;&#12290;&#20854;&#20013;&#23567;&#22411;&#27169;&#22411;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;&#25991;&#26412;&#65292;&#32780;&#22823;&#22411;&#27169;&#22411;&#21017;&#22312;&#24517;&#35201;&#26102;&#20197;&#38750;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#23545;&#23567;&#22411;&#27169;&#22411;&#30340;&#39044;&#27979;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#25512;&#29702;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#20351;&#24471;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#38271;&#26102;&#38388;&#30340;&#25512;&#29702;&#24310;&#36831;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#20351;&#29992;&#24182;&#19988;&#20351;&#24471;&#23427;&#20204;&#22312;&#21508;&#31181;&#23454;&#26102;&#24212;&#29992;&#20013;&#36807;&#20110;&#26114;&#36149;&#12290;&#22312;&#33258;&#22238;&#24402;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#30001;&#20110;&#27169;&#22411;&#38656;&#35201;&#36845;&#20195;&#22320;&#36816;&#34892;&#25165;&#33021;&#36880;&#20010;&#29983;&#25104;&#26631;&#35760;&#65292;&#22240;&#27492;&#25512;&#29702;&#24310;&#36831;&#26356;&#21152;&#20005;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Big Little Decoder&#65288;BiLD&#65289;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#24212;&#29992;&#30340;&#25512;&#29702;&#25928;&#29575;&#21644;&#24310;&#36831;&#12290;BiLD&#26694;&#26550;&#21253;&#21547;&#20004;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#23427;&#20204;&#21327;&#20316;&#22320;&#29983;&#25104;&#25991;&#26412;&#12290;&#23567;&#22411;&#27169;&#22411;&#33258;&#22238;&#24402;&#22320;&#36816;&#34892;&#20197;&#20302;&#24310;&#36831;&#29983;&#25104;&#25991;&#26412;&#65292;&#22823;&#22411;&#27169;&#22411;&#21482;&#22312;&#38656;&#35201;&#26102;&#20197;&#38750;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#35843;&#25972;&#23567;&#22411;&#27169;&#22411;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#25552;&#39640;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#21644;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28176;&#36827;&#33976;&#39311;&#26426;&#21046;&#65292;&#20351;&#23567;&#22411;&#27169;&#22411;&#36880;&#28176;&#22320;&#20174;&#22823;&#22411;&#27169;&#22411;&#20013;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;BiLD&#26694;&#26550;&#26174;&#33879;&#38477;&#20302;&#20102;&#25512;&#29702;&#24310;&#36831;&#65292;&#21516;&#26102;&#22312;&#20445;&#25345;&#19982;&#22823;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent emergence of Large Language Models based on the Transformer architecture has enabled dramatic advancements in the field of Natural Language Processing. However, these models have long inference latency, which limits their deployment, and which makes them prohibitively expensive for various real-time applications. The inference latency is further exacerbated by autoregressive generative tasks, as models need to run iteratively to generate tokens sequentially without leveraging token-level parallelization. To address this, we propose Big Little Decoder (BiLD), a framework that can improve inference efficiency and latency for a wide range of text generation applications. The BiLD framework contains two models with different sizes that collaboratively generate text. The small model runs autoregressively to generate text with a low inference cost, and the large model is only invoked occasionally to refine the small model's inaccurate predictions in a non-autoregressive manner. To
&lt;/p&gt;</description></item><item><title>&#20044;&#20811;&#20848;&#21361;&#26426;&#24341;&#36215;&#20102;&#27431;&#27954;&#23545;&#31227;&#27665;&#35758;&#39064;&#24577;&#24230;&#30340;&#21464;&#21270;&#65292;&#29305;&#21035;&#26159;&#23545;&#26469;&#33258;&#20044;&#20811;&#20848;&#30340;&#38590;&#27665;&#12290;&#30740;&#31350;&#32773;&#36816;&#29992;&#22810;&#35821;&#35328;&#20998;&#26512;&#25216;&#26415;&#23545;&#26032;&#38395;&#21644;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#30456;&#20851;&#25253;&#36947;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#23545;&#31227;&#27665;&#35758;&#39064;&#35752;&#35770;&#30340;&#37325;&#26500;&#12290;</title><link>http://arxiv.org/abs/2302.02813</link><description>&lt;p&gt;
&#31227;&#27665;&#35758;&#39064;&#30340;&#20877;&#23450;&#20041;&#65311;&#20044;&#20811;&#20848;&#21361;&#26426;&#26399;&#38388;&#27431;&#27954;&#24577;&#24230;&#21464;&#21270;&#30340;&#22810;&#35821;&#35328;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Migration Reframed? A multilingual analysis on the stance shift in Europe during the Ukrainian crisis. (arXiv:2302.02813v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02813
&lt;/p&gt;
&lt;p&gt;
&#20044;&#20811;&#20848;&#21361;&#26426;&#24341;&#36215;&#20102;&#27431;&#27954;&#23545;&#31227;&#27665;&#35758;&#39064;&#24577;&#24230;&#30340;&#21464;&#21270;&#65292;&#29305;&#21035;&#26159;&#23545;&#26469;&#33258;&#20044;&#20811;&#20848;&#30340;&#38590;&#27665;&#12290;&#30740;&#31350;&#32773;&#36816;&#29992;&#22810;&#35821;&#35328;&#20998;&#26512;&#25216;&#26415;&#23545;&#26032;&#38395;&#21644;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#30456;&#20851;&#25253;&#36947;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#23545;&#31227;&#27665;&#35758;&#39064;&#35752;&#35770;&#30340;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20044;&#20811;&#20848;&#25112;&#20105;&#20284;&#20046;&#31215;&#26497;&#25913;&#21464;&#20102;&#27431;&#27954;&#23545;&#31227;&#27665;&#36825;&#19968;&#20851;&#38190;&#31038;&#20250;&#35758;&#39064;&#30340;&#24577;&#24230;&#8212;&#8212;&#33267;&#23569;&#23545;&#26469;&#33258;&#20044;&#20811;&#20848;&#30340;&#38590;&#27665;&#26469;&#35828;&#22914;&#27492;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35813;&#35758;&#39064;&#22312;&#32593;&#32476;&#26032;&#38395;&#21644;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#21453;&#26144;&#65292;&#20197;&#27492;&#23558;&#32593;&#19978;&#30340;&#35758;&#39064;&#34920;&#36798;&#19982;&#31038;&#20250;&#23545;&#20854;&#30340;&#24863;&#30693;&#32852;&#31995;&#36215;&#26469;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32467;&#21512;&#24182;&#25913;&#32534;&#20102;&#39046;&#20808;&#30340;&#33258;&#21160;&#25991;&#26412;&#22788;&#29702;&#25216;&#26415;&#65292;&#37319;&#29992;&#26032;&#22411;&#30340;&#22810;&#35821;&#35328;&#31435;&#22330;&#26816;&#27979;&#26041;&#27861;&#12290;&#20174;2021&#24180;9&#26376;&#24320;&#22987;&#30340;&#19968;&#24180;&#20869;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;565&#23478;&#27431;&#27954;&#26032;&#38395;&#26426;&#26500;&#21457;&#24067;&#30340;550&#19975;&#26465;&#25512;&#29305;&#24086;&#23376;&#21644;&#22238;&#22797;&#65292;&#36827;&#34892;&#20102;&#20851;&#20110;&#31227;&#27665;&#30456;&#20851;&#30340;&#23186;&#20307;&#25253;&#36947;&#21644;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#20114;&#21160;&#30340;&#22810;&#35821;&#35328;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#23454;&#38469;&#19978;&#23384;&#22312;&#19968;&#31181;&#35752;&#35770;&#37325;&#22609;&#65292;&#21487;&#20197;&#36890;&#36807;&#26415;&#35821;&#30340;&#21464;&#21270;&#26469;&#35828;&#26126;&#65292;&#20363;&#22914;&#65292;&#20174;&#8220;&#31227;&#27665;&#8221;&#21040;&#8220;&#38590;&#27665;&#8221;&#65292;&#29978;&#33267;&#24120;&#24120;&#24378;&#35843;&#8220;&#30495;&#27491;&#30340;&#38590;&#27665;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
The war in Ukraine seems to have positively changed the attitude toward the critical societal topic of migration in Europe -- at least towards refugees from Ukraine. We investigate whether this impression is substantiated by how the topic is reflected in online news and social media, thus linking the representation of the issue on the Web to its perception in society. For this purpose, we combine and adapt leading-edge automatic text processing for a novel multilingual stance detection approach. Starting from 5.5M Twitter posts published by 565 European news outlets in one year, beginning September 2021, plus replies, we perform a multilingual analysis of migration-related media coverage and associated social media interaction for Europe and selected European countries.  The results of our analysis show that there is actually a reframing of the discussion illustrated by the terminology change, e.g., from "migrant" to "refugee", often even accentuated with phrases such as "real refugees
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.10410</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#30340;&#19968;&#20010;&#27169;&#22411;&#65306;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#20302;&#36164;&#28304;&#38382;&#39064;&#26159;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#20808;&#21069;&#20856;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#20016;&#23500;&#36164;&#28304;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#33719;&#24471;NER&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#30446;&#26631;&#39046;&#22495;&#12290;&#30001;&#20110;&#19981;&#21516;&#39046;&#22495;&#23454;&#20307;&#31867;&#22411;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#35843;&#25972;&#25152;&#26377;PLMs&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#21482;&#20851;&#27880;&#20110;&#21033;&#29992;&#19968;&#20010;&#26222;&#36890;&#26469;&#28304;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#65292;&#32780;&#26410;&#33021;&#25104;&#21151;&#22320;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;PLM&#24341;&#20837;&#20102;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#36328;&#39046;&#22495;NER(CP-NER)&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#29992;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#32780;&#26080;&#38656;&#32467;&#26500;&#20462;&#25913;&#12290;&#25105;&#20204;&#21033;&#29992;&#20923;&#32467;&#30340;PLMs&#24182;&#36827;&#34892;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. To address these issues, we introduce Collaborative Domain-Prefix Tuning for cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically, we present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications. We utilize frozen PLMs and conduct collaborative domain-prefix tuning
&lt;/p&gt;</description></item><item><title>ODEX&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#22495;&#25191;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;CODEX&#21644;CODEGEN&#20998;&#21035;&#34920;&#29616;&#19981;&#21516;&#30340;&#34892;&#20026;&#12290;ODEx&#23558;&#26377;&#21161;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#20195;&#30721;&#29983;&#25104;&#30340;&#24320;&#25918;&#22495;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.10481</link><description>&lt;p&gt;
&#22522;&#20110;&#25191;&#34892;&#30340;&#26041;&#27861;&#35780;&#20272;&#24320;&#25918;&#22495;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Execution-Based Evaluation for Open-Domain Code Generation. (arXiv:2212.10481v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10481
&lt;/p&gt;
&lt;p&gt;
ODEX&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#22495;&#25191;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;CODEX&#21644;CODEGEN&#20998;&#21035;&#34920;&#29616;&#19981;&#21516;&#30340;&#34892;&#20026;&#12290;ODEx&#23558;&#26377;&#21161;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#20195;&#30721;&#29983;&#25104;&#30340;&#24320;&#25918;&#22495;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23558;&#32534;&#30721;&#26597;&#35810;&#30340;&#33539;&#22260;&#25193;&#23637;&#21040;&#26356;&#21152;&#23454;&#38469;&#30340;&#29615;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ODEx&#65292;&#31532;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#22495;&#25191;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#21040;Python&#20195;&#30721;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;ODEx&#20849;&#26377;945&#20010;NL-Code&#23545;&#65292;&#28085;&#30422;79&#20010;&#19981;&#21516;&#30340;&#24211;&#65292;&#20197;&#21450;1,707&#20010;&#20379;&#25191;&#34892;&#30340;&#20154;&#24037;&#32534;&#20889;&#30340;&#27979;&#35797;&#29992;&#20363;&#12290;&#25105;&#20204;&#20174;StackOverflow&#35770;&#22363;&#33719;&#24471;NL-Code&#23545;&#65292;&#40723;&#21169;&#33258;&#28982;&#21644;&#23454;&#29992;&#30340;&#32534;&#30721;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;ODEx&#25903;&#25345;&#22235;&#31181;&#33258;&#28982;&#35821;&#35328;&#65292;&#21363;&#33521;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#26085;&#35821;&#21644;&#20420;&#35821;&#12290;ODEx&#25581;&#31034;&#20102;&#26368;&#39640;&#25191;&#34892;&#25928;&#26524;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#26377;&#36259;&#34892;&#20026;&#24046;&#24322;&#12290;&#34429;&#28982;CODEX&#30340;&#24635;&#20307;&#32467;&#26524;&#26356;&#22909;&#65292;&#20294;CODEGEN&#36890;&#36807;&#25193;&#23637;&#32780;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24615;&#33021; - CODEGEN 6.1B&#19982;CODEX 12B&#34920;&#29616;&#30456;&#24403;&#12290;&#20004;&#20010;&#27169;&#22411;&#37117;&#26174;&#31034;&#20986;&#24320;&#25918;&#22495;&#21644;&#23553;&#38381;&#22495;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#65292;&#20294;CODEGEN&#24046;&#36317;&#24448;&#24448;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#65292;&#32780;CODEX&#24046;&#36317;&#21017;&#20250;&#22686;&#21152;&#12290;&#25105;&#20204;&#37322;&#25918;ODEx&#20197;&#20419;&#36827;&#23545;&#20195;&#30721;&#29983;&#25104;&#31038;&#21306;&#30340;&#24320;&#25918;&#22495;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
To extend the scope of coding queries to more realistic settings, we propose ODEX, the first Open-Domain EXecution-based natural language (NL) to Python code generation dataset. ODEX has 945 NL-Code pairs spanning 79 diverse libraries, along with 1,707 human-written test cases for execution. Our NL-Code pairs are harvested from StackOverflow forums to encourage natural and practical coding queries. Moreover, ODEX supports four natural languages as intents, in English, Spanish, Japanese, and Russian. ODEX unveils intriguing behavioral differences among top-performing code language models (LM). While CODEX achieves better overall results, CODEGEN improves effectively via scaling -- CODEGEN 6.1B performs comparably with CODEX 12B. Both models show substantial gaps between open and closed domains, but CODEGEN gaps tend to decrease with model size while CODEX gaps increase. We release ODEX to facilitate research into open-domain problems for the code generation community.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RaLP &#26694;&#26550;&#65292;&#20854;&#20351;&#29992;&#25552;&#31034;&#21644;&#26631;&#31614;&#26816;&#32034;&#22686;&#24378;&#21477;&#23376;&#32534;&#30721;&#22120;&#36827;&#34892;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#25551;&#36848;&#19981;&#24403;&#30340;&#26631;&#31614;&#65292;&#21516;&#26102;&#23558;&#25552;&#31034;&#19982;&#36755;&#20837;&#25991;&#26412;&#23884;&#20837;&#30456;&#20284;&#24615;&#30456;&#23545;&#36739;&#39640;&#30340;&#26631;&#31614;&#20851;&#32852;&#65292;&#21462;&#24471;&#20102;&#19982;&#36739;&#22823;&#22522;&#32447;&#31454;&#20105;&#24615;&#33021;&#25110;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10391</link><description>&lt;p&gt;
&#21033;&#29992;&#25552;&#31034;&#21644;&#26631;&#31614;&#26816;&#32034;&#22686;&#24378;&#21477;&#23376;&#32534;&#30721;&#22120;&#36827;&#34892;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Empowering Sentence Encoders with Prompting and Label Retrieval for Zero-shot Text Classification. (arXiv:2212.10391v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RaLP &#26694;&#26550;&#65292;&#20854;&#20351;&#29992;&#25552;&#31034;&#21644;&#26631;&#31614;&#26816;&#32034;&#22686;&#24378;&#21477;&#23376;&#32534;&#30721;&#22120;&#36827;&#34892;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#25551;&#36848;&#19981;&#24403;&#30340;&#26631;&#31614;&#65292;&#21516;&#26102;&#23558;&#25552;&#31034;&#19982;&#36755;&#20837;&#25991;&#26412;&#23884;&#20837;&#30456;&#20284;&#24615;&#30456;&#23545;&#36739;&#39640;&#30340;&#26631;&#31614;&#20851;&#32852;&#65292;&#21462;&#24471;&#20102;&#19982;&#36739;&#22823;&#22522;&#32447;&#31454;&#20105;&#24615;&#33021;&#25110;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#24615;&#39044;&#35757;&#32451;&#65292;&#21477;&#23376;&#32534;&#30721;&#22120;&#36890;&#24120;&#20250;&#20248;&#21270;&#20197;&#20415;&#22312;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#23558;&#35821;&#20041;&#30456;&#20284;&#30340;&#26679;&#26412;&#38752;&#36817;&#24444;&#27492;&#23450;&#20301;&#12290;&#26412;&#25991;&#20851;&#27880;&#20854;&#23884;&#20837;&#31354;&#38388;&#21487;&#20197;&#20415;&#20110;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#35821;&#20041;&#19981;&#21516;&#30340;&#26679;&#26412;&#24050;&#32463;&#34987;&#24456;&#22909;&#22320;&#20998;&#31163;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550; RaLP&#65288;&#20351;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#30340;&#26631;&#31614;&#26816;&#32034;&#22686;&#24378;&#25552;&#31034;&#65289;&#20351;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#23545;&#25552;&#31034;&#26631;&#31614;&#20505;&#36873;&#36827;&#34892;&#32534;&#30721;&#65292;&#28982;&#21518;&#23558;&#20855;&#26377;&#26368;&#39640;&#30456;&#20284;&#24615;&#30340;&#25552;&#31034;&#23884;&#20837;&#19982;&#36755;&#20837;&#25991;&#26412;&#23884;&#20837;&#20851;&#32852;&#20026;&#26631;&#31614;&#12290;&#20026;&#20102;&#34917;&#20607;&#20854;&#21407;&#22987;&#26684;&#24335;&#20013;&#21487;&#33021;&#25551;&#36848;&#19981;&#24403;&#30340;&#26631;&#31614;&#65292;RaLP&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#19982;&#21407;&#22987;&#26631;&#31614;&#25552;&#31034;&#35821;&#20041;&#30456;&#20284;&#30340;&#21477;&#23376;&#65292;&#24182;&#23558;&#23427;&#20204;&#29992;&#20316;&#38468;&#21152;&#30340;&#20266;&#26631;&#31614;&#25552;&#31034;&#12290;RaLP&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#30340;&#21508;&#31181;&#23553;&#38381;&#38598;&#20998;&#31867;&#21644;&#22810;&#36873;&#39064; QA &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33267;&#23569;&#19982;&#36739;&#22823;&#22522;&#32447;&#31454;&#20105;&#24615;&#33021;&#25110;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With contrastive pre-training, sentence encoders are generally optimized to locate semantically similar samples closer to each other in their embedding spaces. In this work, we focus on the potential of their embedding spaces to be readily adapted to zero-shot text classification, as semantically distinct samples are already well-separated. Our framework, RaLP (Retrieval augmented Label Prompts for sentence encoder), encodes prompted label candidates with a sentence encoder, then assigns the label whose prompt embedding has the highest similarity with the input text embedding. In order to compensate for the potentially poorly descriptive labels in their original format, RaLP retrieves sentences that are semantically similar to the original label prompt from external corpora and use them as additional pseudo-label prompts. RaLP achieves competitive or stronger performance than much larger baselines on various closed-set classification and multiple-choice QA datasets under zero-shot sett
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#25991;&#26412;&#29983;&#25104;&#35780;&#20215;&#25351;&#26631;&#30340;&#40065;&#26834;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#65292;&#21457;&#29616;&#29616;&#26377;&#35780;&#20215;&#25351;&#26631;&#23384;&#22312;&#19968;&#20123;&#30450;&#28857;&#21644;&#20559;&#35265;&#65292;&#20363;&#22914;BERTScore&#23545;&#25688;&#35201;&#20013;&#30340;&#25130;&#26029;&#35823;&#24046;&#26080;&#27861;&#24456;&#22909;&#22320;&#22788;&#29702;&#65292;MAUVE&#23545;&#20110;&#29983;&#25104;&#30340;&#24320;&#22836;&#25110;&#20013;&#38388;&#30340;&#35823;&#24046;&#19981;&#25935;&#24863;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#23454;&#29616;&#26356;&#21487;&#38752;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20215;&#12290;</title><link>http://arxiv.org/abs/2212.10020</link><description>&lt;p&gt;
&#35770;&#22522;&#20110;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20215;&#25351;&#26631;&#30340;&#30450;&#28857;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Blind Spots of Model-Based Evaluation Metrics for Text Generation. (arXiv:2212.10020v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#25991;&#26412;&#29983;&#25104;&#35780;&#20215;&#25351;&#26631;&#30340;&#40065;&#26834;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#65292;&#21457;&#29616;&#29616;&#26377;&#35780;&#20215;&#25351;&#26631;&#23384;&#22312;&#19968;&#20123;&#30450;&#28857;&#21644;&#20559;&#35265;&#65292;&#20363;&#22914;BERTScore&#23545;&#25688;&#35201;&#20013;&#30340;&#25130;&#26029;&#35823;&#24046;&#26080;&#27861;&#24456;&#22909;&#22320;&#22788;&#29702;&#65292;MAUVE&#23545;&#20110;&#29983;&#25104;&#30340;&#24320;&#22836;&#25110;&#20013;&#38388;&#30340;&#35823;&#24046;&#19981;&#25935;&#24863;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#23454;&#29616;&#26356;&#21487;&#38752;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26377;&#29992;&#20294;&#24120;&#24120;&#34987;&#24573;&#30053;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20215;&#25351;&#26631;&#40065;&#26834;&#24615;&#20998;&#26512;&#26041;&#27861;&#65306;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#12290;&#25105;&#20204;&#38543;&#26426;&#35774;&#35745;&#24182;&#21512;&#25104;&#20102;&#21508;&#31181;&#21487;&#33021;&#30340;&#35823;&#24046;&#65292;&#24182;&#26816;&#26597;&#23427;&#20204;&#26159;&#21542;&#20250;&#23548;&#33268;&#35780;&#20215;&#25351;&#26631;&#20998;&#25968;&#30340;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#31995;&#21015;&#26368;&#26032;&#35780;&#20215;&#25351;&#26631;&#65292;&#29992;&#20110;&#24320;&#25918;&#24335;&#29983;&#25104;&#12289;&#32763;&#35793;&#21644;&#25688;&#35201;&#31561;&#20219;&#21153;&#12290;&#23454;&#39564;&#25581;&#31034;&#20102;&#29616;&#26377;&#35780;&#20215;&#25351;&#26631;&#20013;&#26377;&#36259;&#30340;&#19981;&#25935;&#24863;&#12289;&#20559;&#35265;&#12289;&#29978;&#33267;&#28431;&#27934;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;BERTScore&#23545;&#25688;&#35201;&#20013;&#30340;&#25130;&#26029;&#35823;&#24046;&#24863;&#21040;&#22256;&#24785;&#65292;&#32780;&#22312;&#29983;&#25104;&#30340;&#24320;&#22836;&#25110;&#20013;&#38388;&#23384;&#22312;&#35823;&#24046;&#26102;MAUVE&#65288;&#22522;&#20110;GPT-2&#65289;&#21017;&#19981;&#25935;&#24863;&#12290;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#30450;&#28857;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#26356;&#21487;&#38752;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20215;&#12290;&#25105;&#20204;&#24050;&#22312;https://github.com/cloudygoose/blindspot_nlg &#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. Basically, we design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric scores. We examine a range of recently proposed evaluation metrics based on pretrained language models, for the tasks of open-ended generation, translation, and summarization. Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics. For example, we find that BERTScore is confused by truncation errors in summarization, and MAUVE (built on top of GPT-2) is insensitive to errors at the beginning or middle of generations. Further, we investigate the reasons behind these blind spots and suggest practical workarounds for a more reliable evaluation of text generation. We have released our code and data at https://github.com/cloudygoose/blindspot_nlg.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FARM&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#29983;&#25104;&#33021;&#22815;&#34987;&#20449;&#20219;&#30340;&#21407;&#29702;&#65292;&#35299;&#20915;&#20102;&#19981;&#23433;&#20840;&#25991;&#26412;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#24110;&#21161;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#20445;&#38556;&#28040;&#36153;&#32773;&#30340;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2212.09667</link><description>&lt;p&gt;
&#27880;&#37325;&#35270;&#35273;&#12289;&#23646;&#24615;&#21644;&#29702;&#24615;&#65306;&#36808;&#21521;&#29289;&#29702;&#23433;&#20840;&#21644;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy AI. (arXiv:2212.09667v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09667
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FARM&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#29983;&#25104;&#33021;&#22815;&#34987;&#20449;&#20219;&#30340;&#21407;&#29702;&#65292;&#35299;&#20915;&#20102;&#19981;&#23433;&#20840;&#25991;&#26412;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#24110;&#21161;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#20445;&#38556;&#28040;&#36153;&#32773;&#30340;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#31995;&#32479;&#24066;&#22330;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#29992;&#25143;&#30340;&#36523;&#20307;&#23433;&#20840;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#19981;&#21463;&#38480;&#21046;&#30340;&#31995;&#32479;&#21487;&#33021;&#20250;&#21521;&#29992;&#25143;&#25512;&#33616;&#21361;&#38505;&#30340;&#34892;&#20026;&#65292;&#23548;&#33268;&#20005;&#37325;&#30340;&#20260;&#23475;&#12290;&#38544;&#34109;&#30340;&#19981;&#23433;&#20840;&#25991;&#26412;&#26159;&#19968;&#20010;&#29305;&#21035;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#25991;&#26412;&#21487;&#33021;&#20250;&#20986;&#29616;&#22312;&#26085;&#24120;&#22330;&#26223;&#20013;&#65292;&#24182;&#19988;&#24456;&#38590;&#34987;&#26816;&#27979;&#20026;&#26377;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FARM&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#22312;&#23433;&#20840;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#21487;&#20449;&#30340;&#21407;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FARM&#27880;&#37325;&#20110;&#32570;&#22833;&#30340;&#30693;&#35782;&#65292;&#20197;&#30830;&#35748;&#22312;&#29305;&#23450;&#24773;&#22659;&#20013;&#36827;&#34892;&#25512;&#29702;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#21487;&#20449;&#28304;&#36827;&#34892;&#24402;&#22240;&#20197;&#33719;&#21462;&#27492;&#20449;&#24687;&#12290;&#36825;&#20123;&#30693;&#35782;&#29992;&#20110;&#20998;&#31867;&#21407;&#22987;&#25991;&#26412;&#30340;&#23433;&#20840;&#24615;&#24182;&#29983;&#25104;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21407;&#29702;&#65292;&#25581;&#31034;&#31995;&#32479;&#23545;&#29305;&#23450;&#29992;&#25143;&#32676;&#20307;&#30340;&#39118;&#38505;&#65292;&#24182;&#24110;&#21161;&#21033;&#30410;&#30456;&#20851;&#32773;&#31649;&#29702;&#20854;&#31995;&#32479;&#30340;&#39118;&#38505;&#65292;&#24110;&#21161;&#25919;&#31574;&#21046;&#23450;&#32773;&#20026;&#28040;&#36153;&#32773;&#23433;&#20840;&#25552;&#20379;&#20855;&#20307;&#30340;&#20445;&#38556;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FARM&#22312;&#35782;&#21035;&#19981;&#23433;&#20840;&#25991;&#26412;&#21644;&#29983;&#25104;&#21487;&#20449;&#30340;&#21407;&#29702;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Users' physical safety is an increasing concern as the market for intelligent systems continues to grow, where unconstrained systems may recommend users dangerous actions that can lead to serious injury. Covertly unsafe text is an area of particular interest, as such text may arise from everyday scenarios and are challenging to detect as harmful. We propose FARM, a novel framework leveraging external knowledge for trustworthy rationale generation in the context of safety. In particular, FARM foveates on missing knowledge to qualify the information required to reason in specific scenarios and retrieves this information with attribution to trustworthy sources. This knowledge is used to both classify the safety of the original text and generate human-interpretable rationales, shedding light on the risk of systems to specific user groups and helping both stakeholders manage the risks of their systems and policymakers to provide concrete safeguards for consumer safety. Our experiments show 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#20013;&#30340;&#27169;&#31946;&#21644;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#26126;&#20102;&#30340;&#21453;&#20107;&#23454;&#31034;&#20363;&#29983;&#25104;&#26041;&#27861;&#21644;&#24369;&#30417;&#30563;&#30340;&#38169;&#35823;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2212.08902</link><description>&lt;p&gt;
&#22788;&#29702;&#27169;&#31946;&#21644;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#65306;&#38024;&#23545;&#25991;&#26412;&#21040;SQL&#30340;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Know What I don't Know: Handling Ambiguous and Unanswerable Questions for Text-to-SQL. (arXiv:2212.08902v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#20013;&#30340;&#27169;&#31946;&#21644;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#26126;&#20102;&#30340;&#21453;&#20107;&#23454;&#31034;&#20363;&#29983;&#25104;&#26041;&#27861;&#21644;&#24369;&#30417;&#30563;&#30340;&#38169;&#35823;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;SQL&#30340;&#20219;&#21153;&#26088;&#22312;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;&#30456;&#24212;&#30340;SQL&#26597;&#35810;&#65292;&#20174;&#32780;&#22312;&#20851;&#31995;&#34920;&#30340;&#19978;&#19979;&#25991;&#20013;&#23436;&#25104;&#26597;&#35810;&#12290;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#22120;&#21487;&#20197;&#20026;&#20219;&#24847;&#29992;&#25143;&#38382;&#39064;&#29983;&#25104;&#8220;&#21512;&#29702;&#8221;&#30340;SQL&#26597;&#35810;&#65292;&#20294;&#36825;&#26679;&#20250;&#26080;&#27861;&#27491;&#30830;&#22788;&#29702;&#20855;&#26377;&#38382;&#39064;&#30340;&#29992;&#25143;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#25991;&#26412;&#21040;SQL&#20013;&#35266;&#23519;&#21040;&#30340;&#27169;&#31946;&#21644;&#26080;&#27861;&#22238;&#31572;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#65292;&#24182;&#23558;&#20854;&#24635;&#32467;&#20026;6&#20010;&#29305;&#24449;&#31867;&#21035;&#12290;&#30456;&#24212;&#22320;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#27599;&#20010;&#31867;&#21035;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#27169;&#31946;&#21644;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#30340;&#35201;&#27714;&#12290;&#22312;&#36827;&#34892;&#36825;&#39033;&#30740;&#31350;&#20043;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21453;&#20107;&#23454;&#30340;&#31034;&#20363;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#27169;&#31946;&#21644;&#26080;&#27861;&#22238;&#31572;&#30340;&#25991;&#26412;&#21040;SQL&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#30340;DTE&#65288;&#26816;&#27979;-&#28982;&#21518;&#35299;&#37322;&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#38169;&#35823;&#26816;&#27979;&#65292;&#26412;&#22320;&#21270;&#21644;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of text-to-SQL aims to convert a natural language question into its corresponding SQL query within the context of relational tables. Existing text-to-SQL parsers generate a "plausible" SQL query for an arbitrary user question, thereby failing to correctly handle problematic user questions. To formalize this problem, we conduct a preliminary study on the observed ambiguous and unanswerable cases in text-to-SQL and summarize them into 6 feature categories. Correspondingly, we identify the causes behind each category and propose requirements for handling ambiguous and unanswerable questions. Following this study, we propose a simple yet effective counterfactual example generation approach that automatically produces ambiguous and unanswerable text-to-SQL examples. Furthermore, we propose a weakly supervised DTE (Detecting-Then-Explaining) model for error detection, localization, and explanation. Experimental results show that our model achieves the best result on both real-world 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#31350;&#20102;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#35797;&#39564;&#21457;&#29616;&#20351;&#29992;&#19981;&#21040;10&#20159;&#21442;&#25968;&#30340;&#26631;&#20934;Transformer&#37197;&#32622;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#24178;&#25200;&#24182;&#20419;&#36827;&#21327;&#21516;&#65292;&#21516;&#26102;&#21457;&#29616;&#35843;&#25972;&#37319;&#26679;&#28201;&#24230;&#20197;&#25511;&#21046;&#25968;&#25454;&#20013;&#27599;&#20010;&#35821;&#35328;&#23545;&#25152;&#21344;&#27604;&#20363;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#35821;&#35328;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2212.07530</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#21407;&#22240;&#21644;&#35299;&#20915;&#26041;&#27861;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Causes and Cures for Interference in Multilingual Translation. (arXiv:2212.07530v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07530
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#31350;&#20102;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#35797;&#39564;&#21457;&#29616;&#20351;&#29992;&#19981;&#21040;10&#20159;&#21442;&#25968;&#30340;&#26631;&#20934;Transformer&#37197;&#32622;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#24178;&#25200;&#24182;&#20419;&#36827;&#21327;&#21516;&#65292;&#21516;&#26102;&#21457;&#29616;&#35843;&#25972;&#37319;&#26679;&#28201;&#24230;&#20197;&#25511;&#21046;&#25968;&#25454;&#20013;&#27599;&#20010;&#35821;&#35328;&#23545;&#25152;&#21344;&#27604;&#20363;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#35821;&#35328;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21487;&#20197;&#20174;&#19981;&#21516;&#35821;&#35328;&#23545;&#20043;&#38388;&#30340;&#21327;&#21516;&#20013;&#33719;&#30410;&#65292;&#20294;&#21516;&#26102;&#20063;&#20250;&#21463;&#21040;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#30446;&#21069;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#20808;&#36827;&#26041;&#27861;&#26088;&#22312;&#28040;&#38500;&#24178;&#25200;&#65292;&#20294;&#25105;&#20204;&#23545;&#24178;&#25200;&#29616;&#35937;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#23548;&#33268;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#21270;&#35797;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#24178;&#25200;&#65288;&#25110;&#21327;&#21516;&#65289;&#20027;&#35201;&#30001;&#27169;&#22411;&#22823;&#23567;&#12289;&#25968;&#25454;&#22823;&#23567;&#21644;&#27599;&#20010;&#35821;&#35328;&#23545;&#22312;&#24635;&#25968;&#25454;&#38598;&#20013;&#25152;&#21344;&#27604;&#20363;&#26469;&#20915;&#23450;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24403;&#27169;&#22411;&#30456;&#23545;&#20110;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#38750;&#24120;&#23567;&#30340;&#26102;&#20505;&#65292;&#20250;&#20986;&#29616;&#20005;&#37325;&#30340;&#24178;&#25200;&#65292;&#32780;&#20351;&#29992;&#19981;&#21040;10&#20159;&#21442;&#25968;&#30340;&#26631;&#20934;Transformer&#37197;&#32622;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#24178;&#25200;&#24182;&#20419;&#36827;&#21327;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#35843;&#25972;&#37319;&#26679;&#28201;&#24230;&#20197;&#25511;&#21046;&#25968;&#25454;&#20013;&#27599;&#20010;&#35821;&#35328;&#23545;&#25152;&#21344;&#27604;&#20363;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#35821;&#35328;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual machine translation models can benefit from synergy between different language pairs, but also suffer from interference. While there is a growing number of sophisticated methods that aim to eliminate interference, our understanding of interference as a phenomenon is still limited. This work identifies the main factors that contribute to interference in multilingual machine translation. Through systematic experimentation, we find that interference (or synergy) are primarily determined by model size, data size, and the proportion of each language pair within the total dataset. We observe that substantial interference occurs mainly when the model is very small with respect to the available training data, and that using standard transformer configurations with less than one billion parameters largely alleviates interference and promotes synergy. Moreover, we show that tuning the sampling temperature to control the proportion of each language pair in the data is key to balancin
&lt;/p&gt;</description></item><item><title>ERNIE-Code&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;116&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;6&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#36328;&#24230;&#25439;&#22351;&#35821;&#35328;&#24314;&#27169;&#21644;&#22522;&#20110;&#26725;&#25509;&#30340;&#32763;&#35793;&#35821;&#35328;&#24314;&#27169;&#20004;&#31181;&#36328;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#20195;&#30721;&#26234;&#33021;&#32456;&#31471;&#20219;&#21153;&#20013;&#20248;&#20110;&#20197;&#21069;&#30340;&#22810;&#35821;&#35328;LLMs&#12290;</title><link>http://arxiv.org/abs/2212.06742</link><description>&lt;p&gt;
ERNIE-Code: &#36229;&#36234;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#36328;&#35821;&#35328;&#32534;&#31243;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages. (arXiv:2212.06742v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06742
&lt;/p&gt;
&lt;p&gt;
ERNIE-Code&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;116&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;6&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#36328;&#24230;&#25439;&#22351;&#35821;&#35328;&#24314;&#27169;&#21644;&#22522;&#20110;&#26725;&#25509;&#30340;&#32763;&#35793;&#35821;&#35328;&#24314;&#27169;&#20004;&#31181;&#36328;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#20195;&#30721;&#26234;&#33021;&#32456;&#31471;&#20219;&#21153;&#20013;&#20248;&#20110;&#20197;&#21069;&#30340;&#22810;&#35821;&#35328;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24037;&#31243;&#24072;&#20351;&#29992;&#21516;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#21487;&#33021;&#20351;&#29992;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#65292;&#36825;&#20250;&#23548;&#33268;&#27807;&#36890;&#21644;&#24037;&#20316;&#25928;&#29575;&#30340;&#24040;&#22823;&#38556;&#30861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35745;&#31639;&#26426;&#31243;&#24207;&#20013;&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26159;&#26377;&#25928;&#30340;&#65292;&#28982;&#32780;&#23427;&#20204;&#24635;&#26159;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#36808;&#21521;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24314;&#31435;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#21644;&#22810;&#35821;&#35328;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#26725;&#26753;&#30340;&#19968;&#27493;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;ERNIE-Code&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;116&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;6&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#37319;&#29992;&#20004;&#31181;&#26222;&#36941;&#30340;&#36328;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65306;&#36328;&#24230;&#25439;&#22351;&#35821;&#35328;&#24314;&#27169;&#20174;&#21333;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#25110;&#32534;&#31243;&#35821;&#35328;&#20013;&#23398;&#20064;&#27169;&#24335;&#65307;&#22522;&#20110;&#26725;&#25509;&#30340;&#32763;&#35793;&#35821;&#35328;&#24314;&#27169;&#20381;&#38752;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#30340;&#24179;&#34892;&#25968;&#25454;&#12290;&#24191;&#27867;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ERNIE-Code&#22312;&#20195;&#30721;&#26234;&#33021;&#30340;&#24191;&#27867;&#32456;&#31471;&#20219;&#21153;&#20013;&#20248;&#20110;&#20197;&#21069;&#30340;&#22810;&#35821;&#35328;LLMs&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;&#20195;&#30721;&#21040;&#25991;&#26412;&#65292;&#25991;&#26412;&#21040;&#20195;&#30721;&#65292;&#20195;&#30721;&#21040;&#20195;&#30721;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software engineers working with the same programming language (PL) may speak different natural languages (NLs) and vice versa, erecting huge barriers to communication and working efficiency. Recent studies have demonstrated the effectiveness of generative pre-training in computer programs, yet they are always English-centric. In this work, we step towards bridging the gap between multilingual NLs and multilingual PLs for large language models (LLMs). We release ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs. We employ two methods for universal cross-lingual pre-training: span-corruption language modeling that learns patterns from monolingual NL or PL; and pivot-based translation language modeling that relies on parallel data of many NLs and PLs. Extensive results show that ERNIE-Code outperforms previous multilingual LLMs for PL or NL across a wide range of end tasks of code intelligence, including multilingual code-to-text, text-to-code, code-to-code, and text-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;15&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#20154;&#26684;&#35745;&#31639;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#20154;&#26684;&#20998;&#31867;&#27861;&#65292;&#27979;&#37327;&#36136;&#37327;&#65292;&#25968;&#25454;&#38598;&#65292;&#24615;&#33021;&#35780;&#20272;&#65292;&#24314;&#27169;&#36873;&#25321;&#20197;&#21450;&#36947;&#24503;&#21644;&#20844;&#24179;&#24615;&#65292;&#26088;&#22312;&#28608;&#21457;&#26356;&#22810;&#30340;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;TPC&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2212.06711</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#20154;&#26684;&#35745;&#31639;&#65306;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
On Text-based Personality Computing: Challenges and Future Directions. (arXiv:2212.06711v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;15&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#20154;&#26684;&#35745;&#31639;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#20154;&#26684;&#20998;&#31867;&#27861;&#65292;&#27979;&#37327;&#36136;&#37327;&#65292;&#25968;&#25454;&#38598;&#65292;&#24615;&#33021;&#35780;&#20272;&#65292;&#24314;&#27169;&#36873;&#25321;&#20197;&#21450;&#36947;&#24503;&#21644;&#20844;&#24179;&#24615;&#65292;&#26088;&#22312;&#28608;&#21457;&#26356;&#22810;&#30340;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;TPC&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#20154;&#26684;&#35745;&#31639;&#65288;TPC&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#24341;&#36215;&#20102;&#35768;&#22810;&#30740;&#31350;&#20852;&#36259;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;15&#20010;&#25361;&#25112;&#65292;&#20540;&#24471;&#30740;&#31350;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#36825;&#20123;&#25361;&#25112;&#25353;&#20197;&#19979;&#20027;&#39064;&#32452;&#32455;&#65306;&#20154;&#26684;&#20998;&#31867;&#27861;&#12289;&#27979;&#37327;&#36136;&#37327;&#12289;&#25968;&#25454;&#38598;&#12289;&#24615;&#33021;&#35780;&#20272;&#12289;&#24314;&#27169;&#36873;&#25321;&#20197;&#21450;&#36947;&#24503;&#21644;&#20844;&#24179;&#24615;&#12290;&#22312;&#24212;&#23545;&#27599;&#20010;&#25361;&#25112;&#26102;&#65292;&#25105;&#20204;&#19981;&#20165;&#32467;&#21512;&#20102;NLP&#21644;&#31038;&#20250;&#31185;&#23398;&#30340;&#35270;&#35282;&#65292;&#36824;&#25552;&#20379;&#20855;&#20307;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#24076;&#26395;&#28608;&#21457;&#26356;&#22810;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;TPC&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based personality computing (TPC) has gained many research interests in NLP. In this paper, we describe 15 challenges that we consider deserving the attention of the research community. These challenges are organized by the following topics: personality taxonomies, measurement quality, datasets, performance evaluation, modelling choices, as well as ethics and fairness. When addressing each challenge, not only do we combine perspectives from both NLP and social sciences, but also offer concrete suggestions. We hope to inspire more valid and reliable TPC research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21253;SODA&#65292;&#21487;&#29992;&#20110;&#25552;&#21462;&#30284;&#30151;&#24739;&#32773;&#30340;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#12290;&#35813;&#21253;&#22312;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#20197;&#29992;&#20110;&#26032;&#30340;&#30142;&#30149;&#39046;&#22495;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#21253;&#22312;&#30284;&#30151;&#20154;&#32676;&#20013;&#25552;&#21462;SDoH&#30340;&#25552;&#21462;&#29575;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2212.03000</link><description>&lt;p&gt;
SODA&#65306;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21253;&#65292;&#29992;&#20110;&#25552;&#21462;&#30284;&#30151;&#30740;&#31350;&#20013;&#30340;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
SODA: A Natural Language Processing Package to Extract Social Determinants of Health for Cancer Studies. (arXiv:2212.03000v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21253;SODA&#65292;&#21487;&#29992;&#20110;&#25552;&#21462;&#30284;&#30151;&#24739;&#32773;&#30340;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#12290;&#35813;&#21253;&#22312;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#20197;&#29992;&#20110;&#26032;&#30340;&#30142;&#30149;&#39046;&#22495;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#21253;&#22312;&#30284;&#30151;&#20154;&#32676;&#20013;&#25552;&#21462;SDoH&#30340;&#25552;&#21462;&#29575;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SODA&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21253;&#65292;&#20854;&#20013;&#21547;&#26377;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#25552;&#21462;&#30284;&#30151;&#24739;&#32773;&#30340;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#65288;SDoH&#65289;&#65292;&#24182;&#26816;&#39564;&#20102;SODA&#22312;&#26032;&#30340;&#30142;&#30149;&#39046;&#22495;&#65288;&#22914;&#20351;&#29992;&#38463;&#29255;&#31867;&#33647;&#29289;&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#30284;&#30151;&#20154;&#32676;&#20013;&#25552;&#21462;SDoH&#30340;&#25552;&#21462;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: We aim to develop an open-source natural language processing (NLP) package, SODA (i.e., SOcial DeterminAnts), with pre-trained transformer models to extract social determinants of health (SDoH) for cancer patients, examine the generalizability of SODA to a new disease domain (i.e., opioid use), and evaluate the extraction rate of SDoH using cancer populations.  Methods: We identified SDoH categories and attributes and developed an SDoH corpus using clinical notes from a general cancer cohort. We compared four transformer-based NLP models to extract SDoH, examined the generalizability of NLP models to a cohort of patients prescribed with opioids, and explored customization strategies to improve performance. We applied the best NLP model to extract 19 categories of SDoH from the breast (n=7,971), lung (n=11,804), and colorectal cancer (n=6,240) cohorts.  Results and Conclusion: We developed a corpus of 629 cancer patients notes with annotations of 13,193 SDoH concepts/attribut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#24847;&#22270;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#32858;&#31867;&#31639;&#27861;&#21644;&#29992;&#25143;&#35805;&#35821;&#23884;&#20837;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;MiniLM&#19982;&#23618;&#27425;&#32858;&#31867;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.02021</link><description>&lt;p&gt;
&#19982;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#24847;&#22270;&#35782;&#21035;&#30456;&#20851;&#30340;&#35805;&#35821;&#23884;&#20837;&#21644;&#32858;&#31867;&#26041;&#27861;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Utterance Embeddings and Clustering Methods Related to Intent Induction for Task-Oriented Dialogue. (arXiv:2212.02021v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#24847;&#22270;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#32858;&#31867;&#31639;&#27861;&#21644;&#29992;&#25143;&#35805;&#35821;&#23884;&#20837;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;MiniLM&#19982;&#23618;&#27425;&#32858;&#31867;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#35774;&#35745;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#22270;&#35889;&#20013;&#30340;&#20856;&#22411;&#25361;&#25112;&#65306;&#20026;&#27599;&#20010;&#23545;&#35805;&#36716;&#25240;&#25351;&#23450;&#24847;&#22270;&#26631;&#31614;&#65288;&#24847;&#22270;&#32858;&#31867;&#65289;&#24182;&#22522;&#20110;&#24847;&#22270;&#32858;&#31867;&#26041;&#27861;&#29983;&#25104;&#19968;&#32452;&#24847;&#22270;&#65288;&#24847;&#22270;&#24402;&#32435;&#65289;&#12290;&#25105;&#20204;&#20551;&#35774;&#33258;&#21160;&#24402;&#32435;&#24847;&#22270;&#26377;&#20004;&#20010;&#26174;&#33879;&#22240;&#32032;&#65306;&#65288;1&#65289;&#24847;&#22270;&#26631;&#31614;&#30340;&#32858;&#31867;&#31639;&#27861;&#21644;&#65288;2&#65289;&#29992;&#25143;&#35805;&#35821;&#23884;&#20837;&#31354;&#38388;&#12290; &#25105;&#20204;&#26681;&#25454;DSTC11&#35780;&#20272;&#27604;&#36739;&#20102;&#29616;&#26377;&#30340;&#25104;&#21697;&#32858;&#31867;&#27169;&#22411;&#21644;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35748;&#30495;&#32771;&#34385;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#20013;&#35805;&#35821;&#23884;&#20837;&#21644;&#32858;&#31867;&#26041;&#27861;&#30340;&#32508;&#21512;&#36873;&#25321;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;MiniLM&#19982;&#23618;&#27425;&#32858;&#31867;&#30456;&#32467;&#21512;&#21487;&#26174;&#33879;&#25552;&#39640;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#20013;&#30340;NMI&#65292;ARI&#65292;F1&#65292;&#20934;&#30830;&#24615;&#21644;&#31034;&#20363;&#35206;&#30422;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/Jeiyoon/dstc11-track2&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The focus of this work is to investigate unsupervised approaches to overcome quintessential challenges in designing task-oriented dialog schema: assigning intent labels to each dialog turn (intent clustering) and generating a set of intents based on the intent clustering methods (intent induction). We postulate there are two salient factors for automatic induction of intents: (1) clustering algorithm for intent labeling and (2) user utterance embedding space. We compare existing off-the-shelf clustering models and embeddings based on DSTC11 evaluation. Our extensive experiments demonstrate that the combined selection of utterance embedding and clustering method in the intent induction task should be carefully considered. We also present that pretrained MiniLM with Agglomerative clustering shows significant improvement in NMI, ARI, F1, accuracy and example coverage in intent induction tasks. The source codes are available at https://github.com/Jeiyoon/dstc11-track2.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#26426;&#35299;&#30721;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#26356;&#25913;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#26356;&#24555;&#22320;&#20174;&#22823;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;&#22914;Transformer&#65289;&#20013;&#37319;&#26679;&#65292;&#21152;&#36895;&#20102;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#36827;&#34892;&#26550;&#26500;&#26356;&#25913;&#12290;</title><link>http://arxiv.org/abs/2211.17192</link><description>&lt;p&gt;
&#22522;&#20110;&#25237;&#26426;&#35299;&#30721;&#30340;Transformer&#24555;&#36895;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Fast Inference from Transformers via Speculative Decoding. (arXiv:2211.17192v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#26426;&#35299;&#30721;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#26356;&#25913;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#26356;&#24555;&#22320;&#20174;&#22823;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;&#22914;Transformer&#65289;&#20013;&#37319;&#26679;&#65292;&#21152;&#36895;&#20102;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#36827;&#34892;&#26550;&#26500;&#26356;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;Transformer&#31561;&#22823;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#36827;&#34892;&#25512;&#29702;&#26159;&#32531;&#24930;&#30340;&#65292;&#22240;&#20026;&#35299;&#30721;K&#20010;&#26631;&#35760;&#38656;&#35201;&#36816;&#34892;K&#27425;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#25237;&#26426;&#35299;&#30721;&#8221;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#26356;&#24555;&#22320;&#20174;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#37319;&#26679;&#65292;&#36890;&#36807;&#24182;&#34892;&#35745;&#31639;&#22810;&#20010;&#26631;&#35760;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#36827;&#34892;&#26550;&#26500;&#26356;&#25913;&#12290;&#25105;&#20204;&#22312;T5-XXL&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#24182;&#26174;&#31034;&#30456;&#23545;&#20110;&#26631;&#20934;T5X&#23454;&#29616;&#65292;&#20854;&#21152;&#36895;&#20102;2X-3X&#65292;&#36755;&#20986;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference from large autoregressive models like Transformers is slow decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VATLM&#30340;&#32479;&#19968;&#36328;&#27169;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#35270;&#21548;&#25991;&#26412;&#36164;&#26009;&#30340;&#39044;&#22788;&#29702;&#19982;&#19968;&#31181;&#32479;&#19968;&#30340;&#36974;&#34109;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#36798;&#21040;&#20248;&#31168;&#30340;&#32852;&#21512;&#22810;&#27169;&#24577;&#34920;&#31034;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.11275</link><description>&lt;p&gt;
VATLM: &#20351;&#29992;&#32479;&#19968;&#30340;&#36974;&#34109;&#39044;&#27979;&#36827;&#34892;&#35270;&#21548;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning. (arXiv:2211.11275v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VATLM&#30340;&#32479;&#19968;&#36328;&#27169;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#35270;&#21548;&#25991;&#26412;&#36164;&#26009;&#30340;&#39044;&#22788;&#29702;&#19982;&#19968;&#31181;&#32479;&#19968;&#30340;&#36974;&#34109;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#36798;&#21040;&#20248;&#31168;&#30340;&#32852;&#21512;&#22810;&#27169;&#24577;&#34920;&#31034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#38899;&#26159;&#20154;&#31867;&#19982;&#22806;&#30028;&#20132;&#27969;&#30340;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#20294;&#26356;&#30495;&#23454;&#30340;&#35821;&#38899;&#20132;&#20114;&#21253;&#21547;&#22810;&#27169;&#24335;&#20449;&#24687;&#65292;&#20363;&#22914;&#35270;&#35273;&#12289;&#25991;&#26412;&#12290;&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#25972;&#21512;&#19981;&#21516;&#30340;&#27169;&#24577;&#20449;&#24687;&#65292;&#21033;&#29992;&#19981;&#21516;&#30340;&#36164;&#28304;&#65288;&#20363;&#22914;&#35270;&#21548;&#23545;&#12289;&#38899;&#39057;&#25991;&#26412;&#23545;&#12289;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#21644;&#26410;&#26631;&#35760;&#30340;&#25991;&#26412;&#65289;&#20419;&#36827;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#36824;&#27809;&#26377;&#34987;&#24456;&#22909;&#22320;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#36328;&#27169;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;VATLM&#65288;Visual-Audio-Text&#35821;&#35328;&#27169;&#22411;&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;VATLM&#37319;&#29992;&#32479;&#19968;&#30340;&#39592;&#24178;&#32593;&#32476;&#26469;&#24314;&#27169;&#27169;&#24577;&#29420;&#31435;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#19977;&#20010;&#31616;&#21333;&#30340;&#27169;&#24577;&#20381;&#36182;&#27169;&#22359;&#23545;&#35270;&#35273;&#12289;&#35821;&#38899;&#21644;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;&#20026;&#20102;&#23558;&#36825;&#19977;&#31181;&#27169;&#24577;&#38598;&#25104;&#21040;&#19968;&#20010;&#20849;&#20139;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;VATLM&#20351;&#29992;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#32479;&#19968;&#20998;&#35789;&#22120;&#32473;&#20986;&#30340;&#32479;&#19968;&#20196;&#29260;&#30340;&#36974;&#34109;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;&#38899;&#39057;-&#35270;&#35273;&#26816;&#32034;&#21644;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#39044;&#35757;&#32451;&#30340;VATLM&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#22312;&#23398;&#20064;&#32852;&#21512;&#22810;&#27169;&#24577;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although speech is a simple and effective way for humans to communicate with the outside world, a more realistic speech interaction contains multimodal information, e.g., vision, text. How to design a unified framework to integrate different modal information and leverage different resources (e.g., visual-audio pairs, audio-text pairs, unlabeled speech, and unlabeled text) to facilitate speech representation learning was not well explored. In this paper, we propose a unified cross-modal representation learning framework VATLM (Visual-Audio-Text Language Model). The proposed VATLM employs a unified backbone network to model the modality-independent information and utilizes three simple modality-dependent modules to preprocess visual, speech, and text inputs. In order to integrate these three modalities into one shared semantic space, VATLM is optimized with a masked prediction task of unified tokens, given by our proposed unified tokenizer. We evaluate the pre-trained VATLM on audio-vis
&lt;/p&gt;</description></item><item><title>NLPeer&#26159;&#19968;&#20010;&#20855;&#26377;&#36328;&#39046;&#22495;&#25366;&#25496;&#33021;&#21147;&#30340;&#21516;&#34892;&#35780;&#23457;&#35745;&#31639;&#30740;&#31350;&#30340;&#32479;&#19968;&#36164;&#28304;&#65292;&#21253;&#25324;&#36229;&#36807;5k&#31687;&#35770;&#25991;&#21644;11k&#20010;&#23457;&#31295;&#25253;&#21578;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#35813;&#36164;&#28304;&#25552;&#20379;&#20102;&#19977;&#20010;&#23457;&#26680;&#21327;&#21161;&#20219;&#21153;&#30340;&#23454;&#29616;&#21644;&#20998;&#26512;&#65292;&#24182;&#20026;&#26356;&#20840;&#38754;&#21644;&#31995;&#32479;&#30340;NLP&#21516;&#34892;&#35780;&#23457;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2211.06651</link><description>&lt;p&gt;
NLPeer: &#20855;&#22791;&#22810;&#39046;&#22495;&#25366;&#25496;&#33021;&#21147;&#30340;&#21516;&#34892;&#35780;&#23457;&#35745;&#31639;&#30740;&#31350;&#30340;&#32479;&#19968;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
NLPeer: A Unified Resource for the Computational Study of Peer Review. (arXiv:2211.06651v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06651
&lt;/p&gt;
&lt;p&gt;
NLPeer&#26159;&#19968;&#20010;&#20855;&#26377;&#36328;&#39046;&#22495;&#25366;&#25496;&#33021;&#21147;&#30340;&#21516;&#34892;&#35780;&#23457;&#35745;&#31639;&#30740;&#31350;&#30340;&#32479;&#19968;&#36164;&#28304;&#65292;&#21253;&#25324;&#36229;&#36807;5k&#31687;&#35770;&#25991;&#21644;11k&#20010;&#23457;&#31295;&#25253;&#21578;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#35813;&#36164;&#28304;&#25552;&#20379;&#20102;&#19977;&#20010;&#23457;&#26680;&#21327;&#21161;&#20219;&#21153;&#30340;&#23454;&#29616;&#21644;&#20998;&#26512;&#65292;&#24182;&#20026;&#26356;&#20840;&#38754;&#21644;&#31995;&#32479;&#30340;NLP&#21516;&#34892;&#35780;&#23457;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#34892;&#35780;&#23457;&#26159;&#23398;&#26415;&#20986;&#29256;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23427;&#38656;&#35201;&#30456;&#24403;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#22521;&#35757;&#65292;&#19988;&#26131;&#21463;&#38169;&#35823;&#21644;&#20559;&#35265;&#24433;&#21709;&#12290;&#21508;&#31181;&#38754;&#21521;&#21516;&#34892;&#35780;&#23457;&#30340;NLP&#24212;&#29992;&#26088;&#22312;&#25903;&#25345;&#23457;&#31295;&#20154;&#22312;&#36825;&#20010;&#22797;&#26434;&#30340;&#36807;&#31243;&#20013;&#65292;&#20294;&#32570;&#23569;&#28165;&#26224;&#25480;&#26435;&#30340;&#25968;&#25454;&#38598;&#21644;&#22810;&#39046;&#22495;&#35821;&#26009;&#24211;&#65292;&#38459;&#30861;&#20102;&#23545;NLP&#29992;&#20110;&#21516;&#34892;&#35780;&#23457;&#30340;&#31995;&#32479;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;NLPeer - &#31532;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#22330;&#25152;&#30340;&#36229;&#36807;5k&#31687;&#35770;&#25991;&#21644;11k&#20010;&#23457;&#31295;&#25253;&#21578;&#30340;&#21512;&#20046;&#36947;&#24503;&#30340;&#36328;&#39046;&#22495;&#35821;&#26009;&#24211;&#12290;&#38500;&#20102;&#26469;&#33258;NLP&#31038;&#21306;&#30340;&#26032;&#30340;&#21253;&#25324;&#35770;&#25991;&#33609;&#31295;&#12289;&#30456;&#26426;&#20934;&#22791;&#22909;&#30340;&#29256;&#26412;&#21644;&#21516;&#34892;&#23457;&#26597;&#24847;&#35265;&#30340;&#25968;&#25454;&#38598;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#22686;&#21152;&#20102;&#20808;&#21069;&#30340;&#21516;&#34892;&#35780;&#23457;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#32463;&#36807;&#35299;&#26512;&#21644;&#32467;&#26500;&#21270;&#30340;&#35770;&#25991;&#34920;&#31034;&#12289;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#21644;&#29256;&#26412;&#20449;&#24687;&#12290;&#25105;&#20204;&#34917;&#20805;&#20102;&#25105;&#20204;&#30340;&#36164;&#28304;&#19982;&#19977;&#20010;&#23457;&#26680;&#21327;&#21161;&#20219;&#21153;&#30340;&#25191;&#34892;&#21644;&#20998;&#26512;&#65292;&#21253;&#25324;&#21019;&#26032;&#30340;&#24341;&#23548;&#24335;&#25195;&#25551;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#26356;&#20840;&#38754;&#21644;&#31995;&#32479;&#30340;NLP&#21516;&#34892;&#35780;&#23457;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#24182;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peer review constitutes a core component of scholarly publishing; yet it demands substantial expertise and training, and is susceptible to errors and biases. Various applications of NLP for peer reviewing assistance aim to support reviewers in this complex process, but the lack of clearly licensed datasets and multi-domain corpora prevent the systematic study of NLP for peer review. To remedy this, we introduce NLPeer -- the first ethically sourced multidomain corpus of more than 5k papers and 11k review reports from five different venues. In addition to the new datasets of paper drafts, camera-ready versions and peer reviews from the NLP community, we establish a unified data representation and augment previous peer review datasets to include parsed and structured paper representations, rich metadata and versioning information. We complement our resource with implementations and analysis of three reviewing assistance tasks, including a novel guided skimming task. Our work paves the pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#32852;&#21512;&#24494;&#35843;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#23558;&#27169;&#22411;&#22823;&#23567;&#20943;&#23569;75%&#21516;&#26102;&#31934;&#24230;&#21644;&#31561;&#35823;&#24046;&#29575;&#25439;&#22833;&#24456;&#23567;&#65292;&#24182;&#34920;&#26126;&#24494;&#35843;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#30456;&#36739;&#20110;&#20923;&#32467;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.16611</link><description>&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#22312;&#22810;&#20219;&#21153;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Knowledge Distillation to Multi-task Speech Representation Learning. (arXiv:2210.16611v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#32852;&#21512;&#24494;&#35843;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#23558;&#27169;&#22411;&#22823;&#23567;&#20943;&#23569;75%&#21516;&#26102;&#31934;&#24230;&#21644;&#31561;&#35823;&#24046;&#29575;&#25439;&#22833;&#24456;&#23567;&#65292;&#24182;&#34920;&#26126;&#24494;&#35843;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#30456;&#36739;&#20110;&#20923;&#32467;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
wav2vec 2.0&#21644;HuBERT&#31561;&#27169;&#22411;&#24050;&#25552;&#20986;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#20174;&#38899;&#39057;&#27874;&#24418;&#20013;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#12290;&#24403;&#23427;&#20204;&#19982;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#20851;&#38190;&#35789;&#26816;&#27979;&#21644;&#35828;&#35805;&#20154;&#39564;&#35777;&#65289;&#30456;&#32467;&#21512;&#26102;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#20102;&#22823;&#37327;&#21442;&#25968;&#65292;&#20854;&#20013;&#26368;&#23567;&#29256;&#26412;&#20855;&#26377;9500&#19975;&#20010;&#21442;&#25968;&#65292;&#36825;&#23545;&#20110;&#36793;&#32536;AI&#35774;&#22791;&#30340;&#37096;&#32626;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#28982;&#21518;&#19982;&#22810;&#20010;&#19979;&#28216;&#35821;&#38899;&#28608;&#27963;&#20219;&#21153;&#36827;&#34892;&#32852;&#21512;&#24494;&#35843;&#12290;&#22312;&#20004;&#20010;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#20943;&#23569;&#20102;75&#65285;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#21516;&#26102;&#19982;&#23436;&#25972;&#22823;&#23567;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#20165;&#26377;0.1&#65285;&#30340;&#31934;&#24230;&#21644;0.9&#65285;&#30340;&#31561;&#35823;&#24046;&#29575;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#24494;&#35843;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#30456;&#23545;&#20110;&#20351;&#29992;&#20923;&#32467;&#30340;&#27169;&#22411;&#20250;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model architectures such as wav2vec 2.0 and HuBERT have been proposed to learn speech representations from audio waveforms in a self-supervised manner. When they are combined with downstream tasks such as keyword spotting and speaker verification, they provide state-of-the-art performance. However, these models use a large number of parameters, the smallest version of which has 95 million parameters. This constitutes a challenge for edge AI device deployments. In this paper, we investigate the application of knowledge distillation to speech representation learning (SRL) models followed by joint fine-tuning with multiple downstream voice-activated tasks. In our experiments on two such tasks, our approach results in nearly 75% reduction in model size while suffering only 0.1% accuracy and 0.9% equal error rate degradation compared to the full-size model. In addition, we show that fine-tuning the SRL models results in a significant performance boost compared to using frozen SRL models.
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#23454;&#29616;&#32511;&#33394;&#32463;&#27982;&#65292;&#38656;&#35201;&#21487;&#38752;&#12289;&#21487;&#27604;&#36739;&#21644;&#21487;&#39564;&#35777;&#30340;&#29615;&#22659;&#22768;&#26126;&#12290;&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29615;&#22659;&#22768;&#26126;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#19987;&#23478;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#26816;&#27979;&#29615;&#22659;&#22768;&#26126;&#22312;&#23395;&#24230;&#30005;&#35805;&#20250;&#35758;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#24182;&#21457;&#29616;&#35813;&#20351;&#29992;&#24773;&#20917;&#33258;2015&#24180;&#20197;&#26469;&#26377;&#31283;&#27493;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2209.00507</link><description>&lt;p&gt;
&#29615;&#22659;&#22768;&#26126;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Environmental Claim Detection. (arXiv:2209.00507v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00507
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#32511;&#33394;&#32463;&#27982;&#65292;&#38656;&#35201;&#21487;&#38752;&#12289;&#21487;&#27604;&#36739;&#21644;&#21487;&#39564;&#35777;&#30340;&#29615;&#22659;&#22768;&#26126;&#12290;&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29615;&#22659;&#22768;&#26126;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#19987;&#23478;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#26816;&#27979;&#29615;&#22659;&#22768;&#26126;&#22312;&#23395;&#24230;&#30005;&#35805;&#20250;&#35758;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#24182;&#21457;&#29616;&#35813;&#20351;&#29992;&#24773;&#20917;&#33258;2015&#24180;&#20197;&#26469;&#26377;&#31283;&#27493;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#23454;&#29616;&#32511;&#33394;&#32463;&#27982;&#65292;&#20844;&#21496;&#25152;&#20316;&#20986;&#30340;&#29615;&#22659;&#22768;&#26126;&#24517;&#39035;&#26159;&#21487;&#38752;&#12289;&#21487;&#27604;&#36739;&#21644;&#21487;&#39564;&#35777;&#30340;&#12290;&#20026;&#20102;&#22823;&#35268;&#27169;&#20998;&#26512;&#36825;&#20123;&#22768;&#26126;&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#26469;&#39318;&#20808;&#26816;&#27979;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#23384;&#22312;&#27492;&#31867;&#25968;&#25454;&#38598;&#25110;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#29615;&#22659;&#22768;&#26126;&#26816;&#27979;&#20219;&#21153;&#12290;&#20026;&#20102;&#37197;&#21512;&#35813;&#20219;&#21153;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#30001;&#19987;&#23478;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#21644;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#39044;&#35272;&#20102;&#27492;&#31867;&#27169;&#22411;&#30340;&#19968;&#20010;&#28508;&#22312;&#24212;&#29992;&#65306;&#25105;&#20204;&#26816;&#27979;&#23395;&#24230;&#30005;&#35805;&#20250;&#35758;&#20013;&#25152;&#20316;&#20986;&#30340;&#29615;&#22659;&#22768;&#26126;&#65292;&#24182;&#21457;&#29616;&#33258;2015&#24180;&#24052;&#40654;&#21327;&#35758;&#20197;&#26469;&#29615;&#22659;&#22768;&#26126;&#30340;&#25968;&#37327;&#36880;&#28176;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
To transition to a green economy, environmental claims made by companies must be reliable, comparable, and verifiable. To analyze such claims at scale, automated methods are needed to detect them in the first place. However, there exist no datasets or models for this. Thus, this paper introduces the task of environmental claim detection. To accompany the task, we release an expert-annotated dataset and models trained on this dataset. We preview one potential application of such models: We detect environmental claims made in quarterly earning calls and find that the number of environmental claims has steadily increased since the Paris Agreement in 2015.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#25968;&#25454;&#36807;&#28388;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;NLP&#27169;&#22411;&#24494;&#35843;&#25928;&#29575;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#20943;&#23569;&#25152;&#38656;&#35757;&#32451;&#26679;&#20363;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#21482;&#26377;&#36731;&#24494;&#31934;&#24230;&#38477;&#20302;&#65292;&#21363;&#20351;&#21482;&#26377;&#19968;&#20010;&#26102;&#26399;&#30340;&#35757;&#32451;&#65292;&#20063;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2207.14386</link><description>&lt;p&gt;
&#22810;&#38454;&#27573;&#25968;&#25454;&#36807;&#28388;&#25552;&#39640;NLP&#27169;&#22411;&#24494;&#35843;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Efficient NLP Model Finetuning via Multistage Data Filtering. (arXiv:2207.14386v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#25968;&#25454;&#36807;&#28388;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;NLP&#27169;&#22411;&#24494;&#35843;&#25928;&#29575;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#20943;&#23569;&#25152;&#38656;&#35757;&#32451;&#26679;&#20363;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#21482;&#26377;&#36731;&#24494;&#31934;&#24230;&#38477;&#20302;&#65292;&#21363;&#20351;&#21482;&#26377;&#19968;&#20010;&#26102;&#26399;&#30340;&#35757;&#32451;&#65292;&#20063;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;NLP&#27169;&#22411;&#24494;&#35843;&#30340;&#37325;&#35201;&#24615;&#65292;&#26412;&#25991;&#26088;&#22312;&#26368;&#22823;&#31243;&#24230;&#25552;&#39640;&#23427;&#30340;&#25928;&#29575;&#12290;&#32771;&#34385;&#21040;&#35757;&#32451;&#26679;&#20363;&#30340;&#20887;&#20313;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35268;&#27169;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#31181;&#20851;&#38190;&#26426;&#20250;&#65306;&#20165;&#20351;&#29992;&#37325;&#35201;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#20197;&#27969;&#24335;&#26041;&#24335;&#36807;&#28388;&#35757;&#32451;&#26679;&#20363;&#65292;&#24182;&#21516;&#26102;&#35757;&#32451;&#30446;&#26631;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20004;&#20010;&#20851;&#38190;&#25216;&#26415;&#26159;&#65306;&#65288;1&#65289;&#33258;&#21160;&#30830;&#23450;&#29992;&#20110;&#36339;&#36807;&#21453;&#21521;&#35757;&#32451;&#30340;&#35757;&#32451;&#25439;&#22833;&#38408;&#20540;&#65307;&#65288;2&#65289;&#36816;&#34892;&#20803;&#39044;&#27979;&#22120;&#20197;&#36827;&#19968;&#27493;&#36339;&#36807;&#21069;&#21521;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#19978;&#36848;&#25216;&#26415;&#38598;&#25104;&#21040;&#19968;&#20010;&#20840;&#38754;&#30340;&#19977;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25152;&#38656;&#30340;&#35757;&#32451;&#26679;&#20363;&#20943;&#23569;&#20102;&#26368;&#22810;5.3&#20493;&#65292;&#24182;&#23558;&#35757;&#32451;&#26102;&#38388;&#32553;&#30701;&#20102;&#26368;&#22810;6.8&#20493;&#65292;&#21516;&#26102;&#21482;&#30475;&#21040;&#24456;&#23567;&#30340;&#31934;&#24230;&#38477;&#20302;&#12290;&#21363;&#20351;&#21482;&#26377;&#19968;&#20010;&#26102;&#26399;&#30340;&#35757;&#32451;&#65292;&#22312;&#27599;&#20010;&#35757;&#32451;&#26679;&#20363;&#21482;&#36935;&#21040;&#19968;&#27425;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#38750;&#24120;&#26377;&#25928;&#12290;&#23427;&#24456;&#23481;&#26131;&#23454;&#29616;&#24182;&#20860;&#23481;&#29616;&#26377;&#30340;&#24494;&#35843;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
As model finetuning is central to the modern NLP, we set to maximize its efficiency. Motivated by redundancy in training examples and the sheer sizes of pretrained models, we exploit a key opportunity: training only on important data. To this end, we set to filter training examples in a streaming fashion, in tandem with training the target model. Our key techniques are two: (1) automatically determine a training loss threshold for skipping backward training passes; (2) run a meta predictor for further skipping forward training passes. We integrate the above techniques in a holistic, three-stage training process. On a diverse set of benchmarks, our method reduces the required training examples by up to 5.3$\times$ and training time by up to 6.8$\times$, while only seeing minor accuracy degradation. Our method is effective even when training one epoch, where each training example is encountered only once. It is simple to implement and is compatible with the existing finetuning techniques
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GPT-3&#30340;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#29983;&#25104;&#19968;&#20010;&#25551;&#36848;&#34920;&#20197;&#22238;&#31572;&#20851;&#20110;&#33402;&#26415;&#21697;&#30340;&#35270;&#35273;&#21644;&#19978;&#19979;&#25991;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#27880;&#37322;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2207.12101</link><description>&lt;p&gt;
GPT-3&#22312;&#25991;&#21270;&#36951;&#20135;&#35270;&#35273;&#38382;&#31572;&#20013;&#26159;&#21542;&#36275;&#22815;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?. (arXiv:2207.12101v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GPT-3&#30340;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#29983;&#25104;&#19968;&#20010;&#25551;&#36848;&#34920;&#20197;&#22238;&#31572;&#20851;&#20110;&#33402;&#26415;&#21697;&#30340;&#35270;&#35273;&#21644;&#19978;&#19979;&#25991;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#27880;&#37322;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#25991;&#21270;&#36951;&#20135;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#26377;&#22823;&#37327;&#38899;&#39057;&#26234;&#33021;&#23548;&#28216;&#12289;&#20114;&#21160;&#21338;&#29289;&#39302;&#21644;&#22686;&#24378;&#29616;&#23454;&#30340;&#24212;&#29992;&#12290;&#25152;&#26377;&#36825;&#20123;&#25216;&#26415;&#37117;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#25165;&#33021;&#26377;&#25928;&#22320;&#24037;&#20316;&#24182;&#23545;&#29992;&#25143;&#26377;&#29992;&#12290;&#22312;&#33402;&#26415;&#21697;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#20123;&#25968;&#25454;&#26159;&#30001;&#19987;&#23478;&#36827;&#34892;&#27880;&#37322;&#30340;&#65292;&#36825;&#26159;&#19968;&#20010;&#26114;&#36149;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#29305;&#21035;&#26159;&#65292;&#23545;&#20110;&#27599;&#20214;&#33402;&#26415;&#21697;&#65292;&#37117;&#24517;&#39035;&#25910;&#38598;&#19968;&#24133;&#33402;&#26415;&#21697;&#30340;&#22270;&#20687;&#21644;&#19968;&#20010;&#25551;&#36848;&#34920;&#20197;&#25191;&#34892;&#24120;&#35265;&#20219;&#21153;&#65292;&#20363;&#22914;&#35270;&#35273;&#38382;&#31572;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#22312;&#36816;&#34892;&#26102;&#29983;&#25104;&#19968;&#20010;&#25551;&#36848;&#34920;&#65292;&#21487;&#29992;&#20110;&#22238;&#31572;&#26377;&#20851;&#33402;&#26415;&#21697;&#30340;&#35270;&#35273;&#21644;&#19978;&#19979;&#25991;&#38382;&#39064;&#65292;&#23436;&#20840;&#36991;&#20813;&#20102;&#22270;&#20687;&#21644;&#27880;&#37322;&#36807;&#31243;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;GPT-3&#20026;&#33402;&#26415;&#21697;&#29983;&#25104;&#25551;&#36848;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#29983;&#25104;&#30340;&#25551;&#36848;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of Deep Learning and Computer Vision in the Cultural Heritage domain is becoming highly relevant in the last few years with lots of applications about audio smart guides, interactive museums and augmented reality. All these technologies require lots of data to work effectively and be useful for the user. In the context of artworks, such data is annotated by experts in an expensive and time consuming process. In particular, for each artwork, an image of the artwork and a description sheet have to be collected in order to perform common tasks like Visual Question Answering. In this paper we propose a method for Visual Question Answering that allows to generate at runtime a description sheet that can be used for answering both visual and contextual questions about the artwork, avoiding completely the image and the annotation process. For this purpose, we investigate on the use of GPT-3 for generating descriptions for artworks analyzing the quality of generated descriptions through
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#25506;&#32034;&#20102;&#20351;&#29992;&#22269;&#38469;&#30142;&#30149;&#20998;&#31867;&#23545;&#20020;&#24202;&#38382;&#39064;&#21015;&#34920;&#26465;&#30446;&#36827;&#34892;&#33258;&#21160;&#32534;&#30721;&#65292;&#25581;&#31034;&#20102;&#19981;&#19968;&#33268;&#30340;&#25163;&#21160;&#32534;&#30721;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#38480;&#21046;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2112.13756</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#30142;&#30149;&#32534;&#30721;&#20998;&#37197;&#20013;&#20020;&#24202;&#38382;&#39064;&#21015;&#34920;&#26465;&#30446;&#30340;&#20108;&#27425;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Secondary Use of Clinical Problem List Entries for Neural Network-Based Disease Code Assignment. (arXiv:2112.13756v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.13756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#25506;&#32034;&#20102;&#20351;&#29992;&#22269;&#38469;&#30142;&#30149;&#20998;&#31867;&#23545;&#20020;&#24202;&#38382;&#39064;&#21015;&#34920;&#26465;&#30446;&#36827;&#34892;&#33258;&#21160;&#32534;&#30721;&#65292;&#25581;&#31034;&#20102;&#19981;&#19968;&#33268;&#30340;&#25163;&#21160;&#32534;&#30721;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#38480;&#21046;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20449;&#24687;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#21322;&#32467;&#26500;&#21270;&#21644;&#37096;&#20998;&#27880;&#37322;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#22823;&#22411;&#23384;&#20648;&#24211;&#65292;&#36825;&#20123;&#25968;&#25454;&#24050;&#32463;&#36798;&#21040;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#36136;&#37327;&#65292;&#20351;&#23427;&#20204;&#23545;&#20110;&#30417;&#30563;&#30340;&#25968;&#25454;&#39537;&#21160;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#38750;&#24120;&#26377;&#36259;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#22269;&#38469;&#30142;&#30149;&#20998;&#31867;&#65288;ICD-10&#65289;&#23545;50&#20010;&#23383;&#31526;&#38271;&#30340;&#20020;&#24202;&#38382;&#39064;&#21015;&#34920;&#26465;&#30446;&#36827;&#34892;&#33258;&#21160;&#32534;&#30721;&#65292;&#24182;&#22312;&#21069;100&#20010;ICD-10&#19977;&#20301;&#30721;&#19978;&#35780;&#20272;&#20102;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#19968;&#20010;&#24555;&#36895;&#25991;&#26412;&#22522;&#32447;&#36798;&#21040;&#20102;0.83&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#65292;&#20854;&#27425;&#26159;&#19968;&#20010;&#23383;&#31526;&#32423;LSTM&#65292;&#20854;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#20026;0.84&#12290;&#34920;&#29616;&#26368;&#20339;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#19979;&#28216;RoBERTa&#27169;&#22411;&#21644;&#33258;&#23450;&#20041;&#35821;&#35328;&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;0.88&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#12290;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20998;&#26512;&#20197;&#21450;&#23545;&#20551;&#38451;&#24615;&#21644;&#20551;&#38452;&#24615;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#19981;&#19968;&#33268;&#30340;&#25163;&#21160;&#32534;&#30721;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#38480;&#21046;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical information systems have become large repositories for semi-structured and partly annotated electronic health record data, which have reached a critical mass that makes them interesting for supervised data-driven neural network approaches. We explored automated coding of 50 character long clinical problem list entries using the International Classification of Diseases (ICD-10) and evaluated three different types of network architectures on the top 100 ICD-10 three-digit codes. A fastText baseline reached a macro-averaged F1-score of 0.83, followed by a character-level LSTM with a macro-averaged F1-score of 0.84. The top performing approach used a downstreamed RoBERTa model with a custom language model, yielding a macro-averaged F1-score of 0.88. A neural network activation analysis together with an investigation of the false positives and false negatives unveiled inconsistent manual coding as a main limiting factor.
&lt;/p&gt;</description></item></channel></rss>