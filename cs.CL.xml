<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#20351;&#29992;T5 Transformer&#27169;&#22411;&#25104;&#21151;&#22320;&#22312;&#23391;&#21152;&#25289;&#35821;&#20013;&#26816;&#27979;&#35821;&#27861;&#38169;&#35823;&#65292;&#24182;&#23545;&#27169;&#22411;&#26816;&#27979;&#21040;&#30340;&#38169;&#35823;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#23558;&#32763;&#35793;&#27169;&#22411;&#36866;&#24212;&#35821;&#27861;&#26816;&#27979;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.10612</link><description>&lt;p&gt;
&#20351;&#29992;T5 Transformer&#27169;&#22411;&#30340;&#23391;&#21152;&#25289;&#35821;&#35821;&#27861;&#38169;&#35823;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bangla Grammatical Error Detection Using T5 Transformer Model. (arXiv:2303.10612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;T5 Transformer&#27169;&#22411;&#25104;&#21151;&#22320;&#22312;&#23391;&#21152;&#25289;&#35821;&#20013;&#26816;&#27979;&#35821;&#27861;&#38169;&#35823;&#65292;&#24182;&#23545;&#27169;&#22411;&#26816;&#27979;&#21040;&#30340;&#38169;&#35823;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#23558;&#32763;&#35793;&#27169;&#22411;&#36866;&#24212;&#35821;&#27861;&#26816;&#27979;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#25991;&#26412;&#30340;&#36716;&#25442;&#21464;&#21387;&#22120;&#65288;T5&#65289;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#23391;&#21152;&#25289;&#35821;&#35821;&#27861;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#36807;&#30340;BanglaT5&#30340;&#23567;&#21464;&#31181;&#65292;&#22312;&#19968;&#20010;&#21253;&#21547;9385&#20010;&#21477;&#23376;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#38169;&#35823;&#34987;&#19987;&#29992;&#20998;&#30028;&#31526;&#25324;&#36215;&#26469;&#12290;T5&#27169;&#22411;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#32763;&#35793;&#65292;&#32780;&#19981;&#26159;&#29305;&#21035;&#20026;&#36825;&#20010;&#20219;&#21153;&#35774;&#35745;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#21518;&#22788;&#29702;&#26469;&#20351;&#20854;&#36866;&#24212;&#38169;&#35823;&#26816;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;T5&#27169;&#22411;&#22312;&#26816;&#27979;&#23391;&#21152;&#25289;&#35821;&#35821;&#27861;&#38169;&#35823;&#26102;&#21487;&#20197;&#23454;&#29616;&#36739;&#20302;&#30340;Levenshtein&#36317;&#31163;&#65292;&#20294;&#26159;&#24517;&#39035;&#36827;&#34892;&#21518;&#22788;&#29702;&#25165;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#22312;&#23545;&#31934;&#32454;&#35843;&#25972;&#30340;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#21518;&#22788;&#29702;&#21518;&#65292;&#26368;&#32456;&#27979;&#35797;&#38598;&#30340;&#24179;&#22343;Levenshtein&#36317;&#31163;&#20026;1.0394&#12290;&#26412;&#25991;&#36824;&#23545;&#27169;&#22411;&#26816;&#27979;&#21040;&#30340;&#38169;&#35823;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20102;&#23558;&#32763;&#35793;&#27169;&#22411;&#36866;&#24212;&#35821;&#27861;&#26816;&#27979;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#65292;&#36825;&#22312;&#25104;&#21151;&#26816;&#27979;&#23391;&#21152;&#25289;&#35821;&#35821;&#27861;&#38169;&#35823;&#26102;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a method for detecting grammatical errors in Bangla using a Text-to-Text Transfer Transformer (T5) Language Model, using the small variant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were bracketed by the dedicated demarcation symbol. The T5 model was primarily designed for translation and is not specifically designed for this task, so extensive post-processing was necessary to adapt it to the task of error detection. Our experiments show that the T5 model can achieve low Levenshtein Distance in detecting grammatical errors in Bangla, but post-processing is essential to achieve optimal performance. The final average Levenshtein Distance after post-processing the output of the fine-tuned model was 1.0394 on a test set of 5000 sentences. This paper also presents a detailed analysis of the errors detected by the model and discusses the challenges of adapting a translation model for grammar. Our approach can be extended to other languages, demonst
&lt;/p&gt;</description></item><item><title>CTRAN&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;CNN-Transformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#25554;&#27133;&#22635;&#20805;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#20102;BERT&#21644;&#22810;&#23618;&#21367;&#31215;&#21644;Transformer&#32534;&#30721;&#22120;&#65292;&#37319;&#29992;&#33258;&#27880;&#24847;&#21147;&#21644;&#23545;&#40784;&#30340;Transformer&#35299;&#30721;&#22120;&#12290;&#35813;&#32593;&#32476;&#22312;ATIS&#21644;SNIPS&#25968;&#25454;&#38598;&#19978;&#30340;&#25554;&#27133;&#22635;&#20805;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.10606</link><description>&lt;p&gt;
CTRAN&#65306;&#22522;&#20110;CNN-Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CTRAN: CNN-Transformer-based Network for Natural Language Understanding. (arXiv:2303.10606v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10606
&lt;/p&gt;
&lt;p&gt;
CTRAN&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;CNN-Transformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#25554;&#27133;&#22635;&#20805;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#20102;BERT&#21644;&#22810;&#23618;&#21367;&#31215;&#21644;Transformer&#32534;&#30721;&#22120;&#65292;&#37319;&#29992;&#33258;&#27880;&#24847;&#21147;&#21644;&#23545;&#40784;&#30340;Transformer&#35299;&#30721;&#22120;&#12290;&#35813;&#32593;&#32476;&#22312;ATIS&#21644;SNIPS&#25968;&#25454;&#38598;&#19978;&#30340;&#25554;&#27133;&#22635;&#20805;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22270;&#26816;&#27979;&#21644;&#25554;&#27133;&#22635;&#20805;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CTRAN&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;CNN-Transformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#24847;&#22270;&#26816;&#27979;&#21644;&#25554;&#27133;&#22635;&#20805;&#12290;&#22312;&#32534;&#30721;&#22120;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;BERT&#65292;&#25509;&#30528;&#20960;&#20010;&#21367;&#31215;&#23618;&#65292;&#24182;&#20351;&#29992;&#31383;&#21475;&#29305;&#24449;&#24207;&#21015;&#37325;&#26032;&#25490;&#21015;&#36755;&#20986;&#12290;&#25105;&#20204;&#22312;&#31383;&#21475;&#29305;&#24449;&#24207;&#21015;&#21518;&#20351;&#29992;&#22534;&#21472;&#30340;Transformer&#32534;&#30721;&#22120;&#12290;&#23545;&#20110;&#24847;&#22270;&#26816;&#27979;&#35299;&#30721;&#22120;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#21518;&#36319;&#19968;&#20010;&#32447;&#24615;&#23618;&#12290;&#22312;&#25554;&#27133;&#22635;&#20805;&#35299;&#30721;&#22120;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23545;&#40784;&#30340;Transformer&#35299;&#30721;&#22120;&#65292;&#23427;&#21033;&#29992;&#20102;&#38646;&#23545;&#35282;&#32447;&#25513;&#30721;&#65292;&#23558;&#36755;&#20986;&#26631;&#31614;&#19982;&#36755;&#20837;&#26631;&#35760;&#23545;&#40784;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32593;&#32476;&#24212;&#29992;&#20110;ATIS&#21644;SNIPS&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#25554;&#27133;&#22635;&#20805;&#20013;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35789;&#23884;&#20837;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#32593;&#32476;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#31574;&#30053;&#27604;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32534;&#30721;&#22120;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intent-detection and slot-filling are the two main tasks in natural language understanding. In this study, we propose CTRAN, a novel encoder-decoder CNN-Transformer-based architecture for intent-detection and slot-filling. In the encoder, we use BERT, followed by several convolutional layers, and rearrange the output using window feature sequence. We use stacked Transformer encoders after the window feature sequence. For the intent-detection decoder, we utilize self-attention followed by a linear layer. In the slot-filling decoder, we introduce the aligned Transformer decoder, which utilizes a zero diagonal mask, aligning output tags with input tokens. We apply our network on ATIS and SNIPS, and surpass the current state-of-the-art in slot-filling on both datasets. Furthermore, we incorporate the language model as word embeddings, and show that this strategy yields a better result when compared to the language model as an encoder.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#24341;&#20837;&#20154;&#24037;&#20849;&#24773;&#30340;&#24605;&#24819;&#36827;&#20837;&#20154;&#26412;&#35774;&#35745;&#65292;&#20197;&#24110;&#21161;&#35774;&#35745;&#24072;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2303.10583</link><description>&lt;p&gt;
&#20154;&#26412;&#35774;&#35745;&#20013;&#30340;&#20154;&#24037;&#20849;&#24773;&#65306;&#19968;&#20010;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Toward Artificial Empathy for Human-Centered Design: A Framework. (arXiv:2303.10583v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#24341;&#20837;&#20154;&#24037;&#20849;&#24773;&#30340;&#24605;&#24819;&#36827;&#20837;&#20154;&#26412;&#35774;&#35745;&#65292;&#20197;&#24110;&#21161;&#35774;&#35745;&#24072;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#36807;&#31243;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#35774;&#35745;&#24072;&#36890;&#36807;&#21457;&#29616;&#26410;&#28385;&#36275;&#30340;&#38656;&#27714;&#21644;&#24320;&#21457;&#21019;&#26032;&#30340;&#27010;&#24565;&#20316;&#20026;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#25506;&#32034;&#26426;&#20250;&#12290;&#20174;&#20154;&#26412;&#35774;&#35745;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35774;&#35745;&#24072;&#24517;&#39035;&#19982;&#20154;&#21457;&#23637;&#20849;&#24773;&#65292;&#25165;&#33021;&#30495;&#27491;&#29702;&#35299;&#20182;&#20204;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#21457;&#23637;&#20849;&#24773;&#26159;&#19968;&#20010;&#22797;&#26434;&#32780;&#20027;&#35266;&#30340;&#36807;&#31243;&#65292;&#38750;&#24120;&#20381;&#36182;&#20110;&#35774;&#35745;&#24072;&#30340;&#20849;&#24773;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#20849;&#24773;&#30340;&#21457;&#23637;&#26159;&#30452;&#35273;&#24615;&#30340;&#65292;&#28508;&#22312;&#38656;&#27714;&#30340;&#21457;&#29616;&#24448;&#24448;&#26159;&#20598;&#28982;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35265;&#35299;&#65292;&#20197;&#25351;&#20986;&#20197;&#20849;&#24773;&#20026;&#26680;&#24515;&#30340;&#20154;&#26412;&#35774;&#35745;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#36328;&#23398;&#31185;&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#25454;&#39537;&#21160;&#30340;&#29992;&#25143;&#30740;&#31350;&#12289;&#20849;&#24773;&#29702;&#35299;&#30340;&#21457;&#23637;&#21644;&#20154;&#24037;&#20849;&#24773;&#31561;&#30740;&#31350;&#39046;&#22495;&#12290;&#22522;&#20110;&#36825;&#20010;&#22522;&#30784;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20154;&#24037;&#20849;&#24773;&#22312;&#20154;&#26412;&#35774;&#35745;&#20013;&#21487;&#20197;&#21457;&#25381;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#35774;&#35745;&#36807;&#31243;&#20013;&#24320;&#21457;&#20154;&#24037;&#20849;&#24773;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the early stages of the design process, designers explore opportunities by discovering unmet needs and developing innovative concepts as potential solutions. From a human-centered design perspective, designers must develop empathy with people to truly understand their needs. However, developing empathy is a complex and subjective process that relies heavily on the designer's empathetic capability. Therefore, the development of empathetic understanding is intuitive, and the discovery of underlying needs is often serendipitous. This paper aims to provide insights from artificial intelligence research to indicate the future direction of AI-driven human-centered design, taking into account the essential role of empathy. Specifically, we conduct an interdisciplinary investigation of research areas such as data-driven user studies, empathetic understanding development, and artificial empathy. Based on this foundation, we discuss the role that artificial empathy can play in human-centered 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;MeToo&#24086;&#25991;&#20013;&#20934;&#30830;&#22320;&#35782;&#21035;&#21644;&#25552;&#21462;&#20986;&#25551;&#36848;&#24615;&#39578;&#25200;&#20107;&#20214;&#12289;&#23545;&#24184;&#23384;&#32773;&#30340;&#24433;&#21709;&#21644;&#35831;&#27714;&#30340;&#24314;&#35758;&#30340;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2303.10573</link><description>&lt;p&gt;
&#20174; MeToo &#24086;&#25991;&#20013;&#25552;&#21462;&#20107;&#20214;&#12289;&#24433;&#21709;&#21644;&#35831;&#27714;&#30340;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Extracting Incidents, Effects, and Requested Advice from MeToo Posts. (arXiv:2303.10573v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;MeToo&#24086;&#25991;&#20013;&#20934;&#30830;&#22320;&#35782;&#21035;&#21644;&#25552;&#21462;&#20986;&#25551;&#36848;&#24615;&#39578;&#25200;&#20107;&#20214;&#12289;&#23545;&#24184;&#23384;&#32773;&#30340;&#24433;&#21709;&#21644;&#35831;&#27714;&#30340;&#24314;&#35758;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#24615;&#39578;&#25200;&#30340;&#24184;&#23384;&#32773;&#32463;&#24120;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20998;&#20139;&#33258;&#24049;&#30340;&#32463;&#21382;&#65292;&#23637;&#31034;&#20182;&#20204;&#30340;&#24863;&#21463;&#21644;&#24773;&#32490;&#65292;&#24182;&#23547;&#27714;&#24314;&#35758;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312; Reddit &#19978;&#65292;&#24184;&#23384;&#32773;&#32463;&#24120;&#20998;&#20139;&#25551;&#36848;&#20197;&#19979;&#19977;&#20010;&#20869;&#23481;&#30340;&#38271;&#24086;&#25991;&#65306;&#65288;i&#65289;&#24615;&#39578;&#25200;&#20107;&#20214;&#12289;&#65288;ii&#65289;&#20854;&#23545;&#24184;&#23384;&#32773;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#20182;&#20204;&#30340;&#24863;&#21463;&#21644;&#24773;&#32490;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#27491;&#22312;&#23547;&#27714;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#31216;&#36825;&#26679;&#30340;&#24086;&#25991;&#20026; MeToo &#24086;&#25991;&#65292;&#23613;&#31649;&#23427;&#20204;&#21487;&#33021;&#27809;&#26377;&#34987;&#26631;&#35760;&#65292;&#24182;&#19988;&#21487;&#33021;&#20986;&#29616;&#22312;&#21508;&#31181;&#23376;&#35770;&#22363;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#27169;&#22411;&#26469;&#20174;&#38271;&#24086;&#25991;&#20013;&#35782;&#21035;&#25551;&#36848;&#19978;&#36848;&#19977;&#31181;&#31867;&#21035;&#30340;&#21477;&#23376;&#65292;&#20197;&#20415;&#24110;&#21161;&#20154;&#21592;&#29702;&#35299;&#24184;&#23384;&#32773;&#30340;&#38656;&#27714;&#12290;&#22312;&#25968;&#25454;&#38598;&#30340;&#21313;&#25240;&#20132;&#21449;&#39564;&#35777;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102; 0.82 &#30340;&#23439; F1 &#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survivors of sexual harassment frequently share their experiences on social media, revealing their feelings and emotions and seeking advice. We observed that on Reddit, survivors regularly share long posts that describe a combination of (i) a sexual harassment incident, (ii) its effect on the survivor, including their feelings and emotions, and (iii) the advice being sought. We term such posts MeToo posts, even though they may not be so tagged and may appear in diverse subreddits. A prospective helper (such as a counselor or even a casual reader) must understand a survivor's needs from such posts. But long posts can be time-consuming to read and respond to.  Accordingly, we address the problem of extracting key information from a long MeToo post. We develop a natural language-based model to identify sentences from a post that describe any of the above three categories.  On ten-fold cross-validation of a dataset, our model achieves a macro F1 score of 0.82.  In addition, we contribute M
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20174;2020&#24180;2&#26376;&#33267;2021&#24180;4&#26376;&#65292;&#36229;&#36807;5400&#19975;&#26465;&#32654;&#22269;&#21644;&#21360;&#24230;&#30340;&#25512;&#25991;&#20013;&#19982;COVID-19&#30456;&#20851;&#30340;&#24773;&#32490;&#34920;&#36798;&#65292;&#21457;&#29616;&#20004;&#20010;&#22269;&#23478;&#30340;&#24773;&#32490;&#34920;&#36798;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#21516;&#26102;&#22312;2020&#24180;&#20013;&#26399;&#24773;&#32490;&#34920;&#36798;&#30340;&#36235;&#21183;&#20986;&#29616;&#20102;&#21453;&#36716;&#12290;</title><link>http://arxiv.org/abs/2303.10560</link><description>&lt;p&gt;
&#20154;&#20204;&#22914;&#20309;&#22312; Twitter &#19978;&#22238;&#24212; COVID-19 &#30123;&#24773;&#65306;&#32654;&#22269;&#21644;&#21360;&#24230;&#30340;&#24773;&#32490;&#34920;&#36798;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
How People Respond to the COVID-19 Pandemic on Twitter: A Comparative Analysis of Emotional Expressions from US and India. (arXiv:2303.10560v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20174;2020&#24180;2&#26376;&#33267;2021&#24180;4&#26376;&#65292;&#36229;&#36807;5400&#19975;&#26465;&#32654;&#22269;&#21644;&#21360;&#24230;&#30340;&#25512;&#25991;&#20013;&#19982;COVID-19&#30456;&#20851;&#30340;&#24773;&#32490;&#34920;&#36798;&#65292;&#21457;&#29616;&#20004;&#20010;&#22269;&#23478;&#30340;&#24773;&#32490;&#34920;&#36798;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#21516;&#26102;&#22312;2020&#24180;&#20013;&#26399;&#24773;&#32490;&#34920;&#36798;&#30340;&#36235;&#21183;&#20986;&#29616;&#20102;&#21453;&#36716;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19 &#30123;&#24773;&#24050;&#22842;&#21435;&#20102;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#30340;&#29983;&#21629;&#65292;&#24341;&#21457;&#20102;&#39640;&#24230;&#30340;&#24773;&#32490;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#32654;&#22269;&#21644;&#21360;&#24230;&#19982; COVID-19 &#30456;&#20851;&#30340;&#21508;&#31181;&#24773;&#32490;&#34920;&#36798;&#65292;&#28085;&#30422;&#20102;&#20174;2020&#24180;2&#26376;&#33267;2021&#24180;4&#26376;&#30340;15&#20010;&#26376;&#20013;&#30340;&#36229;&#36807;5400&#19975;&#26465;&#25512;&#25991;&#12290;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#24773;&#32490;&#20998;&#26512;&#21644;&#20027;&#39064;&#24314;&#27169;&#31639;&#27861;&#65292;&#30740;&#31350;&#20102;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#24773;&#32490;&#65288;&#24656;&#24807;&#12289;&#24868;&#24594;&#12289;&#24555;&#20048;&#21644;&#24754;&#20260;&#65289;&#21450;&#20854;&#19982;&#26102;&#38388;&#21644;&#22320;&#28857;&#30456;&#20851;&#30340;&#21464;&#21270;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#24656;&#24807;&#24773;&#32490;&#38477;&#20302;&#12289;&#24868;&#24594;&#21644;&#24555;&#20048;&#24773;&#32490;&#27874;&#21160;&#30340;&#24773;&#20917;&#19979;&#65292;2020&#24180;&#30340;&#36235;&#21183;&#21457;&#29983;&#20102;&#21453;&#36716;&#65292;&#30452;&#21040;2021&#24180;&#21069;&#22235;&#20010;&#26376;&#30340;&#26032;&#24773;&#20917;&#20877;&#27425;&#23548;&#33268;&#36825;&#31181;&#36235;&#21183;&#30340;&#20986;&#29616;&#12290;&#35752;&#35770;&#20102;&#26816;&#27979;&#21040;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has claimed millions of lives worldwide and elicited heightened emotions. This study examines the expression of various emotions pertaining to COVID-19 in the United States and India as manifested in over 54 million tweets, covering the fifteen-month period from February 2020 through April 2021, a period which includes the beginnings of the huge and disastrous increase in COVID-19 cases that started to ravage India in March 2021. Employing pre-trained emotion analysis and topic modeling algorithms, four distinct types of emotions (fear, anger, happiness, and sadness) and their time- and location-associated variations were examined. Results revealed significant country differences and temporal changes in the relative proportions of fear, anger, and happiness, with fear declining and anger and happiness fluctuating in 2020 until new situations over the first four months of 2021 reversed the trends. Detected differences are discussed briefly in terms of the latent to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27169;&#24335;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#22312;&#20004;&#31181;&#19981;&#21516;&#35760;&#24518;&#26041;&#24335;&#65288;&#22810;&#26679;&#24615;&#21644;&#35814;&#23613;&#24615;&#65289;&#26041;&#38754;&#19981;&#21516;&#30340;&#34920;&#29616;&#65292;&#24182;&#35748;&#20026;&#20248;&#31168;&#26041;&#27861;&#24212;&#21516;&#26102;&#36861;&#27714;&#36825;&#20004;&#31181;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.10527</link><description>&lt;p&gt;
&#20004;&#31181;&#35760;&#24518;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Two Kinds of Recall. (arXiv:2303.10527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10527
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27169;&#24335;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#22312;&#20004;&#31181;&#19981;&#21516;&#35760;&#24518;&#26041;&#24335;&#65288;&#22810;&#26679;&#24615;&#21644;&#35814;&#23613;&#24615;&#65289;&#26041;&#38754;&#19981;&#21516;&#30340;&#34920;&#29616;&#65292;&#24182;&#35748;&#20026;&#20248;&#31168;&#26041;&#27861;&#24212;&#21516;&#26102;&#36861;&#27714;&#36825;&#20004;&#31181;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#65292;&#22522;&#20110;&#27169;&#24335;&#30340;&#27169;&#22411;&#25797;&#38271;&#20934;&#30830;&#24615;&#65292;&#32780;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#21017;&#26356;&#25797;&#38271;&#21484;&#22238;&#29575;&#12290;&#20294;&#36825;&#30830;&#23454;&#26159;&#36825;&#26679;&#21527;&#65311;&#20316;&#32773;&#35748;&#20026;&#65292;&#26377;&#20004;&#31181;&#35760;&#24518;&#26041;&#24335;&#65306;d-recall&#65292;&#20195;&#34920;&#22810;&#26679;&#24615;&#65292;&#21644;e-recall&#65292;&#20195;&#34920;&#35814;&#23613;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30830;&#23454;&#22312;d-recall&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#27169;&#24335;&#26041;&#27861;&#65292;&#20294;&#26377;&#26102;&#20505;&#27169;&#24335;&#26041;&#27861;&#20173;&#28982;&#22312;e-recall&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#12290;&#29702;&#24819;&#30340;&#26041;&#27861;&#24212;&#35813;&#21516;&#26102;&#36861;&#27714;&#36825;&#20004;&#31181;&#26041;&#24335;&#65292;&#24182;&#19988;&#36825;&#31181;&#29702;&#24819;&#24212;&#35813;&#21453;&#26144;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is an established assumption that pattern-based models are good at precision, while learning based models are better at recall. But is that really the case? I argue that there are two kinds of recall: d-recall, reflecting diversity, and e-recall, reflecting exhaustiveness. I demonstrate through experiments that while neural methods are indeed significantly better at d-recall, it is sometimes the case that pattern-based methods are still substantially better at e-recall. Ideal methods should aim for both kinds, and this ideal should in turn be reflected in our evaluations.
&lt;/p&gt;</description></item><item><title>AdaLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#39044;&#31639;&#20998;&#37197;&#26041;&#27861;&#65292;&#29992;&#20110;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#12290;&#23558;&#22686;&#37327;&#26356;&#26032;&#30340;&#39044;&#31639;&#26681;&#25454;&#26435;&#37325;&#30697;&#38453;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#20998;&#37197;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#24494;&#35843;&#34920;&#29616;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.10512</link><description>&lt;p&gt;
&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#30340;&#33258;&#36866;&#24212;&#39044;&#31639;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning. (arXiv:2303.10512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10512
&lt;/p&gt;
&lt;p&gt;
AdaLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#39044;&#31639;&#20998;&#37197;&#26041;&#27861;&#65292;&#29992;&#20110;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#12290;&#23558;&#22686;&#37327;&#26356;&#26032;&#30340;&#39044;&#31639;&#26681;&#25454;&#26435;&#37325;&#30697;&#38453;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#20998;&#37197;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#24494;&#35843;&#34920;&#29616;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#23545;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#20102;&#19968;&#31181;&#37325;&#35201;&#30340;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#30340;&#20570;&#27861;&#26159;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#21442;&#25968;&#65292;&#24403;&#23384;&#22312;&#22823;&#37327;&#19979;&#28216;&#20219;&#21153;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#24494;&#35843;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#20197;&#20197;&#21442;&#25968;&#26377;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#35757;&#32451;&#21152;&#26435;&#30340;&#22686;&#37327;&#26356;&#26032;&#65292;&#20363;&#22914;&#20302;&#31209;&#22686;&#37327;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23558;&#22686;&#37327;&#26356;&#26032;&#30340;&#39044;&#31639;&#22343;&#21248;&#20998;&#37197;&#21040;&#25152;&#26377;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#30697;&#38453;&#19978;&#65292;&#24573;&#30053;&#20102;&#19981;&#21516;&#26435;&#37325;&#21442;&#25968;&#30340;&#19981;&#21516;&#37325;&#35201;&#24615;&#12290;&#32467;&#26524;&#65292;&#24494;&#35843;&#30340;&#34920;&#29616;&#26159;&#27425;&#20248;&#30340;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaLoRA&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#33258;&#36866;&#24212;&#20998;&#37197;&#26435;&#37325;&#30697;&#38453;&#30340;&#21442;&#25968;&#39044;&#31639;&#12290;&#29305;&#21035;&#22320;&#65292;AdaLoRA&#23558;&#22686;&#37327;&#26356;&#26032;&#30340;&#21442;&#25968;&#21270;&#20026;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#24418;&#24335;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#20351;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#21098;&#26525;&#22855;&#24322;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine-tuning performance is suboptimal. To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#27880;&#37322;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#22768;&#23398;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;ASR&#31995;&#32479;&#65292;&#24182;&#22312;&#39046;&#22495;&#29305;&#23450;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#21830;&#19994;ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10510</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#35821;&#38899;&#35782;&#21035;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning System for Domain-specific speech Recognition. (arXiv:2303.10510v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#27880;&#37322;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#22768;&#23398;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;ASR&#31995;&#32479;&#65292;&#24182;&#22312;&#39046;&#22495;&#29305;&#23450;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#21830;&#19994;ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#26426;&#35821;&#38899;&#25509;&#21475;&#36234;&#26469;&#36234;&#20415;&#25463;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#21830;&#19994;ASR&#31995;&#32479;&#36890;&#24120;&#22312;&#39046;&#22495;&#29305;&#23450;&#35821;&#38899;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#20316;&#32773;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;DeepSpeech2&#21644;Wav2Vec2&#22768;&#23398;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#21463;&#30410;&#29305;&#23450;&#30340;ASR&#31995;&#32479;&#12290;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#27880;&#37322;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#65292;&#21482;&#38656;&#23569;&#37327;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#12290;&#26368;&#20339;&#24615;&#33021;&#26469;&#33258;&#19968;&#31181;&#32463;&#36807;&#24494;&#35843;&#30340;Wav2Vec2-Large-LV60&#22768;&#23398;&#27169;&#22411;&#65292;&#24102;&#26377;&#22806;&#37096;KenLM&#65292;&#22312;&#21463;&#30410;&#29305;&#23450;&#35821;&#38899;&#19978;&#36229;&#36234;&#20102;Google&#21644;AWS ASR&#31995;&#32479;&#12290;&#36824;&#30740;&#31350;&#20102;&#23558;&#23481;&#26131;&#20986;&#38169;&#30340;ASR&#36716;&#24405;&#20316;&#20026;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#30340;&#19968;&#37096;&#20998;&#30340;&#21487;&#34892;&#24615;&#12290;&#21463;&#30410;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#39046;&#22495;&#29305;&#23450;&#24494;&#35843;&#30340;ASR&#31995;&#32479;&#21487;&#20197;&#36229;&#36234;&#21830;&#19994;ASR&#31995;&#32479;&#24182;&#25552;&#39640;NLU&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As human-machine voice interfaces provide easy access to increasingly intelligent machines, many state-of-the-art automatic speech recognition (ASR) systems are proposed. However, commercial ASR systems usually have poor performance on domain-specific speech especially under low-resource settings. The author works with pre-trained DeepSpeech2 and Wav2Vec2 acoustic models to develop benefit-specific ASR systems. The domain-specific data are collected using proposed semi-supervised learning annotation with little human intervention. The best performance comes from a fine-tuned Wav2Vec2-Large-LV60 acoustic model with an external KenLM, which surpasses the Google and AWS ASR systems on benefit-specific speech. The viability of using error prone ASR transcriptions as part of spoken language understanding (SLU) is also investigated. Results of a benefit-specific natural language understanding (NLU) task show that the domain-specific fine-tuned ASR system can outperform the commercial ASR sys
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#20294;&#36825;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2303.10475</link><description>&lt;p&gt;
&#20165;&#20165;&#25552;&#31034;&#36275;&#22815;&#20102;&#21527;&#65311;&#19981;&#26159;&#30340;&#12290;&#25351;&#23548;&#23398;&#20064;&#30340;&#20840;&#38754;&#21644;&#26356;&#24191;&#38420;&#35270;&#35282;&#65288;arXiv&#65306;2303.10475v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning. (arXiv:2303.10475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10475
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#20294;&#36825;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#35821;&#20041;&#21487;&#20197;&#36890;&#36807;&#19968;&#32452;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#25110;&#19968;&#26465;&#25991;&#26412;&#25351;&#20196;&#26469;&#34920;&#36798;&#12290;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#30340;&#21487;&#29992;&#24615;&#12290;&#36825;&#24341;&#36215;&#20102;&#20004;&#20010;&#38382;&#39064;&#65306;&#39318;&#20808;&#65292;&#25910;&#38598;&#20219;&#21153;&#29305;&#23450;&#26631;&#35760;&#31034;&#20363;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#65292;&#25110;&#32773;&#31995;&#32479;&#38656;&#35201;&#31435;&#21363;&#22788;&#29702;&#26032;&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#36825;&#19981;&#26159;&#29992;&#25143;&#21451;&#22909;&#30340;&#65292;&#22240;&#20026;&#26368;&#32456;&#29992;&#25143;&#21487;&#33021;&#26356;&#24895;&#24847;&#22312;&#20351;&#29992;&#31995;&#32479;&#20043;&#21069;&#25552;&#20379;&#20219;&#21153;&#25551;&#36848;&#32780;&#19981;&#26159;&#19968;&#32452;&#31034;&#20363;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#31038;&#21306;&#20173;&#28982;&#38754;&#20020;&#30528;&#19968;&#20123;&#20849;&#21516;&#30340;&#38382;&#39064;&#12290;&#26412;&#27425;&#35843;&#26597;&#26088;&#22312;&#24635;&#32467;&#25351;&#23548;&#23398;&#20064;&#30340;&#24403;&#21069;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;
&lt;/p&gt;
&lt;p&gt;
Task semantics can be expressed by a set of input-to-output examples or a piece of textual instruction. Conventional machine learning approaches for natural language processing (NLP) mainly rely on the availability of large-scale sets of task-specific examples. Two issues arise: first, collecting task-specific labeled examples does not apply to scenarios where tasks may be too complicated or costly to annotate, or the system is required to handle a new task immediately; second, this is not user-friendly since end-users are probably more willing to provide task description rather than a set of examples before using the system. Therefore, the community is paying increasing interest in a new supervision-seeking paradigm for NLP: learning from task instructions. Despite its impressive progress, there are some common issues that the community struggles with. This survey paper tries to summarize the current research on instruction learning, particularly, by answering the following questions:
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SPDF&#31639;&#27861;&#26469;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#23494;&#38598;&#24494;&#35843;&#21017;&#21487;&#20197;&#20445;&#35777;&#39640;&#24615;&#33021;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.10464</link><description>&lt;p&gt;
SPDF&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#31232;&#30095;&#39044;&#35757;&#32451;&#21644;&#23494;&#38598;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models. (arXiv:2303.10464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SPDF&#31639;&#27861;&#26469;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#23494;&#38598;&#24494;&#35843;&#21017;&#21487;&#20197;&#20445;&#35777;&#39640;&#24615;&#33021;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#22810;&#39033;&#31361;&#30772;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#35821;&#35328;&#27169;&#22411;&#39318;&#20808;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#36328;&#22495;&#30693;&#35782;&#30340;&#39044;&#35757;&#32451;&#65288;&#20363;&#22914;&#65292;Pile&#12289;MassiveText&#31561;&#65289;&#65292;&#28982;&#21518;&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65288;&#20363;&#22914;&#65292;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12289;&#25991;&#26412;&#25688;&#35201;&#31561;&#65289;&#12290;&#34429;&#28982;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#26377;&#21161;&#20110;&#25552;&#39640;LLM&#24615;&#33021;&#65292;&#20294;&#36825;&#20063;&#24102;&#26469;&#20102;&#26497;&#20026;&#31105;&#27490;&#24615;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#39044;&#35757;&#32451;LLMs&#36890;&#24120;&#38656;&#35201;&#27604;&#24494;&#35843;&#28436;&#20064;&#26356;&#22810;&#30340;FLOPs&#65292;&#20004;&#20010;&#38454;&#27573;&#20043;&#38388;&#30340;&#27169;&#22411;&#23481;&#37327;&#36890;&#24120;&#20445;&#25345;&#19981;&#21464;&#12290;&#20026;&#20102;&#23454;&#29616;&#30456;&#23545;&#20110;&#35757;&#32451;FLOPs&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#20004;&#20010;&#38454;&#27573;&#20043;&#38388;&#35299;&#32806;&#27169;&#22411;&#23481;&#37327;&#65292;&#24182;&#24341;&#20837;&#31232;&#30095;&#39044;&#35757;&#32451;&#21644;&#23494;&#38598;&#24494;&#35843;&#65288;SPDF&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#20165;&#35757;&#32451;&#23376;&#38598;&#26435;&#37325;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pre-training and fine-tuning paradigm has contributed to a number of breakthroughs in Natural Language Processing (NLP). Instead of directly training on a downstream task, language models are first pre-trained on large datasets with cross-domain knowledge (e.g., Pile, MassiveText, etc.) and then fine-tuned on task-specific data (e.g., natural language generation, text summarization, etc.). Scaling the model and dataset size has helped improve the performance of LLMs, but unfortunately, this also leads to highly prohibitive computational costs. Pre-training LLMs often require orders of magnitude more FLOPs than fine-tuning and the model capacity often remains the same between the two phases. To achieve training efficiency w.r.t training FLOPs, we propose to decouple the model capacity between the two phases and introduce Sparse Pre-training and Dense Fine-tuning (SPDF). In this work, we show the benefits of using unstructured weight sparsity to train only a subset of weights during 
&lt;/p&gt;</description></item><item><title>GazeReader&#26159;&#19968;&#31181;&#21033;&#29992;&#32593;&#32476;&#25668;&#20687;&#22836;&#26469;&#26816;&#27979;&#26410;&#30693;&#21333;&#35789;&#30340;&#26041;&#27861;&#65292;&#21487;&#36741;&#21161;&#33521;&#35821;&#20316;&#20026;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#30340;&#38405;&#35835;&#20307;&#39564;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#20026;98.09%&#21644;75.73%&#12290;</title><link>http://arxiv.org/abs/2303.10443</link><description>&lt;p&gt;
GazeReader&#65306;&#21033;&#29992;&#32593;&#32476;&#25668;&#20687;&#22836;&#26816;&#27979;&#33521;&#35821;&#20316;&#20026;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#30340;&#26410;&#30693;&#21333;&#35789;
&lt;/p&gt;
&lt;p&gt;
GazeReader: Detecting Unknown Word Using Webcam for English as a Second Language (ESL) Learners. (arXiv:2303.10443v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10443
&lt;/p&gt;
&lt;p&gt;
GazeReader&#26159;&#19968;&#31181;&#21033;&#29992;&#32593;&#32476;&#25668;&#20687;&#22836;&#26469;&#26816;&#27979;&#26410;&#30693;&#21333;&#35789;&#30340;&#26041;&#27861;&#65292;&#21487;&#36741;&#21161;&#33521;&#35821;&#20316;&#20026;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#30340;&#38405;&#35835;&#20307;&#39564;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#20026;98.09%&#21644;75.73%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#26410;&#30693;&#21333;&#35789;&#30340;&#25216;&#26415;&#21487;&#20197;&#20026;&#36741;&#21161;&#33521;&#35821;&#20316;&#20026;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#30340;&#26032;&#24212;&#29992;&#25552;&#20379;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20182;&#20204;&#30340;&#38405;&#35835;&#20307;&#39564;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#20195;&#26410;&#30693;&#21333;&#35789;&#26816;&#27979;&#26041;&#27861;&#38656;&#35201;&#20855;&#26377;&#39640;&#31934;&#24230;&#30340;&#19987;&#29992;&#30524;&#21160;&#20202;&#65292;&#36825;&#20123;&#35774;&#22791;&#23545;&#20110;&#32456;&#31471;&#29992;&#25143;&#26469;&#35828;&#19981;&#26131;&#33719;&#21462;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GazeReader&#65292;&#19968;&#31181;&#21482;&#20351;&#29992;&#32593;&#32476;&#25668;&#20687;&#22836;&#30340;&#26410;&#30693;&#21333;&#35789;&#26816;&#27979;&#26041;&#27861;&#12290;GazeReader&#36319;&#36394;&#23398;&#20064;&#32773;&#30340;&#20957;&#35270;&#65292;&#28982;&#21518;&#24212;&#29992;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#32534;&#30721;&#25991;&#26412;&#20449;&#24687;&#20197;&#23450;&#20301;&#26410;&#30693;&#21333;&#35789;&#12290;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#30693;&#35782;&#22686;&#24378;&#25216;&#26415;&#65292;&#21253;&#25324;&#35789;&#39057;&#12289;&#35789;&#24615;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#20026;98.09%&#21644;75.73%&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;ESL&#38405;&#35835;&#30340;&#35774;&#35745;&#33539;&#22260;&#24182;&#35752;&#35770;&#20102;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic unknown word detection techniques can enable new applications for assisting English as a Second Language (ESL) learners, thus improving their reading experiences. However, most modern unknown word detection methods require dedicated eye-tracking devices with high precision that are not easily accessible to end-users. In this work, we propose GazeReader, an unknown word detection method only using a webcam. GazeReader tracks the learner's gaze and then applies a transformer-based machine learning model that encodes the text information to locate the unknown word. We applied knowledge enhancement including term frequency, part of speech, and named entity recognition to improve the performance. The user study indicates that the accuracy and F1-score of our method were 98.09% and 75.73%, respectively. Lastly, we explored the design scope for ESL reading and discussed the findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20572;&#29992;&#35789;&#22312;&#36719;&#20214;&#24037;&#31243;&#25991;&#26723;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#32463;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#20572;&#29992;&#35789;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30740;&#31350;&#24037;&#20855;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;19&#20010;&#35780;&#20272;&#25514;&#26045;&#20013;&#26377;17&#20010;&#35780;&#20272;&#25514;&#26045;&#21463;&#30410;&#20110;&#20572;&#29992;&#35789;&#30340;&#28040;&#38500;&#12290;</title><link>http://arxiv.org/abs/2303.10439</link><description>&lt;p&gt;
&#22788;&#29702;&#36719;&#20214;&#24037;&#31243;&#25991;&#26723;&#30340;&#20572;&#29992;&#35789;&#65306;&#23427;&#20204;&#37325;&#35201;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Stop Words for Processing Software Engineering Documents: Do they Matter?. (arXiv:2303.10439v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20572;&#29992;&#35789;&#22312;&#36719;&#20214;&#24037;&#31243;&#25991;&#26723;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#32463;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#20572;&#29992;&#35789;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30740;&#31350;&#24037;&#20855;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;19&#20010;&#35780;&#20272;&#25514;&#26045;&#20013;&#26377;17&#20010;&#35780;&#20272;&#25514;&#26045;&#21463;&#30410;&#20110;&#20572;&#29992;&#35789;&#30340;&#28040;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20572;&#29992;&#35789;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#20855;&#26377;&#39044;&#27979;&#24615;&#30340;&#65292;&#22240;&#27492;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#36890;&#24120;&#20250;&#34987;&#21435;&#38500;&#12290;&#28982;&#32780;&#65292;&#19981;&#30830;&#23450;&#24615;&#35789;&#27719;&#30340;&#23450;&#20041;&#26159;&#27169;&#31946;&#30340;&#65292;&#22240;&#27492;&#22823;&#22810;&#25968;&#31639;&#27861;&#20351;&#29992;&#22522;&#20110;&#36890;&#29992;&#30693;&#35782;&#30340;&#20572;&#29992;&#35789;&#21015;&#34920;&#26469;&#21435;&#38500;&#20572;&#29992;&#35789;&#12290;&#23398;&#32773;&#20204;&#19968;&#30452;&#22312;&#23601;&#20572;&#29992;&#35789;&#30340;&#20351;&#29992;&#20215;&#20540;&#36827;&#34892;&#35752;&#35770;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20572;&#29992;&#35789;&#21435;&#38500;&#22312;&#36719;&#20214;&#24037;&#31243;&#32972;&#26223;&#19979;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22797;&#21046;&#24182;&#23454;&#39564;&#20102;&#19977;&#20010;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#24037;&#20855;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#30456;&#20851;&#25991;&#26412;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#26469;&#33258; Stack Overflow &#30340;10,000&#20010;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20256;&#32479;&#30340;&#20449;&#24687;&#35770;&#26041;&#27861;&#35782;&#21035;&#20102;200&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#20572;&#29992;&#35789;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#20572;&#29992;&#35789;&#19982;&#20351;&#29992;&#36890;&#29992;&#20572;&#29992;&#21015;&#34920;&#30456;&#27604;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#30740;&#31350;&#24037;&#20855;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;19&#20010;&#35780;&#20272;&#25514;&#26045;&#20013;&#26377;17&#20010;&#35780;&#20272;&#25514;&#26045;&#21463;&#30410;&#20110;&#20572;&#29992;&#35789;&#30340;&#28040;&#38500;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#22312;&#22788;&#29702;&#36719;&#20214;&#24037;&#31243;&#25991;&#26723;&#20013;&#21435;&#38500;&#39046;&#22495;&#29305;&#23450;&#30340;&#20572;&#29992;&#35789;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stop words, which are considered non-predictive, are often eliminated in natural language processing tasks. However, the definition of uninformative vocabulary is vague, so most algorithms use general knowledge-based stop lists to remove stop words. There is an ongoing debate among academics about the usefulness of stop word elimination, especially in domain-specific settings. In this work, we investigate the usefulness of stop word removal in a software engineering context. To do this, we replicate and experiment with three software engineering research tools from related work. Additionally, we construct a corpus of software engineering domain-related text from 10,000 Stack Overflow questions and identify 200 domain-specific stop words using traditional information-theoretic methods. Our results show that the use of domain-specific stop words significantly improved the performance of research tools compared to the use of a general stop list and that 17 out of 19 evaluation measures sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#30340;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2303.10430</link><description>&lt;p&gt;
NoisyHate&#65306;&#22312;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#19979;&#23545;&#20869;&#23481;&#23457;&#26680;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
NoisyHate: Benchmarking Content Moderation Machine Learning Models with Human-Written Perturbations Online. (arXiv:2303.10430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#30340;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#65292;&#20855;&#26377;&#26377;&#23475;&#20869;&#23481;&#30340;&#22312;&#32447;&#25991;&#26412;&#26159;&#19968;&#31181;&#23041;&#32961;&#65292;&#21487;&#33021;&#20250;&#24341;&#36215;&#32593;&#32476;&#39578;&#25200;&#12290;&#23613;&#31649;&#35768;&#22810;&#24179;&#21488;&#37319;&#21462;&#20102;&#25514;&#26045;&#65292;&#20363;&#22914;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#26469;&#20943;&#23569;&#20854;&#24433;&#21709;&#65292;&#20294;&#37027;&#20123;&#26377;&#23475;&#20869;&#23481;&#21457;&#24067;&#32773;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#20462;&#25913;&#26377;&#23475;&#35789;&#27719;&#30340;&#25340;&#20889;&#26469;&#36867;&#36991;&#31995;&#32479;&#12290;&#36825;&#20123;&#20462;&#25913;&#21518;&#30340;&#21333;&#35789;&#20063;&#31216;&#20026;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#25200;&#21160;&#12290;&#35768;&#22810;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#23450;&#30340;&#25216;&#26415;&#26469;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33719;&#24471;&#35782;&#21035;&#36825;&#20123;&#25200;&#21160;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25200;&#21160;&#19982;&#20154;&#31867;&#32534;&#20889;&#30340;&#25200;&#21160;&#20043;&#38388;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25307;&#21215;&#20102;&#19968;&#32452;&#24037;&#20154;&#26469;&#35780;&#20272;&#27492;&#27979;&#35797;&#38598;&#30340;&#36136;&#37327;&#24182;&#21024;&#38500;&#20302;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26816;&#26597;&#25105;&#20204;&#30340;&#25200;&#21160;&#26159;&#21542;&#21487;&#20197;&#24402;&#19968;&#21270;&#20026;&#20854;&#24178;&#20928;&#29256;&#26412;&#65292;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#27979;&#35797;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online texts with toxic content are a threat in social media that might cause cyber harassment. Although many platforms applied measures, such as machine learning-based hate-speech detection systems, to diminish their effect, those toxic content publishers can still evade the system by modifying the spelling of toxic words. Those modified words are also known as human-written text perturbations. Many research works developed certain techniques to generate adversarial samples to help the machine learning models obtain the ability to recognize those perturbations. However, there is still a gap between those machine-generated perturbations and human-written perturbations. In this paper, we introduce a benchmark test set containing human-written perturbations online for toxic speech detection models. We also recruited a group of workers to evaluate the quality of this test set and dropped low-quality samples. Meanwhile, to check if our perturbation can be normalized to its clean version, w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;GPT-3&#21644;GPT-3.5&#31995;&#21015;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#21457;&#23637;&#36235;&#21183;&#65292;&#24182;&#21457;&#29616;&#26368;&#26032;&#30340;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#21069;&#19968;&#20195;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2303.10420</link><description>&lt;p&gt;
GPT-3&#21644;GPT-3.5&#31995;&#21015;&#27169;&#22411;&#30340;&#20840;&#38754;&#33021;&#21147;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models. (arXiv:2303.10420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;GPT-3&#21644;GPT-3.5&#31995;&#21015;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#21457;&#23637;&#36235;&#21183;&#65292;&#24182;&#21457;&#29616;&#26368;&#26032;&#30340;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#21069;&#19968;&#20195;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT&#31995;&#21015;&#27169;&#22411;&#65292;&#22914;GPT-3&#12289;CodeX&#12289;InstructGPT&#12289;ChatGPT&#31561;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24050;&#26377;&#22823;&#37327;&#30740;&#31350;&#25506;&#35752;&#20102;GPT&#31995;&#21015;&#27169;&#22411;&#19982;&#31934;&#35843;&#27169;&#22411;&#22312;&#33021;&#21147;&#19978;&#30340;&#24046;&#24322;&#65292;&#20294;&#23545;&#20110;GPT&#31995;&#21015;&#27169;&#22411;&#30340;&#33021;&#21147;&#38543;&#26102;&#38388;&#28436;&#21270;&#30340;&#30740;&#31350;&#21364;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#20026;&#20102;&#20840;&#38754;&#20998;&#26512;GPT&#31995;&#21015;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#20845;&#20010;&#20195;&#34920;&#24615;&#27169;&#22411;&#65292;&#21253;&#25324;&#20004;&#20010;GPT-3&#31995;&#21015;&#27169;&#22411;&#65288;&#21363;davinci&#21644;text-davinci-001&#65289;&#21644;&#22235;&#20010;GPT-3.5&#31995;&#21015;&#27169;&#22411;&#65288;&#21363;code-davinci-002&#12289;text-davinci-002&#12289;text-davinci-003&#21644;gpt-3.5-turbo&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;21&#20010;&#25968;&#25454;&#38598;&#22312;&#20061;&#20010;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#19978;&#35780;&#20272;&#23427;&#20204;&#30340;&#34920;&#29616;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#27599;&#20010;&#20219;&#21153;&#20013;&#19981;&#21516;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;GPT&#31995;&#21015;&#27169;&#22411;&#30340;&#25972;&#20307;&#33021;&#21147;&#32487;&#32493;&#38543;&#26102;&#38388;&#28436;&#21270;&#65292;&#26368;&#26032;&#30340;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#21069;&#19968;&#20195;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT series models, such as GPT-3, CodeX, InstructGPT, ChatGPT, and so on, have gained considerable attention due to their exceptional natural language processing capabilities. However, despite the abundance of research on the difference in capabilities between GPT series models and fine-tuned models, there has been limited attention given to the evolution of GPT series models' capabilities over time. To conduct a comprehensive analysis of the capabilities of GPT series models, we select six representative models, comprising two GPT-3 series models (i.e., davinci and text-davinci-001) and four GPT-3.5 series models (i.e., code-davinci-002, text-davinci-002, text-davinci-003, and gpt-3.5-turbo). We evaluate their performance on nine natural language understanding (NLU) tasks using 21 datasets. In particular, we compare the performance and robustness of different models for each task under zero-shot and few-shot scenarios. Our extensive experiments reveal that the overall ability of GPT s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#34920;&#25512;&#29702;&#30340;&#24320;&#25918;&#24335;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#24120;&#35782;&#38382;&#39064;&#30340;&#38544;&#24335;&#22810;&#36339;&#25512;&#29702;&#24182;&#26500;&#24314;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#24320;&#25918;&#24335;&#30693;&#35782;&#22270;&#65292;&#26377;&#26395;&#22312;&#27809;&#26377;&#39044;&#23450;&#20041;&#31572;&#26696;&#36873;&#39033;&#30340;&#30495;&#23454;&#24773;&#22659;&#24212;&#29992;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.10395</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#22270;&#34920;&#25512;&#29702;&#30340;&#24320;&#25918;&#24335;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Graph-Guided Reasoning Approach for Open-ended Commonsense Question Answering. (arXiv:2303.10395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10395
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#34920;&#25512;&#29702;&#30340;&#24320;&#25918;&#24335;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#24120;&#35782;&#38382;&#39064;&#30340;&#38544;&#24335;&#22810;&#36339;&#25512;&#29702;&#24182;&#26500;&#24314;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#24320;&#25918;&#24335;&#30693;&#35782;&#22270;&#65292;&#26377;&#26395;&#22312;&#27809;&#26377;&#39044;&#23450;&#20041;&#31572;&#26696;&#36873;&#39033;&#30340;&#30495;&#23454;&#24773;&#22659;&#24212;&#29992;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#22810;&#39033;&#36873;&#25321;&#30340;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27169;&#22411;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#27492;&#31867;&#38382;&#31572;&#31995;&#32479;&#65292;&#22240;&#20026;&#27809;&#26377;&#25552;&#20379;&#31572;&#26696;&#20505;&#36873;&#20154;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25361;&#25112;&#38598;&#65288;OpenCSR&#65289;&#29992;&#20110;&#24320;&#25918;&#24335;&#24120;&#35782;&#25512;&#29702;&#65292;&#20854;&#20013;&#21253;&#21547;&#33258;&#28982;&#31185;&#23398;&#38382;&#39064;&#32780;&#27809;&#26377;&#39044;&#23450;&#20041;&#30340;&#36873;&#39033;&#12290;&#22312;OpenCSR&#25361;&#25112;&#38598;&#20013;&#65292;&#35768;&#22810;&#38382;&#39064;&#38656;&#35201;&#38544;&#21547;&#30340;&#22810;&#36339;&#25512;&#29702;&#24182;&#19988;&#20915;&#31574;&#31354;&#38388;&#24456;&#22823;&#65292;&#21453;&#26144;&#20102;&#36825;&#39033;&#20219;&#21153;&#30340;&#22256;&#38590;&#24615;&#36136;&#12290;&#29616;&#26377;&#30340;OpenCSR&#24037;&#20316;&#20165;&#30528;&#30524;&#20110;&#25913;&#36827;&#26816;&#32034;&#36807;&#31243;&#65292;&#20174;&#25991;&#26412;&#30693;&#35782;&#24211;&#20013;&#25552;&#21462;&#30456;&#20851;&#20107;&#23454;&#21477;&#23376;&#65292;&#32780;&#23558;&#37325;&#35201;&#19988;&#38750;&#24179;&#20961;&#30340;&#25512;&#29702;&#20219;&#21153;&#36229;&#20986;&#33539;&#22260;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#33539;&#22260;&#65292;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#25903;&#25745;&#20107;&#23454;&#26500;&#24314;&#38382;&#39064;&#30456;&#20851;&#24320;&#25918;&#30693;&#35782;&#22270;&#30340;&#25512;&#29702;&#22120;&#65292;&#24182;&#37319;&#29992;&#39034;&#24207;
&lt;/p&gt;
&lt;p&gt;
Recently, end-to-end trained models for multiple-choice commonsense question answering (QA) have delivered promising results. However, such question-answering systems cannot be directly applied in real-world scenarios where answer candidates are not provided. Hence, a new benchmark challenge set for open-ended commonsense reasoning (OpenCSR) has been recently released, which contains natural science questions without any predefined choices. On the OpenCSR challenge set, many questions require implicit multi-hop reasoning and have a large decision space, reflecting the difficult nature of this task. Existing work on OpenCSR sorely focuses on improving the retrieval process, which extracts relevant factual sentences from a textual knowledge base, leaving the important and non-trivial reasoning task outside the scope. In this work, we extend the scope to include a reasoner that constructs a question-dependent open knowledge graph based on retrieved supporting facts and employs a sequentia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;WFST&#26694;&#26550;&#30340;&#30340;RNN-Transducer Losses&#24378;&#22823;&#19988;&#21487;&#25193;&#23637;&#30340;&#23454;&#29616;&#65292;&#8220;Compose-Transducer&#8221;&#21644;&#8220;Grid-Transducer&#8221;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;W-Transducer Loss&#26469;&#23637;&#31034;&#32452;&#20214;&#30340;&#26131;&#25193;&#23637;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;W-Transducer&#65288;W-RNNT&#65289;&#34920;&#29616;&#20986;&#27604;&#26631;&#20934;RNN-T&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10384</link><description>&lt;p&gt;
&#22522;&#20110;WFST&#26694;&#26550;&#30340;RNN-Transducer Losses&#24378;&#22823;&#19988;&#21487;&#25193;&#23637;&#30340;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Powerful and Extensible WFST Framework for RNN-Transducer Losses. (arXiv:2303.10384v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;WFST&#26694;&#26550;&#30340;&#30340;RNN-Transducer Losses&#24378;&#22823;&#19988;&#21487;&#25193;&#23637;&#30340;&#23454;&#29616;&#65292;&#8220;Compose-Transducer&#8221;&#21644;&#8220;Grid-Transducer&#8221;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;W-Transducer Loss&#26469;&#23637;&#31034;&#32452;&#20214;&#30340;&#26131;&#25193;&#23637;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;W-Transducer&#65288;W-RNNT&#65289;&#34920;&#29616;&#20986;&#27604;&#26631;&#20934;RNN-T&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#36716;&#31227;&#22120;&#65288;WFST&#65289;&#30340;&#26694;&#26550;&#65292;&#20197;&#31616;&#21270;&#23545;RNN-Transducer&#65288;RNN-T&#65289; Losses&#30340;&#20462;&#25913;&#24320;&#21457;&#12290;&#29616;&#26377;&#30340;RNN-T&#23454;&#29616;&#20351;&#29992;&#19982;CUDA&#30456;&#20851;&#30340;&#20195;&#30721;&#65292;&#38590;&#20197;&#25193;&#23637;&#21644;&#35843;&#35797;&#12290;WFST&#26131;&#20110;&#26500;&#24314;&#21644;&#25193;&#23637;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#21487;&#35270;&#21270;&#36827;&#34892;&#35843;&#35797;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#22522;&#20110;WFST&#30340;RNN-T&#23454;&#29616;&#65306;&#65288;1&#65289;&#8220;Compose-Transducer&#8221;&#65292;&#23427;&#22522;&#20110;&#22768;&#23398;&#21644;&#25991;&#26412;&#26550;&#26500;&#30340;WFST&#22270;&#32452;&#21512;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#21644;&#26131;&#20110;&#20462;&#25913;&#65307;&#65288;2&#65289;&#8220;Grid-Transducer&#8221;&#65292;&#30452;&#25509;&#26500;&#24314;&#26230;&#26684;&#29992;&#20110;&#36827;&#19968;&#27493;&#35745;&#31639;&#65292;&#26368;&#32039;&#20945;&#21644;&#35745;&#31639;&#25928;&#29575;&#26368;&#39640;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;W-Transducer Loss&#65292;&#21363;Connectionist Temporal Classification with Wild Cards&#30340;&#36866;&#24212;&#24615;&#65292;&#23637;&#31034;&#20102;&#32452;&#20214;&#30340;&#26131;&#25193;&#23637;&#24615;&#12290;&#22312;&#32570;&#23569;&#36716;&#24405;&#24320;&#22836;&#37096;&#20998;&#30340;&#24369;&#30417;&#30563;&#25968;&#25454;&#35774;&#32622;&#20013;&#65292;W-Transducer&#65288;W-RNNT&#65289;&#22987;&#32456;&#20248;&#20110;&#26631;&#20934;RNN-T&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a framework based on Weighted Finite-State Transducers (WFST) to simplify the development of modifications for RNN-Transducer (RNN-T) loss. Existing implementations of RNN-T use CUDA-related code, which is hard to extend and debug. WFSTs are easy to construct and extend, and allow debugging through visualization. We introduce two WFST-powered RNN-T implementations: (1) "Compose-Transducer", based on a composition of the WFST graphs from acoustic and textual schema -- computationally competitive and easy to modify; (2) "Grid-Transducer", which constructs the lattice directly for further computations -- most compact, and computationally efficient. We illustrate the ease of extensibility through introduction of a new W-Transducer loss -- the adaptation of the Connectionist Temporal Classification with Wild Cards. W-Transducer (W-RNNT) consistently outperforms the standard RNN-T in a weakly-supervised data setup with missing parts of transcriptions at the beginning and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#27604;&#36739;&#21644;&#20998;&#26512;&#19981;&#21516;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#31616;&#26131;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;RoBERTa&#21644;ELECTRA&#27169;&#22411;&#22312;KGQA&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2303.10368</link><description>&lt;p&gt;
&#31616;&#26131;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Pre-trained Language Models in Simple Knowledge Graph Question Answering. (arXiv:2303.10368v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10368
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#27604;&#36739;&#21644;&#20998;&#26512;&#19981;&#21516;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#31616;&#26131;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;RoBERTa&#21644;ELECTRA&#27169;&#22411;&#22312;KGQA&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22914;BERT&#26368;&#36817;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#37324;&#31243;&#30865;&#12290;&#29616;&#22312;&#65292;NLP&#30028;&#26222;&#36941;&#35748;&#20026;&#37319;&#29992;PLMs&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#39592;&#24178;&#26159;&#19968;&#31181;&#20849;&#35782;&#12290;&#22312;&#26368;&#36817;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#65288;KGQA&#65289;&#30740;&#31350;&#20013;&#65292;BERT&#25110;&#20854;&#21464;&#20307;&#24050;&#25104;&#20026;&#20854;KGQA&#27169;&#22411;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#32570;&#20047;&#23545;KGQA&#20013;&#19981;&#21516;PLMs&#24615;&#33021;&#30340;&#20840;&#38754;&#30740;&#31350;&#21644;&#27604;&#36739;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#20004;&#31181;&#22522;&#20110;PLMs&#30340;&#22522;&#26412;KGQA&#26694;&#26550;&#65292;&#27604;&#36739;&#20102;&#20061;&#31181;PLMs&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#27969;&#34892;&#30340;SimpleQuestions&#22522;&#20934;&#27979;&#35797;&#30340;&#19977;&#20010;&#26356;&#22823;&#35268;&#27169;&#30340;KGs&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#30740;&#31350;PLMs&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#20180;&#32454;&#20998;&#26512;&#20102;&#25152;&#26377;&#22522;&#20110;PLMs&#30340;KGQA&#22522;&#30784;&#26694;&#26550;&#22312;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#20854;&#20182;&#20004;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;WebQuestionSP&#21644;FreebaseQA&#19978;&#30340;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;RoBERTa&#21644;ELECTRA&#27169;&#22411;&#22312;KGQA&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained language models (PLMs) such as BERT have recently achieved great success and become a milestone in natural language processing (NLP). It is now the consensus of the NLP community to adopt PLMs as the backbone for downstream tasks. In recent works on knowledge graph question answering (KGQA), BERT or its variants have become necessary in their KGQA models. However, there is still a lack of comprehensive research and comparison of the performance of different PLMs in KGQA. To this end, we summarize two basic KGQA frameworks based on PLMs without additional neural network modules to compare the performance of nine PLMs in terms of accuracy and efficiency. In addition, we present three benchmarks for larger-scale KGs based on the popular SimpleQuestions benchmark to investigate the scalability of PLMs. We carefully analyze the results of all PLMs-based KGQA basic frameworks on these benchmarks and two other popular datasets, WebQuestionSP and FreebaseQA, and find th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#20013;&#30340;&#37096;&#20998;&#30693;&#35782;&#24211;&#25512;&#29702;&#38382;&#39064;&#65292;&#21457;&#29616;&#30001;&#20110;&#31934;&#24230;&#19979;&#38477;&#23548;&#33268;EL&#24615;&#33021;&#20986;&#29616;&#28798;&#38590;&#24615;&#19979;&#38477;&#65292;&#32780;&#19988;EL&#33539;&#20363;&#26080;&#27861;&#22788;&#29702;&#26080;&#27861;&#38142;&#25509;&#30340;&#25552;&#21450;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36174;&#22238;&#26041;&#27861;&#26469;&#35299;&#20915;NIL&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.10330</link><description>&lt;p&gt;
&#25506;&#32034;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#20013;&#30340;&#37096;&#20998;&#30693;&#35782;&#24211;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Exploring Partial Knowledge Base Inference in Biomedical Entity Linking. (arXiv:2303.10330v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#20013;&#30340;&#37096;&#20998;&#30693;&#35782;&#24211;&#25512;&#29702;&#38382;&#39064;&#65292;&#21457;&#29616;&#30001;&#20110;&#31934;&#24230;&#19979;&#38477;&#23548;&#33268;EL&#24615;&#33021;&#20986;&#29616;&#28798;&#38590;&#24615;&#19979;&#38477;&#65292;&#32780;&#19988;EL&#33539;&#20363;&#26080;&#27861;&#22788;&#29702;&#26080;&#27861;&#38142;&#25509;&#30340;&#25552;&#21450;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36174;&#22238;&#26041;&#27861;&#26469;&#35299;&#20915;NIL&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#21629;&#21517;&#23454;&#20307;&#28040;&#27495;&#65288;NED&#65289;&#12290;EL&#27169;&#22411;&#22312;&#30001;&#39044;&#23450;&#20041;&#30340;&#30693;&#35782;&#24211;&#26631;&#35760;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#24773;&#20917;&#26159;&#21482;&#26377;&#30693;&#35782;&#24211;&#30340;&#23376;&#38598;&#20013;&#30340;&#23454;&#20307;&#23545;&#21033;&#30410;&#30456;&#20851;&#32773;&#26377;&#20215;&#20540;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#24773;&#20917;&#20026;&#37096;&#20998;&#30693;&#35782;&#24211;&#25512;&#29702;&#65306;&#20351;&#29992;&#19968;&#20010;&#30693;&#35782;&#24211;&#23545;EL&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23545;&#20854;&#37096;&#20998;&#36827;&#34892;&#25512;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#36825;&#31181;&#23454;&#38469;&#19978;&#38750;&#24120;&#26377;&#20215;&#20540;&#20294;&#26126;&#26174;&#19981;&#22815;&#30740;&#31350;&#30340;&#24773;&#20917;&#30340;&#35814;&#32454;&#23450;&#20041;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;EL&#33539;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#37096;&#20998;&#30693;&#35782;&#24211;&#25512;&#29702;&#22522;&#20934;&#65292;&#24182;&#21457;&#29616;&#30001;&#20110;&#22823;&#37327;&#31934;&#24230;&#19979;&#38477;&#23548;&#33268;EL&#24615;&#33021;&#20986;&#29616;&#28798;&#38590;&#24615;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#36825;&#20123;EL&#33539;&#20363;&#26080;&#27861;&#27491;&#30830;&#22788;&#29702;&#26080;&#27861;&#38142;&#25509;&#30340;&#25552;&#21450;&#65288;NIL&#65289;&#65292;&#22240;&#27492;&#23427;&#20204;&#23545;&#37096;&#20998;&#30693;&#35782;&#24211;&#25512;&#29702;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#36174;&#22238;&#26041;&#27861;&#26469;&#35299;&#20915;NIL&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical entity linking (EL) consists of named entity recognition (NER) and named entity disambiguation (NED). EL models are trained on corpora labeled by a predefined KB. However, it is a common scenario that only entities within a subset of the KB are precious to stakeholders. We name this scenario partial knowledge base inference: training an EL model with one KB and inferring on the part of it without further training. In this work, we give a detailed definition and evaluation procedures for this practically valuable but significantly understudied scenario and evaluate methods from three representative EL paradigms. We construct partial KB inference benchmarks and witness a catastrophic degradation in EL performance due to dramatically precision drop. Our findings reveal these EL paradigms can not correctly handle unlinkable mentions (NIL), so they are not robust to partial KB inference. We also propose two simple-and-effective redemption methods to combat the NIL issue with litt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#33258;&#21160;&#38382;&#39064;&#25688;&#35201;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21457;&#29616;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#25688;&#35201;&#31995;&#32479;&#23384;&#22312;&#19981;&#21516;&#20540;&#24471;&#20851;&#27880;&#30340;&#29305;&#24449;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.10328</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#33258;&#21160;&#38382;&#39064;&#25688;&#35201;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Revisiting Automatic Question Summarization Evaluation in the Biomedical Domain. (arXiv:2303.10328v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#33258;&#21160;&#38382;&#39064;&#25688;&#35201;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21457;&#29616;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#25688;&#35201;&#31995;&#32479;&#23384;&#22312;&#19981;&#21516;&#20540;&#24471;&#20851;&#27880;&#30340;&#29305;&#24449;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#35780;&#20272;&#25351;&#26631;&#36890;&#36807;&#25552;&#20379;&#24555;&#36895;&#19988;&#20844;&#27491;&#30340;&#25688;&#35201;&#36136;&#37327;&#35780;&#20272;&#65292;&#20419;&#36827;&#20102;&#25688;&#35201;&#26041;&#27861;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25351;&#26631;&#22823;&#22810;&#26159;&#20026;&#19968;&#33324;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#26032;&#38395;&#21644;&#20250;&#35758;&#35760;&#24405;&#65292;&#25110;&#20854;&#20182;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#24320;&#21457;&#30340;&#12290;&#36825;&#20123;&#25351;&#26631;&#34987;&#24212;&#29992;&#20110;&#35780;&#20272;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#25688;&#35201;&#31995;&#32479;&#65292;&#20363;&#22914;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#25688;&#35201;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20102;&#35299;&#24120;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#26159;&#21542;&#33021;&#22815;&#35780;&#20272;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#33258;&#21160;&#21270;&#25688;&#35201;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#24182;&#20174;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#25688;&#35201;&#20219;&#21153;&#30340;&#22235;&#20010;&#19981;&#21516;&#26041;&#38754;&#35780;&#20272;&#20102;&#25688;&#35201;&#36136;&#37327;&#12290;&#26681;&#25454;&#20154;&#31867;&#35780;&#21028;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#25688;&#35201;&#31995;&#32479;&#23384;&#22312;&#19981;&#21516;&#20540;&#24471;&#20851;&#27880;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic evaluation metrics have been facilitating the rapid development of automatic summarization methods by providing instant and fair assessments of the quality of summaries. Most metrics have been developed for the general domain, especially news and meeting notes, or other language-generation tasks. However, these metrics are applied to evaluate summarization systems in different domains, such as biomedical question summarization. To better understand whether commonly used evaluation metrics are capable of evaluating automatic summarization in the biomedical domain, we conduct human evaluations of summarization quality from four different aspects of a biomedical question summarization task. Based on human judgments, we identify different noteworthy features for current automatic metrics and summarization systems as well. We also release a dataset of our human annotations to aid the research of summarization evaluation metrics in the biomedical domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25235;&#21462;Gab.com&#19978;40&#19975;&#20010;&#24656;&#24807;&#35328;&#35770;&#21644;70&#22810;&#19975;&#20010;&#20167;&#24680;&#35328;&#35770;&#65292;&#25581;&#31034;&#20102;&#24656;&#24807;&#35328;&#35770;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#26222;&#36941;&#24615;&#21644;&#25928;&#26524;&#12290;&#21457;&#24067;&#22823;&#37327;&#24656;&#24807;&#35328;&#35770;&#30340;&#29992;&#25143;&#27604;&#21457;&#24067;&#22823;&#37327;&#20167;&#24680;&#35328;&#35770;&#30340;&#29992;&#25143;&#26356;&#23481;&#26131;&#33719;&#24471;&#36861;&#38543;&#32773;&#21644;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#21344;&#25454;&#26680;&#24515;&#22320;&#20301;&#65292;&#20182;&#20204;&#36824;&#33021;&#26356;&#26377;&#25928;&#22320;&#25509;&#35302;&#21040;&#33391;&#24615;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2303.10311</link><description>&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#20013;&#24656;&#24807;&#35328;&#35770;&#30340;&#20852;&#36215;
&lt;/p&gt;
&lt;p&gt;
On the rise of fear speech in online social media. (arXiv:2303.10311v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25235;&#21462;Gab.com&#19978;40&#19975;&#20010;&#24656;&#24807;&#35328;&#35770;&#21644;70&#22810;&#19975;&#20010;&#20167;&#24680;&#35328;&#35770;&#65292;&#25581;&#31034;&#20102;&#24656;&#24807;&#35328;&#35770;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#26222;&#36941;&#24615;&#21644;&#25928;&#26524;&#12290;&#21457;&#24067;&#22823;&#37327;&#24656;&#24807;&#35328;&#35770;&#30340;&#29992;&#25143;&#27604;&#21457;&#24067;&#22823;&#37327;&#20167;&#24680;&#35328;&#35770;&#30340;&#29992;&#25143;&#26356;&#23481;&#26131;&#33719;&#24471;&#36861;&#38543;&#32773;&#21644;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#21344;&#25454;&#26680;&#24515;&#22320;&#20301;&#65292;&#20182;&#20204;&#36824;&#33021;&#26356;&#26377;&#25928;&#22320;&#25509;&#35302;&#21040;&#33391;&#24615;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23457;&#26597;&#65292;&#20197;&#38450;&#27490;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#30340;&#20256;&#25773;&#65292;&#36825;&#20123;&#35328;&#35770;&#36890;&#24120;&#20805;&#28385;&#26377;&#27602;&#30340;&#35789;&#27719;&#65292;&#26159;&#38024;&#23545;&#20010;&#20154;&#25110;&#31038;&#21306;&#30340;&#12290;&#30001;&#20110;&#36825;&#31181;&#20005;&#26684;&#30340;&#23457;&#26597;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26032;&#39062;&#32780;&#24494;&#22937;&#30340;&#25216;&#26415;&#27491;&#22312;&#34987;&#37096;&#32626;&#12290;&#20854;&#20013;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#24656;&#24807;&#35328;&#35770;&#12290;&#27491;&#22914;&#20854;&#21517;&#31216;&#25152;&#31034;&#65292;&#24656;&#24807;&#35328;&#35770;&#35797;&#22270;&#22312;&#30446;&#26631;&#31038;&#21306;&#20013;&#24341;&#36215;&#24656;&#24807;&#12290;&#34429;&#28982;&#23427;&#26159;&#24494;&#22937;&#30340;&#65292;&#20294;&#23427;&#21487;&#33021;&#38750;&#24120;&#26377;&#25928;&#65292;&#32463;&#24120;&#23558;&#31038;&#21306;&#25512;&#21521;&#36523;&#20307;&#20914;&#31361;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#31038;&#20132;&#23186;&#20307;&#20013;&#23427;&#20204;&#30340;&#26222;&#36941;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#26088;&#22312;&#20102;&#35299;&#20174;Gab.com&#25910;&#38598;&#30340;40&#19975;&#20010;&#24656;&#24807;&#35328;&#35770;&#21644;70&#22810;&#19975;&#20010;&#20167;&#24680;&#35328;&#35770;&#30340;&#26222;&#21450;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21457;&#24067;&#22823;&#37327;&#24656;&#24807;&#35328;&#35770;&#30340;&#29992;&#25143;&#27604;&#21457;&#24067;&#22823;&#37327;&#20167;&#24680;&#35328;&#35770;&#30340;&#29992;&#25143;&#33719;&#21462;&#26356;&#22810;&#30340;&#36861;&#38543;&#32773;&#65292;&#24182;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#21344;&#25454;&#26356;&#20026;&#26680;&#24515;&#30340;&#20301;&#32622;&#12290;&#19982;&#20167;&#24680;&#35328;&#35770;&#29992;&#25143;&#30456;&#27604;&#65292;&#20182;&#20204;&#36824;&#33021;&#26356;&#26377;&#25928;&#22320;&#25509;&#35302;&#21040;&#33391;&#24615;&#29992;&#25143;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#29992;&#25143;&#23545;&#24656;&#24807;&#35328;&#35770;&#30340;&#25509;&#21463;&#31243;&#24230;&#22312;&#36825;&#19968;&#29616;&#35937;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#30740;&#31350;&#32467;&#26524;&#30340;&#21547;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#26816;&#27979;&#21644;&#39044;&#38450;&#31038;&#20132;&#23186;&#20307;&#20013;&#24656;&#24807;&#35328;&#35770;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, social media platforms are heavily moderated to prevent the spread of online hate speech, which is usually fertile in toxic words and is directed toward an individual or a community. Owing to such heavy moderation, newer and more subtle techniques are being deployed. One of the most striking among these is fear speech. Fear speech, as the name suggests, attempts to incite fear about a target community. Although subtle, it might be highly effective, often pushing communities toward a physical conflict. Therefore, understanding their prevalence in social media is of paramount importance. This article presents a large-scale study to understand the prevalence of 400K fear speech and over 700K hate speech posts collected from Gab.com. Remarkably, users posting a large number of fear speech accrue more followers and occupy more central positions in social networks than users posting a large number of hate speech. They can also reach out to benign users more effectively than hate sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20250;&#35805;&#26641;&#25628;&#32034;(CTS)&#65292;&#23427;&#21487;&#20197;&#26550;&#36215;FAQ&#21644;&#23545;&#35805;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#39046;&#22495;&#19987;&#23478;&#21487;&#20197;&#23450;&#20041;&#23545;&#35805;&#26641;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#25442;&#20026;&#19968;&#20010;&#26377;&#25928;&#30340;&#23545;&#35805;&#31574;&#30053;&#65292;&#21482;&#23398;&#20064;&#25552;&#20986;&#23548;&#33322;&#29992;&#25143;&#36798;&#21040;&#30446;&#26631;&#25152;&#38656;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.10227</link><description>&lt;p&gt;
&#20250;&#35805;&#26641;&#25628;&#32034;&#65306;&#19968;&#39033;&#26032;&#30340;&#28151;&#21512;&#23545;&#35805;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Conversational Tree Search: A New Hybrid Dialog Task. (arXiv:2303.10227v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20250;&#35805;&#26641;&#25628;&#32034;(CTS)&#65292;&#23427;&#21487;&#20197;&#26550;&#36215;FAQ&#21644;&#23545;&#35805;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#39046;&#22495;&#19987;&#23478;&#21487;&#20197;&#23450;&#20041;&#23545;&#35805;&#26641;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#25442;&#20026;&#19968;&#20010;&#26377;&#25928;&#30340;&#23545;&#35805;&#31574;&#30053;&#65292;&#21482;&#23398;&#20064;&#25552;&#20986;&#23548;&#33322;&#29992;&#25143;&#36798;&#21040;&#30446;&#26631;&#25152;&#38656;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25509;&#21475;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#21644;&#26041;&#20415;&#30340;&#26041;&#24335;&#65292;&#35753;&#29992;&#25143;&#33719;&#21462;&#21407;&#26412;&#21487;&#33021;&#38590;&#20197;&#25110;&#19981;&#26041;&#20415;&#33719;&#24471;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30028;&#38754;&#22823;&#20307;&#19978;&#21487;&#20197;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;FAQ&#65292;&#29992;&#25143;&#24517;&#39035;&#25552;&#20986;&#26126;&#30830;&#30340;&#38382;&#39064;&#20197;&#26816;&#32034;&#19968;&#33324;&#30340;&#31572;&#26696;&#65307;&#25110;&#32773;&#23545;&#35805;&#65292;&#29992;&#25143;&#24517;&#39035;&#36981;&#24490;&#39044;&#23450;&#20041;&#30340;&#36335;&#24452;&#20294;&#21487;&#33021;&#20250;&#25509;&#25910;&#21040;&#20010;&#24615;&#21270;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20250;&#35805;&#26641;&#25628;&#32034;(CTS)&#65292;&#23427;&#26550;&#36215;&#20102;&#20449;&#24687;&#26816;&#32034;&#39118;&#26684;FAQ&#21644;&#38754;&#21521;&#20219;&#21153;&#23545;&#35805;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#20801;&#35768;&#39046;&#22495;&#19987;&#23478;&#23450;&#20041;&#23545;&#35805;&#26641;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#25442;&#20026;&#19968;&#20010;&#26377;&#25928;&#30340;&#23545;&#35805;&#31574;&#30053;&#65292;&#21482;&#23398;&#20064;&#25552;&#20986;&#23548;&#33322;&#29992;&#25143;&#36798;&#21040;&#30446;&#26631;&#25152;&#38656;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26053;&#34892;&#25253;&#38144;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#39033;&#20219;&#21153;&#30340;&#22522;&#32447;&#65288;baseline&#65289;&#20197;&#21450;&#19968;&#39033;&#26032;&#39062;&#30340;&#28145;&#24230;&#22686;&#24378;&#23398;&#20064;&#26550;&#26500;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26032;&#30340;&#26550;&#26500;&#32508;&#21512;&#20102;FAQ&#21644;&#23545;&#35805;&#30340;&#20248;&#28857;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational interfaces provide a flexible and easy way for users to seek information that may otherwise be difficult or inconvenient to obtain. However, existing interfaces generally fall into one of two categories: FAQs, where users must have a concrete question in order to retrieve a general answer, or dialogs, where users must follow a predefined path but may receive a personalized answer. In this paper, we introduce Conversational Tree Search (CTS) as a new task that bridges the gap between FAQ-style information retrieval and task-oriented dialog, allowing domain-experts to define dialog trees which can then be converted to an efficient dialog policy that learns only to ask the questions necessary to navigate a user to their goal. We collect a dataset for the travel reimbursement domain and demonstrate a baseline as well as a novel deep Reinforcement Learning architecture for this task. Our results show that the new architecture combines the positive aspects of both the FAQ and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;ChatGPT&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;5&#20998;&#21046;&#30340;4.1&#20998;&#65292;&#20449;&#24687;&#32570;&#22833;&#29575;&#21644;&#20449;&#24687;&#38169;&#35823;&#29575;&#22343;&#36739;&#20302;&#65292;ChatGPT&#25552;&#20379;&#30340;&#24314;&#35758;&#22823;&#37117;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2303.09038</link><description>&lt;p&gt;
&#21033;&#29992;ChatGPT&#21644;Prompt Learning&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65306;&#32467;&#26524;&#12289;&#38480;&#21046;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential. (arXiv:2303.09038v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;ChatGPT&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;5&#20998;&#21046;&#30340;4.1&#20998;&#65292;&#20449;&#24687;&#32570;&#22833;&#29575;&#21644;&#20449;&#24687;&#38169;&#35823;&#29575;&#22343;&#36739;&#20302;&#65292;ChatGPT&#25552;&#20379;&#30340;&#24314;&#35758;&#22823;&#37117;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20854;&#31867;&#20284;&#20154;&#31867;&#34920;&#36798;&#21644;&#25512;&#29702;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992;ChatGPT&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#20415;&#24739;&#32773;&#21644;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#32773;&#24471;&#21040;&#26356;&#22909;&#30340;&#21307;&#30103;&#25945;&#32946;&#12290;&#30740;&#31350;&#37319;&#38598;&#20102;62&#20221;&#20302;&#21058;&#37327;&#33016;&#37096;CT&#32954;&#30284;&#31579;&#26597;&#25195;&#25551;&#21644;76&#20221;&#33041;MRI&#36716;&#31227;&#24615;&#31579;&#26597;&#25195;&#25551;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#26681;&#25454;&#25918;&#23556;&#31185;&#21307;&#24072;&#30340;&#35780;&#20215;&#65292;ChatGPT&#21487;&#20197;&#25104;&#21151;&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;5&#20998;&#21046;&#30340;4.1&#20998;&#65292;&#20449;&#24687;&#32570;&#22833;0.07&#22788;&#65292;&#20449;&#24687;&#38169;&#35823;0.11&#22788;&#12290;&#23601;ChatGPT&#25552;&#20379;&#30340;&#24314;&#35758;&#32780;&#35328;&#65292;&#23427;&#20204;&#26159;&#19968;&#33324;&#24615;&#30340;&#30456;&#20851;&#24314;&#35758;&#65292;&#20363;&#22914;&#20445;&#25345;&#19982;&#21307;&#29983;&#30340;&#38543;&#35775;&#21644;&#23494;&#20999;&#30417;&#27979;&#20219;&#20309;&#30151;&#29366;&#65292;&#23545;&#20110;&#20849;138&#20010;&#30149;&#20363;&#20013;&#30340;&#32422;37&#65285;&#65292;ChatGPT&#25552;&#20379;&#20102;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#26377;&#20851;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large language model called ChatGPT has drawn extensively attention because of its human-like expression and reasoning abilities. In this study, we investigate the feasibility of using ChatGPT in experiments on using ChatGPT to translate radiology reports into plain language for patients and healthcare providers so that they are educated for improved healthcare. Radiology reports from 62 low-dose chest CT lung cancer screening scans and 76 brain MRI metastases screening scans were collected in the first half of February for this study. According to the evaluation by radiologists, ChatGPT can successfully translate radiology reports into plain language with an average score of 4.1 in the five-point system with 0.07 places of information missing and 0.11 places of misinformation. In terms of the suggestions provided by ChatGPT, they are general relevant such as keeping following-up with doctors and closely monitoring any symptoms, and for about 37% of 138 cases in total ChatGPT offer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25216;&#26415;&#24182;&#25552;&#20986;&#20102;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#24320;&#21457;&#20840;&#38754;&#35780;&#20272;&#25351;&#26631;&#21644;&#24320;&#28304; LLM &#25152;&#26500;&#25104;&#30340;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2303.07205</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#23383;&#30340;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
The Science of Detecting LLM-Generated Texts. (arXiv:2303.07205v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25216;&#26415;&#24182;&#25552;&#20986;&#20102;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#24320;&#21457;&#20840;&#38754;&#35780;&#20272;&#25351;&#26631;&#21644;&#24320;&#28304; LLM &#25152;&#26500;&#25104;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#20986;&#29616;&#23548;&#33268;&#20102;&#39640;&#24230;&#22797;&#26434;&#19988;&#20960;&#20046;&#38590;&#20197;&#21306;&#20998;&#20986;&#26159;&#21542;&#20026;&#20154;&#31867;&#21019;&#20316;&#30340; LLM &#29983;&#25104;&#25991;&#23383;&#12290;&#20294;&#26159;&#65292;&#36825;&#20063;&#24341;&#21457;&#20102;&#23545;&#27492;&#31867;&#25991;&#23383;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#65292;&#20363;&#22914;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#21644;&#22312;&#25945;&#32946;&#31995;&#32479;&#20013;&#36896;&#25104;&#28151;&#20081;&#12290;&#23613;&#31649;&#24050;&#25552;&#20986;&#35768;&#22810;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#20851;&#20110;&#20854;&#25104;&#23601;&#21644;&#25361;&#25112;&#30340;&#20840;&#38754;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#12290;&#26412;&#25991;&#26088;&#22312;&#27010;&#36848;&#29616;&#26377;&#30340; LLM &#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25216;&#26415;&#65292;&#24182;&#22686;&#24378;&#23545;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#25511;&#21046;&#21644;&#30417;&#31649;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#65292;&#21253;&#25324;&#24320;&#21457;&#20840;&#38754;&#35780;&#20272;&#25351;&#26631;&#21644;&#24320;&#28304; LLM &#25152;&#26500;&#25104;&#30340;&#23041;&#32961;&#65292;&#20197;&#25512;&#21160; LLM &#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large language models (LLMs) has resulted in the production of LLM-generated texts that is highly sophisticated and almost indistinguishable from texts written by humans. However, this has also sparked concerns about the potential misuse of such texts, such as spreading misinformation and causing disruptions in the education system. Although many detection approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. This survey aims to provide an overview of existing LLM-generated text detection techniques and enhance the control and regulation of language generation models. Furthermore, we emphasize crucial considerations for future research, including the development of comprehensive evaluation metrics and the threat posed by open-source LLMs, to drive progress in the area of LLM-generated text detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#24335;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#20351;&#29992;&#29992;&#25143;&#27169;&#25311;&#22120;&#36827;&#34892;&#20132;&#20114;&#24335;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#30495;&#23454;&#29992;&#25143;&#21442;&#19982;&#30340;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#32534;&#36753;&#25351;&#23548;&#27169;&#22411;&#26397;&#30528;&#32473;&#23450;&#30446;&#26631;&#21069;&#36827;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.00908</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Interactive Text Generation. (arXiv:2303.00908v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#24335;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#20351;&#29992;&#29992;&#25143;&#27169;&#25311;&#22120;&#36827;&#34892;&#20132;&#20114;&#24335;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#30495;&#23454;&#29992;&#25143;&#21442;&#19982;&#30340;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#32534;&#36753;&#25351;&#23548;&#27169;&#22411;&#26397;&#30528;&#32473;&#23450;&#30446;&#26631;&#21069;&#36827;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#27599;&#22825;&#37117;&#35201;&#19982;&#25991;&#26412;&#12289;&#22270;&#29255;&#12289;&#20195;&#30721;&#25110;&#20854;&#20182;&#32534;&#36753;&#22120;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24456;&#23569;&#22312;&#21453;&#26144;&#29992;&#25143;&#19982;&#32534;&#36753;&#22120;&#20043;&#38388;&#20114;&#21160;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#26159;&#21487;&#20197;&#29702;&#35299;&#30340;&#65292;&#22240;&#20026;&#20351;&#29992;&#30495;&#23454;&#29992;&#25143;&#36827;&#34892;AI&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#20165;&#36895;&#24230;&#24930;&#19988;&#25104;&#26412;&#39640;&#65292;&#32780;&#19988;&#36825;&#20123;&#27169;&#22411;&#25152;&#23398;&#20064;&#30340;&#20869;&#23481;&#21487;&#33021;&#29305;&#23450;&#20110;&#29992;&#25143;&#30028;&#38754;&#35774;&#35745;&#36873;&#25321;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#24847;&#21619;&#30528;&#22823;&#22810;&#25968;&#25991;&#26412;&#12289;&#20195;&#30721;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#38750;&#20132;&#20114;&#35774;&#32622;&#19978;&#65292;&#21363;&#27169;&#22411;&#34987;&#26399;&#26395;&#22312;&#27809;&#26377;&#32771;&#34385;&#20219;&#20309;&#26469;&#33258;&#24895;&#24847;&#24110;&#21161;&#30340;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#25152;&#26377;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#20132;&#20114;&#24335;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#20801;&#35768;&#20351;&#29992;&#29992;&#25143;&#27169;&#25311;&#22120;&#36827;&#34892;&#20132;&#20114;&#24335;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#26080;&#38656;&#28041;&#21450;&#30495;&#23454;&#29992;&#25143;&#30340;&#25104;&#26412;&#65292;&#27169;&#25311;&#22120;&#36890;&#36807;&#25552;&#20379;&#32534;&#36753;&#25351;&#23548;&#27169;&#22411;&#26397;&#30528;&#32473;&#23450;&#30340;&#30446;&#26631;&#25991;&#26412;&#21069;&#36827;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#35757;&#32451;&#25105;&#20204;&#30340;&#20132;&#20114;&#24335;&#27169;&#22411;&#65292;&#24182;&#19982;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#38750;&#20132;&#20114;&#24335;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Users interact with text, image, code, or other editors on a daily basis. However, machine learning models are rarely trained in the settings that reflect the interactivity between users and their editor. This is understandable as training AI models with real users is not only slow and costly, but what these models learn may be specific to user interface design choices. Unfortunately, this means most of the research on text, code, and image generation has focused on non-interactive settings, whereby the model is expected to get everything right without accounting for any input from a user who may be willing to help.  We introduce a new Interactive Text Generation task that allows training generation models interactively without the costs of involving real users, by using user simulators that provide edits that guide the model towards a given target text. We train our interactive models using Imitation Learning, and our experiments against competitive non-interactive generation models s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23398;&#26657;&#35821;&#26009;&#24211;&#33258;&#21160;&#30830;&#23450;&#25512;&#33616;&#32473;&#23398;&#29983;&#30340;&#25945;&#26448;&#19982;&#23398;&#29983;&#30340;&#25945;&#32946;&#26448;&#26009;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#20026;&#25945;&#32946;&#31995;&#32479;&#36873;&#25321;&#36866;&#24403;&#30340;&#20869;&#23481;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2303.00465</link><description>&lt;p&gt;
&#20044;&#20857;&#21035;&#20811;&#25991;&#26412;&#19982;&#23398;&#29983;&#25945;&#32946;&#28508;&#21147;&#30340;&#23545;&#24212;&#20851;&#31995;&#65306;&#23398;&#26657;&#35821;&#26009;&#24211;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Uzbek text's correspondence with the educational potential of pupils: a case study of the School corpus. (arXiv:2303.00465v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23398;&#26657;&#35821;&#26009;&#24211;&#33258;&#21160;&#30830;&#23450;&#25512;&#33616;&#32473;&#23398;&#29983;&#30340;&#25945;&#26448;&#19982;&#23398;&#29983;&#30340;&#25945;&#32946;&#26448;&#26009;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#20026;&#25945;&#32946;&#31995;&#32479;&#36873;&#25321;&#36866;&#24403;&#30340;&#20869;&#23481;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#31995;&#32479;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#36873;&#25321;&#36866;&#21512;&#23398;&#29983;&#24180;&#40836;&#21644;&#24605;&#32500;&#28508;&#21147;&#30340;&#20869;&#23481;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23567;&#23398;&#19968;&#24180;&#32423;&#21040;&#22235;&#24180;&#32423;&#30340;&#23454;&#39564;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#23398;&#26657;&#35821;&#26009;&#24211;&#33258;&#21160;&#30830;&#23450;&#25512;&#33616;&#32473;&#23398;&#29983;&#30340;&#25945;&#26448;&#19982;&#23398;&#29983;&#30340;&#25945;&#32946;&#26448;&#26009;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;&#20044;&#20857;&#21035;&#20811;&#26031;&#22374;&#24188;&#20799;&#22253;&#21644;&#23398;&#26657;&#25945;&#32946;&#37096;&#30830;&#35748;&#30340;25&#26412;&#23398;&#26657;&#25945;&#31185;&#20070;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#25991;&#26412;&#30340;TF-IDF&#20998;&#25968;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#21521;&#37327;&#34920;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;&#20313;&#24358;&#30456;&#20284;&#24615;&#31639;&#27861;&#27604;&#36739;&#25152;&#32473;&#30340;&#25945;&#32946;&#26448;&#26009;&#19982;&#23398;&#26657;&#35821;&#26009;&#24211;&#30456;&#24212;&#29677;&#32423;&#30340;&#20869;&#23481;&#12290;&#26681;&#25454;&#35745;&#31639;&#32467;&#26524;&#65292;&#30830;&#23450;&#25152;&#32473;&#25945;&#32946;&#26448;&#26009;&#26159;&#21542;&#36866;&#21512;&#23398;&#29983;&#30340;&#25945;&#32946;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the major challenges of an educational system is choosing appropriate content considering pupils' age and intellectual potential. In this article the experiment of primary school grades (from 1st to 4th grades) is considered for automatically determining the correspondence of an educational materials recommended for pupils by using the School corpus where it includes the dataset of 25 school textbooks confirmed by the Ministry of preschool and school education of the Republic of Uzbekistan. In this case, TF-IDF scores of the texts are determined, they are converted into a vector representation, and the given educational materials are compared with the corresponding class of the School corpus using the cosine similarity algorithm. Based on the results of the calculation, it is determined whether the given educational material is appropriate or not appropriate for the pupils' educational potential.
&lt;/p&gt;</description></item><item><title>AugGPT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24544;&#23454;&#22320;&#20445;&#30041;&#27491;&#30830;&#26631;&#35760;&#30340;&#29983;&#25104;&#25968;&#25454;&#24182;&#25552;&#20379;&#36275;&#22815;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#26679;&#26412;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.13007</link><description>&lt;p&gt;
AugGPT&#65306;&#21033;&#29992;ChatGPT&#36827;&#34892;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
AugGPT: Leveraging ChatGPT for Text Data Augmentation. (arXiv:2302.13007v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13007
&lt;/p&gt;
&lt;p&gt;
AugGPT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24544;&#23454;&#22320;&#20445;&#30041;&#27491;&#30830;&#26631;&#35760;&#30340;&#29983;&#25104;&#25968;&#25454;&#24182;&#25552;&#20379;&#36275;&#22815;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#26679;&#26412;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26159;&#21463;&#38480;&#21046;&#30340;&#26679;&#26412;&#37327;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#20811;&#26381;&#25361;&#25112;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#30446;&#26631;&#22495;&#30340;&#25968;&#25454;&#36890;&#24120;&#26356;&#21152;&#31232;&#32570;&#19988;&#36136;&#37327;&#26356;&#20302;&#65292;&#36825;&#19968;&#25361;&#25112;&#29305;&#21035;&#31361;&#20986;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#25968;&#25454;&#19981;&#21464;&#24615;&#24182;&#22686;&#21152;&#26679;&#26412;&#37327;&#65292;&#32531;&#35299;&#36825;&#31181;&#25361;&#25112;&#30340;&#19968;&#31181;&#33258;&#28982;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#31574;&#30053;&#26159;&#25191;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#35201;&#20040;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#25968;&#25454;&#30340;&#27491;&#30830;&#26631;&#35760;&#65288;&#32570;&#20047;&#24544;&#23454;&#24230;&#65289;&#65292;&#35201;&#20040;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#25968;&#25454;&#20855;&#26377;&#36275;&#22815;&#30340;&#22810;&#26679;&#24615;&#65288;&#32570;&#20047;&#32039;&#20945;&#24615;&#65289;&#65292;&#25110;&#32773;&#20004;&#32773;&#37117;&#26377;&#12290;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#36817;&#25104;&#21151;&#65292;&#23588;&#20854;&#26159;&#22312;&#24320;&#21457;ChatGPT&#26041;&#38754;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#65288;&#21517;&#20026;AugGPT&#65289;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text data augmentation is an effective strategy for overcoming the challenge of limited sample sizes in many natural language processing (NLP) tasks. This challenge is especially prominent in the few-shot learning scenario, where the data in the target domain is generally much scarcer and of lowered quality. A natural and widely-used strategy to mitigate such challenges is to perform data augmentation to better capture the data invariance and increase the sample size. However, current text data augmentation methods either can't ensure the correct labeling of the generated data (lacking faithfulness) or can't ensure sufficient diversity in the generated data (lacking compactness), or both. Inspired by the recent success of large language models, especially the development of ChatGPT, which demonstrated improved language comprehension abilities, in this work, we propose a text data augmentation approach based on ChatGPT (named AugGPT). AugGPT rephrases each sentence in the training sampl
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36923;&#36753;&#24341;&#23548;&#30340;&#20174;&#22024;&#26434;&#30340;&#20247;&#21253;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#25913;&#36827;&#25991;&#26412;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#65292;&#23398;&#20064;&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#25928;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26694;&#26550;&#33021;&#22815;&#34701;&#21512;&#36923;&#36753;&#30693;&#35782;&#65292;&#25552;&#39640;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2302.06337</link><description>&lt;p&gt;
&#21033;&#29992;&#36923;&#36753;&#23398;&#20064;&#20174;&#22024;&#26434;&#30340;&#20247;&#21253;&#26631;&#31614;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Noisy Crowd Labels with Logics. (arXiv:2302.06337v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06337
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36923;&#36753;&#24341;&#23548;&#30340;&#20174;&#22024;&#26434;&#30340;&#20247;&#21253;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#25913;&#36827;&#25991;&#26412;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#65292;&#23398;&#20064;&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#25928;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26694;&#26550;&#33021;&#22815;&#34701;&#21512;&#36923;&#36753;&#30693;&#35782;&#65292;&#25552;&#39640;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#31526;&#21495;&#36923;&#36753;&#30693;&#35782;&#38598;&#25104;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20174;&#22024;&#26434;&#30340;&#20247;&#21253;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36923;&#36753;&#24341;&#23548;&#30340;&#20174;&#22024;&#26434;&#30340;&#20247;&#21253;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#26694;&#26550;&#65288;Logic-LNCL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31867;&#20284;EM&#30340;&#36845;&#20195;&#36923;&#36753;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#22024;&#26434;&#30340;&#26631;&#35760;&#25968;&#25454;&#21644;&#24863;&#20852;&#36259;&#30340;&#36923;&#36753;&#35268;&#21017;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#19982;&#20256;&#32479;&#30340;EM&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#21547;&#19968;&#31181;&#8220;&#20266;E&#27493;&#39588;&#8221;&#65292;&#20174;&#36923;&#36753;&#35268;&#21017;&#20013;&#33976;&#39311;&#20986;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#28982;&#21518;&#22312;&#8220;&#20266;M&#27493;&#39588;&#8221;&#20013;&#20351;&#29992;&#35813;&#30446;&#26631;&#26469;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#22312;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#25216;&#26415;&#24182;&#20026;&#20174;&#22024;&#26434;&#30340;&#20247;&#21253;&#26631;&#31614;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the integration of symbolic logic knowledge into deep neural networks for learning from noisy crowd labels. We introduce Logic-guided Learning from Noisy Crowd Labels (Logic-LNCL), an EM-alike iterative logic knowledge distillation framework that learns from both noisy labeled data and logic rules of interest. Unlike traditional EM methods, our framework contains a ``pseudo-E-step'' that distills from the logic rules a new type of learning target, which is then used in the ``pseudo-M-step'' for training the classifier. Extensive evaluations on two real-world datasets for text sentiment classification and named entity recognition demonstrate that the proposed framework improves the state-of-the-art and provides a new solution to learning from noisy crowd labels.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36828;&#31243;&#30417;&#30563;&#30340;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#21477;&#23376;&#20013;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#20449;&#24687;&#26469;&#20943;&#23569;&#24178;&#25200;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#23454;&#20307;&#20851;&#31995;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02078</link><description>&lt;p&gt;
&#22522;&#20110;&#32454;&#31890;&#24230;&#35821;&#20041;&#20449;&#24687;&#30340;&#20851;&#31995;&#25277;&#21462;FGSI&#65306;&#19968;&#31181;&#22522;&#20110;&#36828;&#31243;&#30417;&#30563;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FGSI: Distant Supervision for Relation Extraction method based on Fine-Grained Semantic Information. (arXiv:2302.02078v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36828;&#31243;&#30417;&#30563;&#30340;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#21477;&#23376;&#20013;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#20449;&#24687;&#26469;&#20943;&#23569;&#24178;&#25200;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#23454;&#20307;&#20851;&#31995;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#25552;&#21462;&#21477;&#23376;&#20013;&#26631;&#35760;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23427;&#22312;&#21477;&#23376;&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#22312;&#21477;&#23376;&#20869;&#37096;&#30340;&#20851;&#38190;&#35821;&#20041;&#20449;&#24687;&#23545;&#23454;&#20307;&#20851;&#31995;&#30340;&#25552;&#21462;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#22522;&#20110;&#36825;&#20010;&#20551;&#35774;&#65292;&#25353;&#29031;&#23454;&#20307;&#22312;&#21477;&#23376;&#20869;&#37096;&#30340;&#20301;&#32622;&#23558;&#21477;&#23376;&#20998;&#20026;&#19977;&#27573;&#65292;&#24182;&#36890;&#36807;&#21477;&#20869;&#27880;&#24847;&#26426;&#21046;&#25214;&#21040;&#21477;&#23376;&#20869;&#37096;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#29305;&#24449;&#65292;&#20197;&#20943;&#23569;&#26080;&#20851;&#22122;&#22768;&#20449;&#24687;&#30340;&#24178;&#25200;&#12290;&#25152;&#25552;&#20986;&#30340;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#21487;&#29992;&#30340;&#27491;&#38754;&#35821;&#20041;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main purpose of relation extraction is to extract the semantic relationships between tagged pairs of entities in a sentence, which plays an important role in the semantic understanding of sentences and the construction of knowledge graphs. In this paper, we propose that the key semantic information within a sentence plays a key role in the relationship extraction of entities. We propose the hypothesis that the key semantic information inside the sentence plays a key role in entity relationship extraction. And based on this hypothesis, we split the sentence into three segments according to the location of the entity from the inside of the sentence, and find the fine-grained semantic features inside the sentence through the intra-sentence attention mechanism to reduce the interference of irrelevant noise information. The proposed relational extraction model can make full use of the available positive semantic information. The experimental results show that the proposed relation extra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;ChatGPT&#30340;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#22312;&#39640;&#36164;&#28304;&#27431;&#27954;&#35821;&#35328;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#25110;&#36828;&#31243;&#35821;&#35328;&#19978;&#34920;&#29616;&#28382;&#21518;&#65307;&#37319;&#29992;&#26530;&#36724;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36828;&#31243;&#35821;&#35328;&#32763;&#35793;&#30340;&#24615;&#33021;&#65307;&#22312;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#25110;Reddit&#35780;&#35770;&#26041;&#38754;&#65292;ChatGPT&#30340;&#34920;&#29616;&#19981;&#22914;&#21830;&#19994;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2301.08745</link><description>&lt;p&gt;
ChatGPT&#20316;&#20026;&#32763;&#35793;&#24341;&#25806;&#65292;&#20381;&#36182;&#20110;GPT-4&#65292;&#26159;&#19968;&#31181;&#22909;&#30340;&#32763;&#35793;&#22120;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine. (arXiv:2301.08745v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;ChatGPT&#30340;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#22312;&#39640;&#36164;&#28304;&#27431;&#27954;&#35821;&#35328;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#25110;&#36828;&#31243;&#35821;&#35328;&#19978;&#34920;&#29616;&#28382;&#21518;&#65307;&#37319;&#29992;&#26530;&#36724;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36828;&#31243;&#35821;&#35328;&#32763;&#35793;&#30340;&#24615;&#33021;&#65307;&#22312;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#25110;Reddit&#35780;&#35770;&#26041;&#38754;&#65292;ChatGPT&#30340;&#34920;&#29616;&#19981;&#22914;&#21830;&#19994;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#23545;ChatGPT&#36827;&#34892;&#20102;&#26426;&#22120;&#32763;&#35793;&#30340;&#21021;&#27493;&#35780;&#20272;&#65292;&#21253;&#25324;&#32763;&#35793;&#25552;&#31034;&#12289;&#22810;&#35821;&#35328;&#32763;&#35793;&#21644;&#32763;&#35793;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;ChatGPT&#24314;&#35758;&#30340;&#25552;&#31034;&#26469;&#35302;&#21457;&#20854;&#32763;&#35793;&#33021;&#21147;&#65292;&#21457;&#29616;&#20505;&#36873;&#25552;&#31034;&#36890;&#24120;&#36816;&#34892;&#33391;&#22909;&#65292;&#34920;&#29616;&#20986;&#36731;&#24494;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#22312;&#39640;&#36164;&#28304;&#30340;&#27431;&#27954;&#35821;&#35328;&#19978;&#65292;ChatGPT&#30340;&#34920;&#29616;&#19982;&#21830;&#19994;&#32763;&#35793;&#20135;&#21697;&#65288;&#20363;&#22914;Google&#32763;&#35793;&#65289;&#30456;&#24403;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#25110;&#36828;&#31243;&#35821;&#35328;&#19978;&#34920;&#29616;&#26174;&#33879;&#28382;&#21518;&#12290;&#23545;&#20110;&#36828;&#31243;&#35821;&#35328;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26377;&#36259;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;$\mathbf{&#26530;&#36724;&#25552;&#31034;}$&#65292;&#21363;&#35753;ChatGPT&#20808;&#23558;&#28304;&#35821;&#35328;&#21477;&#23376;&#32763;&#35793;&#25104;&#39640;&#36164;&#28304;&#30340;&#36724;&#35821;&#35328;&#65292;&#20877;&#32763;&#35793;&#30446;&#26631;&#35821;&#35328;&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#32763;&#35793;&#24615;&#33021;&#12290;&#33267;&#20110;&#32763;&#35793;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#25110;Reddit&#35780;&#35770;&#26041;&#38754;&#65292;ChatGPT&#30340;&#34920;&#29616;&#19981;&#22914;&#21830;&#19994;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report provides a preliminary evaluation of ChatGPT for machine translation, including translation prompt, multilingual translation, and translation robustness. We adopt the prompts advised by ChatGPT to trigger its translation ability and find that the candidate prompts generally work well and show minor performance differences. By evaluating on a number of benchmark test sets, we find that ChatGPT performs competitively with commercial translation products (e.g., Google Translate) on high-resource European languages but lags behind significantly on low-resource or distant languages. For distant languages, we explore an interesting strategy named $\mathbf{pivot~prompting}$ that asks ChatGPT to translate the source sentence into a high-resource pivot language before into the target language, which improves the translation performance significantly. As for the translation robustness, ChatGPT does not perform as well as the commercial systems on biomedical abstracts or Reddit commen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35823;&#24046;&#24341;&#23548;&#30340;&#27169;&#22411;&#29992;&#20110;&#25913;&#36827;&#27721;&#35821;&#25340;&#20889;&#32416;&#38169;&#65292;&#23427;&#37319;&#29992;&#20102;&#38646;-shot&#35823;&#24046;&#26816;&#27979;&#12289;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#39640;&#24230;&#24182;&#34892;&#30340;&#35299;&#30721;&#31561;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.06323</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#35823;&#24046;&#24341;&#23548;&#30340;&#27169;&#22411;&#29992;&#20110;&#27721;&#35821;&#25340;&#20889;&#32416;&#38169;
&lt;/p&gt;
&lt;p&gt;
An Error-Guided Correction Model for Chinese Spelling Error Correction. (arXiv:2301.06323v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35823;&#24046;&#24341;&#23548;&#30340;&#27169;&#22411;&#29992;&#20110;&#25913;&#36827;&#27721;&#35821;&#25340;&#20889;&#32416;&#38169;&#65292;&#23427;&#37319;&#29992;&#20102;&#38646;-shot&#35823;&#24046;&#26816;&#27979;&#12289;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#39640;&#24230;&#24182;&#34892;&#30340;&#35299;&#30721;&#31561;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#27721;&#35821;&#25340;&#20889;&#32416;&#38169;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#20173;&#26377;&#36827;&#19968;&#27493;&#25552;&#39640;&#30340;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35823;&#24046;&#24341;&#23548;&#30340;&#32416;&#38169;&#27169;&#22411;&#65288;EGCM&#65289;&#26469;&#25913;&#36827;&#27721;&#35821;&#25340;&#20889;&#32416;&#38169;&#12290;&#36890;&#36807;&#20511;&#37492;BERT&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;-shot&#35823;&#24046;&#26816;&#27979;&#26041;&#27861;&#26469;&#36827;&#34892;&#21021;&#27493;&#26816;&#27979;&#65292;&#36825;&#26679;&#21487;&#20197;&#25351;&#23548;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#32534;&#30721;&#26102;&#26356;&#22810;&#22320;&#20851;&#27880;&#21487;&#33021;&#38169;&#35823;&#30340;&#26631;&#35760;&#65292;&#24182;&#22312;&#29983;&#25104;&#26102;&#36991;&#20813;&#20462;&#25913;&#27491;&#30830;&#30340;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#38598;&#25104;&#35823;&#24046;&#28151;&#28102;&#38598;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#36731;&#26494;&#21306;&#20998;&#26131;&#28151;&#28102;&#30340;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25903;&#25345;&#39640;&#24230;&#24182;&#34892;&#30340;&#35299;&#30721;&#65292;&#20197;&#28385;&#36275;&#23454;&#38469;&#24212;&#29992;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although existing neural network approaches have achieved great success on Chinese spelling correction, there is still room to improve. The model is required to avoid over-correction and to distinguish a correct token from its phonological and visually similar ones. In this paper, we propose an error-guided correction model (EGCM) to improve Chinese spelling correction. By borrowing the powerful ability of BERT, we propose a novel zero-shot error detection method to do a preliminary detection, which guides our model to attend more on the probably wrong tokens in encoding and to avoid modifying the correct tokens in generating. Furthermore, we introduce a new loss function to integrate the error confusion set, which enables our model to distinguish easily misused tokens. Moreover, our model supports highly parallel decoding to meet real application requirements. Experiments are conducted on widely used benchmarks. Our model achieves superior performance against state-of-the-art approach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10,917&#31687;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#25353;&#20027;&#39064;&#23545;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#65292;&#23545;&#26032;&#38395;&#32467;&#26500;&#12289;&#20998;&#31867;&#21644;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2212.12061</link><description>&lt;p&gt;
MN-DS&#65306;&#26032;&#38395;&#25991;&#31456;&#23618;&#27425;&#20998;&#31867;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MN-DS: A Multilabeled News Dataset for News Articles Hierarchical Classification. (arXiv:2212.12061v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10,917&#31687;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#25353;&#20027;&#39064;&#23545;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#65292;&#23545;&#26032;&#38395;&#32467;&#26500;&#12289;&#20998;&#31867;&#21644;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;10,917&#31687;&#26032;&#38395;&#25991;&#31456;&#65292;&#28085;&#30422;&#20102;&#20174;2019&#24180;1&#26376;1&#26085;&#21040;2019&#24180;12&#26376;31&#26085;&#30340;&#23618;&#27425;&#26032;&#38395;&#20998;&#31867;&#12290;&#25105;&#20204;&#26681;&#25454;17&#20010;&#19968;&#32423;&#31867;&#21035;&#21644;109&#20010;&#20108;&#32423;&#31867;&#21035;&#30340;&#23618;&#27425;&#20998;&#31867;&#25163;&#21160;&#26631;&#35760;&#20102;&#36825;&#20123;&#25991;&#31456;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#33258;&#21160;&#25353;&#20027;&#39064;&#20998;&#31867;&#26032;&#38395;&#25991;&#31456;&#12290;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#20174;&#20107;&#26032;&#38395;&#32467;&#26500;&#12289;&#20998;&#31867;&#21644;&#26681;&#25454;&#21457;&#24067;&#30340;&#26032;&#38395;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a dataset of 10,917 news articles with hierarchical news categories collected between January 1st 2019, and December 31st 2019. We manually labelled the articles based on a hierarchical taxonomy with 17 first-level and 109 second-level categories. This dataset can be used to train machine learning models for automatically classifying news articles by topic. This dataset can be helpful for researchers working on news structuring, classification, and predicting future events based on released news.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLM-Planner&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#23454;&#20307;&#20195;&#29702;&#36827;&#34892;&#23569;&#26679;&#26412;&#35268;&#21010;&#65292;&#20197;&#23454;&#20307;&#20195;&#29702;&#30446;&#21069;&#25152;&#22312;&#30340;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#22686;&#24378;LLMs&#29983;&#25104;&#21644;&#26356;&#26032;&#35745;&#21010;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#22810;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#24320;&#21457;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.04088</link><description>&lt;p&gt;
LLM-Planner: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#23454;&#20307;&#20195;&#29702;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models. (arXiv:2212.04088v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLM-Planner&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#23454;&#20307;&#20195;&#29702;&#36827;&#34892;&#23569;&#26679;&#26412;&#35268;&#21010;&#65292;&#20197;&#23454;&#20307;&#20195;&#29702;&#30446;&#21069;&#25152;&#22312;&#30340;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#22686;&#24378;LLMs&#29983;&#25104;&#21644;&#26356;&#26032;&#35745;&#21010;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#22810;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#24320;&#21457;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35268;&#21010;&#22120;&#65292;&#35753;&#23454;&#20307;&#20195;&#29702;&#21487;&#20197;&#25353;&#29031;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#22312;&#35270;&#35273;&#24863;&#30693;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#30340;&#39640;&#25968;&#25454;&#25104;&#26412;&#21644;&#20302;&#26679;&#26412;&#25928;&#29575;&#38459;&#30861;&#20102;&#22810;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#24320;&#21457;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLM-Planner&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#23454;&#20307;&#20195;&#29702;&#36827;&#34892;&#23569;&#26679;&#26412;&#35268;&#21010;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#20307;&#20195;&#29702;&#30446;&#21069;&#25152;&#22312;&#30340;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#22686;&#24378;LLMs&#29983;&#25104;&#21644;&#26356;&#26032;&#35745;&#21010;&#12290;&#22312;ALFRED&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#65306;&#23613;&#31649;&#20351;&#29992;&#30340;&#37197;&#23545;&#35757;&#32451;&#25968;&#25454;&#19981;&#21040;0.5&#65285;&#65292;LLM-Planner&#30340;&#34920;&#29616;&#19982;&#20351;&#29992;&#23436;&#25972;&#35757;&#32451;&#25968;&#25454;&#35757;&#32451;&#30340;&#26368;&#26032;&#22522;&#32447;&#30456;&#24403;&#12290;&#29616;&#26377;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#23436;&#25104;&#20219;&#20309;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Attention on Attention&#26426;&#21046;&#26469;&#25552;&#39640;&#22522;&#20110;Transformer&#26041;&#27861;&#30340;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;VieCap4H&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#21407;&#22987;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2211.05405</link><description>&lt;p&gt;
VieCap4H-VLSP 2021&#65306;&#20351;&#29992;Attention on Attention&#22686;&#24378;&#29289;&#20307;&#20851;&#31995;Transformer&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#36234;&#21335;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#33021;&#21147;.
&lt;/p&gt;
&lt;p&gt;
VieCap4H-VLSP 2021: ObjectAoA-Enhancing performance of Object Relation Transformer with Attention on Attention for Vietnamese image captioning. (arXiv:2211.05405v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Attention on Attention&#26426;&#21046;&#26469;&#25552;&#39640;&#22522;&#20110;Transformer&#26041;&#27861;&#30340;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;VieCap4H&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#21407;&#22987;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26159;&#19968;&#39033;&#38656;&#35201;&#29702;&#35299;&#35270;&#35273;&#20449;&#24687;&#24182;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#22270;&#20687;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#29289;&#20307;&#20851;&#31995;Transformer&#32467;&#26500;&#30340;Attention on Attention&#26426;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22522;&#20110;Transformer&#26041;&#27861;&#30340;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#12290;&#22312;VieCap4H&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;VLSP&#20030;&#21150;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#20844;&#20849;&#27979;&#35797;&#21644;&#31169;&#20154;&#27979;&#35797;&#20013;&#37117;&#26174;&#33879;&#20248;&#20110;&#20854;&#21407;&#22987;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image captioning is currently a challenging task that requires the ability to both understand visual information and use human language to describe this visual information in the image. In this paper, we propose an efficient way to improve the image understanding ability of transformer-based method by extending Object Relation Transformer architecture with Attention on Attention mechanism. Experiments on the VieCap4H dataset show that our proposed method significantly outperforms its original structure on both the public test and private test of the Image Captioning shared task held by VLSP.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#22312;&#24494;&#35843;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26102;&#38450;&#27490;&#20010;&#20154;&#20449;&#24687;&#27844;&#38706;&#12290;</title><link>http://arxiv.org/abs/2210.15042</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#19979;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#31169;&#26377;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Privately Fine-Tuning Large Language Models with Differential Privacy. (arXiv:2210.15042v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15042
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#22312;&#24494;&#35843;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26102;&#38450;&#27490;&#20010;&#20154;&#20449;&#24687;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24050;&#32463;&#22312;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#31361;&#30772;&#24615;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#35777;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#20174;&#36825;&#20123;LLM&#20013;&#25552;&#21462;/&#37325;&#24314;&#20986;&#30830;&#20999;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#27844;&#38706;&#20010;&#20154;&#35782;&#21035;&#20449;&#24687;&#12290;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#35880;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#22312;&#35757;&#32451;&#25110;&#24494;&#35843;LLM&#36807;&#31243;&#20013;&#28155;&#21152;&#22122;&#22768;&#65292;&#20351;&#25552;&#21462;&#35757;&#32451;&#25968;&#25454;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Large Language Models (LLMs) are an integral part of modern AI that have led to breakthrough performances in complex AI tasks. Major AI companies with expensive infrastructures are able to develop and train these large models with billions and millions of parameters from scratch. Third parties, researchers, and practitioners are increasingly adopting these pre-trained models and fine-tuning them on their private data to accomplish their downstream AI tasks. However, it has been shown that an adversary can extract/reconstruct the exact training samples from these LLMs, which can lead to revealing personally identifiable information. The issue has raised deep concerns about the privacy of LLMs. Differential privacy (DP) provides a rigorous framework that allows adding noise in the process of training or fine-tuning LLMs such that extracting the training data becomes infeasible (i.e., with a cryptographically small success probability). While the theoretical privacy guarantees
&lt;/p&gt;</description></item><item><title>PACIFIC&#26159;&#19968;&#20010;&#37329;&#34701;&#39046;&#22495;&#30340;&#20027;&#21160;&#23545;&#35805;&#38382;&#31572;&#31995;&#32479;&#65292;&#29305;&#28857;&#26159;&#25968;&#23383;&#25512;&#29702;&#21644;&#28151;&#21512;&#35821;&#22659;&#65292;&#25552;&#20986;&#20102;PCQA&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;UniPCQA&#26469;&#36866;&#24212;&#36825;&#20010;&#20219;&#21153;&#12290;&#22312;PACIFIC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2210.08817</link><description>&lt;p&gt;
PACIFIC&#65306;&#38754;&#21521;&#37329;&#34701;&#39046;&#22495;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#20027;&#21160;&#23545;&#35805;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
PACIFIC: Towards Proactive Conversational Question Answering over Tabular and Textual Data in Finance. (arXiv:2210.08817v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08817
&lt;/p&gt;
&lt;p&gt;
PACIFIC&#26159;&#19968;&#20010;&#37329;&#34701;&#39046;&#22495;&#30340;&#20027;&#21160;&#23545;&#35805;&#38382;&#31572;&#31995;&#32479;&#65292;&#29305;&#28857;&#26159;&#25968;&#23383;&#25512;&#29702;&#21644;&#28151;&#21512;&#35821;&#22659;&#65292;&#25552;&#20986;&#20102;PCQA&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;UniPCQA&#26469;&#36866;&#24212;&#36825;&#20010;&#20219;&#21153;&#12290;&#22312;PACIFIC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20419;&#36827;&#37329;&#34701;&#39046;&#22495;&#28151;&#21512;&#35821;&#22659;&#19979;&#30340;&#23545;&#35805;&#38382;&#31572;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PACIFIC&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#19982;&#29616;&#26377;&#30340;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;PACIFIC&#20855;&#26377;&#19977;&#20010;&#20851;&#38190;&#29305;&#24449;&#65306;&#20027;&#21160;&#24615;&#12289;&#25968;&#23383;&#25512;&#29702;&#21644;&#34920;&#26684;&#21644;&#25991;&#26412;&#30340;&#28151;&#21512;&#35821;&#22659;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#30740;&#31350;&#20027;&#21160;&#23545;&#35805;&#38382;&#31572;&#31995;&#32479;(PCQA)&#65292;&#23427;&#32467;&#21512;&#20102;&#28548;&#28165;&#38382;&#39064;&#29983;&#25104;&#21644;&#23545;&#35805;&#38382;&#31572;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;UniPCQA&#65292;&#23558;PCQA&#20013;&#36755;&#20837;&#21644;&#36755;&#20986;&#20869;&#23481;&#30340;&#28151;&#21512;&#26684;&#24335;&#36866;&#24212;&#20110;Seq2Seq&#38382;&#39064;&#65292;&#21253;&#25324;&#23558;&#25968;&#23383;&#25512;&#29702;&#36807;&#31243;&#37325;&#26032;&#23450;&#20041;&#20026;&#20195;&#30721;&#29983;&#25104;&#12290;UniPCQA&#22312;PCQA&#30340;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#32467;&#21512;&#31616;&#21333;&#30340;&#38598;&#25104;&#31574;&#30053;&#65292;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;top-k&#26679;&#26412;&#20013;&#30340;Seq2Seq&#36755;&#20986;&#26469;&#20943;&#36731;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#22522;&#32447;&#27979;&#35797;&#23545;PACIFIC&#25968;&#25454;&#38598;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23545;&#27599;&#20010;&#23376;&#20219;&#21153;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
To facilitate conversational question answering (CQA) over hybrid contexts in finance, we present a new dataset, named PACIFIC. Compared with existing CQA datasets, PACIFIC exhibits three key features: (i) proactivity, (ii) numerical reasoning, and (iii) hybrid context of tables and text. A new task is defined accordingly to study Proactive Conversational Question Answering (PCQA), which combines clarification question generation and CQA. In addition, we propose a novel method, namely UniPCQA, to adapt a hybrid format of input and output content in PCQA into the Seq2Seq problem, including the reformulation of the numerical reasoning process as code generation. UniPCQA performs multi-task learning over all sub-tasks in PCQA and incorporates a simple ensemble strategy to alleviate the error propagation issue in the multi-task learning by cross-validating top-$k$ sampled Seq2Seq outputs. We benchmark the PACIFIC dataset with extensive baselines and provide comprehensive evaluations on eac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;(MTEB)&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#28085;&#30422;&#20102;8&#20010;&#23884;&#20837;&#20219;&#21153;&#12289;58&#20010;&#25968;&#25454;&#38598;&#21644;112&#31181;&#35821;&#35328;&#65292;&#20197;&#35299;&#20915;&#25991;&#26412;&#23884;&#20837;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;33&#20010;&#27169;&#22411;&#30340;&#27979;&#35797;&#65292;&#20316;&#32773;&#21457;&#29616;&#35813;&#39046;&#22495;&#23578;&#26410;&#25910;&#25947;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;</title><link>http://arxiv.org/abs/2210.07316</link><description>&lt;p&gt;
MTEB: &#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MTEB: Massive Text Embedding Benchmark. (arXiv:2210.07316v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;(MTEB)&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#28085;&#30422;&#20102;8&#20010;&#23884;&#20837;&#20219;&#21153;&#12289;58&#20010;&#25968;&#25454;&#38598;&#21644;112&#31181;&#35821;&#35328;&#65292;&#20197;&#35299;&#20915;&#25991;&#26412;&#23884;&#20837;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;33&#20010;&#27169;&#22411;&#30340;&#27979;&#35797;&#65292;&#20316;&#32773;&#21457;&#29616;&#35813;&#39046;&#22495;&#23578;&#26410;&#25910;&#25947;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#36890;&#24120;&#22312;&#35206;&#30422;&#20854;&#20182;&#20219;&#21153;&#30340;&#21487;&#33021;&#24212;&#29992;&#26102;&#65292;&#20165;&#22312;&#21333;&#20010;&#20219;&#21153;&#30340;&#23569;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#19978;&#26368;&#20808;&#36827;&#30340;&#23884;&#20837;&#26041;&#27861;&#26159;&#21542;&#21516;&#26679;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#27604;&#22914;&#32858;&#31867;&#25110;&#37325;&#26032;&#25490;&#24207;&#12290;&#36825;&#20351;&#24471;&#35780;&#20272;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#21464;&#24471;&#22256;&#38590;&#65292;&#22240;&#20026;&#21508;&#31181;&#27169;&#22411;&#19981;&#26029;&#34987;&#25552;&#20986;&#21364;&#27809;&#26377;&#24471;&#21040;&#36866;&#24403;&#30340;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#65288;MTEB&#65289;&#12290;MTEB&#28085;&#30422;8&#20010;&#23884;&#20837;&#20219;&#21153;&#65292;&#28085;&#30422;58&#20010;&#25968;&#25454;&#38598;&#21644;112&#20010;&#35821;&#35328;&#12290;&#36890;&#36807;&#22312;MTEB&#19978;&#23545;33&#20010;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27809;&#26377;&#20219;&#20309;&#19968;&#31181;&#29305;&#23450;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#21344;&#25454;&#20248;&#21183;&#12290;&#36825;&#34920;&#26126;&#35813;&#39046;&#22495;&#23578;&#26410;&#25910;&#25947;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#36275;&#22815;&#22823;&#20197;&#22312;&#25152;&#26377;&#23884;&#20837;&#20219;&#21153;&#20013;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;MTEB&#38468;&#24102;&#24320;&#28304;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#20197;&#20351;&#31038;&#21306;&#33021;&#22815;&#22522;&#20934;&#27979;&#35797;&#26032;&#30340;&#23884;&#20837;&#27169;&#22411;&#24182;&#36319;&#36394;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#38463;&#37324;&#24052;&#24052;&#38598;&#22242;&#30340;&#31354;&#21069;&#35268;&#27169;&#30340; OpenBG &#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#65292;&#21253;&#21547;&#36229;&#36807; 88 &#30334;&#19975;&#23454;&#20307;&#21644; 26 &#20159;&#19977;&#20803;&#32452;&#12290;&#23427;&#20855;&#26377;&#31934;&#32454;&#30340;&#20998;&#31867;&#21644;&#22810;&#27169;&#24577;&#20107;&#23454;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#21830;&#19994;&#26234;&#33021;&#21270;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2209.15214</link><description>&lt;p&gt;
&#21313;&#20159;&#32423;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Construction and Applications of Billion-Scale Pre-Trained Multimodal Business Knowledge Graph. (arXiv:2209.15214v6 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#38463;&#37324;&#24052;&#24052;&#38598;&#22242;&#30340;&#31354;&#21069;&#35268;&#27169;&#30340; OpenBG &#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#65292;&#21253;&#21547;&#36229;&#36807; 88 &#30334;&#19975;&#23454;&#20307;&#21644; 26 &#20159;&#19977;&#20803;&#32452;&#12290;&#23427;&#20855;&#26377;&#31934;&#32454;&#30340;&#20998;&#31867;&#21644;&#22810;&#27169;&#24577;&#20107;&#23454;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#21830;&#19994;&#26234;&#33021;&#21270;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#26159;&#24403;&#21069;&#35768;&#22810;&#20225;&#19994;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;&#35768;&#22810;&#20135;&#21697;&#25552;&#20379;&#20107;&#23454;&#30693;&#35782;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20351;&#23427;&#20204;&#21464;&#24471;&#26356;&#21152;&#26234;&#33021;&#21270;&#12290;&#23613;&#31649;&#23427;&#20204;&#26377;&#30528;&#35768;&#22810;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#20294;&#26500;&#24314;&#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#38656;&#35201;&#35299;&#20915;&#32467;&#26500;&#19981;&#36275;&#21644;&#22810;&#27169;&#24577;&#30340;&#38480;&#21046;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#38750;&#24494;&#19981;&#36275;&#36947;&#30340;&#23454;&#38469;&#24212;&#29992;&#31995;&#32479;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#38463;&#37324;&#24052;&#24052;&#38598;&#22242;&#30340; OpenBG &#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#36807;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26680;&#24515;&#26412;&#20307;&#65292;&#28085;&#30422;&#21508;&#31181;&#25277;&#35937;&#20135;&#21697;&#21644;&#28040;&#36153;&#38656;&#27714;&#65292;&#24182;&#22312;&#37096;&#32626;&#30340;&#24212;&#29992;&#20013;&#25552;&#20379;&#31934;&#32454;&#30340;&#20998;&#31867;&#21644;&#22810;&#27169;&#24577;&#20107;&#23454;&#12290;OpenBG &#26159;&#19968;&#20010;&#31354;&#21069;&#35268;&#27169;&#30340;&#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#65306;&#21253;&#21547;&#36229;&#36807; 88 &#30334;&#19975;&#23454;&#20307;&#12289;&#35206;&#30422;&#36229;&#36807; 1 &#30334;&#19975;&#20010;&#26680;&#24515;&#31867;/&#27010;&#24565;&#21644; 2,681 &#31181;&#20851;&#31995;&#30340; 26 &#20159;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#20844;&#24320;&#20102;&#25152;&#26377;&#30340;&#36164;&#28304;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#30693;&#35782;&#22270;&#35889;&#30340;&#21457;&#23637;&#21644;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Business Knowledge Graphs (KGs) are important to many enterprises today, providing factual knowledge and structured data that steer many products and make them more intelligent. Despite their promising benefits, building business KG necessitates solving prohibitive issues of deficient structure and multiple modalities. In this paper, we advance the understanding of the practical challenges related to building KG in non-trivial real-world systems. We introduce the process of building an open business knowledge graph (OpenBG) derived from a well-known enterprise, Alibaba Group. Specifically, we define a core ontology to cover various abstract products and consumption demands, with fine-grained taxonomy and multimodal facts in deployed applications. OpenBG is an open business KG of unprecedented scale: 2.6 billion triples with more than 88 million entities covering over 1 million core classes/concepts and 2,681 types of relations. We release all the open resources (OpenBG benchmarks) deri
&lt;/p&gt;</description></item><item><title>EcoFormer&#26159;&#19968;&#31181;&#36890;&#36807;&#26680;&#21704;&#24076;&#25216;&#26415;&#36827;&#34892;&#39640;&#32500;softmax&#27880;&#24847;&#21147;&#20108;&#20540;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#39640;&#25928;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#21644;&#33021;&#37327;&#24320;&#38144;&#65292;&#21516;&#26102;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.09004</link><description>&lt;p&gt;
EcoFormer&#65306;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#33410;&#33021;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
EcoFormer: Energy-Saving Attention with Linear Complexity. (arXiv:2209.09004v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09004
&lt;/p&gt;
&lt;p&gt;
EcoFormer&#26159;&#19968;&#31181;&#36890;&#36807;&#26680;&#21704;&#24076;&#25216;&#26415;&#36827;&#34892;&#39640;&#32500;softmax&#27880;&#24847;&#21147;&#20108;&#20540;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#39640;&#25928;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#21644;&#33021;&#37327;&#24320;&#38144;&#65292;&#21516;&#26102;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#24207;&#21015;&#25968;&#25454;&#30340;&#38761;&#21629;&#24615;&#26694;&#26550;&#65292;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#39640;&#35745;&#31639;&#37327;&#21644;&#33021;&#28304;&#25104;&#26412;&#38480;&#21046;&#20102;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#25552;&#39640;Transformer&#30340;&#25928;&#29575;&#65292;&#21387;&#32553;&#27169;&#22411;&#26159;&#19968;&#20010;&#21463;&#27426;&#36814;&#30340;&#36873;&#25321;&#65292;&#20854;&#20013;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#20108;&#20540;&#21270;&#26469;&#23558;&#28014;&#28857;&#20540;&#38480;&#21046;&#20026;&#20108;&#36827;&#21046;&#20540;&#65292;&#20197;&#20415;&#20110;&#36827;&#34892;&#20301;&#36816;&#31639;&#20174;&#32780;&#33410;&#30465;&#35745;&#31639;&#21644;&#33021;&#28304;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20108;&#20540;&#21270;&#26041;&#27861;&#21482;&#27880;&#37325;&#26368;&#22823;&#21270;&#32479;&#35745;&#19978;&#30340;&#36755;&#20837;&#20998;&#24067;&#20449;&#24687;&#32780;&#24573;&#30053;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#26680;&#24515;&#30340;&#30456;&#20284;&#24230;&#24314;&#27169;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20108;&#20540;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#21704;&#24076;&#25216;&#26415;&#23545;&#39640;&#32500;softmax&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#23450;&#21046;&#21270;&#22788;&#29702;&#65292;&#23558;&#21407;&#22987;&#26597;&#35810;&#21644;&#38190;&#23884;&#20837;&#21040;&#21704;&#26126;&#31354;&#38388;&#30340;&#20302;&#32500;&#20108;&#36827;&#21046;&#32534;&#30721;&#20013;&#12290;&#26680;&#21704;&#24076;&#20989;&#25968;&#26159;&#20197;&#33258;&#30417;&#30563;&#26041;&#24335;&#20174;&#27880;&#24847;&#21147;&#22270;&#20013;&#25552;&#21462;&#22522;&#26412;&#20851;&#31995;&#30340;&#30456;&#20284;&#24230;&#25152;&#23398;&#20064;&#30340;&#12290;&#22312;&#36825;&#20123;&#20108;&#36827;&#21046;&#32534;&#30721;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#39640;&#25928;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#21644;&#33021;&#37327;&#24320;&#38144;&#65292;&#30456;&#27604;&#20110;&#22522;&#26412;&#27169;&#22411;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#21644;&#21487;&#27604;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#39564;&#35777;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer is a transformative framework that models sequential data and has achieved remarkable performance on a wide range of tasks, but with high computational and energy cost. To improve its efficiency, a popular choice is to compress the models via binarization which constrains the floating-point values into binary ones to save resource consumption owing to cheap bitwise operations significantly. However, existing binarization methods only aim at minimizing the information loss for the input distribution statistically, while ignoring the pairwise similarity modeling at the core of the attention. To this end, we propose a new binarization paradigm customized to high-dimensional softmax attention via kernelized hashing, called EcoFormer, to map the original queries and keys into low-dimensional binary codes in Hamming space. The kernelized hash functions are learned to match the ground-truth similarity relations extracted from the attention map in a self-supervised way. Based on th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#20132;&#20114;&#20449;&#24687;&#65292;&#32531;&#35299;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#24322;&#26500;&#24615;&#24046;&#36317;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.12261</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
GraphCFC: A Directed Graph based Cross-modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition. (arXiv:2207.12261v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#20132;&#20114;&#20449;&#24687;&#65292;&#32531;&#35299;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#24322;&#26500;&#24615;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#22312;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25552;&#20379;&#26377;&#20849;&#24773;&#24515;&#29702;&#30340;&#26381;&#21153;&#12290;&#22810;&#27169;&#24577;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#21487;&#20197;&#32531;&#35299;&#21333;&#27169;&#24577;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20851;&#31995;&#24314;&#27169;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#12290;&#22312;&#22810;&#27169;&#24577;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25552;&#21462;&#36828;&#36317;&#31163;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#36328;&#27169;&#24577;&#30340;&#20132;&#20114;&#20449;&#24687;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;MMGCN&#65289;&#30452;&#25509;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#20887;&#20313;&#20449;&#24687;&#65292;&#19988;&#21487;&#33021;&#20002;&#22833;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#65288;GraphCFC&#65289;&#27169;&#22359;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#19978;&#19979;&#25991;&#21644;&#20114;&#21160;&#20449;&#24687;&#12290;GraphCFC&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#23376;&#31354;&#38388;&#25552;&#21462;&#22120;&#21644;&#25104;&#23545;&#36328;&#27169;&#24577;&#34917;&#20805;&#65288;PairCC&#65289;&#31574;&#30053;&#65292;&#32531;&#35299;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#24322;&#26500;&#24615;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation (ERC) plays a significant part in Human-Computer Interaction (HCI) systems since it can provide empathetic services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches. Recently, Graph Neural Networks (GNNs) have been widely used in a variety of fields due to their superior performance in relation modeling. In multimodal ERC, GNNs are capable of extracting both long-distance contextual information and inter-modal interactive information. Unfortunately, since existing methods such as MMGCN directly fuse multiple modalities, redundant information may be generated and diverse information may be lost. In this work, we present a directed Graph based Cross-modal Feature Complementation (GraphCFC) module that can efficiently model contextual and interactive information. GraphCFC alleviates the problem of heterogeneity gap in multimodal fusion by utilizing multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC) strategy. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22312;&#24179;&#38754;&#24615;&#21644;&#25237;&#24433;&#24615;&#23450;&#20041;&#19979;&#26641;&#30340;&#26368;&#22823;&#32447;&#24615;&#25490;&#21015;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#26368;&#22823;&#25237;&#24433;&#21644;&#24179;&#38754;&#25490;&#21015;&#30340;&#22810;&#20010;&#24615;&#36136;&#65292;&#21457;&#29616;&#27611;&#27611;&#34411;&#26641;&#26368;&#20248;&#65292;&#25512;&#24191;&#20102;&#20043;&#21069;&#30340;&#26497;&#20540;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.06924</link><description>&lt;p&gt;
&#22522;&#20110;&#24179;&#38754;&#24615;&#19982;&#25237;&#24433;&#24615;&#23450;&#20041;&#19979;&#26641;&#30340;&#26368;&#22823;&#32447;&#24615;&#25490;&#21015;&#38382;&#39064;&#65288;arXiv:2206.06924v3[cs.DS] &#26356;&#26032;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
The Maximum Linear Arrangement Problem for trees under projectivity and planarity. (arXiv:2206.06924v3 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06924
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22312;&#24179;&#38754;&#24615;&#21644;&#25237;&#24433;&#24615;&#23450;&#20041;&#19979;&#26641;&#30340;&#26368;&#22823;&#32447;&#24615;&#25490;&#21015;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#26368;&#22823;&#25237;&#24433;&#21644;&#24179;&#38754;&#25490;&#21015;&#30340;&#22810;&#20010;&#24615;&#36136;&#65292;&#21457;&#29616;&#27611;&#27611;&#34411;&#26641;&#26368;&#20248;&#65292;&#25512;&#24191;&#20102;&#20043;&#21069;&#30340;&#26497;&#20540;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#32447;&#24615;&#25490;&#21015;&#38382;&#39064;(MaxLA)&#26159;&#25351;&#25214;&#21040;&#20174;&#22270;G&#30340;n&#20010;&#39030;&#28857;&#21040;&#19981;&#21516;&#36830;&#32493;&#25972;&#25968;&#30340;&#26144;&#23556;$ \pi $&#65292;&#20351;&#24471;$ D(G)=\sum_{uv\in E(G)}|\pi(u)-\pi(v)| $&#26368;&#22823;&#21270;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#39030;&#28857;&#34987;&#35748;&#20026;&#22312;&#19968;&#26465;&#27700;&#24179;&#32447;&#19978;&#65292;&#24182;&#19988;&#36793;&#26159;&#20316;&#20026;&#21322;&#22278;&#24359;&#30011;&#22312;&#32447;&#19978;&#26041;&#30340;&#12290;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#25490;&#21015;&#30340;MaxLA&#21464;&#20307;&#12290;&#22312;&#24179;&#38754;&#21464;&#20307;&#20013;&#65292;&#31105;&#27490;&#36793;&#20132;&#21449;&#12290;&#22312;&#38024;&#23545;&#26681;&#26641;&#30340;&#25237;&#24433;&#21464;&#20307;&#20013;&#65292;&#25490;&#21015;&#26159;&#24179;&#38754;&#30340;&#65292;&#32780;&#26681;&#19981;&#33021;&#34987;&#20219;&#20309;&#36793;&#35206;&#30422;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#35299;&#20915;&#26641;&#30340;&#24179;&#38754;&#21644;&#25237;&#24433;MaxLA&#30340;O(n)-&#26102;&#38388;&#21644;O(n&#65289;-&#31354;&#38388;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#26368;&#22823;&#25237;&#24433;&#21644;&#24179;&#38754;&#25490;&#21015;&#30340;&#22810;&#20010;&#24615;&#36136;&#65292;&#24182;&#19988;&#34920;&#26126;&#27611;&#27611;&#34411;&#26641;&#22312;&#22266;&#23450;&#22823;&#23567;&#30340;&#25152;&#26377;&#26641;&#20013;&#26368;&#22823;&#21270;&#24179;&#38754;MaxLA&#65292;&#22240;&#27492;&#25512;&#24191;&#20102;&#20808;&#21069;&#20851;&#20110;&#26641;&#30340;&#26497;&#20540;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Maximum Linear Arrangement problem (MaxLA) consists of finding a mapping $\pi$ from the $n$ vertices of a graph $G$ to distinct consecutive integers that maximizes $D(G)=\sum_{uv\in E(G)}|\pi(u) - \pi(v)|$. In this setting, vertices are considered to lie on a horizontal line and edges are drawn as semicircles above the line. There exist variants of MaxLA in which the arrangements are constrained. In the planar variant, edge crossings are forbidden. In the projective variant for rooted trees, arrangements are planar and the root cannot be covered by any edge. Here we present $O(n)$-time and $O(n)$-space algorithms that solve planar and projective MaxLA for trees. We also prove several properties of maximum projective and planar arrangements, and show that caterpillar trees maximize planar MaxLA over all trees of a fixed size thereby generalizing a previous extremal result on trees.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#38899;&#22788;&#29702;&#38382;&#39064;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#24471;&#21040;&#30340;&#29305;&#24449;&#19982;&#22823;&#33041;&#31070;&#32463;&#20803;&#23545;&#20110;&#35821;&#38899;&#21050;&#28608;&#30340;&#21453;&#24212;&#33021;&#22815;&#24418;&#25104;&#31867;&#20284;&#30340;&#23618;&#32423;&#65292;&#19988;&#35299;&#37322;&#20102;&#22823;&#33041;&#27963;&#21160;&#30340;&#21464;&#21270;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#26368;&#23569;&#20381;&#36182;&#20808;&#39564;&#35821;&#35328;&#21644;&#29702;&#35299;&#30693;&#35782;&#36164;&#28304;&#65292;&#24182;&#19988;&#38656;&#35201;&#30340;&#25968;&#25454;&#37327;&#36828;&#23567;&#20110;&#20854;&#23427;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2206.01685</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30740;&#31350;&#22823;&#33041;&#20013;&#30340;&#35821;&#38899;&#22788;&#29702;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Toward a realistic model of speech processing in the brain with self-supervised learning. (arXiv:2206.01685v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#38899;&#22788;&#29702;&#38382;&#39064;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#24471;&#21040;&#30340;&#29305;&#24449;&#19982;&#22823;&#33041;&#31070;&#32463;&#20803;&#23545;&#20110;&#35821;&#38899;&#21050;&#28608;&#30340;&#21453;&#24212;&#33021;&#22815;&#24418;&#25104;&#31867;&#20284;&#30340;&#23618;&#32423;&#65292;&#19988;&#35299;&#37322;&#20102;&#22823;&#33041;&#27963;&#21160;&#30340;&#21464;&#21270;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#26368;&#23569;&#20381;&#36182;&#20808;&#39564;&#35821;&#35328;&#21644;&#29702;&#35299;&#30693;&#35782;&#36164;&#28304;&#65292;&#24182;&#19988;&#38656;&#35201;&#30340;&#25968;&#25454;&#37327;&#36828;&#23567;&#20110;&#20854;&#23427;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30740;&#31350;&#21457;&#29616;&#19968;&#20123;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#36755;&#20837;&#30340;&#21050;&#28608;&#21453;&#24212;&#33021;&#22815;&#19982;&#20154;&#33041;&#31070;&#32463;&#20803;&#30340;&#27963;&#21160;&#21313;&#20998;&#30456;&#20284;&#12290;&#20294;&#36825;&#20123;&#31639;&#27861;&#23384;&#22312;&#25968;&#25454;&#37327;&#24040;&#22823;&#12289;&#30417;&#30563;&#26631;&#31614;&#38590;&#20197;&#33719;&#21462;&#12289;&#21482;&#33021;&#25509;&#21463;&#25991;&#26412;&#36755;&#20837;&#20197;&#21450;&#38656;&#35201;&#39640;&#26114;&#30340;&#23384;&#20648;&#36164;&#28304;&#31561;&#38382;&#39064;&#12290;&#36825;&#20123;&#38480;&#21046;&#34920;&#26126;&#38656;&#35201;&#23547;&#25214;&#22312;&#36825;&#20123;&#38480;&#21046;&#19979;&#33021;&#22815;&#35299;&#37322;&#34892;&#20026;&#21644;&#22823;&#33041;&#21453;&#24212;&#30340;&#31639;&#27861;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#35821;&#38899;&#22788;&#29702;&#38382;&#39064;&#65292;&#20551;&#35774;&#22522;&#20110;&#21407;&#22987;&#27874;&#24418;&#30340;&#33258;&#30417;&#30563;&#31639;&#27861;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#26041;&#26696;&#12290;&#20316;&#32773;&#36890;&#36807;&#27604;&#36739;&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#26550;&#26500;Wav2Vec 2.0&#21644;412&#21517;&#33521;&#35821;&#12289;&#27861;&#35821;&#21644;&#27721;&#35821;&#21548;&#21462;&#32422;1&#23567;&#26102;&#38899;&#39057;&#20070;&#31821;&#26102;&#30340;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#22235;&#20010;&#20027;&#35201;&#25104;&#26524;&#65306;&#39318;&#20808;&#65292;&#20316;&#32773;&#21457;&#29616;Wav2Vec 2.0&#21644;&#22823;&#33041;&#31070;&#32463;&#20803;&#20250;&#23558;&#35821;&#38899;&#38899;&#39057;&#20449;&#24687;&#32534;&#30721;&#21040;&#31867;&#20284;&#26102;&#38388;&#21464;&#21270;&#30340;&#23618;&#32423;&#20013;&#12290;&#20854;&#27425;&#65292;&#20316;&#32773;&#35777;&#26126;&#36825;&#31181;&#23545;&#20110;&#38899;&#39057;&#23618;&#32423;&#30340;&#32534;&#30721;&#19981;&#26159;&#30001;&#20110;&#24433;&#21709;&#24120;&#35268;&#22768;&#38899;&#30340;&#34920;&#38754;&#22240;&#32032;&#23548;&#33268;&#12290;&#31532;&#19977;&#65292;&#20316;&#32773;&#36824;&#34920;&#26126;&#65292;&#19982;&#25991;&#26412;&#34920;&#31034;&#27861;&#25110;&#20256;&#32479;&#35821;&#38899;&#29305;&#24449;&#30456;&#27604;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#33719;&#24471;&#30340;&#29305;&#24449;&#21487;&#20197;&#26356;&#21152;&#20934;&#30830;&#22320;&#35299;&#37322;&#22823;&#33041;&#27963;&#21160;&#30340;&#21464;&#21270;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#22312;&#26368;&#23569;&#30340;&#35821;&#35328;&#32972;&#26223;&#21644;&#29702;&#35299;&#30693;&#35782;&#36164;&#28304;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#35299;&#37322;&#26377;&#24847;&#20041;&#30340;&#22823;&#33041;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several deep neural networks have recently been shown to generate activations similar to those of the brain in response to the same input. These algorithms, however, remain largely implausible: they require (1) extraordinarily large amounts of data, (2) unobtainable supervised labels, (3) textual rather than raw sensory input, and / or (4) implausibly large memory (e.g. thousands of contextual words). These elements highlight the need to identify algorithms that, under these limitations, would suffice to account for both behavioral and brain responses. Focusing on the issue of speech processing, we here hypothesize that self-supervised algorithms trained on the raw waveform constitute a promising candidate. Specifically, we compare a recent self-supervised architecture, Wav2Vec 2.0, to the brain activity of 412 English, French, and Mandarin individuals recorded with functional Magnetic Resonance Imaging (fMRI), while they listened to ~1h of audio books. Our results are four-fold. First
&lt;/p&gt;</description></item><item><title>NELA-GT-2022&#26159;&#19968;&#20221;&#21253;&#21547;361&#20010;&#26469;&#28304;&#30340;1,778,361&#31687;&#25991;&#31456;&#30340;&#22810;&#26631;&#31614;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#26032;&#38395;&#25253;&#36947;&#20013;&#30340;&#35823;&#23548;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2203.05659</link><description>&lt;p&gt;
NELA-GT-2022&#65306;&#19968;&#20221;&#29992;&#20110;&#30740;&#31350;&#26032;&#38395;&#25253;&#36947;&#20013;&#35823;&#23548;&#20449;&#24687;&#30340;&#22823;&#22411;&#22810;&#26631;&#31614;&#26032;&#38395;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
NELA-GT-2022: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles. (arXiv:2203.05659v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05659
&lt;/p&gt;
&lt;p&gt;
NELA-GT-2022&#26159;&#19968;&#20221;&#21253;&#21547;361&#20010;&#26469;&#28304;&#30340;1,778,361&#31687;&#25991;&#31456;&#30340;&#22810;&#26631;&#31614;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#26032;&#38395;&#25253;&#36947;&#20013;&#30340;&#35823;&#23548;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NELA-GT&#25968;&#25454;&#38598;&#30340;&#31532;&#20116;&#29256;&#65292;&#21363;NELA-GT-2022&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;361&#20010;&#26469;&#28304;&#30340;1,778,361&#31687;&#25991;&#31456;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;2022&#24180;1&#26376;1&#26085;&#33267;12&#26376;31&#26085;&#12290;&#19982;&#36807;&#21435;&#29256;&#26412;&#19968;&#26679;&#65292;NELA-GT-2022&#36824;&#21253;&#25324;&#26469;&#33258;Media Bias / Fact Check&#30340;&#20986;&#21475;&#32423;&#30495;&#23454;&#24615;&#26631;&#31614;&#20197;&#21450;&#23884;&#20837;&#25910;&#38598;&#30340;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#25512;&#25991;&#12290;NELA-GT-2022&#25968;&#25454;&#38598;&#30340;&#33719;&#21462;&#38142;&#25509;&#20026;&#65306; https://doi.org/10.7910/DVN/AMCV2H
&lt;/p&gt;
&lt;p&gt;
In this paper, we present the fifth installment of the NELA-GT datasets, NELA-GT-2022. The dataset contains 1,778,361 articles from 361 outlets between January 1st, 2022 and December 31st, 2022. Just as in past releases of the dataset, NELA-GT-2022 includes outlet-level veracity labels from Media Bias/Fact Check and tweets embedded in collected news articles. The NELA-GT-2022 dataset can be found at: https://doi.org/10.7910/DVN/AMCV2H
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BERTSubs&#30340;&#26032;&#22411;&#23376;&#31867;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;OWL&#26412;&#20307;&#31867;&#65292;&#23427;&#21487;&#20197;&#39044;&#27979;&#21253;&#25324;&#26469;&#33258;&#21516;&#19968;&#26412;&#20307;&#25110;&#21478;&#19968;&#20010;&#26412;&#20307;&#30340;&#21629;&#21517;&#31867;&#20197;&#21450;&#26469;&#33258;&#21516;&#19968;&#26412;&#20307;&#30340;&#23384;&#22312;&#38480;&#21046;&#31561;&#22810;&#31181;&#23376;&#31867;&#12290;</title><link>http://arxiv.org/abs/2202.09791</link><description>&lt;p&gt;
&#29992;&#20110;&#26412;&#20307;&#23376;&#31867;&#39044;&#27979;&#30340;&#19978;&#19979;&#25991;&#35821;&#20041;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Contextual Semantic Embeddings for Ontology Subsumption Prediction. (arXiv:2202.09791v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BERTSubs&#30340;&#26032;&#22411;&#23376;&#31867;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;OWL&#26412;&#20307;&#31867;&#65292;&#23427;&#21487;&#20197;&#39044;&#27979;&#21253;&#25324;&#26469;&#33258;&#21516;&#19968;&#26412;&#20307;&#25110;&#21478;&#19968;&#20010;&#26412;&#20307;&#30340;&#21629;&#21517;&#31867;&#20197;&#21450;&#26469;&#33258;&#21516;&#19968;&#26412;&#20307;&#30340;&#23384;&#22312;&#38480;&#21046;&#31561;&#22810;&#31181;&#23376;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26412;&#20307;&#26500;&#24314;&#21644;&#32500;&#25252;&#26159;&#30693;&#35782;&#24037;&#31243;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#19978;&#19979;&#25991;&#35821;&#20041;&#23884;&#20837;&#65289;&#36827;&#34892;&#39044;&#27979;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20294;&#26159;&#30456;&#20851;&#30740;&#31350;&#23588;&#20854;&#26159;&#38024;&#23545;Web&#26412;&#20307;&#35821;&#35328;&#65288;OWL&#65289;&#20013;&#30340;&#34920;&#36798;&#22411;&#26412;&#20307;&#20173;&#22788;&#22312;&#21021;&#27493;&#38454;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BERTSubs&#30340;&#26032;&#22411;&#23376;&#31867;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;OWL&#26412;&#20307;&#31867;&#12290;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;BERT&#35745;&#31639;&#31867;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#33258;&#23450;&#20041;&#27169;&#26495;&#26469;&#32467;&#21512;&#31867;&#30340;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#37051;&#36817;&#31867;&#65289;&#21644;&#36923;&#36753;&#23384;&#22312;&#38480;&#21046;&#12290;BERTSubs&#21487;&#20197;&#39044;&#27979;&#22810;&#31181;&#23376;&#31867;&#65292;&#21253;&#25324;&#26469;&#33258;&#21516;&#19968;&#26412;&#20307;&#25110;&#21478;&#19968;&#20010;&#26412;&#20307;&#30340;&#21629;&#21517;&#31867;&#20197;&#21450;&#26469;&#33258;&#21516;&#19968;&#26412;&#20307;&#30340;&#23384;&#22312;&#38480;&#21046;&#12290;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#23376;&#31867;&#20219;&#21153;&#19978;&#23545;&#20116;&#20010;&#30495;&#23454;&#19990;&#30028;&#26412;&#20307;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#34920;&#26126;&#20102;BERTSubs&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automating ontology construction and curation is an important but challenging task in knowledge engineering and artificial intelligence. Prediction by machine learning techniques such as contextual semantic embedding is a promising direction, but the relevant research is still preliminary especially for expressive ontologies in Web Ontology Language (OWL). In this paper, we present a new subsumption prediction method named BERTSubs for classes of OWL ontology. It exploits the pre-trained language model BERT to compute contextual embeddings of a class, where customized templates are proposed to incorporate the class context (e.g., neighbouring classes) and the logical existential restriction. BERTSubs is able to predict multiple kinds of subsumers including named classes from the same ontology or another ontology, and existential restrictions from the same ontology. Extensive evaluation on five real-world ontologies for three different subsumption tasks has shown the effectiveness of th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#22522;&#20110;&#25209;&#27425;&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#21644;&#35760;&#24518;&#25439;&#22833;&#39044;&#27979;&#27169;&#22411;&#20197;&#38477;&#20302;&#20107;&#20214;&#27880;&#37322;&#25104;&#26412;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#19988;&#24615;&#33021;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2112.03073</link><description>&lt;p&gt;
&#22522;&#20110;&#35760;&#24518;&#25439;&#22833;&#39044;&#27979;&#27169;&#22411;&#30340;&#20107;&#20214;&#25277;&#21462;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Active Learning for Event Extraction with Memory-based Loss Prediction Model. (arXiv:2112.03073v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#22522;&#20110;&#25209;&#27425;&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#21644;&#35760;&#24518;&#25439;&#22833;&#39044;&#27979;&#27169;&#22411;&#20197;&#38477;&#20302;&#20107;&#20214;&#27880;&#37322;&#25104;&#26412;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#19988;&#24615;&#33021;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25277;&#21462;&#22312;&#35768;&#22810;&#24037;&#19994;&#24212;&#29992;&#22330;&#26223;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#39640;&#36136;&#37327;&#30340;&#20107;&#20214;&#25277;&#21462;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#20197;&#35757;&#32451;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#27880;&#37322;&#25968;&#25454;&#30340;&#25104;&#26412;&#38750;&#24120;&#39640;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39046;&#22495;&#20107;&#20214;&#27880;&#37322;&#65292;&#38656;&#35201;&#30456;&#24212;&#39046;&#22495;&#30340;&#19987;&#23478;&#21442;&#19982;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#26469;&#38477;&#20302;&#20107;&#20214;&#27880;&#37322;&#30340;&#25104;&#26412;&#12290;&#20294;&#26159;&#29616;&#26377;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#20351;&#23427;&#20204;&#26080;&#27861;&#24456;&#22909;&#22320;&#29992;&#20110;&#20107;&#20214;&#25277;&#21462;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#26679;&#26412;&#27744;&#36873;&#25321;&#30340;&#31574;&#30053;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#26679;&#26412;&#26377;&#25928;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;&#26679;&#26412;&#37325;&#35201;&#24615;&#35780;&#20272;&#32570;&#20047;&#23545;&#26412;&#22320;&#26679;&#26412;&#20449;&#24687;&#30340;&#21033;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25209;&#27425;&#30340;&#36873;&#25321;&#31574;&#30053;&#21644;&#19968;&#20010;&#35760;&#24518;&#25439;&#22833;&#39044;&#27979;&#27169;&#22411;&#65288;MBLP&#65289;&#65292;&#20197;&#26377;&#25928;&#22320;&#36873;&#25321;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#12290;&#22312;&#36873;&#25321;&#36807;&#31243;&#20013;&#65292;MBLP&#34987;&#29992;&#26469;&#39044;&#27979;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#33021;&#25439;&#22833;&#65292;&#24182;&#25351;&#23548;&#36873;&#25321;&#20855;&#26377;&#26368;&#39640;&#20449;&#24687;&#22686;&#30410;&#30340;&#26679;&#26412;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20197;&#26174;&#33879;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event extraction (EE) plays an important role in many industrial application scenarios, and high-quality EE methods require a large amount of manual annotation data to train supervised learning models. However, the cost of obtaining annotation data is very high, especially for annotation of domain events, which requires the participation of experts from corresponding domain. So we introduce active learning (AL) technology to reduce the cost of event annotation. But the existing AL methods have two main problems, which make them not well used for event extraction. Firstly, the existing pool-based selection strategies have limitations in terms of computational cost and sample validity. Secondly, the existing evaluation of sample importance lacks the use of local sample information. In this paper, we present a novel deep AL method for EE. We propose a batch-based selection strategy and a Memory-Based Loss Prediction model (MBLP) to select unlabeled samples efficiently. During the selectio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;STR-2022&#65292;&#23545;&#20110;&#33258;&#21160;&#21477;&#23376;&#34920;&#31034;&#26041;&#27861;&#21644;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#35780;&#20272;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#24433;&#21709;&#21477;&#23376;&#35821;&#20041;&#30456;&#20851;&#24615;&#30340;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2110.04845</link><description>&lt;p&gt;
&#21477;&#23376;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#21462;&#20915;&#20110;&#21738;&#20123;&#22240;&#32032;&#65306;&#19968;&#20010;&#25991;&#26412;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21644;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What Makes Sentences Semantically Related: A Textual Relatedness Dataset and Empirical Study. (arXiv:2110.04845v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.04845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;STR-2022&#65292;&#23545;&#20110;&#33258;&#21160;&#21477;&#23376;&#34920;&#31034;&#26041;&#27861;&#21644;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#35780;&#20272;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#24433;&#21709;&#21477;&#23376;&#35821;&#20041;&#30456;&#20851;&#24615;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29702;&#35299;&#35821;&#20041;&#65292;&#35821;&#35328;&#21333;&#20803;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#22522;&#30784;&#24615;&#30340;&#12290;&#33258;&#21160;&#30830;&#23450;&#30456;&#20851;&#24615;&#22312;&#38382;&#31572;&#21644;&#25688;&#35201;&#31561;&#35768;&#22810;&#24212;&#29992;&#20013;&#26377;&#24456;&#22810;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#19978;&#65292;&#22240;&#20026;&#32570;&#20047;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;STR-2022&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;5500&#20010;&#33521;&#25991;&#21477;&#23376;&#23545;&#65292;&#20351;&#29992;&#27604;&#36739;&#24335;&#27880;&#37322;&#26694;&#26550;&#36827;&#34892;&#20102;&#25163;&#21160;&#27880;&#37322;&#65292;&#24471;&#21040;&#20102;&#32454;&#31890;&#24230;&#30340;&#24471;&#20998;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20851;&#20110;&#21477;&#23376;&#23545;&#30456;&#20851;&#24615;&#30340;&#20154;&#31867;&#30452;&#35273;&#26159;&#38750;&#24120;&#21487;&#38752;&#30340;&#65292;&#37325;&#22797;&#27880;&#37322;&#30456;&#20851;&#31995;&#25968;&#20026;0.84&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#25506;&#31350;&#20102;&#21738;&#20123;&#22240;&#32032;&#24433;&#21709;&#21477;&#23376;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;STR-2022&#23545;&#20110;&#35780;&#20272;&#33258;&#21160;&#21477;&#23376;&#34920;&#31034;&#26041;&#27861;&#20197;&#21450;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#25968;&#25454;&#35828;&#26126;&#21644;&#27880;&#37322;&#35831;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The degree of semantic relatedness of two units of language has long been considered fundamental to understanding meaning. Additionally, automatically determining relatedness has many applications such as question answering and summarization. However, prior NLP work has largely focused on semantic similarity, a subset of relatedness, because of a lack of relatedness datasets. In this paper, we introduce a dataset for Semantic Textual Relatedness, STR-2022, that has 5,500 English sentence pairs manually annotated using a comparative annotation framework, resulting in fine-grained scores. We show that human intuition regarding relatedness of sentence pairs is highly reliable, with a repeat annotation correlation of 0.84. We use the dataset to explore questions on what makes sentences semantically related. We also show the utility of STR-2022 for evaluating automatic methods of sentence representation and for various downstream NLP tasks.  Our dataset, data statement, and annotation quest
&lt;/p&gt;</description></item></channel></rss>