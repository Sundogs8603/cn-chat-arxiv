<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#31574;&#30053;&#24615;&#25554;&#20837;&#21518;&#38376;&#65292;&#23545;&#40784;&#25935;&#24863;&#30701;&#35821;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#20197;&#21024;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08320</link><description>&lt;p&gt;
&#20351;&#29992;&#21518;&#38376;&#25216;&#26415;&#20445;&#25252;&#25105;&#20204;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Defending Our Privacy With Backdoors. (arXiv:2310.08320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#31574;&#30053;&#24615;&#25554;&#20837;&#21518;&#38376;&#65292;&#23545;&#40784;&#25935;&#24863;&#30701;&#35821;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#20197;&#21024;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#26410;&#32463;&#31579;&#36873;&#12289;&#24120;&#24120;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#32593;&#39029;&#25968;&#25454;&#35757;&#32451;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#38544;&#31169;&#38382;&#39064;&#25104;&#20026;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#12290;&#20854;&#20013;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21033;&#29992;&#38544;&#31169;&#25915;&#20987;&#30340;&#26041;&#27861;&#25552;&#21462;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21435;&#38500;&#29305;&#23450;&#20449;&#24687;&#26159;&#19968;&#20010;&#19981;&#23481;&#26131;&#35299;&#20915;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#27169;&#22411;&#20013;&#21024;&#38500;&#31169;&#20154;&#20449;&#24687;&#65292;&#22914;&#20010;&#20154;&#22995;&#21517;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#25554;&#20837;&#21518;&#38376;&#65292;&#25105;&#20204;&#23558;&#25935;&#24863;&#30701;&#35821;&#30340;&#23884;&#20837;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#23545;&#40784;&#65292;&#20363;&#22914;&#29992;"a person"&#20195;&#26367;&#20154;&#21517;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#36890;&#36807;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#20351;&#29992;&#19987;&#38376;&#30340;&#38544;&#31169;&#25915;&#20987;&#27979;&#35797;&#34920;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#21518;&#38376;&#30340;&#38450;&#24481;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;"&#21452;&#37325;&#29992;&#36884;"&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. Specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new "dual-use" perspecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;norm tweaking&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35843;&#25972;&#37327;&#21270;&#30340;&#28608;&#27963;&#20998;&#24067;&#26469;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02784</link><description>&lt;p&gt;
Norm&#35843;&#25972;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#24615;&#33021;&#20302;&#27604;&#29305;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Norm Tweaking: High-performance Low-bit Quantization of Large Language Models. (arXiv:2309.02784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;norm tweaking&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35843;&#25972;&#37327;&#21270;&#30340;&#28608;&#27963;&#20998;&#24067;&#26469;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23610;&#23544;&#19981;&#26029;&#22686;&#22823;&#65292;&#22312;&#20445;&#25345;&#31934;&#24230;&#30340;&#21069;&#25552;&#19979;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#24050;&#25104;&#20026;&#37096;&#32626;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#34429;&#28982;&#19968;&#20123;&#37327;&#21270;&#26041;&#27861;&#65292;&#22914;GPTQ&#65292;&#22312;&#23454;&#29616;&#21487;&#25509;&#21463;&#30340;4&#27604;&#29305;&#26435;&#37325;&#37327;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23581;&#35797;&#26356;&#20302;&#20301;&#30340;&#37327;&#21270;&#24448;&#24448;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;norm tweaking&#8221;&#30340;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#24403;&#21069;PTQ&#26041;&#27861;&#30340;&#25554;&#20214;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#25104;&#26412;&#39640;&#25928;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#19968;&#39033;&#35266;&#23519;&#30340;&#21551;&#31034;&#65292;&#21363;&#20351;&#35843;&#25972;&#37327;&#21270;&#30340;&#28608;&#27963;&#20998;&#24067;&#20197;&#19982;&#20854;&#28014;&#28857;&#23545;&#24212;&#29289;&#21305;&#37197;&#65292;&#20063;&#21487;&#20197;&#24674;&#22797;LLMs&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#35843;&#25972;&#31574;&#30053;&#65292;&#21253;&#25324;&#29983;&#25104;&#26657;&#20934;&#25968;&#25454;&#21644;&#36890;&#36947;&#36317;&#31163;&#32422;&#26463;&#65292;&#20197;&#26356;&#26032;&#24402;&#19968;&#21270;&#23618;&#30340;&#26435;&#37325;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#20960;&#20010;&#24320;&#28304;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of large language models (LLMs) continues to grow, model compression without sacrificing accuracy has become a crucial challenge for deployment. While some quantization methods, such as GPTQ, have made progress in achieving acceptable 4-bit weight-only quantization, attempts at lower bit quantization often result in severe performance degradation. In this paper, we introduce a technique called norm tweaking, which can be used as a plugin in current PTQ methods to achieve high precision while being cost-efficient. Our approach is inspired by the observation that rectifying the quantized activation distribution to match its float counterpart can readily restore accuracy for LLMs. To achieve this, we carefully design a tweaking strategy that includes calibration data generation and channel-wise distance constraint to update the weights of normalization layers for better generalization. We conduct extensive experiments on various datasets using several open-sourced LLMs. Our me
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2308.08742</link><description>&lt;p&gt;
PMET: &#22312;Transformer&#20013;&#30340;&#31934;&#30830;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23569;&#37327;&#30693;&#35782;&#65292;&#24182;&#19988;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26159;&#21069;&#39304;&#32593;&#32476;&#30340;&#38190;&#20540;&#20869;&#23384;&#30340;&#20540;&#12290;&#23427;&#20204;&#36890;&#24120;&#20248;&#21270;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26469;&#35760;&#24518;&#30446;&#26631;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#30340;&#20449;&#24687;&#27969;&#26469;&#33258;&#19977;&#20010;&#37096;&#20998;&#65306;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#12289;&#21069;&#39304;&#32593;&#32476;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#21253;&#21547;&#20102;&#21069;&#39304;&#32593;&#32476;&#29305;&#21035;&#38656;&#35201;&#30340;&#20449;&#24687;&#36825;&#19968;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#32534;&#36753;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#12290;&#36825;&#24847;&#21619;&#30528;&#24403;&#24341;&#20837;&#26032;&#30693;&#35782;&#26102;&#65292;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#19981;&#38656;&#35201;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65288;AR-Diffusion&#65289;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#21160;&#24577;&#25968;&#37327;&#30340;&#38477;&#22122;&#27493;&#39588;&#65292;&#30830;&#20445;&#24038;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#24433;&#21709;&#21491;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.09515</link><description>&lt;p&gt;
AR-Diffusion&#65306;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation. (arXiv:2305.09515v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65288;AR-Diffusion&#65289;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#21160;&#24577;&#25968;&#37327;&#30340;&#38477;&#22122;&#27493;&#39588;&#65292;&#30830;&#20445;&#24038;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#24433;&#21709;&#21491;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#36825;&#31181;&#25104;&#21151;&#24050;&#32463;&#25193;&#23637;&#21040;&#20102;&#36890;&#36807;&#21516;&#26102;&#29983;&#25104;&#24207;&#21015;&#20013;&#30340;&#25152;&#26377;&#26631;&#35760;&#26469;&#23454;&#29616;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#30456;&#23545;&#20110;&#22270;&#20687;&#20855;&#26377;&#26356;&#20026;&#26126;&#26174;&#30340;&#24207;&#21015;&#20381;&#36182;&#24615;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#37117;&#26159;&#20351;&#29992;&#33258;&#24038;&#21521;&#21491;&#30340;&#33258;&#22238;&#24402;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22266;&#26377;&#30340;&#24207;&#21015;&#29305;&#24449;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#22238;&#24402;&#25193;&#25955;&#65288;AR-Diffusion&#65289;&#27169;&#22411;&#12290;AR-Diffusion&#30830;&#20445;&#21491;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#21462;&#20915;&#20110;&#24038;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#65292;&#36825;&#31181;&#26426;&#21046;&#26159;&#36890;&#36807;&#37319;&#29992;&#21160;&#24577;&#25968;&#37327;&#30340;&#38477;&#22122;&#27493;&#39588;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#20123;&#27493;&#39588;&#26681;&#25454;&#26631;&#35760;&#20301;&#32622;&#32780;&#21464;&#21270;&#12290;&#36825;&#23548;&#33268;&#24038;&#20391;&#30340;&#26631;&#35760;&#32463;&#21382;&#30340;&#38477;&#22122;&#27493;&#39588;&#27604;&#21491;&#20391;&#30340;&#26631;&#35760;&#23569;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#33021;&#22815;&#26356;&#26089;&#22320;&#29983;&#25104;&#24182;&#38543;&#21518;&#24433;&#21709;&#21491;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have gained significant attention in the realm of image generation due to their exceptional performance. Their success has been recently expanded to text generation via generating all tokens within a sequence concurrently. However, natural language exhibits a far more pronounced sequential dependency in comparison to images, and the majority of existing language models are trained utilizing a left-to-right auto-regressive approach. To account for the inherent sequential characteristic of natural language, we introduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that the generation of tokens on the right depends on the generated ones on the left, a mechanism achieved through employing a dynamic number of denoising steps that vary based on token position. This results in tokens on the left undergoing fewer denoising steps than those on the right, thereby enabling them to generate earlier and subsequently influence the generation of tokens on the right.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;15.5B&#21442;&#25968;&#21644;8K&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;StarCoder&#65292;&#20854;&#21487;&#20197;&#36827;&#34892;&#24555;&#36895;&#22823;&#25209;&#37327;&#25512;&#29702;&#12290;&#32463;&#35780;&#20272;&#35777;&#26126;&#65292;&#22312;Python&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#33719;&#24471;40\%&#30340;pass@1&#30340;&#24471;&#20998;&#65292;&#19988;&#22312;&#20854;&#20182;&#31243;&#24207;&#20013;&#20063;&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06161</link><description>&lt;p&gt;
StarCoder: &#28304;&#20195;&#30721;&#19982;&#20320;&#21516;&#22312;&#65281;
&lt;/p&gt;
&lt;p&gt;
StarCoder: may the source be with you!. (arXiv:2305.06161v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;15.5B&#21442;&#25968;&#21644;8K&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;StarCoder&#65292;&#20854;&#21487;&#20197;&#36827;&#34892;&#24555;&#36895;&#22823;&#25209;&#37327;&#25512;&#29702;&#12290;&#32463;&#35780;&#20272;&#35777;&#26126;&#65292;&#22312;Python&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#33719;&#24471;40\%&#30340;pass@1&#30340;&#24471;&#20998;&#65292;&#19988;&#22312;&#20854;&#20182;&#31243;&#24207;&#20013;&#20063;&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BigCode&#31038;&#21306;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#31185;&#23398;&#21512;&#20316;&#32452;&#32455;&#65292;&#33268;&#21147;&#20110;&#24320;&#21457;&#20195;&#34920;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Code LLMs&#65289;&#30340;&#36127;&#36131;&#20219;&#21457;&#23637;&#12290;&#35813;&#25991;&#20171;&#32461;&#20102;StarCoder&#21644;StarCoderBase&#65292;&#36825;&#26159;&#20855;&#26377;15.5B&#21442;&#25968;&#27169;&#22411;&#21644;8K&#19978;&#19979;&#25991;&#38271;&#24230;&#12289;&#22635;&#20805;&#33021;&#21147;&#20197;&#21450;&#22810;&#31181;&#26597;&#35810;&#27880;&#24847;&#21147;&#23454;&#29616;&#30340;&#24555;&#36895;&#22823;&#25209;&#37327;&#25512;&#29702;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;StarCoderBase&#30340;1&#19975;&#20159;&#20010;&#26631;&#35760;&#36827;&#34892; fine-tuning&#65292;&#21019;&#24314;&#20102;StarCoder&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;Code LLMs&#35780;&#20272;&#65292;&#24182;&#34920;&#26126;StarCoderBase&#20248;&#20110;&#25903;&#25345;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#27599;&#20010;&#24320;&#25918;Code LLM&#65292;&#24182;&#19982;OpenAI code-cushman-001&#27169;&#22411;&#30456;&#21305;&#37197;&#25110;&#20248;&#20110;&#35813;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;StarCoder&#22312;Python&#19978;&#20063;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#65292;&#33021;&#22815;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#33719;&#24471;40\%&#30340;pass@1&#30340;&#24471;&#20998;&#65292;&#24182;&#20173;&#28982;&#20445;&#25345;&#20854;&#22312;&#20854;&#20182;&#31243;&#24207;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\% pass@1 on HumanEval, and still retains its performance on other program
&lt;/p&gt;</description></item><item><title>Structure-CLIP&#20351;&#29992;&#25991;&#26412;&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20351;&#29992;&#22330;&#26223;&#22270;&#24378;&#21270;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06152</link><description>&lt;p&gt;
Structure-CLIP: &#32467;&#21512;&#32467;&#26500;&#30693;&#35782;&#20248;&#21270;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Structure-CLIP: Enhance Multi-modal Language Representations with Structure Knowledge. (arXiv:2305.06152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06152
&lt;/p&gt;
&lt;p&gt;
Structure-CLIP&#20351;&#29992;&#25991;&#26412;&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20351;&#29992;&#22330;&#26223;&#22270;&#24378;&#21270;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#38656;&#35201;&#23545;&#25991;&#26412;&#36827;&#34892;&#35814;&#32454;&#35821;&#20041;&#29702;&#35299;&#30340;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#19978;&#36890;&#24120;&#34920;&#29616;&#36739;&#24046;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#19968;&#20123;&#30740;&#31350;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#21477;&#23376;&#20013;&#23384;&#22312;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#26469;&#22686;&#24378;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26694;&#26550;Structure-CLIP&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#38544;&#24335;&#35814;&#32454;&#35821;&#20041;&#65292;&#20197;&#22686;&#24378;&#31934;&#32454;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;(1)&#25105;&#20204;&#20351;&#29992;&#22330;&#26223;&#22270;&#26469;&#26356;&#21152;&#20851;&#27880;&#25991;&#26412;&#20013;&#30340;&#35814;&#32454;&#35821;&#20041;&#23398;&#20064;&#65292;&#24182;&#20805;&#20998;&#25506;&#32034;&#32454;&#31890;&#24230;&#35821;&#20041;&#20043;&#38388;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;(2)&#25105;&#20204;&#32467;&#21512;&#22330;&#26223;&#22270;&#30340;&#30693;&#35782;&#24378;&#21270;&#26694;&#26550;&#26469;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale vision-language pre-training has shown promising advances on various downstream tasks and achieved significant performance in multi-modal understanding and generation tasks. However, existing methods often perform poorly on image-text matching tasks that require a detailed semantics understanding of the text. Although there have been some works on this problem, they do not sufficiently exploit the structural knowledge present in sentences to enhance multi-modal language representations, which leads to poor performance. In this paper, we present an end-to-end framework Structure-CLIP, which integrates latent detailed semantics from the text to enhance fine-grained semantic representations. Specifically, (1) we use scene graphs in order to pay more attention to the detailed semantic learning in the text and fully explore structured knowledge between fine-grained semantics, and (2) we utilize the knowledge-enhanced framework with the help of the scene graph to make full use of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#32534;&#31243;&#35821;&#35328;&#24182;&#36890;&#36807;&#23398;&#20064;&#32534;&#31243;&#26041;&#27861;&#35753;&#22823;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31243;&#24207;&#24182;&#25351;&#23548;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35299;&#20915;&#32534;&#31243;&#20219;&#21153;&#19978;&#27604;&#22522;&#32447;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.10464</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning to Program with Natural Language. (arXiv:2304.10464v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#32534;&#31243;&#35821;&#35328;&#24182;&#36890;&#36807;&#23398;&#20064;&#32534;&#31243;&#26041;&#27861;&#35753;&#22823;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31243;&#24207;&#24182;&#25351;&#23548;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35299;&#20915;&#32534;&#31243;&#20219;&#21153;&#19978;&#27604;&#22522;&#32447;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#26412;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#36825;&#24341;&#36215;&#20102;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#24076;&#26395;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#65292;&#25105;&#20204;&#38656;&#35201;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32534;&#31243;&#65292;&#28982;&#21518;&#25353;&#29031;&#31243;&#24207;&#29983;&#25104;&#29305;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#32534;&#31243;&#35821;&#35328;&#26469;&#25551;&#36848;&#20219;&#21153;&#36807;&#31243;&#65292;&#20351;&#23427;&#20204;&#26131;&#20110;&#20154;&#31867;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#12290;&#34429;&#28982;&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31243;&#24207;&#65292;&#20294;&#36825;&#20123;&#31243;&#24207;&#21487;&#33021;&#20173;&#28982;&#23384;&#22312;&#38169;&#35823;&#25110;&#19981;&#23436;&#25972;&#30340;&#27493;&#39588;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#23398;&#20064;&#32534;&#31243;&#65288;LP&#65289;&#30340;&#26041;&#27861;&#65292;&#35201;&#27714;&#22823;&#35821;&#35328;&#27169;&#22411;&#20174;&#22797;&#26434;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#31243;&#24207;&#65292;&#28982;&#21518;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#31243;&#24207;&#26469;&#25351;&#23548;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;AMPS&#65288;&#39640;&#20013;&#25968;&#23398;&#65289;&#21644;Math&#65288;&#31454;&#36187;&#25968;&#23398;&#38382;&#39064;&#65289;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#27979;&#35797;ChatGP&#35299;&#20915;&#32534;&#31243;&#20219;&#21153;&#26102;&#65292;LP&#33021;&#22815;&#23454;&#29616;80%&#30340;&#25104;&#21151;&#29575;&#65292;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable performance in various basic natural language tasks, which raises hopes for achieving Artificial General Intelligence. To better complete complex tasks, we need LLMs to program for the task and then follow the program to generate a specific solution for the test sample. We propose using natural language as a new programming language to describe task procedures, making them easily understandable to both humans and LLMs. LLMs are capable of directly generating natural language programs, but these programs may still contain factual errors or incomplete steps. Therefore, we further propose the Learning to Program (LP) method to ask LLMs themselves to learn natural language programs from the training dataset of complex tasks and then use the learned program to guide inference. Our experiments on the AMPS (high school math) and Math (competition mathematics problems) datasets demonstrate the effectiveness of our approach. When testing ChatGP
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;USNID&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#30340;&#26032;&#24847;&#22270;&#21457;&#29616;&#65292;&#35299;&#20915;&#20102;&#21033;&#29992;&#26377;&#38480;&#25110;&#26080;&#26631;&#35760;&#25968;&#25454;&#26102;&#38590;&#20197;&#25429;&#25417;&#22797;&#26434;&#35821;&#20041;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#32858;&#31867;&#26426;&#21046;&#26469;&#25552;&#39640;&#33258;&#25105;&#30417;&#30563;&#30446;&#26631;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#21457;&#29616;&#32454;&#31890;&#24230;&#30340;&#24847;&#22270;&#31751;&#12290;</title><link>http://arxiv.org/abs/2304.07699</link><description>&lt;p&gt;
USNID: &#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#26032;&#24847;&#22270;&#21457;&#29616;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
USNID: A Framework for Unsupervised and Semi-supervised New Intent Discovery. (arXiv:2304.07699v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;USNID&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#30340;&#26032;&#24847;&#22270;&#21457;&#29616;&#65292;&#35299;&#20915;&#20102;&#21033;&#29992;&#26377;&#38480;&#25110;&#26080;&#26631;&#35760;&#25968;&#25454;&#26102;&#38590;&#20197;&#25429;&#25417;&#22797;&#26434;&#35821;&#20041;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#32858;&#31867;&#26426;&#21046;&#26469;&#25552;&#39640;&#33258;&#25105;&#30417;&#30563;&#30446;&#26631;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#21457;&#29616;&#32454;&#31890;&#24230;&#30340;&#24847;&#22270;&#31751;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#24847;&#22270;&#21457;&#29616;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#20351;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#38656;&#27714;&#24182;&#25552;&#20379;&#21451;&#22909;&#30340;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#26377;&#38480;&#25110;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#25429;&#25417;&#31163;&#25955;&#25991;&#26412;&#34920;&#31034;&#30340;&#22797;&#26434;&#35821;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;USNID&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#26032;&#24847;&#22270;&#21457;&#29616;&#65292;&#20855;&#26377;&#19977;&#20010;&#20851;&#38190;&#25216;&#26415;&#65306;&#20805;&#20998;&#21033;&#29992;&#26080;&#30417;&#30563;&#25110;&#21322;&#30417;&#30563;&#25968;&#25454;&#25366;&#25496;&#27973;&#23618;&#35821;&#20041;&#30456;&#20284;&#24615;&#20851;&#31995;&#65307;&#35774;&#35745;&#32858;&#31867;&#26426;&#21046;&#35299;&#20915;&#31751;&#20998;&#37197;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65307;&#25429;&#33719;&#26080;&#30417;&#30563;&#25110;&#21322;&#30417;&#30563;&#25968;&#25454;&#20013;&#30340;&#39640;&#32423;&#35821;&#20041;&#65292;&#36890;&#36807;&#21516;&#26102;&#20248;&#21270;&#32858;&#31867;&#21644;&#33258;&#25105;&#30417;&#30563;&#26469;&#21457;&#29616;&#32454;&#31890;&#24230;&#30340;&#24847;&#22270;&#31751;&#12290;
&lt;/p&gt;
&lt;p&gt;
New intent discovery is of great value to natural language processing, allowing for a better understanding of user needs and providing friendly services. However, most existing methods struggle to capture the complicated semantics of discrete text representations when limited or no prior knowledge of labeled data is available. To tackle this problem, we propose a novel framework called USNID for unsupervised and semi-supervised new intent discovery, which has three key technologies. First, it takes full use of unsupervised or semi-supervised data to mine shallow semantic similarity relations and provide well-initialized representations for clustering. Second, it designs a centroid-guided clustering mechanism to address the issue of cluster allocation inconsistency and provide high-quality self-supervised targets for representation learning. Third, it captures high-level semantics in unsupervised or semi-supervised data to discover fine-grained intent-wise clusters by optimizing both cl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(GPT&#21644;BERT)&#35782;&#21035;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#24615;&#33021;, &#32467;&#26524;&#26174;&#31034;BERT&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#20854;&#20013;PubMedBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#31934;&#24230;&#21644;F1&#20998;&#25968;&#65292;BioM-ALBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.17728</link><description>&lt;p&gt;
&#22522;&#20110;GPT&#21644;BERT&#30340;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#37492;&#23450;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of GPT and BERT-based models on identifying protein-protein interactions in biomedical text. (arXiv:2303.17728v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17728
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(GPT&#21644;BERT)&#35782;&#21035;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#24615;&#33021;, &#32467;&#26524;&#26174;&#31034;BERT&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#20854;&#20013;PubMedBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#31934;&#24230;&#21644;F1&#20998;&#25968;&#65292;BioM-ALBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;(PPIs)&#23545;&#20110;&#29702;&#35299;&#36951;&#20256;&#26426;&#21046;&#12289;&#30142;&#30149;&#21457;&#30149;&#26426;&#29702;&#21644;&#33647;&#29289;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#21644;&#20934;&#30830;&#25552;&#21462;PPIs&#20197;&#20419;&#36827;&#31185;&#23398;&#30693;&#35782;&#30340;&#21457;&#25496;&#12290;&#24050;&#32463;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;(GPT)&#21644;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#21464;&#21387;&#22120;(BERT)&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#25163;&#21160;&#32534;&#21046;&#30340;LLL&#22522;&#20934;&#35821;&#26009;&#24211;&#35780;&#20272;&#20102;&#21508;&#31181;GPT&#21644;BERT&#27169;&#22411;&#30340;PPI&#35782;&#21035;&#24615;&#33021;&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;77&#20010;&#21477;&#23376;&#20013;&#30340;164&#20010;PPIs&#12290;BERT&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;PubMedBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#31934;&#24230;(85.17%)&#21644;F1&#20998;&#25968;(86.47%)&#65292;BioM-ALBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#21484;&#22238;&#29575;(93.83%)&#12290;&#23613;&#31649;GPT-4&#27809;&#26377;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#20854;&#24615;&#33021;&#21487;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting protein-protein interactions (PPIs) is crucial for understanding genetic mechanisms, disease pathogenesis, and drug design. However, with the fast-paced growth of biomedical literature, there is a growing need for automated and accurate extraction of PPIs to facilitate scientific knowledge discovery. Pre-trained language models, such as generative pre-trained transformer (GPT) and bidirectional encoder representations from transformers (BERT), have shown promising results in natural language processing (NLP) tasks. We evaluated the PPI identification performance of various GPT and BERT models using a manually curated benchmark corpus of 164 PPIs in 77 sentences from learning language in logic (LLL). BERT-based models achieved the best overall performance, with PubMedBERT achieving the highest precision (85.17%) and F1-score (86.47%) and BioM-ALBERT achieving the highest recall (93.83%). Despite not being explicitly trained for biomedical texts, GPT-4 achieved comparable perfo
&lt;/p&gt;</description></item></channel></rss>