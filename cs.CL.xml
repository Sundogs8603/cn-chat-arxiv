<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#21270;&#30340;&#21518;&#26399;&#20132;&#20114;&#22810;&#27169;&#26816;&#32034;&#26041;&#27861;&#65288;FLMR&#65289;&#26469;&#25913;&#36827;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#30693;&#35782;&#26816;&#32034;&#12290;FLMR&#36890;&#36807;&#33719;&#21462;&#34917;&#20805;&#30340;&#22270;&#20687;&#34920;&#31034;&#24182;&#20351;&#29992;&#19982;&#29616;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#30456;&#23545;&#40784;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;RA-VQA&#20013;&#26816;&#32034;&#22120;&#30340;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.17133</link><description>&lt;p&gt;
&#31934;&#32454;&#21270;&#30340;&#21518;&#26399;&#20132;&#20114;&#22810;&#27169;&#26816;&#32034;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering. (arXiv:2309.17133v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#21270;&#30340;&#21518;&#26399;&#20132;&#20114;&#22810;&#27169;&#26816;&#32034;&#26041;&#27861;&#65288;FLMR&#65289;&#26469;&#25913;&#36827;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#30693;&#35782;&#26816;&#32034;&#12290;FLMR&#36890;&#36807;&#33719;&#21462;&#34917;&#20805;&#30340;&#22270;&#20687;&#34920;&#31034;&#24182;&#20351;&#29992;&#19982;&#29616;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#30456;&#23545;&#40784;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;RA-VQA&#20013;&#26816;&#32034;&#22120;&#30340;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;KB-VQA&#65289;&#35201;&#27714;VQA&#31995;&#32479;&#21033;&#29992;&#29616;&#26377;&#30693;&#35782;&#24211;&#20013;&#30340;&#30693;&#35782;&#26469;&#22238;&#31572;&#19982;&#35270;&#35273;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;RA-VQA&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;KB-VQA&#38382;&#39064;&#65292;&#39318;&#20808;&#20351;&#29992;&#23494;&#38598;&#27573;&#33853;&#26816;&#32034;&#65288;DPR&#65289;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#25991;&#26723;&#22238;&#31572;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31934;&#32454;&#21270;&#30340;&#21518;&#26399;&#20132;&#20114;&#22810;&#27169;&#26816;&#32034;&#65288;FLMR&#65289;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;RA-VQA&#20013;&#30340;&#30693;&#35782;&#26816;&#32034;&#12290;FLMR&#35299;&#20915;&#20102;RA-VQA&#26816;&#32034;&#22120;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;&#65288;1&#65289;&#36890;&#36807;&#22270;&#20687;&#21040;&#25991;&#26412;&#36716;&#25442;&#33719;&#24471;&#30340;&#22270;&#20687;&#34920;&#31034;&#21487;&#33021;&#19981;&#23436;&#25972;&#21644;&#19981;&#20934;&#30830;&#65292;&#65288;2&#65289;&#26597;&#35810;&#21644;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#26159;&#36890;&#36807;&#19968;&#32500;&#23884;&#20837;&#35745;&#31639;&#30340;&#65292;&#21487;&#33021;&#23545;&#26356;&#32454;&#31890;&#24230;&#30340;&#30456;&#20851;&#24615;&#19981;&#25935;&#24863;&#12290;FLMR&#36890;&#36807;&#20351;&#29992;&#19982;&#29616;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#30456;&#23545;&#40784;&#30340;&#35270;&#35273;&#27169;&#22411;&#33719;&#21462;&#34917;&#20805;&#22270;&#20687;&#34920;&#31034;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-based Visual Question Answering (KB-VQA) requires VQA systems to utilize knowledge from existing knowledge bases to answer visually-grounded questions. Retrieval-Augmented Visual Question Answering (RA-VQA), a strong framework to tackle KB-VQA, first retrieves related documents with Dense Passage Retrieval (DPR) and then uses them to answer questions. This paper proposes Fine-grained Late-interaction Multi-modal Retrieval (FLMR) which significantly improves knowledge retrieval in RA-VQA. FLMR addresses two major limitations in RA-VQA's retriever: (1) the image representations obtained via image-to-text transforms can be incomplete and inaccurate and (2) relevance scores between queries and documents are computed with one-dimensional embeddings, which can be insensitive to finer-grained relevance. FLMR overcomes these limitations by obtaining image representations that complement those from the image-to-text transforms using a vision model aligned with an existing text-based r
&lt;/p&gt;</description></item><item><title>&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#26159;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;&#33521;&#22269;&#30340;&#36229;&#36807;250,000&#20010;&#27861;&#24237;&#26696;&#20363;&#12290;&#22312;&#35813;&#35821;&#26009;&#24211;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26696;&#20363;&#32467;&#26524;&#30340;&#19987;&#23478;&#27880;&#35299;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#20102;&#26696;&#20363;&#32467;&#26524;&#25552;&#21462;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.12269</link><description>&lt;p&gt;
&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#65306;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
The Cambridge Law Corpus: A Corpus for Legal AI Research. (arXiv:2309.12269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12269
&lt;/p&gt;
&lt;p&gt;
&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#26159;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;&#33521;&#22269;&#30340;&#36229;&#36807;250,000&#20010;&#27861;&#24237;&#26696;&#20363;&#12290;&#22312;&#35813;&#35821;&#26009;&#24211;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26696;&#20363;&#32467;&#26524;&#30340;&#19987;&#23478;&#27880;&#35299;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#20102;&#26696;&#20363;&#32467;&#26524;&#25552;&#21462;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#65288;CLC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;&#12290;&#23427;&#21253;&#21547;&#20102;&#26469;&#33258;&#33521;&#22269;&#30340;&#36229;&#36807;250,000&#20010;&#27861;&#24237;&#26696;&#20363;&#12290;&#22823;&#37096;&#20998;&#26696;&#20363;&#26469;&#33258;21&#19990;&#32426;&#65292;&#20294;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;&#20102;16&#19990;&#32426;&#20197;&#26469;&#30340;&#26696;&#20363;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#35813;&#35821;&#26009;&#24211;&#30340;&#39318;&#27425;&#21457;&#24067;&#65292;&#21253;&#25324;&#21407;&#22987;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#12290;&#22312;&#35821;&#26009;&#24211;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;638&#20010;&#26696;&#20363;&#30340;&#27861;&#24459;&#19987;&#23478;&#23545;&#26696;&#20363;&#32467;&#26524;&#30340;&#27880;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;GPT-3&#12289;GPT-4&#21644;RoBERTa&#27169;&#22411;&#36827;&#34892;&#26696;&#20363;&#32467;&#26524;&#25552;&#21462;&#65292;&#20197;&#25552;&#20379;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27861;&#24459;&#21644;&#20262;&#29702;&#35752;&#35770;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#26448;&#26009;&#21487;&#33021;&#20855;&#26377;&#25935;&#24863;&#24615;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#35813;&#35821;&#26009;&#24211;&#21482;&#20250;&#22312;&#19968;&#23450;&#38480;&#21046;&#19979;&#29992;&#20110;&#30740;&#31350;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Cambridge Law Corpus (CLC), a corpus for legal AI research. It consists of over 250 000 court cases from the UK. Most cases are from the 21st century, but the corpus includes cases as old as the 16th century. This paper presents the first release of the corpus, containing the raw text and meta-data. Together with the corpus, we provide annotations on case outcomes for 638 cases, done by legal experts. Using our annotated data, we have trained and evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to provide benchmarks. We include an extensive legal and ethical discussion to address the potentially sensitive nature of this material. As a consequence, the corpus will only be released for research purposes under certain restrictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#39537;&#21160;&#30340;&#24605;&#32500;&#38142;&#65288;KD-CoT&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#20462;&#25913;LLMs&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#36890;&#36807;&#19982;&#22806;&#37096;&#30693;&#35782;&#30340;&#20132;&#20114;&#26469;&#35299;&#20915;&#24187;&#35273;&#21644;&#38169;&#35823;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13259</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;CoT&#65306;&#25506;&#32034;LLMs&#20013;&#23545;&#30693;&#35782;&#23494;&#38598;&#22411;&#38382;&#31572;&#36827;&#34892;&#24544;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering. (arXiv:2308.13259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#39537;&#21160;&#30340;&#24605;&#32500;&#38142;&#65288;KD-CoT&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#20462;&#25913;LLMs&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#36890;&#36807;&#19982;&#22806;&#37096;&#30693;&#35782;&#30340;&#20132;&#20114;&#26469;&#35299;&#20915;&#24187;&#35273;&#21644;&#38169;&#35823;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37197;&#22791;&#20102;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#24187;&#35273;&#21644;&#26080;&#27861;&#35775;&#38382;&#22806;&#37096;&#30693;&#35782;&#65292;LLMs&#22312;&#23545;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#22914;&#30693;&#35782;&#24211;&#38382;&#31572;&#65289;&#36827;&#34892;&#25512;&#29702;&#26102;&#24120;&#24120;&#20250;&#20135;&#29983;&#19981;&#27491;&#30830;&#25110;&#19981;&#24544;&#23454;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#39537;&#21160;&#30340;&#24605;&#32500;&#38142;&#65288;KD-CoT&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#22806;&#37096;&#30693;&#35782;&#30340;&#20132;&#20114;&#26469;&#39564;&#35777;&#21644;&#20462;&#25913;CoT&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#20811;&#26381;&#24187;&#35273;&#21644;&#38169;&#35823;&#20256;&#25773;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;LLMs&#30340;CoT&#25512;&#29702;&#36807;&#31243;&#35268;&#33539;&#21270;&#20026;&#32467;&#26500;&#21270;&#30340;&#22810;&#36718;&#38382;&#31572;&#26684;&#24335;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;LLMs&#19982;&#19968;&#20010;&#38382;&#31572;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#65292;&#35813;&#31995;&#32479;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#24182;&#22522;&#20110;&#26816;&#32034;&#21040;&#30340;&#20934;&#30830;&#31572;&#26696;&#20135;&#29983;&#24544;&#23454;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;KBQA CoT&#38598;&#21512;&#20419;&#36827;&#20102;LLMs&#30340;&#32467;&#26500;&#21270;CoT&#25512;&#29702;&#65292;&#23427;&#20316;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown impressive reasoning ability in various downstream tasks. Even so, suffering from hallucinations and the inability to access external knowledge, LLMs often come with incorrect or unfaithful intermediate reasoning steps, especially in the context of answering knowledge-intensive tasks such as KBQA. To alleviate this issue, we propose a framework called Knowledge-Driven Chain-of-Thought (KD-CoT) to verify and modify reasoning traces in CoT via interaction with external knowledge, and thus overcome the hallucinations and error propagation. Concretely, we formulate the CoT rationale process of LLMs into a structured multi-round QA format. In each round, LLMs interact with a QA system that retrieves external knowledge and produce faithful reasoning traces based on retrieved precise answers. The structured CoT reasoning of LLMs is facilitated by our developed KBQA CoT collection, which serves as in-context learning
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;Electra Transformer&#12289;GloVe&#21644;LSTM&#27169;&#22411;&#30340;&#21019;&#26032;&#38382;&#39064;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;TREC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#34701;&#21512;&#19981;&#21516;&#25216;&#26415;&#21487;&#20197;&#33719;&#24471;&#26356;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.06828</link><description>&lt;p&gt;
&#38382;&#39064;&#20998;&#31867;&#30340;&#38598;&#25104;&#26041;&#27861;&#65306;&#34701;&#21512;Electra Transformer&#12289;GloVe&#21644;LSTM
&lt;/p&gt;
&lt;p&gt;
An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM. (arXiv:2308.06828v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;Electra Transformer&#12289;GloVe&#21644;LSTM&#27169;&#22411;&#30340;&#21019;&#26032;&#38382;&#39064;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;TREC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#34701;&#21512;&#19981;&#21516;&#25216;&#26415;&#21487;&#20197;&#33719;&#24471;&#26356;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24050;&#32463;&#25104;&#20026;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#23427;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#39064;&#20998;&#31867;&#26041;&#38754;&#12290;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#23376;&#39046;&#22495;&#65292;&#38382;&#39064;&#20998;&#31867;&#19987;&#27880;&#20110;&#30830;&#23450;&#25152;&#38656;&#20449;&#24687;&#30340;&#31867;&#22411;&#65292;&#36825;&#26159;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#31561;&#19979;&#28216;&#24212;&#29992;&#30340;&#22522;&#26412;&#27493;&#39588;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38382;&#39064;&#20998;&#31867;&#38598;&#25104;&#26041;&#27861;&#65292;&#23558;Electra&#12289;GloVe&#21644;LSTM&#27169;&#22411;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#12290;&#35813;&#27169;&#22411;&#22312;&#33879;&#21517;&#30340;TREC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#25972;&#21512;&#36825;&#20123;&#19981;&#21516;&#25216;&#26415;&#21487;&#20197;&#24471;&#21040;&#26356;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;Electra&#25552;&#20379;&#20102;&#22522;&#20110;transformer&#30340;&#22797;&#26434;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;GloVe&#25552;&#20379;&#20102;&#20840;&#23616;&#21521;&#37327;&#34920;&#31034;&#20197;&#25429;&#25417;&#35789;&#32423;&#35821;&#20041;&#65292;LSTM&#21017;&#36129;&#29486;&#20102;&#24207;&#21015;&#23398;&#20064;&#33021;&#21147;&#20197;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) has emerged as a crucial technology for understanding and generating human language, playing an essential role in tasks such as machine translation, sentiment analysis, and more pertinently, question classification. As a subfield within NLP, question classification focuses on determining the type of information being sought, a fundamental step for downstream applications like question answering systems. This study presents an innovative ensemble approach for question classification, combining the strengths of Electra, GloVe, and LSTM models. Rigorously tested on the well-regarded TREC dataset, the model demonstrates how the integration of these disparate technologies can lead to superior results. Electra brings in its transformer-based capabilities for complex language understanding, GloVe offers global vector representations for capturing word-level semantics, and LSTM contributes its sequence learning abilities to model long-term dependencies. By fus
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#26469;&#22635;&#20805;&#36890;&#29992;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.06985</link><description>&lt;p&gt;
&#36808;&#21521;&#22635;&#20805;&#36890;&#29992;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Populating Generalizable Engineering Design Knowledge. (arXiv:2307.06985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06985
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#26469;&#22635;&#20805;&#36890;&#29992;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22635;&#20805;&#36890;&#29992;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;head entity :: relationship :: tail entity&#24418;&#24335;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#20107;&#23454;&#21487;&#20197;&#22312;&#19987;&#21033;&#25991;&#20214;&#20869;&#37096;&#21644;&#36328;&#25991;&#20214;&#20043;&#38388;&#32452;&#21512;&#24418;&#25104;&#30693;&#35782;&#22270;&#65292;&#29992;&#20316;&#34920;&#31034;&#21644;&#23384;&#20648;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#24037;&#31243;&#35774;&#35745;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#20851;&#31995;&#26469;&#22635;&#20805;&#32479;&#35745;&#36817;&#20284;&#32780;&#38750;&#20107;&#23454;&#30340;&#19977;&#20803;&#32452;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#26631;&#35760;&#22120;&#26469;&#35782;&#21035;&#21477;&#23376;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;&#22312;&#30830;&#23450;&#20102;&#19968;&#23545;&#23454;&#20307;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#21478;&#19968;&#20010;&#26631;&#35760;&#22120;&#26469;&#35782;&#21035;&#29305;&#23450;&#34920;&#31034;&#36825;&#23545;&#23454;&#20307;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#31995;&#26631;&#35760;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#20123;&#26631;&#35760;&#22120;&#65292;&#25105;&#20204;&#25163;&#21160;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;44,227&#20010;&#21477;&#23376;&#21644;&#30456;&#24212;&#20107;&#23454;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#23558;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#36890;&#24120;&#25512;&#33616;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20854;&#20013;&#25105;&#20204;&#39044;.
&lt;/p&gt;
&lt;p&gt;
Aiming to populate generalizable engineering design knowledge, we propose a method to extract facts of the form head entity :: relationship :: tail entity from sentences found in patent documents. These facts could be combined within and across patent documents to form knowledge graphs that serve as schemes for representing as well as storing design knowledge. Existing methods in engineering design literature often utilise a set of predefined relationships to populate triples that are statistical approximations rather than facts. In our method, we train a tagger to identify both entities and relationships from a sentence. Given a pair of entities thus identified, we train another tagger to identify the relationship tokens that specifically denote the relationship between the pair. For training these taggers, we manually construct a dataset of 44,227 sentences and corresponding facts. We also compare the performance of the method against typically recommended approaches, wherein, we pre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;</title><link>http://arxiv.org/abs/2307.06775</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;&#19982;&#24179;&#21488;&#26080;&#20851;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#39278;&#39135;&#32010;&#20081;&#30340;&#35786;&#26029;&#21644;&#19982;&#20043;&#30456;&#20851;&#30340;&#27515;&#20129;&#25968;&#37327;&#22823;&#24133;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#20896;&#30123;&#24773;&#26399;&#38388;&#12290;&#36825;&#31181;&#24040;&#22823;&#22686;&#38271;&#37096;&#20998;&#26469;&#28304;&#20110;&#30123;&#24773;&#30340;&#21387;&#21147;&#65292;&#20294;&#20063;&#19982;&#31038;&#20132;&#23186;&#20307;&#30340;&#26292;&#38706;&#22686;&#21152;&#26377;&#20851;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#20805;&#26021;&#30528;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#20869;&#23481;&#21487;&#20197;&#35825;&#21457;&#35266;&#30475;&#32773;&#30340;&#39278;&#39135;&#32010;&#20081;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#22522;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#32452;&#21512;&#21028;&#26029;&#32473;&#23450;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26159;&#21542;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#12290;&#20174;Twitter&#25910;&#38598;&#20102;&#19968;&#20010;&#24102;&#26377;&#26631;&#31614;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#21313;&#20108;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#26681;&#25454;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26368;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;RoBERTa&#21644;MaxViT&#34701;&#21512;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. Such content can induce eating disorders in viewers. This study aimed to create a multimodal deep learning model capable of determining whether a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#32447;&#24615;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#33021;&#21147;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#24314;&#27169;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2307.06290</link><description>&lt;p&gt;
&#25351;&#20196;&#25366;&#25496;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Instruction Mining: High-Quality Instruction Data Selection for Large Language Models. (arXiv:2307.06290v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#32447;&#24615;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#33021;&#21147;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#24314;&#27169;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#32463;&#21382;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#12290;&#23613;&#31649;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#36171;&#20104;&#27169;&#22411;&#24378;&#22823;&#30340;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#22238;&#24212;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#26377;&#26102;&#20173;&#28982;&#26080;&#27861;&#29702;&#35299;&#20154;&#31867;&#25351;&#20196;&#12290;&#20026;&#20102;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#25351;&#20196;&#24494;&#35843;&#24050;&#25104;&#20026;&#35813;&#39046;&#22495;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#20173;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#36136;&#37327;&#30340;&#32447;&#24615;&#35268;&#21017;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#20307;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#26469;&#36827;&#34892;InstructMining&#30340;&#24314;&#27169;&#12290;&#20026;&#20102;&#30740;&#31350;&#25968;&#25454;&#36136;&#37327;&#19982;&#36825;&#20123;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32454;&#33268;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models typically undergo two training stages, pretraining and finetuning. Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times. To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area. Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data. However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow. In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality. We formulate InstructMining using specific natural language indicators. To investigate the relationship between data quality and these indicators, we further conduct extensive fine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BeaverTails&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;LLM&#30340;&#23433;&#20840;&#23545;&#40784;&#12290;&#35813;&#25968;&#25454;&#38598;&#20998;&#24320;&#27880;&#37322;&#20102;&#38382;&#31572;&#23545;&#30340;&#26377;&#29992;&#24615;&#21644;&#26080;&#23475;&#24615;&#65292;&#20026;&#23433;&#20840;&#24320;&#21457;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2307.04657</link><description>&lt;p&gt;
BeaverTails&#65306;&#36890;&#36807;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#38598;&#25913;&#21892;LLM&#30340;&#23433;&#20840;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset. (arXiv:2307.04657v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BeaverTails&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;LLM&#30340;&#23433;&#20840;&#23545;&#40784;&#12290;&#35813;&#25968;&#25454;&#38598;&#20998;&#24320;&#27880;&#37322;&#20102;&#38382;&#31572;&#23545;&#30340;&#26377;&#29992;&#24615;&#21644;&#26080;&#23475;&#24615;&#65292;&#20026;&#23433;&#20840;&#24320;&#21457;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;BeaverTails&#8221;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#23545;&#40784;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#29420;&#29305;&#22320;&#23545;&#38382;&#31572;&#23545;&#30340;&#26377;&#29992;&#24615;&#21644;&#26080;&#23475;&#24615;&#36827;&#34892;&#20102;&#20998;&#24320;&#27880;&#37322;&#65292;&#20174;&#32780;&#20026;&#36825;&#20123;&#20851;&#38190;&#23646;&#24615;&#25552;&#20379;&#20102;&#19981;&#21516;&#30340;&#35266;&#28857;&#12290;&#24635;&#20849;&#65292;&#25105;&#20204;&#20026;30,207&#20010;&#38382;&#31572;&#23545;&#21644;30,144&#23545;&#19987;&#23478;&#27604;&#36739;&#25968;&#25454;&#25910;&#38598;&#20102;&#23433;&#20840;&#20803;&#26631;&#31614;&#65292;&#29992;&#20110;&#34913;&#37327;&#26377;&#29992;&#24615;&#21644;&#26080;&#23475;&#24615;&#25351;&#26631;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;BeaverTails&#22312;&#20869;&#23481;&#31649;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20854;&#22312;LLM&#20013;&#23454;&#26045;&#23454;&#38469;&#23433;&#20840;&#25514;&#26045;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20010;&#25968;&#25454;&#38598;&#20026;&#31038;&#21306;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#65292;&#20026;LLM&#30340;&#23433;&#20840;&#24320;&#21457;&#21644;&#37096;&#32626;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the \textsc{BeaverTails} dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 30,207 question-answer (QA) pairs and 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;SPAE&#65292;&#20351;&#29992;&#35821;&#20041;&#37329;&#23383;&#22612;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#20923;&#32467;LLM&#25191;&#34892;&#28041;&#21450;&#38750;&#35821;&#35328;&#27169;&#24577;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#22270;&#20687;&#36716;&#21270;&#20026;LLM&#21487;&#29702;&#35299;&#30340;&#35789;&#27719;&#26631;&#35760;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#25552;&#21319;&#20102;&#20923;&#32467;LLM&#22312;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;25%&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2306.17842</link><description>&lt;p&gt;
SPAE: &#22522;&#20110;&#35821;&#20041;&#37329;&#23383;&#22612;&#33258;&#32534;&#30721;&#22120;&#30340;&#20923;&#32467;LLM&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs. (arXiv:2306.17842v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;SPAE&#65292;&#20351;&#29992;&#35821;&#20041;&#37329;&#23383;&#22612;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#20923;&#32467;LLM&#25191;&#34892;&#28041;&#21450;&#38750;&#35821;&#35328;&#27169;&#24577;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#22270;&#20687;&#36716;&#21270;&#20026;LLM&#21487;&#29702;&#35299;&#30340;&#35789;&#27719;&#26631;&#35760;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#25552;&#21319;&#20102;&#20923;&#32467;LLM&#22312;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;25%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Semantic Pyramid AutoEncoder (SPAE)&#65292;&#20351;&#20923;&#32467;&#30340;LLM&#33021;&#22815;&#25191;&#34892;&#28041;&#21450;&#38750;&#35821;&#35328;&#27169;&#24577;&#65288;&#22914;&#22270;&#20687;&#25110;&#35270;&#39057;&#65289;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;SPAE&#22312;&#21407;&#22987;&#20687;&#32032;&#21644;&#20174;LLM&#35789;&#27719;&#34920;&#20013;&#25552;&#21462;&#30340;&#21487;&#35299;&#37322;&#30340;&#35789;&#27719;&#26631;&#35760;&#65288;&#25110;&#21333;&#35789;&#65289;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;&#29983;&#25104;&#30340;&#26631;&#35760;&#25429;&#25417;&#20102;&#35270;&#35273;&#37325;&#24314;&#25152;&#38656;&#30340;&#35821;&#20041;&#21547;&#20041;&#21644;&#32454;&#31890;&#24230;&#32454;&#33410;&#65292;&#23558;&#35270;&#35273;&#20869;&#23481;&#36716;&#21270;&#20026;LLM&#33021;&#29702;&#35299;&#30340;&#35821;&#35328;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#25191;&#34892;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#65292;&#19982;&#20923;&#32467;&#30340;PaLM 2&#21644;GPT 3.5&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#39564;&#35777;&#23454;&#12290;&#22312;&#30456;&#21516;&#30340;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#25104;&#21151;&#20351;&#20923;&#32467;LLM&#29983;&#25104;&#22270;&#20687;&#20869;&#23481;&#65292;&#24182;&#22312;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36229;&#36807;&#29616;&#26377;&#25216;&#26415;25%&#20197;&#19978;&#30340;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#26469;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#27880;&#20837;&#30340;&#26041;&#27861;AutoPoison&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#27602;&#21270;&#65292;&#23545;&#25163;&#33021;&#22815;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.17194</link><description>&lt;p&gt;
&#20851;&#20110;&#25351;&#20196;&#35843;&#25972;&#30340;&#21487;&#21033;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Exploitability of Instruction Tuning. (arXiv:2306.17194v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17194
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#26469;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#27880;&#20837;&#30340;&#26041;&#27861;AutoPoison&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#27602;&#21270;&#65292;&#23545;&#25163;&#33021;&#22815;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#26159;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#30340;&#26377;&#25928;&#25216;&#26415;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#23545;&#25163;&#22914;&#20309;&#36890;&#36807;&#21521;&#35757;&#32451;&#25968;&#25454;&#27880;&#20837;&#29305;&#23450;&#30340;&#25351;&#20196;&#36319;&#38543;&#31034;&#20363;&#26469;&#21033;&#29992;&#25351;&#20196;&#35843;&#25972;&#65292;&#20174;&#32780;&#26377;&#24847;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#20363;&#22914;&#65292;&#23545;&#25163;&#21487;&#20197;&#36890;&#36807;&#27880;&#20837;&#25552;&#21450;&#30446;&#26631;&#20869;&#23481;&#30340;&#35757;&#32451;&#31034;&#20363;&#65292;&#24182;&#24341;&#35825;&#19979;&#28216;&#27169;&#22411;&#23637;&#31034;&#27492;&#31867;&#34892;&#20026;&#26469;&#23454;&#29616;&#20869;&#23481;&#27880;&#20837;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25968;&#25454;&#27880;&#20837;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;AutoPoison&#12290;&#23427;&#20351;&#29992;&#20102;&#19968;&#20010;&#39044;&#35328;&#27169;&#22411;&#26469;&#23558;&#22810;&#26679;&#25915;&#20987;&#30446;&#26631;&#33258;&#28982;&#32780;&#36830;&#36143;&#22320;&#27880;&#20837;&#21040;&#27602;&#21270;&#25968;&#25454;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#20010;&#23454;&#20363;&#25915;&#20987;&#65306;&#20869;&#23481;&#27880;&#20837;&#21644;&#36807;&#24230;&#25298;&#32477;&#25915;&#20987;&#65292;&#27599;&#20010;&#25915;&#20987;&#37117;&#26088;&#22312;&#35825;&#23548;&#29305;&#23450;&#30340;&#21487;&#21033;&#29992;&#34892;&#20026;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#25968;&#25454;&#27880;&#20837;&#26041;&#26696;&#30340;&#24378;&#24230;&#21644;&#38544;&#34109;&#24615;&#36827;&#34892;&#20102;&#37327;&#21270;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#27602;&#21270;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;AutoPoison&#20801;&#35768;&#23545;&#25163;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning is an effective technique to align large language models (LLMs) with human intents. In this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model's behavior. For example, an adversary can achieve content injection by injecting training examples that mention target content and eliciting such behavior from downstream models. To achieve this goal, we propose \textit{AutoPoison}, an automated data poisoning pipeline. It naturally and coherently incorporates versatile attack goals into poisoned data with the help of an oracle LLM. We showcase two example attacks: content injection and over-refusal attacks, each aiming to induce a specific exploitable behavior. We quantify and benchmark the strength and the stealthiness of our data poisoning scheme. Our results show that AutoPoison allows an adversary to change a model's behavior by poisoning only
&lt;/p&gt;</description></item><item><title>InterCode&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#32534;&#30721;&#30340;&#26631;&#20934;&#21270;&#21644;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#25191;&#34892;&#21453;&#39304;&#20316;&#20026;&#35266;&#23519;&#65292;&#24182;&#25552;&#20379;&#20102;&#23433;&#20840;&#21487;&#37325;&#29616;&#30340;&#25191;&#34892;&#29615;&#22659;&#65292;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26032;&#30340;&#20132;&#20114;&#24335;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.14898</link><description>&lt;p&gt;
InterCode:&#26631;&#20934;&#21270;&#21644;&#22522;&#20934;&#27979;&#35797;&#20855;&#26377;&#25191;&#34892;&#21453;&#39304;&#30340;&#20132;&#20114;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback. (arXiv:2306.14898v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14898
&lt;/p&gt;
&lt;p&gt;
InterCode&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#32534;&#30721;&#30340;&#26631;&#20934;&#21270;&#21644;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#25191;&#34892;&#21453;&#39304;&#20316;&#20026;&#35266;&#23519;&#65292;&#24182;&#25552;&#20379;&#20102;&#23433;&#20840;&#21487;&#37325;&#29616;&#30340;&#25191;&#34892;&#29615;&#22659;&#65292;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26032;&#30340;&#20132;&#20114;&#24335;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20197;&#22522;&#26412;&#20132;&#20114;&#26041;&#24335;&#32534;&#20889;&#20195;&#30721;&#65292;&#24182;&#20381;&#36182;&#20110;&#25345;&#32493;&#30340;&#25191;&#34892;&#21453;&#39304;&#26469;&#32416;&#27491;&#38169;&#35823;&#65292;&#35299;&#20915;&#27495;&#20041;&#21644;&#20998;&#35299;&#20219;&#21153;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;LLM&#23637;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32534;&#30721;&#33021;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;&#32534;&#30721;&#22522;&#20934;&#20027;&#35201;&#32771;&#34385;&#38745;&#24577;&#30340;&#25351;&#20196;&#21040;&#20195;&#30721;&#24207;&#21015;&#36716;&#25442;&#36807;&#31243;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#20256;&#25773;&#21644;&#29983;&#25104;&#30340;&#20195;&#30721;&#19982;&#20854;&#26368;&#32456;&#25191;&#34892;&#29615;&#22659;&#20043;&#38388;&#30340;&#33073;&#33410;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;InterCode&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#12289;&#28789;&#27963;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#20132;&#20114;&#24335;&#32534;&#30721;&#26694;&#26550;&#65292;&#20316;&#20026;&#19968;&#20010;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29615;&#22659;&#65292;&#20351;&#29992;&#20195;&#30721;&#20316;&#20026;&#34892;&#21160;&#65292;&#25191;&#34892;&#21453;&#39304;&#20316;&#20026;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#35821;&#35328;&#21644;&#24179;&#21488;&#26080;&#20851;&#65292;&#20351;&#29992;&#29420;&#31435;&#30340;Docker&#29615;&#22659;&#25552;&#20379;&#23433;&#20840;&#21644;&#21487;&#37325;&#29616;&#30340;&#25191;&#34892;&#65292;&#24182;&#19988;&#19982;&#20256;&#32479;&#30340;seq2seq&#32534;&#30721;&#26041;&#27861;&#24320;&#31665;&#21363;&#29992;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#24320;&#21457;&#26032;&#30340;&#20132;&#20114;&#24335;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;InterCode&#21019;&#24314;...
&lt;/p&gt;
&lt;p&gt;
Humans write code in a fundamentally interactive manner and rely on constant execution feedback to correct errors, resolve ambiguities, and decompose tasks. While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations. Our framework is language and platform agnostic, uses self-contained Docker environments to provide safe and reproducible execution, and is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation. We use InterCode to create t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21322;&#36879;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#65292;&#40723;&#21169;&#27169;&#22411;&#29983;&#25104;&#26356;&#35814;&#32454;&#30340;&#38271;&#23383;&#24149;&#12290;</title><link>http://arxiv.org/abs/2306.13460</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#36879;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23398;&#20064;&#25551;&#36848;&#24615;&#22270;&#20687;&#23383;&#24149;
&lt;/p&gt;
&lt;p&gt;
Learning Descriptive Image Captioning via Semipermeable Maximum Likelihood Estimation. (arXiv:2306.13460v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21322;&#36879;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#65292;&#40723;&#21169;&#27169;&#22411;&#29983;&#25104;&#26356;&#35814;&#32454;&#30340;&#38271;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#26088;&#22312;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#35270;&#35273;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26159;&#35757;&#32451;&#30446;&#26631;&#65292;&#23383;&#24149;&#27169;&#22411;&#22312;&#39044;&#27979;&#19982;&#26631;&#31614;&#19981;&#21305;&#37197;&#26102;&#20250;&#21463;&#21040;&#24809;&#32602;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21322;&#36879;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;SMILE&#65289;&#26041;&#27861;&#65292;&#20801;&#35768;&#20016;&#23500;&#24615;&#20248;&#21270;&#21516;&#26102;&#38459;&#27490;&#31616;&#27905;&#24615;&#20248;&#21270;&#65292;&#20174;&#32780;&#40723;&#21169;&#27169;&#22411;&#29983;&#25104;&#26356;&#35814;&#32454;&#30340;&#38271;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image captioning aims to describe visual content in natural language. As 'a picture is worth a thousand words', there could be various correct descriptions for an image. However, with maximum likelihood estimation as the training objective, the captioning model is penalized whenever its prediction mismatches with the label. For instance, when the model predicts a word expressing richer semantics than the label, it will be penalized and optimized to prefer more concise expressions, referred to as conciseness optimization. In contrast, predictions that are more concise than labels lead to richness optimization. Such conflicting optimization directions could eventually result in the model generating general descriptions. In this work, we introduce Semipermeable MaxImum Likelihood Estimation (SMILE), which allows richness optimization while blocking conciseness optimization, thus encouraging the model to generate longer captions with more details. Extensive experiments on two mainstream im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;SituatedGen&#65292;&#35201;&#27714;&#20855;&#26377;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#30340;&#26426;&#22120;&#29983;&#25104;&#19968;&#23545;&#23545;&#27604;&#21477;&#23376;&#65292;&#20197;&#34701;&#20837;&#22320;&#29702;&#21644;&#26102;&#38388;&#32972;&#26223;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#33521;&#35821;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#30446;&#21069;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#20855;&#26377;&#24120;&#35782;&#21512;&#29702;&#24615;&#30340;&#21477;&#23376;&#30340;&#29983;&#25104;&#65292;&#36828;&#36828;&#33853;&#21518;&#20110;&#20154;&#31867;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.12552</link><description>&lt;p&gt;
SituatedGen: &#23558;&#22320;&#29702;&#21644;&#26102;&#38388;&#32972;&#26223;&#34701;&#20837;&#29983;&#25104;&#24335;&#24120;&#35782;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning. (arXiv:2306.12552v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;SituatedGen&#65292;&#35201;&#27714;&#20855;&#26377;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#30340;&#26426;&#22120;&#29983;&#25104;&#19968;&#23545;&#23545;&#27604;&#21477;&#23376;&#65292;&#20197;&#34701;&#20837;&#22320;&#29702;&#21644;&#26102;&#38388;&#32972;&#26223;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#33521;&#35821;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#30446;&#21069;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#20855;&#26377;&#24120;&#35782;&#21512;&#29702;&#24615;&#30340;&#21477;&#23376;&#30340;&#29983;&#25104;&#65292;&#36828;&#36828;&#33853;&#21518;&#20110;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#24120;&#35782;&#25512;&#29702;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#29983;&#25104;&#24335;&#24120;&#35782;&#25512;&#29702;&#26159;&#19968;&#39033;&#20219;&#21153;&#65292;&#35201;&#27714;&#26426;&#22120;&#22312;&#32473;&#23450;&#19968;&#32452;&#20851;&#38190;&#35789;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#24120;&#35782;&#21512;&#29702;&#24615;&#32452;&#21512;&#20986;&#19968;&#21477;&#36830;&#36143;&#30340;&#21477;&#23376;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#38024;&#23545;&#29983;&#25104;&#24335;&#24120;&#35782;&#25512;&#29702;&#30340;&#25968;&#25454;&#38598;&#20381;focus everyday scenarios&#65292;&#20294;&#26159;&#26426;&#22120;&#22312;&#29305;&#23450;&#30340;&#22320;&#29702;&#21644;&#26102;&#38388;&#32972;&#26223;&#19979;&#29702;&#35299;&#30340;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;SituatedGen&#65292;&#35201;&#27714;&#20855;&#26377;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#30340;&#26426;&#22120;&#29983;&#25104;&#19968;&#23545;&#23545;&#27604;&#21477;&#23376;&#65292;&#32473;&#23450;&#30340;&#20851;&#38190;&#35789;&#21253;&#25324;&#22320;&#29702;&#25110;&#26102;&#38388;&#23454;&#20307;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#20221;&#30456;&#24212;&#30340;&#33521;&#35821;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;8,268&#23545;&#23545;&#27604;&#21477;&#23376;&#65292;&#36825;&#20123;&#21477;&#23376;&#24314;&#31435;&#22312;&#29616;&#26377;&#30340;&#20960;&#20010;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#19978;&#65292;&#20154;&#24037;&#24037;&#20316;&#37327;&#26368;&#23567;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#29983;&#25104;&#20855;&#26377;&#24120;&#35782;&#21512;&#29702;&#24615;&#30340;&#21477;&#23376;&#65292;&#24182;&#19988;&#20173;&#28982;&#36828;&#36828;&#33853;&#21518;&#20110;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, commonsense reasoning in text generation has attracted much attention. Generative commonsense reasoning is the task that requires machines, given a group of keywords, to compose a single coherent sentence with commonsense plausibility. While existing datasets targeting generative commonsense reasoning focus on everyday scenarios, it is unclear how well machines reason under specific geographical and temporal contexts. We formalize this challenging task as SituatedGen, where machines with commonsense should generate a pair of contrastive sentences given a group of keywords including geographical or temporal entities. We introduce a corresponding English dataset consisting of 8,268 contrastive sentence pairs, which are built upon several existing commonsense reasoning benchmarks with minimal manual labor. Experiments show that state-of-the-art generative language models struggle to generate sentences with commonsense plausibility and still lag far behind human performance. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; Quilt-1M &#30340;&#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#21644;&#25991;&#23383;&#23545;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992; YouTube &#19978;&#30340;&#19987;&#23478;&#21307;&#29983;&#25945;&#31243;&#35270;&#39057;&#20026;&#20027;&#35201;&#26469;&#28304;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#20351;&#24471;&#30284;&#30151;&#32452;&#32455;&#23398;&#39046;&#22495;&#30340;&#34920;&#24449;&#23398;&#20064;&#21462;&#24471;&#31867;&#20284;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.11207</link><description>&lt;p&gt;
Quilt-1M: &#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#25991;&#23383;&#23545;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Quilt-1M: One Million Image-Text Pairs for Histopathology. (arXiv:2306.11207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; Quilt-1M &#30340;&#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#21644;&#25991;&#23383;&#23545;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992; YouTube &#19978;&#30340;&#19987;&#23478;&#21307;&#29983;&#25945;&#31243;&#35270;&#39057;&#20026;&#20027;&#35201;&#26469;&#28304;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#20351;&#24471;&#30284;&#30151;&#32452;&#32455;&#23398;&#39046;&#22495;&#30340;&#34920;&#24449;&#23398;&#20064;&#21462;&#24471;&#31867;&#20284;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24212;&#29992;&#30340;&#21152;&#36895;&#20351;&#24471;&#22312;&#32447;&#22270;&#20687;&#21644;&#25991;&#23383;&#25968;&#25454;&#22823;&#37327;&#28044;&#29616;&#65292;&#20294;&#21307;&#23398;&#39046;&#22495;&#65288;&#29305;&#21035;&#26159;&#30284;&#30151;&#32452;&#32455;&#23398;&#65289;&#31867;&#20284;&#30340;&#25968;&#25454;&#21364;&#24456;&#31232;&#23569;&#65292;&#36825;&#38459;&#30861;&#20102;&#21307;&#23398;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#21033;&#29992;YouTube&#19978;&#30340;&#19987;&#23478;&#21307;&#29983;&#25945;&#31243;&#35270;&#39057;&#65292;&#20174;&#20013;&#36873;&#25321;&#20102; 1,087 &#23567;&#26102;&#30340;&#21307;&#23398;&#32452;&#32455;&#23398;&#35270;&#39057;&#65292;&#20197;&#27492;&#33258;&#21160;&#31579;&#36873;&#20986;&#20849;&#21253;&#21547; 768,826 &#20010;&#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#21450;&#20854;&#23545;&#24212;&#30340;&#25991;&#23383;&#23545;&#30340; Quilt &#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent accelerations in multi-modal applications have been made possible with the plethora of image and text data available online. However, the scarcity of analogous data in the medical field, specifically in histopathology, has halted comparable progress. To enable similar representation learning for histopathology, we turn to YouTube, an untapped resource of videos, offering $1,087$ hours of valuable educational histopathology videos from expert clinicians. From YouTube, we curate Quilt: a large-scale vision-language dataset consisting of $768,826$ image and text pairs. Quilt was automatically curated using a mixture of models, including large language models, handcrafted algorithms, human knowledge databases, and automatic speech recognition. In comparison, the most comprehensive datasets curated for histopathology amass only around $200$K samples. We combine Quilt with datasets from other sources, including Twitter, research papers, and the internet in general, to create an even l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#27979;&#35797;&#38382;&#39064;&#30340;&#29305;&#24615;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#26356;&#23569;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#35813;&#26694;&#26550;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#19982;&#20154;&#31867;&#36827;&#34892;&#36731;&#26494;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2306.10512</link><description>&lt;p&gt;
&#39640;&#25928;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65306;&#33258;&#36866;&#24212;&#27979;&#35797;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective. (arXiv:2306.10512v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#27979;&#35797;&#38382;&#39064;&#30340;&#29305;&#24615;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#26356;&#23569;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#35813;&#26694;&#26550;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#19982;&#20154;&#31867;&#36827;&#34892;&#36731;&#26494;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#23637;&#29616;&#20102;&#19968;&#20123;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#20026;&#20102;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#36825;&#20123;&#33021;&#21147;&#65292;&#36890;&#24120;&#37319;&#29992;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#65288;&#22914;&#25991;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#24515;&#29702;&#23398;&#65289;&#30340;&#22810;&#20010;&#22522;&#20934;&#65288;&#21363;&#26631;&#20934;&#27979;&#35797;&#38382;&#39064;&#38598;&#65289;&#65292;&#24182;&#25253;&#21578;&#20256;&#32479;&#24230;&#37327;&#25351;&#26631;&#65288;&#22914;&#20934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#65289;&#12290;&#28982;&#32780;&#65292;&#20174;&#35748;&#30693;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#35780;&#20272;LLMs&#30340;&#26041;&#27861;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#19988;&#19981;&#20934;&#30830;&#12290;&#21463;&#24515;&#29702;&#27979;&#37327;&#23398;&#20013;&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#65288;CAT&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;LLM&#35780;&#20272;&#30340;&#33258;&#36866;&#24212;&#27979;&#35797;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#26681;&#25454;&#27169;&#22411;&#30340;&#34920;&#29616;&#21160;&#24577;&#35843;&#25972;&#27979;&#35797;&#38382;&#39064;&#30340;&#29305;&#24615;&#65288;&#22914;&#38590;&#24230;&#65289;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#26631;&#20934;&#30340;&#27979;&#35797;&#38598;&#24182;&#31616;&#21333;&#25253;&#21578;&#20934;&#30830;&#29575;&#12290;&#36825;&#20351;&#24471;&#33021;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#26356;&#23569;&#30340;&#38382;&#39064;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20351;LLMs&#33021;&#22815;&#19982;&#20154;&#31867;&#36827;&#34892;&#36731;&#26494;&#27604;&#36739;&#65292;&#36825;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), like ChatGPT, have shown some human-like cognitive abilities. For comparing these abilities of different models, several benchmarks (i.e. sets of standard test questions) from different fields (e.g., Literature, Biology and Psychology) are often adopted and the test results under traditional metrics such as accuracy, recall and F1, are reported. However, such way for evaluating LLMs can be inefficient and inaccurate from the cognitive science perspective. Inspired by Computerized Adaptive Testing (CAT) used in psychometrics, we propose an adaptive testing framework for LLM evaluation. Rather than using a standard test set and simply reporting accuracy, this approach dynamically adjusts the characteristics of the test questions, such as difficulty, based on the model's performance. This allows for a more accurate estimation of the model's abilities, using fewer questions. More importantly, it allows LLMs to be compared with humans easily, which is essential
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#32467;&#21512;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#22359;&#21464;&#25442;&#22120;&#65292;&#26088;&#22312;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.09539</link><description>&lt;p&gt;
&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Block-State Transformer. (arXiv:2306.09539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#32467;&#21512;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#22359;&#21464;&#25442;&#22120;&#65292;&#26088;&#22312;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#22312;&#38656;&#35201;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#24615;&#24182;&#19988;&#38656;&#35201;&#39640;&#25928;&#25193;&#23637;&#21040;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#24778;&#20154;&#30340;&#25928;&#26524;&#12290;&#23613;&#31649;&#26368;&#21021;&#26159;&#20026;&#36830;&#32493;&#20449;&#21495;&#35774;&#35745;&#30340;&#65292;&#20294;SSM&#22312;&#35270;&#35273;&#21644;&#38899;&#39057;&#31561;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#65307;&#28982;&#32780;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;SSM&#20173;&#28982;&#33853;&#21518;&#20110;Transformers&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#23618;&#65292;&#23427;&#22312;&#20869;&#37096;&#32452;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#38271;&#36317;&#31163;&#19978;&#19979;&#25991;&#21270;&#30340;SSM&#23376;&#23618;&#21644;&#19968;&#20010;&#29992;&#20110;&#30701;&#26399;&#24207;&#21015;&#34920;&#31034;&#30340;&#22359;&#21464;&#25442;&#22120;&#23376;&#23618;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#12289;&#23436;&#20840;&#21487;&#24182;&#34892;&#30340;&#38598;&#25104;SSM&#21644;&#22359;&#27880;&#24847;&#21147;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#30340;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compa
&lt;/p&gt;</description></item><item><title>&#26681;&#25454;&#20154;&#31867;&#35760;&#24518;&#21644;&#25512;&#29702;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#28436;&#21270;LLM&#26234;&#33021;&#20307;&#26694;&#26550;REMEMBERER&#65292;&#36890;&#36807;&#20026;LLM&#35013;&#22791;&#38271;&#26399;&#32463;&#39564;&#35760;&#24518;&#65292;&#21487;&#20197;&#20026;&#19981;&#21516;&#20219;&#21153;&#25552;&#20379;&#20248;&#24322;&#30340;&#26234;&#33021;&#20307;&#65292;&#20854;&#26500;&#25104;&#20102;&#21322;&#21442;&#25968;RL&#20195;&#29702;&#12290;&#25104;&#21151;&#29575;&#36229;&#36807;&#20808;&#21069;SOTA 4&#65285;&#21644;2&#65285;&#12290;</title><link>http://arxiv.org/abs/2306.07929</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21322;&#21442;&#25968;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Is Semi-Parametric Reinforcement Learning Agent. (arXiv:2306.07929v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07929
&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#20154;&#31867;&#35760;&#24518;&#21644;&#25512;&#29702;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#28436;&#21270;LLM&#26234;&#33021;&#20307;&#26694;&#26550;REMEMBERER&#65292;&#36890;&#36807;&#20026;LLM&#35013;&#22791;&#38271;&#26399;&#32463;&#39564;&#35760;&#24518;&#65292;&#21487;&#20197;&#20026;&#19981;&#21516;&#20219;&#21153;&#25552;&#20379;&#20248;&#24322;&#30340;&#26234;&#33021;&#20307;&#65292;&#20854;&#26500;&#25104;&#20102;&#21322;&#21442;&#25968;RL&#20195;&#29702;&#12290;&#25104;&#21151;&#29575;&#36229;&#36807;&#20808;&#21069;SOTA 4&#65285;&#21644;2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#35748;&#30693;&#31185;&#23398;&#23545;&#20154;&#31867;&#35760;&#24518;&#21644;&#25512;&#29702;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#28436;&#21270;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#26234;&#33021;&#20307;&#26694;&#26550;REMEMBERER&#12290;&#36890;&#36807;&#20026;LLM&#35013;&#22791;&#38271;&#26399;&#32463;&#39564;&#35760;&#24518;&#65292;REMEMBERER&#33021;&#22815;&#21033;&#29992;&#36807;&#21435;&#21095;&#38598;&#30340;&#32463;&#39564;&#65292;&#29978;&#33267;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#30446;&#26631;&#25552;&#20379;&#20248;&#24322;&#30340;LLM&#26234;&#33021;&#20307;&#65292;&#36825;&#20248;&#20110;&#20855;&#26377;&#22266;&#23450;&#23454;&#20363;&#25110;&#20855;&#26377;&#30701;&#26242;&#24037;&#20316;&#35760;&#24518;&#30340;LLM&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#32463;&#39564;&#35760;&#24518;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLEM&#65289;&#26469;&#26356;&#26032;&#35760;&#24518;&#12290;&#22240;&#27492;&#65292;&#25972;&#20010;&#31995;&#32479;&#21487;&#20197;&#20174;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#24182;&#22312;&#19981;&#24494;&#35843;LLM&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#21457;&#23637;&#20854;&#33021;&#21147;&#12290;&#20197;&#27492;&#26041;&#24335;&#65292;&#25152;&#25552;&#20986;&#30340;REMEMBERER&#26500;&#25104;&#20102;&#21322;&#21442;&#25968;RL&#20195;&#29702;&#12290;&#22312;&#20004;&#20010;RL&#20219;&#21153;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#20197;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#12290;&#19981;&#21516;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#38598;&#30340;&#24179;&#22343;&#32467;&#26524;&#23545;&#20110;&#25104;&#21151;&#29575;&#36229;&#36807;&#20808;&#21069;SOTA 4&#65285;&#21644;2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER. By equipping the LLM with a long-term experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by 4% and 2% for the success rate 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FactorCL&#65292;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#20165;&#32771;&#34385;&#36328;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#65292;&#36824;&#33021;&#25429;&#25417;&#36328;&#27169;&#24577;&#21807;&#19968;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.05268</link><description>&lt;p&gt;
&#20998;&#35299;&#23545;&#27604;&#23398;&#20064;&#65306;&#36229;&#36234;&#22810;&#35270;&#35282;&#20887;&#20313;
&lt;/p&gt;
&lt;p&gt;
Factorized Contrastive Learning: Going Beyond Multi-view Redundancy. (arXiv:2306.05268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FactorCL&#65292;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#20165;&#32771;&#34385;&#36328;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#65292;&#36824;&#33021;&#25429;&#25417;&#36328;&#27169;&#24577;&#21807;&#19968;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#65292;&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#31181;&#29305;&#21035;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#23398;&#20064;&#20855;&#26377;&#20016;&#23500;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#34920;&#31034;&#65292;&#21482;&#38656;&#37197;&#23545;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#22270;&#20687;&#26631;&#39064;&#25110;&#35270;&#39057;&#38899;&#39057;&#23545;&#65289;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#22522;&#30784;&#26159;&#22810;&#35270;&#35282;&#20887;&#20313;&#30340;&#20551;&#35774;&#8212;&#8212;&#36328;&#27169;&#24577;&#38388;&#20849;&#20139;&#20449;&#24687;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#26159;&#24517;&#35201;&#19988;&#36275;&#22815;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#20063;&#21253;&#21547;&#22312;&#36328;&#27169;&#24577;&#21807;&#19968;&#21306;&#22495;&#20013;&#65306;&#19968;&#31181;&#20165;&#23384;&#22312;&#20110;&#19968;&#20010;&#27169;&#24577;&#20013;&#20294;&#19982;&#20219;&#21153;&#20173;&#28982;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#22914;&#20309;&#23398;&#20064;&#33258;&#25105;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#20197;&#25429;&#33719;&#19982;&#19979;&#28216;&#20219;&#21153;&#30456;&#20851;&#30340;&#20849;&#20139;&#21644;&#21807;&#19968;&#20449;&#24687;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;FactorCL&#65292;&#20197;&#36229;&#36234;&#22810;&#35270;&#35282;&#20887;&#20313;&#12290;FactorCL&#30340;&#22522;&#30784;&#26159;&#19977;&#20010;&#26032;&#30340;&#36129;&#29486;&#65306;&#65288;1&#65289;&#23558;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#20998;&#35299;&#20026;&#20849;&#20139;&#21644;&#21807;&#19968;&#34920;&#31034;&#65292;&#65288;2&#65289;&#38480;&#21046;&#20849;&#20139;&#21644;&#21807;&#19968;&#25104;&#20998;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#65288;3&#65289;&#20351;&#29992;&#22240;&#23376;&#27491;&#21017;&#21270;&#20419;&#36827;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) cap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38381;&#21512;&#24418;&#24335;&#30340;&#26041;&#27861;LEACE&#65292;&#21487;&#22312;&#21024;&#38500;&#25351;&#23450;&#29305;&#24449;&#30340;&#21516;&#26102;&#23613;&#21487;&#33021;&#23569;&#22320;&#25913;&#21464;&#34920;&#31034;&#65292;&#24182;&#21487;&#35777;&#26126;&#38450;&#27490;&#25152;&#26377;&#32447;&#24615;&#20998;&#31867;&#22120;&#26816;&#27979;&#21040;&#27010;&#24565;&#12290;&#20316;&#32773;&#29992;&#8220;&#27010;&#24565;&#25830;&#38500;&#8221;&#36825;&#19968;&#26032;&#26041;&#27861;&#23558;&#20854;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#24615;&#30340;&#20381;&#36182;&#24615;&#21644;&#20943;&#23569;BERT&#23884;&#20837;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#20219;&#21153;&#20013;&#24471;&#20986;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.03819</link><description>&lt;p&gt;
LEACE&#65306;&#38381;&#21512;&#24418;&#24335;&#20013;&#30340;&#23436;&#32654;&#32447;&#24615;&#27010;&#24565;&#25830;&#38500;
&lt;/p&gt;
&lt;p&gt;
LEACE: Perfect linear concept erasure in closed form. (arXiv:2306.03819v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38381;&#21512;&#24418;&#24335;&#30340;&#26041;&#27861;LEACE&#65292;&#21487;&#22312;&#21024;&#38500;&#25351;&#23450;&#29305;&#24449;&#30340;&#21516;&#26102;&#23613;&#21487;&#33021;&#23569;&#22320;&#25913;&#21464;&#34920;&#31034;&#65292;&#24182;&#21487;&#35777;&#26126;&#38450;&#27490;&#25152;&#26377;&#32447;&#24615;&#20998;&#31867;&#22120;&#26816;&#27979;&#21040;&#27010;&#24565;&#12290;&#20316;&#32773;&#29992;&#8220;&#27010;&#24565;&#25830;&#38500;&#8221;&#36825;&#19968;&#26032;&#26041;&#27861;&#23558;&#20854;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#24615;&#30340;&#20381;&#36182;&#24615;&#21644;&#20943;&#23569;BERT&#23884;&#20837;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#20219;&#21153;&#20013;&#24471;&#20986;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#25830;&#38500;&#26088;&#22312;&#20174;&#34920;&#24449;&#20013;&#21024;&#38500;&#25351;&#23450;&#30340;&#29305;&#24449;&#12290;&#23427;&#21487;&#20197;&#25552;&#39640;&#20844;&#24179;&#24615;&#65288;&#20363;&#22914;&#65292;&#38450;&#27490;&#20998;&#31867;&#22120;&#20351;&#29992;&#24615;&#21035;&#25110;&#31181;&#26063;&#65289;&#21644;&#21487;&#35299;&#37322;&#24615;&#65288;&#20363;&#22914;&#65292;&#21024;&#38500;&#27010;&#24565;&#20197;&#35266;&#23519;&#27169;&#22411;&#34892;&#20026;&#30340;&#21464;&#21270;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LEAst-squares&#27010;&#24565;&#25830;&#38500;&#65288;LEACE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38381;&#21512;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#21487;&#35777;&#26126;&#38450;&#27490;&#25152;&#26377;&#32447;&#24615;&#20998;&#31867;&#22120;&#26816;&#27979;&#21040;&#27010;&#24565;&#65292;&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#25913;&#21464;&#34920;&#31034;&#65292;&#22914;&#24191;&#27867;&#31867;&#21035;&#30340;&#33539;&#25968;&#25152;&#27979;&#37327;&#30340;&#37027;&#26679;&#12290;&#25105;&#20204;&#20351;&#29992;&#21517;&#20026;&#8220;&#27010;&#24565;&#25830;&#38500;&#8221;&#30340;&#26032;&#26041;&#27861;&#23558;LEACE&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25830;&#38500;&#27599;&#20010;&#23618;&#20013;&#30340;&#30446;&#26631;&#27010;&#24565;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#24615;&#20449;&#24687;&#30340;&#20381;&#36182;&#24615;&#65292;&#20197;&#21450;&#20943;&#23569;BERT&#23884;&#20837;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/EleutherAI/concept-erasure&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept erasure aims to remove specified features from a representation. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the representation as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called "concept scrubbing," which erases target concept information from every layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Code is available at https://github.com/EleutherAI/concept-erasure.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23545;&#33258;&#21160;&#29983;&#25104;&#30340;&#24773;&#24863;&#26354;&#32447;&#36827;&#34892;&#20102;&#31995;&#32479;&#21644;&#23450;&#37327;&#30340;&#35780;&#20272;&#65292;&#24182;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#35789;&#20856;&#26041;&#27861;&#20004;&#31181;&#29983;&#25104;&#24773;&#24863;&#26354;&#32447;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#34429;&#28982;&#35789;&#20856;&#26041;&#27861;&#22312;&#23454;&#20363;&#32423;&#24773;&#24863;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#65292;&#20294;&#22312;&#32858;&#21512;&#20449;&#24687;&#26102;&#29983;&#25104;&#24773;&#24863;&#26354;&#32447;&#30340;&#20934;&#30830;&#24615;&#38750;&#24120;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#33258;&#21160;&#32763;&#35793;&#33521;&#35821;&#24773;&#24863;&#35789;&#20856;&#65292;&#21487;&#20197;&#22312;&#36164;&#28304;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24773;&#24863;&#26354;&#32447;&#12290;</title><link>http://arxiv.org/abs/2306.02213</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#35780;&#20272;&#24773;&#24863;&#26354;&#32447;&#65306;&#24357;&#21512;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#20840;&#29699;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Evaluating Emotion Arcs Across Languages: Bridging the Global Divide in Sentiment Analysis. (arXiv:2306.02213v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23545;&#33258;&#21160;&#29983;&#25104;&#30340;&#24773;&#24863;&#26354;&#32447;&#36827;&#34892;&#20102;&#31995;&#32479;&#21644;&#23450;&#37327;&#30340;&#35780;&#20272;&#65292;&#24182;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#35789;&#20856;&#26041;&#27861;&#20004;&#31181;&#29983;&#25104;&#24773;&#24863;&#26354;&#32447;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#34429;&#28982;&#35789;&#20856;&#26041;&#27861;&#22312;&#23454;&#20363;&#32423;&#24773;&#24863;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#65292;&#20294;&#22312;&#32858;&#21512;&#20449;&#24687;&#26102;&#29983;&#25104;&#24773;&#24863;&#26354;&#32447;&#30340;&#20934;&#30830;&#24615;&#38750;&#24120;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#33258;&#21160;&#32763;&#35793;&#33521;&#35821;&#24773;&#24863;&#35789;&#20856;&#65292;&#21487;&#20197;&#22312;&#36164;&#28304;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24773;&#24863;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#26354;&#32447;&#25429;&#25417;&#20102;&#19968;&#20010;&#20154;&#65288;&#25110;&#19968;&#20010;&#32676;&#20307;&#65289;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#24773;&#24863;&#29366;&#24577;&#12290;&#23427;&#20204;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#21644;&#30740;&#31350;&#39046;&#22495;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#33258;&#21160;&#29983;&#25104;&#30340;&#24773;&#24863;&#26354;&#32447;&#30340;&#35780;&#20272;&#24037;&#20316;&#24456;&#23569;&#12290;&#36825;&#26159;&#22240;&#20026;&#24314;&#31435;&#30495;&#23454;&#65288;&#40644;&#37329;&#65289;&#24773;&#24863;&#26354;&#32447;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23545;&#33258;&#21160;&#29983;&#25104;&#30340;&#24773;&#24863;&#26354;&#32447;&#36827;&#34892;&#20102;&#31995;&#32479;&#21644;&#23450;&#37327;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#24773;&#24863;&#26354;&#32447;&#29983;&#25104;&#26041;&#27861;&#65306;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#21644;&#20165;&#35789;&#20856;&#65288;LexO&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;9&#31181;&#35821;&#35328;&#30340;18&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#34920;&#26126;&#23613;&#31649;&#22312;&#23454;&#20363;&#32423;&#24773;&#24863;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#24046;&#65292;&#20294;LexO&#26041;&#27861;&#22312;&#20174;&#25968;&#30334;&#20010;&#23454;&#20363;&#20013;&#32858;&#21512;&#20449;&#24687;&#26102;&#29983;&#25104;&#24773;&#24863;&#26354;&#32447;&#30340;&#20934;&#30830;&#24615;&#38750;&#24120;&#39640;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#20845;&#31181;&#38750;&#27954;&#22303;&#33879;&#35821;&#35328;&#20197;&#21450;&#38463;&#25289;&#20271;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#33521;&#35821;&#24773;&#24863;&#35789;&#20856;&#30340;&#33258;&#21160;&#32763;&#35793;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24773;&#24863;&#26354;&#32447;&#65292;&#32780;&#36164;&#28304;&#24320;&#38144;&#30456;&#23545;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion arcs capture how an individual (or a population) feels over time. They are widely used in industry and research; however, there is little work on evaluating the automatically generated arcs. This is because of the difficulty of establishing the true (gold) emotion arc. Our work, for the first time, systematically and quantitatively evaluates automatically generated emotion arcs. We also compare two common ways of generating emotion arcs: Machine-Learning (ML) models and Lexicon-Only (LexO) methods. By running experiments on 18 diverse datasets in 9 languages, we show that despite being markedly poor at instance level emotion classification, LexO methods are highly accurate at generating emotion arcs when aggregating information from hundreds of instances. We also show, through experiments on six indigenous African languages, as well as Arabic, and Spanish, that automatic translations of English emotion lexicons can be used to generate high-quality emotion arcs in less-resource 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Fine-Grained RLHF&#26694;&#26550;&#65292;&#20351;&#29992;&#31934;&#32454;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#20316;&#20026;&#26126;&#30830;&#30340;&#35757;&#32451;&#20449;&#21495;&#26469;&#35757;&#32451;&#21644;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#22810;&#20010;&#32454;&#33268;&#30340;&#22870;&#21169;&#27169;&#22411;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01693</link><description>&lt;p&gt;
&#31934;&#32454;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Fine-Grained Human Feedback Gives Better Rewards for Language Model Training. (arXiv:2306.01693v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Fine-Grained RLHF&#26694;&#26550;&#65292;&#20351;&#29992;&#31934;&#32454;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#20316;&#20026;&#26126;&#30830;&#30340;&#35757;&#32451;&#20449;&#21495;&#26469;&#35757;&#32451;&#21644;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#22810;&#20010;&#32454;&#33268;&#30340;&#22870;&#21169;&#27169;&#22411;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#34920;&#29616;&#20986;&#19981;&#33391;&#30340;&#25991;&#26412;&#29983;&#25104;&#34892;&#20026;&#65292;&#21253;&#25324;&#29983;&#25104;&#34394;&#20551;&#12289;&#26377;&#23475;&#25110;&#26080;&#20851;&#30340;&#36755;&#20986;&#12290;&#26368;&#36817;&#65292;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;-&#20854;&#20013;&#20154;&#31867;&#23545;LM&#36755;&#20986;&#30340;&#20559;&#22909;&#35780;&#20215;&#34987;&#36716;&#21270;&#20026;&#23398;&#20064;&#20449;&#21495;-&#24050;&#32463;&#26174;&#31034;&#20986;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25972;&#20307;&#21453;&#39304;&#23545;&#38271;&#25991;&#26412;&#36755;&#20986;&#20256;&#36798;&#30340;&#20449;&#24687;&#26377;&#38480;&#65307;&#23427;&#19981;&#34920;&#26126;&#36755;&#20986;&#30340;&#21738;&#20123;&#26041;&#38754;&#24433;&#21709;&#20102;&#29992;&#25143;&#30340;&#20559;&#22909;&#65307;&#20363;&#22914;&#65292;&#21738;&#20123;&#37096;&#20998;&#21253;&#21547;&#20160;&#20040;&#31867;&#22411;&#30340;&#38169;&#35823;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31934;&#32454;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#65288;&#20363;&#22914;&#65292;&#21738;&#20010;&#21477;&#23376;&#26159;&#38169;&#35823;&#30340;&#65292;&#21738;&#20010;&#23376;&#21477;&#26159;&#26080;&#20851;&#30340;&#65289;&#20316;&#20026;&#26126;&#30830;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Fine-Grained RLHF&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#35757;&#32451;&#21644;&#23398;&#20064;&#19982;&#19981;&#21516;&#21453;&#39304;&#31867;&#22411;&#30456;&#20851;&#30340;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#30340;&#31934;&#32454;&#21270;&#22870;&#21169;&#21151;&#33021;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#20197;&#19979;&#20004;&#20010;&#29305;&#24449;&#65306;&#65288;1&#65289;&#23494;&#24230;&#65292;&#20197;&#22312;&#29983;&#25104;&#27599;&#20010;&#27573;&#33853;&#65288;&#20363;&#22914;&#19968;&#20010;&#21477;&#23376;&#65289;&#21518;&#25552;&#20379;&#22870;&#21169;&#65307; &#65288;2&#65289;&#24182;&#20837;&#19981;&#21516;&#21453;&#39304;&#31867;&#22411;&#30340;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF) - where human preference judgments on LM outputs are transformed into a learning signal - has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#27880;&#24847;&#21147;&#25925;&#38556;&#30340;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#32763;&#36716;-&#32763;&#36716;&#35821;&#35328;&#24314;&#27169;&#26469;&#20998;&#26512;&#36825;&#20010;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;Transformer FFLMs&#32463;&#24120;&#20986;&#29616;&#25512;&#29702;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2306.00946</link><description>&lt;p&gt;
&#25581;&#31034;Attention&#25925;&#38556;&#30340;&#32763;&#36716;-&#32763;&#36716;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Exposing Attention Glitches with Flip-Flop Language Modeling. (arXiv:2306.00946v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#27880;&#24847;&#21147;&#25925;&#38556;&#30340;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#32763;&#36716;-&#32763;&#36716;&#35821;&#35328;&#24314;&#27169;&#26469;&#20998;&#26512;&#36825;&#20010;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;Transformer FFLMs&#32463;&#24120;&#20986;&#29616;&#25512;&#29702;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20160;&#20040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26102;&#20250;&#36755;&#20986;&#20107;&#23454;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#38169;&#35823;&#30340;&#25512;&#29702;&#65311;&#36825;&#20123;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25191;&#34892;&#38271;&#38142;&#25512;&#29702;&#26102;&#65292;&#30446;&#21069;&#20284;&#20046;&#26159;&#20026;&#20102;&#23427;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#32508;&#21512;&#30693;&#35782;&#12289;&#35821;&#29992;&#21644;&#25277;&#35937;&#24605;&#32500;&#32780;&#24517;&#39035;&#20184;&#20986;&#30340;&#20195;&#20215;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#20010;&#26681;&#26412;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#30830;&#23450;&#24182;&#20998;&#26512;&#20102;&#27880;&#24847;&#21147;&#25925;&#38556;&#29616;&#35937;&#65292;&#20854;&#20013;Transformer&#26550;&#26500;&#30340;&#24402;&#32435;&#24615;&#20559;&#35265;&#38388;&#27463;&#24615;&#22320;&#26410;&#33021;&#25429;&#25417;&#21040;&#31283;&#20581;&#30340;&#25512;&#29702;&#12290;&#20026;&#20102;&#38548;&#31163;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32763;&#36716;-&#32763;&#36716;&#35821;&#35328;&#24314;&#27169;&#65288;FFLM&#65289;&#65292;&#36825;&#26159;&#19968;&#32452;&#21442;&#25968;&#21270;&#21512;&#25104;&#22522;&#20934;&#65292;&#26088;&#22312;&#25506;&#32034;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#22806;&#25512;&#34892;&#20026;&#12290;&#36825;&#20010;&#31616;&#21333;&#30340;&#29983;&#25104;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#22312;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#20013;&#22797;&#21046;&#20108;&#36827;&#21046;&#31526;&#21495;&#65292;&#24573;&#30053;&#20013;&#38388;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#21457;&#29616;Transformer FFLMs&#22312;&#25512;&#29702;&#38169;&#35823;&#26041;&#38754;&#23384;&#22312;&#30528;&#38271;&#23614;&#29616;&#35937;&#65292;&#20854;&#20013;&#19968;&#20123;&#25105;&#20204;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
Why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of attention glitches, in which the Transformer architecture's inductive biases intermittently fail to capture robust reasoning. To isolate the issue, we introduce flip-flop language modeling (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, some of which we ca
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#30011;&#31508;&#30340;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#24494;&#35843;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26681;&#25454;&#30446;&#26631;&#21306;&#22495;&#20462;&#25913;AI&#21512;&#25104;&#22270;&#20687;&#24182;&#20445;&#30041;&#21407;&#22987;&#19978;&#19979;&#25991;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#26041;&#27861;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21487;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00219</link><description>&lt;p&gt;
&#25193;&#25955;&#30011;&#31508;&#65306;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#32534;&#36753;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images. (arXiv:2306.00219v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#30011;&#31508;&#30340;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#24494;&#35843;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26681;&#25454;&#30446;&#26631;&#21306;&#22495;&#20462;&#25913;AI&#21512;&#25104;&#22270;&#20687;&#24182;&#20445;&#30041;&#21407;&#22987;&#19978;&#19979;&#25991;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#26041;&#27861;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21487;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#38480;&#21046;&#65292;&#29983;&#25104;&#30340;&#22270;&#20687;&#32463;&#24120;&#21253;&#21547;&#19981;&#33391;&#30340;&#20266;&#24433;&#25110;&#20854;&#20182;&#38169;&#35823;&#12290;&#29616;&#26377;&#30340;&#24494;&#35843;&#29983;&#25104;&#22270;&#20687;&#30340;&#25216;&#26415;&#35201;&#20040;&#32791;&#26102;&#65288;&#25163;&#21160;&#32534;&#36753;&#65289;&#65292;&#35201;&#20040;&#20135;&#29983;&#19981;&#22815;&#23436;&#32654;&#30340;&#32467;&#26524;&#65288;&#20462;&#34917;&#65289;&#65292;&#35201;&#20040;&#20250;&#23548;&#33268;&#25972;&#20307;&#22270;&#20687;&#20135;&#29983;&#24847;&#24819;&#19981;&#21040;&#30340;&#21464;&#21270;&#65288;&#21464;&#20307;&#36873;&#25321;&#21644;&#25552;&#31034;&#24494;&#35843;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#30011;&#31508;&#30340;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#24494;&#35843;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24494;&#35843;AI&#21512;&#25104;&#22270;&#20687;&#20013;&#25152;&#38656;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#20013;&#22312;&#30446;&#26631;&#21306;&#22495;&#24341;&#20837;&#20102;&#26032;&#30340;&#38543;&#26426;&#22122;&#22768;&#27169;&#24335;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#20445;&#30041;&#20854;&#20182;&#21306;&#22495;&#21407;&#22987;&#19978;&#19979;&#25991;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#23545;&#25351;&#23450;&#21306;&#22495;&#36827;&#34892;&#26356;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#33402;&#26415;&#23478;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#23558;&#25105;&#20204;&#30340;&#25216;&#26415;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models have made remarkable advancements in generating high-quality images. However, generated images often contain undesirable artifacts or other errors due to model limitations. Existing techniques to fine-tune generated images are time-consuming (manual editing), produce poorly-integrated results (inpainting), or result in unexpected changes across the entire image (variation selection and prompt fine-tuning). In this work, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool to efficiently fine-tune desired regions within an AI-synthesized image. Our method introduces new random noise patterns at targeted regions during the reverse diffusion process, enabling the model to efficiently make changes to the specified regions while preserving the original context for the rest of the image. We evaluate our method's usability and effectiveness through a user study with artists, comparing our technique against other state-of-the-art image inpaintin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Language augmented CLIP&#65288;LaCLIP&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#37325;&#20889;&#26469;&#22686;&#24378;CLIP&#35757;&#32451;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#37325;&#26032;&#20070;&#20889;&#19982;&#27599;&#20010;&#22270;&#20687;&#20851;&#32852;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#30340;&#20851;&#38190;&#27010;&#24565;&#21644;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.20088</link><description>&lt;p&gt;
&#25913;&#36827;CLIP&#35757;&#32451;&#30340;&#35821;&#35328;&#37325;&#20889;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving CLIP Training with Language Rewrites. (arXiv:2305.20088v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Language augmented CLIP&#65288;LaCLIP&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#37325;&#20889;&#26469;&#22686;&#24378;CLIP&#35757;&#32451;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#37325;&#26032;&#20070;&#20889;&#19982;&#27599;&#20010;&#22270;&#20687;&#20851;&#32852;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#30340;&#20851;&#38190;&#27010;&#24565;&#21644;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#26159;&#20351;&#29992;&#25104;&#23545;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#36716;&#31227;&#35270;&#35273;&#27169;&#22411;&#30340;&#26368;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;CLIP&#27169;&#22411;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#25968;&#25454;&#22686;&#24378;&#26469;&#38450;&#27490;&#36807;&#25311;&#21512;&#21644;&#25463;&#24452;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;CLIP&#35757;&#32451;&#33539;&#24335;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#20165;&#24212;&#29992;&#20110;&#22270;&#20687;&#36755;&#20837;&#65292;&#32780;&#35821;&#35328;&#36755;&#20837;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#65292;&#38480;&#21046;&#20102;&#22810;&#26679;&#25991;&#26412;&#23545;&#30456;&#21516;&#22270;&#20687;&#30340;&#26292;&#38706;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Language augmented CLIP&#65288;LaCLIP&#65289;&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#37325;&#20889;&#26469;&#22686;&#24378;CLIP&#35757;&#32451;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#37325;&#26032;&#20070;&#20889;&#19982;&#27599;&#20010;&#22270;&#20687;&#20851;&#32852;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#36825;&#20123;&#37325;&#26032;&#20070;&#20889;&#30340;&#25991;&#26412;&#22312;&#21477;&#23376;&#32467;&#26500;&#21644;&#35789;&#27719;&#26041;&#38754;&#21576;&#29616;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21407;&#22987;&#20851;&#38190;&#27010;&#24565;&#21644;&#24847;&#20041;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;LaCLIP&#38543;&#26426;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pre-training (CLIP) stands as one of the most effective and scalable methods for training transferable vision models using paired image and text data. CLIP models are trained using contrastive loss, which typically relies on data augmentations to prevent overfitting and shortcuts. However, in the CLIP training paradigm, data augmentations are exclusively applied to image inputs, while language inputs remain unchanged throughout the entire training process, limiting the exposure of diverse texts to the same image. In this paper, we introduce Language augmented CLIP (LaCLIP), a simple yet highly effective approach to enhance CLIP training through language rewrites. Leveraging the in-context learning capability of large language models, we rewrite the text descriptions associated with each image. These rewritten texts exhibit diversity in sentence structure and vocabulary while preserving the original key concepts and meanings. During training, LaCLIP randomly s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20195;&#29702;&#31243;&#24207;SheetCopilot&#65292;&#35813;&#31243;&#24207;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#36719;&#20214;&#25191;&#34892;&#30005;&#23376;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#31561;&#20219;&#21153;&#12290;&#35813;&#31243;&#24207;&#35774;&#35745;&#20102;&#19968;&#32452;&#25277;&#35937;&#30340;&#30005;&#23376;&#34920;&#26684;&#36719;&#20214;&#21151;&#33021;&#21407;&#23376;&#21160;&#20316;&#20197;&#21450;&#22522;&#20110;&#29366;&#24577;&#26426;&#30340;&#20219;&#21153;&#35268;&#21010;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;LLMs&#19982;&#30005;&#23376;&#34920;&#26684;&#30340;&#40065;&#26834;&#20132;&#20114;&#65292;&#21487;&#20197;&#21333;&#27425;&#27491;&#30830;&#23436;&#25104;44.3&#65285;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.19308</link><description>&lt;p&gt;
SheetCopilot: &#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#36719;&#20214;&#29983;&#20135;&#21147;&#25552;&#21319;&#21040;&#26032;&#30340;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models. (arXiv:2305.19308v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20195;&#29702;&#31243;&#24207;SheetCopilot&#65292;&#35813;&#31243;&#24207;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#36719;&#20214;&#25191;&#34892;&#30005;&#23376;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#31561;&#20219;&#21153;&#12290;&#35813;&#31243;&#24207;&#35774;&#35745;&#20102;&#19968;&#32452;&#25277;&#35937;&#30340;&#30005;&#23376;&#34920;&#26684;&#36719;&#20214;&#21151;&#33021;&#21407;&#23376;&#21160;&#20316;&#20197;&#21450;&#22522;&#20110;&#29366;&#24577;&#26426;&#30340;&#20219;&#21153;&#35268;&#21010;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;LLMs&#19982;&#30005;&#23376;&#34920;&#26684;&#30340;&#40065;&#26834;&#20132;&#20114;&#65292;&#21487;&#20197;&#21333;&#27425;&#27491;&#30830;&#23436;&#25104;44.3&#65285;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#32456;&#31471;&#29992;&#25143;&#33457;&#36153;&#20102;&#25968;&#21313;&#20159;&#23567;&#26102;&#23436;&#25104;&#35832;&#22914;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#21644;&#39033;&#30446;&#26102;&#38388;&#36724;&#35843;&#24230;&#31561;&#26085;&#24120;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#22823;&#22810;&#26159;&#37325;&#22797;&#24615;&#30340;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#32456;&#31471;&#29992;&#25143;&#32570;&#20047;&#33258;&#21160;&#21270;&#36825;&#20123;&#32321;&#29712;&#24037;&#20316;&#30340;&#25216;&#33021;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#29992;&#33258;&#28982;&#35821;&#35328;&#29992;&#25143;&#35831;&#27714;&#25351;&#23548;&#36719;&#20214;&#25104;&#20026;&#20102;&#19968;&#20010;&#21487;&#36798;&#25104;&#30340;&#30446;&#26631;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;SheetCopilot&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#25509;&#21463;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21644;&#25511;&#21046;&#30005;&#23376;&#34920;&#26684;&#20197;&#28385;&#36275;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#21407;&#23376;&#21160;&#20316;&#20316;&#20026;&#30005;&#23376;&#34920;&#26684;&#36719;&#20214;&#21151;&#33021;&#30340;&#25277;&#35937;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#22522;&#20110;&#29366;&#24577;&#26426;&#30340;&#20219;&#21153;&#35268;&#21010;&#26694;&#26550;&#65292;&#20197;&#20415;LLMs&#19982;&#30005;&#23376;&#34920;&#26684;&#36827;&#34892;&#40065;&#26834;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;221&#31181;&#30005;&#23376;&#34920;&#26684;&#25511;&#21046;&#20219;&#21153;&#30340;&#20195;&#34920;&#24615;&#25968;&#25454;&#38598;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#35780;&#20272;&#31649;&#36947;&#65292;&#20197;&#20005;&#26684;&#35780;&#20272;LLMs&#22312;&#36719;&#20214;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;SheetCopilot&#21333;&#27425;&#27491;&#30830;&#23436;&#25104;44.3&#65285;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill of automating away these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent which takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3\% of tasks for a single 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#31639;&#27861;LANCE&#65292;&#36890;&#36807;&#29983;&#25104;&#35821;&#35328;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#27979;&#35797;&#22270;&#20687;&#26469;&#21387;&#21147;&#27979;&#35797;&#35270;&#35273;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#25913;&#21464;&#27169;&#22411;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#22810;&#26679;&#12289;&#36924;&#30495;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#22270;&#20687;&#12290;&#36890;&#36807;&#23545;&#22810;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#27169;&#22411;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#23545;&#19981;&#21516;&#31867;&#22411;&#32534;&#36753;&#30340;&#25935;&#24863;&#24615;&#65292;&#25581;&#31034;&#20102;ImageNet&#20013;&#20808;&#21069;&#26410;&#30693;&#30340;&#31867;&#21035;&#23618;&#27425;&#27169;&#22411;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.19164</link><description>&lt;p&gt;
LANCE&#65306;&#36890;&#36807;&#29983;&#25104;&#35821;&#35328;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#23545;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images. (arXiv:2305.19164v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#31639;&#27861;LANCE&#65292;&#36890;&#36807;&#29983;&#25104;&#35821;&#35328;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#27979;&#35797;&#22270;&#20687;&#26469;&#21387;&#21147;&#27979;&#35797;&#35270;&#35273;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#25913;&#21464;&#27169;&#22411;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#22810;&#26679;&#12289;&#36924;&#30495;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#22270;&#20687;&#12290;&#36890;&#36807;&#23545;&#22810;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#27169;&#22411;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#23545;&#19981;&#21516;&#31867;&#22411;&#32534;&#36753;&#30340;&#25935;&#24863;&#24615;&#65292;&#25581;&#31034;&#20102;ImageNet&#20013;&#20808;&#21069;&#26410;&#30693;&#30340;&#31867;&#21035;&#23618;&#27425;&#27169;&#22411;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35821;&#35328;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#27979;&#35797;&#22270;&#20687;&#65288;LANCE&#65289;&#26469;&#23545;&#35757;&#32451;&#36807;&#30340;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20511;&#37492;&#20102;&#26368;&#36817;&#35821;&#35328;&#24314;&#27169;&#21644;&#22522;&#20110;&#25991;&#26412;&#32534;&#36753;&#30340;&#22270;&#20687;&#22788;&#29702;&#30340;&#36827;&#23637;&#65292;&#22312;&#19981;&#25913;&#21464;&#27169;&#22411;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#19968;&#22871;&#22810;&#26679;&#65292;&#36924;&#30495;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#22270;&#20687;&#22686;&#21152;&#20102;&#19968;&#20010;IID&#27979;&#35797;&#38598;&#21512;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#23545;&#22810;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#26174;&#33879;&#32780;&#19968;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#31867;&#22411;&#32534;&#36753;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#25581;&#31034;ImageNet&#20013;&#20808;&#21069;&#26410;&#30693;&#30340;&#31867;&#21035;&#23618;&#27425;&#27169;&#22411;&#20559;&#24046;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/virajprabhu/lance&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an automated algorithm to stress-test a trained visual model by generating language-guided counterfactual test images (LANCE). Our method leverages recent progress in large language modeling and text-based image editing to augment an IID test set with a suite of diverse, realistic, and challenging test images without altering model weights. We benchmark the performance of a diverse set of pre-trained models on our generated data and observe significant and consistent performance drops. We further analyze model sensitivity across different types of edits, and demonstrate its applicability at surfacing previously unknown class-level model biases in ImageNet. Code is available at https://github.com/virajprabhu/lance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KARD&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21521;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21152;&#20837;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#26816;&#32034;&#21040;&#30340;&#22686;&#24378;&#30693;&#35782;&#26469;&#35299;&#20915;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#20013;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18395</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#25512;&#29702;&#33976;&#39311;&#65306;&#38754;&#21521;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks. (arXiv:2305.18395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KARD&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21521;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21152;&#20837;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#26816;&#32034;&#21040;&#30340;&#22686;&#24378;&#30693;&#35782;&#26469;&#35299;&#20915;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#20013;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#22797;&#21512;&#30693;&#35782;&#29702;&#35299;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#35745;&#31639;&#35201;&#27714;&#39640;&#19988;&#28041;&#21450;&#25968;&#25454;&#38544;&#31169;&#65292;&#23558;&#27492;&#31867;&#27169;&#22411;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#36890;&#36807;&#24494;&#35843;&#20855;&#26377;&#26631;&#35760;&#25968;&#25454;&#25110;&#33976;&#39311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26500;&#24314;&#20219;&#21153;&#29305;&#23450;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#26159;&#30001;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#25152;&#38656;&#30693;&#35782;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#12290;&#22312;&#29702;&#35770;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#22686;&#24378;&#30340;&#25512;&#29702;&#33976;&#39311; (KARD) &#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24494;&#35843;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#26816;&#32034;&#21040;&#30340;&#22686;&#24378;&#30693;&#35782;&#30340;&#20381;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#37325;&#25490;&#22120;&#65292;&#29992;&#20110;&#33719;&#24471;&#19982;&#20381;&#25454;&#29983;&#25104;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;KARD&#22312;&#19977;&#39033;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#26174;&#30528;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#23610;&#23544;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#36798;&#21040;&#19982;LLMs&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promising performance in knowledge-intensive reasoning tasks that require a compound understanding of knowledge. However, deployment of the LLMs in real-world applications can be challenging due to their high computational requirements and concerns on data privacy. Previous studies have focused on building task-specific small language models (LMs) by fine-tuning them with labeled data or distilling LLMs. However, these approaches are ill-suited for knowledge-intensive reasoning tasks due to the limited capacity of small LMs in memorizing the knowledge required. Motivated by our theoretical analysis on memorization, we propose Knowledge-Augmented Reasoning Distillation (KARD), a novel method that fine-tunes small LMs to generate rationales with augmented knowledge retrieved from an external knowledge base. Moreover, we further propose a neural reranker to obtain documents relevant to rationale generation. We empirically show that KARD significantl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;Transformers&#20013;&#30340;&#33258;&#21457;&#27169;&#22359;&#21270;&#29616;&#35937;&#65292;&#21457;&#29616;&#31070;&#32463;&#20803;&#21487;&#20197;&#36827;&#34892;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#24314;&#31435;&#36215;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#27492;&#32467;&#26500;&#21487;&#34987;&#26377;&#25928;&#25200;&#21160;&#12290;</title><link>http://arxiv.org/abs/2305.18390</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;Transformers&#20013;&#30340;&#33258;&#21457;&#27169;&#22359;&#21270;
&lt;/p&gt;
&lt;p&gt;
Emergent Modularity in Pre-trained Transformers. (arXiv:2305.18390v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;Transformers&#20013;&#30340;&#33258;&#21457;&#27169;&#22359;&#21270;&#29616;&#35937;&#65292;&#21457;&#29616;&#31070;&#32463;&#20803;&#21487;&#20197;&#36827;&#34892;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#24314;&#31435;&#36215;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#27492;&#32467;&#26500;&#21487;&#34987;&#26377;&#25928;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;Transformers&#20013;&#30340;&#27169;&#22359;&#21270;&#29305;&#24449;&#65292;&#36825;&#26159;&#20154;&#33041;&#20013;&#24120;&#35265;&#30340;&#29305;&#28857;&#65292;&#34987;&#35748;&#20026;&#23545;&#20110;&#26222;&#36941;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20027;&#35201;&#32771;&#34385;&#20102;&#27169;&#22359;&#21270;&#30340;&#20004;&#20010;&#20027;&#35201;&#29305;&#24449;&#65306;&#65288;1&#65289;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#19987;&#19994;&#21270;&#65306;&#25105;&#20204;&#35780;&#20272;&#20102;&#27599;&#20010;&#31070;&#32463;&#20803;&#26159;&#21542;&#20027;&#35201;&#19987;&#19994;&#21270;&#20110;&#26576;&#19968;&#21151;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#26159;&#30340;&#12290;&#65288;2&#65289;&#22522;&#20110;&#21151;&#33021;&#32858;&#31867;&#30340;&#31070;&#32463;&#20803;&#20998;&#32452;&#65306;&#25105;&#20204;&#25506;&#31350;&#20102;&#23558;&#31070;&#32463;&#20803;&#25353;&#21151;&#33021;&#20998;&#32452;&#30340;&#32467;&#26500;&#23547;&#25214;&#26041;&#27861;&#65292;&#27599;&#20010;&#27169;&#22359;&#22343;&#20026;&#20854;&#30456;&#24212;&#21151;&#33021;&#24037;&#20316;&#12290;&#37492;&#20110;&#21487;&#33021;&#23384;&#22312;&#30340;&#22823;&#37327;&#32467;&#26500;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#20102;&#20998;&#23618;&#19987;&#23478;&#27169;&#22411;&#36523;&#19978;&#65292;&#24182;&#23558;&#31070;&#32463;&#20803;&#21010;&#20998;&#20026;&#19987;&#23478;&#65292;&#36890;&#24120;&#20026;&#19981;&#21516;&#30340;&#36755;&#20837;&#28608;&#27963;&#19981;&#21516;&#30340;&#19987;&#23478;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#21151;&#33021;&#19987;&#23478;&#65292;&#32858;&#38598;&#20102;&#26576;&#19968;&#21151;&#33021;&#30340;&#31070;&#32463;&#20803;&#12290;&#27492;&#22806;&#65292;&#25200;&#21160;&#21151;&#33021;&#19987;&#23478;&#30340;&#28608;&#27963;&#26174;&#33879;&#24433;&#21709;&#20102;&#30456;&#24212;&#30340;f&#38190;
&lt;/p&gt;
&lt;p&gt;
This work examines the presence of modularity in pre-trained Transformers, a feature commonly found in human brains and thought to be vital for general intelligence. In analogy to human brains, we consider two main characteristics of modularity: (1) functional specialization of neurons: we evaluate whether each neuron is mainly specialized in a certain function, and find that the answer is yes. (2) function-based neuron grouping: we explore finding a structure that groups neurons into modules by function, and each module works for its corresponding function. Given the enormous amount of possible structures, we focus on Mixture-of-Experts as a promising candidate, which partitions neurons into experts and usually activates different experts for different inputs. Experimental results show that there are functional experts, where clustered are the neurons specialized in a certain function. Moreover, perturbing the activations of functional experts significantly affects the corresponding f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SUFO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24494;&#35843;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22810;&#31181;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17588</link><description>&lt;p&gt;
&#35786;&#26029;&#21464;&#21387;&#22120;&#65306;&#25581;&#31034;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290; (arXiv:2305.17588v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making. (arXiv:2305.17588v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17588
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SUFO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24494;&#35843;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22810;&#31181;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#20026;&#20102;&#24314;&#31435;&#20449;&#20219;&#21644;&#30830;&#20445;&#23433;&#20840;&#65292;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#20351;&#29992;&#26377;&#38480;&#30340;&#20020;&#24202;&#35760;&#24405;&#23545;&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#36827;&#34892;&#24494;&#35843;&#20197;&#36741;&#21161;&#20020;&#24202;&#20915;&#31574;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SUFO&#30340;&#31995;&#32479;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22686;&#24378;&#20102;&#24494;&#35843;&#30340;&#21464;&#21387;&#22120;&#29305;&#24449;&#31354;&#38388;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;SUFO&#21033;&#29992;&#19968;&#31995;&#21015;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#21253;&#25324;&#30417;&#30563;&#25506;&#32034;&#12289;&#26080;&#30417;&#30563;&#30456;&#20284;&#24615;&#20998;&#26512;&#12289;&#29305;&#24449;&#21160;&#24577;&#21644;&#24322;&#24120;&#20540;&#20998;&#26512;&#65292;&#26469;&#35299;&#20915;&#20851;&#20110;&#27169;&#22411;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#30495;&#23454;&#19990;&#30028;&#30149;&#29702;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;MedNLI&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20116;&#20010;110M&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#20998;&#20026;&#36890;&#29992;&#39046;&#22495;&#65288;BERT, TNLR&#65289;&#12289;&#28151;&#21512;&#39046;&#22495;&#65288;BioBERT, Clinical BioBERT&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#65288;PubMedBERT&#65289;&#32452;&#12290;&#25105;&#20204;&#30340;SUFO&#20998;&#26512;&#25581;&#31034;&#20102;&#65306;(1)
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability. We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20013;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22823;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#24182;&#22312;&#23545;&#40784;&#22522;&#20934;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#23637;&#31034;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16960</link><description>&lt;p&gt;
&#22312;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20013;&#35757;&#32451;&#31038;&#20250;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Socially Aligned Language Models in Simulated Human Society. (arXiv:2305.16960v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20013;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22823;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#24182;&#22312;&#23545;&#40784;&#22522;&#20934;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#23637;&#31034;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#20013;&#30340;&#31038;&#20250;&#23545;&#40784;&#26088;&#22312;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#25353;&#29031;&#26082;&#23450;&#30340;&#31038;&#20250;&#20215;&#20540;&#34892;&#20107;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#19981;&#21516;&#65292;&#20154;&#20204;&#36890;&#36807;&#31038;&#20132;&#20114;&#21160;&#24471;&#20986;&#23545;&#20215;&#20540;&#21028;&#26029;&#30340;&#20849;&#35782;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21017;&#22312;&#23396;&#31435;&#22320;&#22797;&#21046;&#20854;&#35757;&#32451;&#35821;&#26009;&#24211;&#26102;&#34987;&#35757;&#32451;&#20986;&#26469;&#65292;&#23548;&#33268;&#22312;&#38476;&#29983;&#22330;&#26223;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#20801;&#35768;LMs&#20174;&#27169;&#25311;&#30340;&#31038;&#20132;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22823;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#22312;&#23545;&#40784;&#22522;&#20934;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#23637;&#31034;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;LMs&#35757;&#32451;&#20013;&#30340;&#33539;&#24335;&#36716;&#21464;&#20351;&#25105;&#20204;&#31163;&#24320;&#21457;&#33021;&#22815;&#24378;&#26377;&#21147;&#19988;&#20934;&#30830;&#21453;&#26144;&#31038;&#20250;&#35268;&#33539;&#21644;&#20215;&#20540;&#30340;AI&#31995;&#32479;&#26356;&#36817;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social alignment in AI systems aims to ensure that these models behave according to established societal values. However, unlike humans, who derive consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks. This work presents a novel training paradigm that permits LMs to learn from simulated social interactions. In comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations. This paradigm shift in the training of LMs brings us a step closer to developing AI systems that can robustly and accurately reflect societal norms and values.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22312;&#26368;&#30495;&#23454;&#21644;&#39640;&#39118;&#38505;&#30340;&#24773;&#22659;&#20013;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#20316;&#32773;&#39318;&#20808;&#26500;&#24314;&#26377;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#31227;&#21040;&#20854;&#20182;&#27169;&#22411;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#35266;&#23519;&#21040;&#40657;&#30418;&#26597;&#35810;&#21487;&#20197;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16934</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Evaluating Adversarial Robustness of Large Vision-Language Models. (arXiv:2305.16934v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16934
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22312;&#26368;&#30495;&#23454;&#21644;&#39640;&#39118;&#38505;&#30340;&#24773;&#22659;&#20013;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#20316;&#32773;&#39318;&#20808;&#26500;&#24314;&#26377;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#31227;&#21040;&#20854;&#20182;&#27169;&#22411;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#35266;&#23519;&#21040;&#40657;&#30418;&#26597;&#35810;&#21487;&#20197;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22914;GPT-4&#22312;&#29983;&#25104;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#35270;&#35273;&#36755;&#20837;&#26041;&#38754;&#65292;&#20351;&#24471;&#20132;&#20114;&#26356;&#26377;&#21019;&#36896;&#21147;&#21644;&#36866;&#24212;&#24615;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#29983;&#25104;&#21152;&#21095;&#20102;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#23545;&#25163;&#21487;&#20197;&#36890;&#36807;&#24494;&#22937;&#22320;&#25805;&#32437;&#26368;&#26131;&#21463;&#25915;&#20987;&#30340;&#27169;&#24577;&#65288;&#20363;&#22914;&#35270;&#35273;&#65289;&#25104;&#21151;&#36991;&#24320;&#25972;&#20010;&#31995;&#32479;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#26368;&#30495;&#23454;&#21644;&#39640;&#39118;&#38505;&#30340;&#24773;&#22659;&#19979;&#35780;&#20272;&#24320;&#28304;&#22823;&#22411;VLMs&#30340;&#40065;&#26834;&#24615;&#65292;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#40657;&#30418;&#35775;&#38382;&#31995;&#32479;&#65292;&#24182;&#35797;&#22270;&#27450;&#39575;&#27169;&#22411;&#36820;&#22238;&#30446;&#26631;&#21709;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#38024;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;CLIP&#21644;BLIP&#65289;&#26500;&#24314;&#26377;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#36716;&#31227;&#21040;&#20854;&#20182;VLMs&#65288;&#22914;MiniGPT-4&#12289;LLaVA&#12289;UniDiffuser&#12289;BLIP-2&#21644;Img2Prompt&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#36825;&#20123;VLMs&#19978;&#36827;&#34892;&#40657;&#30418;&#26597;&#35810;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.16380</link><description>&lt;p&gt;
&#25195;&#25551;&#19982;&#25293;&#29031;&#65306;&#29702;&#35299;1&#23618;Transformer&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26631;&#35760;&#32452;&#25104;
&lt;/p&gt;
&lt;p&gt;
Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#22312;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20854;&#22914;&#20309;&#24037;&#20316;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#39044;&#27979;&#24615;&#25439;&#22833;&#65292;&#34920;&#31034;&#22914;&#20309;&#20174;&#26799;&#24230;&#35757;&#32451;&#21160;&#24577;&#20013;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#38024;&#23545;&#20855;&#26377;&#19968;&#20010;&#33258;&#25105;&#20851;&#27880;&#23618;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;1&#23618;Transformer&#65292;&#25105;&#20204;&#20197;&#25968;&#23398;&#20005;&#35880;&#30340;&#26041;&#24335;&#20998;&#26512;&#20854;&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#25171;&#24320;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#32452;&#21512;&#36755;&#20837;&#26631;&#35760;&#30340;&#21160;&#24577;&#36807;&#31243;&#30340;&#40657;&#30418;&#23376;&#65292;&#24182;&#25581;&#31034;&#20102;&#24213;&#23618;&#24402;&#32435;&#20559;&#24046;&#30340;&#26412;&#36136;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#12289;&#38271;&#36755;&#20837;&#24207;&#21015;&#21644;&#35299;&#30721;&#22120;&#23618;&#23398;&#20064;&#36895;&#24230;&#24555;&#20110;&#33258;&#25105;&#20851;&#27880;&#23618;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65306;&#20174;&#22343;&#21248;&#27880;&#24847;&#21147;&#24320;&#22987;&#65292;&#23427;&#36880;&#28176;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#65292;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#30452;&#21040;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#34987;&#25195;&#25551;&#24182;&#24635;&#32467;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#26174;&#31034;&#20102;&#26631;&#35760;&#39057;&#29575;&#21644;&#19978;&#19979;&#25991;&#22914;&#20309;&#24433;&#21709;&#27880;&#24847;&#26435;&#37325;&#65292;&#20197;&#21450;&#33258;&#25105;&#20851;&#27880;&#23618;&#21021;&#22987;&#21270;&#22914;&#20309;&#24433;&#21709;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends mor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#27867;&#21270;&#12289;&#19981;&#30830;&#23450;&#24615;&#21644;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23547;&#27714;&#35299;&#20915;&#20551;&#26032;&#38395;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;GPT-4&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#27867;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#22312;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#12289;&#28201;&#24230;&#12289;&#25552;&#31034;&#12289;&#29256;&#26412;&#25511;&#21046;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#32593;&#32476;&#26816;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#38469;&#35265;&#35299;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#24067;&#20102;&#26032;&#39062;&#30340;&#33521;&#27861;&#37197;&#23545;&#20551;&#26032;&#38395;&#25968;&#25454;&#38598;LIAR-New&#65292;&#20026;&#20449;&#24687;&#30495;&#23454;&#24615;&#35780;&#20272;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2305.14928</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#38752;&#30340;&#20551;&#26032;&#38395;&#32531;&#35299;&#65306;&#27867;&#21270;&#65292;&#19981;&#30830;&#23450;&#24615;&#21644;GPT-4
&lt;/p&gt;
&lt;p&gt;
Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4. (arXiv:2305.14928v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#27867;&#21270;&#12289;&#19981;&#30830;&#23450;&#24615;&#21644;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23547;&#27714;&#35299;&#20915;&#20551;&#26032;&#38395;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;GPT-4&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#27867;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#22312;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#12289;&#28201;&#24230;&#12289;&#25552;&#31034;&#12289;&#29256;&#26412;&#25511;&#21046;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#32593;&#32476;&#26816;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#38469;&#35265;&#35299;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#24067;&#20102;&#26032;&#39062;&#30340;&#33521;&#27861;&#37197;&#23545;&#20551;&#26032;&#38395;&#25968;&#25454;&#38598;LIAR-New&#65292;&#20026;&#20449;&#24687;&#30495;&#23454;&#24615;&#35780;&#20272;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#31038;&#20250;&#25361;&#25112;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#23578;&#26410;&#25214;&#21040;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20851;&#27880;&#27867;&#21270;&#65292;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20415;&#22312;&#26080;&#27861;&#23436;&#32654;&#20998;&#31867;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#26356;&#23454;&#29992;&#30340;&#24037;&#20855;&#26469;&#35780;&#20272;&#20449;&#24687;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;GPT-4&#22312;&#22810;&#20010;&#35774;&#23450;&#21644;&#35821;&#35328;&#20013;&#21487;&#20197;&#32988;&#36807;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25506;&#32034;&#27867;&#21270;&#65292;&#25581;&#31034;&#20102;GPT-4&#21644;RoBERTa-large&#22312;&#22833;&#25928;&#27169;&#24335;&#19978;&#30340;&#24046;&#24322;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#26816;&#27979;&#21040;&#19981;&#21487;&#33021;&#30340;&#20363;&#23376;&#24182;&#26174;&#33879;&#25913;&#36827;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#65292;&#28201;&#24230;&#65292;&#25552;&#31034;&#65292;&#29256;&#26412;&#25511;&#21046;&#65292;&#21487;&#35299;&#37322;&#24615;&#21644;&#32593;&#32476;&#26816;&#32034;&#30340;&#32467;&#26524;&#65292;&#27599;&#20010;&#32467;&#26524;&#37117;&#25552;&#20379;&#20102;&#23454;&#38469;&#30340;&#35265;&#35299;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#20855;&#26377;&#26032;&#39062;&#30340;&#33521;&#27861;&#37197;&#23545;&#20551;&#26032;&#38395;&#25968;&#25454;&#21644;&#21487;&#34892;&#24615;&#26631;&#31614;&#30340;LIAR-New&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation poses a critical societal challenge, and current approaches have yet to produce an effective solution. We propose focusing on generalization, uncertainty, and how to leverage recent large language models, in order to create more practical tools to evaluate information veracity in contexts where perfect classification is impossible. We first demonstrate that GPT-4 can outperform prior methods in multiple settings and languages. Next, we explore generalization, revealing that GPT-4 and RoBERTa-large exhibit differences in failure modes. Third, we propose techniques to handle uncertainty that can detect impossible examples and strongly improve outcomes. We also discuss results on other language models, temperature, prompting, versioning, explainability, and web retrieval, each one providing practical insights and directions for future research. Finally, we publish the LIAR-New dataset with novel paired English and French misinformation data and Possibility labels that indic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#23545;&#27604;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#21333;&#20010;&#21521;&#37327;&#34920;&#31034;&#26631;&#35821;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#22788;&#29702;&#26356;&#21152;&#22797;&#26434;&#30340;&#36755;&#20837;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#26576;&#20123;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#32534;&#30721;&#22120;&#12290;&#20165;&#22522;&#20110;&#25991;&#26412;&#30340;&#24674;&#22797;&#24615;&#33021;&#33021;&#22815;&#39044;&#27979;&#22810;&#27169;&#24577;&#21305;&#37197;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14897</link><description>&lt;p&gt;
&#25991;&#26412;&#32534;&#30721;&#22120;&#38480;&#21046;&#20102;&#23545;&#27604;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Text encoders bottleneck compositionality in contrastive vision-language models. (arXiv:2305.14897v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#23545;&#27604;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#21333;&#20010;&#21521;&#37327;&#34920;&#31034;&#26631;&#35821;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#22788;&#29702;&#26356;&#21152;&#22797;&#26434;&#30340;&#36755;&#20837;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#26576;&#20123;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#32534;&#30721;&#22120;&#12290;&#20165;&#22522;&#20110;&#25991;&#26412;&#30340;&#24674;&#22797;&#24615;&#33021;&#33021;&#22815;&#39044;&#27979;&#22810;&#27169;&#24577;&#21305;&#37197;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24615;&#33021;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VL&#65289;&#22914;CLIP&#20351;&#29992;&#21333;&#19968;&#21521;&#37327;&#34920;&#31034;&#26631;&#39064;&#12290;&#22312;&#36825;&#20010;&#29942;&#39048;&#20013;&#22833;&#21435;&#20102;&#22810;&#23569;&#20851;&#20110;&#35821;&#35328;&#30340;&#20449;&#24687;&#65311;&#25105;&#20204;&#39318;&#20808;&#31574;&#21010;&#20102;CompPrompts&#65292;&#36825;&#26159;&#19968;&#32452;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#22270;&#20687;&#26631;&#39064;&#65292;VL&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#25429;&#25417;&#21040;&#65288;&#20363;&#22914;&#65292;&#21333;&#20010;&#23545;&#35937;&#65292;&#21040;&#23545;&#35937;+&#23646;&#24615;&#65292;&#21040;&#22810;&#20010;&#20114;&#21160;&#23545;&#35937;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#20165;&#22522;&#20110;&#25991;&#26412;&#30340;&#24674;&#22797;&#25506;&#38024;&#65292;&#26088;&#22312;&#20174;&#20960;&#20010;VL&#27169;&#22411;&#29983;&#25104;&#30340;&#21333;&#19968;&#21521;&#37327;&#25991;&#26412;&#34920;&#31034;&#20013;&#37325;&#24314;&#26631;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#22270;&#20687;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#26356;&#24191;&#27867;&#30340;&#22330;&#26223;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;CLIP&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#26356;&#22797;&#26434;&#30340;&#36755;&#20837;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#21253;&#25324;&#23545;&#35937;&#20851;&#31995;&#12289;&#23646;&#24615;-&#23545;&#35937;&#20851;&#32852;&#12289;&#35745;&#25968;&#21644;&#21542;&#23450;&#65307;2&#65289;&#19968;&#20123;&#25991;&#26412;&#32534;&#30721;&#22120;&#27604;&#20854;&#20182;&#32534;&#30721;&#22120;&#35201;&#22909;&#24471;&#22810;&#65307;3&#65289;&#20165;&#22522;&#20110;&#25991;&#26412;&#30340;&#24674;&#22797;&#24615;&#33021;&#39044;&#27979;&#20102;ControlledImCaps&#19978;&#30340;&#22810;&#27169;&#24577;&#21305;&#37197;&#24615;&#33021;&#65306;&#36825;&#26159;&#25105;&#20204;&#25910;&#38598;&#21644;&#21457;&#24067;&#30340;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performant vision-language (VL) models like CLIP represent captions using a single vector. How much information about language is lost in this bottleneck? We first curate CompPrompts, a set of increasingly compositional image captions that VL models should be able to capture (e.g., single object, to object+property, to multiple interacting objects). Then, we train text-only recovery probes that aim to reconstruct captions from single-vector text representations produced by several VL models. This approach does not require images, allowing us to test on a broader range of scenes compared to prior work. We find that: 1) CLIP's text encoder falls short on more compositional inputs, including object relationships, attribute-object association, counting, and negations; 2) some text encoders work significantly better than others; and 3) text-only recovery performance predicts multi-modal matching performance on ControlledImCaps: a new evaluation benchmark we collect and release consisting of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20934;&#27979;&#35797;MQuAKE&#65292;&#36890;&#36807;&#22810;&#36339;&#38382;&#39064;&#35780;&#20272;&#32534;&#36753;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#22238;&#31572;&#22240;&#32534;&#36753;&#20107;&#23454;&#32780;&#31572;&#26696;&#24212;&#35813;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#21484;&#22238;&#24050;&#32534;&#36753;&#30340;&#20107;&#23454;&#65292;&#20294;&#22312;&#22810;&#36339;&#38382;&#39064;&#19978;&#34920;&#29616;&#28798;&#38590;&#24615;&#22833;&#36133;&#12290;</title><link>http://arxiv.org/abs/2305.14795</link><description>&lt;p&gt;
MQuAKE&#65306;&#36890;&#36807;&#22810;&#36339;&#38382;&#39064;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions. (arXiv:2305.14795v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20934;&#27979;&#35797;MQuAKE&#65292;&#36890;&#36807;&#22810;&#36339;&#38382;&#39064;&#35780;&#20272;&#32534;&#36753;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#22238;&#31572;&#22240;&#32534;&#36753;&#20107;&#23454;&#32780;&#31572;&#26696;&#24212;&#35813;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#21484;&#22238;&#24050;&#32534;&#36753;&#30340;&#20107;&#23454;&#65292;&#20294;&#22312;&#22810;&#36339;&#38382;&#39064;&#19978;&#34920;&#29616;&#28798;&#38590;&#24615;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#23384;&#20648;&#30340;&#20449;&#24687;&#24456;&#24555;&#23601;&#20250;&#36807;&#26102;&#65292;&#37325;&#26032;&#35757;&#32451;&#24182;&#38750;&#24635;&#26159;&#21487;&#34892;&#30340;&#36873;&#25321;&#12290;&#36825;&#20419;&#20351;&#20154;&#20204;&#24320;&#21457;&#20102;&#36890;&#36807;&#26356;&#26032;&#27169;&#22411;&#26435;&#37325;&#27880;&#20837;&#26032;&#20107;&#23454;&#30340;&#19968;&#31995;&#21015;&#25216;&#26415;&#12290;&#24403;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#38750;&#24120;&#26377;&#38480;&#65292;&#20027;&#35201;&#39564;&#35777;&#32534;&#36753;&#20107;&#23454;&#30340;&#21484;&#22238;&#29575;&#65292;&#20294;&#26356;&#25913;&#19968;&#20010;&#20107;&#23454;&#24212;&#35813;&#20250;&#23545;&#27169;&#22411;&#30340;&#30456;&#20851;&#20449;&#24565;&#20135;&#29983;&#36830;&#38145;&#21453;&#24212;&#12290;&#22914;&#26524;&#25105;&#20204;&#32534;&#36753;&#33521;&#22269;&#39318;&#30456;&#20026;Rishi Sunak&#65292;&#37027;&#20040;&#23545;&#20110;&#8220;&#35841;&#26159;&#33521;&#22269;&#39318;&#30456;&#30340;&#37197;&#20598;&#8221;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24212;&#35813;&#24471;&#21040;&#19968;&#20010;&#19981;&#21516;&#30340;&#31572;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;MQuAKE&#65288;&#29992;&#20110;&#30693;&#35782;&#32534;&#36753;&#30340;&#22810;&#36339;&#38382;&#31572;&#65289;&#65292;&#21253;&#25324;&#22810;&#36339;&#38382;&#39064;&#65292;&#35780;&#20272;&#32534;&#36753;&#21518;&#30340;&#27169;&#22411;&#26159;&#21542;&#27491;&#30830;&#22238;&#31572;&#37027;&#20123;&#22240;&#32534;&#36753;&#20107;&#23454;&#32780;&#31572;&#26696;&#24212;&#35813;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#21484;&#22238;&#24050;&#32534;&#36753;&#30340;&#20107;&#23454;&#65292;&#20294;&#23427;&#20204;&#22312;&#26500;&#24314;&#30340;&#22810;&#36339;&#38382;&#39064;&#19978;&#36973;&#36935;&#20102;&#28798;&#38590;&#24615;&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#23545;LLMs&#30340;&#35780;&#20272;&#24517;&#39035;&#36229;&#36234;&#31616;&#21333;&#30340;&#20107;&#23454;&#21484;&#22238;&#65292;&#24182;&#32435;&#20837;&#26356;&#24494;&#22937;&#30340;&#30693;&#35782;&#32534;&#36753;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model's related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister? In this work, we present a benchmark MQuAKE (Multi-hop Question Answering for Knowledge Editing) comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions. We thus 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38382;&#31572;&#30340;&#21435;&#25991;&#26412;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23637;&#31034;&#25552;&#21462;&#30340;&#25991;&#26412;&#25688;&#24405;&#12290;&#22312;&#38382;&#31572;&#21644;&#24341;&#35777;&#19978;&#30340;&#34920;&#29616;&#31867;&#20284;&#20110;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#21450;&#20559;&#22909;&#30340;&#21487;&#25511;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14772</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25511;&#30340;&#22522;&#20110;&#38382;&#31572;&#30340;&#21435;&#25991;&#26412;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Controllable QA-based Framework for Decontextualization. (arXiv:2305.14772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38382;&#31572;&#30340;&#21435;&#25991;&#26412;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23637;&#31034;&#25552;&#21462;&#30340;&#25991;&#26412;&#25688;&#24405;&#12290;&#22312;&#38382;&#31572;&#21644;&#24341;&#35777;&#19978;&#30340;&#34920;&#29616;&#31867;&#20284;&#20110;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#21450;&#20559;&#22909;&#30340;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#38656;&#35201;&#23558;&#25552;&#21462;&#30340;&#25688;&#24405;&#23637;&#31034;&#32473;&#29992;&#25143;&#65292;&#36825;&#20123;&#25688;&#24405;&#24448;&#24448;&#38656;&#35201;&#35299;&#32806;&#21407;&#26469;&#30340;&#25991;&#26412;&#25165;&#33021;&#26356;&#22909;&#22320;&#21576;&#29616;&#32473;&#29992;&#25143;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#38382;&#31572;&#21644;&#24341;&#35777;&#19978;&#30340;&#21435;&#25991;&#26412;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38382;&#31572;&#30340;&#21435;&#25991;&#26412;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26356;&#22909;&#22320;&#28385;&#36275;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#21450;&#20559;&#22909;&#65292;&#24182;&#19988;&#22312;&#32467;&#26524;&#19978;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;&#31471;&#21040;&#31471;&#26041;&#27861;&#30340;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#21516;&#26102;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#35813;&#26694;&#26550;&#23558;&#29992;&#25143;&#20559;&#22909;&#34701;&#20837;&#21040;&#31995;&#32479;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world applications require surfacing extracted snippets to users, whether motivated by assistive tools for literature surveys or document cross-referencing, or needs to mitigate and recover from model generated inaccuracies., Yet, these passages can be difficult to consume when divorced from their original document context. In this work, we explore the limits of LLMs to perform decontextualization of document snippets in user-facing scenarios, focusing on two real-world settings - question answering and citation context previews for scientific documents. We propose a question-answering framework for decontextualization that allows for better handling of user information needs and preferences when determining the scope of rewriting. We present results showing state-of-the-art LLMs under our framework remain competitive with end-to-end approaches. We also explore incorporating user preferences into the system, finding our framework allows for controllability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;Bradley-Terry-Luce&#27169;&#22411;&#23545;OpenAI&#20844;&#24067;&#30340;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#20013;&#25152;&#34164;&#21547;&#30340;&#22266;&#26377;&#20559;&#22909;&#65292;&#25552;&#20986;&#20102;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#31574;&#30053;&#65292;&#24182;&#20026;&#26500;&#24314;&#24179;&#34913;&#30340;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.14702</link><description>&lt;p&gt;
&#36890;&#36807;GPT-4&#20998;&#26512;&#24433;&#21709;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#30340;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Analyzing Influential Factors in Human Preference Judgments via GPT-4. (arXiv:2305.14702v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;Bradley-Terry-Luce&#27169;&#22411;&#23545;OpenAI&#20844;&#24067;&#30340;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#20013;&#25152;&#34164;&#21547;&#30340;&#22266;&#26377;&#20559;&#22909;&#65292;&#25552;&#20986;&#20102;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#31574;&#30053;&#65292;&#24182;&#20026;&#26500;&#24314;&#24179;&#34913;&#30340;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#22312;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#21644;&#35780;&#20272;&#33258;&#21160;&#25688;&#35201;&#24230;&#37327;&#26041;&#38754;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20123;&#20559;&#22909;&#21028;&#26029;&#30340;&#20849;&#21516;&#24433;&#21709;&#21644;&#22240;&#32032;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#31561;&#38382;&#39064;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20173;&#36739;&#20026;&#26377;&#38480;&#12290;&#26412;&#25991;&#21033;&#29992;Bradley-Terry-Luce&#27169;&#22411;&#23545;OpenAI&#20844;&#24067;&#30340;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#35782;&#21035;&#20102;&#21487;&#33021;&#24433;&#21709;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#20013;&#25152;&#34164;&#21547;&#30340;&#22266;&#26377;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#20102;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#31574;&#30053;&#65292;&#26368;&#21518;&#23545;&#20110;&#22914;&#20309;&#26500;&#24314;&#24179;&#34913;&#30340;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pairwise human judgments are pivotal in guiding large language models (LLMs) to generate outputs that align with human preferences. They are also often used in summarization evaluation, complementing existing automatic metrics. Despite their significance, however, there has been limited research probing these pairwise human judgments. The collective impact and respective weights of factors such as informativeness, coherence, fluency, and factual consistency remain elusive. The impact of hidden factors on the final judgment is also unclear. In this paper, we conduct an in-depth examination of a dataset of pairwise human judgments released by OpenAI. Utilizing the Bradley-Terry-Luce model, we identify key factors that could potentially influence human judgments. Our research uncovers the inherent preferences embedded in human judgments and suggests strategies to boost sample efficiency. Finally, we provide insights on the construction of balanced datasets for human judgment evaluations, 
&lt;/p&gt;</description></item><item><title>ALGO&#26694;&#26550;&#20351;&#29992;&#30001;LLM&#29983;&#25104;&#30340;&#31070;&#35861;&#25351;&#23548;&#21019;&#36896;&#21644;&#39564;&#35777;&#31639;&#27861;&#31243;&#24207;&#65292;&#20197;&#25552;&#39640;&#29616;&#26377;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#31639;&#27861;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14591</link><description>&lt;p&gt;
ALGO&#65306;&#20351;&#29992;&#29983;&#25104;&#30340;&#31070;&#35861;&#39564;&#35777;&#31243;&#24207;&#30340;&#21512;&#25104;&#31639;&#27861;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers. (arXiv:2305.14591v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14591
&lt;/p&gt;
&lt;p&gt;
ALGO&#26694;&#26550;&#20351;&#29992;&#30001;LLM&#29983;&#25104;&#30340;&#31070;&#35861;&#25351;&#23548;&#21019;&#36896;&#21644;&#39564;&#35777;&#31639;&#27861;&#31243;&#24207;&#65292;&#20197;&#25552;&#39640;&#29616;&#26377;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#31639;&#27861;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large language models, LLMs)&#22312;&#23454;&#29616;&#20195;&#30721;&#30340;&#21151;&#33021;&#25551;&#36848;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#38656;&#35201;&#30830;&#23450;&#36866;&#24403;&#31639;&#27861;&#30340;&#31639;&#27861;&#38382;&#39064;&#19978;&#20127;&#38656;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;LLM&#29983;&#25104;&#30340;&#31243;&#24207;&#32570;&#20047;&#20445;&#35777;&#27491;&#30830;&#24615;&#24182;&#38656;&#35201;&#20154;&#24037;&#39564;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ALGO&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#30001;LLM&#29983;&#25104;&#30340;&#31070;&#35861;&#25351;&#23548;&#21019;&#36896;&#21644;&#39564;&#35777;&#31639;&#27861;&#31243;&#24207;&#12290;ALGO&#39318;&#20808;&#36890;&#36807;&#20419;&#20351;LLM&#26522;&#20030;&#30456;&#20851;&#21464;&#37327;&#30340;&#25152;&#26377;&#32452;&#21512;&#26469;&#29983;&#25104;&#20855;&#26377;&#21487;&#33021;&#30340;&#27491;&#30830;&#24615;&#20294;&#21487;&#33021;&#36739;&#24930;&#30340;&#21442;&#32771;&#31070;&#35861;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#35813;&#31070;&#35861;&#25351;&#23548;&#20219;&#24847;&#25628;&#32034;&#31574;&#30053;&#26469;&#25506;&#32034;&#31639;&#27861;&#31354;&#38388;&#24182;&#39564;&#35777;&#21512;&#25104;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#31070;&#35861;&#22312;88%&#30340;&#24773;&#20917;&#19979;&#26159;&#27491;&#30830;&#30340;&#12290;&#20351;&#29992;&#36825;&#20123;&#31070;&#35861;&#20316;&#20026;&#39564;&#35777;&#31243;&#24207;&#65292;ALGO&#21487;&#20197;&#20197;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#24335;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#65292;&#20197;&#25552;&#39640;&#20854;&#31639;&#27861;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) excel at implementing code from functionality descriptions, but struggle with algorithmic problems that require not only implementation but also identification of the suitable algorithm. Moreover, LLM-generated programs lack guaranteed correctness and require human verification. To address these challenges, we propose ALGO, a framework that synthesizes Algorithmic programs with LLM-Generated Oracles to guide the creation and verify their correctness. ALGO first generates a probably correct but possibly slow reference oracle by prompting an LLM to exhaustively enumerate all the combinations of relevant variables. This oracle is then utilized to guide an arbitrary search strategy in exploring the algorithm space and to verify the algorithms synthesized. Our study shows that the LLM-generated oracles are correct for 88% of the cases. With the oracles as verifiers, ALGO can be integrated with any existing code generation model in a model-agnostic manner to enha
&lt;/p&gt;</description></item><item><title>WikiChat&#26159;&#19968;&#31181;&#20197;&#23569;&#26679;&#26412;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#23545;&#32500;&#22522;&#30334;&#31185;&#36827;&#34892; grounding&#65292;&#23427;&#20960;&#20046;&#19981;&#20250;&#20135;&#29983;&#24187;&#35273;&#65292;&#20855;&#26377;&#39640;&#23545;&#35805;&#33021;&#21147;&#21644;&#20302;&#24310;&#36831;&#12290;WikiChat&#20174;LLM&#20013;&#29983;&#25104;&#21709;&#24212;&#65292;&#20445;&#30041;&#22522;&#20110;&#20107;&#23454;&#30340;&#20869;&#23481;&#65292;&#24182;&#20174;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#32467;&#21512;&#65292;&#24418;&#25104;&#30495;&#23454;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#22238;&#22797;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#23427;&#27604;&#20854;&#20182;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#33021;&#22312;&#27169;&#25311;&#23545;&#35805;&#20013;&#36798;&#21040;97.3%&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14292</link><description>&lt;p&gt;
WikiChat: &#36890;&#36807;&#23545;&#32500;&#22522;&#30334;&#31185;&#30340;&#23569;&#26679;&#26412;&#24341;&#20837;&#65292;&#38459;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia. (arXiv:2305.14292v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14292
&lt;/p&gt;
&lt;p&gt;
WikiChat&#26159;&#19968;&#31181;&#20197;&#23569;&#26679;&#26412;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#23545;&#32500;&#22522;&#30334;&#31185;&#36827;&#34892; grounding&#65292;&#23427;&#20960;&#20046;&#19981;&#20250;&#20135;&#29983;&#24187;&#35273;&#65292;&#20855;&#26377;&#39640;&#23545;&#35805;&#33021;&#21147;&#21644;&#20302;&#24310;&#36831;&#12290;WikiChat&#20174;LLM&#20013;&#29983;&#25104;&#21709;&#24212;&#65292;&#20445;&#30041;&#22522;&#20110;&#20107;&#23454;&#30340;&#20869;&#23481;&#65292;&#24182;&#20174;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#32467;&#21512;&#65292;&#24418;&#25104;&#30495;&#23454;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#22238;&#22797;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#23427;&#27604;&#20854;&#20182;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#33021;&#22312;&#27169;&#25311;&#23545;&#35805;&#20013;&#36798;&#21040;97.3%&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20960;&#20046;&#19981;&#20250;&#20135;&#29983;&#24187;&#35273;&#12289;&#20855;&#26377;&#39640;&#23545;&#35805;&#33021;&#21147;&#21644;&#20302;&#24310;&#36831;&#30340;&#22522;&#20110;&#23569;&#26679;&#26412;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#8212;&#8212;WikiChat&#12290;WikiChat&#22522;&#20110;&#33521;&#25991;&#32500;&#22522;&#30334;&#31185;&#36827;&#34892; grounding&#65292;&#36825;&#26159;&#26368;&#22823;&#30340;&#31934;&#36873;&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;WikiChat&#20174;LLM&#20013;&#29983;&#25104;&#21709;&#24212;&#65292;&#20165;&#20445;&#30041;&#22522;&#20110;&#20107;&#23454;&#30340;&#20869;&#23481;&#65292;&#24182;&#19982;&#20174;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#21040;&#30340;&#38468;&#21152;&#20449;&#24687;&#32467;&#21512;&#65292;&#24418;&#25104;&#30495;&#23454;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#22238;&#22797;&#12290;&#25105;&#20204;&#23558; WikiChat &#26681;&#25454; GPT-4 &#36827;&#34892;&#20102;&#33976;&#39311;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#21442;&#25968;&#20026;7B&#30340; LLaMA &#27169;&#22411;&#65292;&#20197;&#26497;&#23569;&#36136;&#37327;&#25439;&#22833;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#24310;&#36831;&#12289;&#25104;&#26412;&#21644;&#38544;&#31169;&#24615;&#65292;&#24182;&#20419;&#36827;&#20102;&#30740;&#31350;&#21644;&#37096;&#32626;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#20154;&#24037;&#21644;LLM&#35780;&#20272;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26368;&#20339;&#31995;&#32479;&#22312;&#27169;&#25311;&#23545;&#35805;&#20013;&#36798;&#21040;&#20102;97.3%&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#19982;&#25152;&#26377;&#22522;&#20110;&#26816;&#32034;&#21644;LLM&#30340;&#22522;&#20934;&#30456;&#27604;&#65292;&#23427;&#22312;&#22836;&#37096;&#12289;&#23614;&#37096;&#21644;&#26368;&#26032;&#30693;&#35782;&#26041;&#38754;&#20998;&#21035;&#25552;&#39640;&#20102;3.9%&#12289;38.6%&#21644;51.0%&#65292;&#19982;GPT-4&#30456;&#27604;&#12290;&#19982;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30456;&#27604;&#65292;WikiChat&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;...
&lt;/p&gt;
&lt;p&gt;
This paper presents the first few-shot LLM-based chatbot that almost never hallucinates and has high conversationality and low latency. WikiChat is grounded on the English Wikipedia, the largest curated free-text corpus.  WikiChat generates a response from an LLM, retains only the grounded facts, and combines them with additional information it retrieves from the corpus to form factual and engaging responses. We distill WikiChat based on GPT-4 into a 7B-parameter LLaMA model with minimal loss of quality, to significantly improve its latency, cost and privacy, and facilitate research and deployment.  Using a novel hybrid human-and-LLM evaluation methodology, we show that our best system achieves 97.3% factual accuracy in simulated conversations. It significantly outperforms all retrieval-based and LLM-based baselines, and by 3.9%, 38.6% and 51.0% on head, tail and recent knowledge compared to GPT-4. Compared to previous state-of-the-art retrieval-based chatbots, WikiChat is also signifi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25552;&#31034;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#35266;&#23519;&#30340;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#32593;&#32476;&#23548;&#33322;&#20013;&#30340;&#25928;&#26524;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26426;&#21046;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14257</link><description>&lt;p&gt;
&#20998;&#23618;&#25552;&#31034;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23548;&#33322;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Prompting Assists Large Language Model on Web Navigation. (arXiv:2305.14257v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14257
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25552;&#31034;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#35266;&#23519;&#30340;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#32593;&#32476;&#23548;&#33322;&#20013;&#30340;&#25928;&#26524;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26426;&#21046;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#22797;&#26434;&#35266;&#23519;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20998;&#23618;&#25552;&#31034;&#26041;&#27861;&#12290;&#19981;&#21516;&#20110;&#20197;&#24448;&#24635;&#26159;&#25226;\emph{&#23436;&#25972;}&#35266;&#23519;&#65288;&#20363;&#22914;&#32593;&#39029;&#65289;&#25918;&#21040;&#25552;&#31034;&#20013;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#19982;&#21160;&#20316;&#30456;&#20851;&#30340;\emph{&#21387;&#32553;}&#21644;\emph{&#30456;&#20851;}&#30340;&#35266;&#23519;&#65292;&#24182;&#20351;&#29992;&#19987;&#38376;&#30340;\summ&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;\actor&#25552;&#31034;&#26681;&#25454;&#24635;&#32467;&#30340;&#35266;&#23519;&#39044;&#27979;&#19979;&#19968;&#20010;&#21160;&#20316;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#20294;&#25105;&#20204;&#23588;&#20854;&#23637;&#31034;&#20102;&#23427;&#22312;&#22797;&#26434;&#30340;&#32593;&#32476;&#23548;&#33322;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#23436;&#25972;&#30340;&#35266;&#23519;&#36890;&#24120;&#21253;&#21547;&#20887;&#20313;&#21644;&#26080;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20219;&#21153;&#25104;&#21151;&#29575;&#19978;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26426;&#21046;6.2\%&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#20855;&#26377;&#38271;&#26102;&#38388;&#35266;&#23519;&#36712;&#36857;&#30340;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) struggle on processing complicated observations in interactive decision making tasks. To alleviate this issue, we propose a simple hierarchical prompting approach. Diverging from previous prompting approaches that always put the \emph{full} observation~(\eg a web page) to the prompt, we propose to first construct an action-aware observation which is more \emph{condensed} and \emph{relevant} with a dedicated \summ prompt. The \actor prompt then predicts the next action based on the summarized observation. While our method has broad applicability, we particularly demonstrate its efficacy in the complex domain of web navigation where a full observation often contains redundant and irrelevant information. Our approach outperforms the previous state-of-the-art prompting mechanis by 6.2\% on task success rate, demonstrating its potential on interactive decision making tasks with long observation traces.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REFLEX&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#28155;&#21152;&#20102;&#21512;&#29702;&#24615;&#21644;&#33258;&#21453;&#24615;&#23618;&#65292;&#20197;&#20351;&#27169;&#22411;&#30340;&#31572;&#26696;&#24471;&#20197;&#35299;&#37322;&#24182;&#28040;&#38500;&#28508;&#22312;&#30340;&#30683;&#30462;&#12290;</title><link>http://arxiv.org/abs/2305.14250</link><description>&lt;p&gt;
&#20855;&#26377;&#21512;&#29702;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Language Models with Rationality. (arXiv:2305.14250v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REFLEX&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#28155;&#21152;&#20102;&#21512;&#29702;&#24615;&#21644;&#33258;&#21453;&#24615;&#23618;&#65292;&#20197;&#20351;&#27169;&#22411;&#30340;&#31572;&#26696;&#24471;&#20197;&#35299;&#37322;&#24182;&#28040;&#38500;&#28508;&#22312;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Models, LLMs)&#22312;&#38382;&#31572;&#20013;&#38750;&#24120;&#25797;&#38271;&#65292;&#20294;&#23427;&#20204;&#30340;&#31572;&#26696;&#19982;&#20854;&#20869;&#22312;&#30340;&#8220;&#20449;&#24565;&#8221;&#20043;&#38388;&#30340;&#20851;&#31995;&#24448;&#24448;&#19981;&#26126;&#30830;&#12290;&#36825;&#31181;&#32570;&#20047;&#35299;&#37322;&#24615;&#38459;&#30861;&#20102;LLMs&#30340;&#24191;&#27867;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#27169;&#22411;&#30340;&#20449;&#24565;&#20197;&#21450;&#23427;&#20204;&#30340;&#25512;&#29702;&#20851;&#31995;&#21464;&#24471;&#26126;&#30830;&#65292;&#24182;&#28040;&#38500;&#21487;&#33021;&#23384;&#22312;&#30340;&#30683;&#30462;&#65292;&#20197;&#20415;&#31572;&#26696;&#33021;&#22815;&#36890;&#36807;&#20174;&#19968;&#33268;&#30340;&#20449;&#24565;&#32593;&#32476;&#20013;&#24471;&#20986;&#30340;&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#38142;&#26469;&#25903;&#25345;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;REFLEX&#65292;&#22312;LLM&#20043;&#19978;&#28155;&#21152;&#20102;&#19968;&#20010;&#20855;&#26377;&#21512;&#29702;&#24615;&#21644;&#33258;&#21453;&#24615;&#30340;&#23618;&#12290;&#39318;&#20808;&#65292;&#32473;&#23450;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#21453;&#21521;&#38142;&#25509;&#36807;&#31243;&#26500;&#24314;&#19968;&#20010;&#20449;&#24565;&#22270;&#65292;&#20197;&#23454;&#29616;&#30456;&#20851;&#27169;&#22411;&#20449;&#24565;(&#21253;&#25324;&#23545;&#31572;&#26696;&#20505;&#36873;&#32773;&#30340;&#20449;&#24565;)&#21450;&#20854;&#25512;&#29702;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#24418;&#24335;&#32422;&#26463;&#25512;&#29702;&#22120;&#35782;&#21035;&#21644;&#26368;&#23567;&#21270;&#35813;&#22270;&#20013;&#30340;&#30683;&#30462;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;REFLEX&#26174;&#33879;&#25552;&#39640;&#20102;&#19968;&#33268;&#24615;(&#32477;&#23545;&#20540;&#25552;&#21319;&#20102;8%-11%)&#65292;&#32780;&#19981;&#25439;&#23475;&#24050;&#26377;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) are proficient at question-answering (QA), it is not always clear how (or even if) an answer follows from their latent "beliefs". This lack of interpretability is a growing impediment to widespread use of LLMs. To address this, our goals are to make model beliefs and their inferential relationships explicit, and to resolve inconsistencies that may exist, so that answers are supported by interpretable chains of reasoning drawn from a consistent network of beliefs. Our approach, which we call REFLEX, is to add a rational, self-reflecting layer on top of the LLM. First, given a question, we construct a belief graph using a backward-chaining process to materialize relevant model beliefs (including beliefs about answer candidates) and their inferential relationships. Second, we identify and minimize contradictions in that graph using a formal constraint reasoner. We find that REFLEX significantly improves consistency (by 8%-11% absolute) without harming ov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#35299;&#30721;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#8220;&#22806;&#21521;&#20869;&#8221;&#29983;&#25104;&#24207;&#21015;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#19978;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#35813;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#22312;2022&#24180;&#21644;2023&#24180;&#20849;&#20139;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.12580</link><description>&lt;p&gt;
&#19968;&#31181;&#21452;&#21521;&#35299;&#30721;&#30340;&#26694;&#26550;&#65306;&#24418;&#24577;&#21464;&#21270;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Framework for Bidirectional Decoding: Case Study in Morphological Inflection. (arXiv:2305.12580v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#35299;&#30721;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#8220;&#22806;&#21521;&#20869;&#8221;&#29983;&#25104;&#24207;&#21015;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#19978;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#35813;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#22312;2022&#24180;&#21644;2023&#24180;&#20849;&#20139;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#22312;&#29983;&#25104;&#24207;&#21015;&#20219;&#21153;&#20013;&#20197;&#20174;&#24038;&#21040;&#21491;&#30340;&#26041;&#24335;&#24050;&#25104;&#20026;&#26631;&#20934;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#8220;&#22806;&#21521;&#20869;&#8221;&#29983;&#25104;&#24207;&#21015;&#30340;&#35299;&#30721;&#26694;&#26550;&#65306;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#65292;&#27169;&#22411;&#36873;&#25321;&#22312;&#24038;&#36793;&#12289;&#21491;&#36793;&#29983;&#25104;&#19968;&#20010;&#26631;&#35760;&#65292;&#25110;&#32773;&#23558;&#24038;&#21491;&#24207;&#21015;&#36830;&#25509;&#36215;&#26469;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#27604;&#20043;&#21069;&#30340;&#21452;&#21521;&#35299;&#30721;&#26356;&#26377;&#21407;&#21017;&#24615;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#25903;&#25345;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#21253;&#25324;&#20960;&#31181;&#35757;&#32451;&#26041;&#27861;&#65292;&#20363;&#22914;&#23558;&#28508;&#22312;&#30340;&#25490;&#24207;&#21464;&#37327;&#36793;&#32536;&#21270;&#30340;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;2022&#24180;&#21644;2023&#24180;&#20849;&#20139;&#20219;&#21153;&#20013;&#21019;&#36896;&#20102;&#26368;&#26032;&#25104;&#26524;(SOTA)&#65292;&#22312;&#24179;&#22343;&#20934;&#30830;&#24615;&#26041;&#38754;&#20998;&#21035;&#27604;&#20854;&#20182;&#26368;&#20339;&#31995;&#32479;&#39640;&#20986;4.7&#20010;&#21644;2.7&#20010;&#28857;&#12290;&#35813;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#38544;&#24335;&#23398;&#20064;&#30001;&#35789;&#24178;&#21644;&#35789;&#32512;&#32452;&#25104;&#30340;&#21333;&#35789;&#30340;&#20998;&#21106;&#28857;&#65292;&#24182;&#22312;&#20855;&#26377;&#36739;&#23569;&#21807;&#19968;&#35789;&#32032;&#30340;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#22522;&#32447;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based encoder-decoder models that generate outputs in a left-to-right fashion have become standard for sequence-to-sequence tasks. In this paper, we propose a framework for decoding that produces sequences from the "outside-in": at each step, the model chooses to generate a token on the left, on the right, or join the left and right sequences. We argue that this is more principled than prior bidirectional decoders. Our proposal supports a variety of model architectures and includes several training methods, such as a dynamic programming algorithm that marginalizes out the latent ordering variable. Our model sets state-of-the-art (SOTA) on the 2022 and 2023 shared tasks, beating the next best systems by over 4.7 and 2.7 points in average accuracy respectively. The model performs particularly well on long sequences, can implicitly learn the split point of words composed of stem and affix, and performs better relative to the baseline on datasets that have fewer unique lemmas (
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LogiCoT, &#19968;&#20010;&#22522;&#20110;GPT-4&#30340;&#36923;&#36753;&#24605;&#32500;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25945;&#25480;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;&#21644;&#24341;&#20986;&#19968;&#33324;&#25512;&#29702;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12147</link><description>&lt;p&gt;
LogiCoT&#65306;&#22522;&#20110;GPT-4&#30340;&#36923;&#36753;&#24605;&#32500;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4. (arXiv:2305.12147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12147
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LogiCoT, &#19968;&#20010;&#22522;&#20110;GPT-4&#30340;&#36923;&#36753;&#24605;&#32500;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25945;&#25480;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;&#21644;&#24341;&#20986;&#19968;&#33324;&#25512;&#29702;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;4&#65288;GPT-4&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#33258;&#25105;&#25351;&#23548;&#35843;&#25972;&#30740;&#31350;&#65288;&#22914;Alpaca&#65289;&#20391;&#37325;&#20110;&#22686;&#24378;&#27169;&#22411;&#30340;&#36890;&#29992;&#33021;&#21147;&#12290;&#36825;&#20123;&#25351;&#20196;&#20351;&#27169;&#22411;&#22312;&#19968;&#33324;&#20219;&#21153;&#65288;&#22914;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#21644;&#37322;&#20041;&#65289;&#19978;&#33021;&#22815;&#36798;&#21040;&#19982;GPT-3.5&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#33021;&#24110;&#21161;&#27169;&#22411;&#22788;&#29702;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;LogiCoT&#65292;&#19968;&#31181;&#26032;&#30340;&#36923;&#36753;&#24605;&#32500;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;GPT-4&#30340;&#36923;&#36753;&#24605;&#32500;&#38142;&#25512;&#29702;&#12290;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;&#25910;&#38598;&#25351;&#20196;&#20197;&#25552;&#31034;GPT-4&#29983;&#25104;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;LogiCoT&#20316;&#20026;&#25945;&#25480;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#25351;&#20196;&#38598;&#65292;&#24182;&#24341;&#20986;&#20102;&#19968;&#33324;&#25512;&#29702;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer 4 (GPT-4) demonstrates impressive chain-of-thought reasoning ability. Recent work on self-instruction tuning, such as Alpaca, has focused on enhancing the general proficiency of models. These instructions enable the model to achieve performance comparable to GPT-3.5 on general tasks like open-domain text generation and paraphrasing. However, they fall short of helping the model handle complex reasoning tasks. To bridge the gap, this paper presents LogiCoT, a new instruction-tuning dataset for Logical Chain-of-Thought reasoning with GPT-4. We elaborate on the process of harvesting instructions for prompting GPT-4 to generate chain-of-thought rationales. LogiCoT serves as an instruction set for teaching models of logical reasoning and elicits general reasoning skills.
&lt;/p&gt;</description></item><item><title>STOAT&#27169;&#22411;&#26159;&#34920;&#26684;&#21644;&#25512;&#29702;&#24847;&#35782;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#25968;&#23383;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#26102;&#38388;&#25512;&#29702;&#12289;&#34920;&#26684;&#30693;&#35782;&#21644;&#23454;&#20307;&#30693;&#35782;&#26041;&#38754;&#26377;&#36739;&#22909;&#30340;&#25511;&#21046;&#65292;&#25552;&#39640;&#20102;&#20998;&#26512;&#21477;&#23376;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.11826</link><description>&lt;p&gt;
STOAT: &#32467;&#26500;&#21270;&#25968;&#25454;&#25511;&#21046;&#24615;&#20998;&#26512;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
STOAT: Structured Data to Analytical Text With Controls. (arXiv:2305.11826v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11826
&lt;/p&gt;
&lt;p&gt;
STOAT&#27169;&#22411;&#26159;&#34920;&#26684;&#21644;&#25512;&#29702;&#24847;&#35782;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#25968;&#23383;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#26102;&#38388;&#25512;&#29702;&#12289;&#34920;&#26684;&#30693;&#35782;&#21644;&#23454;&#20307;&#30693;&#35782;&#26041;&#38754;&#26377;&#36739;&#22909;&#30340;&#25511;&#21046;&#65292;&#25552;&#39640;&#20102;&#20998;&#26512;&#21477;&#23376;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#38656;&#35201;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#20197;&#29983;&#25104;&#25551;&#36848;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#34920;&#29616;&#20986;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#20363;&#22914;&#34920;&#26684;&#65289;&#29983;&#25104;&#20998;&#26512;&#25991;&#26412;&#12290;&#22312;&#65288;Gupta et al.,2020&#65289;&#25552;&#20986;&#30340;&#20998;&#31867;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#20197;&#19979;&#25512;&#29702;&#31867;&#21035;&#30340;&#21487;&#25511;&#21046;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;&#65306;&#25968;&#23383;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#26102;&#38388;&#25512;&#29702;&#12289;&#34920;&#26684;&#30693;&#35782;&#21644;&#23454;&#20307;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STOAT&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#34920;&#26684;&#21644;&#25512;&#29702;&#24847;&#35782;&#65292;&#24182;&#36890;&#36807;&#30690;&#37327;&#37327;&#21270;&#23558;&#32473;&#23450;&#30340;&#25512;&#29702;&#31867;&#21035;&#27880;&#20837;&#36755;&#20986;&#20013;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#20998;&#26512;&#21477;&#23376;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;iToTTo&#21644;Infotabs&#30340;PARENT&#25351;&#26631;&#19978;&#20998;&#21035;&#25552;&#20379;&#20102;10.19&#65285;&#21644;1.13&#65285;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#25551;&#36848;&#26356;&#21152;&#20934;&#30830;&#21644;&#20998;&#26512;&#65292;&#20154;&#31867;&#35780;&#20272;&#20013;&#22686;&#21152;&#20102;15.3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent language models have made tremendous progress in the structured data to text generation task. However, these models still give sub-optimal performance where logical inference is required to generate the descriptions. In this work, we specifically focus on analytical text generation from structured data such as tables. Building on the taxonomy proposed in (Gupta et al., 2020) we focus on controllable table to text generation for the following reasoning categories: numerical reasoning, commonsense reasoning, temporal reasoning, table knowledge, and entity knowledge. We propose STOAT model, which is table and reasoning aware, with vector-quantization to infuse the given reasoning categories in the output. We observe that our model provides 10.19%, 1.13% improvement on the PARENT metric in iToTTo and Infotabs for the analytical sentence task. We also found that our model generates 15.3% more faithful and analytical descriptions as compared to the baseline models in human evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#38598;&#25104;GPT-k&#26469;&#25552;&#39640;T2I&#29983;&#25104;&#20013;&#30340;&#32534;&#36753;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26356;&#25797;&#38271;&#35843;&#25972;&#65288;&#20462;&#25913;&#65289;&#25991;&#26412;&#20013;&#30340;&#20462;&#39280;&#35821;&#65292;&#32780;&#20154;&#31867;&#20542;&#21521;&#20110;&#26367;&#25442;&#21333;&#35789;&#21644;&#30701;&#35821;&#12290;</title><link>http://arxiv.org/abs/2305.11317</link><description>&lt;p&gt;
&#21512;&#20316;&#29983;&#25104;AI&#65306;&#38598;&#25104;GPT-k&#20197;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#25552;&#39640;&#32534;&#36753;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation. (arXiv:2305.11317v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#38598;&#25104;GPT-k&#26469;&#25552;&#39640;T2I&#29983;&#25104;&#20013;&#30340;&#32534;&#36753;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26356;&#25797;&#38271;&#35843;&#25972;&#65288;&#20462;&#25913;&#65289;&#25991;&#26412;&#20013;&#30340;&#20462;&#39280;&#35821;&#65292;&#32780;&#20154;&#31867;&#20542;&#21521;&#20110;&#26367;&#25442;&#21333;&#35789;&#21644;&#30701;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#39046;&#22495;&#22312;&#30740;&#31350;&#30028;&#21644;&#29992;&#25143;&#20013;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#34429;&#28982;T2I&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#29992;&#25143;&#24120;&#36935;&#21040;&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#38656;&#35201;&#37325;&#22797;&#32534;&#36753;&#36755;&#20837;&#25552;&#31034;&#25165;&#33021;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#22270;&#20687;&#65292;&#36825;&#26159;&#32791;&#26102;&#19988;&#21171;&#21160;&#24378;&#24230;&#22823;&#30340;&#12290;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-k&#65289;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#25913;&#36827;T2I&#29983;&#25104;&#20013;&#25552;&#31034;&#32534;&#36753;&#36807;&#31243;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;GPT-k&#24120;&#35265;&#30340;&#32534;&#36753;&#26041;&#24335;&#65292;&#35780;&#20272;GPT-k&#22312;&#25512;&#21160;T2I&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#26816;&#26597;&#21487;&#33021;&#24433;&#21709;&#27492;&#36807;&#31243;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-k&#27169;&#22411;&#26356;&#27880;&#37325;&#25554;&#20837;&#20462;&#25913;&#22120;&#65292;&#32780;&#20154;&#31867;&#20542;&#21521;&#20110;&#26367;&#25442;&#21333;&#35789;&#21644;&#30701;&#35821;&#65292;&#21253;&#25324;&#23545;&#20027;&#39064;&#30340;&#26356;&#25913;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-k&#22312;&#35843;&#25972;&#20462;&#25913;&#22120;&#26041;&#38754;&#27604;&#36739;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of text-to-image (T2I) generation has garnered significant attention both within the research community and among everyday users. Despite the advancements of T2I models, a common issue encountered by users is the need for repetitive editing of input prompts in order to receive a satisfactory image, which is time-consuming and labor-intensive. Given the demonstrated text generation power of large-scale language models, such as GPT-k, we investigate the potential of utilizing such models to improve the prompt editing process for T2I generation. We conduct a series of experiments to compare the common edits made by humans and GPT-k, evaluate the performance of GPT-k in prompting T2I, and examine factors that may influence this process. We found that GPT-k models focus more on inserting modifiers while humans tend to replace words and phrases, which includes changes to the subject matter. Experimental results show that GPT-k are more effective in adjusting modifiers rather than p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#20016;&#23500;&#30340;&#23454;&#20307;&#32463;&#39564;&#36827;&#34892;&#24494;&#35843;, &#20197;&#25552;&#39640;&#20854;&#22312;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#24182;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#20445;&#25345;&#25110;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10626</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36935;&#35265;&#19990;&#30028;&#27169;&#22411;&#65306;&#23454;&#20307;&#32463;&#39564;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#20016;&#23500;&#30340;&#23454;&#20307;&#32463;&#39564;&#36827;&#34892;&#24494;&#35843;, &#20197;&#25552;&#39640;&#20854;&#22312;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#24182;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#20445;&#25345;&#25110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LMs) &#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#22312;&#22788;&#29702;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#31616;&#21333;&#25512;&#29702;&#21644;&#35268;&#21010;&#38382;&#39064;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#20363;&#22914;&#29702;&#35299;&#29289;&#20307;&#27704;&#24658;&#25110;&#35268;&#21010;&#23478;&#24237;&#27963;&#21160;&#12290;&#36825;&#31181;&#38480;&#21046;&#28304;&#20110; LM &#20165;&#21463;&#20070;&#38754;&#35821;&#35328;&#35757;&#32451;&#65292;&#32570;&#23569;&#24517;&#35201;&#30340;&#23454;&#20307;&#30693;&#35782;&#21644;&#25216;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378; LM &#30340;&#26041;&#27861;&#65292;&#21363;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#23454;&#20307;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#19968;&#33324;&#35821;&#35328;&#33021;&#21147;&#12290;&#26412;&#26041;&#27861;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#37096;&#32626;&#19968;&#20010;&#34701;&#20837;&#23454;&#20307;&#32463;&#39564;&#30340;&#20195;&#29702;&#65292;&#29305;&#21035;&#26159;&#19968;&#20010;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#30340;&#20223;&#30495;&#22120;(VirtualHome)&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#23454;&#20307;&#32463;&#39564;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#20123;&#32463;&#39564;&#24494;&#35843; LM &#65292;&#20197;&#25945;&#25480;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#21508;&#31181;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#20363;&#22914;&#35268;&#21010;&#21644;&#23436;&#25104;&#30446;&#26631;&#12289;&#29289;&#20307;&#27704;&#24658;&#21644;&#36319;&#36394;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#20197;&#21033;&#29992;&#20854;&#20182;&#27169;&#25311;&#22120;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#30528;&#25552;&#39640;&#20102; LM &#22312;&#19968;&#31995;&#21015;&#29289;&#29702;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#30041;&#24182;&#32463;&#24120;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;GLMs&#65289;&#30340;&#30693;&#35782;&#36755;&#20986;&#12290;&#36890;&#36807;&#23545;14&#31181;GLMs&#36827;&#34892;&#32508;&#21512;&#27604;&#36739;&#65292;&#21457;&#29616;&#30693;&#35782;&#36981;&#24490;&#32553;&#25918;&#23450;&#24459;&#65292;&#32780;&#23545;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10519</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Statistical Knowledge Assessment for Generative Language Models. (arXiv:2305.10519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;GLMs&#65289;&#30340;&#30693;&#35782;&#36755;&#20986;&#12290;&#36890;&#36807;&#23545;14&#31181;GLMs&#36827;&#34892;&#32508;&#21512;&#27604;&#36739;&#65292;&#21457;&#29616;&#30693;&#35782;&#36981;&#24490;&#32553;&#25918;&#23450;&#24459;&#65292;&#32780;&#23545;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;GLMs&#65289;&#23637;&#31034;&#20102;&#23384;&#20648;&#20107;&#23454;&#30693;&#35782;&#21644;&#39640;&#25928;&#22238;&#31572;&#26597;&#35810;&#30340;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#32473;&#23450;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;GLM&#26159;&#21542;&#22987;&#32456;&#29983;&#25104;&#20107;&#23454;&#27491;&#30830;&#30340;&#31572;&#26696;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;&#28508;&#21464;&#37327;&#21644;KaRR&#24230;&#37327;&#25351;&#23548;&#30340;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;&#26694;&#26550;&#65292;&#35813;&#24230;&#37327;&#36890;&#36807;&#35745;&#31639;&#27169;&#22411;&#22312;&#21508;&#31181;&#25991;&#26412;&#24418;&#24335;&#19978;&#30340;&#36830;&#32493;&#27010;&#29575;&#37327;&#21270;&#20854;&#30693;&#35782;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#23545;14&#31181;GLM&#30340;&#30693;&#35782;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#21253;&#25324;LLaMA&#12289;Alpaca&#12289;OPT&#21644;&#20854;&#20182;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;&#28085;&#30422;&#20102;600&#31181;&#20851;&#31995;&#31867;&#22411;&#65292;&#24182;&#26174;&#31034;&#20986;&#19982;&#20154;&#31867;&#35780;&#20272;&#30340;&#24378;&#30456;&#20851;&#24615;&#65288;0.43 Kendall's $\tau$&#65289;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#20855;&#26377;&#30456;&#21516;&#25903;&#26550;&#32467;&#26500;&#30340;GLM&#30340;&#30693;&#35782;&#36981;&#24490;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#19988;&#22312;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#24494;&#35843;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#25345;&#32493;&#29983;&#25104;&#20107;&#23454;&#27491;&#30830;&#30340;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Language Models (GLMs) have demonstrated capabilities to store factual knowledge and answer queries efficiently. Given varying prompts, does a GLM consistently generate factually correct answers? In this paper, we introduce a statistical knowledge assessment framework guided by latent variables and the KaRR metric, which quantifies a model's knowledge by computing its continuous probability across diverse text forms. We conduct a comprehensive comparison of knowledge across 14 GLMs using our framework, including LLaMA, Alpaca, OPT, and others. Our statistical knowledge assessment encompasses 600 relation types and exhibits a strong correlation (0.43 Kendall's $\tau$) with human evaluation. Our findings reveal that the knowledge in GLMs with the same backbone architecture adheres to the scaling law, and that tuning on instruction-following data may compromise the model's ability to generate factually correct text consistently.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;NLP&#39046;&#22495;&#20869;&#23578;&#26410;&#30740;&#31350;&#30340;&#38382;&#39064;&#65306;&#24773;&#26223;&#21644;&#32454;&#33268;&#22320;&#29702;&#35299;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;PersoNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10156</link><description>&lt;p&gt;
&#38405;&#35835;&#36807;&#31243;&#20013;&#23545;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Personality Understanding of Fictional Characters during Book Reading. (arXiv:2305.10156v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;NLP&#39046;&#22495;&#20869;&#23578;&#26410;&#30740;&#31350;&#30340;&#38382;&#39064;&#65306;&#24773;&#26223;&#21644;&#32454;&#33268;&#22320;&#29702;&#35299;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;PersoNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#26159;&#38405;&#35835;&#25925;&#20107;&#30340;&#20851;&#38190;&#12290;&#38543;&#30528;&#35835;&#32773;&#19982;&#25925;&#20107;&#30340;&#20114;&#21160;&#65292;&#20182;&#20204;&#23545;&#19968;&#20010;&#20154;&#29289;&#30340;&#29702;&#35299;&#20250;&#26681;&#25454;&#26032;&#30340;&#20107;&#20214;&#21644;&#20449;&#24687;&#32780;&#28436;&#21464;&#65307;&#24182;&#19988;&#21487;&#20197;&#24863;&#30693;&#21040;&#22810;&#20010;&#31934;&#32454;&#30340;&#20010;&#24615;&#26041;&#38754;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#65306;&#24773;&#22659;&#21644;&#31934;&#32454;&#30340;&#20010;&#24615;&#29702;&#35299;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;NLP&#39046;&#22495;&#20013;&#27809;&#26377;&#24471;&#21040;&#30740;&#31350;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#27169;&#20223;&#38405;&#35835;&#36807;&#31243;&#30340;&#36866;&#24403;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;PersoNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;&#27880;&#37322;&#31574;&#30053;&#28041;&#21450;&#29992;&#22312;&#32447;&#38405;&#35835;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#31508;&#35760;&#20316;&#20026;&#21407;&#22987;&#20070;&#31821;&#30340;&#20195;&#29702;&#36827;&#34892;&#27880;&#37322;&#12290;&#23454;&#39564;&#21644;&#20154;&#20307;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26500;&#24314;&#26082;&#26377;&#25928;&#21448;&#20934;&#30830;&#65307;&#25105;&#20204;&#30340;&#20219;&#21153;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#38271;&#26399;&#30340;&#19978;&#19979;&#25991;&#20197;&#23454;&#29616;&#23545;&#26426;&#22120;&#21644;&#20154;&#31867;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/Gorov/personet_acl23&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comprehending characters' personalities is a crucial aspect of story reading. As readers engage with a story, their understanding of a character evolves based on new events and information; and multiple fine-grained aspects of personalities can be perceived. This leads to a natural problem of situated and fine-grained personality understanding. The problem has not been studied in the NLP field, primarily due to the lack of appropriate datasets mimicking the process of book reading. We present the first labeled dataset PersoNet for this problem. Our novel annotation strategy involves annotating user notes from online reading apps as a proxy for the original books. Experiments and human studies indicate that our dataset construction is both efficient and accurate; and our task heavily relies on long-term context to achieve accurate predictions for both machines and humans. The dataset is available at https://github.com/Gorov/personet_acl23.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22270;&#24418;(NLGraph)&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20110;&#22270;&#24418;&#38382;&#39064;&#35299;&#20915;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#22270;&#24418;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLM(GPT-3/4)&#20855;&#26377;&#30456;&#24212;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10037</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#20915;&#22270;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Language Models Solve Graph Problems in Natural Language?. (arXiv:2305.10037v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22270;&#24418;(NLGraph)&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20110;&#22270;&#24418;&#38382;&#39064;&#35299;&#20915;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#22270;&#24418;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLM(GPT-3/4)&#20855;&#26377;&#30456;&#24212;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#19968;&#20123;&#20855;&#26377;&#38544;&#24335;&#22270;&#24418;&#32467;&#26500;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#35268;&#21010;&#12289;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#25110;&#30693;&#35782;&#25506;&#32034;&#12289;&#32467;&#26500;&#21270;&#24120;&#35782;&#25512;&#29702;&#31561;&#31561;&#12290;&#34429;&#28982;LLM&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;LLM&#26159;&#21542;&#33021;&#22815;&#26174;&#24335;&#22788;&#29702;&#22270;&#24418;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#23558;&#23427;&#20204;&#26144;&#23556;&#21040;&#22522;&#20110;&#27010;&#24565;&#30340;&#31354;&#38388;&#20013;&#65292;&#24182;&#25191;&#34892;&#32467;&#26500;&#21270;&#25805;&#20316;&#20173;&#28982;&#23578;&#26410;&#24471;&#21040;&#36275;&#22815;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22270;&#24418;(NLGraph)&#65292;&#23427;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#22522;&#20110;&#22270;&#24418;&#38382;&#39064;&#35299;&#20915;&#20840;&#38754;&#27979;&#35797;&#12290;NLGraph&#21253;&#21547;29,370&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20843;&#20010;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#65292;&#20174;&#31616;&#21333;&#30340;&#36830;&#25509;&#21644;&#26368;&#30701;&#36335;&#24452;&#21040;&#22797;&#26434;&#30340;&#26368;&#22823;&#27969;&#21644;&#27169;&#25311;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#20219;&#21153;&#19981;&#31561;&#12290;&#25105;&#20204;&#22312;NLGraph&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;LLM(GPT-3/4)&#65292;&#24182;&#21457;&#29616;1)&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#30456;&#24212;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#65307;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language
&lt;/p&gt;</description></item><item><title>The Vault&#26159;&#19968;&#20010;&#25552;&#20379;&#20102;10&#31181;&#27969;&#34892;&#32534;&#31243;&#35821;&#35328;&#30340;40&#30334;&#19975;&#34892;&#20195;&#30721;-&#25991;&#26412;&#23545;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#38754;&#21521;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#65292;&#26377;&#26395;&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.06156</link><description>&lt;p&gt;
The Vault&#65306;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20026;&#20419;&#36827;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#32780;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation. (arXiv:2305.06156v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06156
&lt;/p&gt;
&lt;p&gt;
The Vault&#26159;&#19968;&#20010;&#25552;&#20379;&#20102;10&#31181;&#27969;&#34892;&#32534;&#31243;&#35821;&#35328;&#30340;40&#30334;&#19975;&#34892;&#20195;&#30721;-&#25991;&#26412;&#23545;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#38754;&#21521;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#65292;&#26377;&#26395;&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; The Vault&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#20195;&#30721;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#38754;&#21521;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#35757;&#32451;&#22522;&#20110;&#20195;&#30721;&#30340;LLM&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#22312;&#22823;&#23567;&#12289;&#36136;&#37327;(&#30001;&#20110;&#22122;&#22768;&#20449;&#21495;)&#21644;&#26684;&#24335;&#65288;&#20165;&#21253;&#21547;&#20195;&#30721;&#20989;&#25968;&#21644;&#25991;&#26412;&#35828;&#26126;&#37197;&#23545;&#65289;&#26041;&#38754;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;The Vault&#36890;&#36807;&#25552;&#20379;10&#31181;&#27969;&#34892;&#32534;&#31243;&#35821;&#35328;&#30340;40&#30334;&#19975;&#34892;&#20195;&#30721;-&#25991;&#26412;&#23545;&#65292;&#24443;&#24213;&#28165;&#38500;10&#31181;&#22810;&#26679;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#21508;&#31181;&#32423;&#21035;&#30340;&#20195;&#30721;-&#25991;&#26412;&#23545;&#65292;&#21253;&#25324;&#31867;&#12289;&#20989;&#25968;&#21644;&#20195;&#30721;&#34892;&#31561;&#32423;&#21035;&#65292;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#20154;&#21592;&#21487;&#20197;&#21033;&#29992;The Vault&#26469;&#35757;&#32451;&#19981;&#21516;&#30340;&#38754;&#21521;&#20195;&#30721;&#30340;LLM&#65292;&#25110;&#32773;&#23558;&#25552;&#20379;&#30340;&#25968;&#25454;&#28165;&#27927;&#26041;&#27861;&#21644;&#33050;&#26412;&#21512;&#24182;&#21040;&#33258;&#24049;&#30340;&#25968;&#25454;&#38598;&#20013;&#26469;&#25913;&#36827;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#23558;The Vault&#20316;&#20026;&#38754;&#21521;&#20195;&#30721;&#30340;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#39044;&#35745;&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#23454;&#36341;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present The Vault, an open-source, large-scale code-text dataset designed to enhance the training of code-focused large language models (LLMs). Existing open-source datasets for training code-based LLMs often face challenges in terms of size, quality (due to noisy signals), and format (only containing code function and text explanation pairings). The Vault overcomes these limitations by providing 40 million code-text pairs across 10 popular programming languages, thorough cleaning for 10+ prevalent issues, and various levels of code-text pairings, including class, function, and line levels. Researchers and practitioners can utilize The Vault for training diverse code-focused LLMs or incorporate the provided data cleaning methods and scripts to improve their datasets. By employing The Vault as the training dataset for code-centric LLMs, we anticipate significant advancements in code understanding and generation tasks, fostering progress in both artificial intelligence research and so
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#25968;&#23398;&#39064;&#35299;&#20915;&#22120;&#65292;&#20351;&#29992;&#32479;&#19968;&#26641;&#32467;&#26500;&#34920;&#31034;&#35299;&#20915;&#26041;&#26696;&#34920;&#36798;&#24335;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#25968;&#23398;&#21464;&#20307;&#26102;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.04556</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#25968;&#23398;&#39064;&#35299;&#20915;&#22120;&#19982;&#32479;&#19968;&#26641;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Non-Autoregressive Math Word Problem Solver with Unified Tree Structure. (arXiv:2305.04556v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04556
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#25968;&#23398;&#39064;&#35299;&#20915;&#22120;&#65292;&#20351;&#29992;&#32479;&#19968;&#26641;&#32467;&#26500;&#34920;&#31034;&#35299;&#20915;&#26041;&#26696;&#34920;&#36798;&#24335;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#25968;&#23398;&#21464;&#20307;&#26102;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25968;&#23398;&#39064;&#35299;&#20915;&#22120;&#20351;&#29992;&#24207;&#21015;&#25110;&#20108;&#21449;&#26641;&#26469;&#34920;&#31034;&#35299;&#20915;&#26041;&#26696;&#34920;&#36798;&#24335;&#24182;&#26681;&#25454;&#32473;&#23450;&#30340;&#38382;&#39064;&#25551;&#36848;&#26469;&#35299;&#30721;&#23427;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#32467;&#26500;&#26080;&#27861;&#22788;&#29702;&#36890;&#36807;&#25968;&#23398;&#21464;&#25442;&#21487;&#20197;&#24471;&#21040;&#30340;&#21464;&#20307;&#65292;&#20363;&#22914;&#65292;&#23545;&#20110;&#30456;&#21516;&#30340;&#38382;&#39064;&#65292;$(a_1+a_2) * a_3$&#21644;$a_1 * a_3+a_2 * a_3$&#37117;&#21487;&#33021;&#26159;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#20204;&#34987;&#34920;&#31034;&#20026;&#19981;&#21516;&#30340;&#34920;&#36798;&#24335;&#24207;&#21015;&#25110;&#26641;&#12290;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#21464;&#20307;&#34920;&#31034;&#20102;&#30456;&#21516;&#36755;&#20837;&#38382;&#39064;&#30340;&#19981;&#21516;&#27714;&#35299;&#36807;&#31243;&#65292;&#20250;&#24341;&#21457;&#20004;&#20010;&#38382;&#39064;&#65306;1&#65289;&#35753;&#27169;&#22411;&#26377;&#25928;&#22320;&#23398;&#20064;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#21464;&#24471;&#22256;&#38590;&#65292;2&#65289;&#22312;&#35780;&#20272;&#26377;&#25928;&#34920;&#36798;&#24335;&#21464;&#20307;&#26102;&#38169;&#35823;&#22320;&#25351;&#20986;\textit{&#38169;&#35823;}&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26641;&#32467;&#26500;&#26469;&#34920;&#31034;&#35299;&#20915;&#26041;&#26696;&#34920;&#36798;&#24335;&#65292;&#20854;&#20013;&#30340;&#20803;&#32032;&#23545;&#20110;&#25152;&#26377;&#30340;&#34920;&#36798;&#24335;&#21464;&#20307;&#26469;&#35828;&#26159;&#21487;&#20132;&#25442;&#19988;&#30456;&#21516;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#33258;&#22238;&#24402;&#27714;&#35299;&#22120;&#65292;&#21629;&#21517;&#20026;\textit{MWP-NAS}&#65292;&#29992;&#20110;&#35299;&#26512;&#38382;&#39064;&#21644;&#27714;&#35299;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing MWP solvers employ sequence or binary tree to present the solution expression and decode it from given problem description. However, such structures fail to handle the variants that can be derived via mathematical manipulation, e.g., $(a_1+a_2) * a_3$ and $a_1 * a_3+a_2 * a_3$ can both be possible valid solutions for a same problem but formulated as different expression sequences or trees. The multiple solution variants depicting different possible solving procedures for the same input problem would raise two issues: 1) making it hard for the model to learn the mapping function between the input and output spaces effectively, and 2) wrongly indicating \textit{wrong} when evaluating a valid expression variant. To address these issues, we introduce a unified tree structure to present a solution expression, where the elements are permutable and identical for all the expression variants. We propose a novel non-autoregressive solver, named \textit{MWP-NAS}, to parse the problem and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#26469;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.03598</link><description>&lt;p&gt;
NLI4CT&#65306;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#26469;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35299;&#37322;&#21644;&#26816;&#32034;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#30340;&#21307;&#23398;&#35777;&#25454;&#65311;&#22810;&#24180;&#26469;&#65292;&#31215;&#32047;&#19979;&#26469;&#30340;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#21253;&#21547;&#20102;&#21457;&#23637;&#20010;&#24615;&#21270;&#21307;&#23398;&#25152;&#24517;&#38656;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#25214;&#21040;&#26368;&#20339;&#30340;&#23454;&#39564;&#27835;&#30103;&#35777;&#25454;&#65292;&#25163;&#21160;&#26816;&#26597;&#36229;&#36807;400,000&#20010;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#26159;&#23454;&#38469;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20801;&#35768;&#21487;&#25193;&#23637;&#35745;&#31639;&#25991;&#26412;&#34164;&#21547;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NLI&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#20043;&#21069;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#26080;&#27861;&#25429;&#25417;CTR&#25512;&#29702;&#30340;&#20840;&#37096;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36164;&#28304;&#65292;&#20197;&#25512;&#36827;&#20851;&#20110;CTR&#25512;&#29702;&#30340;NLI&#30740;&#31350;&#12290;&#35813;&#36164;&#28304;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#20197;&#35777;&#26126;&#39044;&#27979;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;NLI4CT&#65292;&#19968;&#20010;&#22522;&#20110;CTR&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical trial reports in order to find the best evidence for experimental treatments. Natural Language Inference (NLI) offers a potential solution to this problem, by allowing the scalable computation of textual entailment. However, existing NLI models perform poorly on biomedical corpora, and previously published datasets fail to capture the full complexity of inference over CTRs. In this work, we present a novel resource to advance research on NLI for reasoning on CTRs. The resource includes two main tasks. Firstly, to determine the inference relation between a natural language statement, and a CTR. Secondly, to retrieve supporting facts to justify the predicted relation. We provide NLI4CT, a corpus of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32763;&#35793;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#12289;&#36328;&#35821;&#35328;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26694;&#26550;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#22810;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.03510</link><description>&lt;p&gt;
&#22522;&#20110;&#32763;&#35793;&#23545;&#40784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36328;&#35821;&#35328;&#36801;&#31227;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment. (arXiv:2305.03510v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32763;&#35793;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#12289;&#36328;&#35821;&#35328;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26694;&#26550;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#22810;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#22312;&#36830;&#25509;&#22270;&#20687;&#21644;&#33521;&#35821;&#25991;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#23613;&#31649;&#26368;&#36817;&#35797;&#22270;&#25193;&#23637;CLIP&#20197;&#25903;&#25345;&#20854;&#20182;&#35821;&#35328;&#65292;&#20294;&#30001;&#20110;&#36164;&#28304;&#19981;&#24179;&#34913;&#65292;&#35266;&#23519;&#21040;&#20102;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#26041;&#27861;&#20250;&#28040;&#32791;&#22823;&#37327;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;&#32763;&#35793;&#30340;&#23545;&#40784;&#26041;&#27861;&#26469;&#20943;&#36731;&#22810;&#35821;&#35328;&#24046;&#24322;&#65292;&#24182;&#25506;&#32034;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;&#22312;XTD&#21644;Multi30K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#38646;-shot&#12289;few-shot&#21644;&#20840;&#25968;&#25454;&#38598;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;11&#31181;&#35821;&#35328;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#20943;&#23569;&#20102;&#35821;&#35328;&#20043;&#38388;&#30340;&#22810;&#35821;&#35328;&#24046;&#24322;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained vision and language models such as CLIP have witnessed remarkable success in connecting images and texts with a primary focus on English texts. Despite recent efforts to extend CLIP to support other languages, disparities in performance among different languages have been observed due to uneven resource availability. Additionally, current cross-lingual transfer methods of those pre-trained models would consume excessive resources for a large number of languages. Therefore, we propose a new parameter-efficient cross-lingual transfer learning framework that utilizes a translation-based alignment method to mitigate multilingual disparities and explores parameter-efficient fine-tuning methods for parameter-efficient cross-lingual transfer. Extensive experiments on XTD and Multi30K datasets, covering 11 languages under zero-shot, few-shot, and full-dataset learning scenarios, show that our framework significantly reduces the multilingual disparities among languages and improves 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35745;&#21010;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#31616;&#27905;&#30340;&#27969;&#31243;&#22270;&#36716;&#21270;&#25104;&#23545;&#35805;&#65292;&#20197;&#29983;&#25104;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#20197;&#27969;&#31243;&#22270;&#20026;&#22522;&#30784;&#30340;&#25925;&#38556;&#25490;&#38500;&#23545;&#35805;&#31995;&#32479;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01323</link><description>&lt;p&gt;
&#23558;&#27969;&#31243;&#22270;&#36716;&#21270;&#20026;&#23545;&#35805;&#65306;&#22522;&#20110;&#35745;&#21010;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#27969;&#31243;&#22270;&#30456;&#20851;&#25925;&#38556;&#25490;&#38500;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Turning Flowchart into Dialog: Plan-based Data Augmentation for Low-Resource Flowchart-grounded Troubleshooting Dialogs. (arXiv:2305.01323v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35745;&#21010;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#31616;&#27905;&#30340;&#27969;&#31243;&#22270;&#36716;&#21270;&#25104;&#23545;&#35805;&#65292;&#20197;&#29983;&#25104;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#20197;&#27969;&#31243;&#22270;&#20026;&#22522;&#30784;&#30340;&#25925;&#38556;&#25490;&#38500;&#23545;&#35805;&#31995;&#32479;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20197;&#27969;&#31243;&#22270;&#20026;&#22522;&#30784;&#30340;&#25925;&#38556;&#25490;&#38500;&#23545;&#35805;&#31995;&#32479;&#65288;FTD&#31995;&#32479;&#65289;&#19968;&#30452;&#22791;&#21463;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#20805;&#20998;&#30340;&#33258;&#28982;&#22522;&#20110;&#27969;&#31243;&#22270;&#30340;&#23545;&#35805;&#25968;&#25454;&#25104;&#26412;&#36739;&#39640;&#65292;&#22240;&#27492;FTD&#31995;&#32479;&#21463;&#21040;&#25968;&#25454;&#31232;&#32570;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35745;&#21010;&#30340;&#25968;&#25454;&#22686;&#24378;&#65288;PlanDA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31616;&#27905;&#30340;&#27969;&#31243;&#22270;&#36716;&#21270;&#20026;&#23545;&#35805;&#65292;&#29983;&#25104;&#22823;&#37327;&#22810;&#26679;&#30340;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#30340;&#29983;&#25104;&#27169;&#22411;&#37319;&#29992;&#20855;&#26377;&#20840;&#23616;&#21644;&#23616;&#37096;&#28508;&#22312;&#35268;&#21010;&#21464;&#37327;&#30340;&#20998;&#23618;&#35268;&#21010;&#31574;&#30053;&#30340;&#21464;&#20998;&#22522;&#26694;&#26550;&#12290;&#22312;FloDial&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PlanDA&#29983;&#25104;&#30340;&#21512;&#25104;&#23545;&#35805;&#25913;&#21892;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#27969;&#31243;&#22270;&#36335;&#24452;&#26816;&#32034;&#21644;&#21709;&#24212;&#29983;&#25104;&#65292;&#29305;&#21035;&#26159;&#22312;&#27969;&#31243;&#22270;&#20197;&#22806;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flowchart-grounded troubleshooting dialogue (FTD) systems, which follow the instructions of a flowchart to diagnose users' problems in specific domains (eg., vehicle, laptop), have been gaining research interest in recent years. However, collecting sufficient dialogues that are naturally grounded on flowcharts is costly, thus FTD systems are impeded by scarce training data. To mitigate the data sparsity issue, we propose a plan-based data augmentation (PlanDA) approach that generates diverse synthetic dialog data at scale by transforming concise flowchart into dialogues. Specifically, its generative model employs a variational-base framework with a hierarchical planning strategy that includes global and local latent planning variables. Experiments on the FloDial dataset show that synthetic dialogue produced by PlanDA improves the performance of downstream tasks, including flowchart path retrieval and response generation, in particular on the Out-of-Flowchart settings. In addition, furt
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#20154;&#36896;&#32467;&#26500;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#20808;&#35757;&#32451;&#21644;&#22312;&#33521;&#35821;&#19978;&#24494;&#35843;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19977;&#31181;&#24402;&#32435;&#20559;&#32622;&#31867;&#22411;&#65306;&#36882;&#24402;&#30340;&#23618;&#32423;&#22788;&#29702;&#12289;&#26080;&#38480;&#21046;&#30340;&#26631;&#35760;-&#26631;&#35760;&#20381;&#36182;&#20197;&#21450;&#22522;&#20110;Zipfian&#24130;&#24459;&#35789;&#27719;&#20998;&#24067;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#25105;&#20204;&#24471;&#20986;&#22797;&#26434;&#26631;&#35760;-&#26631;&#35760;&#20132;&#20114;&#24418;&#25104;&#20102;&#26368;&#22909;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2304.13060</link><description>&lt;p&gt;
&#21482;&#29992;&#32467;&#26500;&#20808;&#39044;&#35757;&#32451;&#65306;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#29702;&#35299;&#35821;&#35328;&#24402;&#32435;&#20559;&#32622;
&lt;/p&gt;
&lt;p&gt;
Pretrain on just structure: Understanding linguistic inductive biases using transfer learning. (arXiv:2304.13060v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13060
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20154;&#36896;&#32467;&#26500;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#20808;&#35757;&#32451;&#21644;&#22312;&#33521;&#35821;&#19978;&#24494;&#35843;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19977;&#31181;&#24402;&#32435;&#20559;&#32622;&#31867;&#22411;&#65306;&#36882;&#24402;&#30340;&#23618;&#32423;&#22788;&#29702;&#12289;&#26080;&#38480;&#21046;&#30340;&#26631;&#35760;-&#26631;&#35760;&#20381;&#36182;&#20197;&#21450;&#22522;&#20110;Zipfian&#24130;&#24459;&#35789;&#27719;&#20998;&#24067;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#25105;&#20204;&#24471;&#20986;&#22797;&#26434;&#26631;&#35760;-&#26631;&#35760;&#20132;&#20114;&#24418;&#25104;&#20102;&#26368;&#22909;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#35770;&#26159;&#20154;&#31867;&#36824;&#26159;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#37117;&#33021;&#22312;&#27809;&#26377;&#26126;&#30830;&#30340;&#32467;&#26500;&#30417;&#30563;&#19979;&#23398;&#20064;&#35821;&#35328;&#12290;&#20160;&#20040;&#26679;&#30340;&#24402;&#32435;&#24335;&#23398;&#20064;&#20559;&#32622;&#20351;&#24471;&#36825;&#31181;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#20154;&#36896;&#32467;&#26500;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#24182;&#22312;&#33521;&#35821;&#19978;&#24494;&#35843;&#26469;&#37319;&#29992;&#19981;&#21516;&#30340;&#24402;&#32435;&#24335;&#23398;&#20064;&#20559;&#32622;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20559;&#32622;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35774;&#32622;&#20351;&#25105;&#20204;&#33021;&#22815;&#31215;&#26497;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#24402;&#32435;&#20559;&#32622;&#30340;&#27604;&#36739;&#25104;&#21151;:1)&#36882;&#24402;&#30340;&#23618;&#32423;&#22788;&#29702;&#30340;&#24402;&#32435;&#20559;&#32622;2)&#19981;&#21463;&#38480;&#30340;&#26631;&#35760;-&#26631;&#35760;&#20381;&#36182;&#65292;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#19981;&#33021;&#30001;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#24314;&#27169;3)Zipfian&#24130;&#24459;&#35789;&#27719;&#20998;&#24067;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22797;&#26434;&#30340;&#26631;&#35760;-&#26631;&#35760;&#20132;&#20114;&#24418;&#25104;&#20102;&#26368;&#22909;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#24182;&#19988;&#36825;&#22312;&#38750;&#19978;&#19979;&#25991;&#26080;&#20851;&#24773;&#20917;&#19979;&#26368;&#20026;&#24378;&#28872;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;Z(targetEntity)&#20998;&#24067;&#22312;&#33521;&#35821;&#19978;&#20063;&#26159;&#21512;&#36866;&#30340;&#39044;&#20808;&#35757;&#32451;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Both humans and transformer language models are able to learn language without explicit structural supervision. What inductive learning biases make this learning possible? In this study, we examine the effect of different inductive learning biases by predisposing language models with structural biases through pretraining on artificial structured data, and then evaluating by fine-tuning on English. Our experimental setup gives us the ability to actively control the inductive bias of language models. With our experiments, we investigate the comparative success of three types of inductive bias: 1) an inductive bias for recursive, hierarchical processing 2) an inductive bias for unrestricted token-token dependencies that can't be modeled by context-free grammars, and 3) an inductive bias for a Zipfian power-law vocabulary distribution. We show that complex token-token interactions form the best inductive biases, and that this is strongest in the non-context-free case. We also show that a Z
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20004;&#20010;&#20219;&#21153;&#23454;&#29616;&#26377;&#25928;&#30340;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#65292;&#20197;&#25552;&#39640;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.12633</link><description>&lt;p&gt;
PUNR: &#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#30340;&#26032;&#38395;&#25512;&#33616;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
PUNR: Pre-training with User Behavior Modeling for News Recommendation. (arXiv:2304.12633v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20004;&#20010;&#20219;&#21153;&#23454;&#29616;&#26377;&#25928;&#30340;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#65292;&#20197;&#25552;&#39640;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#25512;&#33616;&#26088;&#22312;&#22522;&#20110;&#29992;&#25143;&#34892;&#20026;&#39044;&#27979;&#28857;&#20987;&#34892;&#20026;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#24314;&#27169;&#29992;&#25143;&#34920;&#31034;&#26159;&#25512;&#33616;&#39318;&#36873;&#26032;&#38395;&#30340;&#20851;&#38190;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#38598;&#20013;&#22312;&#30417;&#30563;&#24494;&#35843;&#38454;&#27573;&#30340;&#25913;&#36827;&#19978;&#12290;&#28982;&#32780;&#65292;&#36824;&#32570;&#20047;&#38024;&#23545;&#29992;&#25143;&#34920;&#31034;&#20248;&#21270;&#30340;&#22522;&#20110;PLM&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#20219;&#21153;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#33539;&#20363;&#65292;&#21363;&#29992;&#25143;&#34892;&#20026;&#25513;&#34109;&#21644;&#29992;&#25143;&#34892;&#20026;&#29983;&#25104;&#65292;&#22343;&#33268;&#21147;&#20110;&#26377;&#25928;&#30340;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#25143;&#34892;&#20026;&#25513;&#34109;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#24674;&#22797;&#22522;&#20110;&#19978;&#19979;&#25991;&#34892;&#20026;&#30340;&#25513;&#34109;&#29992;&#25143;&#34892;&#20026;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21040;&#26356;&#24378;&#22823;&#12289;&#26356;&#20840;&#38754;&#30340;&#29992;&#25143;&#26032;&#38395;&#38405;&#35835;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#29992;&#25143;&#34892;&#20026;&#29983;&#25104;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#22686;&#24378;&#20174;&#29992;&#25143;&#32534;&#30721;&#22120;&#27966;&#29983;&#20986;&#30340;&#29992;&#25143;&#34920;&#31034;&#21521;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19978;&#36848;&#39044;&#35757;&#32451;&#30340;&#29992;&#25143;&#24314;&#27169;&#26469;&#36827;&#34892;&#26032;&#38395;&#25512;&#33616;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
News recommendation aims to predict click behaviors based on user behaviors. How to effectively model the user representations is the key to recommending preferred news. Existing works are mostly focused on improvements in the supervised fine-tuning stage. However, there is still a lack of PLM-based unsupervised pre-training methods optimized for user representations. In this work, we propose an unsupervised pre-training paradigm with two tasks, i.e. user behavior masking and user behavior generation, both towards effective user behavior modeling. Firstly, we introduce the user behavior masking pre-training task to recover the masked user behaviors based on their contextual behaviors. In this way, the model could capture a much stronger and more comprehensive user news reading pattern. Besides, we incorporate a novel auxiliary user behavior generation pre-training task to enhance the user representation vector derived from the user encoder. We use the above pre-trained user modeling en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21452;&#37325;&#20351;&#29992;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20221;&#23450;&#21046;&#30340;&#21452;&#37325;&#20351;&#29992;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#29366;&#20917;&#21644;&#21487;&#33021;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.08315</link><description>&lt;p&gt;
&#33606;&#26840;&#29611;&#29808;&#65306;&#25506;&#31350;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#21452;&#37325;&#20351;&#29992;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;
Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing. (arXiv:2304.08315v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21452;&#37325;&#20351;&#29992;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20221;&#23450;&#21046;&#30340;&#21452;&#37325;&#20351;&#29992;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#29366;&#20917;&#21644;&#21487;&#33021;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#37325;&#20351;&#29992;&#26159;&#25351;&#26377;&#24847;&#23558;&#25216;&#26415;&#21644;&#31185;&#23398;&#25104;&#26524;&#29992;&#20110;&#26377;&#23475;&#30446;&#30340;&#30340;&#38382;&#39064;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#23578;&#26410;&#26126;&#30830;&#23450;&#20041;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;NLP&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#21644;&#22312;&#31038;&#20250;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#20869;&#37096;&#36816;&#34892;&#26041;&#24335;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#36879;&#26126;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#21452;&#37325;&#20351;&#29992;&#30340;&#38382;&#39064;&#20197;&#21450;&#38480;&#21046;&#21452;&#37325;&#20351;&#29992;&#30340;&#28508;&#22312;&#26041;&#27861;&#23545;&#20110;&#20943;&#23569;&#30740;&#31350;&#21644;&#24320;&#21457;&#30340;&#28508;&#22312;&#21361;&#23475;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;NLP&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#23545;&#35813;&#38382;&#39064;&#30340;&#28145;&#24230;&#29702;&#35299;&#21644;&#35266;&#28857;&#65292;&#24182;&#35780;&#20272;&#29616;&#26377;&#30340;&#25903;&#25345;&#24773;&#20917;&#12290;&#26681;&#25454;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20221;&#23450;&#21046;&#30340;&#21452;&#37325;&#20351;&#29992;&#23450;&#20041;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#23545;&#20182;&#20204;&#30340;&#30740;&#31350;&#30340;&#28508;&#22312;&#21452;&#37325;&#20351;&#29992;&#38382;&#39064;&#34920;&#31034;&#20851;&#20999;&#65292;&#20294;&#21482;&#37319;&#21462;&#26377;&#38480;&#30340;&#34892;&#21160;&#12290;&#22522;&#20110;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#29366;&#20917;&#21644;&#21487;&#33021;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dual use, the intentional, harmful reuse of technology and scientific artefacts, is a problem yet to be well-defined within the context of Natural Language Processing (NLP). However, as NLP technologies continue to advance and become increasingly widespread in society, their inner workings have become increasingly opaque. Therefore, understanding dual use concerns and potential ways of limiting them is critical to minimising the potential harms of research and development. In this paper, we conduct a survey of NLP researchers and practitioners to understand the depth and their perspective of the problem as well as to assess existing available support. Based on the results of our survey, we offer a definition of dual use that is tailored to the needs of the NLP community. The survey revealed that a majority of researchers are concerned about the potential dual use of their research but only take limited action toward it. In light of the survey results, we discuss the current state and p
&lt;/p&gt;</description></item><item><title>Multimodal C4&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#12289;&#20197;&#22270;&#20687;&#19982;&#25991;&#26412;&#20132;&#26367;&#24418;&#24335;&#23384;&#22312;&#30340;&#25968;&#25454;&#24211;&#65292;&#20854;&#20351;&#29992;&#32447;&#24615;&#20998;&#37197;&#31639;&#27861;&#23558;&#22270;&#20687;&#25918;&#21040;&#38271;&#25991;&#26412;&#27573;&#33853;&#20013;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#21644;&#22797;&#26434;&#30456;&#20851;&#24230;&#25552;&#31034;&#30340;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2304.06939</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;C4&#65306;&#19968;&#31181;&#21253;&#21547;&#22823;&#37327;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#24320;&#25918;&#24335;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text. (arXiv:2304.06939v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06939
&lt;/p&gt;
&lt;p&gt;
Multimodal C4&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#12289;&#20197;&#22270;&#20687;&#19982;&#25991;&#26412;&#20132;&#26367;&#24418;&#24335;&#23384;&#22312;&#30340;&#25968;&#25454;&#24211;&#65292;&#20854;&#20351;&#29992;&#32447;&#24615;&#20998;&#37197;&#31639;&#27861;&#23558;&#22270;&#20687;&#25918;&#21040;&#38271;&#25991;&#26412;&#27573;&#33853;&#20013;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#21644;&#22797;&#26434;&#30456;&#20851;&#24230;&#25552;&#31034;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#25903;&#25345;&#20219;&#24847;&#20132;&#26367;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;, &#36825;&#31181;&#26684;&#24335;&#19981;&#20165;&#21487;&#20197;&#36890;&#36807;&#20132;&#26367;&#29420;&#31435;&#30417;&#30563;&#30340;(&#22270;&#20687;,&#25991;&#26412;)&#31034;&#20363;&#26469;&#36827;&#34892;&#20302;&#27425;&#23398;&#20064;,&#32780;&#19988;&#21487;&#20197;&#24212;&#23545;&#26356;&#22797;&#26434;&#30340;&#25552;&#31034;, &#28041;&#21450;&#22270;&#20687;&#38388;&#20114;&#21160;,&#20363;&#22914;&#8220;&#22270;&#20687;A&#21644;&#22270;&#20687;B&#26377;&#20160;&#20040;&#20849;&#21516;&#20043;&#22788;?&#8221;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#31867;&#20284;&#20110;&#20132;&#26367;&#22270;&#20687;+&#25991;&#26412;&#30340;web&#35821;&#26009;&#24211;&#12290;&#20294;&#26159;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#31181;&#24418;&#24335;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#36824;&#27809;&#26377;&#20844;&#24320;&#25552;&#20379;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;Multimodal C4 (mmc4)&#65292;&#36825;&#26159;&#19968;&#20010;&#21152;&#24378;&#29256;&#30340;c4&#25991;&#26412;&#24211;&#65292;&#20854;&#20013;&#25554;&#20837;&#20102;&#22270;&#20687;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#32447;&#24615;&#20998;&#37197;&#31639;&#27861;&#65292;&#20351;&#29992;CLIP&#29305;&#24449;&#23558;&#22270;&#20687;&#25918;&#21040;&#26356;&#38271;&#30340;&#25991;&#26412;&#20307;&#20013;&#65292;&#27492;&#36807;&#31243;&#20248;&#20110;&#20854;&#20182;&#26367;&#20195;&#26041;&#26696;&#12290;mmc4&#28085;&#30422;&#20102;&#35832;&#22914;&#28921;&#39274;&#65292;&#26053;&#28216;&#65292;&#25216;&#26415;&#31561;&#26085;&#24120;&#20027;&#39064;&#12290;&#23545;&#38543;&#26426;&#26679;&#26412;&#30340;&#25163;&#21160;&#26816;&#26597;&#34920;&#26126;&#65292;&#32477;&#22823;&#22810;&#25968;(90%)&#30340;&#22270;&#20687;&#19982;&#20027;&#39064;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context vision and language models like Flamingo support arbitrarily interleaved sequences of images and text as input. This format not only enables few-shot learning via interleaving independent supervised (image, text) examples, but also, more complex prompts involving interaction between images, e.g., "What do image A and image B have in common?" To support this interface, pretraining occurs over web corpora that similarly contain interleaved images+text. To date, however, large-scale data of this form have not been publicly available.  We release Multimodal C4 (mmc4), an augmentation of the popular text-only c4 corpus with images interleaved. We use a linear assignment algorithm to place images into longer bodies of text using CLIP features, a process that we show outperforms alternatives. mmc4 spans everyday topics like cooking, travel, technology, etc. A manual inspection of a random sample of documents shows that a vast majority (90%) of images are topically relevant, and tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#65292;&#35777;&#26126;&#20854;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#21487;&#20197;&#24847;&#22806;&#22320;&#24573;&#30053;&#25552;&#31034;&#35821;&#20041;&#65292;&#24182;&#19988;&#36328;&#35821;&#35328;&#31034;&#20363;&#21487;&#20197;&#20026;&#20302;&#36164;&#28304;&#32763;&#35793;&#25552;&#20379;&#26356;&#22909;&#30340;&#20219;&#21153;&#25351;&#23548;&#12290;&#20294;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#27169;&#22411;ChatGPT&#20173;&#28982;&#33853;&#21518;&#20110;&#30417;&#30563;&#22522;&#32447;NLLB&#12290;</title><link>http://arxiv.org/abs/2304.04675</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;&#65306;&#23454;&#35777;&#32467;&#26524;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis. (arXiv:2304.04675v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#65292;&#35777;&#26126;&#20854;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#21487;&#20197;&#24847;&#22806;&#22320;&#24573;&#30053;&#25552;&#31034;&#35821;&#20041;&#65292;&#24182;&#19988;&#36328;&#35821;&#35328;&#31034;&#20363;&#21487;&#20197;&#20026;&#20302;&#36164;&#28304;&#32763;&#35793;&#25552;&#20379;&#26356;&#22909;&#30340;&#20219;&#21153;&#25351;&#23548;&#12290;&#20294;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#27169;&#22411;ChatGPT&#20173;&#28982;&#33853;&#21518;&#20110;&#30417;&#30563;&#22522;&#32447;NLLB&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;(MMT)&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#22238;&#31572;&#20004;&#20010;&#38382;&#39064;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;LLMs&#22312;MMT&#20013;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#65306;1) LLMs&#22312;&#32763;&#35793;&#22823;&#37327;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#22914;&#20309;&#65311;2) &#21738;&#20123;&#22240;&#32032;&#20250;&#24433;&#21709;LLMs&#22312;&#32763;&#35793;&#20013;&#30340;&#34920;&#29616;&#65311;&#25105;&#20204;&#35780;&#20272;&#20102;&#21253;&#25324;XGLM&#12289;OPT&#12289;BLOOMZ&#21644;ChatGPT&#22312;&#20869;&#30340;&#20960;&#20010;&#21463;&#27426;&#36814;&#30340;LLMs&#22312;102&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#27169;&#22411;ChatGPT&#22312;83.33%&#30340;&#32763;&#35793;&#26041;&#21521;&#19978;&#20063;&#33853;&#21518;&#20110;&#30417;&#30563;&#22522;&#32447;NLLB&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#29992;&#20110;MMT&#26102;&#65292;LLMs&#34920;&#29616;&#20986;&#26032;&#30340;&#24037;&#20316;&#27169;&#24335;&#12290;&#39318;&#20808;&#65292;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#65292;&#25552;&#31034;&#35821;&#20041;&#21487;&#33021;&#20250;&#34987;&#24847;&#22806;&#22320;&#24573;&#30053;&#65292;&#21363;&#20351;&#25552;&#31034;&#19981;&#21512;&#29702;&#65292;LLMs&#20173;&#28982;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#36328;&#35821;&#35328;&#31034;&#20363;&#21487;&#20197;&#20026;&#20302;&#36164;&#28304;&#32763;&#35793;&#25552;&#20379;&#27604;&#30456;&#21516;&#35821;&#35328;&#23545;&#20013;&#30340;&#31034;&#20363;&#26356;&#22909;&#30340;&#20219;&#21153;&#25351;&#23548;&#12290;&#31532;&#19977;&#65292;&#24403;&#32763;&#35793;&#20302;&#36164;&#28304;&#35821;&#35328;&#26102;&#65292;LLMs&#24448;&#24448;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;LLMs&#22312;MMT&#20013;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating a massive number of languages? 2) Which factors affect LLMs' performance in translation? We evaluate popular LLMs, including XGLM, OPT, BLOOMZ, and ChatGPT, on 102 languages. Our empirical results show that even the best model ChatGPT still lags behind the supervised baseline NLLB in 83.33% of translation directions. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, prompt semantics can surprisingly be ignored when given in-context exemplars, where LLMs still show strong performance even with unreasonable prompts. Second, cross-lingual exemplars can provide better task instruction for low-resource translation than exemplars in the same language pairs. Third
&lt;/p&gt;</description></item><item><title>MEGClass&#26159;&#19968;&#31181;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#30340;&#25991;&#26412;&#31890;&#24230;&#23454;&#29616;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#23618;&#20851;&#27880;&#26426;&#21046;&#20174;&#25991;&#26723;&#12289;&#21477;&#23376;&#21644;&#21333;&#35789;&#20013;&#25552;&#21462;&#36830;&#36143;&#19988;&#22810;&#26679;&#30340;&#23376;&#25991;&#26412;&#65292;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#35752;&#35770;&#22810;&#20010;&#20027;&#39064;&#30340;&#25991;&#26723;&#65292;&#24182;&#19988;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.01969</link><description>&lt;p&gt;
MEGClass: &#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#30340;&#25991;&#26412;&#31890;&#24230;&#23454;&#29616;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
MEGClass: Text Classification with Extremely Weak Supervision via Mutually-Enhancing Text Granularities. (arXiv:2304.01969v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01969
&lt;/p&gt;
&lt;p&gt;
MEGClass&#26159;&#19968;&#31181;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#30340;&#25991;&#26412;&#31890;&#24230;&#23454;&#29616;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#23618;&#20851;&#27880;&#26426;&#21046;&#20174;&#25991;&#26723;&#12289;&#21477;&#23376;&#21644;&#21333;&#35789;&#20013;&#25552;&#21462;&#36830;&#36143;&#19988;&#22810;&#26679;&#30340;&#23376;&#25991;&#26412;&#65292;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#35752;&#35770;&#22810;&#20010;&#20027;&#39064;&#30340;&#25991;&#26723;&#65292;&#24182;&#19988;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#65292;&#36825;&#22312;&#21160;&#24577;&#26032;&#20852;&#39046;&#22495;&#20013;&#26159;&#26114;&#36149;&#30340;&#12290;&#26576;&#20123;&#26041;&#27861;&#36890;&#36807;&#20165;&#20381;&#36182;&#31867;&#21517;&#34920;&#38754;&#25991;&#26412;&#20316;&#20026;&#26497;&#24369;&#30417;&#30563;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#32771;&#34385;&#21040;&#21333;&#19968;&#31867;&#21035;&#25991;&#26723;&#35752;&#35770;&#22810;&#20010;&#20027;&#39064;&#30340;&#24773;&#20917;&#12290;&#20027;&#39064;&#22810;&#26679;&#24615;&#21644;&#27169;&#31946;&#30340;&#21477;&#23376;&#21487;&#33021;&#20250;&#24341;&#20837;&#22122;&#22768;&#21040;&#25991;&#26723;&#30340;&#24213;&#23618;&#34920;&#31034;&#65292;&#20174;&#32780;&#24433;&#21709;&#39044;&#27979;&#31867;&#21035;&#30340;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#29420;&#31435;&#22320;&#20851;&#27880;&#25991;&#26723;&#12289;&#21477;&#23376;&#25110;&#21333;&#35789;&#30340;&#25991;&#26412;&#31890;&#24230;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#25105;&#20204;&#32852;&#21512;&#20174;&#25152;&#26377;&#19977;&#32773;&#20013;&#25552;&#21462;&#31895;&#31890;&#24230;&#25110;&#32454;&#31890;&#24230;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#26469;&#35782;&#21035;&#20998;&#31867;&#30340;&#37325;&#35201;&#23376;&#25991;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MEGClass&#65292;&#19968;&#31181;&#21033;&#29992;&#30456;&#20114;&#22686;&#24378;&#30340;&#25991;&#26412;&#31890;&#24230;&#36827;&#34892;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MEGClass&#36890;&#36807;&#20998;&#23618;&#20851;&#27880;&#26426;&#21046;&#20174;&#25991;&#26723;&#12289;&#21477;&#23376;&#21644;&#21333;&#35789;&#20013;&#25552;&#21462;&#36830;&#36143;&#19988;&#22810;&#26679;&#30340;&#23376;&#25991;&#26412;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#21644;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#31890;&#24230;&#30340;&#24369;&#20449;&#21495;&#65292;&#20197;&#20934;&#30830;&#20998;&#31867;&#25991;&#26723;&#65292;&#21363;&#20351;&#23427;&#20204;&#35752;&#35770;&#22810;&#20010;&#20027;&#39064;&#12290;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#26497;&#24369;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26377;&#30528;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification typically requires a substantial amount of human-annotated data to serve as supervision, which is costly to obtain in dynamic emerging domains. Certain methods seek to address this problem by solely relying on the surface text of class names to serve as extremely weak supervision. However, existing methods fail to account for single-class documents discussing multiple topics. Both topic diversity and vague sentences may introduce noise into the document's underlying representation and consequently the precision of the predicted class. Furthermore, current work focuses on text granularities (documents, sentences, or words) independently, which limits the degree of coarse- or fine-grained context that we can jointly extract from all three to identify significant subtext for classification. In order to address this problem, we propose MEGClass, an extremely weakly-supervised text classification method to exploit Mutually-Enhancing Text Granularities. Specifically, MEGC
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#65288;&#23884;&#20837;&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#26469;&#23454;&#29616;&#23398;&#20064;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#36127;&#37319;&#26679;&#26041;&#27861;&#24182;&#22312;&#22810;&#39033;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17475</link><description>&lt;p&gt;
&#36229;&#36234;&#36127;&#37319;&#26679;&#30340;&#39640;&#25928;&#20998;&#24067;&#24335;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient distributed representations beyond negative sampling. (arXiv:2303.17475v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#65288;&#23884;&#20837;&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#26469;&#23454;&#29616;&#23398;&#20064;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#36127;&#37319;&#26679;&#26041;&#27861;&#24182;&#22312;&#22810;&#39033;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23398;&#20064;&#20998;&#24067;&#24335;&#34920;&#31034;&#65288;&#20063;&#31216;&#20026;&#23884;&#20837;&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#20010;&#31867;&#20284;&#20110;Word2Vec&#31639;&#27861;&#20013;&#24341;&#20837;&#24182;&#22312;&#22810;&#20010;&#24037;&#20316;&#20013;&#37319;&#29992;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#23454;&#29616;&#12290;&#20248;&#21270;&#35745;&#31639;&#30340;&#29942;&#39048;&#26159;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#35745;&#31639;&#65292;&#36825;&#38656;&#35201;&#19982;&#26679;&#26412;&#22823;&#23567;&#21576;&#20108;&#27425;&#27604;&#20363;&#30340;&#25805;&#20316;&#25968;&#12290;&#36825;&#31181;&#22797;&#26434;&#24230;&#19981;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#25152;&#20197;&#36127;&#37319;&#26679;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19982;&#26679;&#26412;&#22823;&#23567;&#32447;&#24615;&#30456;&#20851;&#30340;&#26102;&#38388;&#20869;&#33719;&#24471;&#20998;&#24067;&#24335;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36127;&#37319;&#26679;&#20250;&#25913;&#21464;&#25439;&#22833;&#20989;&#25968;&#65292;&#22240;&#27492;&#35299;&#20915;&#30340;&#26159;&#19982;&#26368;&#21021;&#25552;&#20986;&#30340;&#19981;&#21516;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#22914;&#20309;&#36890;&#36807;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#65292;&#20174;&#32780;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#26469;&#23398;&#20064;&#20998;&#24067;&#24335;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23884;&#20837;&#36136;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#26041;&#38754;&#20248;&#20110;&#36127;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article describes an efficient method to learn distributed representations, also known as embeddings. This is accomplished minimizing an objective function similar to the one introduced in the Word2Vec algorithm and later adopted in several works. The optimization computational bottleneck is the calculation of the softmax normalization constants for which a number of operations scaling quadratically with the sample size is required. This complexity is unsuited for large datasets and negative sampling is a popular workaround, allowing one to obtain distributed representations in linear time with respect to the sample size. Negative sampling consists, however, in a change of the loss function and hence solves a different optimization problem from the one originally proposed. Our contribution is to show that the sotfmax normalization constants can be estimated in linear time, allowing us to design an efficient optimization strategy to learn distributed representations. We test our ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#21033;&#29992;&#19981;&#23545;&#31216;&#24615;&#36827;&#34892;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38024;&#23545;&#32467;&#26500;&#21270;&#36755;&#20986;&#38382;&#39064;&#20135;&#29983;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#22312;&#23553;&#38381;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21512;&#25104;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;180&#19975;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#27492;&#25968;&#25454;&#38598;&#23545;&#23567;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#36828;&#36229;&#20808;&#21069;&#39046;&#20808;&#25216;&#26415;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.04132</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#23545;&#31216;&#24615;&#36827;&#34892;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#65306;SynthIE&#21644;&#20449;&#24687;&#25552;&#21462;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction. (arXiv:2303.04132v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#21033;&#29992;&#19981;&#23545;&#31216;&#24615;&#36827;&#34892;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38024;&#23545;&#32467;&#26500;&#21270;&#36755;&#20986;&#38382;&#39064;&#20135;&#29983;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#22312;&#23553;&#38381;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21512;&#25104;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;180&#19975;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#27492;&#25968;&#25454;&#38598;&#23545;&#23567;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#36828;&#36229;&#20808;&#21069;&#39046;&#20808;&#25216;&#26415;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#21363;&#20351;&#23545;&#20110;LLM&#26080;&#27861;&#30452;&#25509;&#35299;&#20915;&#30340;&#20219;&#21153;&#65292;&#20063;&#21487;&#20197;&#21512;&#25104;&#29983;&#25104;&#26377;&#29992;&#30340;&#25968;&#25454;&#65306;&#23545;&#20110;&#20855;&#26377;&#32467;&#26500;&#21270;&#36755;&#20986;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#25552;&#31034;LLM&#22312;&#21453;&#21521;&#26041;&#21521;&#19978;&#25191;&#34892;&#20219;&#21153;&#65292;&#36890;&#36807;&#20026;&#30446;&#26631;&#36755;&#20986;&#32467;&#26500;&#29983;&#25104;&#21512;&#29702;&#30340;&#36755;&#20837;&#25991;&#26412;&#12290;&#21033;&#29992;&#20219;&#21153;&#22256;&#38590;&#24230;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#21487;&#20197;&#29983;&#25104;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#22797;&#26434;&#20219;&#21153;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#23553;&#38381;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#39046;&#22495;&#38590;&#20197;&#25910;&#38598;&#21040;&#30495;&#23454;&#25968;&#25454;&#65292;&#33267;&#20170;&#27809;&#26377;&#20196;&#20154;&#28385;&#24847;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#12290;&#25105;&#20204;&#21512;&#25104;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;180&#19975;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#35777;&#26126;&#20854;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#20855;&#26377;&#26356;&#22909;&#30340;&#36136;&#37327;&#65292;&#24182;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#23545;&#23567;&#22411;&#27169;&#22411;&#65288;220M&#21644;770M&#21442;&#25968;&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#20123;&#27169;&#22411;&#34987;&#31216;&#20026;SynthIE&#65292;&#20197;&#36828;&#36828;&#36229;&#36807;&#20808;&#21069;&#39046;&#20808;&#25216;&#26415;&#30340;&#27700;&#24179;&#65288;&#20855;&#26377;&#30456;&#21516;&#30340;&#27169;&#22411;&#22823;&#23567;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, establish its superior quality compared to existing datasets in a human evaluation, and use it to finetune small models (220M and 770M parameters), termed SynthIE, that outperform the prior state of the art (with equal model size) by a substantial margin of 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.12247</link><description>&lt;p&gt;
&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#65306;&#19968;&#31181;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12247
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#25152;&#38656;&#30340;&#20132;&#20114;&#22914;&#20309;&#36827;&#34892;&#37327;&#21270;&#65311;&#26368;&#36866;&#21512;&#25429;&#25417;&#36825;&#20123;&#20132;&#20114;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#20160;&#20040;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;&#26469;&#37327;&#21270;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#19977;&#20010;&#34913;&#37327;&#26631;&#20934;&#31216;&#20026;&#22810;&#27169;&#24577;&#20998;&#24067;&#65288;&#25110;&#31616;&#31216;PID&#65289;&#30340;PID&#32479;&#35745;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#20998;&#24067;&#12290;&#20026;&#20102;&#39564;&#35777;PID&#20272;&#35745;&#65292;&#25105;&#20204;&#22312;&#24050;&#30693;PID&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#28151;&#21512;&#19987;&#23478;&#23545;&#35805;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#26032;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65288;MoE-LMs&#65289;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#38024;&#23545;&#23545;&#35805;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#35805;&#31649;&#29702;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10850</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#28151;&#21512;&#19987;&#23478;&#23545;&#35805;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management. (arXiv:2302.10850v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#28151;&#21512;&#19987;&#23478;&#23545;&#35805;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#26032;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65288;MoE-LMs&#65289;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#38024;&#23545;&#23545;&#35805;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#35805;&#31649;&#29702;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#24320;&#21457;&#23545;&#35805;&#31649;&#29702;&#65288;DM&#65289;&#20195;&#29702;&#65292;&#23454;&#29616;&#38750;&#30446;&#26631;&#23548;&#21521;&#65292;&#36827;&#34892;&#23500;&#26377;&#20869;&#23481;&#30340;&#23545;&#35805;&#65292;&#26368;&#22823;&#21270;&#29992;&#25143;&#28385;&#24847;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#23545;&#35805;&#32842;&#22825;&#26426;&#22120;&#20154;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#24378;&#21270;&#23398;&#20064;&#38656;&#35201;&#22312;&#32447;&#25506;&#32034;&#20197;&#26377;&#25928;&#23398;&#20064;&#65292;&#32780;&#25910;&#38598;&#26032;&#39062;&#30340;&#20154;&#26426;&#20132;&#20114;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#19981;&#23433;&#20840;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#38754;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#32452;&#21512;&#21160;&#20316;&#31354;&#38388;&#26102;&#21464;&#24471;&#26356;&#20026;&#20005;&#37325;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#20197;&#35789;&#32423;&#21035;&#29983;&#25104;&#21709;&#24212;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22810;&#31181;&#38024;&#23545;&#23545;&#35805;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65288;MoE-LMs&#65289; - &#19968;&#31181;&#25429;&#25417;&#22810;&#26679;&#35821;&#20041;&#65292;&#29983;&#25104;&#21453;&#26144;&#19981;&#21516;&#24847;&#22270;&#30340;&#35805;&#35821;&#30340;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#22810;&#36718;&#23545;&#35805;&#31649;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;MoE-LM&#32467;&#26500;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#34892;&#21160;&#31354;&#38388;&#30340;&#22823;&#23567;&#65292;&#24182;&#25552;&#39640;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23545;&#35805;&#31649;&#29702;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has shown great promise for developing dialogue management (DM) agents that are non-myopic, conduct rich conversations, and maximize overall user satisfaction. Despite recent developments in RL and language models (LMs), using RL to power conversational chatbots remains challenging, in part because RL requires online exploration to learn effectively, whereas collecting novel human-bot interactions can be expensive and unsafe. This issue is exacerbated by the combinatorial action spaces facing these algorithms, as most LM agents generate responses at the word level. We develop a variety of RL algorithms, specialized to dialogue planning, that leverage recent Mixture-of-Expert Language Models (MoE-LMs) -- models that capture diverse semantics, generate utterances reflecting different intents, and are amenable for multi-turn DM. By exploiting MoE-LM structure, our methods significantly reduce the size of the action space and improve the efficacy of RL-based DM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#24341;&#20837;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;NLI&#27169;&#22411;&#35780;&#20272;&#29983;&#25104;&#30340;&#21477;&#23376;&#26159;&#21542;&#31526;&#21512;&#12289;&#19982;&#21407;&#22987;&#25991;&#26412;&#30456;&#30683;&#30462;&#25110;&#20013;&#31435;&#12290;&#26368;&#22823;&#21270;&#20013;&#31435;&#31867;&#21035;&#30340;NLI&#31574;&#30053;&#25552;&#20379;&#20102;&#26368;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#25991;&#26412;&#65292;&#26080;&#35770;&#21442;&#25968;&#21462;&#20540;&#22914;&#20309;&#12290;</title><link>http://arxiv.org/abs/2302.08577</link><description>&lt;p&gt;
&#20445;&#25345;&#20013;&#31435;&#65306;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25913;&#36827;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Keep it Neutral: Using Natural Language Inference to Improve Generation. (arXiv:2302.08577v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#24341;&#20837;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;NLI&#27169;&#22411;&#35780;&#20272;&#29983;&#25104;&#30340;&#21477;&#23376;&#26159;&#21542;&#31526;&#21512;&#12289;&#19982;&#21407;&#22987;&#25991;&#26412;&#30456;&#30683;&#30462;&#25110;&#20013;&#31435;&#12290;&#26368;&#22823;&#21270;&#20013;&#31435;&#31867;&#21035;&#30340;NLI&#31574;&#30053;&#25552;&#20379;&#20102;&#26368;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#25991;&#26412;&#65292;&#26080;&#35770;&#21442;&#25968;&#21462;&#20540;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#24341;&#20837;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;NLI&#27169;&#22411;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#21477;&#23376;&#26159;&#21542;&#31526;&#21512;&#12289;&#19982;&#21407;&#22987;&#25991;&#26412;&#30456;&#30683;&#30462;&#25110;&#20013;&#31435;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;NLI&#20219;&#21153;&#33021;&#22815;&#39044;&#27979;GPT-3&#29983;&#25104;&#38169;&#35823;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#20026;GPT-J&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;NLI&#30340;&#29983;&#25104;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#38169;&#35823;&#31867;&#22411;&#21644;&#25972;&#20307;&#36136;&#37327;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26680;&#24515;&#37319;&#26679;&#30340;&#38543;&#26426;&#21442;&#25968;&#20540;&#36739;&#39640;&#26102;&#65292;&#26368;&#22823;&#21270;&#34164;&#28085;&#20851;&#31995;&#30340;NLI&#31574;&#30053;&#25913;&#21892;&#20102;&#25991;&#26412;&#29983;&#25104;&#65292;&#32780;&#22312;&#21442;&#25968;&#20540;&#36739;&#20302;&#26102;&#65292;&#26368;&#22823;&#21270;&#30683;&#30462;&#20851;&#31995;&#30340;&#31574;&#30053;&#23454;&#38469;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#22823;&#21270;&#20013;&#31435;&#31867;&#21035;&#30340;NLI&#31574;&#30053;&#25552;&#20379;&#20102;&#26368;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#25991;&#26412;&#65288;&#26174;&#33879;&#20248;&#20110;&#26222;&#36890;&#29983;&#25104;&#22120;&#65289;&#65292;&#26080;&#35770;&#21442;&#25968;&#21462;&#20540;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore incorporating natural language inference (NLI) into the text generative pipeline by using a pre-trained NLI model to assess whether a generated sentence entails, contradicts, or is neutral to the prompt and preceding text. First, we show that the NLI task is predictive of generation errors made by GPT-3. We use these results to develop an NLI-informed generation procedure for GPT-J. Then, we evaluate these generations by obtaining human annotations on error types and overall quality. We find that an NLI strategy of maximizing entailment improves text generation when the nucleus sampling randomness parameter value is high, while one which maximizes contradiction is in fact productive when the parameter value is low. Overall, though, we demonstrate that an NLI strategy of maximizing the neutral class provides the highest quality of generated text (significantly better than the vanilla generations), regardless of parameter value.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35838;&#31243;&#37319;&#26679;&#31574;&#30053;&#30340;&#23494;&#38598;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#20266;&#26597;&#35810;&#65292;&#36880;&#27493;&#22686;&#24378;&#20102;&#29983;&#25104;&#30340;&#26597;&#35810;&#21644;&#30495;&#23454;&#26597;&#35810;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.09114</link><description>&lt;p&gt;
CAPSTONE: &#20351;&#29992;&#35838;&#31243;&#37319;&#26679;&#36827;&#34892;&#23494;&#38598;&#26816;&#32034;&#19982;&#25991;&#26723;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
CAPSTONE: Curriculum Sampling for Dense Retrieval with Document Expansion. (arXiv:2212.09114v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35838;&#31243;&#37319;&#26679;&#31574;&#30053;&#30340;&#23494;&#38598;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#20266;&#26597;&#35810;&#65292;&#36880;&#27493;&#22686;&#24378;&#20102;&#29983;&#25104;&#30340;&#26597;&#35810;&#21644;&#30495;&#23454;&#26597;&#35810;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#32534;&#30721;&#22120;&#24050;&#25104;&#20026;&#23494;&#38598;&#26816;&#32034;&#30340;&#20107;&#23454;&#26631;&#20934;&#26550;&#26500;&#12290;&#36890;&#24120;&#65292;&#23427;&#29420;&#31435;&#35745;&#31639;&#26597;&#35810;&#21644;&#25991;&#26723;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#22240;&#27492;&#26410;&#33021;&#20805;&#20998;&#25429;&#25417;&#26597;&#35810;&#21644;&#25991;&#26723;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#33719;&#24471;&#26597;&#35810;&#30456;&#20851;&#30340;&#25991;&#26723;&#34920;&#31034;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23427;&#36890;&#36807;&#30495;&#23454;&#26597;&#35810;&#25193;&#23637;&#25991;&#26723;&#65292;&#20294;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#23427;&#29992;&#29983;&#25104;&#30340;&#26597;&#35810;&#26367;&#25442;&#30495;&#23454;&#26597;&#35810;&#12290;&#36825;&#31181;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#23548;&#33268;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#22312;&#35745;&#31639;&#25991;&#26723;&#34920;&#31034;&#26102;&#26356;&#21152;&#37325;&#35270;&#26597;&#35810;&#20449;&#24687;&#65292;&#32780;&#24573;&#35270;&#25991;&#26723;&#12290;&#22240;&#27492;&#65292;&#23427;&#30340;&#24615;&#33021;&#27604;&#26222;&#36890;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#36824;&#35201;&#24046;&#65292;&#22240;&#20026;&#23427;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#29983;&#25104;&#30340;&#26597;&#35810;&#21644;&#30495;&#23454;&#26597;&#35810;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#37319;&#26679;&#31574;&#30053;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20266;&#26597;&#35810;&#65292;&#24182;&#36880;&#27493;&#22686;&#24378;&#29983;&#25104;&#30340;&#26597;&#35810;&#21644;&#30495;&#23454;&#26597;&#35810;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dual-encoder has become the de facto architecture for dense retrieval. Typically, it computes the latent representations of the query and document independently, thus failing to fully capture the interactions between the query and document. To alleviate this, recent research has focused on obtaining query-informed document representations. During training, it expands the document with a real query, but during inference, it replaces the real query with a generated one. This inconsistency between training and inference causes the dense retrieval model to prioritize query information while disregarding the document when computing the document representation. Consequently, it performs even worse than the vanilla dense retrieval model because its performance heavily relies on the relevance between the generated queries and the real query.In this paper, we propose a curriculum sampling strategy that utilizes pseudo queries during training and progressively enhances the relevance between 
&lt;/p&gt;</description></item><item><title>MelHuBERT&#26159;&#22522;&#20110;Mel&#39057;&#35889;&#22270;&#30340;&#31616;&#21270;&#29256;HuBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#25439;&#22833;&#20989;&#25968;&#12289;&#36755;&#20837;&#34920;&#31034;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#65292;&#22312;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#21033;&#34920;&#29616;&#65292;&#33410;&#30465;&#20102;31.2%&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#21644;33.5%&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2211.09944</link><description>&lt;p&gt;
MelHuBERT: &#19968;&#31181;&#22522;&#20110;Mel&#39057;&#35889;&#22270;&#30340;&#31616;&#21270;HuBERT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MelHuBERT: A simplified HuBERT on Mel spectrograms. (arXiv:2211.09944v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09944
&lt;/p&gt;
&lt;p&gt;
MelHuBERT&#26159;&#22522;&#20110;Mel&#39057;&#35889;&#22270;&#30340;&#31616;&#21270;&#29256;HuBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#25439;&#22833;&#20989;&#25968;&#12289;&#36755;&#20837;&#34920;&#31034;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#65292;&#22312;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#21033;&#34920;&#29616;&#65292;&#33410;&#30465;&#20102;31.2%&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#21644;33.5%&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#33258;&#30417;&#30563;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#22810;&#20010;GPU&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20005;&#37325;&#38480;&#21046;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#20943;&#23569;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;HuBERT&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#25104;&#21151;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#12290;&#25105;&#20204;&#25913;&#36827;&#24182;&#31616;&#21270;&#20102;&#20960;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#25439;&#22833;&#20989;&#25968;&#12289;&#36755;&#20837;&#34920;&#31034;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;MelHuBERT&#22312;&#38899;&#32032;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#22343;&#33021;&#21462;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#33410;&#30465;&#20102;31.2%&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#65292;&#25110;&#31561;&#25928;&#22320;&#27599;&#31186;&#35821;&#38899;&#33410;&#30465;&#20102;33.5%&#30340;MACs&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/nervjack2/MelHuBERT&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised models have had great success in learning speech representations that can generalize to various downstream tasks. However, most self-supervised models require a large amount of compute and multiple GPUs to train, significantly hampering the development of self-supervised learning. In an attempt to reduce the computation of training, we revisit the training of HuBERT, a highly successful self-supervised model. We improve and simplify several key components, including the loss function, input representation, and training in multiple stages. Our model, MelHuBERT, is able to achieve favorable performance on phone recognition, speaker identification, and automatic speech recognition against HuBERT, while saving 31.2% of the pre-training time, or equivalently 33.5% MACs per one second speech. The code and pre-trained models are available in https://github.com/nervjack2/MelHuBERT.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20174;&#24179;&#34892;&#35821;&#38899;/&#25991;&#26412;&#25968;&#25454;&#20013;&#25366;&#25496;&#35789;&#36793;&#30028;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#36328;&#39046;&#22495;&#21644;&#36164;&#28304;&#21294;&#20047;&#22330;&#26223;&#19979;&#26174;&#33879;&#25552;&#39640;&#27721;&#35821;&#35789;&#20998;&#21106;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.17122</link><description>&lt;p&gt;
&#22312;&#35821;&#38899;&#20013;&#25366;&#25496;&#35789;&#36793;&#30028;&#20316;&#20026;&#22825;&#28982;&#27880;&#37322;&#30340;&#35789;&#20998;&#21106;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Mining Word Boundaries in Speech as Naturally Annotated Word Segmentation Data. (arXiv:2210.17122v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20174;&#24179;&#34892;&#35821;&#38899;/&#25991;&#26412;&#25968;&#25454;&#20013;&#25366;&#25496;&#35789;&#36793;&#30028;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#36328;&#39046;&#22495;&#21644;&#36164;&#28304;&#21294;&#20047;&#22330;&#26223;&#19979;&#26174;&#33879;&#25552;&#39640;&#27721;&#35821;&#35789;&#20998;&#21106;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21463;&#26089;&#26399;&#23545;&#20110;&#25506;&#32034;&#27721;&#35821;&#35789;&#20998;&#21106;&#22825;&#28982;&#27880;&#37322;&#25968;&#25454;&#30340;&#30740;&#31350;&#20197;&#21450;&#26368;&#36817;&#20851;&#20110;&#35821;&#38899;&#19982;&#25991;&#26412;&#22788;&#29702;&#38598;&#25104;&#30340;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#39318;&#27425;&#25552;&#20986;&#20102;&#20174;&#24179;&#34892;&#35821;&#38899;/&#25991;&#26412;&#25968;&#25454;&#20013;&#25366;&#25496;&#35789;&#36793;&#30028;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#19982;&#25105;&#20204;&#23454;&#39564;&#20013;&#20351;&#29992;&#30340;&#35789;&#20998;&#21106;&#25968;&#25454;&#30456;&#20851;&#30340;&#20114;&#32852;&#32593;&#28304;&#25910;&#38598;&#24179;&#34892;&#35821;&#38899;/&#25991;&#26412;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#33719;&#24471;&#23383;&#31526;&#32423;&#23545;&#40784;&#65292;&#24182;&#26681;&#25454;&#30456;&#37051;&#23383;&#31526;&#20043;&#38388;&#30340;&#20572;&#39039;&#26102;&#38271;&#35774;&#35745;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#26469;&#30830;&#23450;&#35789;&#36793;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#23436;&#25972;-&#28982;&#21518;-&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#39069;&#22806;&#30340;&#22825;&#28982;&#27880;&#37322;&#25968;&#25454;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36328;&#39046;&#22495;&#21644;&#36164;&#28304;&#21294;&#20047;&#22330;&#26223;&#19979;&#30340;&#35789;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by early research on exploring naturally annotated data for Chinese word segmentation (CWS), and also by recent research on integration of speech and text processing, this work for the first time proposes to mine word boundaries from parallel speech/text data. First we collect parallel speech/text data from two Internet sources that are related with CWS data used in our experiments. Then, we obtain character-level alignments and design simple heuristic rules for determining word boundaries according to pause duration between adjacent characters. Finally, we present an effective complete-then-train strategy that can better utilize extra naturally annotated data for model training. Experiments demonstrate our approach can significantly boost CWS performance in both cross-domain and low-resource scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#23558;&#20174;&#19968;&#33324;&#39046;&#22495;&#25110;&#30456;&#20851;&#39046;&#22495;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#24494;&#35843;&#21040;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.12770</link><description>&lt;p&gt;
&#20851;&#20110;&#20020;&#24202;&#25991;&#26412;&#25366;&#25496;&#30340;&#36328;&#39046;&#22495;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65306;&#22312;&#25968;&#25454;&#21463;&#38480;&#24494;&#35843;&#20013;&#23427;&#20204;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
On Cross-Domain Pre-Trained Language Models for Clinical Text Mining: How Do They Perform on Data-Constrained Fine-Tuning?. (arXiv:2210.12770v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#23558;&#20174;&#19968;&#33324;&#39046;&#22495;&#25110;&#30456;&#20851;&#39046;&#22495;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#24494;&#35843;&#21040;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#20351;&#29992;&#20174;&#19968;&#33324;&#25110;&#30456;&#20851;&#39046;&#22495;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#23558;&#20854;&#24494;&#35843;&#21040;&#29305;&#23450;&#39046;&#22495;&#21644;&#20219;&#21153;&#19978;&#65292;&#24182;&#20351;&#29992;&#26032;&#20219;&#21153;&#20013;&#21487;&#29992;&#30340;&#26377;&#38480;&#36164;&#28304;&#36827;&#34892;&#24494;&#35843;&#65292;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#19968;&#20010;&#27969;&#34892;&#30340;&#23454;&#36341;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#36825;&#31181;&#20551;&#35774;&#65292;&#24182;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20855;&#20307;&#26159;&#22312;&#33647;&#29289;&#21450;&#20854;&#30456;&#20851;&#23646;&#24615;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;Transformer&#27169;&#22411;&#21644;&#36890;&#36807;&#24494;&#35843;BERT-based LLMs&#65288;&#21253;&#25324;BERT-base&#12289;BioBERT&#21644;ClinicalBERT&#65289;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#20123;&#27169;&#22411;&#21450;&#20854;&#25193;&#23637;&#27169;&#22411;&#19982;&#24102;&#26377;CRF&#23618;&#30340;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;n2c2-2018&#20849;&#20139;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24320;&#21457;&#21644;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;1&#65289;CRF&#23618;&#23545;&#25152;&#26377;&#31070;&#32463;&#27169;&#22411;&#37117;&#36215;&#21040;&#20102;&#31215;&#26497;&#30340;&#24433;&#21709;&#65307;2&#65289;&#22312;&#20351;&#29992;&#23439;&#24179;&#22343;F1&#23545;BIO-strict&#36328;&#24230;&#32423;&#21035;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#24494;&#35843;&#30340;LLMs&#33719;&#24471;&#20102;0.83+&#30340;&#24471;&#20998;&#65292;&#32780;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;TransformerCRF&#27169;&#22411;&#24471;&#20998;&#20026;0.78+&#65292;&#35777;&#26126;&#20102;&#24494;&#35843;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Models (LLMs) pre-trained from general or related domain data to a specific domain and task using a limited amount of resources available in the new task has been a popular practice in NLP fields. In this work, we re-visit this assumption, and carry out investigation in clinical NLP, specifically named-entity recognition on Drugs and their related Attributes. We compare Transformer models that are learned from scratch to fine-tuning BERT-based LLMs including BERT-base, BioBERT, and ClinicalBERT. We also investigate the comparison of such models and their extended models with a CRF layer for continuous learning. We use n2c2-2018 shared task data for model development and evaluations. The experimental outcomes show that 1) the CRF layer makes a difference for all neural models; 2) on BIO-strict span level evaluation using macro-average F1, while the fine-tuned LLMs achieved scores 0.83+, the TransformerCRF model learned from scratch achieved 0.78+ demonstrating
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#31532;&#19968;&#20010;&#31995;&#32479;&#30740;&#31350;&#36229;&#20998;&#24067;&#27867;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#19981;&#21516;&#32500;&#24230;&#30340;&#28304;&#20195;&#30721;&#25968;&#25454;&#23646;&#24615;&#21644;&#24494;&#35843;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30740;&#31350;&#20102;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2210.04802</link><description>&lt;p&gt;
SimSCOOD: Fine-tuned&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#36229;&#20998;&#24067;&#27867;&#21270;&#30340;&#31995;&#32479;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models. (arXiv:2210.04802v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#31532;&#19968;&#20010;&#31995;&#32479;&#30740;&#31350;&#36229;&#20998;&#24067;&#27867;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#19981;&#21516;&#32500;&#24230;&#30340;&#28304;&#20195;&#30721;&#25968;&#25454;&#23646;&#24615;&#21644;&#24494;&#35843;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30740;&#31350;&#20102;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20195;&#30721;&#25968;&#25454;&#38598;&#24050;&#32463;&#36234;&#26469;&#36234;&#23481;&#26131;&#22320;&#29992;&#20110;&#39044;&#35757;&#32451;&#28304;&#20195;&#30721;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24494;&#35843;&#38454;&#27573;&#26469;&#35828;&#65292;&#33719;&#21462;&#20195;&#34920;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#20197;&#20805;&#20998;&#35206;&#30422;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#20195;&#30721;&#20998;&#24067;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#20219;&#21153;&#29305;&#23450;&#24615;&#21644;&#26377;&#38480;&#30340;&#26631;&#27880;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#36951;&#24536;&#20197;&#21069;&#33719;&#24471;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#36825;&#20123;&#38382;&#39064;&#23548;&#33268;&#20102;&#36229;&#20998;&#24067;&#27867;&#21270;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#30340;&#25512;&#29702;&#34892;&#20026;&#20986;&#29616;&#24847;&#22806;&#24773;&#20917;&#65292;&#36825;&#23578;&#26410;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31995;&#32479;&#26041;&#27861;&#65292;&#27169;&#25311;&#20102;&#19981;&#21516;&#32500;&#24230;&#28304;&#20195;&#30721;&#25968;&#25454;&#23646;&#24615;&#30340;&#21508;&#31181;&#36229;&#20998;&#24067;&#22330;&#26223;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#20123;&#22330;&#26223;&#20013;&#24494;&#35843;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#24494;&#35843;&#26041;&#27861;&#65288;&#21253;&#25324;&#20840;&#24494;&#35843;&#21644;&#20302;&#31209;&#36866;&#24212;&#24494;&#35843;&#26041;&#27861;&#65289;&#19979;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#21508;&#20010;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large code datasets have become increasingly accessible for pre-training source code models. However, for the fine-tuning phase, obtaining representative training data that fully covers the code distribution for specific downstream tasks remains challenging due to the task-specific nature and limited labeling resources. Moreover, fine-tuning pretrained models can result in forgetting previously acquired pre-training knowledge. These lead to out-of-distribution (OOD) generalization issues with unexpected model inference behaviors that have not been systematically studied yet. In this paper, we contribute the first systematic approach that simulates various OOD scenarios along different dimensions of source code data properties and study the fine-tuned model behaviors in such scenarios. We investigate the behaviors of models under different fine-tuning methodologies, including full fine-tuning and Low-Rank Adaptation (LoRA) fine-tuning methods. Our comprehensive analysis, conducted on fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#29702;&#35299;&#19982;&#30446;&#26631;&#20107;&#20214;&#26377;&#20851;&#30340;&#27493;&#39588;&#20107;&#20214;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#24182;&#36129;&#29486;&#20102;&#19968;&#20010;&#30001;&#19987;&#23478;&#25163;&#21160;&#27880;&#37322;&#27493;&#39588;&#37325;&#35201;&#24615;&#30340;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2210.04074</link><description>&lt;p&gt;
&#25152;&#26377;&#27493;&#39588;&#37117;&#21516;&#31561;&#37325;&#35201;&#21527;&#65311;&#22522;&#20110;&#20107;&#20214;&#30340;&#37325;&#35201;&#24615;&#26816;&#27979;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are All Steps Equally Important? Benchmarking Essentiality Detection of Events. (arXiv:2210.04074v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#29702;&#35299;&#19982;&#30446;&#26631;&#20107;&#20214;&#26377;&#20851;&#30340;&#27493;&#39588;&#20107;&#20214;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#24182;&#36129;&#29486;&#20102;&#19968;&#20010;&#30001;&#19987;&#23478;&#25163;&#21160;&#27880;&#37322;&#27493;&#39588;&#37325;&#35201;&#24615;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20197;&#19981;&#21516;&#30340;&#32454;&#31890;&#24230;&#34920;&#36798;&#20107;&#20214;&#65292;&#20854;&#20013;&#31895;&#31890;&#24230;&#20107;&#20214;&#65288;&#30446;&#26631;&#65289;&#21487;&#20197;&#32454;&#20998;&#20026;&#26356;&#32454;&#31890;&#24230;&#30340;&#20107;&#20214;&#24207;&#21015;&#65288;&#27493;&#39588;&#65289;&#12290;&#29702;&#35299;&#20107;&#20214;&#36807;&#31243;&#30340;&#19968;&#20010;&#20851;&#38190;&#20294;&#24120;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#26159;&#35748;&#35782;&#21040;&#24182;&#38750;&#25152;&#26377;&#27493;&#39588;&#20107;&#20214;&#23545;&#20110;&#23436;&#25104;&#30446;&#26631;&#20855;&#26377;&#30456;&#21516;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24403;&#21069;&#27169;&#22411;&#29702;&#35299;&#19982;&#30446;&#26631;&#20107;&#20214;&#26377;&#20851;&#30340;&#27493;&#39588;&#20107;&#20214;&#30340;&#37325;&#35201;&#24615;&#30340;&#31243;&#24230;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#35748;&#30693;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#33021;&#21147;&#20351;&#26426;&#22120;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#23545;&#26085;&#24120;&#20219;&#21153;&#30340;&#20808;&#20915;&#26465;&#20214;&#21644;&#24517;&#35201;&#21162;&#21147;&#30340;&#24120;&#35782;&#25512;&#29702;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#20010;&#20248;&#36136;&#30340;&#35821;&#26009;&#24211;&#65288;&#30446;&#26631;&#65292;&#27493;&#39588;&#65289;&#23545;&#65292;&#35813;&#35821;&#26009;&#24211;&#20174;&#31038;&#21306;&#25351;&#21335;&#32593;&#31449;WikiHow&#25910;&#38598;&#65292;&#36890;&#36807;&#19987;&#23478;&#25163;&#21160;&#27880;&#37322;&#27493;&#39588;&#30340;&#37325;&#35201;&#24615;&#12290;&#39640;&#19968;&#33268;&#24615;&#30340;&#26631;&#27880;&#32773;&#38388;&#19968;&#33268;&#24615;&#34920;&#26126;&#20154;&#31867;&#23545;&#20107;&#20214;&#37325;&#35201;&#24615;&#20855;&#26377;&#19968;&#33268;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#35780;&#20272;&#22810;&#20010;&#32479;&#35745;&#27169;&#22411;&#20043;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#27169;&#22411;&#22312;&#29702;&#35299;&#27493;&#39588;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#36824;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language expresses events with varying granularities, where coarse-grained events (goals) can be broken down into finer-grained event sequences (steps). A critical yet overlooked aspect of understanding event processes is recognizing that not all step events hold equal importance toward the completion of a goal. In this paper, we address this gap by examining the extent to which current models comprehend the essentiality of step events in relation to a goal event. Cognitive studies suggest that such capability enables machines to emulate human commonsense reasoning about preconditions and necessary efforts of everyday tasks. We contribute a high-quality corpus of (goal, step) pairs gathered from the community guideline website WikiHow, with steps manually annotated for their essentiality concerning the goal by experts. The high inter-annotator agreement demonstrates that humans possess a consistent understanding of event essentiality. However, after evaluating multiple statisti
&lt;/p&gt;</description></item><item><title>Hansel&#26159;&#19968;&#20010;&#20013;&#25991;Few-Shot&#21644;Zero-Shot&#23454;&#20307;&#38142;&#25509;&#22522;&#20934;&#65292;&#22635;&#34917;&#20102;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#20851;&#27880;&#23614;&#37096;&#21644;&#26032;&#20852;&#23454;&#20307;&#30340;&#25968;&#25454;&#38598;&#30340;&#31354;&#30333;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#38598;&#30001;&#20154;&#24037;&#27880;&#37322;&#21644;&#23457;&#26680;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#26041;&#27861;&#25910;&#38598;Zero-Shot&#23454;&#20307;&#38142;&#25509;&#25968;&#25454;&#38598;&#65292;&#24182;&#20197;Wikidata&#20316;&#20026;&#30446;&#26631;&#30693;&#35782;&#24211;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#23454;&#20307;&#38142;&#25509;&#31995;&#32479;&#22312;Hansel&#19978;&#34920;&#29616;&#36739;&#24046;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22522;&#32447;&#65292;&#22312;Few-Shot&#19978;&#36798;&#21040;&#20102;46.2%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;Zero-Shot&#19978;&#36798;&#21040;&#20102;76.6%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;TAC-KBP2015&#20013;&#25991;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.13005</link><description>&lt;p&gt;
Hansel: &#19968;&#20010;&#20013;&#25991; Few-Shot &#21644; Zero-Shot &#23454;&#20307;&#38142;&#25509;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Hansel: A Chinese Few-Shot and Zero-Shot Entity Linking Benchmark. (arXiv:2207.13005v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.13005
&lt;/p&gt;
&lt;p&gt;
Hansel&#26159;&#19968;&#20010;&#20013;&#25991;Few-Shot&#21644;Zero-Shot&#23454;&#20307;&#38142;&#25509;&#22522;&#20934;&#65292;&#22635;&#34917;&#20102;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#20851;&#27880;&#23614;&#37096;&#21644;&#26032;&#20852;&#23454;&#20307;&#30340;&#25968;&#25454;&#38598;&#30340;&#31354;&#30333;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#38598;&#30001;&#20154;&#24037;&#27880;&#37322;&#21644;&#23457;&#26680;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#26041;&#27861;&#25910;&#38598;Zero-Shot&#23454;&#20307;&#38142;&#25509;&#25968;&#25454;&#38598;&#65292;&#24182;&#20197;Wikidata&#20316;&#20026;&#30446;&#26631;&#30693;&#35782;&#24211;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#23454;&#20307;&#38142;&#25509;&#31995;&#32479;&#22312;Hansel&#19978;&#34920;&#29616;&#36739;&#24046;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22522;&#32447;&#65292;&#22312;Few-Shot&#19978;&#36798;&#21040;&#20102;46.2%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;Zero-Shot&#19978;&#36798;&#21040;&#20102;76.6%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;TAC-KBP2015&#20013;&#25991;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#31995;&#32479;&#22266;&#21270;&#20102;&#23545;&#27969;&#34892;&#24230;&#30340;&#20559;&#35265;&#65292;&#28982;&#32780;&#38500;&#20102;&#33521;&#35821;&#20043;&#22806;&#65292;&#27809;&#26377;&#19987;&#27880;&#20110;&#23614;&#37096;&#21644;&#26032;&#20852;&#23454;&#20307;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Hansel&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20013;&#25991;&#22522;&#20934;&#65292;&#22635;&#34917;&#20102;&#38750;&#33521;&#35821; Few-Shot &#21644; Zero-Shot &#23454;&#20307;&#38142;&#25509;&#25361;&#25112;&#30340;&#31354;&#30333;&#12290;Hansel&#30340;&#27979;&#35797;&#38598;&#26159;&#20154;&#24037;&#27880;&#37322;&#21644;&#23457;&#26680;&#30340;&#65292;&#20351;&#29992;&#19968;&#31181;&#25910;&#38598; Zero-Shot EL &#25968;&#25454;&#38598;&#30340;&#26032;&#26041;&#27861;&#21019;&#24314;&#12290;&#23427;&#28085;&#30422;&#20102;&#21253;&#25324;&#26032;&#38395;&#12289;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21644;&#20854;&#20182;&#32593;&#32476;&#25991;&#31456;&#31561;&#22810;&#26679;&#21270;&#30340;10,000&#20010;&#25991;&#26723;&#65292;&#24182;&#20197;Wikidata&#20316;&#20026;&#20854;&#30446;&#26631;&#30693;&#35782;&#24211;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827; EL &#31995;&#32479;&#22312;Hansel&#19978;&#24615;&#33021;&#36739;&#24046;&#65288;Few-Shot &#19978;&#30340;R@1&#20026;36.6%&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22522;&#32447;&#65292;&#22312;Few-Shot&#19978;&#24471;&#20998;&#20026;46.2%&#65292;&#22312;Zero-Shot&#19978;&#24471;&#20998;&#20026;76.6%&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#22522;&#32447;&#22312;TAC-KBP2015&#20013;&#25991;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern Entity Linking (EL) systems entrench a popularity bias, yet there is no dataset focusing on tail and emerging entities in languages other than English. We present Hansel, a new benchmark in Chinese that fills the vacancy of non-English few-shot and zero-shot EL challenges. The test set of Hansel is human annotated and reviewed, created with a novel method for collecting zero-shot EL datasets. It covers 10K diverse documents in news, social media posts and other web articles, with Wikidata as its target Knowledge Base. We demonstrate that the existing state-of-the-art EL system performs poorly on Hansel (R@1 of 36.6% on Few-Shot). We then establish a strong baseline that scores a R@1 of 46.2% on Few-Shot and 76.6% on Zero-Shot on our dataset. We also show that our baseline achieves competitive results on TAC-KBP2015 Chinese Entity Linking task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#36890;&#36807;&#28151;&#20837;&#20869;&#23481;&#26469;&#24433;&#21709;&#31572;&#26696;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2207.07051</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26174;&#31034;&#23545;&#25512;&#29702;&#20219;&#21153;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#30340;&#20869;&#23481;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Language models show human-like content effects on reasoning tasks. (arXiv:2207.07051v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#36890;&#36807;&#28151;&#20837;&#20869;&#23481;&#26469;&#24433;&#21709;&#31572;&#26696;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#25512;&#29702;&#26159;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#39640;&#20110;&#38543;&#26426;&#30340;&#24615;&#33021;&#65292;&#20294;&#23384;&#22312;&#35768;&#22810;&#19981;&#23436;&#21892;&#20043;&#22788;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#25277;&#35937;&#25512;&#29702;&#20063;&#26159;&#19981;&#23436;&#32654;&#30340;&#12290;&#20363;&#22914;&#65292;&#20154;&#31867;&#25512;&#29702;&#21463;&#21040;&#25105;&#20204;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#21644;&#20449;&#24565;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#8220;&#20869;&#23481;&#25928;&#24212;&#8221;&#65307;&#24403;&#38382;&#39064;&#30340;&#35821;&#20041;&#20869;&#23481;&#25903;&#25345;&#27491;&#30830;&#30340;&#36923;&#36753;&#25512;&#29702;&#26102;&#65292;&#20154;&#31867;&#26356;&#21487;&#38752;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#36825;&#20123;&#20869;&#23481;&#32416;&#32544;&#30340;&#25512;&#29702;&#27169;&#24335;&#22312;&#20851;&#20110;&#20154;&#31867;&#26234;&#33021;&#22522;&#26412;&#24615;&#36136;&#30340;&#20105;&#35770;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#28151;&#20837;&#20869;&#23481;&#26469;&#22238;&#31572;&#36923;&#36753;&#38382;&#39064;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#25429;&#25417;&#20102;&#19968;&#20123;&#20154;&#31867;&#30693;&#35782;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#25506;&#32034;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#21028;&#26029;&#19977;&#27573;&#35770;&#30340;&#36923;&#36753;&#26377;&#25928;&#24615;&#21644;Wason&#36873;&#25321;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable "content effects"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models $\unicode{x2014}$ whose prior expectations capture some aspects of human knowledge $\unicode{x2014}$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Machine Personality Inventory (MPI)&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20010;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LLM)&#20855;&#26377;&#20010;&#24615;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;Personality Prompting (P^2)&#26041;&#27861;&#65292;&#29992;&#20110;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;LLM&#20855;&#26377;&#29305;&#23450;&#20010;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.07550</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#35825;&#23548;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Inducing Personality in Pre-trained Language Models. (arXiv:2206.07550v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Machine Personality Inventory (MPI)&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20010;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LLM)&#20855;&#26377;&#20010;&#24615;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;Personality Prompting (P^2)&#26041;&#27861;&#65292;&#29992;&#20110;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;LLM&#20855;&#26377;&#29305;&#23450;&#20010;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#36215;&#28304;&#20110;&#21746;&#23398;&#25506;&#32034;&#65292;&#20851;&#27880;&#20010;&#20307;&#22312;&#24605;&#32771;&#12289;&#24773;&#24863;&#21644;&#34892;&#20026;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#19982;&#20154;&#31867;&#26085;&#24120;&#21512;&#20316;&#30340;&#31038;&#20132;&#26426;&#22120;&#65292;&#25105;&#20204;&#24819;&#30693;&#36947;&#65306;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#25317;&#26377;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20010;&#24615;&#65311;&#22914;&#26524;&#26159;&#65292;&#25105;&#20204;&#22914;&#20309;&#35780;&#20272;&#23427;&#20204;&#65311;&#36827;&#19968;&#27493;&#22320;&#65292;&#22312;&#27492;&#35780;&#20272;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#22914;&#20309;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;&#20855;&#26377;&#29305;&#23450;&#20010;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#65311;&#20026;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#22120;&#20010;&#24615;&#24211;(Machine Personality Inventory, MPI)&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#30340;&#20010;&#24615;&#12290;MPI&#36981;&#24490;&#26631;&#20934;&#21270;&#30340;&#20010;&#24615;&#27979;&#35797;&#65292;&#22522;&#20110;&#20116;&#22240;&#32032;&#20154;&#26684;&#29702;&#35770;&#21644;&#20154;&#26684;&#35780;&#20272;&#24211;&#24314;&#31435;&#12290;&#36890;&#36807;&#29992;MPI&#31995;&#32479;&#22320;&#35780;&#20272;LLM&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;LLM&#30340;&#20010;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#20010;&#24615;&#25552;&#31034;(Personality Prompting, P^2)&#26041;&#27861;&#65292;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;LLMs&#20855;&#26377;&#29305;&#23450;&#30340;&#20010;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Originating as a philosophical quest, the study of personality concerns how individuals differ in thinking, feeling, and behaving. Towards building social machines that work with humans on a daily basis, we are motivated to ask: Do existing Large Language Models (LLMs) possess personalities akin to their human counterparts? If so, how can we evaluate them? Further, given this evaluation framework, how can we induce a particular personality in a controllable fashion? To answer these three questions, we propose the Machine Personality Inventory (MPI) dataset for evaluating the machine personality; MPI follows standardized personality tests, built upon the Big Five Personality Factors (Big Five) theory and personality assessment inventories. By systematically evaluating LLMs with MPI, we provide the first piece of evidence showing the existence of personality in LLMs. We further devise a Personality Prompting (P^2) method to induce LLMs with a specific personality in a controllable manner
&lt;/p&gt;</description></item><item><title>Speculative Decoding&#26159;&#19968;&#31181;&#26032;&#22411;&#35299;&#30721;&#33539;&#24335;&#65292;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;AT&#65289;&#21644;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;NAT&#65289;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#26080;&#25439;&#21152;&#36895;&#30340;&#32763;&#35793;&#26041;&#27861;&#12290;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#65292;&#23427;&#25512;&#27979;&#24615;&#22320;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#39564;&#35777;&#27169;&#22411;&#30830;&#20445;&#32763;&#35793;&#32467;&#26524;&#19982;AT&#23436;&#20840;&#30456;&#21516;&#12290;&#36890;&#36807;&#25512;&#27979;&#35299;&#30721;&#21644;&#39564;&#35777;&#30340;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#35299;&#30721;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#19981;&#21464;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21407;&#22987;&#30340;SpecDec&#19982;AT&#36138;&#23146;&#35299;&#30721;&#30340;&#32467;&#26524;&#23436;&#20840;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2203.16487</link><description>&lt;p&gt;
Speculative Decoding: &#26080;&#25439;&#21152;&#36895;&#33258;&#22238;&#24402;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Speculative Decoding: Lossless Speedup of Autoregressive Translation. (arXiv:2203.16487v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16487
&lt;/p&gt;
&lt;p&gt;
Speculative Decoding&#26159;&#19968;&#31181;&#26032;&#22411;&#35299;&#30721;&#33539;&#24335;&#65292;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;AT&#65289;&#21644;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;NAT&#65289;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#26080;&#25439;&#21152;&#36895;&#30340;&#32763;&#35793;&#26041;&#27861;&#12290;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#65292;&#23427;&#25512;&#27979;&#24615;&#22320;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#39564;&#35777;&#27169;&#22411;&#30830;&#20445;&#32763;&#35793;&#32467;&#26524;&#19982;AT&#23436;&#20840;&#30456;&#21516;&#12290;&#36890;&#36807;&#25512;&#27979;&#35299;&#30721;&#21644;&#39564;&#35777;&#30340;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#35299;&#30721;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#19981;&#21464;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21407;&#22987;&#30340;SpecDec&#19982;AT&#36138;&#23146;&#35299;&#30721;&#30340;&#32467;&#26524;&#23436;&#20840;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20043;&#21069;&#19968;&#20123;&#29306;&#29298;&#32763;&#35793;&#36136;&#37327;&#21152;&#36895;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;AT&#65289;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Speculative Decoding&#65288;SpecDec&#65289;-&#19968;&#31181;&#21463;&#35745;&#31639;&#26426;&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#25512;&#27979;&#25191;&#34892;&#21551;&#21457;&#30340;&#26032;&#22411;&#35299;&#30721;&#33539;&#24335;&#65292;&#23427;&#32467;&#21512;&#20102;AT&#21644;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;NAT&#65289;&#30340;&#21508;&#33258;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#22312;&#32763;&#35793;&#36807;&#31243;&#20013;&#30340;&#26080;&#25439;&#21152;&#36895;&#12290;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#65292;SpecDec&#39318;&#20808;&#20351;&#29992;NAT&#27169;&#22411;&#25512;&#27979;&#24615;&#22320;&#39044;&#27979;&#65288;&#21363;&#35299;&#30721;&#65289;&#19979;&#19968;&#20010;k&#20010;&#26631;&#35760;&#65292;&#28982;&#21518;&#20351;&#29992;AT&#27169;&#22411;&#39564;&#35777;&#36825;&#20123;&#26631;&#35760;&#65292;&#21482;&#26377;&#36890;&#36807;&#39564;&#35777;&#30340;&#39044;&#27979;&#26631;&#35760;&#25165;&#20250;&#34987;&#25509;&#21463;&#20316;&#20026;&#35299;&#30721;&#32467;&#26524;&#65292;&#20197;&#30830;&#20445;&#20854;&#32763;&#35793;&#32467;&#26524;&#19982;AT&#23436;&#20840;&#30456;&#21516;&#12290;NAT&#30340;&#25512;&#27979;&#21644;AT&#30340;&#39564;&#35777;&#20043;&#38388;&#30340;&#21327;&#20316;&#20351;&#24471;&#35299;&#30721;&#36895;&#24230;&#22823;&#22823;&#25552;&#39640;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#32763;&#35793;&#36136;&#37327;&#65292;&#36825;&#26159;&#30001;&#20110;&#25512;&#27979;&#35299;&#30721;&#25152;&#25903;&#25345;&#30340;&#24182;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#22312;4&#20010;&#26631;&#20934;WMT&#32763;&#35793;&#22522;&#20934;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#35777;&#23454;&#21407;&#22987;&#30340;SpecDec&#19982;AT&#36138;&#23146;&#35299;&#30721;&#30340;&#32467;&#26524;&#23436;&#20840;&#30456;&#21516;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422; $k$&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different from some previous work accelerating autoregressive translation (AT) at the sacrifice of quality, we propose Speculative Decoding (SpecDec) -a novel decoding paradigm inspired by speculative execution in computer architecture, which combines respective advantages of AT and non-autoregressive translation (NAT) for lossless speedup of translation. At each decoding step, SpecDec first speculatively drafts (i.e. decodes) next $k$ tokens with an NAT model and then verifies them with an AT model, where only the drafted tokens passing the verification are accepted as decoded tokens for guaranteeing its translation result is exactly the same as AT. The collaboration of NAT drafting and AT verification leads to a much higher decoding speed without quality loss due to parallel computing enabled by speculative decoding.  We conduct experiments in 4 standard WMT translation benchmarks and confirm the vanilla SpecDec yields exactly the same results as AT greedy decoding with an around $
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;CLIP&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#20854;&#32479;&#19968;&#24615;&#21644;&#23545;&#40784;&#24615;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23884;&#20837;&#30340;&#20256;&#36882;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#29983;&#25104;&#38590;&#36127;&#26679;&#26412;&#65292;&#24182;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2203.03897</link><description>&lt;p&gt;
&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#29992;&#20110;&#40065;&#26834;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Geodesic Multi-Modal Mixup for Robust Fine-Tuning. (arXiv:2203.03897v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;CLIP&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#20854;&#32479;&#19968;&#24615;&#21644;&#23545;&#40784;&#24615;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23884;&#20837;&#30340;&#20256;&#36882;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#29983;&#25104;&#38590;&#36127;&#26679;&#26412;&#65292;&#24182;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#22411;&#27169;&#22411;&#65292;&#22914;CLIP&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#25552;&#20379;&#21487;&#36716;&#31227;&#30340;&#23884;&#20837;&#65292;&#24182;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#23398;&#20064;&#21040;&#30340;&#22810;&#27169;&#22411;&#23884;&#20837;&#30340;&#20998;&#26512;&#30456;&#23545;&#36739;&#23569;&#65292;&#23884;&#20837;&#30340;&#21487;&#36716;&#31227;&#24615;&#26377;&#24453;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;CLIP&#20026;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#20445;&#30041;&#20102;&#20998;&#31163;&#30340;&#23884;&#20837;&#23376;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#32479;&#19968;&#23545;&#40784;&#30340;&#35270;&#35282;&#23545;&#20854;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#34913;&#37327;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#24494;&#35843;&#20043;&#21518;&#65292;CLIP&#20173;&#28982;&#20445;&#25345;&#30528;&#36739;&#24046;&#30340;&#32479;&#19968;&#24615;&#21644;&#23545;&#40784;&#24615;&#12290;&#36825;&#31181;&#32570;&#20047;&#23545;&#40784;&#21644;&#32479;&#19968;&#24615;&#21487;&#33021;&#38480;&#21046;&#20102;&#23884;&#20837;&#30340;&#20256;&#36882;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#40065;&#26834;&#34920;&#31034;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#23545;&#40784;&#21644;&#32479;&#19968;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#23884;&#20837;&#28151;&#21512;&#22312;&#19968;&#36215;&#65292;&#22312;&#36229;&#29699;&#38754;&#19978;&#29983;&#25104;&#38590;&#36127;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained multi-modal models, such as CLIP, provide transferable embeddings and show promising results in diverse applications. However, the analysis of learned multi-modal embeddings is relatively unexplored, and the embedding transferability can be improved. In this work, we observe that CLIP holds separated embedding subspaces for two different modalities, and then we investigate it through the lens of uniformity-alignment to measure the quality of learned representation. Both theoretically and empirically, we show that CLIP retains poor uniformity and alignment even after fine-tuning. Such a lack of alignment and uniformity might restrict the transferability and robustness of embeddings. To this end, we devise a new fine-tuning method for robust representation equipping better alignment and uniformity. First, we propose a Geodesic Multi-Modal Mixup that mixes the embeddings of image and text to generate hard negative samples on the hypersphere. Then, we fine-tune the model on har
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;NLP&#20013;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#24182;&#20174;&#19977;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#23545;&#29616;&#26377;&#30340;KEPLMs&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#26368;&#21518;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#20013;KEPLMs&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2110.00269</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Knowledge Enhanced Pre-trained Models. (arXiv:2110.00269v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.00269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;NLP&#20013;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#24182;&#20174;&#19977;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#23545;&#29616;&#26377;&#30340;KEPLMs&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#26368;&#21518;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#20013;KEPLMs&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#23398;&#20064;&#20102;&#20449;&#24687;&#20016;&#23500;&#30340;&#35789;&#34920;&#31034;&#65292;&#22312;&#32454;&#35843;&#20043;&#21518;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#40065;&#26834;&#24615;&#24046;&#21644;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#27880;&#20837;&#30693;&#35782;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#31216;&#20026;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(KEPLMs)&#12290;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#28145;&#20837;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;NLP&#20013;KEPLMs&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#19977;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#31995;&#32479;&#22320;&#20998;&#31867;&#20102;&#29616;&#26377;&#30340;KEPLMs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#20013;KEPLMs&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models learn informative word representations on a large-scale text corpus through self-supervised learning, which has achieved promising performance in fields of natural language processing (NLP) after fine-tuning. These models, however, suffer from poor robustness and lack of interpretability. We refer to pre-trained language models with knowledge injection as knowledge-enhanced pre-trained language models (KEPLMs). These models demonstrate deep understanding and logical reasoning and introduce interpretability. In this survey, we provide a comprehensive overview of KEPLMs in NLP. We first discuss the advancements in pre-trained language models and knowledge representation learning. Then we systematically categorize existing KEPLMs from three different perspectives. Finally, we outline some potential directions of KEPLMs for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#22870;&#21169;&#30340;&#26463;&#25628;&#32034;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#20851;&#38190;&#35789;&#29983;&#25104;&#20013;&#30340;&#24207;&#21015;&#38271;&#24230;&#20559;&#24046;&#21644;&#26463;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#20851;&#38190;&#35789;&#30340;&#35299;&#30721;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/1909.09485</link><description>&lt;p&gt;
BSDAR: &#22522;&#20110;&#27880;&#24847;&#21147;&#22870;&#21169;&#30340;&#31070;&#32463;&#20851;&#38190;&#35789;&#29983;&#25104;&#20013;&#30340;&#26463;&#25628;&#32034;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
BSDAR: Beam Search Decoding with Attention Reward in Neural Keyphrase Generation. (arXiv:1909.09485v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1909.09485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#22870;&#21169;&#30340;&#26463;&#25628;&#32034;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#20851;&#38190;&#35789;&#29983;&#25104;&#20013;&#30340;&#24207;&#21015;&#38271;&#24230;&#20559;&#24046;&#21644;&#26463;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#20851;&#38190;&#35789;&#30340;&#35299;&#30721;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20027;&#35201;&#30740;&#31350;&#31070;&#32463;&#20851;&#38190;&#35789;&#29983;&#25104;&#20013;&#30340;&#20004;&#20010;&#24120;&#35265;&#35299;&#30721;&#38382;&#39064;&#65306;&#24207;&#21015;&#38271;&#24230;&#20559;&#24046;&#21644;&#26463;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#32423;&#21644;ngram&#32423;&#22870;&#21169;&#20989;&#25968;&#30340;&#26463;&#25628;&#32034;&#35299;&#30721;&#31574;&#30053;&#65292;&#20197;&#22312;&#27979;&#35797;&#26102;&#32422;&#26463;&#21644;&#20248;&#21270;Seq2Seq&#25512;&#29702;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#31616;&#21333;&#30340;&#25552;&#26696;&#21487;&#20197;&#20811;&#26381;&#31639;&#27861;&#23545;&#36739;&#30701;&#21644;&#20960;&#20046;&#30456;&#21516;&#30340;&#24207;&#21015;&#30340;&#20559;&#22909;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#28304;&#25991;&#26412;&#20013;&#23384;&#22312;&#21644;&#19981;&#23384;&#22312;&#30340;&#20851;&#38190;&#35789;&#30340;&#35299;&#30721;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study mainly investigates two common decoding problems in neural keyphrase generation: sequence length bias and beam diversity. To tackle the problems, we introduce a beam search decoding strategy based on word-level and ngram-level reward function to constrain and refine Seq2Seq inference at test time. Results show that our simple proposal can overcome the algorithm bias to shorter and nearly identical sequences, resulting in a significant improvement of the decoding performance on generating keyphrases that are present and absent in source text.
&lt;/p&gt;</description></item></channel></rss>