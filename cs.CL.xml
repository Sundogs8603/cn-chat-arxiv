<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25513;&#34109;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#21387;&#32553;&#22810;&#35821;&#31181;ASR&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#36866;&#24212;&#23376;&#32593;&#32476;&#32467;&#26500;&#65292;&#33021;&#22815;&#22312;&#20943;&#23569;&#24615;&#33021;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#25110;&#31232;&#30095;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#20462;&#21098;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#38024;&#23545;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#23545;&#29305;&#23450;&#35821;&#35328;&#36827;&#34892;&#20462;&#21098;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.13018</link><description>&lt;p&gt;
&#21160;&#24577;ASR&#36335;&#24452;&#65306;&#19968;&#31181;&#33258;&#36866;&#24212;&#25513;&#34109;&#26041;&#27861;&#29992;&#20110;&#21387;&#32553;&#22810;&#35821;&#31181;ASR&#27169;&#22411;&#30340;&#39640;&#25928;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model. (arXiv:2309.13018v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25513;&#34109;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#21387;&#32553;&#22810;&#35821;&#31181;ASR&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#36866;&#24212;&#23376;&#32593;&#32476;&#32467;&#26500;&#65292;&#33021;&#22815;&#22312;&#20943;&#23569;&#24615;&#33021;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#25110;&#31232;&#30095;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#20462;&#21098;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#38024;&#23545;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#23545;&#29305;&#23450;&#35821;&#35328;&#36827;&#34892;&#20462;&#21098;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24615;&#33021;&#25439;&#22833;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#21387;&#32553;&#22810;&#35821;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#23545;&#27599;&#31181;&#35821;&#35328;&#36816;&#34892;&#22810;&#36718;&#20462;&#21098;&#21644;&#37325;&#26032;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25513;&#34109;&#26041;&#27861;&#65292;&#20197;&#20004;&#31181;&#22330;&#26223;&#39640;&#25928;&#22320;&#20462;&#21098;&#22810;&#35821;&#31181;ASR&#27169;&#22411;&#65292;&#20998;&#21035;&#24471;&#21040;&#20102;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#25110;&#31232;&#30095;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#65288;&#31216;&#20026;&#21160;&#24577;ASR&#36335;&#24452;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21160;&#24577;&#22320;&#36866;&#24212;&#23376;&#32593;&#32476;&#65292;&#36991;&#20813;&#23545;&#22266;&#23450;&#30340;&#23376;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#36807;&#26089;&#20915;&#31574;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38024;&#23545;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#26102;&#20248;&#20110;&#29616;&#26377;&#30340;&#20462;&#21098;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35828;&#26126;&#20102;&#21160;&#24577;ASR&#36335;&#24452;&#36890;&#36807;&#33258;&#19981;&#21516;&#30340;&#23376;&#32593;&#32476;&#21021;&#22987;&#21270;&#36827;&#34892;&#35843;&#25972;&#65292;&#20849;&#21516;&#21457;&#29616;&#21644;&#35757;&#32451;&#26356;&#22909;&#30340;&#21333;&#19968;&#22810;&#35821;&#31181;&#27169;&#22411;&#30340;&#23376;&#32593;&#32476;&#65288;&#36335;&#24452;&#65289;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23545;&#29305;&#23450;&#35821;&#35328;&#36827;&#34892;&#20462;&#21098;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network pruning offers an effective method for compressing a multilingual automatic speech recognition (ASR) model with minimal performance loss. However, it entails several rounds of pruning and re-training needed to be run for each language. In this work, we propose the use of an adaptive masking approach in two scenarios for pruning a multilingual ASR model efficiently, each resulting in sparse monolingual models or a sparse multilingual model (named as Dynamic ASR Pathways). Our approach dynamically adapts the sub-network, avoiding premature decisions about a fixed sub-network structure. We show that our approach outperforms existing pruning methods when targeting sparse monolingual models. Further, we illustrate that Dynamic ASR Pathways jointly discovers and trains better sub-networks (pathways) of a single multilingual model by adapting from different sub-network initializations, thereby reducing the need for language-specific pruning.
&lt;/p&gt;</description></item><item><title>ReConcile&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#36718;&#35752;&#35770;&#21644;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#33021;&#21147;&#30340;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.13007</link><description>&lt;p&gt;
ReConcile&#65306;&#22278;&#26700;&#20250;&#35758;&#36890;&#36807;&#22810;&#20803;LLM&#30340;&#20849;&#35782;&#25913;&#36827;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs. (arXiv:2309.13007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13007
&lt;/p&gt;
&lt;p&gt;
ReConcile&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#36718;&#35752;&#35770;&#21644;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#33021;&#21147;&#30340;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20173;&#28982;&#22312;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#21463;&#21040;&#24515;&#26234;&#31038;&#20250;&#29702;&#35770;&#65288;Minsky, 1988&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReConcile&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22810;&#26679;&#30340;LLM&#20195;&#29702;&#20154;&#20043;&#38388;&#30340;&#22278;&#26700;&#20250;&#35758;&#26469;&#20419;&#36827;&#22810;&#26679;&#30340;&#24605;&#24819;&#21644;&#35752;&#35770;&#65292;&#20174;&#32780;&#25913;&#36827;&#19968;&#33268;&#24615;&#12290;ReConcile&#36890;&#36807;&#36827;&#34892;&#22810;&#36718;&#35752;&#35770;&#12289;&#23398;&#20064;&#35828;&#26381;&#20854;&#20182;&#20195;&#29702;&#20154;&#25913;&#36827;&#31572;&#26696;&#20197;&#21450;&#37319;&#29992;&#32622;&#20449;&#24230;&#21152;&#26435;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;ReConcile&#36890;&#36807;&#8220;&#35752;&#35770;&#25552;&#31034;&#8221;&#26469;&#21551;&#21160;&#20195;&#29702;&#20154;&#38388;&#30340;&#35752;&#35770;&#65292;&#20854;&#20013;&#21253;&#25324;&#19978;&#19968;&#36718;&#27599;&#20010;&#20195;&#29702;&#20154;&#29983;&#25104;&#30340;&#31572;&#26696;&#21644;&#35299;&#37322;&#30340;&#20998;&#32452;&#12289;&#23427;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#29992;&#20110;&#35828;&#26381;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#31572;&#26696;&#20462;&#27491;&#20154;&#31867;&#35299;&#37322;&#30340;&#28436;&#31034;&#12290;&#36825;&#20010;&#35752;&#35770;&#25552;&#31034;&#20351;&#27599;&#20010;&#20195;&#29702;&#20154;&#33021;&#22815;&#26681;&#25454;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#35265;&#35299;&#20462;&#35746;&#33258;&#24049;&#30340;&#22238;&#31572;&#12290;&#19968;&#26086;&#36798;&#25104;&#19968;&#33268;&#24182;&#32467;&#26463;&#35752;&#35770;&#65292;ReConcile&#25191;&#34892;&#19968;&#27425;&#20840;&#20307;&#25237;&#31080;&#20197;&#30830;&#23450;&#26368;&#32456;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) still struggle with complex reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents to foster diverse thoughts and discussion for improved consensus. ReConcile enhances the reasoning capabilities of LLMs by holding multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their uncertainties, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. This discussion prompt enables each agent to revise their responses in light of insights from other agents. Once a consensus is reached and the discussion ends, ReConcil
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#32763;&#35793;&#30340;&#38024;&#23545;&#29305;&#23450;&#35266;&#20247;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#31034;&#20363;&#35299;&#37322;&#26469;&#35299;&#20915;&#32763;&#35793;&#20013;&#30340;&#25991;&#21270;&#24046;&#24322;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#21477;&#23376;&#20013;&#21253;&#21547;&#35299;&#37322;&#30340;&#27604;&#20363;&#12290;</title><link>http://arxiv.org/abs/2309.12998</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#30340;&#38024;&#23545;&#29305;&#23450;&#35266;&#20247;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Audience-specific Explanations for Machine Translation. (arXiv:2309.12998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12998
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#32763;&#35793;&#30340;&#38024;&#23545;&#29305;&#23450;&#35266;&#20247;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#31034;&#20363;&#35299;&#37322;&#26469;&#35299;&#20915;&#32763;&#35793;&#20013;&#30340;&#25991;&#21270;&#24046;&#24322;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#21477;&#23376;&#20013;&#21253;&#21547;&#35299;&#37322;&#30340;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#26576;&#20123;&#35789;&#30340;&#32763;&#35793;&#21363;&#20351;&#32763;&#35793;&#20102;&#20063;&#20250;&#22240;&#20026;&#19981;&#21516;&#30340;&#25991;&#21270;&#32972;&#26223;&#23548;&#33268;&#30446;&#26631;&#35821;&#35328;&#35266;&#20247;&#26080;&#27861;&#29702;&#35299;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#20026;&#36825;&#20123;&#35789;&#28155;&#21152;&#35299;&#37322;&#12290;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#35782;&#21035;&#36825;&#20123;&#35789;&#25110;&#30701;&#35821;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20174;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#31034;&#20363;&#35299;&#37322;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#21253;&#21547;&#38656;&#35201;&#35299;&#37322;&#30340;&#35789;&#30340;&#21477;&#23376;&#30340;&#31232;&#32570;&#24615;&#20351;&#24471;&#26500;&#24314;&#35757;&#32451;&#25968;&#25454;&#38598;&#26497;&#20854;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#25216;&#26415;&#65292;&#21487;&#20197;&#20174;&#22823;&#22411;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#36825;&#20123;&#35299;&#37322;&#12290;&#22312;&#33521;&#35821;-&gt;&#24503;&#35821;&#35821;&#35328;&#23545;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25552;&#21462;&#20986;&#36229;&#36807;10%&#30340;&#21477;&#23376;&#21253;&#21547;&#35299;&#37322;&#65292;&#32780;&#21407;&#22987;&#21477;&#23376;&#20013;&#21482;&#26377;1.9%&#21253;&#21547;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#22312;&#33521;&#35821;-&gt;&#27861;&#35821;&#21644;&#33521;&#35821;-&gt;&#20013;&#25991;&#35821;&#35328;&#23545;&#19978;&#30340;&#23454;&#39564;&#20063;&#26174;&#31034;&#20986;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine translation, a common problem is that the translation of certain words even if translated can cause incomprehension of the target language audience due to different cultural backgrounds. A solution to solve this problem is to add explanations for these words. In a first step, we therefore need to identify these words or phrases. In this work we explore techniques to extract example explanations from a parallel corpus. However, the sparsity of sentences containing words that need to be explained makes building the training dataset extremely difficult. In this work, we propose a semi-automatic technique to extract these explanations from a large parallel corpus. Experiments on English-&gt;German language pair show that our method is able to extract sentence so that more than 10% of the sentences contain explanation, while only 1.9% of the original sentences contain explanations. In addition, experiments on English-&gt;French and English-&gt;Chinese language pairs also show similar conc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PerNee&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#35782;&#21035;&#20013;&#24515;&#20803;&#32032;&#26469;&#25552;&#21462;&#23884;&#22871;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#29616;&#26377;NEE&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#20013;&#24515;&#20803;&#32032;&#21452;&#37325;&#36523;&#20221;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23558;&#20107;&#20214;&#31867;&#22411;&#21644;&#21442;&#25968;&#35282;&#33394;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20197;&#25552;&#39640;NEE&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12960</link><description>&lt;p&gt;
&#22522;&#20110;&#20013;&#24515;&#20803;&#32032;&#35782;&#21035;&#30340;&#23884;&#22871;&#20107;&#20214;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Nested Event Extraction upon Pivot Element Recogniton. (arXiv:2309.12960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PerNee&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#35782;&#21035;&#20013;&#24515;&#20803;&#32032;&#26469;&#25552;&#21462;&#23884;&#22871;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#29616;&#26377;NEE&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#20013;&#24515;&#20803;&#32032;&#21452;&#37325;&#36523;&#20221;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23558;&#20107;&#20214;&#31867;&#22411;&#21644;&#21442;&#25968;&#35282;&#33394;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20197;&#25552;&#39640;NEE&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#22871;&#20107;&#20214;&#25277;&#21462;&#65288;NEE&#65289;&#26088;&#22312;&#25552;&#21462;&#21253;&#21547;&#20854;&#20182;&#20107;&#20214;&#20316;&#20026;&#20854;&#21442;&#25968;&#30340;&#22797;&#26434;&#20107;&#20214;&#32467;&#26500;&#12290;&#23884;&#22871;&#20107;&#20214;&#28041;&#21450;&#19968;&#31181;&#31216;&#20026;&#20013;&#24515;&#20803;&#32032;&#65288;PEs&#65289;&#30340;&#20803;&#32032;&#65292;&#23427;&#21516;&#26102;&#20316;&#20026;&#22806;&#37096;&#20107;&#20214;&#30340;&#21442;&#25968;&#21644;&#20869;&#37096;&#20107;&#20214;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#23427;&#20204;&#36830;&#25509;&#25104;&#23884;&#22871;&#32467;&#26500;&#12290;PEs&#30340;&#36825;&#31181;&#29305;&#27530;&#29305;&#24615;&#32473;&#29616;&#26377;&#30340;NEE&#26041;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#33021;&#24456;&#22909;&#22320;&#22788;&#29702;PEs&#30340;&#21452;&#37325;&#36523;&#20221;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#31216;&#20026;PerNee&#65292;&#20027;&#35201;&#22522;&#20110;&#35782;&#21035;PEs&#26469;&#25552;&#21462;&#23884;&#22871;&#20107;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PerNee&#39318;&#20808;&#35782;&#21035;&#20869;&#37096;&#21644;&#22806;&#37096;&#20107;&#20214;&#30340;&#35302;&#21457;&#22120;&#65292;&#28982;&#21518;&#36890;&#36807;&#20998;&#31867;&#35302;&#21457;&#22120;&#23545;&#20043;&#38388;&#20851;&#31995;&#31867;&#22411;&#26469;&#35782;&#21035;PEs&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#22909;&#30340;&#35302;&#21457;&#22120;&#21644;&#21442;&#25968;&#34920;&#31034;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;NEE&#24615;&#33021;&#65292;PerNee&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23558;&#20107;&#20214;&#31867;&#22411;&#21644;&#21442;&#25968;&#35282;&#33394;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;NEE&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;Gen&#65289;
&lt;/p&gt;
&lt;p&gt;
Nested Event Extraction (NEE) aims to extract complex event structures where an event contains other events as its arguments recursively. Nested events involve a kind of Pivot Elements (PEs) that simultaneously act as arguments of outer events and as triggers of inner events, and thus connect them into nested structures. This special characteristic of PEs brings challenges to existing NEE methods, as they cannot well cope with the dual identities of PEs. Therefore, this paper proposes a new model, called PerNee, which extracts nested events mainly based on recognizing PEs. Specifically, PerNee first recognizes the triggers of both inner and outer events and further recognizes the PEs via classifying the relation type between trigger pairs. In order to obtain better representations of triggers and arguments to further improve NEE performance, it incorporates the information of both event types and argument roles into PerNee through prompt learning. Since existing NEE datasets (e.g., Gen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35299;&#37322;&#25552;&#31034;&#31574;&#30053;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#23454;&#20854;&#22312;&#22797;&#26434;&#23545;&#35805;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12940</link><description>&lt;p&gt;
&#33258;&#35299;&#37322;&#25552;&#31034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#39640;&#23545;&#35805;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Self-Explanation Prompting Improves Dialogue Understanding in Large Language Models. (arXiv:2309.12940v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35299;&#37322;&#25552;&#31034;&#31574;&#30053;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#23454;&#20854;&#22312;&#22797;&#26434;&#23545;&#35805;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#36890;&#36807;&#22810;&#36718;&#23545;&#35805;&#24110;&#21161;&#29992;&#25143;&#25191;&#34892;&#21508;&#31181;&#27963;&#21160;&#65292;&#20294;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24448;&#24448;&#38590;&#20197;&#29702;&#35299;&#36825;&#20123;&#22797;&#26434;&#30340;&#35821;&#22659;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#33258;&#35299;&#37322;&#8221;&#25552;&#31034;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;LLMs&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#36825;&#31181;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#27861;&#35201;&#27714;&#27169;&#22411;&#22312;&#25191;&#34892;&#20219;&#21153;&#20043;&#21069;&#20998;&#26512;&#27599;&#20010;&#23545;&#35805;&#35805;&#35821;&#65292;&#20174;&#32780;&#25913;&#21892;&#21508;&#31181;&#23545;&#35805;&#20013;&#24515;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26469;&#33258;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#25454;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#38646;&#26679;&#26412;&#25552;&#31034;&#65292;&#24182;&#19988;&#19982;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#30456;&#24403;&#25110;&#36229;&#36807;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#25552;&#39640;LLMs&#22312;&#22797;&#26434;&#23545;&#35805;&#20219;&#21153;&#20013;&#30340;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogue (TOD) systems facilitate users in executing various activities via multi-turn dialogues, but Large Language Models (LLMs) often struggle to comprehend these intricate contexts. In this study, we propose a novel "Self-Explanation" prompting strategy to enhance the comprehension abilities of LLMs in multi-turn dialogues. This task-agnostic approach requires the model to analyze each dialogue utterance before task execution, thereby improving performance across various dialogue-centric tasks. Experimental results from six benchmark datasets confirm that our method consistently outperforms other zero-shot prompts and matches or exceeds the efficacy of few-shot prompts, demonstrating its potential as a powerful tool in enhancing LLMs' comprehension in complex dialogue tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#24863;&#30693;&#30340;&#28145;&#20266;&#25991;&#26412;&#20316;&#32773;&#35782;&#21035;&#26041;&#27861;TopRoBERTa&#65292;&#36890;&#36807;&#25429;&#25417;&#28145;&#20266;&#25991;&#26412;&#20013;&#30340;&#26356;&#22810;&#35821;&#35328;&#27169;&#24335;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#20316;&#32773;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.12934</link><description>&lt;p&gt;
TopRoBERTa&#65306;&#25299;&#25169;&#24863;&#30693;&#30340;&#28145;&#20266;&#25991;&#26412;&#20316;&#32773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
TopRoBERTa: Topology-Aware Authorship Attribution of Deepfake Texts. (arXiv:2309.12934v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#24863;&#30693;&#30340;&#28145;&#20266;&#25991;&#26412;&#20316;&#32773;&#35782;&#21035;&#26041;&#27861;TopRoBERTa&#65292;&#36890;&#36807;&#25429;&#25417;&#28145;&#20266;&#25991;&#26412;&#20013;&#30340;&#26356;&#22810;&#35821;&#35328;&#27169;&#24335;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#20316;&#32773;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#29983;&#25104;&#24320;&#25918;&#24615;&#12289;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#20123;&#25991;&#26412;&#24456;&#38590;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#25991;&#26412;&#21306;&#20998;&#24320;&#26469;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#31216;&#20026;&#8220;&#28145;&#20266;&#25991;&#26412;&#8221;&#12290;&#30446;&#21069;&#65292;huggingface&#27169;&#22411;&#23384;&#20648;&#24211;&#20013;&#26377;&#36229;&#36807;11K&#20010;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#24694;&#24847;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#20351;&#29992;&#36825;&#20123;&#24320;&#28304;&#30340;LLM&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#26377;&#23475;&#25991;&#26412;&#21644;&#34394;&#20551;&#20449;&#24687;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24076;&#26395;&#26377;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;&#33021;&#22815;&#30830;&#23450;&#32473;&#23450;&#30340;&#25991;&#26412;&#26159;&#21542;&#20026;&#28145;&#20266;&#25991;&#26412;&#65292;&#21363;&#36890;&#36807;&#22270;&#28789;&#27979;&#35797;&#65288;TT&#65289;&#26469;&#21028;&#26029;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26356;&#19968;&#33324;&#29256;&#26412;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#22810;&#31867;&#21035;&#35774;&#32622;&#19979;&#30340;&#8220;&#20316;&#32773;&#35782;&#21035;&#65288;AA&#65289;&#8221;&#65292;&#21363;&#19981;&#20165;&#30830;&#23450;&#32473;&#23450;&#30340;&#25991;&#26412;&#26159;&#21542;&#20026;&#28145;&#20266;&#25991;&#26412;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#30830;&#23450;&#21738;&#20010;LLM&#26159;&#20316;&#32773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TopRoBERTa&#65292;&#36890;&#36807;&#21253;&#21547;&#26356;&#22810;&#28145;&#20266;&#25991;&#26412;&#20013;&#30340;&#35821;&#35328;&#27169;&#24335;&#26469;&#25913;&#36827;&#29616;&#26377;&#30340;AA&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Large Language Models (LLMs) have enabled the generation of open-ended high-quality texts, that are non-trivial to distinguish from human-written texts. We refer to such LLM-generated texts as \emph{deepfake texts}. There are currently over 11K text generation models in the huggingface model repo. As such, users with malicious intent can easily use these open-sourced LLMs to generate harmful texts and misinformation at scale. To mitigate this problem, a computational method to determine if a given text is a deepfake text or not is desired--i.e., Turing Test (TT). In particular, in this work, we investigate the more general version of the problem, known as \emph{Authorship Attribution (AA)}, in a multi-class setting--i.e., not only determining if a given text is a deepfake text or not but also being able to pinpoint which LLM is the author. We propose \textbf{TopRoBERTa} to improve existing AA solutions by capturing more linguistic patterns in deepfake texts by includ
&lt;/p&gt;</description></item><item><title>&#22312;&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#20013;&#65292;&#36890;&#36807;&#20026;&#26631;&#35760;&#21644;[CLS]&#31526;&#21495;&#20998;&#21035;&#20351;&#29992;&#24402;&#19968;&#21270;&#23618;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12931</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#20013;&#30340;&#20998;&#21035;&#24402;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Separate Normalization in Self-supervised Transformers. (arXiv:2309.12931v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12931
&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#20013;&#65292;&#36890;&#36807;&#20026;&#26631;&#35760;&#21644;[CLS]&#31526;&#21495;&#20998;&#21035;&#20351;&#29992;&#24402;&#19968;&#21270;&#23618;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#30340;&#35757;&#32451;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#20197;&#24448;&#30340;&#22522;&#20110;&#21464;&#24418;&#22120;&#30340;&#27169;&#22411;&#65288;&#22914;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65289;&#36890;&#24120;&#20250;&#20026;[CLS]&#31526;&#21495;&#21644;&#26631;&#35760;&#20351;&#29992;&#21333;&#29420;&#30340;&#24402;&#19968;&#21270;&#23618;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#25913;&#65292;&#20026;&#26631;&#35760;&#21644;[CLS]&#31526;&#21495;&#20998;&#21035;&#20351;&#29992;&#24402;&#19968;&#21270;&#23618;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#24182;&#22686;&#24378;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#32531;&#35299;&#23558;&#30456;&#21516;&#30340;&#24402;&#19968;&#21270;&#32479;&#35745;&#25968;&#25454;&#24212;&#29992;&#20110;&#20004;&#31181;&#26631;&#35760;&#31867;&#22411;&#21487;&#33021;&#24102;&#26469;&#30340;&#36127;&#38754;&#25928;&#26524;&#65292;&#36825;&#20123;&#32479;&#35745;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#19982;&#23427;&#20204;&#21508;&#33258;&#30340;&#35282;&#33394;&#26368;&#20339;&#21305;&#37197;&#12290;&#36890;&#36807;&#20351;&#29992;&#21333;&#29420;&#30340;&#24402;&#19968;&#21270;&#23618;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;[CLS]&#23884;&#20837;&#33021;&#22815;&#26356;&#22909;&#22320;&#32534;&#30721;&#20840;&#23616;&#35821;&#22659;&#20449;&#24687;&#65292;&#24182;&#22312;&#20854;&#38750;&#21508;&#21521;&#21516;&#24615;&#31354;&#38388;&#20013;&#20998;&#24067;&#26356;&#22343;&#21248;&#12290;&#24403;&#29992;&#36825;&#20004;&#20010;&#21333;&#29420;&#30340;&#24402;&#19968;&#21270;&#23618;&#26367;&#25442;&#24120;&#35268;&#30340;&#24402;&#19968;&#21270;&#23618;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24179;&#22343;&#24615;&#33021;&#25552;&#21319;&#20102;2.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised training methods for transformers have demonstrated remarkable performance across various domains. Previous transformer-based models, such as masked autoencoders (MAE), typically utilize a single normalization layer for both the [CLS] symbol and the tokens. We propose in this paper a simple modification that employs separate normalization layers for the tokens and the [CLS] symbol to better capture their distinct characteristics and enhance downstream task performance. Our method aims to alleviate the potential negative effects of using the same normalization statistics for both token types, which may not be optimally aligned with their individual roles. We empirically show that by utilizing a separate normalization layer, the [CLS] embeddings can better encode the global contextual information and are distributed more uniformly in its anisotropic space. When replacing the conventional normalization layer with the two separate layers, we observe an average 2.7% performa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#21152;&#24378;&#21305;&#37197;&#26694;&#26550;&#65288;ProtoEM&#65289;&#29992;&#20110;&#32852;&#21512;&#25277;&#21462;&#22810;&#31181;&#31867;&#22411;&#30340;&#20107;&#20214;&#20851;&#31995;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33719;&#21462;&#27599;&#31181;&#31867;&#22411;&#30340;&#20107;&#20214;&#20851;&#31995;&#30340;&#21407;&#22411;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21305;&#37197;&#36825;&#20123;&#20851;&#31995;&#65292;&#20174;&#32780;&#20840;&#38754;&#29702;&#35299;&#23427;&#20204;&#30340;&#20869;&#22312;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.12892</link><description>&lt;p&gt;
ProtoEM&#65306;&#19968;&#31181;&#29992;&#20110;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#30340;&#21407;&#22411;&#21152;&#24378;&#21305;&#37197;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ProtoEM: A Prototype-Enhanced Matching Framework for Event Relation Extraction. (arXiv:2309.12892v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#21152;&#24378;&#21305;&#37197;&#26694;&#26550;&#65288;ProtoEM&#65289;&#29992;&#20110;&#32852;&#21512;&#25277;&#21462;&#22810;&#31181;&#31867;&#22411;&#30340;&#20107;&#20214;&#20851;&#31995;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33719;&#21462;&#27599;&#31181;&#31867;&#22411;&#30340;&#20107;&#20214;&#20851;&#31995;&#30340;&#21407;&#22411;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21305;&#37197;&#36825;&#20123;&#20851;&#31995;&#65292;&#20174;&#32780;&#20840;&#38754;&#29702;&#35299;&#23427;&#20204;&#30340;&#20869;&#22312;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#65288;ERE&#65289;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#22810;&#31181;&#31867;&#22411;&#30340;&#20107;&#20214;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#21333;&#29420;&#23558;&#20107;&#20214;&#20851;&#31995;&#20998;&#31867;&#20026;&#19981;&#21516;&#31867;&#21035;&#65292;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#36825;&#20123;&#20851;&#31995;&#30340;&#20869;&#22312;&#35821;&#20041;&#12290;&#20026;&#20102;&#20840;&#38754;&#29702;&#35299;&#23427;&#20204;&#30340;&#20869;&#22312;&#35821;&#20041;&#65292;&#26412;&#25991;&#38024;&#23545;&#27599;&#31181;&#31867;&#22411;&#30340;&#20107;&#20214;&#20851;&#31995;&#33719;&#21462;&#21407;&#22411;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#21152;&#24378;&#21305;&#37197;&#65288;ProtoEM&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#25277;&#21462;&#22810;&#31181;&#31867;&#22411;&#30340;&#20107;&#20214;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ProtoEM&#20197;&#20004;&#27493;&#26041;&#24335;&#25552;&#21462;&#20107;&#20214;&#20851;&#31995;&#65292;&#21363;&#21407;&#22411;&#34920;&#31034;&#21644;&#21407;&#22411;&#21305;&#37197;&#12290;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;&#20026;&#20102;&#25429;&#25417;&#19981;&#21516;&#20107;&#20214;&#20851;&#31995;&#30340;&#20869;&#28085;&#65292;ProtoEM&#21033;&#29992;&#31034;&#20363;&#26469;&#34920;&#31034;&#19982;&#36825;&#20123;&#20851;&#31995;&#30456;&#23545;&#24212;&#30340;&#21407;&#22411;&#12290;&#38543;&#21518;&#65292;&#20026;&#20102;&#25429;&#25417;&#20107;&#20214;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#65292;&#23427;&#20026;&#19982;&#36825;&#20123;&#20851;&#31995;&#30456;&#23545;&#24212;&#30340;&#21407;&#22411;&#26500;&#24314;&#20102;&#19968;&#20010;&#20381;&#36182;&#22270;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event Relation Extraction (ERE) aims to extract multiple kinds of relations among events in texts. However, existing methods singly categorize event relations as different classes, which are inadequately capturing the intrinsic semantics of these relations. To comprehensively understand their intrinsic semantics, in this paper, we obtain prototype representations for each type of event relation and propose a Prototype-Enhanced Matching (ProtoEM) framework for the joint extraction of multiple kinds of event relations. Specifically, ProtoEM extracts event relations in a two-step manner, i.e., prototype representing and prototype matching. In the first step, to capture the connotations of different event relations, ProtoEM utilizes examples to represent the prototypes corresponding to these relations. Subsequently, to capture the interdependence among event relations, it constructs a dependency graph for the prototypes corresponding to these relations and utilized a Graph Neural Network (
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35782;&#21035;&#23545;&#35805;&#20013;&#20154;&#31867;&#24773;&#24863;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#24320;&#25918;&#39046;&#22495;&#38386;&#32842;&#23545;&#35805;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#20855;&#26377;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12881</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Affect Recognition in Conversations Using Large Language Models. (arXiv:2309.12881v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35782;&#21035;&#23545;&#35805;&#20013;&#20154;&#31867;&#24773;&#24863;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#24320;&#25918;&#39046;&#22495;&#38386;&#32842;&#23545;&#35805;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#20855;&#26377;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35782;&#21035;&#22312;&#20154;&#31867;&#20132;&#27969;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28085;&#30422;&#24773;&#32490;&#12289;&#24515;&#24773;&#21644;&#24863;&#21463;&#12290;&#22312;&#20250;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#35782;&#21035;&#21644;&#22238;&#24212;&#20154;&#31867;&#24773;&#24863;&#32447;&#32034;&#30340;&#33021;&#21147;&#23545;&#20110;&#21019;&#24314;&#24341;&#20154;&#20837;&#32988;&#19988;&#23500;&#26377;&#21516;&#29702;&#24515;&#30340;&#20114;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#20013;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#30340;&#33021;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#24320;&#25918;&#39046;&#22495;&#38386;&#32842;&#23545;&#35805;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12290;&#21033;&#29992;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;IEMOCAP&#12289;EmoWOZ&#21644;DAIC-WOZ&#65292;&#28085;&#30422;&#20102;&#20174;&#26085;&#24120;&#23545;&#35805;&#21040;&#20020;&#24202;&#38754;&#35797;&#30340;&#19981;&#21516;&#31867;&#22411;&#23545;&#35805;&#65292;&#25105;&#20204;&#35780;&#20272;&#24182;&#27604;&#36739;&#20102;LLMs&#22312;&#24773;&#24863;&#35782;&#21035;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#33021;&#21147;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20197;&#21450;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#26469;&#25552;&#39640;&#27169;&#22411;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#32771;&#34385;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Affect recognition, encompassing emotions, moods, and feelings, plays a pivotal role in human communication. In the realm of conversational artificial intelligence (AI), the ability to discern and respond to human affective cues is a critical factor for creating engaging and empathetic interactions. This study delves into the capacity of large language models (LLMs) to recognise human affect in conversations, with a focus on both open-domain chit-chat dialogues and task-oriented dialogues. Leveraging three diverse datasets, namely IEMOCAP, EmoWOZ, and DAIC-WOZ, covering a spectrum of dialogues from casual conversations to clinical interviews, we evaluated and compared LLMs' performance in affect recognition. Our investigation explores the zero-shot and few-shot capabilities of LLMs through in-context learning (ICL) as well as their model capacities through task-specific fine-tuning. Additionally, this study takes into account the potential impact of automatic speech recognition (ASR) e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#39046;&#22495;&#36866;&#24212;&#23545;&#38463;&#25289;&#20271;&#26426;&#22120;&#32763;&#35793;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#24179;&#34892;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#35797;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#23545;&#20110;&#25552;&#39640;&#38463;&#25289;&#20271;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#33021;&#20855;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.12863</link><description>&lt;p&gt;
&#38024;&#23545;&#38463;&#25289;&#20271;&#37329;&#34701;&#25991;&#26412;&#30340;&#39046;&#22495;&#36866;&#24212;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation for Arabic Machine Translation: The Case of Financial Texts. (arXiv:2309.12863v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12863
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#39046;&#22495;&#36866;&#24212;&#23545;&#38463;&#25289;&#20271;&#26426;&#22120;&#32763;&#35793;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#24179;&#34892;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#35797;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#23545;&#20110;&#25552;&#39640;&#38463;&#25289;&#20271;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#33021;&#20855;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#26102;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;&#30340;NMT&#31995;&#32479;&#22312;&#39046;&#22495;&#22806;&#30340;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#20102;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#24120;&#27604;&#36890;&#29992;&#30340;NMT&#31995;&#32479;&#26377;&#26356;&#22909;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#23613;&#31649;&#22312;NMT&#30340;&#33521;&#35821;&#21644;&#20854;&#20182;&#27431;&#27954;&#35821;&#35328;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#25345;&#32493;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#30340;&#39046;&#22495;&#36866;&#24212;&#22312;&#25991;&#29486;&#20013;&#21463;&#21040;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#38463;&#25289;&#20271;&#26426;&#22120;&#32763;&#35793;&#65288;AMT&#65289;&#39046;&#22495;&#29305;&#23450;&#36866;&#24212;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#23578;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#20013;&#65292;&#22914;&#37329;&#34701;&#26032;&#38395;&#25991;&#31456;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#31934;&#24515;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;-&#33521;&#35821;&#65288;AR-EN&#65289;&#37329;&#34701;&#39046;&#22495;&#32763;&#35793;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#20197;&#35780;&#20272;&#19981;&#21516;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#20960;&#20010;&#39044;&#35757;&#32451;&#30340;NMT&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#21253;&#25324;ChatGPT-3.5 Turbo&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation (NMT) has shown impressive performance when trained on large-scale corpora. However, generic NMT systems have demonstrated poor performance on out-of-domain translation. To mitigate this issue, several domain adaptation methods have recently been proposed which often lead to better translation quality than genetic NMT systems. While there has been some continuous progress in NMT for English and other European languages, domain adaption in Arabic has received little attention in the literature. The current study, therefore, aims to explore the effectiveness of domain-specific adaptation for Arabic MT (AMT), in yet unexplored domain, financial news articles. To this end, we developed carefully a parallel corpus for Arabic-English (AR- EN) translation in the financial domain for benchmarking different domain adaptation methods. We then fine-tuned several pre-trained NMT and Large Language models including ChatGPT-3.5 Turbo on our dataset. The results showed that
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20998;&#21106;&#27169;&#22411;&#65288;VLSM&#65289;&#65292;&#32467;&#26524;&#26174;&#31034;&#21512;&#25104;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#20998;&#21106;&#27169;&#22411;&#30340;&#25351;&#26631;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.12829</link><description>&lt;p&gt;
&#21512;&#25104;&#25552;&#21319;&#65306;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#20013;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Synthetic Boost: Leveraging Synthetic Data for Enhanced Vision-Language Segmentation in Echocardiography. (arXiv:2309.12829v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20998;&#21106;&#27169;&#22411;&#65288;VLSM&#65289;&#65292;&#32467;&#26524;&#26174;&#31034;&#21512;&#25104;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#20998;&#21106;&#27169;&#22411;&#30340;&#25351;&#26631;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20998;&#21106;&#23545;&#20110;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#24515;&#34880;&#31649;&#30142;&#30149;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36229;&#22768;&#22270;&#20687;&#30340;&#21464;&#24322;&#24615;&#21644;&#22266;&#26377;&#25361;&#25112;&#38459;&#30861;&#20102;&#31934;&#30830;&#30340;&#20998;&#21106;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#32852;&#21512;&#34920;&#31034;&#65292;&#35270;&#35273;-&#35821;&#35328;&#20998;&#21106;&#27169;&#22411;&#65288;VLSM&#65289;&#21487;&#20197;&#34701;&#20837;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#31934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#36229;&#22768;&#24515;&#21160;&#22270;&#20013;&#32570;&#20047;&#29616;&#25104;&#30340;&#25968;&#25454;&#38459;&#30861;&#20102;VLSM&#30340;&#35757;&#32451;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#35821;&#20041;&#25193;&#25955;&#27169;&#22411;&#65288;SDM&#65289;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;&#30340;VLSM&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#12289;&#20998;&#21106;&#25513;&#27169;&#21644;&#20803;&#25968;&#25454;&#20013;&#33258;&#21160;&#25552;&#21462;&#30340;&#22810;&#20010;&#23646;&#24615;&#23548;&#20986;&#30340;&#19971;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#25552;&#31034;&#26469;&#35780;&#20272;&#20004;&#20010;&#27969;&#34892;&#30340;VLSM&#27169;&#22411;&#65288;CLIPSeg&#21644;CRIS&#65289;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#39044;&#35757;&#32451;VLSM&#26102;&#65292;&#36716;&#25442;&#21644;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate segmentation is essential for echocardiography-based assessment of cardiovascular diseases (CVDs). However, the variability among sonographers and the inherent challenges of ultrasound images hinder precise segmentation. By leveraging the joint representation of image and text modalities, Vision-Language Segmentation Models (VLSMs) can incorporate rich contextual information, potentially aiding in accurate and explainable segmentation. However, the lack of readily available data in echocardiography hampers the training of VLSMs. In this study, we explore using synthetic datasets from Semantic Diffusion Models (SDMs) to enhance VLSMs for echocardiography segmentation. We evaluate results for two popular VLSMs (CLIPSeg and CRIS) using seven different kinds of language prompts derived from several attributes, automatically extracted from echocardiography images, segmentation masks, and their metadata. Our results show improved metrics and faster convergence when pretraining VLSMs
&lt;/p&gt;</description></item><item><title>StyloMetrix&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#20379;&#35206;&#30422;&#35821;&#27861;&#12289;&#21477;&#27861;&#21644;&#35789;&#27719;&#31561;&#21508;&#20010;&#26041;&#38754;&#30340;&#25991;&#20307;&#25991;&#26412;&#34920;&#31034;&#12290;&#23427;&#30340;&#24402;&#19968;&#21270;&#36755;&#20986;&#21487;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#24182;&#21487;&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#23884;&#20837;&#23618;&#30340;&#26377;&#20215;&#20540;&#34917;&#20805;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;StyloMetrix&#21521;&#37327;&#22312;&#20869;&#23481;&#20998;&#31867;&#21644;&#23884;&#20837;&#23618;&#22686;&#24378;&#26041;&#38754;&#30340;&#24212;&#29992;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12810</link><description>&lt;p&gt;
StyloMetrix:&#19968;&#31181;&#29992;&#20110;&#34920;&#31034;&#25991;&#20307;&#21521;&#37327;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
StyloMetrix: An Open-Source Multilingual Tool for Representing Stylometric Vectors. (arXiv:2309.12810v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12810
&lt;/p&gt;
&lt;p&gt;
StyloMetrix&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#20379;&#35206;&#30422;&#35821;&#27861;&#12289;&#21477;&#27861;&#21644;&#35789;&#27719;&#31561;&#21508;&#20010;&#26041;&#38754;&#30340;&#25991;&#20307;&#25991;&#26412;&#34920;&#31034;&#12290;&#23427;&#30340;&#24402;&#19968;&#21270;&#36755;&#20986;&#21487;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#24182;&#21487;&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#23884;&#20837;&#23618;&#30340;&#26377;&#20215;&#20540;&#34917;&#20805;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;StyloMetrix&#21521;&#37327;&#22312;&#20869;&#23481;&#20998;&#31867;&#21644;&#23884;&#20837;&#23618;&#22686;&#24378;&#26041;&#38754;&#30340;&#24212;&#29992;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#20851;&#20110;&#24320;&#28304;&#22810;&#35821;&#35328;&#24037;&#20855;StyloMetrix&#30340;&#27010;&#36848;&#12290;&#23427;&#25552;&#20379;&#20102;&#35206;&#30422;&#35821;&#27861;&#12289;&#21477;&#27861;&#21644;&#35789;&#27719;&#31561;&#21508;&#20010;&#26041;&#38754;&#30340;&#25991;&#20307;&#25991;&#26412;&#34920;&#31034;&#12290;StyloMetrix&#35206;&#30422;&#20102;&#22235;&#31181;&#35821;&#35328;&#65306;&#27874;&#20848;&#35821;&#20316;&#20026;&#20027;&#35201;&#35821;&#35328;&#65292;&#33521;&#35821;&#12289;&#20044;&#20811;&#20848;&#35821;&#21644;&#20420;&#35821;&#12290;&#27599;&#20010;&#29305;&#24449;&#30340;&#24402;&#19968;&#21270;&#36755;&#20986;&#21487;&#20197;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26377;&#30410;&#26469;&#28304;&#65292;&#24182;&#23545;&#20219;&#20309;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#23884;&#20837;&#23618;&#36215;&#21040;&#26377;&#20215;&#20540;&#30340;&#34917;&#20805;&#12290;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;StyloMetrix&#21521;&#37327;&#24212;&#29992;&#30340;&#31616;&#26126;&#20294;&#35814;&#23613;&#30340;&#27010;&#36848;&#65292;&#20197;&#21450;&#23545;&#24320;&#21457;&#30340;&#35821;&#35328;&#29305;&#24449;&#38598;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#31616;&#21333;&#31639;&#27861;&#65288;&#22914;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#12289;&#25237;&#31080;&#20998;&#31867;&#22120;&#12289;&#36923;&#36753;&#22238;&#24402;&#31561;&#65289;&#30340;&#30417;&#30563;&#20869;&#23481;&#20998;&#31867;&#20013;&#65292;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28145;&#24230;&#23398;&#20064;&#35780;&#20272;&#25581;&#31034;&#20102;StyloMetrix&#21521;&#37327;&#22312;&#22686;&#24378;&#20174;Transformer&#26550;&#26500;&#20013;&#25552;&#21462;&#30340;&#23884;&#20837;&#23618;&#26041;&#38754;&#30340;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims to provide an overview on the open-source multilanguage tool called StyloMetrix. It offers stylometric text representations that cover various aspects of grammar, syntax and lexicon. StyloMetrix covers four languages: Polish as the primary language, English, Ukrainian and Russian. The normalized output of each feature can become a fruitful course for machine learning models and a valuable addition to the embeddings layer for any deep learning algorithm. We strive to provide a concise, but exhaustive overview on the application of the StyloMetrix vectors as well as explain the sets of the developed linguistic features. The experiments have shown promising results in supervised content classification with simple algorithms as Random Forest Classifier, Voting Classifier, Logistic Regression and others. The deep learning assessments have unveiled the usefulness of the StyloMetrix vectors at enhancing an embedding layer extracted from Transformer architectures. The StyloMetri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#30340;&#20010;&#24615;&#21270;&#33521;&#35821;&#38405;&#35835;&#29702;&#35299;&#36741;&#21161;&#31995;&#32479;ChatPRCS&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#36827;&#33021;&#21147;&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#39044;&#27979;&#12289;&#38382;&#39064;&#29983;&#25104;&#12289;&#33258;&#21160;&#35780;&#20272;&#31561;&#26041;&#27861;&#26469;&#22686;&#24378;&#38405;&#35835;&#29702;&#35299;&#25945;&#23398;&#12290;&#20351;&#29992;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#29983;&#25104;&#36866;&#24403;&#38590;&#24230;&#30340;&#38382;&#39064;&#12290;&#36741;&#21161;&#31995;&#32479;&#25552;&#20379;&#20102;&#20010;&#20307;&#21270;&#30340;&#38405;&#35835;&#29702;&#35299;&#35757;&#32451;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2309.12808</link><description>&lt;p&gt;
ChatPRCS: &#22522;&#20110;ChatGPT&#30340;&#20010;&#24615;&#21270;&#33521;&#35821;&#38405;&#35835;&#29702;&#35299;&#36741;&#21161;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
ChatPRCS: A Personalized Support System for English Reading Comprehension based on ChatGPT. (arXiv:2309.12808v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#30340;&#20010;&#24615;&#21270;&#33521;&#35821;&#38405;&#35835;&#29702;&#35299;&#36741;&#21161;&#31995;&#32479;ChatPRCS&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#36827;&#33021;&#21147;&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#39044;&#27979;&#12289;&#38382;&#39064;&#29983;&#25104;&#12289;&#33258;&#21160;&#35780;&#20272;&#31561;&#26041;&#27861;&#26469;&#22686;&#24378;&#38405;&#35835;&#29702;&#35299;&#25945;&#23398;&#12290;&#20351;&#29992;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#29983;&#25104;&#36866;&#24403;&#38590;&#24230;&#30340;&#38382;&#39064;&#12290;&#36741;&#21161;&#31995;&#32479;&#25552;&#20379;&#20102;&#20010;&#20307;&#21270;&#30340;&#38405;&#35835;&#29702;&#35299;&#35757;&#32451;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#23398;&#20064;&#33521;&#35821;&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#38405;&#35835;&#29702;&#35299;&#20027;&#35201;&#21253;&#25324;&#38405;&#35835;&#25991;&#31456;&#21644;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#26377;&#25928;&#32451;&#20064;&#30340;&#22797;&#26434;&#24615;&#23548;&#33268;&#23398;&#29983;&#36935;&#21040;&#26631;&#20934;&#21270;&#38382;&#39064;&#65292;&#20351;&#20854;&#38590;&#20197;&#19982;&#20010;&#20307;&#21270;&#23398;&#20064;&#32773;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#30456;&#21305;&#37197;&#12290;&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#25552;&#20379;&#30340;&#20808;&#36827;&#33021;&#21147;&#65292;&#22522;&#20110;&#36817;&#21457;&#23637;&#21306;&#29702;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20010;&#24615;&#21270;&#38405;&#35835;&#29702;&#35299;&#36741;&#21161;&#31995;&#32479;ChatPRCS&#12290;ChatPRCS&#37319;&#29992;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#39044;&#27979;&#12289;&#38382;&#39064;&#29983;&#25104;&#12289;&#33258;&#21160;&#35780;&#20272;&#31561;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#38405;&#35835;&#29702;&#35299;&#25945;&#23398;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#23398;&#20064;&#32773;&#30340;&#21382;&#21490;&#25968;&#25454;&#39044;&#27979;&#20182;&#20204;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#65292;&#20026;&#29983;&#25104;&#20855;&#26377;&#30456;&#24212;&#38590;&#24230;&#27700;&#24179;&#30340;&#38382;&#39064;&#22880;&#23450;&#22522;&#30784;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#30340;&#38405;&#35835;&#29702;&#35299;&#25903;&#25345;&#21151;&#33021;&#65292;&#22914;&#38382;&#39064;&#29983;&#25104;&#12289;&#33258;&#21160;&#35780;&#20272;&#31561;&#65292;&#26469;&#24110;&#21161;&#23398;&#29983;&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#38405;&#35835;&#29702;&#35299;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a common approach to learning English, reading comprehension primarily entails reading articles and answering related questions. However, the complexity of designing effective exercises results in students encountering standardized questions, making it challenging to align with individualized learners' reading comprehension ability. By leveraging the advanced capabilities offered by large language models, exemplified by ChatGPT, this paper presents a novel personalized support system for reading comprehension, referred to as ChatPRCS, based on the Zone of Proximal Development theory. ChatPRCS employs methods including reading comprehension proficiency prediction, question generation, and automatic evaluation, among others, to enhance reading comprehension instruction. First, we develop a new algorithm that can predict learners' reading comprehension abilities using their historical data as the foundation for generating questions at an appropriate level of difficulty. Second, a serie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FuRePA&#30340;&#26032;&#30340;MHQA&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;fra&#31639;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#20449;&#24687;&#26816;&#32034;&#22120;&#21463;&#21040;LLMs&#29983;&#25104;&#26597;&#35810;&#36136;&#37327;&#20302;&#21644;LLMs&#34987;IR&#25552;&#20379;&#30340;&#26080;&#20851;&#30693;&#35782;&#35823;&#23548;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12767</link><description>&lt;p&gt;
&#19982;&#35745;&#21010;&#35780;&#20272;&#30340;&#26368;&#36828;&#25512;&#29702;&#65306;&#20855;&#26377;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31283;&#23450;&#25512;&#29702;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Furthest Reasoning with Plan Assessment: Stable Reasoning Path with Retrieval-Augmented Large Language Models. (arXiv:2309.12767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FuRePA&#30340;&#26032;&#30340;MHQA&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;fra&#31639;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#20449;&#24687;&#26816;&#32034;&#22120;&#21463;&#21040;LLMs&#29983;&#25104;&#26597;&#35810;&#36136;&#37327;&#20302;&#21644;LLMs&#34987;IR&#25552;&#20379;&#30340;&#26080;&#20851;&#30693;&#35782;&#35823;&#23548;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#24378;&#22823;&#30340;&#25512;&#29702;&#21644;&#29983;&#25104;&#22120;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65288;&#22914;&#38382;&#31572;&#65289;&#20013;&#23637;&#29616;&#20986;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#22810;&#36339;&#38382;&#31572;&#65288;MHQA&#65289;&#26159;&#19968;&#20010;&#24191;&#27867;&#35752;&#35770;&#30340;&#31867;&#21035;&#65292;&#38656;&#35201;LLMs&#19982;&#22806;&#37096;&#30693;&#35782;&#30340;&#26080;&#32541;&#38598;&#25104;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;LLMs&#29983;&#25104;&#25512;&#29702;&#36335;&#24452;&#21644;&#35745;&#21010;&#65292;&#24182;&#21033;&#29992;IR&#36845;&#20195;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#22266;&#26377;&#32570;&#38519;&#12290;&#19968;&#26041;&#38754;&#65292;&#20449;&#24687;&#26816;&#32034;&#22120;&#65288;IR&#65289;&#21463;&#21040;LLMs&#29983;&#25104;&#26597;&#35810;&#36136;&#37327;&#20302;&#30340;&#24433;&#21709;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;LLMs&#24456;&#23481;&#26131;&#34987;IR&#25552;&#20379;&#30340;&#26080;&#20851;&#30693;&#35782;&#35823;&#23548;&#12290;&#36825;&#20123;&#19981;&#20934;&#30830;&#24615;&#30001;IR&#19982;LLMs&#20043;&#38388;&#30340;&#36845;&#20195;&#20132;&#20114;&#32047;&#31215;&#65292;&#26368;&#32456;&#23548;&#33268;&#25928;&#26524;&#30340;&#28798;&#38590;&#24615;&#34928;&#20943;&#12290;&#20026;&#20102;&#20811;&#26381;&#20197;&#19978;&#38556;&#30861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;MHQA&#27969;&#27700;&#32447;&#65292;&#31216;&#20026;&#26368;&#36828;&#25512;&#29702;&#19982;&#35745;&#21010;&#35780;&#20272;&#65288;FuRePA&#65289;&#65292;&#21253;&#25324;&#19968;&#20010;&#25913;&#36827;&#30340;fra
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), acting as a powerful reasoner and generator, exhibit extraordinary performance across various natural language tasks, such as question answering (QA). Among these tasks, Multi-Hop Question Answering (MHQA) stands as a widely discussed category, necessitating seamless integration between LLMs and the retrieval of external knowledge. Existing methods employ LLM to generate reasoning paths and plans, and utilize IR to iteratively retrieve related knowledge, but these approaches have inherent flaws. On one hand, Information Retriever (IR) is hindered by the low quality of generated queries by LLM. On the other hand, LLM is easily misguided by the irrelevant knowledge by IR. These inaccuracies, accumulated by the iterative interaction between IR and LLM, lead to a disaster in effectiveness at the end. To overcome above barriers, in this paper, we propose a novel pipeline for MHQA called Furthest-Reasoning-with-Plan-Assessment (FuRePA), including an improved fra
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#38899;&#39057;&#22686;&#24378;&#20026;&#20302;&#36164;&#28304;&#33258;&#25105;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#32508;&#21512;&#22686;&#24378;&#65288;&#22122;&#22768;/&#38899;&#39640;&#65289;&#26159;&#26368;&#20339;&#30340;&#22686;&#24378;&#31574;&#30053;&#65292;&#36229;&#36807;&#20102;&#37325;&#38899;&#21644;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2309.12763</link><description>&lt;p&gt;
&#20943;&#23569;&#12289;&#22797;&#29992;&#12289;&#22238;&#25910;&#65306;&#19982;&#20854;&#20182;&#35821;&#35328;&#22686;&#24378;&#30456;&#27604;&#65292;&#34987;&#25200;&#21160;&#25968;&#25454;&#23545;&#20302;&#36164;&#28304;&#33258;&#25105;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Reduce, Reuse, Recycle: Is Perturbed Data better than Other Language augmentation for Low Resource Self-Supervised Speech Models. (arXiv:2309.12763v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12763
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38899;&#39057;&#22686;&#24378;&#20026;&#20302;&#36164;&#28304;&#33258;&#25105;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#32508;&#21512;&#22686;&#24378;&#65288;&#22122;&#22768;/&#38899;&#39640;&#65289;&#26159;&#26368;&#20339;&#30340;&#22686;&#24378;&#31574;&#30053;&#65292;&#36229;&#36807;&#20102;&#37325;&#38899;&#21644;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65288;SSRL&#65289;&#24050;&#32463;&#25913;&#21892;&#20102;&#19979;&#28216;&#38899;&#32032;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#30456;&#23545;&#20110;&#21463;&#30417;&#30563;&#30340;&#27169;&#22411;&#12290;&#35757;&#32451;SSRL&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#20174;&#20854;&#20182;&#35821;&#35328;&#20013;&#36716;&#31227;&#30693;&#35782;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#38899;&#39057;&#22686;&#24378;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#39044;&#35757;&#32451;SSRL&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#19979;&#28216;&#20219;&#21153;&#30340;&#38899;&#32032;&#35782;&#21035;&#12290;&#25105;&#20204;&#23545;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#27604;&#36739;&#65292;&#21253;&#25324;&#38899;&#39640;&#21464;&#21270;&#12289;&#22122;&#22768;&#28155;&#21152;&#12289;&#26377;&#37325;&#38899;&#30340;&#30446;&#26631;&#35821;&#38899;&#21644;&#20854;&#20182;&#35821;&#35328;&#30340;&#35821;&#38899;&#12290;&#25105;&#20204;&#21457;&#29616;&#32508;&#21512;&#22686;&#24378;&#65288;&#22122;&#22768;/&#38899;&#39640;&#65289;&#26159;&#26368;&#22909;&#30340;&#22686;&#24378;&#31574;&#30053;&#65292;&#36229;&#36807;&#20102;&#37325;&#38899;&#21644;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#25968;&#37327;&#21644;&#31867;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#32771;&#23519;&#20102;&#22686;&#24378;&#25968;&#25454;&#30340;&#32553;&#25918;&#22240;&#23376;&#65292;&#20197;&#36798;&#21040;&#19982;&#39044;&#35757;&#32451;&#30446;&#26631;&#22495;&#35821;&#38899;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Self-supervised representation learning (SSRL) has improved the performance on downstream phoneme recognition versus supervised models. Training SSRL models requires a large amount of pre-training data and this poses a challenge for low resource languages. A common approach is transferring knowledge from other languages. Instead, we propose to use audio augmentation to pre-train SSRL models in a low resource condition and evaluate phoneme recognition as downstream task. We performed a systematic comparison of augmentation techniques, namely: pitch variation, noise addition, accented target-language speech and other language speech. We found combined augmentations (noise/pitch) was the best augmentation strategy outperforming accent and language knowledge transfer. We compared the performance with various quantities and types of pre-training data. We examined the scaling factor of augmented data to achieve equivalent performance to models pre-trained with target domain speech. Our findi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32842;&#22825;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#24178;&#25200;&#38382;&#39064;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#25345;&#32493;&#27969;&#21160;&#30340;&#20449;&#24687;&#20043;&#38388;&#21487;&#33021;&#20250;&#36973;&#21463;&#24178;&#25200;&#65292;&#23548;&#33268;&#36951;&#24536;&#20043;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#38477;&#20302;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12727</link><description>&lt;p&gt;
&#32842;&#22825;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#24178;&#25200;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
In-context Interference in Chat-based Large Language Models. (arXiv:2309.12727v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32842;&#22825;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#24178;&#25200;&#38382;&#39064;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#25345;&#32493;&#27969;&#21160;&#30340;&#20449;&#24687;&#20043;&#38388;&#21487;&#33021;&#20250;&#36973;&#21463;&#24178;&#25200;&#65292;&#23548;&#33268;&#36951;&#24536;&#20043;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#38477;&#20302;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#33021;&#21147;&#21644;&#24191;&#27867;&#30340;&#19990;&#30028;&#30693;&#35782;&#32780;&#23545;&#31038;&#20250;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#12290;&#21019;&#24314;&#20102;&#21508;&#31181;&#24212;&#29992;&#21644;&#24037;&#20855;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20197;&#40657;&#30418;&#22330;&#26223;&#19982;&#36825;&#20123;&#27169;&#22411;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22330;&#26223;&#30340;&#38480;&#21046;&#20043;&#19968;&#26159;&#29992;&#25143;&#26080;&#27861;&#20462;&#25913;&#27169;&#22411;&#30340;&#20869;&#37096;&#30693;&#35782;&#65292;&#28155;&#21152;&#25110;&#20462;&#25913;&#20869;&#37096;&#30693;&#35782;&#30340;&#21807;&#19968;&#26041;&#27861;&#26159;&#22312;&#24403;&#21069;&#20132;&#20114;&#36807;&#31243;&#20013;&#26126;&#30830;&#25552;&#21450;&#12290;&#36825;&#31181;&#23398;&#20064;&#36807;&#31243;&#31216;&#20026;&#19978;&#19979;&#25991;&#35757;&#32451;&#65292;&#25351;&#30340;&#26159;&#38480;&#23450;&#22312;&#29992;&#25143;&#24403;&#21069;&#20250;&#35805;&#25110;&#19978;&#19979;&#25991;&#20013;&#36827;&#34892;&#30340;&#35757;&#32451;&#12290;&#19978;&#19979;&#25991;&#23398;&#20064;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#65292;&#20294;&#20063;&#23384;&#22312;&#24456;&#23569;&#30740;&#31350;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#35828;&#26126;&#20102;&#27169;&#22411;&#21487;&#33021;&#20250;&#36973;&#21463;&#22312;&#19978;&#19979;&#25991;&#20013;&#19981;&#26029;&#27969;&#21160;&#30340;&#20449;&#24687;&#20043;&#38388;&#30340;&#24178;&#25200;&#65292;&#20174;&#32780;&#23548;&#33268;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#38500;&#20102;&#23637;&#31034;&#38382;&#39064;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have had a huge impact on society due to their impressive capabilities and vast knowledge of the world. Various applications and tools have been created that allow users to interact with these models in a black-box scenario. However, one limitation of this scenario is that users cannot modify the internal knowledge of the model, and the only way to add or modify internal knowledge is by explicitly mentioning it to the model during the current interaction. This learning process is called in-context training, and it refers to training that is confined to the user's current session or context. In-context learning has significant applications, but also has limitations that are seldom studied. In this paper, we present a study that shows how the model can suffer from interference between information that continually flows in the context, causing it to forget previously learned knowledge, which can reduce the model's performance. Along with showing the problem, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#30452;&#25509;&#39044;&#27979;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#24471;&#21040;&#30340;&#30456;&#20284;&#24615;&#26356;&#21152;&#31526;&#21512;&#25105;&#20204;&#23545;&#40065;&#26834;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#39044;&#26399;&#12290;</title><link>http://arxiv.org/abs/2309.12697</link><description>&lt;p&gt;
&#35821;&#20041;&#30456;&#20284;&#24615;&#39044;&#27979;&#20248;&#20110;&#20854;&#20182;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity prediction is better than other semantic similarity measures. (arXiv:2309.12697v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#30452;&#25509;&#39044;&#27979;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#24471;&#21040;&#30340;&#30456;&#20284;&#24615;&#26356;&#21152;&#31526;&#21512;&#25105;&#20204;&#23545;&#40065;&#26834;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#36890;&#24120;&#36890;&#36807;&#26816;&#26597;&#23376;&#24207;&#21015;&#30340;&#37325;&#21472;&#65288;&#20363;&#22914;BLEU&#65289;&#25110;&#20351;&#29992;&#23884;&#20837;&#65288;&#20363;&#22914;BERTScore&#65292;S-BERT&#65289;&#26469;&#34913;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#25105;&#20204;&#20165;&#23545;&#34913;&#37327;&#35821;&#20041;&#30456;&#20284;&#24615;&#24863;&#20852;&#36259;&#26102;&#65292;&#30452;&#25509;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#30456;&#20284;&#24615;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#24494;&#35843;&#30340;STS-B&#27169;&#22411;&#65292;&#23450;&#20041;&#20102;STSScore&#26041;&#27861;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#25152;&#24471;&#21040;&#30340;&#30456;&#20284;&#24615;&#19982;&#25105;&#20204;&#23545;&#40065;&#26834;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#39044;&#26399;&#26356;&#21152;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity between natural language texts is typically measured either by looking at the overlap between subsequences (e.g., BLEU) or by using embeddings (e.g., BERTScore, S-BERT). Within this paper, we argue that when we are only interested in measuring the semantic similarity, it is better to directly predict the similarity using a fine-tuned model for such a task. Using a fine-tuned model for the STS-B from the GLUE benchmark, we define the STSScore approach and show that the resulting similarity is better aligned with our expectations on a robust semantic similarity measure than other approaches.
&lt;/p&gt;</description></item><item><title>AMPLIFY&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Mixup&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#21407;&#22987;&#26679;&#26412;&#20013;&#30340;&#22122;&#38899;&#21644;&#24322;&#24120;&#20540;&#23545;&#20110;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12689</link><description>&lt;p&gt;
AMPLIFY: &#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Mixup&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26631;&#31614;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
AMPLIFY:Attention-based Mixup for Performance Improvement and Label Smoothing in Transformer. (arXiv:2309.12689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12689
&lt;/p&gt;
&lt;p&gt;
AMPLIFY&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Mixup&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#21407;&#22987;&#26679;&#26412;&#20013;&#30340;&#22122;&#38899;&#21644;&#24322;&#24120;&#20540;&#23545;&#20110;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#21407;&#22987;&#26679;&#26412;&#30340;&#32447;&#24615;&#32452;&#21512;&#29983;&#25104;&#26032;&#30340;&#22686;&#24378;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#21407;&#22987;&#26679;&#26412;&#20013;&#23384;&#22312;&#22122;&#38899;&#25110;&#24322;&#24120;&#29305;&#24449;&#65292;Mixup&#21487;&#33021;&#23558;&#20854;&#20256;&#25773;&#21040;&#22686;&#24378;&#26679;&#26412;&#20013;&#65292;&#23548;&#33268;&#27169;&#22411;&#23545;&#36825;&#20123;&#24322;&#24120;&#20540;&#36807;&#20110;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Mixup&#26041;&#27861;&#31216;&#20026;AMPLIFY&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;Transformer&#33258;&#36523;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20943;&#23569;&#21407;&#22987;&#26679;&#26412;&#20013;&#22122;&#38899;&#21644;&#24322;&#24120;&#20540;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#26080;&#38656;&#22686;&#21152;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#20302;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#24120;&#35265;Mixup&#26041;&#27861;&#65288;&#20363;&#22914;&#35821;&#21477;Mixup&#65289;&#20013;&#36164;&#28304;&#28040;&#32791;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26356;&#23567;&#30340;&#35745;&#31639;&#36164;&#28304;&#25104;&#26412;&#19979;&#65292;AMPLIFY&#22312;7&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;Mixup&#26041;&#27861;&#65292;&#20026;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is an effective data augmentation method that generates new augmented samples by aggregating linear combinations of different original samples. However, if there are noises or aberrant features in the original samples, Mixup may propagate them to the augmented samples, leading to over-sensitivity of the model to these outliers . To solve this problem, this paper proposes a new Mixup method called AMPLIFY. This method uses the Attention mechanism of Transformer itself to reduce the influence of noises and aberrant values in the original samples on the prediction results, without increasing additional trainable parameters, and the computational cost is very low, thereby avoiding the problem of high resource consumption in common Mixup methods such as Sentence Mixup . The experimental results show that, under a smaller computational resource cost, AMPLIFY outperforms other Mixup methods in text classification tasks on 7 benchmark datasets, providing new ideas and new ways to further
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;JCoLA&#65288;&#26085;&#26412;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#35821;&#26009;&#24211;&#65289;&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;10,020&#20010;&#21477;&#23376;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#31867;&#22411;&#30340;&#26085;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#27861;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2309.12676</link><description>&lt;p&gt;
JCoLA: &#26085;&#26412;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
JCoLA: Japanese Corpus of Linguistic Acceptability. (arXiv:2309.12676v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JCoLA&#65288;&#26085;&#26412;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#35821;&#26009;&#24211;&#65289;&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;10,020&#20010;&#21477;&#23376;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#31867;&#22411;&#30340;&#26085;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#27861;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#20869;&#21270;&#21477;&#27861;&#30693;&#35782;&#30340;&#31243;&#24230;&#36824;&#26377;&#38480;&#65292;&#22240;&#27492;&#26368;&#36817;&#24050;&#32463;&#26500;&#24314;&#20102;&#21508;&#31181;&#25968;&#25454;&#38598;&#26469;&#20419;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#19978;&#30340;&#21477;&#27861;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;JCoLA&#65288;&#26085;&#26412;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#35821;&#26009;&#24211;&#65289;&#65292;&#23427;&#21253;&#21547;10,020&#20010;&#36890;&#36807;&#20108;&#20803;&#21487;&#25509;&#21463;&#24615;&#21028;&#26029;&#27880;&#37322;&#30340;&#21477;&#23376;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#20123;&#21477;&#23376;&#26159;&#20174;&#35821;&#35328;&#23398;&#25945;&#26448;&#12289;&#25163;&#20876;&#21644;&#26399;&#21002;&#25991;&#31456;&#20013;&#25163;&#21160;&#25552;&#21462;&#30340;&#65292;&#24182;&#20998;&#20026;&#39046;&#22495;&#20869;&#25968;&#25454;&#65288;86&#65285;&#65307;&#20174;&#25945;&#26448;&#21644;&#25163;&#20876;&#20013;&#25552;&#21462;&#30340;&#30456;&#23545;&#31616;&#21333;&#30340;&#21487;&#25509;&#21463;&#24615;&#21028;&#26029;&#65289;&#21644;&#39046;&#22495;&#22806;&#25968;&#25454;&#65288;14&#65285;&#65307;&#20174;&#26399;&#21002;&#25991;&#31456;&#20013;&#25552;&#21462;&#30340;&#29702;&#35770;&#19978;&#37325;&#35201;&#30340;&#21487;&#25509;&#21463;&#24615;&#21028;&#26029;&#65289;&#65292;&#21518;&#32773;&#25353;&#29031;12&#31181;&#35821;&#35328;&#29616;&#35937;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;9&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26085;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#27861;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural language models have exhibited outstanding performance in a range of downstream tasks. However, there is limited understanding regarding the extent to which these models internalize syntactic knowledge, so that various datasets have recently been constructed to facilitate syntactic evaluation of language models across languages. In this paper, we introduce JCoLA (Japanese Corpus of Linguistic Acceptability), which consists of 10,020 sentences annotated with binary acceptability judgments. Specifically, those sentences are manually extracted from linguistics textbooks, handbooks and journal articles, and split into in-domain data (86 %; relatively simple acceptability judgments extracted from textbooks and handbooks) and out-of-domain data (14 %; theoretically significant acceptability judgments extracted from journal articles), the latter of which is categorized by 12 linguistic phenomena. We then evaluate the syntactic knowledge of 9 different types of Japanese language models 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#25552;&#31034;&#31574;&#30053;&#21644;&#26816;&#32034;&#24605;&#36335;&#65292;&#29992;&#20110;&#35299;&#20915;&#34920;-&#25991;&#26412;&#28151;&#21512;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20351;&#27169;&#22411;&#20855;&#22791;&#22788;&#29702;&#28151;&#21512;&#25968;&#25454;&#30340;&#26816;&#32034;&#24605;&#32771;&#33021;&#21147;&#65292;&#24182;&#22312;MultiHiertt&#25968;&#25454;&#38598;&#19978;&#30340;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12669</link><description>&lt;p&gt;
HRoT: &#28151;&#21512;&#25552;&#31034;&#31574;&#30053;&#21644;&#26816;&#32034;&#24605;&#36335;&#29992;&#20110;&#34920;-&#25991;&#26412;&#28151;&#21512;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering. (arXiv:2309.12669v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#25552;&#31034;&#31574;&#30053;&#21644;&#26816;&#32034;&#24605;&#36335;&#65292;&#29992;&#20110;&#35299;&#20915;&#34920;-&#25991;&#26412;&#28151;&#21512;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20351;&#27169;&#22411;&#20855;&#22791;&#22788;&#29702;&#28151;&#21512;&#25968;&#25454;&#30340;&#26816;&#32034;&#24605;&#32771;&#33021;&#21147;&#65292;&#24182;&#22312;MultiHiertt&#25968;&#25454;&#38598;&#19978;&#30340;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#30340;&#34920;&#26684;&#21644;&#25991;&#26412;&#25968;&#25454;(TextTableQA)&#19978;&#22238;&#31572;&#25968;&#20540;&#38382;&#39064;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#37325;&#22823;&#20851;&#27880;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24605;&#36335;&#38142;&#25552;&#31034;&#25104;&#20026;&#20102;&#36825;&#20010;&#39046;&#22495;&#20013;&#29305;&#21035;&#27969;&#34892;&#30340;&#20004;&#20010;&#30740;&#31350;&#20027;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#28151;&#21512;&#25552;&#31034;&#31574;&#30053;&#21644;&#26816;&#32034;&#24605;&#36335;&#30340;&#26032;&#25552;&#31034;&#31574;&#30053;&#65292;&#29992;&#20110;&#25991;&#26412;&#21644;&#34920;&#26684;&#28151;&#21512;&#38382;&#31572;&#12290;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#25105;&#20204;&#20419;&#20351;&#27169;&#22411;&#22312;&#22788;&#29702;&#28151;&#21512;&#25968;&#25454;&#26102;&#33021;&#22815;&#20855;&#22791;&#26816;&#32034;&#24605;&#32771;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#22312;MultiHiertt&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20840;&#37096;&#30417;&#30563;SOTA&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering numerical questions over hybrid contents from the given tables and text(TextTableQA) is a challenging task. Recently, Large Language Models (LLMs) have gained significant attention in the NLP community. With the emergence of large language models, In-Context Learning and Chain-of-Thought prompting have become two particularly popular research topics in this field. In this paper, we introduce a new prompting strategy called Hybrid prompt strategy and Retrieval of Thought for TextTableQA. Through In-Context Learning, we prompt the model to develop the ability of retrieval thinking when dealing with hybrid data. Our method achieves superior performance compared to the fully-supervised SOTA on the MultiHiertt dataset in the few-shot setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#21477;&#23376;&#23884;&#20837;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#35299;&#30721;&#20102;&#21452;&#20154;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#65292;&#24182;&#21457;&#29616;&#22312;&#20914;&#31361;&#23545;&#35805;&#20013;&#65292;&#22971;&#23376;&#30340;&#24773;&#24863;&#19982;&#35821;&#20041;&#30456;&#20284;&#24615;&#21576;&#27491;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.12646</link><description>&lt;p&gt;
&#22312;&#21452;&#20154;&#23545;&#35805;&#20013;&#35299;&#30721;&#24773;&#24863;&#65306;&#36890;&#36807;&#21477;&#23376;&#23884;&#20837;&#21033;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Decoding Affect in Dyadic Conversations: Leveraging Semantic Similarity through Sentence Embedding. (arXiv:2309.12646v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#21477;&#23376;&#23884;&#20837;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#35299;&#30721;&#20102;&#21452;&#20154;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#65292;&#24182;&#21457;&#29616;&#22312;&#20914;&#31361;&#23545;&#35805;&#20013;&#65292;&#22971;&#23376;&#30340;&#24773;&#24863;&#19982;&#35821;&#20041;&#30456;&#20284;&#24615;&#21576;&#27491;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#31361;&#26174;&#20102;&#21477;&#23376;&#23884;&#20837;&#22312;&#27979;&#37327;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#22312;&#20998;&#26512;&#29616;&#23454;&#20013;&#30340;&#21452;&#20154;&#20114;&#21160;&#24182;&#39044;&#27979;&#23545;&#35805;&#21442;&#19982;&#32773;&#24773;&#24863;&#26041;&#38754;&#30340;&#24212;&#29992;&#20173;&#28982;&#24456;&#23569;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#21033;&#29992;50&#23545;&#22827;&#22971;&#20043;&#38388;&#20851;&#20110;&#20914;&#31361;&#21644;&#24841;&#24555;&#27963;&#21160;&#30340;&#21475;&#22836;&#23545;&#35805;&#12290;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;all-MiniLM-L6-v2&#26469;&#33719;&#24471;&#27599;&#20010;&#21457;&#35328;&#32773;&#35805;&#35821;&#30340;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23884;&#20837;&#30456;&#37051;&#35805;&#35821;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24615;&#30340;&#24179;&#22343;&#20540;&#23545;&#23545;&#35805;&#30340;&#25972;&#20307;&#30456;&#20284;&#24615;&#36827;&#34892;&#37327;&#21270;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35821;&#20041;&#30456;&#20284;&#24615;&#19982;&#22971;&#23376;&#22312;&#20914;&#31361;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#21576;&#27491;&#30456;&#20851;&#65288;&#20294;&#22312;&#24841;&#24555;&#23545;&#35805;&#20013;&#19981;&#30456;&#20851;&#65289;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#23545;&#35805;&#31867;&#22411;&#22914;&#20309;&#65292;&#37117;&#26410;&#35266;&#23519;&#21040;&#36825;&#31181;&#30456;&#20851;&#24615;&#19982;&#19976;&#22827;&#30340;&#24773;&#24863;&#20043;&#38388;&#12290;&#20004;&#20010;&#39564;&#35777;&#26816;&#39564;&#36827;&#19968;&#27493;&#25903;&#25345;&#20102;t
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Natural Language Processing (NLP) have highlighted the potential of sentence embeddings in measuring semantic similarity. Yet, its application in analyzing real-world dyadic interactions and predicting the affect of conversational participants remains largely uncharted. To bridge this gap, the present study utilizes verbal conversations within 50 married couples talking about conflicts and pleasant activities. Transformer-based model all-MiniLM-L6-v2 was employed to obtain the embeddings of the utterances from each speaker. The overall similarity of the conversation was then quantified by the average cosine similarity between the embeddings of adjacent utterances. Results showed that semantic similarity had a positive association with wives' affect during conflict (but not pleasant) conversations. Moreover, this association was not observed with husbands' affect regardless of conversation types. Two validation checks further provided support for the validity of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#31569;&#21512;&#21516;&#39118;&#38505;&#35782;&#21035;&#26041;&#27861;&#65292;&#21033;&#29992;&#20855;&#22791;&#24314;&#31569;&#21512;&#21516;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#19987;&#23478;&#30340;&#21512;&#21516;&#23457;&#26597;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#26080;&#38656;&#35843;&#25972;&#65292;&#33021;&#22815;&#35782;&#21035;&#24314;&#31569;&#21512;&#21516;&#39118;&#38505;&#65292;&#24182;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12626</link><description>&lt;p&gt;
&#22522;&#20110;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#31569;&#21512;&#21516;&#39118;&#38505;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Construction contract risk identification based on knowledge-augmented language model. (arXiv:2309.12626v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#31569;&#21512;&#21516;&#39118;&#38505;&#35782;&#21035;&#26041;&#27861;&#65292;&#21033;&#29992;&#20855;&#22791;&#24314;&#31569;&#21512;&#21516;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#19987;&#23478;&#30340;&#21512;&#21516;&#23457;&#26597;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#26080;&#38656;&#35843;&#25972;&#65292;&#33021;&#22815;&#35782;&#21035;&#24314;&#31569;&#21512;&#21516;&#39118;&#38505;&#65292;&#24182;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24314;&#31569;&#39033;&#30446;&#20013;&#65292;&#21512;&#21516;&#23457;&#26597;&#26159;&#38450;&#27490;&#28508;&#22312;&#25439;&#22833;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#29992;&#20110;&#23457;&#26597;&#24314;&#31569;&#21512;&#21516;&#30340;&#26041;&#27861;&#32570;&#20047;&#25928;&#26524;&#21644;&#21487;&#38752;&#24615;&#65292;&#23548;&#33268;&#32791;&#26102;&#21644;&#23481;&#26131;&#20986;&#38169;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25913;&#38761;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#35299;&#20915;&#19987;&#38376;&#38382;&#39064;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20855;&#22791;&#24314;&#31569;&#21512;&#21516;&#30693;&#35782;&#30340;LLMs&#26469;&#27169;&#25311;&#20154;&#31867;&#19987;&#23478;&#30340;&#21512;&#21516;&#23457;&#26597;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26080;&#35843;&#25972;&#26041;&#27861;&#23558;&#24314;&#31569;&#21512;&#21516;&#39046;&#22495;&#30693;&#35782;&#32467;&#21512;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#35782;&#21035;&#24314;&#31569;&#21512;&#21516;&#39118;&#38505;&#12290;&#22312;&#26500;&#24314;&#39046;&#22495;&#30693;&#35782;&#24211;&#26102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26377;&#21161;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#30340;&#24314;&#31569;&#21512;&#21516;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#24212;&#29992;&#20110;&#24314;&#31569;&#21512;&#21516;&#30340;&#35782;&#21035;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contract review is an essential step in construction projects to prevent potential losses. However, the current methods for reviewing construction contracts lack effectiveness and reliability, leading to time-consuming and error-prone processes. While large language models (LLMs) have shown promise in revolutionizing natural language processing (NLP) tasks, they struggle with domain-specific knowledge and addressing specialized issues. This paper presents a novel approach that leverages LLMs with construction contract knowledge to emulate the process of contract review by human experts. Our tuning-free approach incorporates construction contract domain knowledge to enhance language models for identifying construction contract risks. The use of a natural language when building the domain knowledge base facilitates practical implementation. We evaluated our method on real construction contracts and achieved solid performance. Additionally, we investigated how large language models employ
&lt;/p&gt;</description></item><item><title>DRG-LLaMA&#26159;&#19968;&#20010;&#36890;&#36807;&#22312;&#20020;&#24202;&#31508;&#35760;&#19978;&#32454;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#20303;&#38498;&#24739;&#32773;&#30340;&#35786;&#26029;&#30456;&#20851;&#20998;&#32452;&#39044;&#27979;&#12290;&#22312;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#19978;&#65292;DRG-LLaMA-7B&#30456;&#23545;&#20110;&#20854;&#20182;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#33021;&#22815;&#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#22522;&#26412;DRG&#21644;&#24182;&#21457;&#30151;/&#21512;&#24182;&#30151;&#65288;CC&#65289;/&#37325;&#22823;&#24182;&#21457;&#30151;&#25110;&#21512;&#24182;&#30151;&#65288;MCC&#65289;&#30340;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2309.12625</link><description>&lt;p&gt;
DRG-LLaMA: &#35843;&#20248;LLaMA&#27169;&#22411;&#20197;&#39044;&#27979;&#20303;&#38498;&#24739;&#32773;&#30340;&#35786;&#26029;&#30456;&#20851;&#20998;&#32452;
&lt;/p&gt;
&lt;p&gt;
DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for Hospitalized Patients. (arXiv:2309.12625v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12625
&lt;/p&gt;
&lt;p&gt;
DRG-LLaMA&#26159;&#19968;&#20010;&#36890;&#36807;&#22312;&#20020;&#24202;&#31508;&#35760;&#19978;&#32454;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#20303;&#38498;&#24739;&#32773;&#30340;&#35786;&#26029;&#30456;&#20851;&#20998;&#32452;&#39044;&#27979;&#12290;&#22312;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#19978;&#65292;DRG-LLaMA-7B&#30456;&#23545;&#20110;&#20854;&#20182;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#33021;&#22815;&#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#22522;&#26412;DRG&#21644;&#24182;&#21457;&#30151;/&#21512;&#24182;&#30151;&#65288;CC&#65289;/&#37325;&#22823;&#24182;&#21457;&#30151;&#25110;&#21512;&#24182;&#30151;&#65288;MCC&#65289;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32654;&#22269;&#20303;&#38498;&#20184;&#36153;&#31995;&#32479;&#20013;&#65292;&#35786;&#26029;&#30456;&#20851;&#20998;&#32452;&#65288;DRG&#65289;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#30446;&#21069;&#30340;&#20998;&#32452;&#36807;&#31243;&#32791;&#26102;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DRG-LLaMA&#65292;&#19968;&#20010;&#22312;&#20020;&#24202;&#31508;&#35760;&#19978;&#36827;&#34892;&#32454;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20197;&#25913;&#21892;DRG&#39044;&#27979;&#12290;&#20351;&#29992;Meta&#30340;LLaMA&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;236,192&#20010;MIMIC-IV&#20986;&#38498;&#25688;&#35201;&#19978;&#36827;&#34892;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#20248;&#21270;&#12290;&#22312;&#36755;&#20837;&#20196;&#29260;&#38271;&#24230;&#20026;512&#30340;&#24773;&#20917;&#19979;&#65292;DRG-LLaMA-7B&#23454;&#29616;&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#20026;0.327&#65292;&#39030;&#32423;&#39044;&#27979;&#20934;&#30830;&#24230;&#20026;52.0&#65285;&#65292;&#23439;&#24179;&#22343;AUC&#20026;0.986&#12290;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#65292;DRG-LLaMA-7B&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#20043;&#21069;&#25253;&#36947;&#30340;&#39046;&#20808;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;ClinicalBERT&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#25552;&#39640;&#20102;40.3&#65285;&#65292;&#30456;&#23545;&#20110;CAML&#25552;&#39640;&#20102;35.7&#65285;&#12290;&#24403;&#24212;&#29992;DRG-LLaMA&#26469;&#39044;&#27979;&#22522;&#26412;DRG&#21644;&#24182;&#21457;&#30151;/&#21512;&#24182;&#30151;&#65288;CC&#65289;/&#37325;&#22823;&#24182;&#21457;&#30151;&#25110;&#21512;&#24182;&#30151;&#65288;MCC&#65289;&#26102;&#65292;&#22522;&#26412;DRG&#30340;&#39030;&#32423;&#39044;&#27979;&#20934;&#30830;&#24230;&#36798;&#21040;&#20102;67.8&#65285;&#65292;&#32780;CC/MCC&#29366;&#24577;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#36798;&#21040;&#20102;67.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the U.S. inpatient payment system, the Diagnosis-Related Group (DRG) plays a key role but its current assignment process is time-consuming. We introduce DRG-LLaMA, a large language model (LLM) fine-tuned on clinical notes for improved DRG prediction. Using Meta's LLaMA as the base model, we optimized it with Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge summaries. With an input token length of 512, DRG-LLaMA-7B achieved a macro-averaged F1 score of 0.327, a top-1 prediction accuracy of 52.0% and a macro-averaged Area Under the Curve (AUC) of 0.986. Impressively, DRG-LLaMA-7B surpassed previously reported leading models on this task, demonstrating a relative improvement in macro-averaged F1 score of 40.3% compared to ClinicalBERT and 35.7% compared to CAML. When DRG-LLaMA is applied to predict base DRGs and complication or comorbidity (CC) / major complication or comorbidity (MCC), the top-1 prediction accuracy reached 67.8% for base DRGs and 67.5% for CC/MCC status. DRG-L
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#27169;&#22411;&#65292;&#19968;&#20010;&#25918;&#22823;&#19981;&#33391;&#27169;&#24335;&#65292;&#19968;&#20010;&#22686;&#24378;&#22810;&#26679;&#24615;&#65292;&#26469;&#35299;&#20915;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#20013;&#22810;&#26679;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12619</link><description>&lt;p&gt;
&#36890;&#36807;&#36864;&#21270;&#27169;&#22411;&#23398;&#20064;&#22810;&#26679;&#21270;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning to Diversify Neural Text Generation via Degenerative Model. (arXiv:2309.12619v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12619
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#27169;&#22411;&#65292;&#19968;&#20010;&#25918;&#22823;&#19981;&#33391;&#27169;&#24335;&#65292;&#19968;&#20010;&#22686;&#24378;&#22810;&#26679;&#24615;&#65292;&#26469;&#35299;&#20915;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#20013;&#22810;&#26679;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26412;&#26041;&#38754;&#24448;&#24448;&#22833;&#36133;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#35782;&#21035;&#21644;&#24809;&#32602;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65288;&#20363;&#22914;&#37325;&#22797;&#12289;&#36807;&#24230;&#20351;&#29992;&#39640;&#39057;&#35789;&#27719;&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35266;&#23519;&#32467;&#26524;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#27169;&#22411;&#20027;&#35201;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#36864;&#21270;&#38382;&#39064;&#30340;&#31034;&#20363;&#20013;&#30340;&#23646;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#27169;&#22411;&#26469;&#39044;&#38450;&#36864;&#21270;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#26088;&#22312;&#25918;&#22823;&#19981;&#33391;&#27169;&#24335;&#30340;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#19987;&#27880;&#20110;&#31532;&#19968;&#20010;&#27169;&#22411;&#26410;&#33021;&#23398;&#20064;&#21040;&#30340;&#27169;&#24335;&#26469;&#22686;&#24378;&#31532;&#20108;&#20010;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#12290;&#23545;&#35821;&#35328;&#24314;&#27169;&#21644;&#23545;&#35805;&#29983;&#25104;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural language models often fail to generate diverse and informative texts, limiting their applicability in real-world problems. While previous approaches have proposed to address these issues by identifying and penalizing undesirable behaviors (e.g., repetition, overuse of frequent words) from language models, we propose an alternative approach based on an observation: models primarily learn attributes within examples that are likely to cause degeneration problems. Based on this observation, we propose a new approach to prevent degeneration problems by training two models. Specifically, we first train a model that is designed to amplify undesirable patterns. We then enhance the diversity of the second model by focusing on patterns that the first model fails to learn. Extensive experiments on two tasks, namely language modeling and dialogue generation, demonstrate the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;25&#20010;ML&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#27169;&#22411;&#21345;&#29255;&#12290;&#23454;&#39564;&#21457;&#29616;&#30446;&#21069;&#23384;&#22312;&#30340;&#25351;&#20196;&#27169;&#22411;&#22312;&#30740;&#31350;&#35770;&#25991;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#20934;&#30830;&#25991;&#26412;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.12616</link><description>&lt;p&gt;
&#35299;&#38145;&#27169;&#22411;&#27934;&#23519;&#21147;&#65306;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#27169;&#22411;&#21345;&#29255;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Unlocking Model Insights: A Dataset for Automated Model Card Generation. (arXiv:2309.12616v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12616
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;25&#20010;ML&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#27169;&#22411;&#21345;&#29255;&#12290;&#23454;&#39564;&#21457;&#29616;&#30446;&#21069;&#23384;&#22312;&#30340;&#25351;&#20196;&#27169;&#22411;&#22312;&#30740;&#31350;&#35770;&#25991;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#20934;&#30830;&#25991;&#26412;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#19981;&#20877;&#23616;&#38480;&#20110;&#26426;&#22120;&#23398;&#20064;&#30028;&#65292;&#38024;&#23545;&#25351;&#20196;&#30340;LMs&#24341;&#21457;&#20102;&#33258;&#20027;AI&#20195;&#29702;&#30340;&#20852;&#36215;&#12290;&#38543;&#30528;LMs&#30340;&#21487;&#35775;&#38382;&#24615;&#22686;&#21152;&#65292;&#25552;&#39640;&#23545;&#20854;&#33021;&#21147;&#12289;&#39044;&#26399;&#29992;&#36884;&#21644;&#24320;&#21457;&#21608;&#26399;&#30340;&#29702;&#35299;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#27169;&#22411;&#21345;&#29255;&#26159;&#35760;&#24405;&#20851;&#20110;ML&#27169;&#22411;&#35814;&#32454;&#20449;&#24687;&#30340;&#24120;&#35265;&#23454;&#36341;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#27169;&#22411;&#21345;&#29255;&#30340;&#29983;&#25104;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;25&#20010;ML&#27169;&#22411;&#30340;500&#20010;&#38382;&#31572;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#27169;&#22411;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#22914;&#35757;&#32451;&#37197;&#32622;&#12289;&#25968;&#25454;&#38598;&#12289;&#20559;&#35265;&#12289;&#26550;&#26500;&#32454;&#33410;&#21644;&#35757;&#32451;&#36164;&#28304;&#12290;&#25105;&#20204;&#38599;&#20323;&#27880;&#37322;&#32773;&#20174;&#21407;&#22987;&#35770;&#25991;&#20013;&#25552;&#21462;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#22238;&#31572;&#38382;&#39064;&#26469;&#25506;&#32034;LMs&#22312;&#29983;&#25104;&#27169;&#22411;&#21345;&#29255;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;ChatGPT-3.5&#12289;LLaMa&#21644;Galactica&#30340;&#21021;&#27493;&#23454;&#39564;&#26174;&#31034;&#20986;&#36825;&#20123;LMs&#23545;&#30740;&#31350;&#35770;&#25991;&#30340;&#29702;&#35299;&#20197;&#21450;&#29983;&#25104;&#20107;&#23454;&#24615;&#25991;&#26412;&#21709;&#24212;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are no longer restricted to ML community, and instruction-tuned LMs have led to a rise in autonomous AI agents. As the accessibility of LMs grows, it is imperative that an understanding of their capabilities, intended usage, and development cycle also improves. Model cards are a popular practice for documenting detailed information about an ML model. To automate model card generation, we introduce a dataset of 500 question-answer pairs for 25 ML models that cover crucial aspects of the model, such as its training configurations, datasets, biases, architecture details, and training resources. We employ annotators to extract the answers from the original paper. Further, we explore the capabilities of LMs in generating model cards by answering questions. Our initial experiments with ChatGPT-3.5, LLaMa, and Galactica showcase a significant gap in the understanding of research papers by these aforementioned LMs as well as generating factual textual responses. We posit 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21327;&#21161;&#19987;&#19994;&#20316;&#23478;&#26041;&#38754;&#30340;&#25928;&#29992;&#65292;&#24182;&#21457;&#29616;&#20316;&#23478;&#20204;&#26356;&#20542;&#21521;&#20110;&#22312;&#32763;&#35793;&#21644;&#23457;&#26597;&#38454;&#27573;&#20013;&#23547;&#27714;LLM&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2309.12570</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#30340;&#21019;&#36896;&#21147;&#25903;&#25345;: &#19968;&#39033;&#28041;&#21450;&#26032;&#20852;&#20316;&#23478;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers. (arXiv:2309.12570v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21327;&#21161;&#19987;&#19994;&#20316;&#23478;&#26041;&#38754;&#30340;&#25928;&#29992;&#65292;&#24182;&#21457;&#29616;&#20316;&#23478;&#20204;&#26356;&#20542;&#21521;&#20110;&#22312;&#32763;&#35793;&#21644;&#23457;&#26597;&#38454;&#27573;&#20013;&#23547;&#27714;LLM&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#20854;&#33021;&#22815;&#36981;&#24490;&#25351;&#20196;&#24182;&#21442;&#19982;&#23545;&#35805;&#20114;&#21160;&#65292;&#24341;&#21457;&#20102;&#22312;&#21508;&#31181;&#25903;&#25345;&#24037;&#20855;&#20013;&#21033;&#29992;&#23427;&#20204;&#30340;&#20852;&#36259;&#22686;&#21152;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#23454;&#35777;&#29992;&#25143;&#30740;&#31350;&#65288;n=30&#65289;&#25506;&#35752;&#20102;&#29616;&#20195;LLM&#22312;&#21327;&#21161;&#19987;&#19994;&#20316;&#23478;&#26041;&#38754;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#30340;&#21512;&#20316;&#20889;&#20316;&#30028;&#38754;&#35774;&#35745;&#22522;&#20110;&#23558;&#20889;&#20316;&#35270;&#20026;&#19968;&#20010;&#30446;&#26631;&#23548;&#21521;&#30340;&#24605;&#32500;&#36807;&#31243;&#30340;&#35748;&#30693;&#36807;&#31243;&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#38750;&#32447;&#24615;&#30340;&#35748;&#30693;&#27963;&#21160;&#65306;&#35268;&#21010;&#12289;&#32763;&#35793;&#21644;&#23457;&#26597;&#12290;&#21442;&#19982;&#32773;&#34987;&#35201;&#27714;&#25552;&#20132;&#19968;&#20221;&#21518;&#23436;&#25104;&#35843;&#26597;&#65292;&#20197;&#25552;&#20379;&#20851;&#20110;LLM&#20316;&#20026;&#20889;&#20316;&#21512;&#20316;&#32773;&#28508;&#21147;&#21644;&#38382;&#39064;&#30340;&#21453;&#39304;&#12290;&#36890;&#36807;&#20998;&#26512;&#20316;&#23478;-LLM&#20114;&#21160;,&#25105;&#20204;&#21457;&#29616;&#20316;&#23478;&#22312;&#19977;&#31181;&#31867;&#22411;&#30340;&#35748;&#30693;&#27963;&#21160;&#20013;&#37117;&#23547;&#27714;LLM&#30340;&#24110;&#21161;&#65292;&#20294;&#20182;&#20204;&#21457;&#29616;LLM&#22312;&#32763;&#35793;&#21644;&#23457;&#26597;&#26041;&#38754;&#26356;&#26377;&#24110;&#21161;&#12290;&#36890;&#36807;&#20998;&#26512;&#20114;&#21160;&#21644;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) capable of following instructions and engaging in conversational interactions sparked increased interest in their utilization across various support tools. We investigate the utility of modern LLMs in assisting professional writers via an empirical user study (n=30). The design of our collaborative writing interface is grounded in the cognitive process model of writing that views writing as a goal-oriented thinking process encompassing non-linear cognitive activities: planning, translating, and reviewing. Participants are asked to submit a post-completion survey to provide feedback on the potential and pitfalls of LLMs as writing collaborators. Upon analyzing the writer-LLM interactions, we find that while writers seek LLM's help across all three types of cognitive activities, they find LLMs more helpful in translation and reviewing. Our findings from analyzing both the interactions and the survey responses highlight future research direc
&lt;/p&gt;</description></item><item><title>PlanFitting&#26159;&#19968;&#20010;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#24110;&#21161;&#29992;&#25143;&#23450;&#21046;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#65292;&#24182;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#23427;&#29983;&#25104;&#20010;&#24615;&#21270;&#12289;&#21487;&#25805;&#20316;&#21644;&#26377;&#25454;&#21487;&#20381;&#30340;&#36816;&#21160;&#35745;&#21010;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12555</link><description>&lt;p&gt;
PlanFitting&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23450;&#21046;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
PlanFitting: Tailoring Personalized Exercise Plans with Large Language Models. (arXiv:2309.12555v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12555
&lt;/p&gt;
&lt;p&gt;
PlanFitting&#26159;&#19968;&#20010;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#24110;&#21161;&#29992;&#25143;&#23450;&#21046;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#65292;&#24182;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#23427;&#29983;&#25104;&#20010;&#24615;&#21270;&#12289;&#21487;&#25805;&#20316;&#21644;&#26377;&#25454;&#21487;&#20381;&#30340;&#36816;&#21160;&#35745;&#21010;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#23545;&#20110;&#30830;&#20445;&#36275;&#22815;&#30340;&#20307;&#32946;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#20154;&#20204;&#30340;&#22797;&#26434;&#26085;&#31243;&#21644;&#32771;&#34385;&#22240;&#32032;&#20197;&#21450;&#35745;&#21010;&#30340;&#21019;&#24314;&#36890;&#24120;&#38656;&#35201;&#19982;&#19987;&#23478;&#30340;&#21453;&#22797;&#27807;&#36890;&#65292;&#36825;&#19968;&#36807;&#31243;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PlanFitting&#65292;&#23427;&#26159;&#19968;&#20010;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#21487;&#20197;&#36741;&#21161;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;PlanFitting&#20351;&#29992;&#25143;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21508;&#31181;&#32422;&#26463;&#21644;&#26597;&#35810;&#65292;&#20174;&#32780;&#20415;&#20110;&#21019;&#24314;&#21644;&#20248;&#21270;&#36866;&#21512;&#20854;&#29305;&#23450;&#24773;&#20917;&#30340;&#27599;&#21608;&#36816;&#21160;&#35745;&#21010;&#65292;&#24182;&#20445;&#25345;&#22522;&#26412;&#21407;&#21017;&#30340;&#25166;&#26681;&#12290;&#36890;&#36807;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#21442;&#19982;&#32773;&#65288;N=18&#65289;&#20351;&#29992;PlanFitting&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#65292;&#32780;&#19987;&#23478;&#35268;&#21010;&#32773;&#65288;N=3&#65289;&#23545;&#36825;&#20123;&#35745;&#21010;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;PlanFitting&#22312;&#29983;&#25104;&#20010;&#24615;&#21270;&#12289;&#21487;&#25805;&#20316;&#21644;&#26377;&#25454;&#21487;&#20381;&#30340;&#36816;&#21160;&#35745;&#21010;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;AI&#21161;&#25163;&#22312;&#21019;&#24314;&#35745;&#21010;&#26041;&#38754;&#30340;&#26410;&#26469;&#35774;&#35745;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
A personally tailored exercise regimen is crucial to ensuring sufficient physical activities, yet challenging to create as people have complex schedules and considerations and the creation of plans often requires iterations with experts. We present PlanFitting, a conversational AI that assists in personalized exercise planning. Leveraging generative capabilities of large language models, PlanFitting enables users to describe various constraints and queries in natural language, thereby facilitating the creation and refinement of their weekly exercise plan to suit their specific circumstances while staying grounded in foundational principles. Through a user study where participants (N=18) generated a personalized exercise plan using PlanFitting and expert planners (N=3) evaluated these plans, we identified the potential of PlanFitting in generating personalized, actionable, and evidence-based exercise plans. We discuss future design opportunities for AI assistants in creating plans that 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#38382;&#21477;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#30340;&#38382;&#39064;&#26159;&#21542;&#21487;&#20197;&#30001;&#21442;&#32771;&#31572;&#26696;&#22238;&#31572;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#25351;&#26631;&#32467;&#26524;&#21487;&#38752;&#65292;&#24182;&#19982;&#20154;&#24037;&#35780;&#20215;&#19968;&#33268;&#12290;&#24182;&#19988;&#36825;&#20010;&#25351;&#26631;&#21487;&#20197;&#34917;&#20805;&#20256;&#32479;&#30340;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2309.12546</link><description>&lt;p&gt;
&#38382;&#21477;&#29983;&#25104;&#30340;&#33258;&#21160;&#22238;&#31572;&#21487;&#34892;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Automatic Answerability Evaluation for Question Generation. (arXiv:2309.12546v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12546
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#38382;&#21477;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#30340;&#38382;&#39064;&#26159;&#21542;&#21487;&#20197;&#30001;&#21442;&#32771;&#31572;&#26696;&#22238;&#31572;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#25351;&#26631;&#32467;&#26524;&#21487;&#38752;&#65292;&#24182;&#19982;&#20154;&#24037;&#35780;&#20215;&#19968;&#33268;&#12290;&#24182;&#19988;&#36825;&#20010;&#25351;&#26631;&#21487;&#20197;&#34917;&#20805;&#20256;&#32479;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#22914;BLEU&#21644;ROUGE&#65292;&#26159;&#20026;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#24320;&#21457;&#30340;&#65292;&#23427;&#20204;&#22522;&#20110;&#29983;&#25104;&#25991;&#26412;&#19982;&#21442;&#32771;&#25991;&#26412;&#20043;&#38388;&#30340;n-gram&#37325;&#21472;&#24230;&#26469;&#34913;&#37327;&#12290;&#36825;&#20123;&#31616;&#21333;&#30340;&#35780;&#20272;&#25351;&#26631;&#21487;&#33021;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#27604;&#22914;&#38382;&#21477;&#29983;&#25104;&#65288;QG&#65289;&#65292;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#22240;&#20026;QG&#38656;&#35201;&#29983;&#25104;&#21487;&#20197;&#30001;&#21442;&#32771;&#31572;&#26696;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26356;&#22797;&#26434;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#20173;&#28982;&#26159;QG&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#32039;&#36843;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#21487;&#22238;&#31572;&#24615;&#24230;&#37327;&#65288;PMAN&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#38382;&#21477;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#30340;&#38382;&#39064;&#26159;&#21542;&#21487;&#20197;&#30001;&#21442;&#32771;&#31572;&#26696;&#22238;&#31572;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#35780;&#20272;&#32467;&#26524;&#21487;&#38752;&#65292;&#24182;&#19982;&#20154;&#24037;&#35780;&#20215;&#19968;&#33268;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24212;&#29992;&#25105;&#20204;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;QG&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#25351;&#26631;&#34917;&#20805;&#20102;&#20256;&#32479;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#22522;&#20110;ChatGPT&#30340;QG&#27169;&#22411;&#30340;&#23454;&#29616;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional automatic evaluation metrics, such as BLEU and ROUGE, developed for natural language generation (NLG) tasks, are based on measuring the n-gram overlap between the generated and reference text. These simple metrics may be insufficient for more complex tasks, such as question generation (QG), which requires generating questions that are answerable by the reference answers. Developing a more sophisticated automatic evaluation metric, thus, remains as an urgent problem in QG research. This work proposes a Prompting-based Metric on ANswerability (PMAN), a novel automatic evaluation metric to assess whether the generated questions are answerable by the reference answers for the QG tasks. Extensive experiments demonstrate that its evaluation results are reliable and align with human evaluations. We further apply our metric to evaluate the performance of QG models, which shows our metric complements conventional metrics. Our implementation of a ChatGPT-based QG model achieves stat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#30740;&#31350;&#29366;&#24577;&#65292;&#20171;&#32461;&#20102;&#20004;&#20010;&#20027;&#35201;&#20998;&#25903;&#65306;&#22522;&#20110;&#36317;&#31163;&#21644;&#22522;&#20110;&#35821;&#20041;&#21305;&#37197;&#30340;&#26041;&#27861;&#12290;&#36824;&#35752;&#35770;&#20102;CompoundE&#21644;CompoundE3D&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#30740;&#31350;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.12501</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embedding: An Overview. (arXiv:2309.12501v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12501
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#30740;&#31350;&#29366;&#24577;&#65292;&#20171;&#32461;&#20102;&#20004;&#20010;&#20027;&#35201;&#20998;&#25903;&#65306;&#22522;&#20110;&#36317;&#31163;&#21644;&#22522;&#20110;&#35821;&#20041;&#21305;&#37197;&#30340;&#26041;&#27861;&#12290;&#36824;&#35752;&#35770;&#20102;CompoundE&#21644;CompoundE3D&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#30740;&#31350;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25968;&#23398;&#27169;&#22411;&#24050;&#34987;&#21033;&#29992;&#26469;&#35774;&#35745;&#23884;&#20837;&#65292;&#20197;&#34920;&#31034;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#21644;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#20123;&#25968;&#23398;&#21551;&#21457;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#22823;&#22411;KG&#20013;&#36827;&#34892;&#25512;&#29702;&#26102;&#39640;&#24230;&#21487;&#25193;&#23637;&#65292;&#32780;&#19988;&#22312;&#24314;&#27169;&#19981;&#21516;&#20851;&#31995;&#27169;&#24335;&#26041;&#38754;&#20855;&#26377;&#24456;&#22810;&#21487;&#35299;&#37322;&#30340;&#20248;&#21183;&#65292;&#36825;&#20123;&#20248;&#21183;&#21487;&#20197;&#36890;&#36807;&#24418;&#24335;&#21270;&#35777;&#26126;&#21644;&#32463;&#39564;&#32467;&#26524;&#26469;&#39564;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;KG&#23436;&#25104;&#39046;&#22495;&#30340;&#24403;&#21069;&#30740;&#31350;&#29366;&#24577;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30528;&#37325;&#20171;&#32461;&#20102;KG&#23884;&#20837;&#65288;KGE&#65289;&#35774;&#35745;&#30340;&#20004;&#20010;&#20027;&#35201;&#20998;&#25903;&#65306;1&#65289;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#21644;2&#65289;&#22522;&#20110;&#35821;&#20041;&#21305;&#37197;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#36235;&#21183;&#65292;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#26126;&#26032;&#39062;&#19988;&#26356;&#26377;&#25928;&#30340;&#27169;&#22411;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20174;2D&#21644;3D&#20223;&#23556;&#25805;&#20316;&#20013;&#27762;&#21462;&#28789;&#24863;&#30340;CompoundE&#21644;CompoundE3D&#12290;&#23427;&#20204;&#28085;&#30422;&#20102;&#21253;&#25324;dis&#22312;&#20869;&#30340;&#24191;&#27867;&#25216;&#26415;&#35859;&#35789;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many mathematical models have been leveraged to design embeddings for representing Knowledge Graph (KG) entities and relations for link prediction and many downstream tasks. These mathematically-inspired models are not only highly scalable for inference in large KGs, but also have many explainable advantages in modeling different relation patterns that can be validated through both formal proofs and empirical results. In this paper, we make a comprehensive overview of the current state of research in KG completion. In particular, we focus on two main branches of KG embedding (KGE) design: 1) distance-based methods and 2) semantic matching-based methods. We discover the connections between recently proposed models and present an underlying trend that might help researchers invent novel and more effective models. Next, we delve into CompoundE and CompoundE3D, which draw inspiration from 2D and 3D affine operations, respectively. They encompass a broad spectrum of techniques including dis
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#21644;&#23376;&#35789;&#26631;&#35760;&#23545;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#26159;&#23548;&#33268;&#24615;&#21035;&#20559;&#35265;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#32780;&#23376;&#35789;&#25286;&#20998;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36890;&#36807;&#20998;&#26512;&#23376;&#35789;&#25286;&#20998;&#21487;&#20197;&#24456;&#22909;&#22320;&#20272;&#35745;&#35757;&#32451;&#25968;&#25454;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20165;&#24494;&#35843;&#26631;&#35760;&#23884;&#20837;&#23618;&#21487;&#20197;&#20943;&#23569;&#22899;&#24615;&#21644;&#30007;&#24615;&#20043;&#38388;&#24615;&#21035;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.12491</link><description>&lt;p&gt;
&#25506;&#32034;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#21644;&#23376;&#35789;&#26631;&#35760;&#23545;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of Training Data Distribution and Subword Tokenization on Gender Bias in Machine Translation. (arXiv:2309.12491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12491
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#21644;&#23376;&#35789;&#26631;&#35760;&#23545;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#26159;&#23548;&#33268;&#24615;&#21035;&#20559;&#35265;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#32780;&#23376;&#35789;&#25286;&#20998;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36890;&#36807;&#20998;&#26512;&#23376;&#35789;&#25286;&#20998;&#21487;&#20197;&#24456;&#22909;&#22320;&#20272;&#35745;&#35757;&#32451;&#25968;&#25454;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20165;&#24494;&#35843;&#26631;&#35760;&#23884;&#20837;&#23618;&#21487;&#20197;&#20943;&#23569;&#22899;&#24615;&#21644;&#30007;&#24615;&#20043;&#38388;&#24615;&#21035;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26631;&#35760;&#21270;&#23545;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#36825;&#26159;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#34987;&#22823;&#22810;&#25968;&#20154;&#24573;&#35270;&#30340;&#19968;&#20010;&#26041;&#38754;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#35757;&#32451;&#25968;&#25454;&#20013;&#24615;&#21035;&#32844;&#19994;&#21517;&#31216;&#30340;&#39057;&#29575;&#12289;&#23427;&#20204;&#22312;&#23376;&#35789;&#26631;&#35760;&#22120;&#35789;&#27719;&#34920;&#20013;&#30340;&#34920;&#31034;&#20197;&#21450;&#24615;&#21035;&#20559;&#35265;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22899;&#24615;&#21644;&#38750;&#21051;&#26495;&#21360;&#35937;&#30340;&#24615;&#21035;&#32844;&#19994;&#21517;&#31216;&#30340;&#21464;&#24418;&#65288;&#20363;&#22914;&#65292;&#35199;&#29677;&#29273;&#35821;&#20013;&#30340;"doctora"&#34920;&#31034;"&#22899;&#21307;&#29983;"&#65289;&#24448;&#24448;&#34987;&#25286;&#20998;&#25104;&#22810;&#20010;&#23376;&#35789;&#26631;&#35760;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#26159;&#23548;&#33268;&#24615;&#21035;&#20559;&#35265;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#20854;&#24433;&#21709;&#22823;&#20110;&#23376;&#35789;&#25286;&#20998;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#26512;&#23376;&#35789;&#25286;&#20998;&#21487;&#20197;&#24456;&#22909;&#22320;&#20272;&#35745;&#35757;&#32451;&#25968;&#25454;&#20013;&#24615;&#21035;&#24418;&#24335;&#30340;&#19981;&#24179;&#34913;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#35821;&#26009;&#24211;&#19981;&#20844;&#24320;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#20165;&#24494;&#35843;&#26631;&#35760;&#23884;&#20837;&#23618;&#21487;&#20197;&#20943;&#23569;&#22899;&#24615;&#21644;&#30007;&#24615;&#20043;&#38388;&#24615;&#21035;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the effect of tokenization on gender bias in machine translation, an aspect that has been largely overlooked in previous works. Specifically, we focus on the interactions between the frequency of gendered profession names in training data, their representation in the subword tokenizer's vocabulary, and gender bias. We observe that female and non-stereotypical gender inflections of profession names (e.g., Spanish "doctora" for "female doctor") tend to be split into multiple subword tokens. Our results indicate that the imbalance of gender forms in the model's training corpus is a major factor contributing to gender bias and has a greater impact than subword splitting. We show that analyzing subword splits provides good estimates of gender-form imbalance in the training data and can be used even when the corpus is not publicly available. We also demonstrate that fine-tuning just the token embedding layer can decrease the gap in gender prediction accuracy between female and male 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#65292;&#21457;&#29616;LLM&#22312;&#25512;&#29702;&#20013;&#23384;&#22312;&#31867;&#20284;&#20110;&#20154;&#31867;&#21551;&#21457;&#24335;&#25512;&#29702;&#30340;&#38169;&#35823;&#65292;&#20294;&#19982;&#20154;&#31867;&#25512;&#29702;&#26377;&#37325;&#35201;&#24046;&#24322;&#65292;&#26368;&#26032;&#30340;LLM&#29256;&#26412;&#20960;&#20046;&#28040;&#38500;&#20102;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#21644;&#26426;&#22120;&#23545;&#30456;&#21516;&#30340;&#25552;&#31034;&#26041;&#26696;&#30340;&#21453;&#24212;&#19981;&#21516;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#25105;&#20204;&#30340;&#35748;&#35782;&#35770;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.12485</link><description>&lt;p&gt;
&#30740;&#31350;&#21644;&#25913;&#36827;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Studying and improving reasoning in humans and machines. (arXiv:2309.12485v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#65292;&#21457;&#29616;LLM&#22312;&#25512;&#29702;&#20013;&#23384;&#22312;&#31867;&#20284;&#20110;&#20154;&#31867;&#21551;&#21457;&#24335;&#25512;&#29702;&#30340;&#38169;&#35823;&#65292;&#20294;&#19982;&#20154;&#31867;&#25512;&#29702;&#26377;&#37325;&#35201;&#24046;&#24322;&#65292;&#26368;&#26032;&#30340;LLM&#29256;&#26412;&#20960;&#20046;&#28040;&#38500;&#20102;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#21644;&#26426;&#22120;&#23545;&#30456;&#21516;&#30340;&#25552;&#31034;&#26041;&#26696;&#30340;&#21453;&#24212;&#19981;&#21516;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#25105;&#20204;&#30340;&#35748;&#35782;&#35770;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20256;&#32479;&#29992;&#20110;&#30740;&#31350;&#65288;&#26377;&#38480;&#65289;&#29702;&#24615;&#30340;&#35748;&#30693;&#24515;&#29702;&#23398;&#24037;&#20855;&#65292;&#30740;&#31350;&#21644;&#27604;&#36739;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21521;&#20154;&#31867;&#21442;&#19982;&#32773;&#21644;&#19968;&#31995;&#21015;&#39044;&#35757;&#32451;&#30340;LLM&#21576;&#29616;&#20102;&#26032;&#30340;&#32463;&#20856;&#35748;&#30693;&#23454;&#39564;&#30340;&#21464;&#20307;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#20132;&#21449;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#21576;&#29616;&#20986;&#31867;&#20284;&#20110;&#24120;&#35265;&#30340;&#38169;&#35823;&#20542;&#21521;&#20110;&#21551;&#21457;&#24335;&#20154;&#31867;&#25512;&#29702;&#30340;&#25512;&#29702;&#38169;&#35823;&#12290;&#23613;&#31649;&#26377;&#36825;&#31181;&#34920;&#38754;&#19978;&#30340;&#30456;&#20284;&#24615;&#65292;&#20154;&#31867;&#21644;LLM&#20043;&#38388;&#30340;&#28145;&#20837;&#27604;&#36739;&#34920;&#26126;&#20102;&#20154;&#31867;&#26679;&#24335;&#25512;&#29702;&#30340;&#37325;&#35201;&#24046;&#24322;&#65292;&#38543;&#30528;&#26368;&#36817;LLM&#29256;&#26412;&#30340;&#25512;&#20986;&#65292;&#27169;&#22411;&#30340;&#38480;&#21046;&#20960;&#20046;&#23436;&#20840;&#28040;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20986;&#65292;&#34429;&#28982;&#21487;&#33021;&#21046;&#23450;&#31574;&#30053;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#20294;&#20154;&#31867;&#21644;&#26426;&#22120;&#23545;&#30456;&#21516;&#30340;&#25552;&#31034;&#26041;&#26696;&#30340;&#21453;&#24212;&#24182;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#36890;&#36807;&#35752;&#35770;&#36825;&#19968;&#35748;&#35782;&#35770;&#30340;&#24433;&#21709;&#26469;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the present study, we investigate and compare reasoning in large language models (LLM) and humans using a selection of cognitive psychology tools traditionally dedicated to the study of (bounded) rationality. To do so, we presented to human participants and an array of pretrained LLMs new variants of classical cognitive experiments, and cross-compared their performances. Our results showed that most of the included models presented reasoning errors akin to those frequently ascribed to error-prone, heuristic-based human reasoning. Notwithstanding this superficial similarity, an in-depth comparison between humans and LLMs indicated important differences with human-like reasoning, with models limitations disappearing almost entirely in more recent LLMs releases. Moreover, we show that while it is possible to devise strategies to induce better performance, humans and machines are not equally-responsive to the same prompting schemes. We conclude by discussing the epistemological implicat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;It-LLMs&#65289;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQ&#65289;&#30340;&#40065;&#26834;&#24615;&#33021;&#21147;&#65292;&#22312;&#36873;&#25321;&#39034;&#24207;&#21464;&#21160;&#26102;&#25581;&#31034;&#20102;&#36873;&#25321;&#20559;&#35265;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12481</link><description>&lt;p&gt;
HANS&#65292;&#20320;&#32874;&#26126;&#21527;&#65311;&#31070;&#32463;&#31995;&#32479;&#30340;Clever Hans&#25928;&#24212;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
HANS, are you clever? Clever Hans Effect Analysis of Neural Systems. (arXiv:2309.12481v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12481
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;It-LLMs&#65289;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQ&#65289;&#30340;&#40065;&#26834;&#24615;&#33021;&#21147;&#65292;&#22312;&#36873;&#25321;&#39034;&#24207;&#21464;&#21160;&#26102;&#25581;&#31034;&#20102;&#36873;&#25321;&#20559;&#35265;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(It-LLMs)&#23637;&#31034;&#20986;&#20102;&#22312;&#35748;&#30693;&#29366;&#24577;&#12289;&#24847;&#22270;&#21644;&#21453;&#24212;&#26041;&#38754;&#25512;&#29702;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#21487;&#20197;&#35753;&#20154;&#20204;&#26377;&#25928;&#22320;&#24341;&#23548;&#21644;&#29702;&#35299;&#26085;&#24120;&#31038;&#20132;&#20114;&#21160;&#12290;&#20107;&#23454;&#19978;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#20010;&#22810;&#39033;&#36873;&#25321;&#39064;(MCQ)&#22522;&#20934;&#26469;&#26500;&#24314;&#23545;&#27169;&#22411;&#33021;&#21147;&#30340;&#30830;&#20999;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#26089;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;It-LLMs&#20013;&#23384;&#22312;&#22266;&#26377;&#30340;&#8220;&#39034;&#24207;&#20559;&#35265;&#8221;&#65292;&#32473;&#36866;&#24403;&#30340;&#35780;&#20272;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22235;&#20010;MCQ&#22522;&#20934;&#23545;It-LLMs&#30340;&#25269;&#25239;&#33021;&#21147;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#36873;&#25321;&#39034;&#24207;&#21464;&#21160;&#26102;&#65292;&#25581;&#31034;&#20102;&#36873;&#25321;&#20559;&#35265;&#24182;&#24341;&#21457;&#20102;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#35752;&#35770;&#12290;&#36890;&#36807;&#31532;&#19968;&#20301;&#32622;&#21644;&#27169;&#22411;&#36873;&#25321;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#20551;&#35774;&#22312;&#27169;&#22411;&#20013;&#23384;&#22312;&#32467;&#26500;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Large Language Models (It-LLMs) have been exhibiting outstanding abilities to reason around cognitive states, intentions, and reactions of all people involved, letting humans guide and comprehend day-to-day social interactions effectively. In fact, several multiple-choice questions (MCQ) benchmarks have been proposed to construct solid assessments of the models' abilities. However, earlier works are demonstrating the presence of inherent "order bias" in It-LLMs, posing challenges to the appropriate evaluation. In this paper, we investigate It-LLMs' resilience abilities towards a series of probing tests using four MCQ benchmarks. Introducing adversarial examples, we show a significant performance gap, mainly when varying the order of the choices, which reveals a selection bias and brings into discussion reasoning abilities. Following a correlation between first positions and model choices due to positional bias, we hypothesized the presence of structural heuristics in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#19982;&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#20132;&#20114;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#31934;&#32454;&#25968;&#25454;&#21512;&#25104;&#21644;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#65288;GlassLLaVA&#65289;&#33021;&#22815;&#20934;&#30830;&#35299;&#37322;&#12289;&#35782;&#21035;&#20851;&#38190;&#29305;&#24449;&#21644;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#30340;SEM&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#31185;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#28789;&#27963;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2309.12460</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#31185;&#23398;&#25104;&#20687;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Multimodal Deep Learning for Scientific Imaging Interpretation. (arXiv:2309.12460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#19982;&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#20132;&#20114;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#31934;&#32454;&#25968;&#25454;&#21512;&#25104;&#21644;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#65288;GlassLLaVA&#65289;&#33021;&#22815;&#20934;&#30830;&#35299;&#37322;&#12289;&#35782;&#21035;&#20851;&#38190;&#29305;&#24449;&#21644;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#30340;SEM&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#31185;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#28789;&#27963;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#25104;&#20687;&#39046;&#22495;&#65292;&#35299;&#37322;&#35270;&#35273;&#25968;&#25454;&#24120;&#24120;&#38656;&#35201;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;&#23545;&#20027;&#39064;&#26448;&#26009;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#22797;&#26434;&#32452;&#21512;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#27169;&#25311;&#24182;&#35780;&#20272;&#19982;&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#65288;SEM&#65289;&#22270;&#20687;&#30340;&#20154;&#31867;&#20132;&#20114;&#65292;&#29305;&#21035;&#26159;&#29627;&#29827;&#26448;&#26009;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20174;&#21516;&#34892;&#35780;&#35758;&#30340;&#25991;&#31456;&#20013;&#25910;&#38598;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#65292;&#36827;&#19968;&#27493;&#20511;&#21161; GPT-4 &#30340;&#33021;&#21147;&#36827;&#34892;&#31934;&#32454;&#25968;&#25454;&#21512;&#25104;&#21644;&#35780;&#20272;&#12290;&#23613;&#31649;&#23384;&#22312;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#32454;&#24494;&#30340;&#35299;&#37322;&#21644;&#19987;&#19994;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#27169;&#22411;&#65288;GlassLLaVA&#65289;&#22312;&#21046;&#23450;&#20934;&#30830;&#30340;&#35299;&#37322;&#12289;&#35782;&#21035;&#20851;&#38190;&#29305;&#24449;&#21644;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#30340;SEM&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#31185;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#28789;&#27963;&#35780;&#20272;&#25351;&#26631;&#65292;&#20351;&#24471;&#36827;&#34892;&#32508;&#21512;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the domain of scientific imaging, interpreting visual data often demands an intricate combination of human expertise and deep comprehension of the subject materials. This study presents a novel methodology to linguistically emulate and subsequently evaluate human-like interactions with Scanning Electron Microscopy (SEM) images, specifically of glass materials. Leveraging a multimodal deep learning framework, our approach distills insights from both textual and visual data harvested from peer-reviewed articles, further augmented by the capabilities of GPT-4 for refined data synthesis and evaluation. Despite inherent challenges--such as nuanced interpretations and the limited availability of specialized datasets--our model (GlassLLaVA) excels in crafting accurate interpretations, identifying key features, and detecting defects in previously unseen SEM images. Moreover, we introduce versatile evaluation metrics, suitable for an array of scientific imaging applications, which allows for
&lt;/p&gt;</description></item><item><title>LongDocFACTScore&#26159;&#19968;&#31181;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#23454;&#35777;&#24615;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#26080;&#27861;&#35780;&#20272;&#38271;&#25991;&#26723;&#25688;&#35201;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12455</link><description>&lt;p&gt;
LongDocFACTScore: &#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#30340;&#23454;&#35777;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LongDocFACTScore: Evaluating the Factuality of Long Document Abstractive Summarisation. (arXiv:2309.12455v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12455
&lt;/p&gt;
&lt;p&gt;
LongDocFACTScore&#26159;&#19968;&#31181;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#23454;&#35777;&#24615;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#26080;&#27861;&#35780;&#20272;&#38271;&#25991;&#26723;&#25688;&#35201;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25345;&#20107;&#23454;&#19968;&#33268;&#24615;&#26159;&#29983;&#25104;&#24615;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#25688;&#35201;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65288;&#22914;ROUGE&#24471;&#20998;&#65289;&#26080;&#27861;&#35780;&#20272;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#27979;&#37327;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#25913;&#36827;&#24230;&#37327;&#26631;&#20934;&#65292;&#20294;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#26377;&#38480;&#21046;&#24615;&#30340;&#20196;&#29260;&#38480;&#21046;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#26377;&#38480;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#29616;&#26377;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#22312;&#24212;&#29992;&#20110;&#38271;&#25991;&#26723;&#25968;&#25454;&#38598;&#26102;&#26159;&#21542;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#22312;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#21151;&#25928;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;LongDocFACTScore&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#24230;&#37327;&#26631;&#20934;&#25193;&#23637;&#21040;&#20219;&#24847;&#38271;&#24230;&#30340;&#25991;&#26723;&#12290;&#35813;&#26694;&#26550;&#22312;&#19982;&#20154;&#31867;&#20107;&#23454;&#19968;&#33268;&#24615;&#24230;&#37327;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maintaining factual consistency is a critical issue in abstractive text summarisation, however, it cannot be assessed by traditional automatic metrics used for evaluating text summarisation, such as ROUGE scoring. Recent efforts have been devoted to developing improved metrics for measuring factual consistency using pre-trained language models, but these metrics have restrictive token limits, and are therefore not suitable for evaluating long document text summarisation. Moreover, there is limited research evaluating whether existing automatic evaluation metrics are fit for purpose when applied to long document data sets. In this work, we evaluate the efficacy of automatic metrics at assessing factual consistency in long document text summarisation and propose a new evaluation framework LongDocFACTScore. This framework allows metrics to be extended to any length document. This framework outperforms existing state-of-the-art metrics in its ability to correlate with human measures of fac
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#21307;&#30103;&#23545;&#35805;&#27169;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#29616;&#26377;&#25351;&#26631;&#23545;&#21307;&#23398;&#21644;&#20581;&#24247;&#27010;&#24565;&#30340;&#29702;&#35299;&#19981;&#36275;&#20197;&#21450;&#24573;&#30053;&#20102;&#29992;&#25143;&#20307;&#39564;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.12444</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;AI&#30340;&#21307;&#30103;&#23545;&#35805;&#25928;&#26524;&#30340;&#37327;&#21270;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI. (arXiv:2309.12444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12444
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#21307;&#30103;&#23545;&#35805;&#27169;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#29616;&#26377;&#25351;&#26631;&#23545;&#21307;&#23398;&#21644;&#20581;&#24247;&#27010;&#24565;&#30340;&#29702;&#35299;&#19981;&#36275;&#20197;&#21450;&#24573;&#30053;&#20102;&#29992;&#25143;&#20307;&#39564;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#23558;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24739;&#32773;&#25252;&#29702;&#36716;&#21464;&#20026;&#26356;&#20010;&#24615;&#21270;&#12289;&#39640;&#25928;&#21644;&#31215;&#26497;&#30340;&#36807;&#31243;&#65292;&#24443;&#24213;&#25913;&#21464;&#21307;&#30103;&#20445;&#20581;&#20132;&#20184;&#26041;&#24335;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#20114;&#21160;&#23545;&#35805;&#27169;&#22411;&#65292;&#24456;&#21487;&#33021;&#25512;&#21160;&#21307;&#30103;&#20445;&#20581;&#30340;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#36716;&#22411;&#12290;&#36890;&#36807;&#25552;&#20379;&#35786;&#26029;&#12289;&#20010;&#24615;&#21270;&#29983;&#27963;&#26041;&#24335;&#24314;&#35758;&#21644;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#31561;&#21508;&#31181;&#26381;&#21153;&#65292;&#30446;&#26631;&#26159;&#22823;&#24133;&#24230;&#25552;&#39640;&#24739;&#32773;&#30340;&#20581;&#24247;&#32467;&#26524;&#65292;&#21516;&#26102;&#20943;&#36731;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#21307;&#30103;&#24212;&#29992;&#30340;&#29983;&#21629;&#20851;&#38190;&#24615;&#35201;&#27714;&#24314;&#31435;&#32479;&#19968;&#20840;&#38754;&#30340;&#23545;&#35805;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#12290;&#24050;&#26377;&#30340;&#38024;&#23545;&#21508;&#31181;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25552;&#20986;&#30340;&#35780;&#20272;&#25351;&#26631;&#22312;&#29702;&#35299;&#21307;&#23398;&#21644;&#20581;&#24247;&#27010;&#24565;&#21450;&#20854;&#22312;&#20419;&#36827;&#24739;&#32773;&#31119;&#31049;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25351;&#26631;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#29992;&#25143;&#20307;&#39564;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence is set to revolutionize healthcare delivery by transforming traditional patient care into a more personalized, efficient, and proactive process. Chatbots, serving as interactive conversational models, will probably drive this patient-centered transformation in healthcare. Through the provision of various services, including diagnosis, personalized lifestyle recommendations, and mental health support, the objective is to substantially augment patient health outcomes, all the while mitigating the workload burden on healthcare providers. The life-critical nature of healthcare applications necessitates establishing a unified and comprehensive set of evaluation metrics for conversational models. Existing evaluation metrics proposed for various generic large language models (LLMs) demonstrate a lack of comprehension regarding medical and health concepts and their significance in promoting patients' well-being. Moreover, these metrics neglect pivotal user-ce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#35299;&#20915;&#25163;&#35821;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#23545;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#22312;&#22810;&#35821;&#31181;&#25163;&#25351;&#25340;&#20889;&#35821;&#26009;&#24211;&#19978;&#26377;&#30410;&#22788;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#35270;&#35273;&#19978;&#30340;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12443</link><description>&lt;p&gt;
&#22810;&#35821;&#31181;&#25163;&#25351;&#25340;&#20889;&#35821;&#26009;&#24211;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning for Multilingual Fingerspelling Corpora. (arXiv:2309.12443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#35299;&#20915;&#25163;&#35821;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#23545;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#22312;&#22810;&#35821;&#31181;&#25163;&#25351;&#25340;&#20889;&#35821;&#26009;&#24211;&#19978;&#26377;&#30410;&#22788;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#35270;&#35273;&#19978;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#35299;&#20915;&#25163;&#35821;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#20998;&#26512;&#12290;&#30001;&#20110;&#35768;&#22810;&#25163;&#35821;&#26159;&#27861;&#22269;&#25163;&#35821;&#30340;&#35821;&#35328;&#21518;&#35028;&#65292;&#23427;&#20204;&#20849;&#20139;&#25163;&#21183;&#37197;&#32622;&#65292;&#39044;&#35757;&#32451;&#26377;&#26395;&#21033;&#29992;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#22312;&#32654;&#22269;&#12289;&#20013;&#22269;&#12289;&#24503;&#22269;&#21644;&#29233;&#23572;&#20848;&#30340;&#25163;&#25351;&#25340;&#20889;&#35821;&#26009;&#24211;&#19978;&#27979;&#35797;&#20102;&#36825;&#19968;&#20551;&#35774;&#12290;&#25105;&#20204;&#30830;&#23454;&#35266;&#23519;&#21040;&#20102;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#65292;&#20294;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#35270;&#35273;&#19978;&#30340;&#30456;&#20284;&#24615;&#32780;&#38750;&#35821;&#35328;&#19978;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We apply active learning to help with data scarcity problems in sign languages. In particular, we perform a novel analysis of the effect of pre-training. Since many sign languages are linguistic descendants of French sign language, they share hand configurations, which pre-training can hopefully exploit. We test this hypothesis on American, Chinese, German, and Irish fingerspelling corpora. We do observe a benefit from pre-training, but this may be due to visual rather than linguistic similarities
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#20302;&#36164;&#28304;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#36827;&#34892;&#22686;&#24378;&#30340;&#21487;&#33021;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#21487;&#20197;&#29992;&#20316;&#20302;&#36164;&#28304;&#35835;&#35299;&#20219;&#21153;&#20013;&#20154;&#24037;&#27880;&#37322;&#32773;&#30340;&#26367;&#20195;&#21697;&#12290;&#36825;&#39033;&#24037;&#20316;&#31361;&#20986;&#20102;LLMs&#20316;&#20026;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#22120;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24182;&#21457;&#24067;&#20102;&#22686;&#24378;&#29256;&#26412;&#30340;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.12426</link><description>&lt;p&gt;
LLMs&#33021;&#22686;&#24378;&#20302;&#36164;&#28304;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#21527;&#65311;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Augment Low-Resource Reading Comprehension Datasets? Opportunities and Challenges. (arXiv:2309.12426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#20302;&#36164;&#28304;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#36827;&#34892;&#22686;&#24378;&#30340;&#21487;&#33021;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#21487;&#20197;&#29992;&#20316;&#20302;&#36164;&#28304;&#35835;&#35299;&#20219;&#21153;&#20013;&#20154;&#24037;&#27880;&#37322;&#32773;&#30340;&#26367;&#20195;&#21697;&#12290;&#36825;&#39033;&#24037;&#20316;&#31361;&#20986;&#20102;LLMs&#20316;&#20026;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#22120;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24182;&#21457;&#24067;&#20102;&#22686;&#24378;&#29256;&#26412;&#30340;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#24191;&#27867;&#30340;NLP&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#24615;&#33021;&#65292;&#33021;&#22815;&#36827;&#34892;&#25512;&#29702;&#21644;&#24212;&#29992;&#24120;&#35782;&#12290;&#19968;&#20010;&#30456;&#20851;&#30340;&#24212;&#29992;&#26159;&#23558;&#23427;&#20204;&#29992;&#20110;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#20197;&#20379;&#21518;&#32493;&#20219;&#21153;&#20351;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992;GPT-4&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#25277;&#21462;&#24335;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#12290;&#33258;&#21160;&#21270;&#30340;&#25968;&#25454;&#27880;&#37322;&#36807;&#31243;&#26377;&#28508;&#21147;&#33410;&#30465;&#22823;&#37327;&#26102;&#38388;&#12289;&#37329;&#38065;&#21644;&#31934;&#21147;&#65292;&#36825;&#20123;&#37117;&#26159;&#29992;&#20110;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#24494;&#35843;&#21518;&#30340;&#24615;&#33021;&#20197;&#21450;&#27880;&#37322;&#30340;&#25104;&#26412;&#65292;&#35780;&#20272;&#20102;GPT-4&#20316;&#20026;&#20302;&#36164;&#28304;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#20154;&#24037;&#27880;&#37322;&#26367;&#20195;&#32773;&#30340;&#24615;&#33021;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#23545;LLMs&#20316;&#20026;QA&#31995;&#32479;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#22120;&#30340;&#39318;&#27425;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#29420;&#29305;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#30340;&#22686;&#24378;&#29256;&#26412;&#65292;&#36825;&#23558;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#37325;&#26032;&#35780;&#20272;LLMs&#22312;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive zero shot performance on a wide range of NLP tasks, demonstrating the ability to reason and apply commonsense. A relevant application is to use them for creating high quality synthetic datasets for downstream tasks. In this work, we probe whether GPT-4 can be used to augment existing extractive reading comprehension datasets. Automating data annotation processes has the potential to save large amounts of time, money and effort that goes into manually labelling datasets. In this paper, we evaluate the performance of GPT-4 as a replacement for human annotators for low resource reading comprehension tasks, by comparing performance after fine tuning, and the cost associated with annotation. This work serves to be the first analysis of LLMs as synthetic data augmenters for QA systems, highlighting the unique opportunities and challenges. Additionally, we release augmented versions of low resource datasets, that will allow the researc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20540;&#20915;&#31574;&#22270;&#30340;&#26032;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#24378;&#32422;&#26463;&#25991;&#26412;&#12290;&#36890;&#36807;&#23558;&#29983;&#25104;&#21477;&#23376;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#31163;&#25955;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#22810;&#20540;&#20915;&#31574;&#22270;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#21487;&#20197;&#24471;&#21040;&#35814;&#23613;&#35299;&#38598;&#12290;&#24212;&#29992;&#35821;&#35328;&#27169;&#22411;&#20445;&#30041;&#26368;&#20339;&#21477;&#23376;&#65292;&#24182;&#22312;&#33521;&#35821;&#21644;&#27861;&#35821;&#19978;&#36827;&#34892;&#20102;&#35814;&#32454;&#35752;&#35770;&#12290;&#35813;&#26041;&#27861;&#30456;&#27604;&#20256;&#32479;&#30340;&#35270;&#21147;&#31579;&#26597;&#27979;&#35797;&#24102;&#26469;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#24182;&#19988;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12415</link><description>&lt;p&gt;
&#32422;&#26463;&#20248;&#20808;&#65306;&#19968;&#31181;&#22522;&#20110;MDD&#30340;&#29983;&#25104;&#21463;&#32422;&#26463;&#21477;&#23376;&#30340;&#26032;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Constraints First: A New MDD-based Model to Generate Sentences Under Constraints. (arXiv:2309.12415v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20540;&#20915;&#31574;&#22270;&#30340;&#26032;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#24378;&#32422;&#26463;&#25991;&#26412;&#12290;&#36890;&#36807;&#23558;&#29983;&#25104;&#21477;&#23376;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#31163;&#25955;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#22810;&#20540;&#20915;&#31574;&#22270;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#21487;&#20197;&#24471;&#21040;&#35814;&#23613;&#35299;&#38598;&#12290;&#24212;&#29992;&#35821;&#35328;&#27169;&#22411;&#20445;&#30041;&#26368;&#20339;&#21477;&#23376;&#65292;&#24182;&#22312;&#33521;&#35821;&#21644;&#27861;&#35821;&#19978;&#36827;&#34892;&#20102;&#35814;&#32454;&#35752;&#35770;&#12290;&#35813;&#26041;&#27861;&#30456;&#27604;&#20256;&#32479;&#30340;&#35270;&#21147;&#31579;&#26597;&#27979;&#35797;&#24102;&#26469;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#24182;&#19988;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#24378;&#32422;&#26463;&#25991;&#26412;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#29992;&#20110;&#35270;&#21147;&#31579;&#26597;&#30340;&#26631;&#20934;&#21270;&#21477;&#23376;&#29983;&#25104;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#31163;&#25955;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#22810;&#20540;&#20915;&#31574;&#22270;(MDD)&#65292;&#36825;&#26159;&#19968;&#31181;&#22788;&#29702;&#32422;&#26463;&#30340;&#33879;&#21517;&#25968;&#25454;&#32467;&#26500;&#12290;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#20013;&#65292;MDD&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#26159;&#21487;&#20197;&#35745;&#31639;&#20986;&#19981;&#38656;&#35201;&#36827;&#34892;&#20219;&#20309;&#25628;&#32034;&#30340;&#35814;&#23613;&#35299;&#38598;&#12290;&#19968;&#26086;&#33719;&#24471;&#20102;&#21477;&#23376;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;(GPT-2)&#26469;&#20445;&#30041;&#26368;&#20339;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#33521;&#35821;&#21644;&#27861;&#35821;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#19968;&#20123;&#19968;&#33268;&#21644;&#21464;&#20301;&#35268;&#21017;&#34987;&#35748;&#20026;&#26356;&#22797;&#26434;&#12290;&#26368;&#21518;&#65292;&#20511;&#21161;&#20110;GPT-2&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25968;&#30334;&#20010;&#30495;&#27491;&#30340;&#20505;&#36873;&#21477;&#23376;&#12290;&#19982;&#22312;&#33879;&#21517;&#30340;&#35270;&#21147;&#31579;&#26597;&#27979;&#35797;(MNREAD)&#20013;&#36890;&#24120;&#21487;&#29992;&#30340;&#20960;&#21313;&#20010;&#21477;&#23376;&#30456;&#27604;&#65292;&#36825;&#22312;&#26631;&#20934;&#21270;&#21477;&#23376;&#29983;&#25104;&#39046;&#22495;&#24102;&#26469;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#21487;&#20197;&#36731;&#26494;&#35843;&#25972;&#35813;&#26041;&#27861;&#36866;&#24212;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#32422;&#26463;&#65292;&#22240;&#27492;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new approach to generating strongly constrained texts. We consider standardized sentence generation for the typical application of vision screening. To solve this problem, we formalize it as a discrete combinatorial optimization problem and utilize multivalued decision diagrams (MDD), a well-known data structure to deal with constraints. In our context, one key strength of MDD is to compute an exhaustive set of solutions without performing any search. Once the sentences are obtained, we apply a language model (GPT-2) to keep the best ones. We detail this for English and also for French where the agreement and conjugation rules are known to be more complex. Finally, with the help of GPT-2, we get hundreds of bona-fide candidate sentences. When compared with the few dozen sentences usually available in the well-known vision screening test (MNREAD), this brings a major breakthrough in the field of standardized sentence generation. Also, as it can be easily adapted 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22522;&#20110;GPT&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#23558;&#39046;&#22495;&#30693;&#35782;&#24211;&#19982;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#20197;&#25552;&#39640;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#24211;&#21644;&#35780;&#20272;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#23398;&#29983;&#21644;&#39046;&#22495;&#19987;&#23478;&#23545;&#20110;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#22238;&#31572;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2309.12367</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;GPT&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#30740;&#31350;&#39046;&#22495;&#30693;&#35782;&#24211;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Examining the Influence of Varied Levels of Domain Knowledge Base Inclusion in GPT-based Intelligent Tutors. (arXiv:2309.12367v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22522;&#20110;GPT&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#23558;&#39046;&#22495;&#30693;&#35782;&#24211;&#19982;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#20197;&#25552;&#39640;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#24211;&#21644;&#35780;&#20272;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#23398;&#29983;&#21644;&#39046;&#22495;&#19987;&#23478;&#23545;&#20110;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#22238;&#31572;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20419;&#36827;&#20102;&#20855;&#26377;&#22797;&#26434;&#23545;&#35805;&#33021;&#21147;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;LLM&#23545;&#26597;&#35810;&#30340;&#22238;&#31572;&#32463;&#24120;&#19981;&#20934;&#30830;&#65292;&#36825;&#38480;&#21046;&#20102;&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#19982;LLM&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#38598;&#25104;&#20197;&#22686;&#21152;&#22238;&#31572;&#21487;&#38752;&#24615;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#24211;&#65292;&#25945;&#32946;&#30417;&#30563;&#21592;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#35838;&#31243;&#65292;&#35813;&#35838;&#31243;&#20250;&#34987;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#33258;&#21160;&#22788;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#35780;&#20272;&#23454;&#39564;&#65292;&#23398;&#29983;&#21442;&#19982;&#32773;&#38656;&#35201;&#22238;&#31572;&#26377;&#20851;&#20154;&#24037;&#26234;&#33021;&#35838;&#31243;&#30340;&#38382;&#39064;&#12290; GPT-4&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20855;&#26377;&#19981;&#21516;&#23618;&#27425;&#30340;KB&#35775;&#38382;&#26435;&#38480;&#65292;&#24182;&#30001;&#20154;&#31867;&#39046;&#22495;&#19987;&#23478;&#35780;&#20272;&#36825;&#20123;&#22238;&#31572;&#12290;&#26368;&#21518;&#65292;&#23398;&#29983;&#23545;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#22238;&#31572;&#36827;&#34892;&#20102;&#19982;&#39046;&#22495;&#19987;&#23478;&#30340;&#20132;&#21449;&#39564;&#35777;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#21508;&#31181;&#25945;&#23398;&#33021;&#21147;&#36827;&#34892;&#20102;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have facilitated the development of chatbots with sophisticated conversational capabilities. However, LLMs exhibit frequent inaccurate responses to queries, hindering applications in educational settings. In this paper, we investigate the effectiveness of integrating a knowledge base (KB) with LLM intelligent tutors to increase response reliability. To achieve this, we design a scaleable KB that affords educational supervisors seamless integration of lesson curricula, which is automatically processed by the intelligent tutoring system. We then detail an evaluation, where student participants were presented with questions about the artificial intelligence curriculum to respond to. GPT-4 intelligent tutors with varying hierarchies of KB access and human domain experts then assessed these responses. Lastly, students cross-examined the intelligent tutors' responses to the domain experts' and ranked their various pedagogical abilities. Res
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#22312;&#31070;&#32463;&#30524;&#31185;&#30142;&#30149;&#35786;&#26029;&#20013;&#30340;&#36741;&#21161;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;13&#20010;&#30149;&#20363;&#20013;&#27491;&#30830;&#35786;&#26029;&#20102;59%&#65292;&#32780;ChatGPT Plus&#21644;&#20004;&#21517;&#31070;&#32463;&#30524;&#31185;&#21307;&#29983;&#30340;&#27491;&#30830;&#29575;&#20998;&#21035;&#36798;&#21040;&#20102;82%&#21644;86%&#12290;&#36825;&#34920;&#26126;ChatGPT&#21487;&#20197;&#20316;&#20026;&#31070;&#32463;&#30524;&#31185;&#30142;&#30149;&#35786;&#26029;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2309.12361</link><description>&lt;p&gt;
ChatGPT&#22522;&#20110;&#30149;&#20363;&#25253;&#21578;&#36741;&#21161;&#31070;&#32463;&#30524;&#31185;&#30142;&#30149;&#30340;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Assisting Diagnosis of Neuro-ophthalmology Diseases Based on Case Reports. (arXiv:2309.12361v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#22312;&#31070;&#32463;&#30524;&#31185;&#30142;&#30149;&#35786;&#26029;&#20013;&#30340;&#36741;&#21161;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;13&#20010;&#30149;&#20363;&#20013;&#27491;&#30830;&#35786;&#26029;&#20102;59%&#65292;&#32780;ChatGPT Plus&#21644;&#20004;&#21517;&#31070;&#32463;&#30524;&#31185;&#21307;&#29983;&#30340;&#27491;&#30830;&#29575;&#20998;&#21035;&#36798;&#21040;&#20102;82%&#21644;86%&#12290;&#36825;&#34920;&#26126;ChatGPT&#21487;&#20197;&#20316;&#20026;&#31070;&#32463;&#30524;&#31185;&#30142;&#30149;&#35786;&#26029;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;&#22522;&#20110;&#35814;&#32454;&#30149;&#20363;&#25551;&#36848;&#36741;&#21161;&#31070;&#32463;&#30524;&#31185;&#30142;&#30149;&#35786;&#26029;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20174;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#22312;&#32447;&#25968;&#25454;&#24211;&#20013;&#36873;&#25321;&#20102;22&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#30524;&#31185;&#30142;&#30149;&#30149;&#20363;&#25253;&#21578;&#12290;&#36825;&#20123;&#30149;&#20363;&#21253;&#25324;&#31070;&#32463;&#30524;&#31185;&#20122;&#19987;&#31185;&#24120;&#35265;&#30340;&#24930;&#24615;&#21644;&#24613;&#24615;&#30142;&#30149;&#12290;&#25105;&#20204;&#25226;&#27599;&#20010;&#30149;&#20363;&#30340;&#25991;&#26412;&#20316;&#20026;&#26032;&#30340;&#25552;&#31034;&#25554;&#20837;&#21040;ChatGPT v3.5&#21644;ChatGPT Plus v4.0&#20013;&#65292;&#24182;&#35810;&#38382;&#26368;&#26377;&#21487;&#33021;&#30340;&#35786;&#26029;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20934;&#30830;&#30340;&#20449;&#24687;&#25552;&#20379;&#32473;&#20004;&#21517;&#31070;&#32463;&#30524;&#31185;&#21307;&#29983;&#65292;&#24182;&#35760;&#24405;&#20182;&#20204;&#30340;&#35786;&#26029;&#32467;&#26524;&#65292;&#28982;&#21518;&#19982;ChatGPT&#30340;&#22238;&#31572;&#36827;&#34892;&#23545;&#27604;&#12290;&#32467;&#26524;&#65306;ChatGPT v3.5&#12289;ChatGPT Plus v4.0&#21644;&#20004;&#21517;&#31070;&#32463;&#30524;&#31185;&#21307;&#29983;&#22312;22&#20010;&#30149;&#20363;&#20013;&#20998;&#21035;&#36798;&#21040;13&#20010;&#65288;59%&#65289;&#12289;18&#20010;&#65288;82%&#65289;&#12289;19&#20010;&#65288;86%&#65289;&#21644;19&#20010;&#65288;86%&#65289;&#30340;&#27491;&#30830;&#35786;&#26029;&#12290;&#21508;&#31181;&#35786;&#26029;&#26469;&#28304;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#22914;&#19979;&#65306;ChatGPT v3.5&#21644;ChatGPT Plus v4.0&#30340;&#19968;&#33268;&#24615;&#20026;13&#20010;...
&lt;/p&gt;
&lt;p&gt;
Objective: To evaluate the efficiency of large language models (LLMs) such as ChatGPT to assist in diagnosing neuro-ophthalmic diseases based on detailed case descriptions. Methods: We selected 22 different case reports of neuro-ophthalmic diseases from a publicly available online database. These cases included a wide range of chronic and acute diseases that are commonly seen by neuro-ophthalmic sub-specialists. We inserted the text from each case as a new prompt into both ChatGPT v3.5 and ChatGPT Plus v4.0 and asked for the most probable diagnosis. We then presented the exact information to two neuro-ophthalmologists and recorded their diagnoses followed by comparison to responses from both versions of ChatGPT. Results: ChatGPT v3.5, ChatGPT Plus v4.0, and the two neuro-ophthalmologists were correct in 13 (59%), 18 (82%), 19 (86%), and 19 (86%) out of 22 cases, respectively. The agreement between the various diagnostic sources were as follows: ChatGPT v3.5 and ChatGPT Plus v4.0, 13 (5
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25277;&#26679;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#31038;&#20250;&#36873;&#25321;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#35757;&#32451;&#26377;&#32032;&#30340;NLP&#27169;&#22411;&#20272;&#35745;&#20102;&#25552;&#26696;&#36890;&#36807;&#30340;&#27010;&#29575;&#65292;&#24182;&#36890;&#36807;&#37319;&#26679;&#22810;&#25968;&#26469;&#20915;&#23450;&#25552;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.12360</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25277;&#26679;&#23454;&#29616;&#39640;&#25928;&#30340;&#31038;&#20250;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Efficient Social Choice via NLP and Sampling. (arXiv:2309.12360v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25277;&#26679;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#31038;&#20250;&#36873;&#25321;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#35757;&#32451;&#26377;&#32032;&#30340;NLP&#27169;&#22411;&#20272;&#35745;&#20102;&#25552;&#26696;&#36890;&#36807;&#30340;&#27010;&#29575;&#65292;&#24182;&#36890;&#36807;&#37319;&#26679;&#22810;&#25968;&#26469;&#20915;&#23450;&#25552;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#24863;&#30693;&#31038;&#20250;&#36873;&#25321;&#35299;&#20915;&#20102;&#19968;&#20123;&#20195;&#29702;&#31038;&#21306;&#38754;&#20020;&#30340;&#22522;&#26412;&#20914;&#31361;&#65292;&#21363;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#21253;&#25324;&#25152;&#26377;&#25104;&#21592;&#30340;&#28212;&#26395;&#19982;&#31038;&#21306;&#25104;&#21592;&#21487;&#25903;&#37197;&#30340;&#26377;&#38480;&#26102;&#38388;&#21644;&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#24863;&#30693;&#31038;&#20250;&#36873;&#25321;&#30340;&#20004;&#31181;&#25216;&#26415;&#32452;&#21512;&#65292;&#21363;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#25277;&#26679;&#12290;&#22522;&#26412;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#20854;&#20013;&#27599;&#20010;&#25913;&#21464;&#29616;&#29366;&#30340;&#27835;&#29702;&#25552;&#26696;&#39318;&#20808;&#21457;&#36865;&#21040;&#35757;&#32451;&#26377;&#32032;&#30340;NLP&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20272;&#35745;&#20102;&#22914;&#26524;&#25152;&#26377;&#31038;&#21306;&#25104;&#21592;&#30452;&#25509;&#23545;&#20854;&#36827;&#34892;&#25237;&#31080;&#65292;&#35813;&#25552;&#26696;&#36890;&#36807;&#30340;&#27010;&#29575;&#65307;&#28982;&#21518;&#65292;&#22522;&#20110;&#36825;&#31181;&#20272;&#35745;&#65292;&#36873;&#25321;&#19968;&#20010;&#30830;&#23450;&#22823;&#23567;&#30340;&#20154;&#32676;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#37319;&#26679;&#22810;&#25968;&#20915;&#23450;&#25552;&#26696;&#12290;&#25105;&#20204;&#26681;&#25454;&#19978;&#36848;&#26041;&#26696;&#24320;&#21457;&#20102;&#20960;&#31181;&#20855;&#20307;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#22810;&#20010;&#20998;&#25955;&#33258;&#27835;&#32452;&#32455;&#65288;DAO&#65289;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-Aware Social Choice tackles the fundamental conflict faced by some agent communities between their desire to include all members in the decision making processes and the limited time and attention that are at the disposal of the community members. Here, we investigate a combination of two techniques for attention-aware social choice, namely Natural Language Processing (NLP) and Sampling. Essentially, we propose a system in which each governance proposal to change the status quo is first sent to a trained NLP model that estimates the probability that the proposal would pass if all community members directly vote on it; then, based on such an estimation, a population sample of a certain size is being selected and the proposal is decided upon by taking the sample majority. We develop several concrete algorithms following the scheme described above and evaluate them using various data, including such from several Decentralized Autonomous Organizations (DAOs).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38669;&#22827;&#26031;&#27888;&#24503;&#25991;&#21270;&#32500;&#24230;&#26694;&#26550;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#19981;&#21516;&#25991;&#21270;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#30340;&#25991;&#21270;&#23545;&#40784;&#27979;&#35797;&#65288;CAT&#65289;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#25991;&#21270;&#22269;&#23478;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#35299;&#37322;&#24615;&#25991;&#21270;&#32500;&#24230;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#33021;&#37327;&#21270;&#20854;&#19982;&#29305;&#23450;&#22269;&#23478;&#30340;&#25991;&#21270;&#23545;&#40784;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.12342</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#23545;&#40784;&#65306;&#22522;&#20110;&#38669;&#22827;&#26031;&#27888;&#24503;&#25991;&#21270;&#32500;&#24230;&#30340;&#35299;&#37322;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions. (arXiv:2309.12342v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38669;&#22827;&#26031;&#27888;&#24503;&#25991;&#21270;&#32500;&#24230;&#26694;&#26550;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#19981;&#21516;&#25991;&#21270;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#30340;&#25991;&#21270;&#23545;&#40784;&#27979;&#35797;&#65288;CAT&#65289;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#25991;&#21270;&#22269;&#23478;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#35299;&#37322;&#24615;&#25991;&#21270;&#32500;&#24230;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#33021;&#37327;&#21270;&#20854;&#19982;&#29305;&#23450;&#22269;&#23478;&#30340;&#25991;&#21270;&#23545;&#40784;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37096;&#32626;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#25991;&#21270;&#23545;&#40784;&#21644;&#23545;&#19981;&#21516;&#25991;&#21270;&#35268;&#33539;&#20010;&#20307;&#30340;&#28508;&#22312;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#25919;&#27835;&#21644;&#31038;&#20250;&#20559;&#35265;&#20197;&#21450;&#20844;&#20247;&#24847;&#35265;&#65292;&#32780;&#26410;&#28041;&#21450;&#25991;&#21270;&#20215;&#20540;&#35266;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#21270;&#23545;&#40784;&#27979;&#35797;&#65288;CAT&#65289;&#65292;&#21033;&#29992;&#38669;&#22827;&#26031;&#27888;&#24503;&#30340;&#25991;&#21270;&#32500;&#24230;&#26694;&#26550;&#37327;&#21270;&#25991;&#21270;&#23545;&#40784;&#65292;&#36890;&#36807;&#28508;&#21464;&#37327;&#20998;&#26512;&#25552;&#20379;&#35299;&#37322;&#24615;&#30340;&#36328;&#25991;&#21270;&#27604;&#36739;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;LLMs&#65288;&#22914;ChatGPT&#21644;Bard&#65289;&#22312;&#19981;&#21516;&#25991;&#21270;&#22269;&#23478;&#65288;&#32654;&#22269;&#12289;&#27801;&#29305;&#38463;&#25289;&#20271;&#12289;&#20013;&#22269;&#21644;&#26031;&#27931;&#20240;&#20811;&#65289;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#20215;&#20540;&#35266;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#39118;&#26684;&#21644;&#36229;&#21442;&#25968;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#37327;&#21270;&#20102;LLMs&#19982;&#29305;&#23450;&#22269;&#23478;&#30340;&#25991;&#21270;&#23545;&#40784;&#31243;&#24230;&#65292;&#32780;&#19988;&#25581;&#31034;&#20102;LLMs&#22312;&#35299;&#37322;&#24615;&#25991;&#21270;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;&#23613;&#31649;&#25152;&#26377;&#30340;LLMs&#37117;&#27809;&#26377;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
The deployment of large language models (LLMs) raises concerns regarding their cultural misalignment and potential ramifications on individuals from various cultural norms. Existing work investigated political and social biases and public opinions rather than their cultural values. To address this limitation, the proposed Cultural Alignment Test (CAT) quantifies cultural alignment using Hofstede's cultural dimension framework, which offers an explanatory cross-cultural comparison through the latent variable analysis. We apply our approach to assess the cultural values embedded in state-of-the-art LLMs, such as: ChatGPT and Bard, across diverse cultures of countries: United States (US), Saudi Arabia, China, and Slovakia, using different prompting styles and hyperparameter settings. Our results not only quantify cultural alignment of LLMs with certain countries, but also reveal the difference between LLMs in explanatory cultural dimensions. While all LLMs did not provide satisfactory res
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#20351;&#29992;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35757;&#32451;&#30340;&#25104;&#26412;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24605;&#32771;&#21307;&#30103;&#26426;&#26500;&#26159;&#21542;&#24212;&#35813;&#35757;&#32451;LLM&#20197;&#21450;&#22914;&#20309;&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#36873;&#25321;&#21512;&#36866;LLM&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.12339</link><description>&lt;p&gt;
&#23545;&#21307;&#30103;&#26426;&#26500;&#22312;&#30005;&#23376;&#30149;&#21382;&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#30340;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Considerations for health care institutions training large language models on electronic health records. (arXiv:2309.12339v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#20351;&#29992;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35757;&#32451;&#30340;&#25104;&#26412;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24605;&#32771;&#21307;&#30103;&#26426;&#26500;&#26159;&#21542;&#24212;&#35813;&#35757;&#32451;LLM&#20197;&#21450;&#22914;&#20309;&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#36873;&#25321;&#21512;&#36866;LLM&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65288;&#22914;ChatGPT&#65289;&#24341;&#36215;&#20102;&#36328;&#23398;&#31185;&#31185;&#23398;&#23478;&#30340;&#20852;&#36259;&#65307;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#23545;LLM&#22312;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#28508;&#22312;&#24212;&#29992;&#20063;&#24341;&#21457;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#21307;&#30103;&#26426;&#26500;&#26377;&#24847;&#35753;LLM&#22312;&#33258;&#24049;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#39318;&#20808;&#24517;&#39035;&#38754;&#23545;&#19968;&#20123;&#26840;&#25163;&#30340;&#38382;&#39064;&#65306;&#20182;&#20204;&#24212;&#35813;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;LLM&#65292;&#36824;&#26159;&#20174;&#24320;&#28304;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65311;&#23545;&#20110;&#39044;&#20808;&#30830;&#23450;&#39044;&#31639;&#30340;&#21307;&#30103;&#26426;&#26500;&#26469;&#35828;&#65292;&#20182;&#20204;&#21487;&#20197;&#36127;&#25285;&#24471;&#36215;&#30340;&#26368;&#22823;LLM&#26159;&#20160;&#20040;&#65311;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#20351;&#29992;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#36827;&#34892;LLM&#35757;&#32451;&#30340;&#25104;&#26412;&#26469;&#23545;&#36825;&#20123;&#38382;&#39064;&#36827;&#34892;&#20102;&#21021;&#27493;&#25506;&#35752;&#12290;&#36825;&#20010;&#20998;&#26512;&#20026;&#20174;&#25968;&#25454;&#35268;&#27169;&#12289;&#35745;&#31639;&#35268;&#27169;&#21644;&#35757;&#32451;&#39044;&#31639;&#30340;&#35282;&#24230;&#24605;&#32771;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) like ChatGPT have excited scientists across fields; in medicine, one source of excitement is the potential applications of LLMs trained on electronic health record (EHR) data. But there are tough questions we must first answer if health care institutions are interested in having LLMs trained on their own data; should they train an LLM from scratch or fine-tune it from an open-source model? For healthcare institutions with a predefined budget, what are the biggest LLMs they can afford? In this study, we take steps towards answering these questions with an analysis on dataset sizes, model sizes, and costs for LLM training using EHR data. This analysis provides a framework for thinking about these questions in terms of data scale, compute scale, and training budgets.
&lt;/p&gt;</description></item><item><title>MetaMath&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#29983;&#25104;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.12284</link><description>&lt;p&gt;
MetaMath&#65306;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#33258;&#24049;&#30340;&#25968;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. (arXiv:2309.12284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12284
&lt;/p&gt;
&lt;p&gt;
MetaMath&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#29983;&#25104;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#26497;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;LLMs&#65288;&#20363;&#22914;LLaMA-2&#65289;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#20173;&#28982;&#36828;&#36828;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#65292;&#21407;&#22240;&#26159;&#22797;&#26434;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MetaMath&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27809;&#26377;&#39069;&#22806;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20197;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#20889;&#20837;&#38382;&#39064;&#26469;&#24341;&#23548;&#25968;&#23398;&#38382;&#39064;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#21517;&#20026;MetaMathQA&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;MetaMathQA&#19978;&#23545;LLaMA-2&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#23545;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#20004;&#20010;&#27969;&#34892;&#22522;&#20934;&#27979;&#35797;&#65288;&#21363;GSM8K&#21644;MATH&#65289;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MetaMath&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#19968;&#22871;&#24320;&#28304;LLMs&#12290;&#25105;&#20204;&#30340;MetaMath-7B&#27169;&#22411;&#22312;GSM8K&#19978;&#36798;&#21040;&#20102;66.4&#65285;&#65292;&#22312;MATH&#19978;&#36798;&#21040;&#20102;19.4&#65285;&#65292;&#36229;&#36807;&#20102;&#30456;&#21516;&#35268;&#27169;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (\eg, LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose \emph{MetaMath}, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called {MetaMathQA}. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (\ie, GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves $66.4\%$ on GSM8K and $19.4\%$ on MATH, exceeding the state-of-the-art models of the same size by 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#38463;&#25289;&#20271;&#25991;&#30340;&#26412;&#22320;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(AceGPT)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#26469;&#22521;&#20859;&#20855;&#22791;&#25991;&#21270;&#24847;&#35782;&#21644;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#65292;&#20197;&#28385;&#36275;&#38463;&#25289;&#20271;&#35821;&#31038;&#21306;&#29305;&#23450;&#24212;&#29992;&#38656;&#27714;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;AceGPT&#22312;&#21508;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#26159;&#26368;&#20808;&#36827;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.12053</link><description>&lt;p&gt;
AceGPT&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26412;&#22320;&#21270;&#20026;&#38463;&#25289;&#20271;&#25991;
&lt;/p&gt;
&lt;p&gt;
AceGPT, Localizing Large Language Models in Arabic. (arXiv:2309.12053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#38463;&#25289;&#20271;&#25991;&#30340;&#26412;&#22320;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(AceGPT)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#26469;&#22521;&#20859;&#20855;&#22791;&#25991;&#21270;&#24847;&#35782;&#21644;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#65292;&#20197;&#28385;&#36275;&#38463;&#25289;&#20271;&#35821;&#31038;&#21306;&#29305;&#23450;&#24212;&#29992;&#38656;&#27714;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;AceGPT&#22312;&#21508;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#26159;&#26368;&#20808;&#36827;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#21457;&#36866;&#29992;&#20110;&#38463;&#25289;&#20271;&#25991;&#30340;&#26412;&#22320;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#36843;&#20999;&#38656;&#27714;&#21644;&#26041;&#27861;&#35770;&#65292;&#38463;&#25289;&#20271;&#25991;&#20855;&#26377;&#29420;&#29305;&#30340;&#25991;&#21270;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#30446;&#21069;&#30340;&#20027;&#27969;&#27169;&#22411;&#22914;ChatGPT&#24182;&#26410;&#20805;&#20998;&#35299;&#20915;&#12290;&#22312;&#32771;&#34385;&#25991;&#21270;&#25935;&#24863;&#24615;&#21644;&#26412;&#22320;&#20215;&#20540;&#35266;&#26102;&#36824;&#23384;&#22312;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25171;&#21253;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#36827;&#19968;&#27493;&#20351;&#29992;&#38463;&#25289;&#20271;&#25991;&#26412;&#36827;&#34892;&#39044;&#35757;&#32451;&#12289;&#20351;&#29992;&#26412;&#22320;&#38463;&#25289;&#20271;&#25351;&#20196;&#21644;&#38463;&#25289;&#20271;&#35821;GPT-4&#22238;&#24212;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;(SFT)&#65292;&#20197;&#21450;&#20351;&#29992;&#23545;&#26412;&#22320;&#25991;&#21270;&#21644;&#20215;&#20540;&#35266;&#25935;&#24863;&#30340;&#22870;&#21169;&#27169;&#22411;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;(RLAIF)&#12290;&#30446;&#26631;&#26159;&#35757;&#32451;&#20855;&#22791;&#25991;&#21270;&#24847;&#35782;&#21644;&#19982;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#38463;&#25289;&#20271;&#25991;LLM&#65292;&#20197;&#28385;&#36275;&#38463;&#25289;&#20271;&#35821;&#31038;&#21306;&#22810;&#26679;&#21270;&#30340;&#29305;&#23450;&#24212;&#29992;&#38656;&#27714;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#24471;&#21040;&#30340;&#21517;&#20026;AceGPT&#30340;&#38463;&#25289;&#20271;&#25991;LLM&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#26159;&#26368;&#20808;&#36827;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the imperative need and methodology for developing a localized Large Language Model (LLM) tailored for Arabic, a language with unique cultural characteristics that are not adequately addressed by current mainstream models like ChatGPT. Key concerns additionally arise when considering cultural sensitivity and local values. To this end, the paper outlines a packaged solution, including further pre-training with Arabic texts, supervised fine-tuning (SFT) using native Arabic instructions and GPT-4 responses in Arabic, and reinforcement learning with AI feedback (RLAIF) using a reward model that is sensitive to local culture and values. The objective is to train culturally aware and value-aligned Arabic LLMs that can serve the diverse application-specific needs of Arabic-speaking communities.  Extensive evaluations demonstrated that the resulting LLM called `\textbf{AceGPT}' is the SOTA open Arabic LLM in various benchmarks, including instruction-following benchmark (i.e
&lt;/p&gt;</description></item><item><title>LMSYS-Chat-1M&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20854;&#22810;&#26679;&#24615;&#21644;&#29992;&#20363;&#23637;&#31034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.11998</link><description>&lt;p&gt;
LMSYS-Chat-1M&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset. (arXiv:2309.11998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11998
&lt;/p&gt;
&lt;p&gt;
LMSYS-Chat-1M&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20854;&#22810;&#26679;&#24615;&#21644;&#29992;&#20363;&#23637;&#31034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#30740;&#31350;&#20154;&#20204;&#22914;&#20309;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19982;&#20854;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LMSYS-Chat-1M&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#19982;25&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#36827;&#34892;&#30340;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;&#25105;&#20204;&#30340;Vicuna&#28436;&#31034;&#21644;Chatbot Arena&#32593;&#31449;&#19978;&#30340;21&#19975;&#20010;&#29420;&#31435;IP&#22320;&#22336;&#20013;&#25910;&#38598;&#32780;&#26469;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#20869;&#23481;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#20854;&#31574;&#21010;&#36807;&#31243;&#12289;&#22522;&#26412;&#32479;&#35745;&#25968;&#25454;&#21644;&#20027;&#39064;&#20998;&#24067;&#65292;&#24378;&#35843;&#20854;&#22810;&#26679;&#24615;&#12289;&#29420;&#29305;&#24615;&#21644;&#35268;&#27169;&#12290;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#29992;&#20363;&#23637;&#31034;&#20102;&#23427;&#30340;&#22810;&#26679;&#24615;&#65306;&#24320;&#21457;&#19982;GPT-4&#34920;&#29616;&#30456;&#20284;&#30340;&#20869;&#23481;&#36807;&#28388;&#27169;&#22411;&#12289;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#22522;&#20934;&#12289;&#35757;&#32451;&#19982;Vicuna&#34920;&#29616;&#30456;&#20284;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12289;&#21019;&#24314;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#38382;&#39064;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25105;&#20204;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is pub
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26680;&#24515;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26041;&#27861;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#24182;&#20511;&#37492;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.11981</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#35780;&#20272;&#26694;&#26550;&#65306;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26410;&#26469;&#24230;&#37327;&#30340;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics. (arXiv:2309.11981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11981
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26680;&#24515;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26041;&#27861;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#24182;&#20511;&#37492;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#65292;&#36825;&#20026;&#37325;&#26032;&#23457;&#35270;&#20256;&#32479;&#30340;&#26426;&#22120;&#26234;&#33021;&#24230;&#37327;&#26041;&#27861;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20174;&#20256;&#32479;&#30340;&#22270;&#28789;&#27979;&#35797;&#36716;&#21521;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26680;&#24515;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#24182;&#20511;&#37492;&#20102;&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#28145;&#21463;&#22810;&#20010;&#23398;&#31185;&#30340;&#21331;&#36234;&#24037;&#20316;&#30340;&#24433;&#21709;&#65292;&#25351;&#20986;&#20102;&#20445;&#25345;&#36328;&#23398;&#31185;&#26725;&#26753;&#24320;&#25918;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#21246;&#21202;&#20102;&#19968;&#20010;&#26356;&#21152;&#31283;&#20581;&#21644;&#21487;&#25345;&#32493;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the burgeoning field of artificial intelligence (AI), the unprecedented progress of large language models (LLMs) in natural language processing (NLP) offers an opportunity to revisit the entire approach of traditional metrics of machine intelligence, both in form and content. As the realm of machine cognitive evaluation has already reached Imitation, the next step is an efficient Language Acquisition and Understanding. Our paper proposes a paradigm shift from the established Turing Test towards an all-embracing framework that hinges on language acquisition, taking inspiration from the recent advancements in LLMs. The present contribution is deeply tributary of the excellent work from various disciplines, point out the need to keep interdisciplinary bridges open, and delineates a more robust and sustainable approach.
&lt;/p&gt;</description></item><item><title>InstructERC&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29983;&#25104;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#27169;&#26495;&#27169;&#22359;&#21644;&#39069;&#22806;&#30340;&#24773;&#24863;&#23545;&#40784;&#20219;&#21153;&#65292;&#25913;&#38761;&#20102;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.11911</link><description>&lt;p&gt;
InstructERC&#65306;&#20511;&#21161;&#26816;&#32034;&#22810;&#20219;&#21153;LLMs&#26694;&#26550;&#25913;&#38761;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework. (arXiv:2309.11911v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11911
&lt;/p&gt;
&lt;p&gt;
InstructERC&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29983;&#25104;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#27169;&#26495;&#27169;&#22359;&#21644;&#39069;&#22806;&#30340;&#24773;&#24863;&#23545;&#40784;&#20219;&#21153;&#65292;&#25913;&#38761;&#20102;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;(ERC)&#30340;&#21457;&#23637;&#19968;&#30452;&#21463;&#21040;&#31649;&#36947;&#35774;&#35745;&#22797;&#26434;&#24615;&#30340;&#38459;&#30861;&#65292;&#23548;&#33268;ERC&#27169;&#22411;&#24448;&#24448;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#23545;&#35805;&#27169;&#24335;&#36807;&#25311;&#21512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;InstructERC&#65292;&#23558;ERC&#20219;&#21153;&#20174;&#21028;&#21035;&#24335;&#26694;&#26550;&#36716;&#21270;&#20026;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29983;&#25104;&#24335;&#26694;&#26550;&#12290;InstructERC&#26377;&#20004;&#20010;&#37325;&#35201;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;InstructERC&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26816;&#32034;&#27169;&#26495;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#21382;&#21490;&#23545;&#35805;&#20869;&#23481;&#12289;&#26631;&#31614;&#35821;&#21477;&#21644;&#24773;&#24863;&#39046;&#22495;&#28436;&#31034;&#19982;&#39640;&#35821;&#20041;&#30456;&#20284;&#24615;&#36827;&#34892;&#25340;&#25509;&#65292;&#24110;&#21161;&#27169;&#22411;&#26126;&#30830;&#22320;&#38598;&#25104;&#22810;&#31890;&#24230;&#23545;&#35805;&#30417;&#30563;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#39069;&#22806;&#30340;&#24773;&#24863;&#23545;&#40784;&#20219;&#21153;&#65292;&#21363;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#24773;&#24863;&#39044;&#27979;&#20219;&#21153;&#65292;&#20197;&#38544;&#24335;&#22320;&#24314;&#27169;&#23545;&#35805;&#35282;&#33394;&#20851;&#31995;&#21644;&#26410;&#26469;&#23545;&#35805;&#24773;&#32490;&#20542;&#21521;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;LLM&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The development of emotion recognition in dialogue (ERC) has been consistently hindered by the complexity of pipeline designs, leading to ERC models that often overfit to specific datasets and dialogue patterns. In this study, we propose a novel approach, namely  InstructERC, to reformulates the ERC task from a discriminative framework to a generative framework based on Large Language Models (LLMs) . InstructERC has two significant contributions: Firstly, InstructERC introduces a simple yet effective retrieval template module, which helps the model explicitly integrate multi-granularity dialogue supervision information by concatenating the historical dialog content, label statement, and emotional domain demonstrations with high semantic similarity. Furthermore, we introduce two additional emotion alignment tasks, namely speaker identification and emotion prediction tasks, to implicitly model the dialogue role relationships and future emotional tendencies in conversations. Our LLM-based
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36890;&#36807;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11895</link><description>&lt;p&gt;
&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Audio Contrastive based Fine-tuning. (arXiv:2309.11895v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36890;&#36807;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#20998;&#31867;&#22312;&#35821;&#38899;&#21644;&#22768;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#23558;&#27169;&#22411;&#25311;&#21512;&#21040;&#35757;&#32451;&#25968;&#25454;&#65288;&#36991;&#20813;&#36807;&#25311;&#21512;&#65289;&#24182;&#20351;&#20854;&#33021;&#22815;&#33391;&#22909;&#22320;&#27867;&#21270;&#21040;&#26032;&#39046;&#22495;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#30528;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio classification plays a crucial role in speech and sound processing tasks with a wide range of applications. There still remains a challenge of striking the right balance between fitting the model to the training data (avoiding overfitting) and enabling it to generalise well to a new domain. Leveraging the transferability of contrastive learning, we introduce Audio Contrastive-based Fine-tuning (AudioConFit), an efficient approach characterised by robust generalisability. Empirical experiments on a variety of audio classification tasks demonstrate the effectiveness and robustness of our approach, which achieves state-of-the-art results in various settings.
&lt;/p&gt;</description></item><item><title>CFGPT&#26159;&#19968;&#20010;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#22269;&#37329;&#34701;&#21161;&#25163;&#65292;&#21253;&#25324;CFData&#29992;&#20110;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#21450;CFLLM&#29992;&#20110;&#22788;&#29702;&#37329;&#34701;&#25991;&#26412;&#65292;CFAPP&#29992;&#20110;&#23454;&#38469;&#37329;&#34701;&#24212;&#29992;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#21508;&#20010;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10654</link><description>&lt;p&gt;
CFGPT: &#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#22269;&#37329;&#34701;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
CFGPT: Chinese Financial Assistant with Large Language Model. (arXiv:2309.10654v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10654
&lt;/p&gt;
&lt;p&gt;
CFGPT&#26159;&#19968;&#20010;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#22269;&#37329;&#34701;&#21161;&#25163;&#65292;&#21253;&#25324;CFData&#29992;&#20110;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#21450;CFLLM&#29992;&#20110;&#22788;&#29702;&#37329;&#34701;&#25991;&#26412;&#65292;CFAPP&#29992;&#20110;&#23454;&#38469;&#37329;&#34701;&#24212;&#29992;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#21508;&#20010;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CFGPT&#30340;&#20013;&#22269;&#37329;&#34701;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#29992;&#20110;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#30340;&#25968;&#25454;&#38598;&#65288;CFData&#65289;&#65292;&#29992;&#20110;&#29087;&#32451;&#22788;&#29702;&#37329;&#34701;&#25991;&#26412;&#30340;&#37329;&#34701;LLM&#65288;CFLLM&#65289;&#65292;&#20197;&#21450;&#29992;&#20110;&#23454;&#38469;&#37329;&#34701;&#24212;&#29992;&#30340;&#37096;&#32626;&#26694;&#26550;&#65288;CFAPP&#65289;&#12290;CFData&#21253;&#25324;&#19968;&#20010;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#27719;&#38598;&#20102;&#20013;&#22269;&#37329;&#34701;&#25968;&#25454;&#21644;&#20998;&#26512;&#65292;&#20197;&#21450;&#24635;&#20849;584M&#20010;&#25991;&#20214;&#21644;141B&#20010;&#26631;&#35760;&#30340;&#36739;&#23567;&#30340;&#36890;&#29992;&#25991;&#26412;&#23376;&#38598;&#65292;&#24182;&#19988;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#38598;&#38024;&#23545;&#20845;&#20010;&#19981;&#21516;&#30340;&#37329;&#34701;&#20219;&#21153;&#36827;&#34892;&#20102;&#23450;&#21046;&#65292;&#20869;&#23481;&#28085;&#30422;&#20102;&#37329;&#34701;&#20998;&#26512;&#21644;&#20915;&#31574;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;1.5M&#20010;&#25351;&#20196;&#23545;&#21644;&#24635;&#35745;1.5B&#20010;&#26631;&#35760;&#12290;CFLLM&#22522;&#20110;InternLM-7B&#36827;&#34892;&#20102;&#24179;&#34913;&#27169;&#22411;&#33021;&#21147;&#30340;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated great potential in natural language processing tasks within the financial domain. In this work, we present a Chinese Financial Generative Pre-trained Transformer framework, named CFGPT, which includes a dataset~(CFData) for pre-training and supervised fine-tuning, a financial LLM~(CFLLM) to adeptly manage financial texts, and a deployment framework~(CFAPP) designed to navigate real-world financial applications. The CFData comprising both a pre-training dataset and a supervised fine-tuning dataset, where the pre-training dataset collates Chinese financial data and analytics, alongside a smaller subset of general-purpose text with 584M documents and 141B tokens in total, and the supervised fine-tuning dataset is tailored for six distinct financial tasks, embodying various facets of financial analysis and decision-making with 1.5M instruction pairs and 1.5B tokens in total. The CFLLM, which is based on InternLM-7B to balance the model capabil
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20419;&#36827;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#65292;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#20102;&#35299;&#20102;&#20182;&#20204;&#23545;LLMs&#30340;&#38656;&#27714;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Talk2Care&#30340;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.09357</link><description>&lt;p&gt;
Talk2Care: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20419;&#36827;&#24322;&#27493;&#24739;&#32773;-&#21307;&#29983;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model. (arXiv:2309.09357v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20419;&#36827;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#65292;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#20102;&#35299;&#20102;&#20182;&#20204;&#23545;LLMs&#30340;&#38656;&#27714;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Talk2Care&#30340;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#22823;&#37327;&#30340;&#36828;&#31243;&#21307;&#30103;&#24212;&#29992;&#31243;&#24207;&#26469;&#24110;&#21161;&#23478;&#24237;&#20013;&#30340;&#32769;&#24180;&#20154;&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;&#65292;&#20294;&#22522;&#26412;&#30340;&#28040;&#24687;&#21644;&#30005;&#35805;&#20173;&#28982;&#26159;&#26368;&#24120;&#35265;&#30340;&#36890;&#20449;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#26377;&#38480;&#30340;&#21487;&#29992;&#24615;&#12289;&#20449;&#24687;&#20002;&#22833;&#21644;&#27969;&#31243;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#20419;&#36827;&#24739;&#32773;-&#21307;&#29983;&#36890;&#20449;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21450;&#20854;&#24378;&#22823;&#30340;&#33258;&#28982;&#23545;&#35805;&#21644;&#25688;&#35201;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLMs&#22312;&#36890;&#20449;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#36824;&#23384;&#22312;&#26377;&#38480;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#20004;&#39033;&#35775;&#35848;&#30740;&#31350;&#65292;&#20998;&#21035;&#19982;&#32769;&#24180;&#20154;(N=10)&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;(N=9)&#36827;&#34892;&#20102;&#20132;&#27969;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#22312;&#24739;&#32773;-&#21307;&#29983;&#24322;&#27493;&#36890;&#20449;&#20013;&#23545;LLMs&#30340;&#38656;&#27714;&#21644;&#26426;&#20250;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;Talk2Care&#65292;&#24182;&#20026;&#20004;&#20010;&#32676;&#20307;&#35774;&#35745;&#20102;&#20132;&#20114;&#32452;&#20214;: (1) &#23545;&#20110;&#32769;&#24180;&#20154;&#65292;&#25105;&#20204;&#21033;&#29992;&#35821;&#38899;&#21161;&#25163;&#30340;&#20415;&#21033;&#24615;&#21644;&#26131;&#20110;&#33719;&#21462;&#24615;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;LLM&#39537;&#21160;&#30340;&#35821;&#38899;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs' role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered VA i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#20998;&#26512;&#20102;&#22312;&#20154;&#25991;&#23398;&#31185;&#21644;&#32771;&#21476;&#23398;&#39046;&#22495;&#20013;&#20845;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#23398;&#26415;&#20889;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#37325;&#26032;&#32452;&#21512;&#29616;&#26377;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20135;&#29983;&#21407;&#21019;&#31185;&#23398;&#20869;&#23481;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08636</link><description>&lt;p&gt;
ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#31185;&#23398;&#20889;&#20316;&#26041;&#38754;&#34920;&#29616;&#22914;&#20309;&#65311;&#65288;&#31532;23&#23395;&#31532;3&#23395;&#65289;&#12290;&#65288;arXiv:2309.08636v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI chatbots at scientific writing? (ver. 23Q3). (arXiv:2309.08636v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#20998;&#26512;&#20102;&#22312;&#20154;&#25991;&#23398;&#31185;&#21644;&#32771;&#21476;&#23398;&#39046;&#22495;&#20013;&#20845;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#23398;&#26415;&#20889;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#37325;&#26032;&#32452;&#21512;&#29616;&#26377;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20135;&#29983;&#21407;&#21019;&#31185;&#23398;&#20869;&#23481;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21382;&#21490;&#19978;&#65292;&#29087;&#32451;&#30340;&#20889;&#20316;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#36827;&#27493;&#30340;&#20851;&#38190;&#65292;&#21019;&#36896;&#24615;&#34920;&#36798;&#34987;&#35270;&#20026;&#20154;&#31867;&#25104;&#23601;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#24335;AI&#30340;&#26368;&#26032;&#36827;&#23637;&#26631;&#24535;&#30528;&#36825;&#19968;&#21465;&#20107;&#30340;&#19968;&#20010;&#36716;&#25240;&#28857;&#65292;&#21253;&#25324;&#22312;&#31185;&#23398;&#20889;&#20316;&#26041;&#38754;&#12290;&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#20845;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20154;&#25991;&#23398;&#31185;&#21644;&#32771;&#21476;&#23398;&#26041;&#38754;&#23398;&#26415;&#20889;&#20316;&#20013;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#26041;&#27861;&#22522;&#20110;&#30001;&#20154;&#31867;&#19987;&#23478;&#23545;AI&#29983;&#25104;&#20869;&#23481;&#36827;&#34892;&#23450;&#37327;&#20934;&#30830;&#24615;&#21644;&#23450;&#24615;&#31934;&#30830;&#24615;&#26631;&#35760;&#12290;&#23450;&#37327;&#20934;&#30830;&#24615;&#35780;&#20272;&#20102;&#20107;&#23454;&#30340;&#27491;&#30830;&#24615;&#65292;&#32780;&#23450;&#24615;&#31934;&#30830;&#24615;&#35780;&#20272;&#20102;&#31185;&#23398;&#36129;&#29486;&#12290;&#34429;&#28982;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#29305;&#21035;&#26159;ChatGPT-4&#65292;&#22312;&#37325;&#26032;&#32452;&#21512;&#29616;&#26377;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#29087;&#32451;&#24615;&#65292;&#20294;&#22312;&#29983;&#25104;&#21407;&#21019;&#31185;&#23398;&#20869;&#23481;&#26041;&#38754;&#22833;&#36133;&#20102;&#12290;&#39034;&#20415;&#25552;&#19968;&#19979;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#26174;&#31034;&#65292;&#38543;&#30528;ChatGPT-4&#65292;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#24050;&#32463;&#20572;&#28382;&#19981;&#21069;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#22797;&#26434;&#19988;&#21453;&#22797;&#26080;&#24120;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Historically, proficient writing was deemed essential for human advancement, with creative expression viewed as one of the hallmarks of human achievement. However, recent advances in generative AI have marked an inflection point in this narrative, including for scientific writing. This article provides a comprehensive analysis of the capabilities and limitations of six AI chatbots in scholarly writing in the humanities and archaeology. The methodology was based on tagging AI generated content for quantitative accuracy and qualitative precision by human experts. Quantitative accuracy assessed the factual correctness, while qualitative precision gauged the scientific contribution. While the AI chatbots, especially ChatGPT-4, demonstrated proficiency in recombining existing knowledge, they failed in generating original scientific content. As a side note, our results also suggest that with ChatGPT-4 the size of the LLMs has plateaued. Furthermore, the paper underscores the intricate and re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#20506;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31532;&#20108;&#27425;&#25171;&#20998;&#26102;&#25552;&#20379;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;boosting&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#20197;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#21644;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#25552;&#31034;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.00723</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#19978;&#19979;&#25991;&#20559;&#20506;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Contextual Biasing of Named-Entities with Large Language Models. (arXiv:2309.00723v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#20506;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31532;&#20108;&#27425;&#25171;&#20998;&#26102;&#25552;&#20379;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;boosting&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#20197;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#21644;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#25552;&#31034;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#20506;&#65292;&#21363;&#22312;&#31532;&#20108;&#27425;&#25171;&#20998;&#26102;&#20026;LLM&#25552;&#20379;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#25171;&#20998;&#26399;&#38388;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#23545;LLM&#36827;&#34892;boosting&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#20123;&#25552;&#31034;&#20449;&#24687;&#21253;&#25324;&#20559;&#20506;&#21015;&#34920;&#21644;&#23569;&#26679;&#26412;&#31034;&#20363;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#20551;&#35774;&#24471;&#20998;&#26102;&#20316;&#20026;&#38468;&#21152;&#20449;&#24687;&#12290;&#38500;&#20102;&#23569;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;LLM&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#21644;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#20026;&#20102;&#25552;&#39640;&#19978;&#19979;&#25991;&#20559;&#20506;&#30340;&#25928;&#29575;&#24182;&#36991;&#20813;&#36229;&#36807;LLMs&#30340;&#26368;&#22823;&#24207;&#21015;&#38271;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#25552;&#31034;&#65292;&#21363;&#20351;&#29992;&#31867;&#21035;&#26631;&#31614;&#39044;&#27979;&#36873;&#25321;&#26368;&#21487;&#33021;&#30340;&#31867;&#21035;&#65292;&#24182;&#20165;&#20351;&#29992;&#36825;&#20010;&#31867;&#21035;&#20013;&#30340;&#23454;&#20307;&#20316;&#20026;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#19978;&#19979;&#25991;&#12290;&#23545;&#20869;&#37096;&#30340;&#21628;&#21483;&#12289;&#28040;&#24687;&#21644;&#21475;&#36848;&#25968;&#25454;&#38598;&#20197;&#21450;SLUE-Voxpopuli&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35789;&#38169;&#35823;&#29575;(WER)&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies contextual biasing with Large Language Models (LLMs), where during second-pass rescoring additional contextual information is provided to a LLM to boost Automatic Speech Recognition (ASR) performance. We propose to leverage prompts for a LLM without fine tuning during rescoring which incorporate a biasing list and few-shot examples to serve as additional information when calculating the score for the hypothesis. In addition to few-shot prompt learning, we propose multi-task training of the LLM to predict both the entity class and the next token. To improve the efficiency for contextual biasing and to avoid exceeding LLMs' maximum sequence lengths, we propose dynamic prompting, where we select the most likely class using the class tag prediction, and only use entities in this class as contexts for next token prediction. Word Error Rate (WER) evaluation is performed on i) an internal calling, messaging, and dictation dataset, and ii) the SLUE-Voxpopuli dataset. Results
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2308.07706</link><description>&lt;p&gt;
&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#25506;&#32034;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models. (arXiv:2308.07706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21508;&#31181;&#20020;&#24202;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#25972;&#21512;&#25991;&#26412;&#25351;&#23548;&#20197;&#22686;&#24378;&#35270;&#35273;&#29305;&#24449;&#20173;&#28982;&#26159;&#19968;&#20010;&#36827;&#23637;&#26377;&#38480;&#30340;&#39046;&#22495;&#12290;&#29616;&#26377;&#21033;&#29992;&#25991;&#26412;&#25351;&#23548;&#30340;&#20998;&#21106;&#27169;&#22411;&#20027;&#35201;&#22312;&#24320;&#25918;&#39046;&#22495;&#22270;&#20687;&#19978;&#35757;&#32451;&#65292;&#36825;&#24341;&#21457;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#30452;&#25509;&#24212;&#29992;&#30340;&#38590;&#39064;&#65292;&#38656;&#35201;&#25163;&#21160;&#20171;&#20837;&#25110;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20174;&#22270;&#20687;&#25551;&#36848;&#21644;&#22270;&#20687;&#20013;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#24471;&#33021;&#22815;&#23545;&#22810;&#26679;&#21270;&#30340;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#12290;&#35813;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#20197;&#35780;&#20272;&#20854;&#20174;&#24320;&#25918;&#39046;&#22495;&#21521;&#21307;&#23398;&#39046;&#22495;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#20013;&#20197;&#21069;&#26410;&#35265;&#22270;&#20687;&#30340;&#22270;&#20687;&#25551;&#36848;&#24341;&#20837;&#20102;&#21464;&#21270;&#65292;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical Image Segmentation is crucial in various clinical applications within the medical domain. While state-of-the-art segmentation models have proven effective, integrating textual guidance to enhance visual features for this task remains an area with limited progress. Existing segmentation models that utilize textual guidance are primarily trained on open-domain images, raising concerns about their direct applicability in the medical domain without manual intervention or fine-tuning.  To address these challenges, we propose using multimodal vision-language models for capturing semantic information from image descriptions and images, enabling the segmentation of diverse medical images. This study comprehensively evaluates existing vision language models across multiple datasets to assess their transferability from the open domain to the medical field. Furthermore, we introduce variations of image descriptions for previously unseen images in the dataset, revealing notable variations 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.03941</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65306;&#28085;&#20041;&#12289;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions. (arXiv:2307.03941v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#26368;&#21021;&#26159;&#30001;&#35895;&#27468;&#35199;&#29677;&#29273;&#19982;&#22467;&#20811;&#26031;&#20869;&#22612;&#32034;&#22996;&#21592;&#20250;(Mario Costeja Gonz\'alez)&#20043;&#38388;&#30340;&#23448;&#21496;&#32467;&#26524;&#32780;&#30830;&#31435;&#30340;&#65292;&#24182;&#19988;&#21518;&#26469;&#34987;&#20316;&#20026;&#27431;&#27954;&#32852;&#30431;&#19968;&#33324;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65288;GDPR&#65289;&#19979;&#30340;&#21024;&#38500;&#26435;&#12290;RTBF&#20801;&#35768;&#20010;&#20154;&#21521;&#32452;&#32455;&#35831;&#27714;&#21024;&#38500;&#20010;&#20154;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25628;&#32034;&#24341;&#25806;&#65292;&#20010;&#20154;&#21487;&#20197;&#21521;&#32452;&#32455;&#21457;&#36865;&#35831;&#27714;&#65292;&#25490;&#38500;&#20182;&#20204;&#30340;&#20449;&#24687;&#22312;&#26597;&#35810;&#32467;&#26524;&#20013;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21644;&#20854;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;LLM&#21551;&#29992;&#30340;&#36719;&#20214;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#34987;&#25490;&#38500;&#22312;RTBF&#20043;&#22806;&#12290;&#30456;&#27604;&#25628;&#32034;&#24341;&#25806;&#20351;&#29992;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;LLMs&#20197;&#19968;&#31181;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#22788;&#29702;&#20449;&#24687;&#65292;&#36825;&#20026;&#31526;&#21512;RTBF&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#20197;&#31526;&#21512;RTBF&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Right to be Forgotten (RTBF) was first established as the result of the ruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\'alez, and was later included as the Right to Erasure under the General Data Protection Regulation (GDPR) of European Union to allow individuals the right to request personal data be deleted by organizations. Specifically for search engines, individuals can send requests to organizations to exclude their information from the query results. With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF. In this paper, we explore these challenges and provide our insights on how to implement technical solutions for the RTBF, including the use of machine unle
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#37096;&#20998;LLMs&#22312;&#29305;&#23450;&#25552;&#31034;&#37197;&#32622;&#19979;&#27169;&#25311;&#30340;&#20154;&#26684;&#21487;&#38752;&#19988;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#22823;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#22609;&#36896;&#12290;</title><link>http://arxiv.org/abs/2307.00184</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;
&lt;/p&gt;
&lt;p&gt;
Personality Traits in Large Language Models. (arXiv:2307.00184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00184
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#37096;&#20998;LLMs&#22312;&#29305;&#23450;&#25552;&#31034;&#37197;&#32622;&#19979;&#27169;&#25311;&#30340;&#20154;&#26684;&#21487;&#38752;&#19988;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#22823;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#22609;&#36896;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20351;&#24471;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#19988;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25991;&#26412;&#12290;&#38543;&#30528;LLMs&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#39537;&#21160;&#23545;&#35805;&#20195;&#29702;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#35757;&#32451;&#22823;&#37327;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#33719;&#24471;&#30340;&#20154;&#26684;&#29305;&#36136;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#30001;&#20110;&#20154;&#26684;&#26159;&#20915;&#23450;&#20132;&#27969;&#25928;&#26524;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#39564;&#35777;&#30340;&#24515;&#29702;&#27979;&#37327;&#27979;&#35797;&#65292;&#24182;&#23545;&#20174;&#24191;&#27867;&#20351;&#29992;&#30340;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#36827;&#34892;&#37327;&#21270;&#12289;&#20998;&#26512;&#21644;&#22609;&#36896;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#26576;&#20123;LLMs&#30340;&#36755;&#20986;&#20013;&#27169;&#25311;&#30340;&#20154;&#26684;&#65288;&#22312;&#29305;&#23450;&#30340;&#25552;&#31034;&#37197;&#32622;&#19979;&#65289;&#26159;&#21487;&#38752;&#21644;&#26377;&#25928;&#30340;&#65307;2&#65289;LLM&#27169;&#25311;&#30340;&#20154;&#26684;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#35777;&#25454;&#23545;&#20110;&#26356;&#22823;&#30340;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#26356;&#24378;&#65307;3&#65289;LLM&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#30340;&#32500;&#24230;&#36827;&#34892;&#22609;&#36896;&#65292;&#20197;&#27169;&#20223;&#29305;&#23450;&#30340;&#20154;&#26684;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant text. As LLMs increasingly power conversational agents, the synthesized personality embedded in these models by virtue of their training on large amounts of human-generated data draws attention. Since personality is an important factor determining the effectiveness of communication, we present a comprehensive method for administering validated psychometric tests and quantifying, analyzing, and shaping personality traits exhibited in text generated from widely-used LLMs. We find that: 1) personality simulated in the outputs of some LLMs (under specific prompting configurations) is reliable and valid; 2) evidence of reliability and validity of LLM-simulated personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific personality profiles. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20449;&#24687;&#26816;&#32034;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;AVIS&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#38382;&#39064;&#25152;&#38656;&#30340;&#22806;&#37096;&#30693;&#35782;&#33719;&#21462;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.08129</link><description>&lt;p&gt;
AVIS:&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#35270;&#35273;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
AVIS: Autonomous Visual Information Seeking with Large Language Models. (arXiv:2306.08129v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20449;&#24687;&#26816;&#32034;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;AVIS&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#38382;&#39064;&#25152;&#38656;&#30340;&#22806;&#37096;&#30693;&#35782;&#33719;&#21462;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23454;&#29616;&#33258;&#20027;&#20449;&#24687;&#26816;&#32034;&#30340;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;AVIS&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLM&#21160;&#24577;&#22320;&#21046;&#23450;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#31574;&#30053;&#65292;&#24182;&#35843;&#26597;&#23427;&#20204;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#33719;&#21462;&#25552;&#20379;&#25152;&#25552;&#20986;&#38382;&#39064;&#25152;&#38656;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#30693;&#35782;&#12290;&#22238;&#31572;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#39064;&#65292;&#22914;&#8220;&#36825;&#24133;&#22270;&#20687;&#25152;&#25551;&#32472;&#30340;&#24314;&#31569;&#29289;&#26159;&#20026;&#20102;&#32426;&#24565;&#21738;&#20010;&#20107;&#20214;&#65311;&#8221;&#65292;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#21576;&#29616;&#20986;&#19968;&#20010;&#32452;&#21512;&#25628;&#32034;&#31354;&#38388;&#65292;&#38656;&#35201;&#19968;&#31995;&#21015;&#34892;&#21160;&#65292;&#21253;&#25324;&#35843;&#29992;API&#12289;&#20998;&#26512;&#23427;&#20204;&#30340;&#21709;&#24212;&#24182;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#29992;&#25143;&#30740;&#31350;&#65292;&#25910;&#38598;&#20102;&#20154;&#31867;&#38754;&#23545;&#36825;&#20010;&#20219;&#21153;&#26102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20915;&#31574;&#23454;&#20363;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#35774;&#35745;&#20102;&#19968;&#20010;&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#30340;&#31995;&#32479;&#65306;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#30830;&#23450;&#19979;&#19968;&#20010;&#35201;&#20351;&#29992;&#30340;&#24037;&#20855;&#65307;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#25512;&#29702;&#22120;&#65292;&#20998;&#26512;&#24182;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an autonomous information seeking visual question answering framework, AVIS. Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions. Responding to visual questions that necessitate external knowledge, such as "What event is commemorated by the building depicted in this image?", is a complex task. This task presents a combinatorial search space that demands a sequence of actions, including invoking APIs, analyzing their responses, and making informed decisions. We conduct a user study to collect a variety of instances of human decision-making when faced with this task. This data is then used to design a system comprised of three components: an LLM-powered planner that dynamically determines which tool to use next, an LLM-powered reasoner that analyzes and extracts key information 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.03341</link><description>&lt;p&gt;
&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#23548;&#20986;&#30495;&#23454;&#30340;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30495;&#23454;&#24615;&#12290;ITI&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27839;&#30528;&#19968;&#32452;&#26041;&#21521;&#31227;&#21160;&#27169;&#22411;&#28608;&#27963;&#65292;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#12290;&#36825;&#31181;&#24178;&#39044;&#26174;&#30528;&#25552;&#39640;&#20102;LLaMA&#27169;&#22411;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#25351;&#20196;&#24494;&#35843;&#30340;LLaMA Alpaca&#19978;&#65292;ITI&#23558;&#20854;&#30495;&#23454;&#24615;&#20174;32.5&#65285;&#25552;&#39640;&#21040;65.1&#65285;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30495;&#23454;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972;&#24178;&#39044;&#24378;&#24230;&#26469;&#24179;&#34913;&#23427;&#12290;ITI &#21462;&#24471;&#20102;&#26368;&#20302;&#31243;&#24230;&#30340;&#24178;&#25200;&#19988;&#35745;&#31639;&#24265;&#20215;&#12290;&#27492;&#22806;&#65292;&#35813;&#25216;&#26415;&#22312;&#25968;&#25454;&#25928;&#29575;&#19978;&#34920;&#29616;&#20248;&#24322;&#65306;&#34429;&#28982;&#20687;RLHF&#36825;&#26679;&#30340;&#26041;&#27861;&#38656;&#35201;&#24191;&#27867;&#27880;&#37322;&#65292;&#20294;&#26159;ITI&#20165;&#20351;&#29992;&#20102;&#20960;&#30334;&#20010;&#20363;&#23376;&#23601;&#33021;&#23450;&#20301;&#30495;&#23454;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#33021;&#20855;&#26377;&#26576;&#31181;&#20869;&#37096;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#26576;&#20107;&#26159;&#30495;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#34920;&#38754;&#19978;&#20135;&#29983;&#20102;&#34394;&#20551;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
&lt;/p&gt;</description></item><item><title>GENTLE&#26159;&#19968;&#20010;&#21253;&#21547;&#19981;&#21516;&#25991;&#20307;&#30340;&#33521;&#25991;NLP&#25361;&#25112;&#25968;&#25454;&#38598;&#65292;&#23545;&#20110;&#21508;&#31181;NLP&#20219;&#21153;&#65292;&#21253;&#25324;&#21477;&#27861;&#20381;&#36182;&#20998;&#26512;&#12289;&#23454;&#20307;&#35782;&#21035;&#12289;&#25351;&#20195;&#28040;&#35299;&#21644;&#31687;&#31456;&#20998;&#26512;&#65292;&#26368;&#20808;&#36827;&#30340;NLP&#31995;&#32479;&#22312;&#26576;&#20123;&#25991;&#20307;&#19978;&#24615;&#33021;&#20005;&#37325;&#38477;&#20302;&#65292;&#36825;&#34920;&#26126;GENTLE&#22312;NLP&#31995;&#32479;&#35780;&#20272;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01966</link><description>&lt;p&gt;
GENTLE: &#19968;&#20010;&#21253;&#21547;&#19981;&#21516;&#25991;&#20307;&#30340;&#22810;&#23618;&#27425;&#33521;&#25991;NLP&#21644;&#35821;&#35328;&#35780;&#20272;&#25361;&#25112;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
GENTLE: A Genre-Diverse Multilayer Challenge Set for English NLP and Linguistic Evaluation. (arXiv:2306.01966v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01966
&lt;/p&gt;
&lt;p&gt;
GENTLE&#26159;&#19968;&#20010;&#21253;&#21547;&#19981;&#21516;&#25991;&#20307;&#30340;&#33521;&#25991;NLP&#25361;&#25112;&#25968;&#25454;&#38598;&#65292;&#23545;&#20110;&#21508;&#31181;NLP&#20219;&#21153;&#65292;&#21253;&#25324;&#21477;&#27861;&#20381;&#36182;&#20998;&#26512;&#12289;&#23454;&#20307;&#35782;&#21035;&#12289;&#25351;&#20195;&#28040;&#35299;&#21644;&#31687;&#31456;&#20998;&#26512;&#65292;&#26368;&#20808;&#36827;&#30340;NLP&#31995;&#32479;&#22312;&#26576;&#20123;&#25991;&#20307;&#19978;&#24615;&#33021;&#20005;&#37325;&#38477;&#20302;&#65292;&#36825;&#34920;&#26126;GENTLE&#22312;NLP&#31995;&#32479;&#35780;&#20272;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;GENTLE&#65292;&#19968;&#20010;&#26032;&#30340;&#28151;&#21512;&#25991;&#20307;&#30340;&#33521;&#25991;&#25361;&#25112;&#35821;&#26009;&#24211;&#65292;&#20849;&#26377;17K&#20010;&#21333;&#35789;&#65292;&#21253;&#25324;8&#31181;&#38750;&#24120;&#35268;&#30340;&#25991;&#26412;&#31867;&#22411;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#35780;&#20272;&#65306;&#23383;&#20856;&#26465;&#30446;&#12289;&#30005;&#23376;&#31454;&#25216;&#35780;&#35770;&#12289;&#27861;&#24459;&#25991;&#20214;&#12289;&#21307;&#23398;&#31508;&#35760;&#12289;&#35799;&#27468;&#12289;&#25968;&#23398;&#35777;&#26126;&#12289;&#25945;&#23398;&#22823;&#32434;&#21644;&#23041;&#32961;&#20449;&#20989;&#12290;GENTLE&#25163;&#21160;&#27880;&#37322;&#20102;&#22810;&#31181;&#27969;&#34892;&#30340;NLP&#20219;&#21153;&#65292;&#21253;&#25324;&#21477;&#27861;&#20381;&#36182;&#20998;&#26512;&#12289;&#23454;&#20307;&#35782;&#21035;&#12289;&#25351;&#20195;&#28040;&#35299;&#21644;&#31687;&#31456;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;GENTLE&#19978;&#35780;&#20215;&#20102;&#26368;&#20808;&#36827;&#30340;NLP&#31995;&#32479;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#23545;&#26576;&#20123;&#25991;&#20307;&#30340;&#24615;&#33021;&#20986;&#29616;&#20005;&#37325;&#38477;&#20302;&#65292;&#36825;&#34920;&#26126;GENTLE&#20316;&#20026;NLP&#31995;&#32479;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present GENTLE, a new mixed-genre English challenge corpus totaling 17K tokens and consisting of 8 unusual text types for out-of domain evaluation: dictionary entries, esports commentaries, legal documents, medical notes, poetry, mathematical proofs, syllabuses, and threat letters. GENTLE is manually annotated for a variety of popular NLP tasks, including syntactic dependency parsing, entity recognition, coreference resolution, and discourse parsing. We evaluate state-of-the-art NLP systems on GENTLE and find severe degradation for at least some genres in their performance on all tasks, which indicates GENTLE's utility as an evaluation dataset for NLP systems.
&lt;/p&gt;</description></item><item><title>FACE&#26159;&#19968;&#32452;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#20154;&#31867;&#21644;&#27169;&#22411;&#20043;&#38388;&#24046;&#36317;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#23427;&#22522;&#20110;&#20613;&#37324;&#21494;&#20998;&#26512;&#21644;&#20132;&#21449;&#29109;&#20272;&#35745;&#65292;&#21487;&#20197;&#21453;&#26144;&#27169;&#22411;&#22823;&#23567;&#12289;&#35299;&#30721;&#37319;&#26679;&#26041;&#27861;&#21644;&#20154;&#31867;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.10307</link><description>&lt;p&gt;
FACE: &#20351;&#29992;&#20132;&#21449;&#29109;&#30340;&#20613;&#37324;&#21494;&#20998;&#26512;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy. (arXiv:2305.10307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10307
&lt;/p&gt;
&lt;p&gt;
FACE&#26159;&#19968;&#32452;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#20154;&#31867;&#21644;&#27169;&#22411;&#20043;&#38388;&#24046;&#36317;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#23427;&#22522;&#20110;&#20613;&#37324;&#21494;&#20998;&#26512;&#21644;&#20132;&#21449;&#29109;&#20272;&#35745;&#65292;&#21487;&#20197;&#21453;&#26144;&#27169;&#22411;&#22823;&#23567;&#12289;&#35299;&#30721;&#37319;&#26679;&#26041;&#27861;&#21644;&#20154;&#31867;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26426;&#22120;&#29983;&#25104;&#30340;&#35821;&#35328;&#19982;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#30340;&#36317;&#31163;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#21463;&#21040;&#35821;&#35328;&#23398;&#24515;&#29702;&#23398;&#20851;&#20110;&#35821;&#35328;&#29109;&#21608;&#26399;&#24615;&#23454;&#35777;&#21457;&#29616;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FACE&#8212;&#8212;&#19968;&#32452;&#22522;&#20110;&#35821;&#35328;&#20132;&#21449;&#29109;&#30340;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#34913;&#37327;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#30340;&#35821;&#35328;&#19982;&#20154;&#31867;&#20070;&#20889;&#35821;&#35328;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#24320;&#25918;&#24335;&#30340;&#29983;&#25104;&#20219;&#21153;&#21644;&#20197;&#21069;&#30740;&#31350;&#30340;&#23454;&#39564;&#25968;&#25454;&#65292;&#25105;&#20204;&#21457;&#29616;FACE&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#20154;&#31867;&#27169;&#22411;&#24046;&#36317;&#65292;&#22312;&#27169;&#22411;&#35268;&#27169;&#19978;&#26377;&#25152;&#32553;&#25918;&#65292;&#21453;&#26144;&#20102;&#19981;&#21516;&#35299;&#30721;&#37319;&#26679;&#26041;&#27861;&#30340;&#32467;&#26524;&#65292;&#19982;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#21028;&#26029;&#20998;&#25968;&#30456;&#20851;&#33391;&#22909;&#12290;FACE&#22312;&#35745;&#31639;&#19978;&#26159;&#39640;&#25928;&#30340;&#65292;&#24182;&#25552;&#20379;&#30452;&#35266;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring the distance between machine-produced and human language is acritical open problem. Inspired by empirical findings from psycholinguistics on theperiodicity of entropy in language, we propose FACE, a set of metrics based onFourier Analysis of the estimated Cross-Entropy of language, for measuring thesimilarity between model-generated and human-written languages. Based on anopen-ended generation task and the experimental data from previous studies, weind that FACE can effectively identify the human-model gap, scales with modelsize, reflects the outcomes of different sampling methods for decoding, correlateswell with other evaluation metrics and with human judgment scores. FACE iscomputationally efficient and provides intuitive interpretations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06569</link><description>&lt;p&gt;
&#22914;&#20309;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#32034;&#24341;&#39033;&#30446;ID
&lt;/p&gt;
&lt;p&gt;
How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23558;&#25512;&#33616;&#20219;&#21153;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25512;&#33616;&#12290;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#24314;&#35758;&#30340;&#39033;&#30446;&#32780;&#19981;&#26159;&#35745;&#31639;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#20013;&#27599;&#20010;&#20505;&#36873;&#39033;&#30446;&#30340;&#25490;&#21517;&#24471;&#20998;&#65292;&#31616;&#21270;&#20102;&#25512;&#33616;&#31649;&#36947;&#65292;&#36991;&#20813;&#20102;&#22810;&#27573;&#36807;&#28388;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#20915;&#23450;&#35201;&#25512;&#33616;&#21738;&#20123;&#39033;&#30446;&#26102;&#29983;&#25104;&#36807;&#38271;&#30340;&#25991;&#26412;&#65292;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#21019;&#24314;LLM&#20860;&#23481;&#30340;&#39033;&#30446;ID&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#65292;&#20197;P5&#20026;&#20195;&#34920;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32034;&#24341;&#26041;&#27861;&#22797;&#21046;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#20960;&#31181;&#24494;&#19981;&#36275;&#36947;&#30340;&#39033;&#30446;&#32034;&#24341;&#26041;&#27861;&#65288;&#22914;&#29420;&#31435;&#32034;&#24341;&#12289;&#26631;&#39064;&#32034;&#24341;&#21644;&#38543;&#26426;&#32034;&#24341;&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#32034;&#24341;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32034;&#24341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#20110;&#24494;&#35843;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20256;&#32479;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#19981;&#33021;&#20445;&#35777;&#19968;&#33268;&#30340;&#25552;&#39640;&#24615;&#33021;&#65292;&#29978;&#33267;&#20250;&#23545;&#19968;&#20123;&#20219;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#26088;&#22312;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#21521;LM&#23637;&#31034;&#20219;&#21153;&#30456;&#20851;&#25991;&#26412;&#21644;&#25552;&#31034;&#27169;&#26495;&#65292;&#20174;&#32780;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01711</link><description>&lt;p&gt;
&#19981;&#20572;&#27490;&#39044;&#35757;&#32451;&#65311;&#35753;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26356;&#21152;&#24378;&#22823;
&lt;/p&gt;
&lt;p&gt;
Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner. (arXiv:2305.01711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#20110;&#24494;&#35843;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20256;&#32479;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#19981;&#33021;&#20445;&#35777;&#19968;&#33268;&#30340;&#25552;&#39640;&#24615;&#33021;&#65292;&#29978;&#33267;&#20250;&#23545;&#19968;&#20123;&#20219;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#26088;&#22312;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#21521;LM&#23637;&#31034;&#20219;&#21153;&#30456;&#20851;&#25991;&#26412;&#21644;&#25552;&#31034;&#27169;&#26495;&#65292;&#20174;&#32780;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#26080;&#26631;&#27880;&#25968;&#25454;&#30340;&#35757;&#32451;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290; &#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;NLP&#20013;&#24191;&#20026;&#25509;&#21463;&#30340;LM&#20219;&#21153;&#30456;&#20851;&#25991;&#26412;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24494;&#35843;&#24615;&#33021;&#30340;&#29702;&#35770;&#12290;&#36890;&#36807;&#22312;&#21322;&#30417;&#30563;&#21644;&#20840;&#30417;&#30563;&#35774;&#32622;&#19979;&#23545;&#20843;&#20010;&#21333;&#21477;&#20219;&#21153;&#21644;&#20843;&#20010;&#21477;&#23545;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#19981;&#33021;&#20445;&#35777;&#19968;&#33268;&#30340;&#25552;&#39640;&#24615;&#33021;&#65292;&#29978;&#33267;&#21487;&#33021;&#23545;&#21477;&#23545;&#20219;&#21153;&#25110;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26041;&#24335;&#26102;&#20250;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65288;PCP&#65289;&#65292;&#23558;&#25351;&#23548;&#35843;&#25972;&#30340;&#24605;&#24819;&#19982;&#20256;&#32479;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#22312;&#24494;&#35843;&#30446;&#26631;&#20043;&#21069;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#21521;LM&#23637;&#31034;&#20219;&#21153;&#30456;&#20851;&#25991;&#26412;&#21644;&#25552;&#31034;&#27169;&#26495;&#65292;&#20174;&#32780;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;FT&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) trained on vast quantities of unlabelled data have greatly advanced the field of natural language processing (NLP). In this study, we re-visit the widely accepted notion in NLP that continued pre-training LMs on task-related texts improves the performance of fine-tuning (FT) in downstream tasks. Through experiments on eight single-sentence tasks and eight sentence-pair tasks in both semi-supervised and fully-supervised settings, we find that conventional continued pre-training does not consistently provide benefits and can even be detrimental for sentence-pair tasks or when prompt-based FT is used. To tackle these issues, we propose Prompt-based Continued Pre-training (PCP), which combines the idea of instruction tuning with conventional continued pre-training. Our approach aims to improve the performance of prompt-based FT by presenting both task-related texts and prompt templates to LMs through unsupervised pre-training objectives before fine-tuning for the targ
&lt;/p&gt;</description></item><item><title>CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.00969</link><description>&lt;p&gt;
CryCeleb: &#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00969
&lt;/p&gt;
&lt;p&gt;
CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;Ubenwa CryCeleb&#25968;&#25454;&#38598;&#8212;&#8212;&#19968;&#20010;&#26631;&#35760;&#30340;&#23156;&#20799;&#21741;&#22768;&#25910;&#38598;&#65292;&#20197;&#21450;&#38468;&#24102;&#30340;CryCeleb 2023&#20219;&#21153;&#8212;&#8212;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#20844;&#20849;&#35828;&#35805;&#20154;&#39564;&#35777;&#25361;&#25112;&#12290;&#25105;&#20204;&#37322;&#25918;&#20986;786&#21517;&#26032;&#29983;&#20799;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#20197;&#40723;&#21169;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the Ubenwa CryCeleb dataset - a labeled collection of infant cries, and the accompanying CryCeleb 2023 task - a public speaker verification challenge based on infant cry sounds. We release for academic usage more than 6 hours of manually segmented cry sounds from 786 newborns to encourage research in infant cry analysis.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SearChain&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#36861;&#28335;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;SearChain&#36890;&#36807;&#28145;&#24230;&#38598;&#25104;LLM&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#23454;&#29616;&#65292;&#20854;&#24605;&#36335;&#26159;&#36890;&#36807;&#26500;&#36896;&#26597;&#35810;&#38142;&#65292;&#23558;&#22810;&#36339;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#65292;&#26368;&#32456;&#25351;&#23548;LLM&#29983;&#25104;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.14732</link><description>&lt;p&gt;
&#22522;&#20110;SearChain&#30340;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#31934;&#30830;&#12289;&#21487;&#20449;&#21644;&#21487;&#36861;&#28335;&#20869;&#23481;&#29983;&#25104;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks. (arXiv:2304.14732v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SearChain&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#36861;&#28335;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;SearChain&#36890;&#36807;&#28145;&#24230;&#38598;&#25104;LLM&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#23454;&#29616;&#65292;&#20854;&#24605;&#36335;&#26159;&#36890;&#36807;&#26500;&#36896;&#26597;&#35810;&#38142;&#65292;&#23558;&#22810;&#36339;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#65292;&#26368;&#32456;&#25351;&#23548;LLM&#29983;&#25104;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#22914;&#20309;&#20351;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#20934;&#30830;&#21487;&#20449;&#22312;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Search-in-the-Chain&#65288;SearChain&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#31561;&#20856;&#22411;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#36861;&#28335;&#24615;&#12290;SearChain&#26159;&#19968;&#20010;&#28145;&#24230;&#38598;&#25104;LLM&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#30340;&#26694;&#26550;&#12290;&#22312;SearChain&#20013;&#65292;LLM&#26500;&#24314;&#26597;&#35810;&#38142;&#65292;&#20316;&#20026;&#22810;&#36339;&#38382;&#39064;&#30340;&#20998;&#35299;&#12290;&#38142;&#30340;&#27599;&#20010;&#33410;&#28857;&#37117;&#26159;&#30001;IR&#23548;&#21521;&#30340;&#26597;&#35810;-&#31572;&#26696;&#23545;&#65292;&#20197;&#21450;&#30001;LLM&#29983;&#25104;&#30340;&#35813;&#26597;&#35810;&#30340;&#31572;&#26696;&#12290;IR&#39564;&#35777;&#12289;&#23436;&#21892;&#21644;&#36319;&#36394;&#38142;&#20013;&#27599;&#20010;&#33410;&#28857;&#30340;&#20449;&#24687;&#65292;&#20197;&#25351;&#23548;LLM&#26500;&#24314;&#27491;&#30830;&#30340;&#26597;&#35810;&#38142;&#65292;&#24182;&#26368;&#32456;&#22238;&#31572;&#22810;&#36339;&#38382;&#39064;&#12290;SearChain&#20351;LLM&#20174;&#19968;&#27425;&#24615;&#31572;&#26696;&#36716;&#21464;&#20026;&#22810;&#27493;&#31572;&#26696;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SearChain&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the wide application of Large Language Models (LLMs) such as ChatGPT, how to make the contents generated by LLM accurate and credible becomes very important, especially in complex knowledge-intensive tasks. In this paper, we propose a novel framework called Search-in-the-Chain (SearChain) to improve the accuracy, credibility and traceability of LLM-generated content for multi-hop question answering, which is a typical complex knowledge-intensive task. SearChain is a framework that deeply integrates LLM and information retrieval (IR). In SearChain, LLM constructs a chain-of-query, which is the decomposition of the multi-hop question. Each node of the chain is a query-answer pair consisting of an IR-oriented query and the answer generated by LLM for this query. IR verifies, completes, and traces the information of each node of the chain, so as to guide LLM to construct the correct chain-of-query, and finally answer the multi-hop question. SearChain makes LLM change from trying to gi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#31867;&#22312;&#23398;&#20064;&#21644;&#25512;&#24191;&#19981;&#21516;&#32467;&#26500;&#31243;&#24230;&#30340;&#35821;&#35328;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#22312;&#31995;&#32479;&#21270;&#27010;&#25324;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#23545;&#20110;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#23398;&#20064;&#21644;&#36827;&#21270;&#26500;&#25104;&#20102;&#19968;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.12239</link><description>&lt;p&gt;
&#32534;&#31243;&#20160;&#20040;&#20351;&#19968;&#31181;&#35821;&#35328;&#26131;&#20110;&#28145;&#24230;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Makes a Language Easy to Deep-Learn?. (arXiv:2302.12239v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#31867;&#22312;&#23398;&#20064;&#21644;&#25512;&#24191;&#19981;&#21516;&#32467;&#26500;&#31243;&#24230;&#30340;&#35821;&#35328;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#22312;&#31995;&#32479;&#21270;&#27010;&#25324;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#23545;&#20110;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#23398;&#20064;&#21644;&#36827;&#21270;&#26500;&#25104;&#20102;&#19968;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25104;&#21151;&#12290;&#35821;&#35328;&#30340;&#19968;&#20010;&#22522;&#26412;&#23646;&#24615;&#26159;&#20854;&#32452;&#25104;&#32467;&#26500;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#31995;&#32479;&#22320;&#20135;&#29983;&#26032;&#30340;&#24847;&#20041;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#19981;&#21516;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#31995;&#32479;&#21270;&#27010;&#25324;&#26041;&#38754;&#19968;&#30452;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#19988;&#22312;&#26032;&#20852;&#36890;&#20449;&#27169;&#25311;&#20013;&#19981;&#19968;&#23450;&#21463;&#30410;&#20110;&#32452;&#25104;&#32467;&#26500;&#12290;&#36825;&#23545;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#23398;&#20064;&#21644;&#36827;&#21270;&#26500;&#25104;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#26263;&#31034;&#20102;&#19981;&#21516;&#23398;&#20064;&#31995;&#32479;&#30340;&#20559;&#35265;&#30340;&#20851;&#38190;&#24046;&#24322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30452;&#25509;&#27979;&#35797;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#21644;&#27010;&#25324;&#19981;&#21516;&#36755;&#20837;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#35821;&#35328;&#22312;&#20854;&#32467;&#26500;&#31243;&#24230;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-3.5&#65288;&#31867;&#20284;&#20110;&#25104;&#24180;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#65289;&#21644;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;&#31867;&#20284;&#20110;&#20799;&#31461;&#31532;&#19968;&#35821;&#35328;&#23398;&#20064;&#32773;&#65289;&#30340;&#35760;&#24518;&#21644;&#27010;&#25324;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#20196;&#20154;&#38663;&#24778;&#30340;
&lt;/p&gt;
&lt;p&gt;
Neural networks drive the success of natural language processing. A fundamental property of language is its compositional structure, allowing humans to produce forms for new meanings systematically. However, unlike humans, neural networks notoriously struggle with systematic generalization, and do not necessarily benefit from compositional structure in emergent communication simulations. This poses a problem for using neural networks to simulate human language learning and evolution, and suggests crucial differences in the biases of the different learning systems. Here, we directly test how neural networks compare to humans in learning and generalizing different input languages that vary in their degree of structure. We evaluate the memorization and generalization capabilities of a pre-trained language model GPT-3.5 (analagous to an adult second language learner) and recurrent neural networks trained from scratch (analaogous to a child first language learner). Our results show striking
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#23545;&#35805;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#30446;&#26631;&#39118;&#26684;&#20013;&#30340;&#23569;&#37327;&#31034;&#20363;&#23545;&#35805;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#32771;&#34385;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39118;&#26684;&#36716;&#31227;&#65292;&#30456;&#27604;&#20110;&#21477;&#23376;&#32423;&#39118;&#26684;&#36716;&#31227;&#65292;&#35813;&#26041;&#27861;&#22312;&#24688;&#24403;&#24615;&#21644;&#35821;&#20041;&#27491;&#30830;&#24615;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.08362</link><description>&lt;p&gt;
&#20351;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#23545;&#35805;&#39118;&#26684;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Conversation Style Transfer using Few-Shot Learning. (arXiv:2302.08362v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#23545;&#35805;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#30446;&#26631;&#39118;&#26684;&#20013;&#30340;&#23569;&#37327;&#31034;&#20363;&#23545;&#35805;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#32771;&#34385;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39118;&#26684;&#36716;&#31227;&#65292;&#30456;&#27604;&#20110;&#21477;&#23376;&#32423;&#39118;&#26684;&#36716;&#31227;&#65292;&#35813;&#26041;&#27861;&#22312;&#24688;&#24403;&#24615;&#21644;&#35821;&#20041;&#27491;&#30830;&#24615;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;&#20391;&#37325;&#20110;&#21477;&#23376;&#32423;&#30340;&#39118;&#26684;&#36716;&#31227;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#19988;&#39118;&#26684;&#26159;&#36890;&#36807;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#27491;&#24335;&#31243;&#24230;&#65289;&#26469;&#25551;&#36848;&#30340;&#12290;&#24403;&#23558;&#39118;&#26684;&#36716;&#31227;&#24212;&#29992;&#20110;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31561;&#23545;&#35805;&#26102;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21463;&#21040;&#36825;&#20123;&#38480;&#21046;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#19978;&#19979;&#25991;&#21487;&#20197;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#32780;&#23545;&#35805;&#20013;&#30340;&#39118;&#26684;&#23646;&#24615;&#24448;&#24448;&#24456;&#38590;&#23450;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23545;&#35805;&#39118;&#26684;&#36716;&#31227;&#24341;&#20837;&#20026;&#19968;&#20010;&#23569;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#65292;&#27169;&#22411;&#36890;&#36807;&#35266;&#23519;&#30446;&#26631;&#39118;&#26684;&#20013;&#30340;&#23569;&#37327;&#31034;&#20363;&#23545;&#35805;&#26469;&#36827;&#34892;&#39118;&#26684;&#36716;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#23558;&#19981;&#24102;&#39118;&#26684;&#30340;&#23545;&#35805;&#20316;&#20026;&#20013;&#36716;&#12290;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#21305;&#37197;&#30446;&#26631;&#39118;&#26684;&#65292;&#21516;&#26102;&#19982;&#35821;&#21477;/&#21477;&#23376;&#32423;&#39118;&#26684;&#36716;&#31227;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24688;&#24403;&#24615;&#21644;&#35821;&#20041;&#27491;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23545;&#35805;&#39118;&#26684;&#36716;&#31227;&#25216;&#26415;&#30340;&#24212;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional text style transfer approaches focus on sentence-level style transfer without considering contextual information, and the style is described with attributes (e.g., formality). When applying style transfer in conversations such as task-oriented dialogues, existing approaches suffer from these limitations as context can play an important role and the style attributes are often difficult to define in conversations. In this paper, we introduce conversation style transfer as a few-shot learning problem, where the model learns to perform style transfer by observing only a few example dialogues in the target style. We propose a novel in-context learning approach to solve the task with style-free dialogues as a pivot. Human evaluation shows that by incorporating multi-turn context, the model is able to match the target style while having better appropriateness and semantic correctness compared to utterance/sentence-level style transfer. Additionally, we show that conversation styl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#21457;&#29616;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#22823;&#20844;&#21496;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#21333;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#21464;&#21270;&#12290;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#35821;&#26009;&#24211;&#22823;&#23567;&#12289;&#36136;&#37327;&#21644;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2212.08390</link><description>&lt;p&gt;
&#20174;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#24471;&#20986;&#30340;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Lessons learned from the evaluation of Spanish Language Models. (arXiv:2212.08390v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08390
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#21457;&#29616;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#22823;&#20844;&#21496;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#21333;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#21464;&#21270;&#12290;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#35821;&#26009;&#24211;&#22823;&#23567;&#12289;&#36136;&#37327;&#21644;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#24433;&#21709;&#65292;&#24050;&#32463;&#35757;&#32451;&#24182;&#21457;&#24067;&#20102;&#19968;&#20123;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#35199;&#29677;&#29273;&#35821;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;BERT&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#35201;&#20040;&#26159;&#22312;&#20351;&#29992;&#38750;&#24120;&#22823;&#30340;&#31169;&#26377;&#35821;&#26009;&#24211;&#30340;&#22823;&#22411;&#39033;&#30446;&#20013;&#24320;&#21457;&#30340;&#65292;&#35201;&#20040;&#26159;&#36890;&#36807;&#21033;&#29992;&#20813;&#36153;&#21487;&#29992;&#25968;&#25454;&#30340;&#23567;&#35268;&#27169;&#23398;&#26415;&#24037;&#20316;&#24320;&#21457;&#30340;&#12290;&#26412;&#25991;&#23545;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27604;&#36739;&#65292;&#24471;&#20986;&#20197;&#19979;&#32467;&#26524;&#65306;&#65288;i&#65289;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#22823;&#20844;&#21496;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#21333;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#21464;&#21270;&#65307;&#65288;ii&#65289;&#21333;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26524;&#24182;&#19981;&#26126;&#30830;&#65292;&#25454;&#35828;&#26356;&#23567;&#19988;&#26356;&#24046;&#30340;&#27169;&#22411;&#20063;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#22522;&#20110;&#36825;&#20123;&#23454;&#35777;&#32467;&#26524;&#65292;&#25105;&#20204;&#20027;&#24352;&#38656;&#35201;&#26356;&#22810;&#30340;&#30740;&#31350;&#26469;&#29702;&#35299;&#20854;&#32972;&#21518;&#30340;&#22240;&#32032;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#35821;&#26009;&#24211;&#30340;&#22823;&#23567;&#12289;&#36136;&#37327;&#21644;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#24433;&#21709;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the impact of language models on the field of Natural Language Processing, a number of Spanish encoder-only masked language models (aka BERTs) have been trained and released. These models were developed either within large projects using very large private corpora or by means of smaller scale academic efforts leveraging freely available data. In this paper we present a comprehensive head-to-head comparison of language models for Spanish with the following results: (i) Previously ignored multilingual models from large companies fare better than monolingual models, substantially changing the evaluation landscape of language models in Spanish; (ii) Results across the monolingual models are not conclusive, with supposedly smaller and inferior models performing competitively. Based on these empirical results, we argue for the need of more research to understand the factors underlying them. In this sense, the effect of corpus size, quality and pre-training techniques need to be further
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#21307;&#30103;&#39046;&#22495;&#20250;&#35805;&#30005;&#35805;&#35821;&#38899;&#30340;&#28151;&#21512;ASR&#31995;&#32479;&#65292;&#26088;&#22312;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#24182;&#20811;&#26381;&#35821;&#35328;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2210.13397</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#21307;&#30103;&#39046;&#22495;&#20250;&#35805;&#30005;&#35805;&#35821;&#38899;&#30340;&#28151;&#21512;ASR&#31995;&#32479;&#30340;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Development of Hybrid ASR Systems for Low Resource Medical Domain Conversational Telephone Speech. (arXiv:2210.13397v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#21307;&#30103;&#39046;&#22495;&#20250;&#35805;&#30005;&#35805;&#35821;&#38899;&#30340;&#28151;&#21512;ASR&#31995;&#32479;&#65292;&#26088;&#22312;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#24182;&#20811;&#26381;&#35821;&#35328;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#38556;&#30861;&#22312;&#25105;&#20204;&#26085;&#30410;&#36830;&#25509;&#21644;&#20840;&#29699;&#21270;&#30340;&#19990;&#30028;&#20013;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#23588;&#20854;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#22914;&#21307;&#38498;&#25110;&#24613;&#35786;&#23460;&#65292;&#27807;&#36890;&#22256;&#38590;&#21644;&#24310;&#35823;&#21487;&#33021;&#23548;&#33268;&#21307;&#30103;&#22833;&#35823;&#21644;&#38750;&#26368;&#20339;&#30340;&#24739;&#32773;&#25252;&#29702;&#12290;&#22312;HYKIST&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;&#24503;&#35821;&#21307;&#29983;&#21644;&#38463;&#25289;&#20271;&#35821;&#25110;&#36234;&#21335;&#35821;&#24739;&#32773;&#20043;&#38388;&#30340;&#20132;&#27969;&#12290;&#30446;&#21069;&#65292;&#21307;&#29983;&#21487;&#20197;&#25171;&#30005;&#35805;&#32473;Triaphon&#26381;&#21153;&#20197;&#33719;&#24471;&#26469;&#33258;&#32763;&#35793;&#21592;&#30340;&#24110;&#21161;&#65292;&#20197;&#20419;&#36827;&#27807;&#36890;&#12290;HYKIST&#30340;&#30446;&#26631;&#26159;&#20026;&#36890;&#24120;&#27809;&#26377;&#19987;&#19994;&#32972;&#26223;&#30340;&#21452;&#35821;&#32763;&#35793;&#21592;&#25552;&#20379;&#33258;&#21160;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#65292;&#20197;&#25913;&#36827;&#24739;&#32773;&#25252;&#29702;&#24182;&#35299;&#20915;&#35821;&#35328;&#38556;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#38024;&#23545;&#21307;&#30103;&#39046;&#22495;&#20250;&#35805;&#30005;&#35805;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#30340;ASR&#31995;&#32479;&#24320;&#21457;&#24037;&#20316;&#65292;&#28041;&#21450;&#20004;&#31181;&#35821;&#35328;&#23545;&#30340;&#25968;&#25454;&#25910;&#38598;&#12289;&#21508;&#31181;&#22768;&#23398;&#27169;&#22411;&#26550;&#26500;&#21644;&#26041;&#35328;&#24341;&#36215;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language barriers present a great challenge in our increasingly connected and global world. Especially within the medical domain, e.g. hospital or emergency room, communication difficulties and delays may lead to malpractice and non-optimal patient care. In the HYKIST project, we consider patient-physician communication, more specifically between a German-speaking physician and an Arabic- or Vietnamese-speaking patient. Currently, a doctor can call the Triaphon service to get assistance from an interpreter in order to help facilitate communication. The HYKIST goal is to support the usually non-professional bilingual interpreter with an automatic speech translation system to improve patient care and help overcome language barriers. In this work, we present our ASR system development efforts for this conversational telephone speech translation task in the medical domain for two languages pairs, data collection, various acoustic model architectures and dialect-induced difficulties.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#24179;&#28369;&#34164;&#21547;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20256;&#36882;&#38142;&#26465;&#21644;&#20351;&#29992;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#25214;&#21040;&#20002;&#22833;&#30340;&#21069;&#25552;&#35859;&#35789;&#30340;&#36817;&#20284;&#65292;&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#30340;&#21484;&#22238;&#29575;&#21644;&#24179;&#22343;&#31934;&#24230;&#65292;&#24182;&#20445;&#25345;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.00318</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#24179;&#28369;&#34164;&#21547;&#22270;
&lt;/p&gt;
&lt;p&gt;
Smoothing Entailment Graphs with Language Models. (arXiv:2208.00318v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#24179;&#28369;&#34164;&#21547;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20256;&#36882;&#38142;&#26465;&#21644;&#20351;&#29992;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#25214;&#21040;&#20002;&#22833;&#30340;&#21069;&#25552;&#35859;&#35789;&#30340;&#36817;&#20284;&#65292;&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#30340;&#21484;&#22238;&#29575;&#21644;&#24179;&#22343;&#31934;&#24230;&#65292;&#24182;&#20445;&#25345;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#35859;&#35789;&#22312;&#35821;&#26009;&#24211;&#20013;&#30340;&#22810;&#26679;&#24615;&#21644;Zipf&#20998;&#24067;&#23548;&#33268;&#36890;&#36807;&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;&#26500;&#24314;&#30340;&#34164;&#21547;&#22270;&#65288;EGs&#65289;&#30340;&#31232;&#30095;&#24615;&#12290;EGs&#26159;&#35745;&#31639;&#39640;&#25928;&#21644;&#21487;&#35299;&#37322;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#20294;&#20316;&#20026;&#31526;&#21495;&#27169;&#22411;&#65292;&#22914;&#26524;&#27979;&#35797;&#26102;&#32570;&#23569;&#26032;&#30340;&#21069;&#25552;&#25110;&#20551;&#35774;&#39030;&#28857;&#65292;&#23427;&#20204;&#20250;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#26381;&#36825;&#31181;&#31526;&#21495;&#27169;&#22411;&#31232;&#30095;&#24615;&#30340;&#29702;&#35770;&#21644;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#26500;&#24314;&#20256;&#36882;&#38142;&#26465;&#26469;&#26500;&#24314;EGs&#30340;&#26368;&#20248;&#24179;&#28369;&#29702;&#35770;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#25214;&#21040;&#20002;&#22833;&#30340;&#21069;&#25552;&#35859;&#35789;&#30340;&#36817;&#20284;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#39640;&#25928;&#12289;&#24320;&#25918;&#22495;&#21644;&#26080;&#30417;&#30563;&#30340;&#24179;&#28369;&#26041;&#27861;&#12290;&#22312;&#20004;&#20010;&#22256;&#38590;&#30340;&#23450;&#21521;&#34164;&#21547;&#25968;&#25454;&#38598;&#19978;&#65292;&#36825;&#25552;&#39640;&#20102;25.1&#21644;16.3&#20010;&#30334;&#20998;&#28857;&#30340;&#21484;&#22238;&#29575;&#65292;&#24182;&#25552;&#39640;&#20102;&#24179;&#22343;&#31934;&#24230;&#24182;&#20445;&#25345;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#19968;&#20010;QA&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;EG&#24179;&#28369;&#22312;&#22238;&#31572;&#25903;&#25345;&#36739;&#23569;&#30340;&#38382;&#39064;&#26102;&#30340;&#26368;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The diversity and Zipfian frequency distribution of natural language predicates in corpora leads to sparsity in Entailment Graphs (EGs) built by Open Relation Extraction (ORE). EGs are computationally efficient and explainable models of natural language inference, but as symbolic models, they fail if a novel premise or hypothesis vertex is missing at test-time. We present theory and methodology for overcoming such sparsity in symbolic models. First, we introduce a theory of optimal smoothing of EGs by constructing transitive chains. We then demonstrate an efficient, open-domain, and unsupervised smoothing method using an off-the-shelf Language Model to find approximations of missing premise predicates. This improves recall by 25.1 and 16.3 percentage points on two difficult directional entailment datasets, while raising average precision and maintaining model explainability. Further, in a QA task we show that EG smoothing is most useful for answering questions with lesser supporting te
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MedLane&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#33258;&#21160;&#20020;&#24202;&#35821;&#35328;&#31616;&#21270;&#26041;&#27861;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;DECLARE&#30340;&#26032;&#27169;&#22411;&#65292;&#19982;&#20843;&#31181;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#29305;&#23450;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2012.02420</link><description>&lt;p&gt;
&#35780;&#20272;&#33258;&#21160;&#20020;&#24202;&#35821;&#35328;&#31616;&#21270;&#30340;&#22522;&#20934;&#65306;&#25968;&#25454;&#38598;&#12289;&#31639;&#27861;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Automated Clinical Language Simplification: Dataset, Algorithm, and Evaluation. (arXiv:2012.02420v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.02420
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MedLane&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#33258;&#21160;&#20020;&#24202;&#35821;&#35328;&#31616;&#21270;&#26041;&#27861;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;DECLARE&#30340;&#26032;&#27169;&#22411;&#65292;&#19982;&#20843;&#31181;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#29305;&#23450;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20581;&#24247;&#32032;&#20859;&#30340;&#24739;&#32773;&#36890;&#24120;&#24456;&#38590;&#29702;&#35299;&#21307;&#23398;&#26415;&#35821;&#21644;&#19987;&#19994;&#21307;&#23398;&#35821;&#35328;&#30340;&#22797;&#26434;&#32467;&#26500;&#12290;&#23613;&#31649;&#26377;&#19968;&#20123;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#21160;&#23558;&#19987;&#19994;&#35821;&#35328;&#32763;&#35793;&#25104;&#26222;&#36890;&#20154;&#21487;&#20197;&#29702;&#35299;&#30340;&#35821;&#35328;&#65292;&#20294;&#20854;&#20013;&#21482;&#26377;&#23569;&#25968;&#20851;&#27880;&#20102;&#20020;&#24202;&#39046;&#22495;&#20013;&#20934;&#30830;&#24615;&#21644;&#21487;&#35835;&#24615;&#30340;&#20004;&#20010;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#20020;&#24202;&#35821;&#35328;&#30340;&#31616;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20294;&#21487;&#24796;&#30340;&#26159;&#65292;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#23578;&#26410;&#23436;&#20840;&#35299;&#20915;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#39033;&#20219;&#21153;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MedLane&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#33258;&#21160;&#20020;&#24202;&#35821;&#35328;&#31616;&#21270;&#26041;&#27861;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DECLARE&#30340;&#26032;&#27169;&#22411;&#65292;&#36981;&#24490;&#20154;&#24037;&#27880;&#37322;&#36807;&#31243;&#65292;&#19982;&#20843;&#31181;&#24378;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20844;&#24179;&#35780;&#20272;&#24615;&#33021;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19977;&#20010;&#29305;&#23450;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;MedLane&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patients with low health literacy usually have difficulty understanding medical jargon and the complex structure of professional medical language. Although some studies are proposed to automatically translate expert language into layperson-understandable language, only a few of them focus on both accuracy and readability aspects simultaneously in the clinical domain. Thus, simplification of the clinical language is still a challenging task, but unfortunately, it is not yet fully addressed in previous work. To benchmark this task, we construct a new dataset named MedLane to support the development and evaluation of automated clinical language simplification approaches. Besides, we propose a new model called DECLARE that follows the human annotation procedure and achieves state-of-the-art performance compared with eight strong baselines. To fairly evaluate the performance, we also propose three specific evaluation metrics. Experimental results demonstrate the utility of the annotated Med
&lt;/p&gt;</description></item></channel></rss>