<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#25968;&#25454;&#21387;&#32553;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#26080;&#25439;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#65292;&#23545;&#35757;&#32451;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#26410;&#35265;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#24456;&#22810;&#27169;&#22411;&#22312;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#21387;&#32553;&#29575;&#26174;&#33879;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.00861</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#21387;&#32553;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models for Generalization and Robustness via Data Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00861
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#21387;&#32553;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#26080;&#25439;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#65292;&#23545;&#35757;&#32451;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#26410;&#35265;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#24456;&#22810;&#27169;&#22411;&#22312;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#21387;&#32553;&#29575;&#26174;&#33879;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#38754;&#20020;&#25968;&#25454;&#27745;&#26579;&#12289;&#23545;&#25552;&#31034;&#25935;&#24863;&#20197;&#21450;&#22522;&#20934;&#27979;&#35797;&#21019;&#24314;&#25104;&#26412;&#39640;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#25439;&#25968;&#25454;&#21387;&#32553;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#27979;&#35797;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#22312;&#20854;&#35757;&#32451;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#27867;&#21270;&#24773;&#20917;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#20174;2017&#24180;&#21040;2023&#24180;&#20849;83&#20010;&#26376;&#30340;&#20840;&#38754;&#27979;&#35797;&#25968;&#25454;&#65292;&#24182;&#26681;&#25454;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#25130;&#27490;&#26085;&#26399;&#23558;&#25968;&#25454;&#20998;&#20026;&#35757;&#32451;&#21644;&#27979;&#35797;&#26399;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65306;1&#65289;&#27979;&#35797;&#26399;&#30340;&#21387;&#32553;&#24615;&#33021;&#20316;&#20026;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#34913;&#37327;&#65307;2&#65289;&#35757;&#32451;&#26399;&#21644;&#27979;&#35797;&#26399;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#20316;&#20026;&#40065;&#26834;&#24615;&#30340;&#34913;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35768;&#22810;&#27169;&#22411;&#30340;&#21387;&#32553;&#29575;&#22312;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#26174;&#33879;&#38477;&#20302;&#65292;&#20294;&#20687;... (&#20869;&#23481;&#36807;&#38271;&#65292;&#30465;&#30053;)
&lt;/p&gt;
&lt;p&gt;
Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation. To address this, we propose a lossless data compression based evaluation approach that tests how models' predictive abilities generalize after their training cutoff. Specifically, we collect comprehensive test data spanning 83 months from 2017 to 2023 and split the data into training and testing periods according to models' training data cutoff. We measure: 1) the compression performance on the testing period as a measure of generalization on unseen data; and 2) the performance gap between the training and testing period as a measure of robustness. Our experiments test 14 representative large language models with various sizes on sources including Wikipedia, news articles, code, arXiv papers, and multi-modal data. We find that the compression rate of many models reduces significantly after their cutoff date, but models such a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#39640;&#25928;&#31934;&#30830;&#20248;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#31574;&#30053;&#21442;&#25968;&#21270;&#20219;&#24847;&#30340;&#24773;&#20917;&#19979;&#65292;&#28176;&#36817;&#22320;&#19982;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#19968;&#33268;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.00856</link><description>&lt;p&gt;
&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#39640;&#25928;&#31934;&#30830;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient and Exact Optimization of Language Model Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#39640;&#25928;&#31934;&#30830;&#20248;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#31574;&#30053;&#21442;&#25968;&#21270;&#20219;&#24847;&#30340;&#24773;&#20917;&#19979;&#65292;&#28176;&#36817;&#22320;&#19982;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#19968;&#33268;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#23545;&#20110;&#20854;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#35813;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#21270;&#21453;&#26144;&#20154;&#31867;&#20559;&#22909;&#30340;&#39044;&#26399;&#22870;&#21169;&#65292;&#24182;&#23613;&#37327;&#20943;&#23567;&#19982;&#21021;&#22987;&#31574;&#30053;&#30340;&#20559;&#24046;&#12290;&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#30452;&#25509;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20854;&#31574;&#30053;&#26356;&#26032;&#30340;&#26041;&#24046;&#24456;&#39640;&#65292;&#38459;&#30861;&#20102;&#39640;&#25928;&#30340;&#31574;&#30053;&#25913;&#36827;&#12290;&#26368;&#36817;&#65292;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#34987;&#25552;&#20986;&#20197;&#30452;&#25509;&#20174;&#20559;&#22909;&#25968;&#25454;&#20013;&#20248;&#21270;&#31574;&#30053;&#12290;&#23613;&#31649;&#23454;&#29616;&#31616;&#21333;&#65292;DPO&#26159;&#22522;&#20110;&#19981;&#19968;&#23450;&#33021;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#30340;&#26368;&#20248;&#31574;&#30053;&#23548;&#20986;&#30340;&#65292;&#36825;&#21066;&#24369;&#20102;&#20854;&#25910;&#25947;&#21040;&#39044;&#26399;&#35299;&#20915;&#26041;&#26696;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31934;&#30830;&#20248;&#21270;&#65288;EXO&#65289;&#30340;&#23545;&#40784;&#30446;&#26631;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#31574;&#30053;&#30340;&#20219;&#24847;&#21442;&#25968;&#21270;&#65292;EXO&#20445;&#35777;&#28176;&#36817;&#22320;&#19982;RL&#31639;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#19968;&#33268;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The alignment of language models with human preferences is vital for their application in real-world tasks. The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy. While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement. Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data. Though simple to implement, DPO is derived based on the optimal policy that is not assured to be achieved in practice, which undermines its convergence to the intended solution.   In this paper, we propose efficient exact optimization (EXO) of the alignment objective. We prove that EXO is guaranteed to optimize in the same direction as the RL algorithms asymptotically for arbitary parametrization of the policy, while enables efficient op
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#20250;&#35758;&#25688;&#35201;&#20219;&#21153;&#65292;&#36890;&#36807;&#27604;&#36739;&#23567;&#22411;&#32039;&#20945;LLMs&#21644;&#38646;&#23556;&#20987;&#36739;&#22823;LLMs&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#23567;&#22411;LLMs&#26080;&#27861;&#36229;&#36234;&#36739;&#22823;&#30340;&#38646;&#23556;&#20987;LLMs&#65292;&#20294;FLAN-T5&#26159;&#19968;&#20010;&#20363;&#22806;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#35768;&#22810;&#38646;&#23556;&#20987;LLMs&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.00841</link><description>&lt;p&gt;
&#23567;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#20250;&#35758;&#25688;&#35201;&#20013;&#26159;&#21542;&#33021;&#22815;&#36229;&#36234;&#20854;&#20307;&#37327;&#65311;
&lt;/p&gt;
&lt;p&gt;
Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#20250;&#35758;&#25688;&#35201;&#20219;&#21153;&#65292;&#36890;&#36807;&#27604;&#36739;&#23567;&#22411;&#32039;&#20945;LLMs&#21644;&#38646;&#23556;&#20987;&#36739;&#22823;LLMs&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#23567;&#22411;LLMs&#26080;&#27861;&#36229;&#36234;&#36739;&#22823;&#30340;&#38646;&#23556;&#20987;LLMs&#65292;&#20294;FLAN-T5&#26159;&#19968;&#20010;&#20363;&#22806;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#35768;&#22810;&#38646;&#23556;&#20987;LLMs&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27809;&#26377;&#26126;&#30830;&#38024;&#23545;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#37096;&#32626;LLMs&#24182;&#38750;&#26131;&#20107;&#65292;&#22240;&#20026;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23454;&#38469;&#24037;&#19994;&#29615;&#22659;&#20013;&#30340;&#20250;&#35758;&#25688;&#35201;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#32463;&#36807;&#24494;&#35843;&#30340;&#23567;&#22411;&#32039;&#20945;LLMs&#65288;&#22914;FLAN-T5&#12289;TinyLLaMA&#12289;LiteLLaMA&#65289;&#19982;&#38646;&#23556;&#20987;&#36739;&#22823;LLMs&#65288;&#22914;LLaMA-2&#12289;GPT-3.5&#12289;PaLM-2&#65289;&#30340;&#24615;&#33021;&#65292;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#20197;&#35299;&#20915;&#21033;&#29992;LLMs&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#24040;&#22823;&#25104;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22823;&#22810;&#25968;&#23567;&#22411;LLMs&#65292;&#21363;&#20351;&#32463;&#36807;&#24494;&#35843;&#65292;&#20063;&#26080;&#27861;&#22312;&#20250;&#35758;&#25688;&#35201;&#25968;&#25454;&#38598;&#20013;&#36229;&#36234;&#36739;&#22823;&#30340;&#38646;&#23556;&#20987;LLMs&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#20363;&#22806;&#26159;FLAN-T5&#65288;780M&#20010;&#21442;&#25968;&#65289;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#35768;&#22810;&#38646;&#23556;&#20987;LLMs&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive capabilities to solve a wide range of tasks without being explicitly fine-tuned on task-specific datasets. However, deploying LLMs in the real world is not trivial, as it requires substantial computing resources. In this paper, we investigate whether smaller, compact LLMs are a good alternative to the comparatively Larger LLMs2 to address significant costs associated with utilizing LLMs in the real world. In this regard, we study the meeting summarization task in a real-world industrial environment and conduct extensive experiments by comparing the performance of fine-tuned compact LLMs (e.g., FLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2, GPT-3.5, PaLM-2). We observe that most smaller LLMs, even after fine-tuning, fail to outperform larger zero-shot LLMs in meeting summarization datasets. However, a notable exception is FLAN-T5 (780M parameters), which performs on par or even better than many zero-sho
&lt;/p&gt;</description></item><item><title>OLMo&#26159;&#19968;&#31181;&#30495;&#27491;&#24320;&#25918;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#20379;&#32473;&#30740;&#31350;&#31038;&#21306;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20197;&#20419;&#36827;&#23545;&#35821;&#35328;&#27169;&#22411;&#31185;&#23398;&#30340;&#30740;&#31350;&#21644;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2402.00838</link><description>&lt;p&gt;
OLMo: &#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
OLMo: Accelerating the Science of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00838
&lt;/p&gt;
&lt;p&gt;
OLMo&#26159;&#19968;&#31181;&#30495;&#27491;&#24320;&#25918;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#20379;&#32473;&#30740;&#31350;&#31038;&#21306;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20197;&#20419;&#36827;&#23545;&#35821;&#35328;&#27169;&#22411;&#31185;&#23398;&#30340;&#30740;&#31350;&#21644;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#21644;&#21830;&#19994;&#20135;&#21697;&#12290;&#38543;&#30528;&#21830;&#19994;&#37325;&#35201;&#24615;&#30340;&#22686;&#21152;&#65292;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#24050;&#32463;&#23553;&#38381;&#36215;&#26469;&#65292;&#21482;&#33021;&#36890;&#36807;&#19987;&#26377;&#25509;&#21475;&#35775;&#38382;&#65292;&#20854;&#35757;&#32451;&#25968;&#25454;&#12289;&#26550;&#26500;&#21644;&#24320;&#21457;&#32454;&#33410;&#27809;&#26377;&#36879;&#38706;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#32454;&#33410;&#23545;&#20110;&#31185;&#23398;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#21253;&#25324;&#20854;&#20559;&#35265;&#21644;&#28508;&#22312;&#39118;&#38505;&#65292;&#25105;&#20204;&#35748;&#20026;&#30740;&#31350;&#31038;&#21306;&#26377;&#26435;&#35775;&#38382;&#24378;&#22823;&#32780;&#30495;&#27491;&#24320;&#25918;&#30340;LM&#12290;&#20026;&#27492;&#65292;&#26412;&#25216;&#26415;&#25253;&#21578;&#35814;&#32454;&#20171;&#32461;&#20102;OLMo&#30340;&#39318;&#20010;&#29256;&#26412;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#12289;&#30495;&#27491;&#24320;&#25918;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#26500;&#24314;&#21644;&#30740;&#31350;&#35821;&#35328;&#24314;&#27169;&#31185;&#23398;&#30340;&#26694;&#26550;&#12290;&#19982;&#20043;&#21069;&#21482;&#21457;&#24067;&#27169;&#22411;&#26435;&#37325;&#21644;&#25512;&#29702;&#20195;&#30721;&#30340;&#21162;&#21147;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#24067;OLMo&#21644;&#25972;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#35757;&#32451;&#25968;&#25454;&#12289;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#30721;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#21457;&#24067;&#33021;&#22686;&#24378;&#24320;&#25918;&#30740;&#31350;&#31038;&#21306;&#30340;&#33021;&#21147;&#65292;&#24182;&#28608;&#21457;&#26356;&#22810;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspi
&lt;/p&gt;</description></item><item><title>ALISON&#26159;&#19968;&#31181;&#24555;&#36895;&#26377;&#25928;&#30340;&#39118;&#26684;&#23398;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;&#26041;&#27861;&#65292;&#36890;&#36807;&#25915;&#20987;AA&#27169;&#22411;&#26469;&#20445;&#25252;&#38544;&#31169;&#65292;&#30456;&#27604;&#31454;&#20105;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#28151;&#28102;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#28151;&#28102;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.00835</link><description>&lt;p&gt;
ALISON: &#24555;&#36895;&#26377;&#25928;&#30340;&#39118;&#26684;&#23398;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;
&lt;/p&gt;
&lt;p&gt;
ALISON: Fast and Effective Stylometric Authorship Obfuscation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00835
&lt;/p&gt;
&lt;p&gt;
ALISON&#26159;&#19968;&#31181;&#24555;&#36895;&#26377;&#25928;&#30340;&#39118;&#26684;&#23398;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;&#26041;&#27861;&#65292;&#36890;&#36807;&#25915;&#20987;AA&#27169;&#22411;&#26469;&#20445;&#25252;&#38544;&#31169;&#65292;&#30456;&#27604;&#31454;&#20105;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#28151;&#28102;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#28151;&#28102;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#24402;&#23646;&#24230;&#65288;AA&#65289;&#21644;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;&#65288;AO&#65289;&#26159;&#38544;&#31169;&#30740;&#31350;&#20013;&#26085;&#30410;&#37325;&#35201;&#30340;&#20004;&#39033;&#31454;&#20105;&#20219;&#21153;&#12290;&#29616;&#20195;AA&#21033;&#29992;&#20316;&#32773;&#30340;&#19968;&#36143;&#20889;&#20316;&#39118;&#26684;&#65292;&#20351;&#29992;AA&#20998;&#31867;&#22120;&#23558;&#25991;&#26412;&#19982;&#20854;&#20316;&#32773;&#21305;&#37197;&#12290;AO&#26159;&#30456;&#24212;&#30340;&#23545;&#25239;&#24615;&#20219;&#21153;&#65292;&#26088;&#22312;&#20197;&#19968;&#31181;&#26041;&#24335;&#20462;&#25913;&#25991;&#26412;&#65292;&#20351;&#20854;&#35821;&#20041;&#24471;&#21040;&#20445;&#30041;&#65292;&#20294;AA&#27169;&#22411;&#26080;&#27861;&#27491;&#30830;&#25512;&#26029;&#20854;&#20316;&#32773;&#12290;&#20026;&#20102;&#35299;&#20915;&#26368;&#20808;&#36827;&#30340;AA&#26041;&#27861;&#24341;&#21457;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;AO&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#20854;&#35757;&#32451;&#21644;&#28151;&#28102;&#36895;&#24230;&#36807;&#24930;&#65288;&#36890;&#24120;&#38656;&#35201;&#25968;&#23567;&#26102;&#65289;&#65292;&#20351;&#29992;&#36215;&#26469;&#20173;&#28982;&#19981;&#22826;&#23454;&#38469;&#12290;&#38754;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;AO&#26041;&#27861;ALISON&#65292;&#23427;&#65288;1&#65289;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;/&#28151;&#28102;&#26102;&#38388;&#65292;&#28436;&#31034;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;AO&#26041;&#27861;&#24555;10&#20493;&#20197;&#19978;&#30340;&#28151;&#28102;&#36895;&#24230;&#65292;&#65288;2&#65289;&#36890;&#36807;&#25915;&#20987;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#19977;&#31181;&#22522;&#20110;Transformer&#30340;AA&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#28151;&#28102;&#25104;&#21151;&#29575;&#65292;&#36890;&#24120;&#27604;&#31454;&#20105;&#26041;&#27861;&#34920;&#29616;&#22909;15%&#12290;
&lt;/p&gt;
&lt;p&gt;
Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research. Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier. AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship. To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours. To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.00798</link><description>&lt;p&gt;
&#27491;&#24335;-LLM&#65306;&#23558;&#24418;&#24335;&#35821;&#35328;&#21644;&#33258;&#28982;&#35821;&#35328;&#38598;&#25104;&#20110;&#21487;&#25511;&#30340;LLM&#26234;&#33021;&#20307;&#20013;
&lt;/p&gt;
&lt;p&gt;
Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#21644;&#25191;&#34892;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#22810;&#27493;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#30340;&#20869;&#23481;&#29983;&#25104;&#36807;&#31243;&#20960;&#20046;&#26080;&#27861;&#25511;&#21046;&#65292;&#24403;&#21069;&#30340;LLM&#26234;&#33021;&#20307;&#32463;&#24120;&#29983;&#25104;&#26080;&#25928;&#25110;&#19981;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#65292;&#36825;&#25439;&#23475;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#24615;&#33021;&#24182;&#30772;&#22351;&#20102;&#29992;&#25143;&#23545;LLM&#26234;&#33021;&#20307;&#30340;&#20449;&#20219;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;LLM&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#21147;&#21644;&#24418;&#24335;&#35821;&#35328;&#30340;&#31934;&#30830;&#24615;&#36827;&#34892;&#25972;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#20154;&#31867;&#29992;&#25143;&#23558;&#20182;&#20204;&#23545;&#35745;&#21010;&#36807;&#31243;&#30340;&#35201;&#27714;&#25110;&#32422;&#26463;&#34920;&#36798;&#20026;&#33258;&#21160;&#26426;&#12290;&#28982;&#21518;&#65292;&#22312;&#33258;&#21160;&#26426;&#30340;&#30417;&#30563;&#19979;&#65292;&#20351;&#29992;&#22522;&#20110;&#22534;&#26632;&#30340;LLM&#35745;&#21010;&#29983;&#25104;&#36807;&#31243;&#26469;&#30830;&#20445;&#29983;&#25104;&#30340;&#35745;&#21010;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#65292;&#20174;&#32780;&#20351;&#35745;&#21010;&#36807;&#31243;&#21487;&#25511;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#20219;&#21153;&#21644;&#23454;&#38469;&#30340;&#30495;&#23454;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#19988;obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.
&lt;/p&gt;
&lt;p&gt;
Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#31216;&#20026;ReAGent&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#25928;&#29575;&#26356;&#39640;&#30340;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.00794</link><description>&lt;p&gt;
ReAGent: &#19968;&#20010;&#38754;&#21521;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#31216;&#20026;ReAGent&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#35745;&#31639;&#19978;&#25928;&#29575;&#26356;&#39640;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65288;FAs&#65289;&#65292;&#22914;&#26799;&#24230;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30830;&#23450;&#25152;&#26377;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;&#29616;&#26377;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20026;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24320;&#21457;&#21644;&#27979;&#35797;FAs&#65292;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#22312;&#25991;&#26412;&#29983;&#25104;&#19978;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;FAs&#26469;&#22788;&#29702;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#22240;&#20026;&#27169;&#22411;&#26550;&#26500;&#21644;&#20219;&#21153;&#35774;&#32622;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#27809;&#26377;&#19968;&#20010;&#36890;&#29992;&#30340;FA&#36866;&#29992;&#20110;&#25152;&#26377;&#27169;&#22411;&#21644;&#20219;&#21153;&#12290;&#36825;&#20351;&#24471;&#38024;&#23545;&#22823;&#22411;LMs&#36873;&#25321;FA&#35745;&#31639;&#19978;&#38750;&#24120;&#26114;&#36149;&#65292;&#22240;&#20026;&#36755;&#20837;&#37325;&#35201;&#24615;&#30340;&#25512;&#23548;&#36890;&#24120;&#38656;&#35201;&#22810;&#20010;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#36882;&#65292;&#21253;&#25324;&#21487;&#33021;&#26159;&#38480;&#21046;&#24615;&#30340;&#26799;&#24230;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29983;&#25104;LMs&#30340;&#27169;&#22411;&#26080;&#20851;FA&#65292;&#31216;&#20026;&#36882;&#24402;&#24402;&#22240;&#29983;&#25104;&#22120;&#65288;ReAGent&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions. Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks. However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively. Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks. This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute. To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent)
&lt;/p&gt;</description></item><item><title>CroissantLLM&#26159;&#19968;&#20010;1.3B&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#24320;&#28304;&#12290;&#27169;&#22411;&#36824;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#26816;&#26597;&#28857;&#65292;&#20197;&#21450;&#19968;&#20010;&#27861;&#35821;&#22522;&#20934;&#27979;&#35797; FrenchBench&#12290;</title><link>https://arxiv.org/abs/2402.00786</link><description>&lt;p&gt;
CroissantLLM: &#19968;&#20010;&#30495;&#27491;&#30340;&#21452;&#35821;&#27861;&#35821;-&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CroissantLLM: A Truly Bilingual French-English Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00786
&lt;/p&gt;
&lt;p&gt;
CroissantLLM&#26159;&#19968;&#20010;1.3B&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#24320;&#28304;&#12290;&#27169;&#22411;&#36824;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#26816;&#26597;&#28857;&#65292;&#20197;&#21450;&#19968;&#20010;&#27861;&#35821;&#22522;&#20934;&#27979;&#35797; FrenchBench&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CroissantLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;3T&#20010;&#33521;&#35821;&#21644;&#27861;&#35821;&#26631;&#35760;&#19978;&#39044;&#35757;&#32451;&#30340;13&#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#30740;&#31350;&#21644;&#24037;&#19994;&#31038;&#21306;&#24102;&#26469;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#12289;&#23436;&#20840;&#24320;&#28304;&#30340;&#21452;&#35821;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#28040;&#36153;&#32423;&#26412;&#22320;&#30828;&#20214;&#19978;&#24555;&#36895;&#36816;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#19968;&#31181;&#20869;&#22312;&#21452;&#35821;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#20010;&#27861;&#35821;&#20998;&#21106;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#25163;&#24037;&#31574;&#21010;&#12289;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#28304;&#12290;&#20026;&#20102;&#35780;&#20272;&#22312;&#33521;&#35821;&#20197;&#22806;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; FrenchBench&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#27169;&#22411;&#22312;&#27861;&#35821;&#35821;&#35328;&#20013;&#24615;&#33021;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20445;&#25345;&#36879;&#26126;&#24230;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#20195;&#30721;&#24211;&#21644;&#21508;&#31181;&#27169;&#22411;&#35268;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#20960;&#21313;&#20010;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#20581;&#24247;-LLM&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#29305;&#24449;&#25552;&#21462;&#21644;&#21307;&#23398;&#30693;&#35782;&#26435;&#34913;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#30340;&#26816;&#32034;&#22686;&#24378;&#30142;&#30149;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#20581;&#24247;&#25253;&#21578;&#65292;&#35843;&#25972;&#29305;&#24449;&#26435;&#37325;&#65292;&#20197;&#21450;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#23478;&#35265;&#35299;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19982;&#20256;&#32479;&#20581;&#24247;&#31649;&#29702;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.00746</link><description>&lt;p&gt;
&#20581;&#24247;-LLM&#65306;&#20010;&#24615;&#21270;&#26816;&#32034;&#22686;&#24378;&#30340;&#30142;&#30149;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#20581;&#24247;-LLM&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#29305;&#24449;&#25552;&#21462;&#21644;&#21307;&#23398;&#30693;&#35782;&#26435;&#34913;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#30340;&#26816;&#32034;&#22686;&#24378;&#30142;&#30149;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#20581;&#24247;&#25253;&#21578;&#65292;&#35843;&#25972;&#29305;&#24449;&#26435;&#37325;&#65292;&#20197;&#21450;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#23478;&#35265;&#35299;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19982;&#20256;&#32479;&#20581;&#24247;&#31649;&#29702;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21355;&#29983;&#20445;&#20581;&#39046;&#22495;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26234;&#33021;&#21307;&#30103;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26234;&#33021;&#21307;&#30103;&#21463;&#38480;&#20110;&#38745;&#24577;&#25968;&#25454;&#21644;&#32479;&#19968;&#26631;&#20934;&#65292;&#26080;&#27861;&#23436;&#20840;&#19982;&#20010;&#20307;&#24773;&#20917;&#38598;&#25104;&#65292;&#21516;&#26102;&#20063;&#38754;&#20020;&#20854;&#20182;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#20581;&#24247;-LLM&#65292;&#23558;&#22823;&#35268;&#27169;&#29305;&#24449;&#25552;&#21462;&#21644;&#21307;&#23398;&#30693;&#35782;&#26435;&#34913;&#35780;&#20998;&#30456;&#32467;&#21512;&#12290;&#19982;&#20256;&#32479;&#20581;&#24247;&#31649;&#29702;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19977;&#20010;&#20027;&#35201;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20581;&#24247;&#25253;&#21578;&#25972;&#21512;&#21040;&#22823;&#27169;&#22411;&#20013;&#65292;&#25552;&#20379;&#35814;&#32454;&#30340;&#20219;&#21153;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#19987;&#19994;&#30340;&#21307;&#23398;&#19987;&#19994;&#30693;&#35782;&#35843;&#25972;&#20581;&#24247;&#29305;&#24449;&#30340;&#26435;&#37325;&#24471;&#20998;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20351;&#29992;&#21322;&#33258;&#21160;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#26512;&#33021;&#21147;&#65292;&#24182;&#25972;&#21512;&#19987;&#23478;&#35265;&#35299;&#20197;&#25552;&#39640;&#30142;&#30149;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) in healthcare has significantly advanced intelligent medical treatment. However, traditional intelligent healthcare is limited by static data and unified standards, preventing full integration with individual situations and other challenges. Hence, a more professional and detailed intelligent healthcare method is needed for development. To this end, we propose an innovative framework named Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring. Compared to traditional health management methods, our approach has three main advantages. First, our method integrates health reports into a large model to provide detailed task information. Second, professional medical expertise is used to adjust the weighted scores of health characteristics. Third, we use a semi-automated feature extraction framework to enhance the analytical power of language models and incorporate expert insights to improve the accuracy of disease predic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Logic-Explainer&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#31526;&#21495;&#32454;&#21270;&#26041;&#27861;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20262;&#29702;NLI&#20013;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#36890;&#36807;&#38598;&#25104;&#22806;&#37096;&#30340;&#36870;&#21521;&#25512;&#29702;&#27714;&#35299;&#22120;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#36880;&#27493;&#23436;&#21892;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#24182;&#39564;&#35777;&#20854;&#27491;&#30830;&#24615;&#65292;&#20943;&#23569;&#19981;&#23436;&#25972;&#24615;&#21644;&#20887;&#20313;&#12290;</title><link>https://arxiv.org/abs/2402.00745</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#31526;&#21495;&#32454;&#21270;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20262;&#29702;&#35299;&#37322;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Logic-Explainer&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#31526;&#21495;&#32454;&#21270;&#26041;&#27861;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20262;&#29702;NLI&#20013;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#36890;&#36807;&#38598;&#25104;&#22806;&#37096;&#30340;&#36870;&#21521;&#25512;&#29702;&#27714;&#35299;&#22120;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#36880;&#27493;&#23436;&#21892;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#24182;&#39564;&#35777;&#20854;&#27491;&#30830;&#24615;&#65292;&#20943;&#23569;&#19981;&#23436;&#25972;&#24615;&#21644;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21450;&#20854;&#25512;&#29702;&#33021;&#21147;&#30340;&#24212;&#29992;&#21644;&#35780;&#20272;&#19978;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;LLMs&#20173;&#28982;&#23481;&#26131;&#20986;&#29616;&#20107;&#23454;&#38169;&#35823;&#21644;&#35299;&#37322;&#19978;&#30340;&#19981;&#19968;&#33268;&#65292;&#38480;&#21046;&#20102;&#22797;&#26434;&#39046;&#22495;&#25512;&#29702;&#30340;&#25511;&#21046;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#20851;&#27880;&#20262;&#29702; NLI&#65292;&#30740;&#31350;&#28151;&#21512;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#22914;&#20309;&#25552;&#21319;LLMs&#20135;&#29983;&#30340;&#20262;&#29702;&#35299;&#37322;&#30340;&#36923;&#36753;&#26377;&#25928;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Logic-Explainer&#30340;&#24402;&#32435;-&#28436;&#32462;&#26694;&#26550;&#65292;&#23558;LLMs&#19982;&#22806;&#37096;&#30340;&#36870;&#21521;&#25512;&#29702;&#27714;&#35299;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#36880;&#27493;&#23436;&#21892;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#24182;&#32852;&#21512;&#39564;&#35777;&#20854;&#27491;&#30830;&#24615;&#12289;&#20943;&#23569;&#19981;&#23436;&#25972;&#24615;&#21644;&#20887;&#20313;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;Logic-Explainer&#21487;&#20197;&#25913;&#36827;&#36890;&#36807;&#29615;&#22659;&#23398;&#20064;&#26041;&#27861;&#21644;Chain-of-Thought&#65288;CoT&#65289;&#29983;&#25104;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains. In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challe
&lt;/p&gt;</description></item><item><title>&#22312;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BATON&#26694;&#26550;&#65292;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#21453;&#39304;&#26469;&#25913;&#21892;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#25968;&#25454;&#38598;&#12289;&#24341;&#20837;&#22870;&#21169;&#27169;&#22411;&#21644;&#24494;&#35843;&#29616;&#26377;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;BATON&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#38899;&#39057;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.00744</link><description>&lt;p&gt;
BATON&#65306;&#23558;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#21453;&#39304;&#36827;&#34892;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
BATON: Aligning Text-to-Audio Model with Human Preference Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00744
&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BATON&#26694;&#26550;&#65292;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#21453;&#39304;&#26469;&#25913;&#21892;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#25968;&#25454;&#38598;&#12289;&#24341;&#20837;&#22870;&#21169;&#27169;&#22411;&#21644;&#24494;&#35843;&#29616;&#26377;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;BATON&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#38899;&#39057;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#30340;&#21457;&#23637;&#65292;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#22266;&#26377;&#20449;&#24687;&#23494;&#24230;&#21644;&#26377;&#38480;&#30340;&#27169;&#22411;&#29702;&#35299;&#33021;&#21147;&#65292;&#36825;&#20123;&#27169;&#22411;&#24456;&#38590;&#29983;&#25104;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#38899;&#39057;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BATON&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#20154;&#31867;&#20559;&#22909;&#21453;&#39304;&#25552;&#39640;&#29983;&#25104;&#38899;&#39057;&#19982;&#25991;&#26412;&#25552;&#31034;&#20043;&#38388;&#23545;&#40784;&#24230;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;BATON&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#38454;&#27573;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#25552;&#31034;&#21644;&#30456;&#24212;&#29983;&#25104;&#38899;&#39057;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22870;&#21169;&#27169;&#22411;&#65292;&#21033;&#29992;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#27169;&#25311;&#20154;&#31867;&#20559;&#22909;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#30340;&#25991;&#26412;-&#38899;&#39057;&#23545;&#20998;&#37197;&#22870;&#21169;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22870;&#21169;&#27169;&#22411;&#23545;&#29616;&#25104;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;BATON&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21407;&#22987;&#38899;&#39057;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of AI-Generated Content (AIGC), text-to-audio models are gaining widespread attention. However, it is challenging for these models to generate audio aligned with human preference due to the inherent information density of natural language and limited model understanding ability. To alleviate this issue, we formulate the BATON, a framework designed to enhance the alignment between generated audio and text prompt using human preference feedback. Our BATON comprises three key stages: Firstly, we curated a dataset containing both prompts and the corresponding generated audio, which was then annotated based on human feedback. Secondly, we introduced a reward model using the constructed dataset, which can mimic human preference by assigning rewards to input text-audio pairs. Finally, we employed the reward model to fine-tune an off-the-shelf text-to-audio model. The experiment results demonstrate that our BATON can significantly improve the generation quality of the orig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#30340;&#23454;&#39564;&#30740;&#31350;&#20102;Transformer&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.00743</link><description>&lt;p&gt;
Transformer&#30340;&#22909;&#22788;&#65306;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#30340;&#23454;&#39564;&#30740;&#31350;&#20102;Transformer&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#25512;&#29702;&#38454;&#27573;&#33021;&#22815;&#23398;&#20064;&#19978;&#19979;&#25991;&#20013;&#30340;&#27010;&#24565;&#12290;&#29616;&#26377;&#30340;&#25991;&#29486;&#65292;&#20363;&#22914;\citet{zhang2023trained,huang2023context}&#23545;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#20294;&#26159;&#20182;&#20204;&#20551;&#35774;&#27599;&#20010;&#26679;&#26412;&#30340;&#36755;&#20837;$x_i$&#21644;&#36755;&#20986;$y_i$&#37117;&#34987;&#23884;&#20837;&#21040;&#30456;&#21516;&#30340;&#20196;&#29260;&#20013;&#65288;&#21363;&#32467;&#26500;&#21270;&#25968;&#25454;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#23427;&#20204;&#21576;&#29616;&#20026;&#20004;&#20010;&#20196;&#29260;&#65288;&#21363;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;\cite{wibisono2023role}&#65289;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#36827;&#34892;&#20102;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;Transformer&#26550;&#26500;&#30340;&#22909;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#30456;&#24212;&#30340;&#29702;&#35770;&#30452;&#35273;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;Transformer&#21487;&#20197;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;Transformer&#20013;&#36215;&#21040;&#19978;&#19979;&#25991;&#23398;&#20064;&#20316;&#29992;&#30340;&#30830;&#20999;&#32452;&#20214;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65288;1&#65289;&#24102;&#26377;&#20004;&#23618;softmax&#65288;&#33258;&#25105;&#65289;&#27880;&#24847;&#21147;&#21644;&#21069;&#30651;&#24615;&#27880;&#24847;&#21147;&#25513;&#30721;&#30340;Transformer&#21487;&#20197;&#20174;&#25552;&#31034;&#20013;&#23398;&#20064;&#65292;&#22914;&#26524;$y_i$&#22312;&#20196;&#29260;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practice, it is observed that transformer-based models can learn concepts in context in the inference stage. While existing literature, e.g., \citet{zhang2023trained,huang2023context}, provide theoretical explanations on this in-context learning ability, they assume the input $x_i$ and the output $y_i$ for each sample are embedded in the same token (i.e., structured data). However, in reality, they are presented in two tokens (i.e., unstructured data \cite{wibisono2023role}). In this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data. We study the exact components in a transformer that facilitate the in-context learning. In particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if $y_i$ is in the token n
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20027;&#35201;&#30740;&#31350;&#20102;&#23545;&#40784;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#20013;&#20986;&#29616;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;&#22870;&#21169;&#27169;&#22411;&#30340;&#36873;&#25321;&#20197;&#21450;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#30340;&#32452;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#27010;&#29575;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#30340;&#33258;&#28982;&#21464;&#25442;&#36873;&#25321;&#65292;&#35813;&#21464;&#25442;&#24378;&#35843;&#25913;&#21892;&#34920;&#29616;&#19981;&#20339;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#27424;&#25311;&#21512;&#21644;&#22870;&#21169;&#27450;&#39575;&#12290;</title><link>https://arxiv.org/abs/2402.00742</link><description>&lt;p&gt;
&#25913;&#21464;&#21644;&#32452;&#21512;&#22870;&#21169;&#20197;&#23545;&#40784;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Transforming and Combining Rewards for Aligning Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20027;&#35201;&#30740;&#31350;&#20102;&#23545;&#40784;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#20013;&#20986;&#29616;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;&#22870;&#21169;&#27169;&#22411;&#30340;&#36873;&#25321;&#20197;&#21450;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#30340;&#32452;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#27010;&#29575;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#30340;&#33258;&#28982;&#21464;&#25442;&#36873;&#25321;&#65292;&#35813;&#21464;&#25442;&#24378;&#35843;&#25913;&#21892;&#34920;&#29616;&#19981;&#20339;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#27424;&#25311;&#21512;&#21644;&#22870;&#21169;&#27450;&#39575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#39318;&#20808;&#20174;&#20559;&#22909;&#25968;&#25454;&#20013;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#35813;&#22870;&#21169;&#27169;&#22411;&#26469;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#20013;&#20986;&#29616;&#30340;&#20004;&#20010;&#23494;&#20999;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22870;&#21169;&#27169;&#22411;&#30340;&#20219;&#20309;&#21333;&#35843;&#21464;&#25442;&#37117;&#20445;&#25345;&#20559;&#22909;&#25490;&#21517;&#65307;&#26159;&#21542;&#26377;&#19968;&#31181;&#27604;&#20854;&#20182;&#36873;&#25321;&#8220;&#26356;&#22909;&#8221;&#30340;&#36873;&#25321;&#65311;&#20854;&#27425;&#65292;&#25105;&#20204;&#32463;&#24120;&#24076;&#26395;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#20010;&#29305;&#24615;&#23545;&#40784;&#65306;&#25105;&#20204;&#22914;&#20309;&#32452;&#21512;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#65311;&#36890;&#36807;&#23545;&#40784;&#36807;&#31243;&#30340;&#27010;&#29575;&#35299;&#37322;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20174;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#23398;&#20064;&#30340;&#22870;&#21169;&#65288;&#24120;&#35265;&#24773;&#20917;&#65289;&#30340;&#33258;&#28982;&#21464;&#25442;&#36873;&#25321;&#12290;&#36825;&#20010;&#27966;&#29983;&#30340;&#21464;&#25442;&#20855;&#26377;&#20004;&#20010;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;&#39318;&#20808;&#65292;&#23427;&#24378;&#35843;&#25913;&#36827;&#34920;&#29616;&#19981;&#20339;&#30340;&#36755;&#20986;&#65292;&#32780;&#19981;&#26159;&#24050;&#32463;&#24471;&#20998;&#33391;&#22909;&#30340;&#36755;&#20986;&#12290;&#36825;&#26082;&#20943;&#36731;&#20102;&#27424;&#25311;&#21512;&#65288;&#20854;&#20013;&#19968;&#20123;&#25552;&#31034;&#27809;&#26377;&#24471;&#21040;&#25913;&#36827;&#65289;&#65292;&#21448;&#20943;&#23569;&#20102;&#22870;&#21169;&#27450;&#39575;&#65288;&#27169;&#22411;&#23398;&#20064;&#21033;&#29992;&#38169;&#35823;&#25351;&#23450;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model. We study two closely related problems that arise in this approach. First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others? Second, we often wish to align language models to multiple properties: how should we combine multiple reward models? Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models. This derived transformation has two important properties. First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well. This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;Transformer&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQVAEs&#65289;&#20013;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#21487;&#25511;&#24615;&#65292;&#25552;&#20986;&#20102;T5VQVAE&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#23548;T5&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#23454;&#29616;&#26356;&#22909;&#30340;&#29983;&#25104;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;T5VQVAE&#22312;&#21487;&#25511;&#24615;&#21644;&#29983;&#25104;&#25928;&#26524;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.00723</link><description>&lt;p&gt;
&#29992;Transformer&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#25913;&#36827;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Improving Semantic Control in Discrete Latent Spaces with Transformer Quantized Variational Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;Transformer&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQVAEs&#65289;&#20013;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#21487;&#25511;&#24615;&#65292;&#25552;&#20986;&#20102;T5VQVAE&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#23548;T5&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#23454;&#29616;&#26356;&#22909;&#30340;&#29983;&#25104;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;T5VQVAE&#22312;&#21487;&#25511;&#24615;&#21644;&#29983;&#25104;&#25928;&#26524;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#21464;&#20998;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23454;&#29616;&#31934;&#30830;&#30340;&#35821;&#20041;&#25511;&#21046;&#23545;&#20110;NLP&#39046;&#22495;&#30340;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#24213;&#23618;&#30340;&#29983;&#25104;&#26426;&#21046;&#21487;&#20197;&#26356;&#22909;&#22320;&#23450;&#20301;&#12289;&#35299;&#37322;&#21644;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#23454;&#29616;&#19968;&#33268;&#30340;&#32467;&#26524;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#21464;&#20998;&#29942;&#39048;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#35821;&#20041;&#20449;&#24687;&#20002;&#22833;&#20197;&#21450;&#35299;&#30721;&#26426;&#21046;&#30340;&#26377;&#38480;&#25511;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQVAE&#65289;&#20013;&#30340;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#65292;&#20197;&#25913;&#36827;Transformer-based VAEs&#20013;&#30340;&#35821;&#20041;&#25511;&#21046;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;T5VQVAE&#65292;&#21033;&#29992;VQVAE&#30340;&#21487;&#25511;&#24615;&#26469;&#25351;&#23548;T5&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#29983;&#25104;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21487;&#25511;&#24615;&#21644;&#29983;&#25104;&#25928;&#26524;&#26041;&#38754;&#65292;T5VQVAE&#20248;&#20110;&#29616;&#26377;&#30340;VAE&#27169;&#22411;&#65292;&#21253;&#25324;Optimus&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving precise semantic control over the latent spaces of Variational AutoEncoders (VAEs) holds significant value for downstream tasks in NLP as the underlying generative mechanisms could be better localised, explained and improved upon. Recent research, however, has struggled to achieve consistent results, primarily due to the inevitable loss of semantic information in the variational bottleneck and limited control over the decoding mechanism. To overcome these challenges, we investigate discrete latent spaces in Vector Quantized Variational AutoEncoders (VQVAEs) to improve semantic control and generation in Transformer-based VAEs. In particular, We propose T5VQVAE, a novel model that leverages the controllability of VQVAEs to guide the self-attention mechanism in T5 at the token-level, exploiting its full generalization capabilities. Experimental results indicate that T5VQVAE outperforms existing state-of-the-art VAE models, including Optimus, in terms of controllability and prese
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#20107;&#23454;&#34920;&#31034;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24178;&#39044;&#25991;&#26412;&#34920;&#31034;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00711</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#34920;&#31034;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Explaining Text Classifiers with Counterfactual Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#20107;&#23454;&#34920;&#31034;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24178;&#39044;&#25991;&#26412;&#34920;&#31034;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#26041;&#27861;&#21487;&#20197;&#20026;&#20998;&#31867;&#22120;&#25552;&#20379;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#20854;&#20013;&#21453;&#20107;&#23454;&#26159;&#25351;&#38500;&#20102;&#19968;&#20010;&#20998;&#31867;&#29305;&#24449;&#20043;&#22806;&#65292;&#19982;&#30495;&#23454;&#35266;&#23519;&#23436;&#20840;&#30456;&#21516;&#30340;&#20551;&#35774;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#26412;&#39046;&#22495;&#26500;&#24314;&#36825;&#31181;&#21453;&#20107;&#23454;&#23384;&#22312;&#29305;&#23450;&#25361;&#25112;&#65292;&#22240;&#20026;&#26576;&#20123;&#23646;&#24615;&#20540;&#21487;&#33021;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#20107;&#20214;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#24178;&#39044;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#24178;&#39044;&#26041;&#27861;&#26159;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#38752;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;Pearl&#30340;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#20013;&#23450;&#20041;&#30340;&#21453;&#20107;&#23454;&#26159;&#19968;&#33268;&#30340;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#22522;&#20110;&#30495;&#23454;&#21453;&#20107;&#23454;&#65288;&#36890;&#36807;&#26126;&#30830;&#30340;&#25991;&#26412;&#24178;&#39044;&#33719;&#24471;&#65289;&#21644;&#25105;&#20204;&#30340;&#21453;&#20107;&#23454;&#65288;&#36890;&#36807;&#23545;&#25991;&#26412;&#34920;&#31034;&#30340;&#24178;&#39044;&#24471;&#21040;&#65289;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
One well motivated explanation method for classifiers leverages counterfactuals which are hypothetical events identical to real observations in all aspects except for one categorical feature. Constructing such counterfactual poses specific challenges for texts, however, as some attribute values may not necessarily align with plausible real-world events. In this paper we propose a simple method for generating counterfactuals by intervening in the space of text representations which bypasses this limitation. We argue that our interventions are minimally disruptive and that they are theoretically sound as they align with counterfactuals as defined in Pearl's causal inference framework. To validate our method, we first conduct experiments on a synthetic dataset of counterfactuals, allowing for a direct comparison between classifier predictions based on ground truth counterfactuals (obtained through explicit text interventions) and our counterfactuals, derived through interventions in the r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#36817;&#37051;&#26041;&#27861;&#25193;&#23637;&#30340;&#38750;&#20132;&#25442;&#30340;&#20849;&#24418;&#35821;&#35328;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20379;&#24102;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#39044;&#27979;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.00707</link><description>&lt;p&gt;
&#38750;&#20132;&#25442;&#30340;&#20849;&#24418;&#35821;&#35328;&#29983;&#25104;&#19982;&#26368;&#36817;&#37051;
&lt;/p&gt;
&lt;p&gt;
Non-Exchangeable Conformal Language Generation with Nearest Neighbors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#36817;&#37051;&#26041;&#27861;&#25193;&#23637;&#30340;&#38750;&#20132;&#25442;&#30340;&#20849;&#24418;&#35821;&#35328;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20379;&#24102;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#39044;&#27979;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#35753;&#20154;&#20204;&#26816;&#26597;&#28508;&#22312;&#30340;&#38169;&#35273;&#21644;&#20351;&#31995;&#32479;&#26356;&#21487;&#38752;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#20849;&#24418;&#39044;&#27979;&#26159;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#20379;&#24102;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#39044;&#27979;&#65292;&#28982;&#32780;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20219;&#20309;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#37117;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#20851;&#20110;&#38750;&#20132;&#25442;&#30340;&#20849;&#24418;&#39044;&#27979;&#30340;&#32467;&#26524;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#35813;&#26041;&#27861;&#20173;&#28982;&#30830;&#20445;&#35206;&#30422;&#33539;&#22260;&#12290;&#32467;&#26524;--&#38750;&#20132;&#25442;&#30340;&#20849;&#24418;&#26680;&#37319;&#26679;&#65292;&#26159;&#23545;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#29983;&#25104;&#30340;&#20849;&#24418;&#39044;&#27979;&#26694;&#26550;&#30340;&#19968;&#31181;&#26032;&#39062;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#20219;&#24847;&#27169;&#22411;&#30340;&#20107;&#21518;&#22788;&#29702;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#65292;&#24182;&#25552;&#20379;&#24102;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#26631;&#35760;&#32423;&#21035;&#12289;&#26657;&#20934;&#30340;&#39044;&#27979;&#38598;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#35328;&#24314;&#27169;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#29983;&#25104;&#36136;&#37327;&#32467;&#26524;&#12290;&#36890;&#36807;&#21516;&#26102;&#20135;&#29983;&#20855;&#26377;&#33391;&#22909;&#35206;&#30422;&#24230;&#30340;&#26356;&#32039;&#23494;&#30340;&#39044;&#27979;&#38598;&#65292;
&lt;/p&gt;
&lt;p&gt;
Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable. Conformal prediction is an attractive framework to provide predictions imbued with statistical guarantees, however, its application to text generation is challenging since any i.i.d. assumptions are not realistic. In this paper, we bridge this gap by leveraging recent results on non-exchangeable conformal prediction, which still ensures bounds on coverage. The result, non-exchangeable conformal nucleus sampling, is a novel extension of the conformal prediction framework to generation based on nearest neighbors. Our method can be used post-hoc for an arbitrary model without extra training and supplies token-level, calibrated prediction sets equipped with statistical guarantees. Experiments in machine translation and language modeling show encouraging results in generation quality. By also producing tighter prediction sets with good cove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#21487;&#25193;&#23637;&#30417;&#30563;&#21644;&#38598;&#25104;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#22312;&#24369;&#25945;&#24072;&#21644;&#24378;&#23398;&#29983;&#20043;&#38388;&#20943;&#23567;&#20102;&#33021;&#21147;&#24046;&#36317;&#65292;&#25552;&#39640;&#20102;&#24369;&#21040;&#24378;&#27867;&#21270;&#33021;&#21147;&#30340;&#26694;&#26550;&#19979;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.00667</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#25193;&#23637;&#30417;&#30563;&#21644;&#38598;&#25104;&#23398;&#20064;&#25552;&#39640;&#24369;&#21040;&#24378;&#27867;&#21270;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Improving Weak-to-Strong Generalization with Scalable Oversight and Ensemble Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#21487;&#25193;&#23637;&#30417;&#30563;&#21644;&#38598;&#25104;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#22312;&#24369;&#25945;&#24072;&#21644;&#24378;&#23398;&#29983;&#20043;&#38388;&#20943;&#23567;&#20102;&#33021;&#21147;&#24046;&#36317;&#65292;&#25552;&#39640;&#20102;&#24369;&#21040;&#24378;&#27867;&#21270;&#33021;&#21147;&#30340;&#26694;&#26550;&#19979;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;OpenAI&#26368;&#26032;&#30340;&#20851;&#20110;&#24369;&#21040;&#24378;&#27867;&#21270;&#33021;&#21147;&#30340;&#36229;&#23545;&#40784;&#24037;&#20316;&#36827;&#34892;&#20102;&#21518;&#32493;&#30740;&#31350;&#12290;&#36229;&#23545;&#40784;&#24037;&#20316;&#26088;&#22312;&#30830;&#20445;&#39640;&#32423;AI&#31995;&#32479;&#22312;&#22788;&#29702;&#22797;&#26434;&#12289;&#39640;&#39118;&#38505;&#20219;&#21153;&#26102;&#19982;&#20154;&#31867;&#30340;&#20215;&#20540;&#21644;&#24847;&#22270;&#20445;&#25345;&#19968;&#33268;&#12290;&#24369;&#21040;&#24378;&#27867;&#21270;&#33021;&#21147;&#30340;&#26694;&#26550;&#20026;&#36825;&#19968;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#24320;&#25299;&#20102;&#26032;&#30340;&#30740;&#31350;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#27169;&#25311;&#20102;W2SG&#26694;&#26550;&#19979;&#30340;&#20004;&#20010;&#36229;&#23545;&#40784;&#38454;&#27573;&#65306;&#36890;&#29992;&#36229;&#20154;&#31867;&#27169;&#22411;&#30340;&#21457;&#23637;&#21644;&#26397;&#30528;&#36229;&#32423;&#26234;&#33021;&#30340;&#36827;&#27493;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#36890;&#36807;&#21487;&#25193;&#23637;&#30417;&#30563;&#21644;&#38598;&#25104;&#23398;&#20064;&#30340;&#32452;&#21512;&#65292;&#36890;&#36807;&#20154;&#31867;&#30417;&#30563;&#25552;&#39640;&#20102;&#24369;&#30417;&#30563;&#30340;&#36136;&#37327;&#65292;&#20943;&#23567;&#20102;&#24369;&#25945;&#24072;&#21644;&#24378;&#23398;&#29983;&#20043;&#38388;&#30340;&#33021;&#21147;&#24046;&#36317;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#21033;&#29992;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#22120;&#20316;&#20026;&#24369;&#30417;&#30563;&#32773;&#12290;&#36890;&#36807;&#36882;&#24402;&#26356;&#26032;&#35813;&#33258;&#21160;&#23545;&#40784;&#22120;&#65292;&#21516;&#27493;&#22686;&#24378;&#20102;&#24369;&#25945;&#24072;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#20174;&#24369;&#21040;&#24378;&#30340;&#36229;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a follow-up study to OpenAI's recent superalignment work on Weak-to-Strong Generalization (W2SG). Superalignment focuses on ensuring that high-level AI systems remain consistent with human values and intentions when dealing with complex, high-risk tasks. The W2SG framework has opened new possibilities for empirical research in this evolving field. Our study simulates two phases of superalignment under the W2SG framework: the development of general superhuman models and the progression towards superintelligence. In the first phase, based on human supervision, the quality of weak supervision is enhanced through a combination of scalable oversight and ensemble learning, reducing the capability gap between weak teachers and strong students. In the second phase, an automatic alignment evaluator is employed as the weak supervisor. By recursively updating this auto aligner, the capabilities of the weak teacher models are synchronously enhanced, achieving weak-to-strong sup
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#22312;&#25910;&#38598;&#21040;&#30340;&#36712;&#36857;&#19978;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34394;&#24187;&#21644;&#32570;&#38519;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00658</link><description>&lt;p&gt;
&#36890;&#36807;&#25910;&#38598;&#36712;&#36857;&#21644;&#21512;&#25104;&#22870;&#21169;&#26469;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#22312;&#25910;&#38598;&#21040;&#30340;&#36712;&#36857;&#19978;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34394;&#24187;&#21644;&#32570;&#38519;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#36880;&#27493;&#21512;&#29702;&#21270;&#29983;&#25104;&#65292;&#23637;&#31034;&#20102;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#37325;&#35201;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23545;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#34394;&#24187;&#21644;&#32570;&#38519;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#20026;&#20102;&#25552;&#39640;&#29983;&#25104;&#21512;&#29702;&#21270;&#30340;&#21487;&#38752;&#24615;&#21644;&#24544;&#23454;&#24615;&#65292;&#27491;&#22312;&#36827;&#34892;&#22823;&#37327;&#24037;&#20316;&#12290;&#26377;&#20123;&#26041;&#27861;&#23558;&#25512;&#29702;&#24314;&#27169;&#20026;&#35268;&#21010;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#21017;&#19987;&#27880;&#20110;&#27880;&#37322;&#30340;&#36807;&#31243;&#30417;&#30563;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#35268;&#21010;&#30340;&#25628;&#32034;&#36807;&#31243;&#24448;&#24448;&#30001;&#20110;&#39057;&#32321;&#35780;&#20272;&#20013;&#38388;&#25512;&#29702;&#29366;&#24577;&#21644;&#24191;&#27867;&#30340;&#25506;&#32034;&#31354;&#38388;&#32780;&#23548;&#33268;&#39640;&#24310;&#36831;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30417;&#30563;&#25512;&#29702;&#36807;&#31243;&#23545;&#20110;LLM&#35757;&#32451;&#26469;&#35828;&#26159;&#26114;&#36149;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26469;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#36712;&#36857;&#30452;&#25509;&#26681;&#25454;&#21512;&#25104;&#30340;&#36807;&#31243;&#22870;&#21169;&#36827;&#34892;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through direct preference optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#26041;&#27861;&#65292;&#20934;&#30830;&#34913;&#37327;&#20102;&#30452;&#25509;&#35821;&#38899;-&#25991;&#26412;&#32763;&#35793;&#31995;&#32479;&#22312;&#28040;&#38500;&#38901;&#24459;&#23545;&#35805;&#20013;&#30340;&#27495;&#20041;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22312;&#38889;&#35821;-&#33521;&#35821;&#32763;&#35793;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#26126;&#30830;&#23637;&#31034;&#20102;&#30452;&#25509;&#32763;&#35793;&#31995;&#32479;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.00632</link><description>&lt;p&gt;
&#38889;&#35821;WH&#30701;&#35821;&#20013;&#20018;&#32852;&#21644;&#30452;&#25509;&#35821;&#38899;-&#25991;&#26412;&#32763;&#35793;&#20013;&#30340;&#38901;&#24459;
&lt;/p&gt;
&lt;p&gt;
Prosody in Cascade and Direct Speech-to-Text Translation: a case study on Korean Wh-Phrases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#26041;&#27861;&#65292;&#20934;&#30830;&#34913;&#37327;&#20102;&#30452;&#25509;&#35821;&#38899;-&#25991;&#26412;&#32763;&#35793;&#31995;&#32479;&#22312;&#28040;&#38500;&#38901;&#24459;&#23545;&#35805;&#20013;&#30340;&#27495;&#20041;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22312;&#38889;&#35821;-&#33521;&#35821;&#32763;&#35793;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#26126;&#30830;&#23637;&#31034;&#20102;&#30452;&#25509;&#32763;&#35793;&#31995;&#32479;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;-&#25991;&#26412;&#32763;&#35793;&#65288;S2TT&#65289;&#36890;&#24120;&#20351;&#29992;&#20018;&#32852;&#31995;&#32479;&#36827;&#34892;&#22788;&#29702;&#65292;&#20854;&#20013;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#29983;&#25104;&#19968;&#20010;&#36716;&#24405;&#65292;&#38543;&#21518;&#20256;&#36882;&#32473;&#32763;&#35793;&#27169;&#22411;&#12290;&#23613;&#31649;&#22312;&#24320;&#21457;&#30452;&#25509;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#20197;&#36991;&#20813;&#20256;&#25773;&#38169;&#35823;&#21644;&#20002;&#22833;&#38750;&#35821;&#35328;&#20869;&#23481;&#26041;&#38754;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#65292;&#20294;&#20043;&#21069;&#30340;&#30452;&#25509;S2TT&#24037;&#20316;&#38590;&#20197;&#30830;&#23450;&#22320;&#35777;&#26126;&#23558;&#22768;&#23398;&#20449;&#21495;&#30452;&#25509;&#25972;&#21512;&#21040;&#32763;&#35793;&#36807;&#31243;&#20013;&#30340;&#20248;&#21183;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#35780;&#20272;&#20197;&#23450;&#37327;&#34913;&#37327;&#30452;&#25509;S2TT&#31995;&#32479;&#22312;&#28040;&#38500;&#38901;&#24459;&#23545;&#35805;&#20013;&#30340;&#27495;&#20041;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;WH&#30701;&#35821;&#30340;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#38889;&#35821;-&#33521;&#35821;&#32763;&#35793;&#31995;&#32479;&#65292;&#23545;&#20110;&#35813;&#27979;&#35797;&#38598;&#26469;&#35828;&#65292;&#20351;&#29992;&#38901;&#24459;&#29305;&#24449;&#26159;&#20135;&#29983;&#20855;&#26377;&#27491;&#30830;&#24847;&#22270;&#30340;&#32763;&#35793;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#26080;&#35770;&#26159;&#38472;&#36848;&#21477;&#12289;&#26159;&#38750;&#38382;&#21477;&#36824;&#26159;&#30097;&#38382;&#21477;&#31561;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#28165;&#26970;&#22320;&#35777;&#26126;&#20102;&#30452;&#25509;&#32763;&#35793;&#31995;&#32479;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech-to-Text Translation (S2TT) has typically been addressed with cascade systems, where speech recognition systems generate a transcription that is subsequently passed to a translation model. While there has been a growing interest in developing direct speech translation systems to avoid propagating errors and losing non-verbal content, prior work in direct S2TT has struggled to conclusively establish the advantages of integrating the acoustic signal directly into the translation process. This work proposes using contrastive evaluation to quantitatively measure the ability of direct S2TT systems to disambiguate utterances where prosody plays a crucial role. Specifically, we evaluated Korean-English translation systems on a test set containing wh-phrases, for which prosodic features are necessary to produce translations with the correct intent, whether it's a statement, a yes/no question, a wh-question, and more. Our results clearly demonstrate the value of direct translation systems
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#36777;&#35770;&#20013;&#35782;&#21035;&#25919;&#27835;&#35282;&#33394;&#30340;&#25361;&#25112;&#65292;&#20351;&#29992;&#20102;&#20256;&#32479;&#30340;NLP&#32452;&#20214;&#19982;LLM&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#21457;&#29616;LLM&#22312;&#29983;&#25104;&#27491;&#30830;&#30340;&#27491;&#24335;&#24418;&#24335;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#23558;LLM&#19982;&#20998;&#31867;&#22120;&#32467;&#21512;&#30340;&#28151;&#21512;&#27169;&#22411;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00620</link><description>&lt;p&gt;
&#35770;&#36848;&#20013;&#30340;&#35282;&#33394;&#35782;&#21035;&#65306;LLMs&#30340;&#25361;&#25112;&#65311;
&lt;/p&gt;
&lt;p&gt;
Actor Identification in Discourse: A Challenge for LLMs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00620
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#36777;&#35770;&#20013;&#35782;&#21035;&#25919;&#27835;&#35282;&#33394;&#30340;&#25361;&#25112;&#65292;&#20351;&#29992;&#20102;&#20256;&#32479;&#30340;NLP&#32452;&#20214;&#19982;LLM&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#21457;&#29616;LLM&#22312;&#29983;&#25104;&#27491;&#30830;&#30340;&#27491;&#24335;&#24418;&#24335;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#23558;LLM&#19982;&#20998;&#31867;&#22120;&#32467;&#21512;&#30340;&#28151;&#21512;&#27169;&#22411;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26500;&#24314;&#36777;&#35770;&#32593;&#32476;&#24182;&#20998;&#26512;&#31038;&#20250;&#36777;&#35770;&#26102;&#65292;&#35782;&#21035;&#25919;&#27835;&#35282;&#33394;&#25552;&#20986;&#30340;&#20027;&#24352;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#35282;&#33394;&#35782;&#21035;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65306;&#36890;&#24120;&#65292;&#20027;&#24352;&#30340;&#26412;&#22320;&#25552;&#21450;&#20154;&#21482;&#26159;&#19968;&#20010;&#20195;&#35789;&#65288;&#8220;&#20182;&#24314;&#35758;[&#20027;&#24352;]&#8221;&#65289;&#65292;&#22240;&#27492;&#24674;&#22797;&#20986;&#27491;&#24335;&#30340;&#35282;&#33394;&#21517;&#31216;&#38656;&#35201;&#36827;&#34892;&#36777;&#35770;&#29702;&#35299;&#12290;&#25105;&#20204;&#23558;&#19968;&#20010;&#20256;&#32479;&#30340;&#19987;&#29992;NLP&#32452;&#20214;&#27969;&#27700;&#32447;&#65288;&#31867;&#20284;&#20110;&#30456;&#20851;&#25351;&#31216;&#20219;&#21153;&#20013;&#24212;&#29992;&#30340;&#32452;&#20214;&#65289;&#19982;&#19968;&#20010;LLM&#36827;&#34892;&#27604;&#36739;&#65292;LLM&#20284;&#20046;&#24456;&#36866;&#21512;&#36825;&#20010;&#29983;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#24503;&#22269;&#25253;&#32440;&#25253;&#36947;&#30340;&#35282;&#33394;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#34920;&#29616;&#20986;&#20046;&#24847;&#26009;&#22320;&#36739;&#24046;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;LLM&#22312;&#35782;&#21035;&#27491;&#30830;&#24341;&#29992;&#26041;&#38754;&#38750;&#24120;&#22909;&#65292;&#20294;&#22312;&#29983;&#25104;&#27491;&#30830;&#30340;&#27491;&#24335;&#24418;&#24335;&#26041;&#38754;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#36825;&#25351;&#21521;&#20102;LLMs&#22312;&#25511;&#21046;&#29983;&#25104;&#36755;&#20986;&#26041;&#38754;&#23384;&#22312;&#30340;&#28508;&#22312;&#38382;&#39064;&#12290;&#20107;&#23454;&#19978;&#65292;&#23558;LLM&#19982;&#20998;&#31867;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#35268;&#33539;&#21270;&#30340;&#28151;&#21512;&#27169;&#22411;&#33021;&#25913;&#21892;LLM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The identification of political actors who put forward claims in public debate is a crucial step in the construction of discourse networks, which are helpful to analyze societal debates. Actor identification is, however, rather challenging: Often, the locally mentioned speaker of a claim is only a pronoun ("He proposed that [claim]"), so recovering the canonical actor name requires discourse understanding. We compare a traditional pipeline of dedicated NLP components (similar to those applied to the related task of coreference) with a LLM, which appears a good match for this generation task. Evaluating on a corpus of German actors in newspaper reports, we find surprisingly that the LLM performs worse. Further analysis reveals that the LLM is very good at identifying the right reference, but struggles to generate the correct canonical form. This points to an underlying issue in LLMs with controlling generated output. Indeed, a hybrid model combining the LLM with a classifier to normaliz
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Reveal&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#35774;&#32622;&#20013;&#23545;&#22797;&#26434;&#24605;&#32500;&#38142;&#30340;&#33258;&#21160;&#39564;&#35777;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#35814;&#23613;&#30340;&#26631;&#31614;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#31572;&#26696;&#20013;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#30340;&#30456;&#20851;&#24615;&#12289;&#24402;&#22240;&#21644;&#36923;&#36753;&#27491;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00559</link><description>&lt;p&gt;
&#19968;&#26465;&#24605;&#32500;&#38142;&#26465;&#30340;&#24378;&#24230;&#21462;&#20915;&#20110;&#26368;&#24369;&#30340;&#29615;&#33410;&#65306;&#19968;&#20010;&#39564;&#35777;&#25512;&#29702;&#38142;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Reveal&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#35774;&#32622;&#20013;&#23545;&#22797;&#26434;&#24605;&#32500;&#38142;&#30340;&#33258;&#21160;&#39564;&#35777;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#35814;&#23613;&#30340;&#26631;&#31614;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#31572;&#26696;&#20013;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#30340;&#30456;&#20851;&#24615;&#12289;&#24402;&#22240;&#21644;&#36923;&#36753;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20419;&#20351;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#36880;&#27493;&#22238;&#31572;&#65288;&#20363;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#65289;&#26159;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#20854;&#20013;&#26356;&#20934;&#30830;&#30340;&#25512;&#29702;&#38142;&#36890;&#24120;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#35752;&#35770;&#20102;&#33258;&#21160;&#39564;&#35777;&#25512;&#29702;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#21644;&#25913;&#21892;&#20854;&#27491;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#27493;&#39588;&#32423;&#25968;&#25454;&#38598;&#65292;&#26080;&#27861;&#23545;&#36825;&#31867;&#39564;&#35777;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Reveal&#65306;&#25512;&#29702;&#39564;&#35777;&#35780;&#20272;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#35774;&#32622;&#20013;&#23545;&#22797;&#26434;&#24605;&#32500;&#38142;&#30340;&#33258;&#21160;&#39564;&#35777;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;Reveal&#21253;&#25324;&#23545;&#35821;&#35328;&#27169;&#22411;&#31572;&#26696;&#20013;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#30340;&#30456;&#20851;&#24615;&#12289;&#24402;&#22240;&#20110;&#35777;&#25454;&#27573;&#33853;&#20197;&#21450;&#36923;&#36753;&#27491;&#30830;&#24615;&#30340;&#20840;&#38754;&#26631;&#31614;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting language models to provide step-by-step answers (e.g., "Chain-of-Thought") is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning steps to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce Reveal: Reasoning Verification Evaluation, a new dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question answering settings. Reveal includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model's answer, across a wide variety of datasets and state-of-the-art language models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36229;&#32423;&#36807;&#28388;&#8221;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#36739;&#23567;&#21644;&#36739;&#24369;&#30340;&#27169;&#22411;&#23545;&#29992;&#20110;&#35757;&#32451;&#36739;&#22823;&#21644;&#36739;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#25968;&#25454;&#36827;&#34892;&#36807;&#28388;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#36807;&#28388;&#25104;&#26412;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00530</link><description>&lt;p&gt;
&#36229;&#32423;&#36807;&#28388;&#65306;&#29992;&#20110;&#24555;&#36895;&#25351;&#20196;&#35843;&#25972;&#30340;&#24369;&#21040;&#24378;&#25968;&#25454;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00530
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36229;&#32423;&#36807;&#28388;&#8221;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#36739;&#23567;&#21644;&#36739;&#24369;&#30340;&#27169;&#22411;&#23545;&#29992;&#20110;&#35757;&#32451;&#36739;&#22823;&#21644;&#36739;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#25968;&#25454;&#36827;&#34892;&#36807;&#28388;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#36807;&#28388;&#25104;&#26412;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#23545;&#20110;&#25913;&#36827;LLM&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36890;&#24120;&#20250;&#36935;&#21040;&#20302;&#36136;&#37327;&#21644;&#20887;&#20313;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#25351;&#20196;&#35843;&#25972;&#30340;&#25968;&#25454;&#36807;&#28388;&#22312;&#25552;&#39640;&#35843;&#25972;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#24456;&#37325;&#35201;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;LLMs&#22312;&#35813;&#36807;&#31243;&#20013;&#30340;&#21442;&#19982;&#65292;&#36825;&#20063;&#23548;&#33268;&#20102;&#39069;&#22806;&#30340;&#25104;&#26412;&#21644;&#35745;&#31639;&#12290;&#20026;&#20102;&#20943;&#23569;&#36807;&#28388;&#25104;&#26412;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36229;&#32423;&#36807;&#28388;&#65306;&#21487;&#20197;&#20351;&#29992;&#36739;&#23567;&#19988;&#36739;&#24369;&#30340;&#27169;&#22411;&#26469;&#36873;&#25321;&#35201;&#35843;&#25972;&#26356;&#22823;&#21644;&#26356;&#24378;&#27169;&#22411;&#30340;&#25968;&#25454;&#21527;&#65311;&#23613;&#31649;&#24369;&#21644;&#24378;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#39640;&#24230;&#19968;&#33268;&#30340;&#33021;&#21147;&#21487;&#20197;&#24863;&#30693;&#25351;&#20196;&#30340;&#38590;&#24230;&#21644;&#25968;&#25454;&#36873;&#25321;&#32467;&#26524;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#26356;&#23567;&#26356;&#39640;&#25928;&#30340;&#27169;&#22411;&#26469;&#36807;&#28388;&#29992;&#20110;&#35757;&#32451;&#26356;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#25968;&#25454;&#12290;&#23427;&#19981;&#20165;&#22823;&#22823;&#21152;&#24555;&#20102;&#25968;&#25454;&#36807;&#28388;&#30340;&#36895;&#24230;&#65292;&#32780;&#19988;&#32463;&#36807;&#36807;&#28388;&#25968;&#25454;&#24494;&#35843;&#30340;LLM&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning is critical to improve LLMs but usually suffers from low-quality and redundant data. Data filtering for instruction tuning has proved important in improving both the efficiency and performance of the tuning process. But it also leads to extra cost and computation due to the involvement of LLMs in this process. To reduce the filtering cost, we study Superfiltering: Can we use a smaller and weaker model to select data for finetuning a larger and stronger model? Despite the performance gap between weak and strong language models, we find their highly consistent capability to perceive instruction difficulty and data selection results. This enables us to use a much smaller and more efficient model to filter the instruction data used to train a larger language model. Not only does it largely speed up the data filtering, but the filtered-data-finetuned LLM achieves even better performance on standard benchmarks. Extensive experiments validate the efficacy and efficiency of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32463;&#27982;&#19988;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;EE-Tuning&#65292;&#21487;&#20197;&#20351;&#29992;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#25968;&#25454;&#38024;&#23545;&#26089;&#26399;&#32456;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#65292;&#36890;&#36807;&#24615;&#33021;&#20248;&#21270;&#21644;3D&#24182;&#34892;&#24615;&#23454;&#29616;&#21331;&#36234;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#19979;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#26089;&#26399;&#32456;&#27490;LLM&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.00518</link><description>&lt;p&gt;
EE-Tuning:&#19968;&#31181;&#32463;&#27982;&#19988;&#21487;&#25193;&#23637;&#30340;&#35843;&#25972;&#26089;&#26399;&#32456;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00518
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32463;&#27982;&#19988;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;EE-Tuning&#65292;&#21487;&#20197;&#20351;&#29992;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#25968;&#25454;&#38024;&#23545;&#26089;&#26399;&#32456;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#65292;&#36890;&#36807;&#24615;&#33021;&#20248;&#21270;&#21644;3D&#24182;&#34892;&#24615;&#23454;&#29616;&#21331;&#36234;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#19979;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#26089;&#26399;&#32456;&#27490;LLM&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;EE-Tuning&#65292;&#19968;&#31181;&#36731;&#37327;&#19988;&#32463;&#27982;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#35757;&#32451;/&#35843;&#25972;&#26089;&#26399;&#32456;&#27490;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#19982;&#23436;&#25972;&#21442;&#25968;&#30340;&#39044;&#35757;&#32451;&#24120;&#35265;&#26041;&#27861;&#19981;&#21516;&#65292;EE-Tuning&#36890;&#36807;&#22312;&#21442;&#25968;&#39640;&#25928;&#26041;&#24335;&#19979;&#22686;&#21152;&#39069;&#22806;&#30340;&#26089;&#26399;&#32456;&#27490;&#23618;&#65292;&#19982;&#20219;&#20309;&#39044;&#35757;&#32451;&#65288;&#21487;&#33021;&#26159;&#24494;&#35843;&#65289;&#30340;&#26631;&#20934;LLM&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#24615;&#33021;&#20248;&#21270;&#21644;&#23436;&#20840;&#20860;&#23481;3D&#24182;&#34892;&#24615;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#23454;&#29616;&#20102;EE-Tuning&#30340;&#21331;&#36234;&#35757;&#32451;&#25928;&#29575;&#12290;&#31995;&#32479;&#23454;&#39564;&#35777;&#23454;&#20102;EE-Tuning&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#19979;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#26089;&#26399;&#32456;&#27490;LLM&#25512;&#29702;&#12290;&#20026;&#20102;&#23558;&#26089;&#26399;&#32456;&#27490;LLMs&#25512;&#24191;&#21040;&#31038;&#21306;&#65292;&#25105;&#20204;&#22312;https://github.com/pan-x-c/EE-LLM&#19978;&#21457;&#24067;&#20102;EE-Tuning&#30340;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces EE-Tuning, a lightweight and economical solution to training/tuning early-exit large language models (LLMs). In contrast to the common approach of full-parameter pre-training, EE-Tuning augments any pre-trained (and possibly fine-tuned) standard LLM with additional early-exit layers that are tuned in a parameter-efficient manner, which requires significantly less computational resources and training data. Our implementation of EE-Tuning achieves outstanding training efficiency via extensive performance optimizations, as well as scalability due to its full compatibility with 3D parallelism. Results of systematic experiments validate the efficacy of EE-Tuning, confirming that effective early-exit LLM inference can be achieved with a limited training budget. In hope of making early-exit LLMs accessible to the community, we release the source code of our implementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.
&lt;/p&gt;</description></item><item><title>SA-MDKIF&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#24182;&#35757;&#32451;&#21307;&#23398;&#25216;&#33021;&#65292;&#24182;&#22312;&#25512;&#29702;&#20013;&#23558;&#20854;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#25552;&#39640;&#20102;&#21307;&#23398;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00474</link><description>&lt;p&gt;
SA-MDKIF&#65306;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00474
&lt;/p&gt;
&lt;p&gt;
SA-MDKIF&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#24182;&#35757;&#32451;&#21307;&#23398;&#25216;&#33021;&#65292;&#24182;&#22312;&#25512;&#29702;&#20013;&#23558;&#20854;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#25552;&#39640;&#20102;&#21307;&#23398;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#26377;&#25928;&#24212;&#29992;&#21463;&#21040;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#32570;&#20047;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#26694;&#26550;SA-MDKIF&#65292;&#26088;&#22312;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#23558;&#21307;&#23398;&#30693;&#35782;&#27880;&#20837;&#36890;&#29992;&#22411;LLMs&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;SA-MDKIF&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#25216;&#33021;&#35757;&#32451;&#21644;&#25216;&#33021;&#36866;&#24212;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;12&#31181;&#22522;&#26412;&#30340;&#21307;&#23398;&#25216;&#33021;&#65292;&#24182;&#20351;&#29992;AdaLoRA&#26681;&#25454;&#25105;&#20204;&#26500;&#24314;&#30340;&#32479;&#19968;&#26684;&#24335;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#36825;&#20123;&#25216;&#33021;&#12290;&#22312;&#19979;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#29305;&#23450;&#20219;&#21153;&#30340;&#19979;&#28216;&#25968;&#25454;&#26469;&#35757;&#32451;&#25216;&#33021;&#36335;&#30001;&#22120;&#65292;&#24182;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;&#35813;&#36335;&#30001;&#22120;&#23558;&#33719;&#21462;&#30340;&#25216;&#33021;&#19982;LLMs&#38598;&#25104;&#12290;&#23545;9&#20010;&#19981;&#21516;&#30340;&#21307;&#23398;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#27604;&#65292;SA-MDKIF&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;10-20&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, their effective application in the medical domain is hampered by a lack of medical domain knowledge. In this study, we present SA-MDKIF, a scalable and adaptable framework that aims to inject medical knowledge into general-purpose LLMs through instruction tuning, thereby enabling adaptability for various downstream tasks. SA-MDKIF consists of two stages: skill training and skill adaptation. In the first stage, we define 12 basic medical skills and use AdaLoRA to train these skills based on uniformly formatted instructional datasets that we have constructed. In the next stage, we train the skill router using task-specific downstream data and use this router to integrate the acquired skills with LLMs during inference. Experimental results on 9 different medical tasks show that SA-MDKIF improves performance by 10-20% compared to the origina
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25991;&#26723;&#20998;&#26512;&#21644;&#25991;&#26723;&#22270;&#20687;&#39044;&#27979;&#20013;&#35757;&#32451;&#35821;&#35328;-&#35270;&#35273;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#34920;&#26126;&#20351;&#29992;&#25351;&#20196;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#20351;&#29992;Polling-based Object Probing Evaluation (POPE)&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#25351;&#20196;&#35843;&#20248;&#24615;&#33021;&#30456;&#23545;&#20110;&#38646;-shot&#24615;&#33021;&#25552;&#39640;&#20102;11&#20493;&#21040;32&#20493;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#38750;&#25351;&#20196;&#24494;&#35843;&#25552;&#39640;&#20102;0.1%&#21040;4.2%&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#22240;&#20026;&#36825;&#20123;&#24615;&#33021;&#20173;&#26410;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#65288;94.36%&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.00453</link><description>&lt;p&gt;
&#25351;&#20196;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Instruction Makes a Difference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00453
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25991;&#26723;&#20998;&#26512;&#21644;&#25991;&#26723;&#22270;&#20687;&#39044;&#27979;&#20013;&#35757;&#32451;&#35821;&#35328;-&#35270;&#35273;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#34920;&#26126;&#20351;&#29992;&#25351;&#20196;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#20351;&#29992;Polling-based Object Probing Evaluation (POPE)&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#25351;&#20196;&#35843;&#20248;&#24615;&#33021;&#30456;&#23545;&#20110;&#38646;-shot&#24615;&#33021;&#25552;&#39640;&#20102;11&#20493;&#21040;32&#20493;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#38750;&#25351;&#20196;&#24494;&#35843;&#25552;&#39640;&#20102;0.1%&#21040;4.2%&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#22240;&#20026;&#36825;&#20123;&#24615;&#33021;&#20173;&#26410;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#65288;94.36%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Instruction Document Visual Question Answering (iDocVQA)&#25968;&#25454;&#38598;&#21644;Large Language Document (LLaDoc)&#27169;&#22411;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;-&#35270;&#35273;&#65288;LV&#65289;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#20998;&#26512;&#21644;&#25991;&#26723;&#22270;&#20687;&#39044;&#27979;&#12290;&#36890;&#24120;&#65292;&#29992;&#20110;DocVQA&#20219;&#21153;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#22312;&#32570;&#20047;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#34920;&#26126;&#20351;&#29992;&#36981;&#24490;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#26032;&#30340;Large Language and Vision Assistant (LLaVA)1.5&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#25991;&#26723;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#22522;&#20110;&#25237;&#31080;&#30340;&#23545;&#35937;&#25506;&#27979;&#35780;&#20272;&#65288;POPE&#65289;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#23548;&#20986;&#27169;&#22411;&#30340;&#23545;&#35937;&#24187;&#35273;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#20196;&#35843;&#20248;&#24615;&#33021;&#30456;&#23545;&#20110;&#38646;-shot&#24615;&#33021;&#25552;&#39640;&#20102;11&#20493;&#21040;32&#20493;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#38750;&#25351;&#20196;&#65288;&#20256;&#32479;&#20219;&#21153;&#65289;&#24494;&#35843;&#25552;&#39640;&#20102;0.1%&#21040;4.2%&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#36798;&#19981;&#21040;&#20154;&#31867;&#24615;&#33021;&#65288;94.36%&#65289;&#65292;&#36825;&#24847;&#21619;&#30528;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Instruction Document Visual Question Answering (iDocVQA) dataset and Large Language Document (LLaDoc) model, for training Language-Vision (LV) models for document analysis and predictions on document images, respectively. Usually, deep neural networks for the DocVQA task are trained on datasets lacking instructions. We show that using instruction-following datasets improves performance. We compare performance across document-related datasets using the recent state-of-the-art (SotA) Large Language and Vision Assistant (LLaVA)1.5 as the base model. We also evaluate the performance of the derived models for object hallucination using the Polling-based Object Probing Evaluation (POPE) dataset. The results show that instruction-tuning performance ranges from 11X to 32X of zero-shot performance and from 0.1% to 4.2% over non-instruction (traditional task) finetuning. Despite the gains, these still fall short of human performance (94.36%), implying there's much room for improveme
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#20855;&#26377;&#31038;&#20250;&#24847;&#35782;&#30340;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#30740;&#31350;&#23545;&#25239;&#21644;&#38543;&#24847;&#23545;&#35805;&#32972;&#26223;&#19979;&#30340;&#20146;&#31038;&#20250;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#27169;&#22411;&#24456;&#38590;&#35782;&#21035;&#33258;&#28982;&#23545;&#35805;&#20013;&#24494;&#22937;&#30340;&#19981;&#23433;&#20840;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#21452;&#27493;&#39588;&#30340;&#24494;&#35843;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#20146;&#31038;&#20250;&#34892;&#20026;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.00446</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#31038;&#20250;&#24847;&#35782;&#30340;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#23545;&#35805;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Dialog Safety using Socially Aware Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#20855;&#26377;&#31038;&#20250;&#24847;&#35782;&#30340;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#30740;&#31350;&#23545;&#25239;&#21644;&#38543;&#24847;&#23545;&#35805;&#32972;&#26223;&#19979;&#30340;&#20146;&#31038;&#20250;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#27169;&#22411;&#24456;&#38590;&#35782;&#21035;&#33258;&#28982;&#23545;&#35805;&#20013;&#24494;&#22937;&#30340;&#19981;&#23433;&#20840;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#21452;&#27493;&#39588;&#30340;&#24494;&#35843;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#20146;&#31038;&#20250;&#34892;&#20026;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30001;&#20110;&#21487;&#33021;&#20135;&#29983;&#19981;&#23433;&#20840;&#12289;&#26377;&#27602;&#12289;&#19981;&#36947;&#24503;&#25110;&#21361;&#38505;&#20869;&#23481;&#30340;&#28508;&#22312;&#39118;&#38505;&#32780;&#24341;&#36215;&#20851;&#27880;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24320;&#21457;&#20102;&#25968;&#25454;&#38598;&#65292;&#25945;&#20250;&#23545;&#35805;&#20195;&#29702;&#20154;&#36866;&#24403;&#22320;&#22238;&#24212;&#29305;&#23450;&#35774;&#35745;&#30340;&#21361;&#38505;&#20869;&#23481;&#30340;&#31038;&#20132;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23545;&#25239;&#24615;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20173;&#28982;&#24456;&#38590;&#35782;&#21035;&#33258;&#28982;&#20986;&#29616;&#22312;&#23545;&#35805;&#20013;&#30340;&#24494;&#22937;&#30340;&#19981;&#23433;&#20840;&#24773;&#20917;&#65292;&#25110;&#22312;&#38543;&#24847;&#29615;&#22659;&#20013;&#24341;&#20837;&#19981;&#24688;&#24403;&#30340;&#22238;&#24212;&#12290;&#20026;&#20102;&#20102;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#31243;&#24230;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#25239;&#21644;&#38543;&#24847;&#23545;&#35805;&#32972;&#26223;&#19979;&#30340;&#20146;&#31038;&#20250;&#24615;&#65292;&#24182;&#22312;&#26159;&#21542;&#20135;&#29983;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#20542;&#21521;&#26041;&#38754;&#23457;&#26597;&#20102;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#27493;&#39588;&#30340;&#24494;&#35843;&#36807;&#31243;&#65292;&#20351;&#29992;&#20855;&#26377;&#31038;&#20250;&#24847;&#35782;&#30340;n&#23545;&#23545;&#27604;&#25439;&#22833;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#36947;&#24503;&#23436;&#25972;&#35821;&#26009;&#24211;&#65288;MIC&#65289;&#31561;&#25968;&#25454;&#38598;&#26469;&#25972;&#21512;&#20146;&#31038;&#20250;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art conversational AI systems raise concerns due to their potential risks of generating unsafe, toxic, unethical, or dangerous content. Previous works have developed datasets to teach conversational agents the appropriate social paradigms to respond effectively to specifically designed hazardous content. However, models trained on these adversarial datasets still struggle to recognize subtle unsafe situations that appear naturally in conversations or introduce an inappropriate response in a casual context. To understand the extent of this problem, we study prosociality in both adversarial and casual dialog contexts and audit the response quality of general-purpose language models in terms of propensity to produce unsafe content. We propose a dual-step fine-tuning process to address these issues using a socially aware n-pair contrastive loss. Subsequently, we train a base model that integrates prosocial behavior by leveraging datasets like Moral Integrity Corpus (MIC) and P
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19987;&#21033;&#21709;&#24212;&#26234;&#33021;&#31995;&#32479;PARIS&#21644;LE-PARIS&#65292;&#36890;&#36807;&#26500;&#24314;OA&#20027;&#39064;&#25968;&#25454;&#24211;&#12289;&#24320;&#21457;&#21709;&#24212;&#27169;&#26495;&#20197;&#21450;&#23454;&#26045;&#25512;&#33616;&#31995;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#65292;&#26088;&#22312;&#21152;&#24555;&#19987;&#21033;&#24459;&#24072;&#22788;&#29702;&#23457;&#26597;&#24847;&#35265;&#22238;&#24212;&#30340;&#25928;&#29575;&#12290; &#36890;&#36807;&#22810;&#33539;&#24335;&#20998;&#26512;&#21644;&#38271;&#26399;&#25968;&#25454;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;OA&#20027;&#39064;&#30340;&#24314;&#35774;&#24615;&#21644;LLM&#23545;&#20110;&#22238;&#24212;&#33258;&#21160;&#29983;&#25104;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00421</link><description>&lt;p&gt;
&#20174;PARIS&#21040;LE-PARIS&#65306;&#36890;&#36807;&#25512;&#33616;&#31995;&#32479;&#21644;&#21327;&#20316;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#19987;&#21033;&#21709;&#24212;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19987;&#21033;&#21709;&#24212;&#26234;&#33021;&#31995;&#32479;PARIS&#21644;LE-PARIS&#65292;&#36890;&#36807;&#26500;&#24314;OA&#20027;&#39064;&#25968;&#25454;&#24211;&#12289;&#24320;&#21457;&#21709;&#24212;&#27169;&#26495;&#20197;&#21450;&#23454;&#26045;&#25512;&#33616;&#31995;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#65292;&#26088;&#22312;&#21152;&#24555;&#19987;&#21033;&#24459;&#24072;&#22788;&#29702;&#23457;&#26597;&#24847;&#35265;&#22238;&#24212;&#30340;&#25928;&#29575;&#12290; &#36890;&#36807;&#22810;&#33539;&#24335;&#20998;&#26512;&#21644;&#38271;&#26399;&#25968;&#25454;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;OA&#20027;&#39064;&#30340;&#24314;&#35774;&#24615;&#21644;LLM&#23545;&#20110;&#22238;&#24212;&#33258;&#21160;&#29983;&#25104;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19987;&#21033;&#23457;&#26597;&#20013;&#65292;&#23545;&#20110;&#21450;&#26102;&#21644;&#26377;&#25928;&#22320;&#22238;&#24212;&#23457;&#26597;&#24847;&#35265;&#65288;OAs&#65289;&#23545;&#20110;&#33719;&#24471;&#19987;&#21033;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#36807;&#21435;&#30340;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24456;&#23569;&#28041;&#21450;&#21040;&#36825;&#19968;&#26041;&#38754;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#19987;&#21033;&#23457;&#26597;&#24847;&#35265;&#21709;&#24212;&#26234;&#33021;&#31995;&#32479;&#65288;PARIS&#65289;&#21450;&#20854;&#20808;&#36827;&#29256;&#26412;LE-PARIS&#12290;&#36825;&#20123;&#31995;&#32479;&#26088;&#22312;&#21152;&#24555;&#19987;&#21033;&#24459;&#24072;&#22312;&#21327;&#20316;&#22788;&#29702;OA&#22238;&#24212;&#26041;&#38754;&#30340;&#25928;&#29575;&#12290;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#21253;&#25324;&#26500;&#24314;OA&#20027;&#39064;&#25968;&#25454;&#24211;&#65292;&#24320;&#21457;&#21709;&#24212;&#27169;&#26495;&#65292;&#20197;&#21450;&#23454;&#26045;&#25512;&#33616;&#31995;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#39564;&#35777;&#28041;&#21450;&#20351;&#29992;USPTO Office Action&#25968;&#25454;&#24211;&#21644;&#24459;&#24072;&#19982;&#25105;&#20204;&#31995;&#32479;&#30340;&#38271;&#26399;&#20132;&#20114;&#25968;&#25454;&#36827;&#34892;&#30340;&#22810;&#33539;&#24335;&#20998;&#26512;&#65292;&#20026;&#26399;&#20845;&#24180;&#12290;&#36890;&#36807;&#20116;&#20010;&#30740;&#31350;&#65292;&#25105;&#20204;&#21033;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#25552;&#20986;&#30340;Delphi&#36807;&#31243;&#26469;&#26816;&#39564;OA&#20027;&#39064;&#30340;&#24314;&#35774;&#24615;&#65288;&#30740;&#31350;1&#21644;2&#65289;&#65292;&#36824;&#26377;&#20351;&#29992;&#25512;&#33616;&#31995;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#26469;&#25552;&#39640;&#22238;&#24212;&#36136;&#37327;&#65288;&#30740;&#31350;3&#21644;4&#65289;&#65292;&#20197;&#21450;&#32463;&#36807;&#35757;&#32451;&#30340;LLM&#23545;&#20110;&#22238;&#24212;&#33258;&#21160;&#29983;&#25104;&#30340;&#21487;&#34892;&#24615;&#65288;&#30740;&#31350;5&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In patent prosecution, timely and effective responses to Office Actions (OAs) are crucial for acquiring patents, yet past automation and AI research have scarcely addressed this aspect. To address this gap, our study introduces the Patent Office Action Response Intelligence System (PARIS) and its advanced version, the Large Language Model Enhanced PARIS (LE-PARIS). These systems are designed to expedite the efficiency of patent attorneys in collaboratively handling OA responses. The systems' key features include the construction of an OA Topics Database, development of Response Templates, and implementation of Recommender Systems and LLM-based Response Generation. Our validation involves a multi-paradigmatic analysis using the USPTO Office Action database and longitudinal data of attorney interactions with our systems over six years. Through five studies, we examine the constructiveness of OA topics (studies 1 and 2) using topic modeling and the proposed Delphi process, the efficacy of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#22522;&#20110;&#25552;&#31034;&#39537;&#21160;&#30340;&#30693;&#35782;&#25429;&#33719;&#65292;&#29305;&#21035;&#20851;&#27880;&#25552;&#31034;&#21040;&#19977;&#20803;&#32452;&#30340;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#19987;&#38376;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#19977;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00414</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22522;&#20110;&#25552;&#31034;&#26102;&#38388;&#30340;&#31526;&#21495;&#30693;&#35782;&#25429;&#33719;
&lt;/p&gt;
&lt;p&gt;
Prompt-Time Symbolic Knowledge Capture with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#22522;&#20110;&#25552;&#31034;&#39537;&#21160;&#30340;&#30693;&#35782;&#25429;&#33719;&#65292;&#29305;&#21035;&#20851;&#27880;&#25552;&#31034;&#21040;&#19977;&#20803;&#32452;&#30340;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#19987;&#38376;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#19977;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#38469;&#24212;&#29992;&#65292;&#22914;&#20010;&#20154;AI&#21161;&#25163;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#29992;&#25143;&#29305;&#23450;&#30340;&#30693;&#35782;&#30456;&#32467;&#21512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;LLMs&#26412;&#36523;&#32570;&#20047;&#22522;&#20110;&#25552;&#31034;&#39537;&#21160;&#30340;&#30693;&#35782;&#25429;&#33719;&#26426;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#29616;&#26377;&#30340;LLMs&#33021;&#21147;&#23454;&#29616;&#22522;&#20110;&#25552;&#31034;&#39537;&#21160;&#30340;&#30693;&#35782;&#25429;&#33719;&#65292;&#29305;&#21035;&#20851;&#27880;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#36890;&#36807;&#20851;&#27880;&#25552;&#31034;&#21040;&#19977;&#20803;&#32452;&#65288;P2T&#65289;&#29983;&#25104;&#26469;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#38646;&#23556;&#36317;&#12289;&#23569;&#23556;&#36317;&#21644;&#24494;&#35843;&#19977;&#31181;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#19987;&#38376;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;https://github.com/HaltiaAI/paper-PTSKC&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting large language models (LLMs) with user-specific knowledge is crucial for real-world applications, such as personal AI assistants. However, LLMs inherently lack mechanisms for prompt-driven knowledge capture. This paper investigates utilizing the existing LLM capabilities to enable prompt-driven knowledge capture, with a particular emphasis on knowledge graphs. We address this challenge by focusing on prompt-to-triple (P2T) generation. We explore three methods: zero-shot prompting, few-shot prompting, and fine-tuning, and then assess their performance via a specialized synthetic dataset. Our code and datasets are publicly available at https://github.com/HaltiaAI/paper-PTSKC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#23545;&#25239;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#23398;&#29983;&#35770;&#25991;&#26816;&#27979;&#65292;&#36890;&#36807;&#26500;&#24314;AIG-ASAP&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#25991;&#26412;&#25200;&#21160;&#26041;&#27861;&#65292;&#25581;&#31034;&#29616;&#26377;&#26816;&#27979;&#22120;&#26131;&#34987;&#33258;&#21160;&#23545;&#25239;&#25915;&#20987;&#25152;&#32469;&#36807;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00412</link><description>&lt;p&gt;
&#38544;&#34255;&#20195;&#31508;&#32773;&#65306;&#23545;AI&#29983;&#25104;&#30340;&#23398;&#29983;&#35770;&#25991;&#26816;&#27979;&#36827;&#34892;&#23545;&#25239;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23545;&#25239;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#23398;&#29983;&#35770;&#25991;&#26816;&#27979;&#65292;&#36890;&#36807;&#26500;&#24314;AIG-ASAP&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#25991;&#26412;&#25200;&#21160;&#26041;&#27861;&#65292;&#25581;&#31034;&#29616;&#26377;&#26816;&#27979;&#22120;&#26131;&#34987;&#33258;&#21160;&#23545;&#25239;&#25915;&#20987;&#25152;&#32469;&#36807;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#22266;&#26377;&#30340;&#39118;&#38505;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#25220;&#34989;&#12289;&#20256;&#25773;&#20551;&#26032;&#38395;&#20197;&#21450;&#25945;&#32946;&#20064;&#39064;&#20013;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#26377;&#20960;&#31181;&#26816;&#27979;&#22120;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#22312;&#23545;&#25239;&#25200;&#21160;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#23398;&#29983;&#35770;&#25991;&#20889;&#20316;&#30340;&#32972;&#26223;&#19979;&#65292;&#20173;&#28982;&#34987;&#36739;&#23569;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#26500;&#24314;AIG-ASAP&#65292;&#19968;&#20010;&#22522;&#20110;AI&#29983;&#25104;&#30340;&#23398;&#29983;&#35770;&#25991;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#19968;&#31995;&#21015;&#39044;&#35745;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#35770;&#25991;&#30340;&#25991;&#26412;&#25200;&#21160;&#26041;&#27861;&#65292;&#21516;&#26102;&#36530;&#36991;&#26816;&#27979;&#12290;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;AIGC&#26816;&#27979;&#22120;&#22312;AIG-ASAP&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#34987;&#30452;&#25509;&#30340;&#33258;&#21160;&#23545;&#25239;&#25915;&#20987;&#25152;&#35268;&#36991;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35789;&#26367;&#25442;&#21644;&#21477;&#23376;&#26367;&#25442;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited remarkable capabilities in text generation tasks. However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises. Although several detectors have been proposed to address these concerns, their effectiveness against adversarial perturbations, specifically in the context of student essay writing, remains largely unexplored. This paper aims to bridge this gap by constructing AIG-ASAP, an AI-generated student essay dataset, employing a range of text perturbation methods that are expected to generate high-quality essays while evading detection. Through empirical experiments, we assess the performance of current AIGC detectors on the AIG-ASAP dataset. The results reveal that the existing detectors can be easily circumvented using straightforward automatic adversarial attacks. Specifically, we explore word substitution and sentence substitu
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;&#30740;&#31350;&#20102;Llama 2 Chat&#20013;&#30340;&#20559;&#35265;&#34920;&#31034;&#38382;&#39064;&#65292;&#21457;&#29616;&#35813;&#27169;&#22411;&#23384;&#22312;&#22266;&#26377;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35266;&#23519;&#21040;&#20559;&#35265;&#19982;&#27169;&#22411;&#25298;&#32477;&#22238;&#24212;&#30340;&#20542;&#21521;&#20043;&#38388;&#23384;&#22312;&#36127;&#30456;&#20851;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.00402</link><description>&lt;p&gt;
&#36890;&#36807;&#28608;&#27963;&#23548;&#21521;&#30740;&#31350; Llama 2 Chat &#20013;&#30340;&#20559;&#35265;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Investigating Bias Representations in Llama 2 Chat via Activation Steering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00402
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;&#30740;&#31350;&#20102;Llama 2 Chat&#20013;&#30340;&#20559;&#35265;&#34920;&#31034;&#38382;&#39064;&#65292;&#21457;&#29616;&#35813;&#27169;&#22411;&#23384;&#22312;&#22266;&#26377;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35266;&#23519;&#21040;&#20559;&#35265;&#19982;&#27169;&#22411;&#25298;&#32477;&#22238;&#24212;&#30340;&#20542;&#21521;&#20043;&#38388;&#23384;&#22312;&#36127;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880; Llama 2 7B Chat &#27169;&#22411;&#12290;&#38543;&#30528; LLMs &#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25972;&#21512;&#21040;&#20855;&#26377;&#37325;&#22823;&#31038;&#20250;&#24433;&#21709;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#19981;&#20250;&#24378;&#21270;&#29616;&#26377;&#30340;&#20559;&#35265;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;&#26469;&#25506;&#27979;&#21644;&#20943;&#36731;&#19982;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23447;&#25945;&#26377;&#20851;&#30340;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25805;&#32437;&#27169;&#22411;&#30340;&#28608;&#27963;&#26469;&#25351;&#23548;&#22238;&#24212;&#26397;&#21521;&#25110;&#36828;&#31163;&#26377;&#20559;&#35265;&#30340;&#36755;&#20986;&#65292;&#21033;&#29992;&#20174;StereoSet&#25968;&#25454;&#38598;&#21644;&#33258;&#23450;&#20041;GPT4&#29983;&#25104;&#30340;&#24615;&#21035;&#20559;&#35265;&#25552;&#31034;&#24471;&#21040;&#30340;&#23548;&#21521;&#21521;&#37327;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;Llama 2 7B Chat&#20013;&#22266;&#26377;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#21363;&#20351;&#22312;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#20043;&#21518;&#20173;&#28982;&#23384;&#22312;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#20559;&#35265;&#19982;&#27169;&#22411;&#25298;&#32477;&#22238;&#24212;&#30340;&#20542;&#21521;&#20043;&#38388;&#23384;&#22312;&#21487;&#39044;&#27979;&#30340;&#36127;&#30456;&#20851;&#20851;&#31995;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013; tend &#36235;&#21521;&#22686;&#21152;&#27169;&#22411;&#23545;&#19981;&#21516;&#24418;&#24335;&#31038;&#20250;&#20559;&#35265;&#30340;&#34920;&#31034;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the challenge of societal bias in Large Language Models (LLMs), focusing on the Llama 2 7B Chat model. As LLMs are increasingly integrated into decision-making processes with substantial societal impact, it becomes imperative to ensure these models do not reinforce existing biases. Our approach employs activation steering to probe for and mitigate biases related to gender, race, and religion. This method manipulates model activations to direct responses towards or away from biased outputs, utilizing steering vectors derived from the StereoSet dataset and custom GPT4 generated gender bias prompts. Our findings reveal inherent gender bias in Llama 2 7B Chat, persisting even after Reinforcement Learning from Human Feedback (RLHF). We also observe a predictable negative correlation between bias and the model's tendency to refuse responses. Significantly, our study uncovers that RLHF tends to increase the similarity in the model's representation of different forms of societal bia
&lt;/p&gt;</description></item><item><title>&#39640;&#25928;&#25506;&#32034;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#21487;&#20197;&#20197;&#36739;&#23569;&#30340;&#26597;&#35810;&#23454;&#29616;&#36739;&#39640;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25506;&#32034;&#26041;&#26696;&#30340;&#36873;&#25321;&#26159;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.00396</link><description>&lt;p&gt;
LLMs&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient Exploration for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00396
&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#25506;&#32034;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#21487;&#20197;&#20197;&#36739;&#23569;&#30340;&#26597;&#35810;&#23454;&#29616;&#36739;&#39640;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25506;&#32034;&#26041;&#26696;&#30340;&#36873;&#25321;&#26159;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#39640;&#25928;&#25506;&#32034;&#22312;&#33719;&#21462;&#20154;&#31867;&#21453;&#39304;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#19968;&#20010;&#20195;&#29702;&#31243;&#24207;&#22312;&#25910;&#21040;&#21453;&#39304;&#26102;&#23558;&#22870;&#21169;&#27169;&#22411;&#25311;&#21512;&#21040;&#26597;&#35810;&#19978;&#12290;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#20195;&#29702;&#31243;&#24207;&#20351;&#29992;&#21452;Thompson&#37319;&#26679;&#29983;&#25104;&#26597;&#35810;&#65292;&#19981;&#30830;&#23450;&#24615;&#30001;&#35748;&#30693;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#25928;&#25506;&#32034;&#20351;&#24471;&#24615;&#33021;&#27700;&#24179;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#26597;&#35810;&#19979;&#36798;&#21040;&#36739;&#39640;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25506;&#32034;&#26041;&#26696;&#30340;&#36873;&#25321;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#24418;&#24577;&#23398;&#21644;&#35789;&#20856;&#23398;&#27169;&#22411;&#25104;&#21151;&#35299;&#20915;&#20102;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#21517;&#35789;&#30340;&#24418;&#24577;&#21644;&#35789;&#27719;&#24314;&#27169;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#21644;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#19978;&#26174;&#33879;&#20248;&#20110;&#24120;&#29992;&#30340;&#20998;&#26512;&#22120;&#21644;&#29983;&#25104;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.00385</link><description>&lt;p&gt;
&#35745;&#31639;&#24418;&#24577;&#23398;&#21644;&#35789;&#20856;&#23398;&#27169;&#22411;&#23545;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#21517;&#35789;&#30340;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Computational Morphology and Lexicography Modeling of Modern Standard Arabic Nominals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#24418;&#24577;&#23398;&#21644;&#35789;&#20856;&#23398;&#27169;&#22411;&#25104;&#21151;&#35299;&#20915;&#20102;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#21517;&#35789;&#30340;&#24418;&#24577;&#21644;&#35789;&#27719;&#24314;&#27169;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#21644;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#19978;&#26174;&#33879;&#20248;&#20110;&#24120;&#29992;&#30340;&#20998;&#26512;&#22120;&#21644;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#21517;&#35789;&#23384;&#22312;&#35768;&#22810;&#24418;&#24577;&#21644;&#35789;&#27719;&#24314;&#27169;&#30340;&#25361;&#25112;&#65292;&#20294;&#20043;&#21069;&#23578;&#26410;&#24471;&#21040;&#19968;&#33268;&#30340;&#35299;&#20915;&#12290;&#26412;&#25991;&#35797;&#22270;&#23450;&#20041;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#21033;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#24418;&#24577;&#23398;&#26694;&#26550;&#26500;&#24314;&#19968;&#20010;&#20840;&#38754;&#19988;&#21487;&#25193;&#23637;&#30340;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#21517;&#35789;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#35774;&#35745;&#35299;&#20915;&#20102;&#21517;&#35789;&#22797;&#26434;&#30340;&#24418;&#24577;&#35268;&#21017;&#20197;&#21450;&#20854;&#33539;&#24335;&#30340;&#19981;&#35268;&#21017;&#24615;&#12290;&#19982;&#24120;&#29992;&#30340;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#24418;&#24577;&#20998;&#26512;&#22120;&#21644;&#29983;&#25104;&#22120;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#29616;&#23637;&#31034;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern Standard Arabic (MSA) nominals present many morphological and lexical modeling challenges that have not been consistently addressed previously. This paper attempts to define the space of such challenges, and leverage a recently proposed morphological framework to build a comprehensive and extensible model for MSA nominals. Our model design addresses the nominals' intricate morphotactics, as well as their paradigmatic irregularities. Our implementation showcases enhanced accuracy and consistency compared to a commonly used MSA morphological analyzer and generator. We make our models publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31038;&#20132;&#23186;&#20307;&#26426;&#22120;&#20154;&#26816;&#27979;&#20013;&#30340;&#26426;&#36935;&#21644;&#39118;&#38505;&#12290;&#36890;&#36807;&#25552;&#20986;&#28151;&#21512;&#24322;&#36136;&#19987;&#23478;&#26694;&#26550;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;LLM&#26426;&#22120;&#20154;&#26816;&#27979;&#22120;&#65292;&#24182;&#21457;&#29616;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#27880;&#31034;&#20363;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#21363;&#21487;&#21462;&#24471;&#36229;&#36807;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;LLM&#24341;&#23548;&#30340;&#25805;&#32437;&#31574;&#30053;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;&#29616;&#26377;&#26426;&#22120;&#20154;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00371</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#26816;&#27979;&#20013;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#20250;&#19982;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31038;&#20132;&#23186;&#20307;&#26426;&#22120;&#20154;&#26816;&#27979;&#20013;&#30340;&#26426;&#36935;&#21644;&#39118;&#38505;&#12290;&#36890;&#36807;&#25552;&#20986;&#28151;&#21512;&#24322;&#36136;&#19987;&#23478;&#26694;&#26550;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;LLM&#26426;&#22120;&#20154;&#26816;&#27979;&#22120;&#65292;&#24182;&#21457;&#29616;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#27880;&#31034;&#20363;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#21363;&#21487;&#21462;&#24471;&#36229;&#36807;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;LLM&#24341;&#23548;&#30340;&#25805;&#32437;&#31574;&#30053;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;&#29616;&#26377;&#26426;&#22120;&#20154;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#26426;&#22120;&#20154;&#26816;&#27979;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#26426;&#22120;&#20154;&#26816;&#27979;&#22120;&#21644;&#23545;&#25239;&#26426;&#22120;&#20154;&#31574;&#30053;&#20043;&#38388;&#30340;&#19968;&#22330;&#20891;&#22791;&#31454;&#36187;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#26368;&#26032;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#31038;&#20132;&#26426;&#22120;&#20154;&#26816;&#27979;&#20013;&#30340;&#26426;&#20250;&#21644;&#39118;&#38505;&#65292;&#23558;&#36825;&#22330;&#20891;&#22791;&#31454;&#36187;&#25552;&#21319;&#21040;&#20102;&#19968;&#20010;&#26032;&#30340;&#27700;&#24179;&#12290;&#20026;&#20102;&#25506;&#32034;&#26426;&#20250;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;LLM&#30340;&#26032;&#39062;&#26426;&#22120;&#20154;&#26816;&#27979;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#24322;&#36136;&#19987;&#23478;&#26694;&#26550;&#65292;&#23545;&#19981;&#21516;&#30340;&#29992;&#25143;&#20449;&#24687;&#27169;&#24577;&#36827;&#34892;&#21010;&#20998;&#21644;&#24449;&#26381;&#12290;&#20026;&#20102;&#25581;&#31034;&#39118;&#38505;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;LLM&#24341;&#23548;&#29992;&#25143;&#25991;&#26412;&#21644;&#32467;&#26500;&#21270;&#20449;&#24687;&#25805;&#32437;&#26469;&#36867;&#36991;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#23545;1,000&#20010;&#27880;&#37322;&#31034;&#20363;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#23601;&#21487;&#20197;&#20135;&#29983;&#19987;&#19994;&#30340;LLM&#65292;&#20854;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#39640;&#36798;9.1%&#65292;&#32780;LLM&#24341;&#23548;&#30340;&#25805;&#32437;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#29616;&#26377;&#26426;&#22120;&#20154;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media bot detection has always been an arms race between advancements in machine learning bot detectors and adversarial bot strategies to evade detection. In this work, we bring the arms race to the next level by investigating the opportunities and risks of state-of-the-art large language models (LLMs) in social bot detection. To investigate the opportunities, we design novel LLM-based bot detectors by proposing a mixture-of-heterogeneous-experts framework to divide and conquer diverse user information modalities. To illuminate the risks, we explore the possibility of LLM-guided manipulation of user textual and structured information to evade detection. Extensive experiments with three LLMs on two datasets demonstrate that instruction tuning on merely 1,000 annotated examples produces specialized LLMs that outperform state-of-the-art baselines by up to 9.1% on both datasets, while LLM-guided manipulation strategies could significantly bring down the performance of existing bot d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30693;&#35782;&#30450;&#21306;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;LLM&#21327;&#20316;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#22312;&#38754;&#23545;&#30693;&#35782;&#30450;&#21306;&#26102;&#25918;&#24323;&#22238;&#31572;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#25552;&#39640;&#25918;&#24323;&#20934;&#30830;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#39640;&#36798;19.3&#65285;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.00367</link><description>&lt;p&gt;
&#19981;&#35201;&#24187;&#35273;&#65292;&#25345;&#35266;&#65306;&#36890;&#36807;&#22810;LLM&#21327;&#20316;&#35782;&#21035;LLM&#30693;&#35782;&#30450;&#21306;
&lt;/p&gt;
&lt;p&gt;
Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30693;&#35782;&#30450;&#21306;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;LLM&#21327;&#20316;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#22312;&#38754;&#23545;&#30693;&#35782;&#30450;&#21306;&#26102;&#25918;&#24323;&#22238;&#31572;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#25552;&#39640;&#25918;&#24323;&#20934;&#30830;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#39640;&#36798;19.3&#65285;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23384;&#22312;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30693;&#35782;&#30340;&#21162;&#21147;&#65292;&#20294;&#30001;&#20110;&#30693;&#35782;&#30340;&#19981;&#26029;&#28436;&#21270;&#65292;LLM&#30693;&#35782;&#30450;&#21306;&#8212;&#8212;LLM&#20013;&#32570;&#22833;&#25110;&#36807;&#26102;&#30340;&#20449;&#24687;&#21487;&#33021;&#20250;&#19968;&#30452;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35782;&#21035;LLM&#30693;&#35782;&#30450;&#21306;&#21644;&#22312;&#23384;&#22312;&#30693;&#35782;&#30450;&#21306;&#26102;&#25918;&#24323;&#22238;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#27169;&#22411;&#26657;&#20934;&#25110;&#36866;&#24212;&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#25913;&#36827;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#22312;&#36991;&#20813;&#29983;&#25104;&#20302;&#32622;&#20449;&#24230;&#36755;&#20986;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#23427;&#20204;&#22312;&#33258;&#25105;&#21453;&#24605;&#21644;&#36807;&#24230;&#20381;&#36182;&#20445;&#30041;&#38598;&#26041;&#38754;&#30340;&#22833;&#36133;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#27169;&#22411;&#21327;&#20316;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;LLM&#25506;&#27979;&#20854;&#20182;LLM&#30340;&#30693;&#35782;&#30450;&#21306;&#65292;&#26080;&#35770;&#26159;&#21512;&#20316;&#36824;&#26159;&#31454;&#20105;&#12290;&#36890;&#36807;&#22312;&#22235;&#20010;&#21253;&#21547;&#22810;&#26679;&#30693;&#35782;&#39046;&#22495;&#30340;&#38382;&#31572;&#20219;&#21153;&#19978;&#23545;&#19977;&#20010;LLM&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#25581;&#31034;LLM&#30693;&#35782;&#30450;&#21306;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#26041;&#27861;&#22312;&#25918;&#24323;&#20934;&#30830;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#39640;&#36798;19.3&#65285;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20559;&#35265;&#26816;&#27979;&#26694;&#26550;IndiVec&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#32454;&#31890;&#24230;&#23186;&#20307;&#20559;&#35265;&#25968;&#25454;&#24211;&#65292;&#24182;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#30830;&#23450;&#36755;&#20837;&#30340;&#20559;&#35265;&#26631;&#31614;&#12290;&#36825;&#19968;&#26694;&#26550;&#20855;&#26377;&#36866;&#24212;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00345</link><description>&lt;p&gt;
IndiVec: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#20559;&#35265;&#25351;&#26631;&#30340;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
IndiVec: An Exploration of Leveraging Large Language Models for Media Bias Detection with Fine-Grained Bias Indicators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00345
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20559;&#35265;&#26816;&#27979;&#26694;&#26550;IndiVec&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#32454;&#31890;&#24230;&#23186;&#20307;&#20559;&#35265;&#25968;&#25454;&#24211;&#65292;&#24182;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#30830;&#23450;&#36755;&#20837;&#30340;&#20559;&#35265;&#26631;&#31614;&#12290;&#36825;&#19968;&#26694;&#26550;&#20855;&#26377;&#36866;&#24212;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#65292;&#22312;&#24403;&#20170;&#24433;&#21709;&#21147;&#24040;&#22823;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22609;&#36896;&#20010;&#20154;&#24577;&#24230;&#21644;&#35266;&#28857;&#30340;&#26102;&#20195;&#65292;&#36825;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#19982;&#20027;&#35201;&#20381;&#36182;&#35757;&#32451;&#29305;&#23450;&#27169;&#22411;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#20808;&#21069;&#24037;&#20316;&#30456;&#27604;&#65292;&#36825;&#23548;&#33268;&#20102;&#26377;&#38480;&#30340;&#36866;&#24212;&#24615;&#21644;&#22312;&#39046;&#22495;&#22806;&#25968;&#25454;&#19978;&#30340;&#27425;&#20248;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20559;&#35265;&#26816;&#27979;&#26694;&#26550;IndiVec&#65292;&#24314;&#31435;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;IndiVec&#39318;&#20808;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#25216;&#26415;&#26500;&#24314;&#19968;&#20010;&#32454;&#31890;&#24230;&#23186;&#20307;&#20559;&#35265;&#25968;&#25454;&#24211;&#12290;&#24403;&#38754;&#23545;&#26032;&#30340;&#36755;&#20837;&#36827;&#34892;&#20559;&#35265;&#26816;&#27979;&#26102;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20250;&#33258;&#21160;&#20174;&#21521;&#37327;&#25968;&#25454;&#24211;&#20013;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#25351;&#26631;&#65292;&#24182;&#37319;&#29992;&#22810;&#25968;&#25237;&#31080;&#30830;&#23450;&#36755;&#20837;&#30340;&#20559;&#35265;&#26631;&#31614;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;IndiVec&#22312;&#36866;&#24212;&#24615;&#65288;&#22312;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#65289;&#21644;&#21487;&#35299;&#37322;&#24615;&#65288;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on media bias detection, crucial in today's era of influential social media platforms shaping individual attitudes and opinions. In contrast to prior work that primarily relies on training specific models tailored to particular datasets, resulting in limited adaptability and subpar performance on out-of-domain data, we introduce a general bias detection framework, IndiVec, built upon large language models. IndiVec begins by constructing a fine-grained media bias database, leveraging the robust instruction-following capabilities of large language models and vector database techniques. When confronted with new input for bias detection, our framework automatically selects the most relevant indicator from the vector database and employs majority voting to determine the input's bias label. IndiVec excels compared to previous methods due to its adaptability (demonstrating consistent performance across diverse datasets from various sources) and explainability (providing exp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20197;&#25919;&#27835;&#20559;&#35265;&#20026;&#20363;&#65292;&#37327;&#21270;&#20102;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#22266;&#26377;&#20559;&#35265;&#12290;&#36890;&#36807;&#23545;&#27604;&#21508;&#31181;&#36866;&#24212;&#26041;&#27861;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#35266;&#28857;&#25688;&#35201;&#20219;&#21153;&#20013;&#21457;&#29616;&#65292;&#35843;&#25972;&#36739;&#23569;&#21442;&#25968;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#26631;&#20934;&#24494;&#35843;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#36739;&#23569;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.00322</link><description>&lt;p&gt;
&#20174;&#39044;&#35757;&#32451;&#21040;&#36866;&#24212;&#36807;&#31243;&#20013;&#30340;&#35266;&#28857;&#25688;&#35201;&#20559;&#35265;&#65306;&#25919;&#27835;&#20559;&#35265;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bias in Opinion Summarisation from Pre-training to Adaptation: A Case Study in Political Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20197;&#25919;&#27835;&#20559;&#35265;&#20026;&#20363;&#65292;&#37327;&#21270;&#20102;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#22266;&#26377;&#20559;&#35265;&#12290;&#36890;&#36807;&#23545;&#27604;&#21508;&#31181;&#36866;&#24212;&#26041;&#27861;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#35266;&#28857;&#25688;&#35201;&#20219;&#21153;&#20013;&#21457;&#29616;&#65292;&#35843;&#25972;&#36739;&#23569;&#21442;&#25968;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#26631;&#20934;&#24494;&#35843;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#36739;&#23569;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35266;&#28857;&#25688;&#35201;&#30340;&#30446;&#26631;&#26159;&#23558;&#20135;&#21697;&#35780;&#20215;&#12289;&#35752;&#35770;&#35770;&#22363;&#21644;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#31561;&#25991;&#20214;&#20013;&#21576;&#29616;&#30340;&#37325;&#35201;&#20449;&#24687;&#21644;&#24847;&#35265;&#24635;&#32467;&#25104;&#31616;&#27905;&#30340;&#25688;&#35201;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26377;&#25928;&#22320;&#20102;&#35299;&#20854;&#20013;&#30340;&#24847;&#35265;&#12290;&#29983;&#25104;&#26377;&#20559;&#35265;&#30340;&#25688;&#35201;&#21487;&#33021;&#20250;&#24433;&#21709;&#20844;&#20247;&#33286;&#35770;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20351;&#29992;&#25277;&#21462;&#27169;&#22411;&#30740;&#31350;&#35266;&#28857;&#25688;&#35201;&#30340;&#20559;&#35265;&#65292;&#20294;&#23545;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20197;&#25919;&#27835;&#20559;&#35265;&#20026;&#26696;&#20363;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#31181;&#37327;&#21270;&#29983;&#25104;&#27169;&#22411;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#28982;&#21518;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#36861;&#28335;&#21040;&#20351;&#29992;&#19981;&#21516;&#27169;&#22411;&#21644;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#31038;&#20132;&#23186;&#20307;&#35266;&#28857;&#25688;&#35201;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#22266;&#26377;&#20559;&#35265;&#12290;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#23545;&#27604;&#20102;&#21508;&#31181;&#36866;&#24212;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#35843;&#25972;&#36739;&#23569;&#21442;&#25968;&#30340;&#27169;&#22411;&#30456;&#27604;&#26631;&#20934;&#30340;&#24494;&#35843;&#27169;&#22411;&#26356;&#21152;&#26080;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opinion summarisation aims to summarise the salient information and opinions presented in documents such as product reviews, discussion forums, and social media texts into short summaries that enable users to effectively understand the opinions therein. Generating biased summaries has the risk of potentially swaying public opinion. Previous studies focused on studying bias in opinion summarisation using extractive models, but limited research has paid attention to abstractive summarisation models. In this study, using political bias as a case study, we first establish a methodology to quantify bias in abstractive models, then trace it from the pre-trained models to the task of summarising social media opinions using different models and adaptation methods. We find that most models exhibit intrinsic bias. Using a social media text summarisation dataset and contrasting various adaptation methods, we find that tuning a smaller number of parameters is less biased compared to standard fine-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#33258;&#28982;&#35821;&#35328;&#20013;&#21333;&#35789;&#30340;&#31561;&#32423;-&#39057;&#29575;&#20851;&#31995;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#30001;&#20844;&#24335;$f\propto r^{-\alpha} \cdot (r+\gamma)^{-\beta}$&#26469;&#27169;&#25311;&#65292;&#20854;&#20013;&#20851;&#38190;&#30340;&#21442;&#25968;&#26159;$\gamma$&#65292;&#23427;&#25551;&#36848;&#20102;&#35789;&#27719;&#22686;&#38271;&#23545;&#35821;&#26009;&#24211;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25628;&#32034;&#26368;&#20248;$\gamma$&#26469;&#20272;&#35745;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#29992;&#26696;&#20363;&#30740;&#31350;&#35770;&#35777;&#20102;&#35813;&#20844;&#24335;&#21644;&#21442;&#25968;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00271</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20013;&#31561;&#32423;-&#39057;&#29575;&#20851;&#31995;&#30340;&#19968;&#20010;&#20851;&#38190;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Crucial Parameter for Rank-Frequency Relation in Natural Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#33258;&#28982;&#35821;&#35328;&#20013;&#21333;&#35789;&#30340;&#31561;&#32423;-&#39057;&#29575;&#20851;&#31995;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#30001;&#20844;&#24335;$f\propto r^{-\alpha} \cdot (r+\gamma)^{-\beta}$&#26469;&#27169;&#25311;&#65292;&#20854;&#20013;&#20851;&#38190;&#30340;&#21442;&#25968;&#26159;$\gamma$&#65292;&#23427;&#25551;&#36848;&#20102;&#35789;&#27719;&#22686;&#38271;&#23545;&#35821;&#26009;&#24211;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25628;&#32034;&#26368;&#20248;$\gamma$&#26469;&#20272;&#35745;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#29992;&#26696;&#20363;&#30740;&#31350;&#35770;&#35777;&#20102;&#35813;&#20844;&#24335;&#21644;&#21442;&#25968;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#26420;&#32032;&#30340;&#24130;&#24459;$f\propto r^{-\alpha}$&#30456;&#27604;&#65292;$f \propto r^{-\alpha} \cdot (r+\gamma)^{-\beta}$&#22312;&#23454;&#35777;&#19978;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#20102;&#33258;&#28982;&#35821;&#35328;&#20013;&#21333;&#35789;&#30340;&#31561;&#32423;-&#39057;&#29575;&#65288;r-f&#65289;&#20851;&#31995;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#36825;&#20010;&#20844;&#24335;&#20013;&#65292;&#21807;&#19968;&#20851;&#38190;&#30340;&#21442;&#25968;&#26159;$\gamma$&#65292;&#23427;&#25551;&#36848;&#20102;&#35789;&#27719;&#22686;&#38271;&#23545;&#35821;&#26009;&#24211;&#30340;&#25269;&#25239;&#21147;&#12290;&#36824;&#25552;&#20986;&#20102;&#36890;&#36807;&#25628;&#32034;&#26368;&#20248;$\gamma$&#26469;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25216;&#26415;&#19978;&#24341;&#20837;&#20102;&#19968;&#20010;&#8220;&#38646;&#35789;&#8221;&#29992;&#20110;&#35745;&#31639;&#12290;&#36890;&#36807;&#20960;&#20010;&#26696;&#20363;&#30740;&#31350;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#20844;&#24335;&#21644;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
$f \propto r^{-\alpha} \cdot (r+\gamma)^{-\beta}$ has been empirically shown more precise than a na\"ive power law $f\propto r^{-\alpha}$ to model the rank-frequency ($r$-$f$) relation of words in natural languages. This work shows that the only crucial parameter in the formulation is $\gamma$, which depicts the resistance to vocabulary growth on a corpus. A method of parameter estimation by searching an optimal $\gamma$ is proposed, where a ``zeroth word'' is introduced technically for the calculation. The formulation and parameters are further discussed with several case studies.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#22120;&#65292;&#27169;&#22411;&#21517;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#24615;&#31574;&#30053;&#25200;&#21160;&#21644;&#22810;&#23545;&#27604;&#23398;&#20064;&#65292;&#22312;&#20943;&#23569;&#38543;&#26426;&#23631;&#34109;&#24341;&#36215;&#30340;&#20449;&#24687;&#20002;&#22833;&#30340;&#21516;&#26102;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#23569;&#26679;&#26412;&#21644;&#20010;&#20307;&#36755;&#20837;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00263</link><description>&lt;p&gt;
DetectGPT&#26159;&#21542;&#20805;&#20998;&#21033;&#29992;&#20102;&#25200;&#21160;&#65311;&#22522;&#20110;&#27169;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#36873;&#25321;&#24615;&#25200;&#21160;&#20250;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Does \textsc{DetectGPT} Fully Utilize Perturbation? Selective Perturbation on Model-Based Contrastive Learning Detector would be Better
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00263
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#22120;&#65292;&#27169;&#22411;&#21517;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#24615;&#31574;&#30053;&#25200;&#21160;&#21644;&#22810;&#23545;&#27604;&#23398;&#20064;&#65292;&#22312;&#20943;&#23569;&#38543;&#26426;&#23631;&#34109;&#24341;&#36215;&#30340;&#20449;&#24687;&#20002;&#22833;&#30340;&#21516;&#26102;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#23569;&#26679;&#26412;&#21644;&#20010;&#20307;&#36755;&#20837;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#20854;&#28389;&#29992;&#30340;&#22686;&#38271;&#20851;&#27880;&#12290;DetectGPT&#26159;&#19968;&#31181;&#38646;-shot&#22522;&#20110;&#24230;&#37327;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;&#25200;&#21160;&#24182;&#23637;&#29616;&#20102;&#24040;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;DetectGPT&#30340;&#38543;&#26426;&#25200;&#21160;&#31574;&#30053;&#21487;&#33021;&#20250;&#24341;&#20837;&#22122;&#22768;&#65292;&#38480;&#21046;&#20102;&#21487;&#21306;&#20998;&#24615;&#21644;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#23427;&#30340;&#36923;&#36753;&#22238;&#24402;&#27169;&#22359;&#20381;&#36182;&#20110;&#35774;&#32622;&#38408;&#20540;&#65292;&#36825;&#20250;&#24433;&#21709;&#20010;&#20307;&#25110;&#23567;&#25209;&#37327;&#36755;&#20837;&#30340;&#27867;&#21270;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#22120;&#65292;&#27169;&#22411;&#21517;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#24615;&#31574;&#30053;&#25200;&#21160;&#26469;&#32531;&#35299;&#38543;&#26426;&#23631;&#34109;&#25152;&#24341;&#36215;&#30340;&#37325;&#35201;&#20449;&#24687;&#20002;&#22833;&#65292;&#24182;&#21033;&#29992;&#22810;&#23545;&#27604;&#23398;&#20064;&#25429;&#25417;&#25200;&#21160;&#26399;&#38388;&#30340;&#38544;&#21547;&#27169;&#24335;&#20449;&#24687;&#65292;&#20415;&#20110;&#23569;&#37327;&#26679;&#26412;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#21517;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#27604;SOTA&#26041;&#27861;&#39640;&#20986;1.20\%&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;...
&lt;/p&gt;
&lt;p&gt;
The burgeoning capabilities of large language models (LLMs) have raised growing concerns about abuse. DetectGPT, a zero-shot metric-based unsupervised machine-generated text detector, first introduces perturbation and shows great performance improvement. However, DetectGPT's random perturbation strategy might introduce noise, limiting the distinguishability and further performance improvements. Moreover, its logit regression module relies on setting the threshold, which harms the generalizability and applicability of individual or small-batch inputs. Hence, we propose a novel detector, \modelname{}, which uses selective strategy perturbation to relieve the important information loss caused by random masking, and multi-pair contrastive learning to capture the implicit pattern information during perturbation, facilitating few-shot performance. The experiments show that \modelname{} outperforms the SOTA method by 1.20\% in accuracy on average on four public datasets. We further analyze th
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#32508;&#21512;&#35843;&#26597;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#28548;&#28165;&#20102;&#24187;&#35273;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23545;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;</title><link>https://arxiv.org/abs/2402.00253</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00253
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#32508;&#21512;&#35843;&#26597;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#28548;&#28165;&#20102;&#24187;&#35273;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23545;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#23454;&#38469;&#30340;&#23454;&#26045;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#8220;&#24187;&#35273;&#8221;&#65292;&#25110;&#32773;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#21363;&#35270;&#35273;&#20869;&#23481;&#19982;&#30456;&#24212;&#25991;&#26412;&#29983;&#25104;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#65292;&#22312;&#21033;&#29992;LVLMs&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#20221;&#32508;&#21512;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;LVLM&#30456;&#20851;&#30340;&#24187;&#35273;&#36827;&#34892;&#20102;&#28145;&#20837;&#21078;&#26512;&#65292;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#27010;&#35272;&#24182;&#20419;&#36827;&#26410;&#26469;&#30340;&#32531;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#28548;&#28165;&#20102;LVLMs&#20013;&#24187;&#35273;&#27010;&#24565;&#65292;&#21576;&#29616;&#20102;&#21508;&#31181;&#24187;&#35273;&#30151;&#29366;&#65292;&#24182;&#24378;&#35843;&#20102;LVLM&#24187;&#35273;&#22266;&#26377;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;LVLM&#29420;&#29305;&#24187;&#35273;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#35770;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#35843;&#26597;&#20102;&#36825;&#20123;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#21253;&#25324;&#26469;&#33258;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#32452;&#20214;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#23545;&#29616;&#26377;&#30340;&#24187;&#35273;&#32531;&#35299;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#21442;&#25968;&#21270;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20915;&#31574;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#36755;&#20837;-&#20915;&#31574;&#20043;&#38388;&#30340;&#36880;&#28857;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#32479;&#35745;&#19978;&#23545;&#20915;&#31574;&#21487;&#20449;&#24230;&#30340;&#35299;&#37322;&#12290;&#21478;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#20915;&#31574;&#20195;&#29702;&#35774;&#35745;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#31034;&#29983;&#25104;&#21160;&#20316;&#65292;&#24182;&#22312;&#23384;&#22312;&#22810;&#20010;&#39640;&#20272;&#35745;&#36880;&#28857;&#20381;&#36182;&#24615;&#30340;&#21160;&#20316;&#26102;&#35201;&#27714;&#29992;&#25143;&#25552;&#20379;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.00251</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#38750;&#21442;&#25968;&#21270;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20915;&#31574;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#21442;&#25968;&#21270;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20915;&#31574;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#36755;&#20837;-&#20915;&#31574;&#20043;&#38388;&#30340;&#36880;&#28857;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#32479;&#35745;&#19978;&#23545;&#20915;&#31574;&#21487;&#20449;&#24230;&#30340;&#35299;&#37322;&#12290;&#21478;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#20915;&#31574;&#20195;&#29702;&#35774;&#35745;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#31034;&#29983;&#25104;&#21160;&#20316;&#65292;&#24182;&#22312;&#23384;&#22312;&#22810;&#20010;&#39640;&#20272;&#35745;&#36880;&#28857;&#20381;&#36182;&#24615;&#30340;&#21160;&#20316;&#26102;&#35201;&#27714;&#29992;&#25143;&#25552;&#20379;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36880;&#27493;&#20915;&#31574;&#35268;&#21010;&#22312;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#21457;&#23637;&#20013;&#21463;&#21040;&#20851;&#27880;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#20915;&#31574;&#35268;&#21010;&#65292;&#20197;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#26159;&#30333;&#30418;&#26041;&#27861;&#65292;&#35201;&#20040;&#26159;&#35745;&#31639;&#22797;&#26434;&#65292;&#38480;&#21046;&#20102;&#40657;&#30418;&#19987;&#26377;LLMs&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#19968;&#31181;&#38750;&#21442;&#25968;&#21270;&#30340;LLMs&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#19968;&#25512;&#29702;&#26377;&#25928;&#22320;&#20272;&#35745;&#36755;&#20837;-&#20915;&#31574;&#20043;&#38388;&#30340;&#36880;&#28857;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#20196;&#29260;logits&#12290;&#35813;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#23545;&#20915;&#31574;&#21487;&#20449;&#24230;&#30340;&#32479;&#35745;&#35299;&#37322;&#12290;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#20915;&#31574;&#20195;&#29702;&#35774;&#35745;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#31034;&#22914;&#8220;&#25171;&#24320;&#28020;&#23460;&#28783;&#8221;&#65292;&#29983;&#25104;&#21160;&#20316;&#12290;&#24403;&#26377;&#22810;&#20010;&#21160;&#20316;&#30340;&#20272;&#35745;&#36880;&#28857;&#20381;&#36182;&#24615;&#37117;&#24456;&#39640;&#26102;&#65292;&#29992;&#25143;&#23558;&#34987;&#35201;&#27714;&#25552;&#20379;&#20559;&#22909;&#12290;&#24635;&#32467;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#26410;&#21442;&#25968;&#21270;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20915;&#31574;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our un
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20165;&#22522;&#20110;&#35299;&#30721;&#22120;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20844;&#20849;&#35821;&#38899;&#35782;&#21035;&#35821;&#26009;&#24211;&#19978;&#30340;&#26497;&#38480;&#65292;&#24320;&#21457;&#20102;&#21517;&#20026;DOTA&#30340;&#27169;&#22411;&#65292;&#22312;&#22823;&#22810;&#25968;&#33521;&#35821;&#35821;&#38899;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#27169;&#22411;&#65292;&#24182;&#20844;&#24320;&#20102;&#20195;&#30721;&#24211;&#21644;&#27169;&#22411;&#26816;&#26597;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.00235</link><description>&lt;p&gt;
&#25506;&#32034;&#20165;&#22522;&#20110;&#35299;&#30721;&#22120;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20844;&#20849;&#35821;&#38899;&#35782;&#21035;&#35821;&#26009;&#24211;&#19978;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Exploring the limits of decoder-only models trained on public speech recognition corpora
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20165;&#22522;&#20110;&#35299;&#30721;&#22120;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20844;&#20849;&#35821;&#38899;&#35782;&#21035;&#35821;&#26009;&#24211;&#19978;&#30340;&#26497;&#38480;&#65292;&#24320;&#21457;&#20102;&#21517;&#20026;DOTA&#30340;&#27169;&#22411;&#65292;&#22312;&#22823;&#22810;&#25968;&#33521;&#35821;&#35821;&#38899;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#27169;&#22411;&#65292;&#24182;&#20844;&#24320;&#20102;&#20195;&#30721;&#24211;&#21644;&#27169;&#22411;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#35268;&#27169;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65288;&#22914;Whisper&#21644;USM&#65289;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#20998;&#21035;&#35757;&#32451;&#20110;100&#19975;&#23567;&#26102;&#30340;&#24369;&#26631;&#27880;&#25968;&#25454;&#21644;1200&#19975;&#23567;&#26102;&#30340;&#32431;&#38899;&#39057;&#19987;&#26377;&#25968;&#25454;&#65292;&#23548;&#33268;&#20102;&#23545;&#22823;&#35268;&#27169;&#20844;&#20849;&#35821;&#38899;&#35782;&#21035;&#35821;&#26009;&#24211;&#21644;&#31454;&#20105;&#24615;&#24320;&#28304;&#27969;&#31243;&#30340;&#26356;&#24378;&#38656;&#27714;&#12290;&#19982;&#19978;&#36848;&#27169;&#22411;&#19981;&#21516;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#20165;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#35757;&#32451;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25552;&#20379;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35832;&#22914;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36873;&#25321;&#21644;&#24314;&#27169;&#32452;&#20214;&#31561;&#22240;&#32032;&#65292;&#20197;&#33719;&#24471;&#20165;&#20351;&#29992;&#20844;&#20849;&#33521;&#35821;&#35821;&#38899;&#35782;&#21035;&#35821;&#26009;&#24211;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;Decoder-Only Transformer for ASR (DOTA)&#27169;&#22411;&#22312;&#20960;&#20046;&#25152;&#26377;&#33521;&#35821;&#35821;&#38899;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#20013;&#20840;&#38754;&#20248;&#20110;Whisper&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#24320;&#28304;&#22797;&#21046;(OWSM)&#65292;&#24182;&#19988;&#22312;15&#20010;&#27979;&#35797;&#38598;&#20013;&#30340;7&#20010;&#20013;&#32988;&#36807;Whisper large-v3&#12290;&#25105;&#20204;&#20197;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#21457;&#24067;&#25105;&#20204;&#30340;&#20195;&#30721;&#24211;&#21644;&#27169;&#22411;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of industrial-scale speech recognition (ASR) models such as Whisper and USM, trained on 1M hours of weakly labelled and 12M hours of audio only proprietary data respectively, has led to a stronger need for large scale public ASR corpora and competitive open source pipelines. Unlike the said models, large language models are typically based on Transformer decoders, and it remains unclear if decoder-only models trained on public data alone can deliver competitive performance. In this work, we investigate factors such as choice of training datasets and modeling components necessary for obtaining the best performance using public English ASR corpora alone. Our Decoder-Only Transformer for ASR (DOTA) model comprehensively outperforms the encoder-decoder open source replication of Whisper (OWSM) on nearly all English ASR benchmarks and outperforms Whisper large-v3 on 7 out of 15 test sets. We release our codebase and model checkpoints under permissive license.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;AI&#31995;&#32479;&#34987;&#24212;&#29992;&#20110;&#25903;&#25345;&#24739;&#32773;&#20449;&#24687;&#38656;&#27714;&#30340;&#30740;&#31350;&#20013;&#65292;&#20197;&#25552;&#39640;&#24739;&#32773;&#23545;&#25918;&#23556;&#23398;&#25968;&#25454;&#30340;&#29702;&#35299;&#21644;&#31649;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19982;&#24739;&#32773;&#21644;&#21307;&#30103;&#19987;&#23478;&#30340;&#23545;&#35805;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24120;&#35265;&#30340;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#21644;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00234</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;AI&#31995;&#32479;&#33021;&#21542;&#25903;&#25345;&#24739;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Generative AI systems Capable of Supporting Information Needs of Patients?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00234
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#31995;&#32479;&#34987;&#24212;&#29992;&#20110;&#25903;&#25345;&#24739;&#32773;&#20449;&#24687;&#38656;&#27714;&#30340;&#30740;&#31350;&#20013;&#65292;&#20197;&#25552;&#39640;&#24739;&#32773;&#23545;&#25918;&#23556;&#23398;&#25968;&#25454;&#30340;&#29702;&#35299;&#21644;&#31649;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19982;&#24739;&#32773;&#21644;&#21307;&#30103;&#19987;&#23478;&#30340;&#23545;&#35805;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24120;&#35265;&#30340;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#26377;&#22797;&#26434;&#30142;&#30149;&#22914;&#30284;&#30151;&#30340;&#24739;&#32773;&#38754;&#20020;&#22797;&#26434;&#30340;&#20449;&#24687;&#25361;&#25112;&#65292;&#20182;&#20204;&#19981;&#20165;&#38656;&#35201;&#20102;&#35299;&#20182;&#20204;&#30340;&#30142;&#30149;&#65292;&#36824;&#38656;&#35201;&#23398;&#20250;&#22914;&#20309;&#31649;&#29702;&#23427;&#12290;&#19982;&#21307;&#30103;&#19987;&#23478;&#65288;&#25918;&#23556;&#31185;&#21307;&#24072;&#12289;&#32959;&#30244;&#31185;&#21307;&#24072;&#65289;&#23494;&#20999;&#20114;&#21160;&#21487;&#20197;&#25552;&#39640;&#24739;&#32773;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#30142;&#30149;&#39044;&#21518;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36164;&#28304;&#23494;&#38598;&#19988;&#21344;&#29992;&#20102;&#19987;&#23478;&#30340;&#26102;&#38388;&#65292;&#20351;&#20182;&#20204;&#26080;&#27861;&#23436;&#25104;&#20854;&#20182;&#20851;&#38190;&#20219;&#21153;&#12290;&#37492;&#20110;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#22312;&#25913;&#36827;&#21307;&#30103;&#31995;&#32479;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#22312;&#25918;&#23556;&#23398;&#25104;&#20687;&#25968;&#25454;&#32972;&#26223;&#19979;&#22914;&#20309;&#36127;&#36131;&#20219;&#22320;&#25903;&#25345;&#24739;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#24418;&#25104;&#24615;&#38656;&#27714;&#21457;&#29616;&#30740;&#31350;&#65292;&#21442;&#19982;&#32773;&#35752;&#35770;&#20102;&#19968;&#20010;&#34394;&#26500;&#36817;&#20146;&#30340;&#33016;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#21644;&#30456;&#20851;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#36890;&#36807;&#23545;&#21442;&#19982;&#32773;&#21644;&#21307;&#30103;&#19987;&#23478;&#20043;&#38388;&#30340;&#23545;&#35805;&#30340;&#20027;&#39064;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#24120;&#35265;&#30340;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patients managing a complex illness such as cancer face a complex information challenge where they not only must learn about their illness but also how to manage it. Close interaction with healthcare experts (radiologists, oncologists) can improve patient learning and thereby, their disease outcome. However, this approach is resource intensive and takes expert time away from other critical tasks. Given the recent advancements in Generative AI models aimed at improving the healthcare system, our work investigates whether and how generative visual question answering systems can responsibly support patient information needs in the context of radiology imaging data. We conducted a formative need-finding study in which participants discussed chest computed tomography (CT) scans and associated radiology reports of a fictitious close relative with a cardiothoracic radiologist. Using thematic analysis of the conversation between participants and medical experts, we identified commonly occurrin
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#20165;&#36827;&#34892;&#21435;&#35782;&#21035;&#25805;&#20316;&#24182;&#19981;&#33021;&#26377;&#25928;&#20445;&#25252;&#38544;&#31169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#20020;&#24202;&#31508;&#35760;&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#20020;&#24202;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#21457;&#29616;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.00179</link><description>&lt;p&gt;
&#19981;&#20165;&#20165;&#21435;&#35782;&#21035;&#21487;&#33021;&#26159;&#19981;&#22815;&#30340;
&lt;/p&gt;
&lt;p&gt;
De-identification is not always enough
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00179
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#20165;&#36827;&#34892;&#21435;&#35782;&#21035;&#25805;&#20316;&#24182;&#19981;&#33021;&#26377;&#25928;&#20445;&#25252;&#38544;&#31169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#20020;&#24202;&#31508;&#35760;&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#20020;&#24202;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#21457;&#29616;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20849;&#20139;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#65292;&#24120;&#24120;&#23558;&#21435;&#35782;&#21035;&#35270;&#20026;&#36275;&#22815;&#20445;&#25252;&#38544;&#31169;&#30340;&#25514;&#26045;&#12290;&#21512;&#25104;&#25968;&#25454;&#20063;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26368;&#36817;&#22312;&#29983;&#25104;&#25968;&#20540;&#21644;&#34920;&#26684;&#25968;&#25454;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#30340;&#25104;&#21151;&#20197;&#21450;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#21512;&#25104;&#30340;&#20020;&#24202;&#31508;&#35760;&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#30740;&#31350;&#30446;&#30340;&#30340;&#30495;&#23454;&#31508;&#35760;&#30340;&#21487;&#34892;&#26367;&#20195;&#21697;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65306;&#65288;i&#65289;&#23545;&#30495;&#23454;&#20020;&#24202;&#31508;&#35760;&#30340;&#21435;&#35782;&#21035;&#24182;&#19981;&#33021;&#20445;&#25252;&#35760;&#24405;&#20813;&#36973;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65307;&#65288;ii&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#20020;&#24202;&#31508;&#35760;&#30340;&#26032;&#26041;&#27861;&#65307;&#65288;iii&#65289;&#22312;&#20020;&#24202;&#39046;&#22495;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#21512;&#25104;&#29983;&#25104;&#31508;&#35760;&#30340;&#24615;&#33021;&#65307;&#65288;iv&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30446;&#26631;&#27169;&#22411;&#30340;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24403;&#21512;&#25104;&#29983;&#25104;&#30340;&#31508;&#35760;&#19982;&#30495;&#23454;&#31508;&#35760;&#30456;&#20284;&#26102;&#65292;&#36825;&#31181;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
For sharing privacy-sensitive data, de-identification is commonly regarded as adequate for safeguarding privacy. Synthetic data is also being considered as a privacy-preserving alternative. Recent successes with numerical and tabular data generative models and the breakthroughs in large generative language models raise the question of whether synthetically generated clinical notes could be a viable alternative to real notes for research purposes. In this work, we demonstrated that (i) de-identification of real clinical notes does not protect records against a membership inference attack, (ii) proposed a novel approach to generate synthetic clinical notes using the current state-of-the-art large language models, (iii) evaluated the performance of the synthetically generated notes in a clinical domain task, and (iv) proposed a way to mount a membership inference attack where the target model is trained with synthetic data. We observed that when synthetically generated notes closely match
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;MEME&#8221;&#30340;&#22810;&#37325;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#35270;&#20026;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#36890;&#36807;&#32467;&#21512;&#8220;&#20266;&#31508;&#35760;&#8221;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;&#32039;&#24613;&#31185;&#23460;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#21333;&#27169;&#24577;&#23884;&#20837;&#26041;&#27861;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#21307;&#38498;&#26426;&#26500;&#20043;&#38388;&#23384;&#22312;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00160</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#37325;&#23884;&#20837;&#27169;&#22411;&#30340;&#22810;&#27169;&#24335;&#20020;&#24202;&#20266;&#31508;&#35760;&#29992;&#20110;&#32039;&#24613;&#31185;&#23460;&#39044;&#27979;&#20219;&#21153;&#30340;&#21307;&#30103;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Multimodal Clinical Pseudo-notes for Emergency Department Prediction Tasks using Multiple Embedding Model for EHR (MEME)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;MEME&#8221;&#30340;&#22810;&#37325;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#35270;&#20026;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#36890;&#36807;&#32467;&#21512;&#8220;&#20266;&#31508;&#35760;&#8221;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;&#32039;&#24613;&#31185;&#23460;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#21333;&#27169;&#24577;&#23884;&#20837;&#26041;&#27861;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#21307;&#38498;&#26426;&#26500;&#20043;&#38388;&#23384;&#22312;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38024;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#22810;&#37325;&#23884;&#20837;&#27169;&#22411;&#65288;MEME&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;EHR&#35270;&#20026;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#8220;&#20266;&#31508;&#35760;&#8221;&#65292;&#21363;&#23545;&#34920;&#26684;&#24418;&#24335;&#30340;EHR&#27010;&#24565;&#65288;&#22914;&#35786;&#26029;&#21644;&#33647;&#29289;&#65289;&#36827;&#34892;&#25991;&#26412;&#34920;&#31034;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;EHR&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#36824;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#20998;&#21035;&#23884;&#20837;&#27599;&#20010;EHR&#27169;&#24577;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22810;&#20010;&#21307;&#38498;&#31995;&#32479;&#30340;&#24613;&#35786;&#31185;&#20013;&#24212;&#29992;MEME&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;MEME&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#21333;&#27169;&#24577;&#23884;&#20837;&#26041;&#27861;&#21644;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#25152;&#26377;&#27979;&#35797;&#27169;&#22411;&#22312;&#19981;&#21516;&#21307;&#38498;&#26426;&#26500;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce Multiple Embedding Model for EHR (MEME), an approach that views Electronic Health Records (EHR) as multimodal data. This approach incorporates "pseudo-notes", textual representations of tabular EHR concepts such as diagnoses and medications, and allows us to effectively employ Large Language Models (LLMs) for EHR representation. This framework also adopts a multimodal approach, embedding each EHR modality separately. We demonstrate the effectiveness of MEME by applying it to several tasks within the Emergency Department across multiple hospital systems. Our findings show that MEME surpasses the performance of both single modality embedding methods and traditional machine learning approaches. However, we also observe notable limitations in generalizability across hospital institutions for all tested models.
&lt;/p&gt;</description></item><item><title>Dolma&#26159;&#19968;&#20010;&#21253;&#21547;&#19977;&#19975;&#20159;&#20010;&#20196;&#29260;&#30340;&#24320;&#25918;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30740;&#31350;&#12290;&#23427;&#21253;&#21547;&#20102;&#26469;&#33258;&#22810;&#31181;&#26469;&#28304;&#30340;&#32593;&#32476;&#20869;&#23481;&#12289;&#31185;&#25216;&#35770;&#25991;&#12289;&#20195;&#30721;&#12289;&#20844;&#20849;&#39046;&#22495;&#22270;&#20070;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;&#30334;&#31185;&#20840;&#20070;&#26448;&#26009;&#12290;&#20026;&#20102;&#20419;&#36827;&#24320;&#25918;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#24320;&#28304;&#20102;&#25968;&#25454;&#25972;&#29702;&#24037;&#20855;&#21253;&#12290;</title><link>https://arxiv.org/abs/2402.00159</link><description>&lt;p&gt;
Dolma:&#19968;&#20010;&#21253;&#21547;&#19977;&#19975;&#20159;&#20010;&#20196;&#29260;&#30340;&#24320;&#25918;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00159
&lt;/p&gt;
&lt;p&gt;
Dolma&#26159;&#19968;&#20010;&#21253;&#21547;&#19977;&#19975;&#20159;&#20010;&#20196;&#29260;&#30340;&#24320;&#25918;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30740;&#31350;&#12290;&#23427;&#21253;&#21547;&#20102;&#26469;&#33258;&#22810;&#31181;&#26469;&#28304;&#30340;&#32593;&#32476;&#20869;&#23481;&#12289;&#31185;&#25216;&#35770;&#25991;&#12289;&#20195;&#30721;&#12289;&#20844;&#20849;&#39046;&#22495;&#22270;&#20070;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;&#30334;&#31185;&#20840;&#20070;&#26448;&#26009;&#12290;&#20026;&#20102;&#20419;&#36827;&#24320;&#25918;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#24320;&#28304;&#20102;&#25968;&#25454;&#25972;&#29702;&#24037;&#20855;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#22788;&#29702;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#28982;&#32780;&#65292;&#20851;&#20110;&#26368;&#20339;&#34920;&#29616;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#22914;&#20309;&#24320;&#21457;&#30340;&#24456;&#22810;&#32454;&#33410;&#24182;&#26410;&#25253;&#36947;&#12290;&#29305;&#21035;&#26159;&#65292;&#20854;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#20449;&#24687;&#24456;&#23569;&#34987;&#35752;&#35770;&#65306;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#24456;&#23569;&#25552;&#20379;&#26377;&#20851;&#20854;&#25968;&#25454;&#30340;&#20219;&#20309;&#20449;&#24687;&#65307;&#21363;&#20351;&#26159;&#24320;&#25918;&#27169;&#22411;&#20063;&#24456;&#23569;&#21457;&#24067;&#23427;&#20204;&#25152;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#65292;&#25110;&#32773;&#25552;&#20379;&#19968;&#20010;&#31934;&#30830;&#30340;&#22797;&#29616;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#36827;&#34892;&#19968;&#20123;&#35821;&#35328;&#24314;&#27169;&#30740;&#31350;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#27604;&#22914;&#29702;&#35299;&#35757;&#32451;&#25968;&#25454;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#12290;&#20026;&#20102;&#20419;&#36827;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#24320;&#25918;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;Dolma&#65292;&#19968;&#20010;&#21253;&#21547;&#19977;&#19975;&#20159;&#20010;&#20196;&#29260;&#30340;&#33521;&#35821;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#21508;&#31181;&#21508;&#26679;&#30340;&#32593;&#32476;&#20869;&#23481;&#12289;&#31185;&#25216;&#35770;&#25991;&#12289;&#20195;&#30721;&#12289;&#20844;&#20849;&#39046;&#22495;&#22270;&#20070;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;&#30334;&#31185;&#20840;&#20070;&#26448;&#26009;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#25972;&#29702;&#24037;&#20855;&#21253;&#65292;&#20197;&#20415;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#21644;&#20877;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have become a critical technology to tackling a wide range of natural language processing tasks, yet many details about how the best-performing language models were developed are not reported. In particular, information about their pretraining corpora is seldom discussed: commercial language models rarely provide any information about their data; even open models rarely release datasets they are trained on, or an exact recipe to reproduce them. As a result, it is challenging to conduct certain threads of language modeling research, such as understanding how training data impacts model capabilities and shapes their limitations. To facilitate open research on language model pretraining, we release Dolma, a three trillion tokens English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. In addition, we open source our data curation toolkit to enable further experimentation and reprodu
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#28041;&#21450;&#20102;&#22823;&#37327;&#30340;&#25968;&#23398;&#38382;&#39064;&#31867;&#22411;&#21644;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.00157</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#36827;&#23637;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Mathematical Reasoning: Progresses and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00157
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#28041;&#21450;&#20102;&#22823;&#37327;&#30340;&#25968;&#23398;&#38382;&#39064;&#31867;&#22411;&#21644;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#26159;&#35780;&#20272;&#20154;&#31867;&#26234;&#33021;&#22522;&#26412;&#35748;&#30693;&#33021;&#21147;&#30340;&#22522;&#30707;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#33258;&#21160;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#25968;&#23398;&#38382;&#39064;&#30340;&#31867;&#22411;&#38750;&#24120;&#24191;&#27867;&#65292;LLM&#30456;&#20851;&#25216;&#26415;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;&#20351;&#24471;&#22914;&#20309;&#21028;&#26029;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#30495;&#27491;&#36827;&#23637;&#21644;&#38556;&#30861;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#35843;&#26597;&#30740;&#31350;&#21253;&#25324;&#20102;&#20197;&#19979;&#22235;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;i&#65289;&#20840;&#38754;&#25506;&#32034;&#21508;&#31181;&#24050;&#32463;&#30740;&#31350;&#30340;&#25968;&#23398;&#38382;&#39064;&#21450;&#20854;&#30456;&#24212;&#25968;&#25454;&#38598;&#65307;ii&#65289;&#30740;&#31350;&#25552;&#20986;&#30340;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;LLM&#25216;&#26415;&#30340;&#33539;&#22260;&#65307;iii&#65289;&#27010;&#36848;&#24433;&#21709;LLM&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#22240;&#32032;&#21644;&#20851;&#27880;&#28857;&#65307;iv&#65289;&#38416;&#26126;&#20173;&#28982;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning serves as a cornerstone for assessing the fundamental cognitive capabilities of human intelligence. In recent times, there has been a notable surge in the development of Large Language Models (LLMs) geared towards the automated resolution of mathematical problems. However, the landscape of mathematical problem types is vast and varied, with LLM-oriented techniques undergoing evaluation across diverse datasets and settings. This diversity makes it challenging to discern the true advancements and obstacles within this burgeoning field. This survey endeavors to address four pivotal dimensions: i) a comprehensive exploration of the various mathematical problems and their corresponding datasets that have been investigated; ii) an examination of the spectrum of LLM-oriented techniques that have been proposed for mathematical problem-solving; iii) an overview of factors and concerns affecting LLMs in solving math; and iv) an elucidation of the persisting challenges with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36755;&#20013;&#35821;&#35328;&#36866;&#37197;&#22120;&#30340;&#20316;&#29992;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#20219;&#21153;&#12289;&#35821;&#35328;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;&#25928;&#26524;&#19981;&#19968;&#33268;&#12290;&#20445;&#30041;&#28304;&#35821;&#35328;&#36866;&#37197;&#22120;&#36890;&#24120;&#23548;&#33268;&#30456;&#21516;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00149</link><description>&lt;p&gt;
&#35821;&#35328;&#36866;&#37197;&#22120;&#22312;&#36328;&#35821;&#35328;NLU&#20256;&#36755;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Language Adapters in Cross-Lingual Transfer for NLU
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36755;&#20013;&#35821;&#35328;&#36866;&#37197;&#22120;&#30340;&#20316;&#29992;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#20219;&#21153;&#12289;&#35821;&#35328;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;&#25928;&#26524;&#19981;&#19968;&#33268;&#12290;&#20445;&#30041;&#28304;&#35821;&#35328;&#36866;&#37197;&#22120;&#36890;&#24120;&#23548;&#33268;&#30456;&#21516;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#27169;&#22359;&#21270;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#39640;&#25928;&#22320;&#36866;&#29992;&#20110;&#26032;&#30340;&#20219;&#21153;&#12289;&#39046;&#22495;&#21644;&#35821;&#35328;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#27809;&#26377;&#38024;&#23545;&#26576;&#31181;&#35821;&#35328;&#30340;&#30417;&#30563;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#35821;&#35328;&#36866;&#37197;&#22120;&#19982;&#20219;&#21153;&#36866;&#37197;&#22120;&#32467;&#21512;&#36215;&#26469;&#23637;&#31034;&#20102;&#28508;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#22522;&#20934;&#27979;&#35797;&#20013;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36755;&#20013;&#35821;&#35328;&#36866;&#37197;&#22120;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#35814;&#32454;&#30340;&#28040;&#34701;&#30740;&#31350;&#65292;&#20351;&#29992;&#20004;&#20010;&#22810;&#35821;&#35328;&#27169;&#22411;&#21644;&#19977;&#20010;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#25506;&#35752;&#20102;&#21253;&#21547;&#30446;&#26631;&#35821;&#35328;&#36866;&#37197;&#22120;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30446;&#26631;&#35821;&#35328;&#36866;&#37197;&#22120;&#30340;&#25928;&#26524;&#22312;&#20219;&#21153;&#12289;&#35821;&#35328;&#21644;&#27169;&#22411;&#20043;&#38388;&#39640;&#24230;&#19981;&#19968;&#33268;&#12290;&#20445;&#30041;&#28304;&#35821;&#35328;&#36866;&#37197;&#22120;&#36890;&#24120;&#20250;&#23548;&#33268;&#30456;&#21516;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#35757;&#32451;&#21518;&#21435;&#25481;&#35821;&#35328;&#36866;&#37197;&#22120;&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#20165;&#26377;&#24369;&#30340;&#36127;&#38754;&#25928;&#26524;&#65292;&#34920;&#26126;&#35821;&#35328;&#36866;&#37197;&#22120;&#23545;&#39044;&#27979;&#27809;&#26377;&#24378;&#28872;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modular deep learning has been proposed for the efficient adaption of pre-trained models to new tasks, domains and languages. In particular, combining language adapters with task adapters has shown potential where no supervised data exists for a language. In this paper, we explore the role of language adapters in zero-shot cross-lingual transfer for natural language understanding (NLU) benchmarks. We study the effect of including a target-language adapter in detailed ablation studies with two multilingual models and three multilingual datasets. Our results show that the effect of target-language adapters is highly inconsistent across tasks, languages and models. Retaining the source-language adapter instead often leads to an equivalent, and sometimes to a better, performance. Removing the language adapter after training has only a weak negative effect, indicating that the language adapters do not have a strong impact on the predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#35805;&#27169;&#22411;&#20013;&#35805;&#35821;&#38271;&#24230;&#22810;&#26679;&#24615;&#23545;&#29983;&#25104;&#21518;&#32493;&#22238;&#22797;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#26576;&#20123;&#23545;&#35805;&#31867;&#22411;&#20013;&#65292;&#21487;&#20197;&#23558;&#35805;&#35821;&#38271;&#24230;&#20943;&#23569;72%&#32780;&#19981;&#24433;&#21709;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.00143</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#24314;&#27169;&#20013;&#35753;&#25925;&#20107;&#31616;&#30701;
&lt;/p&gt;
&lt;p&gt;
Making a Long Story Short in Conversation Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#35805;&#27169;&#22411;&#20013;&#35805;&#35821;&#38271;&#24230;&#22810;&#26679;&#24615;&#23545;&#29983;&#25104;&#21518;&#32493;&#22238;&#22797;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#26576;&#20123;&#23545;&#35805;&#31867;&#22411;&#20013;&#65292;&#21487;&#20197;&#23558;&#35805;&#35821;&#38271;&#24230;&#20943;&#23569;72%&#32780;&#19981;&#24433;&#21709;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#36866;&#24212;&#20102;&#20855;&#26377;&#29420;&#29305;&#20010;&#24615;&#21644;&#19981;&#21516;&#20889;&#20316;&#39118;&#26684;&#30340;&#22810;&#26679;&#21270;&#29992;&#25143;&#12290;&#22312;&#22810;&#36718;&#23545;&#35805;&#24314;&#27169;&#39046;&#22495;&#65292;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#35805;&#35821;&#38271;&#24230;&#30340;&#22810;&#26679;&#24615;&#23545;&#23545;&#35805;&#27169;&#22411;&#29983;&#25104;&#30340;&#21518;&#32493;&#22238;&#22797;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;GPT-3&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#22810;&#20010;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#65292;&#25105;&#20204;&#23545;&#23545;&#35805;&#27169;&#22411;&#30340;&#36825;&#19968;&#26041;&#38754;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35805;&#35821;&#38271;&#24230;&#19982;&#23545;&#35805;&#31995;&#32479;&#29983;&#25104;&#30340;&#21518;&#32493;&#22238;&#22797;&#36136;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#32463;&#39564;&#24615;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#31867;&#22411;&#30340;&#23545;&#35805;&#20013;&#65292;&#35805;&#35821;&#38271;&#24230;&#21487;&#20197;&#20943;&#23569;72%&#65292;&#32780;&#21518;&#32493;&#22238;&#22797;&#30340;&#36136;&#37327;&#27809;&#26377;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversation systems accommodate diverse users with unique personalities and distinct writing styles. Within the domain of multi-turn dialogue modeling, this work studies the impact of varied utterance lengths on the quality of subsequent responses generated by conversation models. Using GPT-3 as the base model, multiple dialogue datasets, and several metrics, we conduct a thorough exploration of this aspect of conversational models. Our analysis sheds light on the complex relationship between utterance lengths and the quality of follow-up responses generated by dialogue systems. Empirical findings suggests that, for certain types of conversations, utterance lengths can be reduced by up to 72% without any noticeable difference in the quality of follow-up responses.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#24120;&#35782;&#25512;&#29702;&#26469;&#24314;&#27169;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#65292;&#36890;&#36807;&#25193;&#23637;&#21040;Deepfake Detection VQA&#20219;&#21153;&#26469;&#27169;&#25311;&#20154;&#31867;&#30452;&#35273;&#65292;&#35299;&#37322;&#26631;&#35760;&#22270;&#20687;&#20026;&#30495;&#23454;&#25110;&#20266;&#36896;&#30340;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2402.00126</link><description>&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#24120;&#35782;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Common Sense Reasoning for Deep Fake Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00126
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#24120;&#35782;&#25512;&#29702;&#26469;&#24314;&#27169;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#65292;&#36890;&#36807;&#25193;&#23637;&#21040;Deepfake Detection VQA&#20219;&#21153;&#26469;&#27169;&#25311;&#20154;&#31867;&#30452;&#35273;&#65292;&#35299;&#37322;&#26631;&#35760;&#22270;&#20687;&#20026;&#30495;&#23454;&#25110;&#20266;&#36896;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#29305;&#24449;&#36827;&#34892;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20108;&#20998;&#31867;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#22312;&#30417;&#30563;&#35757;&#32451;&#19979;&#25552;&#21462;&#20102;&#21487;&#33021;&#30340;&#20266;&#36896;&#29305;&#24449;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#34920;&#31034;&#19981;&#33258;&#28982;&#30340;&#8220;&#38750;&#29289;&#29702;&#8221;&#35821;&#20041;&#38754;&#37096;&#23646;&#24615; - &#27169;&#31946;&#30340;&#21457;&#38469;&#32447;&#12289;&#21452;&#30473;&#27611;&#12289;&#20725;&#30828;&#30340;&#30643;&#23380;&#25110;&#19981;&#33258;&#28982;&#30340;&#30382;&#32932;&#30528;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#38754;&#37096;&#23646;&#24615;&#36890;&#24120;&#36890;&#36807;&#24120;&#35782;&#25512;&#29702;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#23481;&#26131;&#24863;&#30693;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26174;&#33879;&#24615;&#22270;&#25552;&#20379;&#35270;&#35273;&#35299;&#37322;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21487;&#33021;&#24456;&#38590;&#34987;&#20154;&#31867;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#24120;&#35782;&#25512;&#29702;&#26469;&#24314;&#27169;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#21040;Deepfake Detection VQA&#65288;DD-VQA&#65289;&#20219;&#21153;&#65292;&#30446;&#30340;&#26159;&#27169;&#25311;&#20154;&#31867;&#30452;&#35273;&#26469;&#35299;&#37322;&#26631;&#35760;&#22270;&#20687;&#20026;&#30495;&#23454;&#25110;&#20266;&#36896;&#30340;&#21407;&#22240;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#19982;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30456;&#20851;&#30340;&#38382;&#39064;&#25552;&#20379;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art approaches rely on image-based features extracted via neural networks for the deepfake detection binary classification. While these approaches trained in the supervised sense extract likely fake features, they may fall short in representing unnatural `non-physical' semantic facial attributes -- blurry hairlines, double eyebrows, rigid eye pupils, or unnatural skin shading. However, such facial attributes are generally easily perceived by humans via common sense reasoning. Furthermore, image-based feature extraction methods that provide visual explanation via saliency maps can be hard to be interpreted by humans. To address these challenges, we propose the use of common sense reasoning to model deepfake detection, and extend it to the Deepfake Detection VQA (DD-VQA) task with the aim to model human intuition in explaining the reason behind labeling an image as either real or fake. To this end, we introduce a new dataset that provides answers to the questions related to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#22522;&#20110;&#27169;&#26495;&#21644;&#38750;&#27169;&#26495;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#27979;&#26041;&#27861;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#27169;&#22411;&#25490;&#21517;&#12289;&#32477;&#23545;&#24471;&#20998;&#21644;&#19982;&#22256;&#24785;&#24230;&#30340;&#20851;&#31995;&#31561;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.00123</link><description>&lt;p&gt;
&#27604;&#36739;&#22522;&#20110;&#27169;&#26495;&#21644;&#38750;&#27169;&#26495;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Comparing Template-based and Template-free Language Model Probing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#22522;&#20110;&#27169;&#26495;&#21644;&#38750;&#27169;&#26495;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#27979;&#26041;&#27861;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#27169;&#22411;&#25490;&#21517;&#12289;&#32477;&#23545;&#24471;&#20998;&#21644;&#19982;&#22256;&#24785;&#24230;&#30340;&#20851;&#31995;&#31561;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#19987;&#23478;&#21046;&#20316;&#30340;&#27169;&#26495;&#21644;&#33258;&#28982;&#21457;&#29983;&#30340;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#27169;&#22411;&#25506;&#27979;&#26041;&#27861;&#30340;&#24046;&#24322;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;16&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;10&#20010;&#33521;&#25991;&#25506;&#27979;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#21253;&#25324;4&#20010;&#22522;&#20110;&#27169;&#26495;&#30340;&#21644;6&#20010;&#38750;&#27169;&#26495;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#38024;&#23545;&#20197;&#19979;&#30740;&#31350;&#38382;&#39064;&#36827;&#34892;&#20102;&#22238;&#31572;&#65306;&#65288;RQ1&#65289;&#27169;&#22411;&#25490;&#21517;&#22312;&#20004;&#31181;&#26041;&#27861;&#20013;&#26159;&#21542;&#19981;&#21516;&#65311;&#65288;RQ2&#65289;&#27169;&#22411;&#30340;&#32477;&#23545;&#24471;&#20998;&#22312;&#20004;&#31181;&#26041;&#27861;&#20013;&#26159;&#21542;&#19981;&#21516;&#65311;&#65288;RQ3&#65289;RQ1&#21644;RQ2&#30340;&#31572;&#26696;&#22312;&#19968;&#33324;&#21644;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#20043;&#38388;&#26159;&#21542;&#19981;&#21516;&#65311;&#25105;&#20204;&#30340;&#21457;&#29616;&#26159;&#65306;1&#65289;&#38500;&#20102;&#39030;&#32423;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#22806;&#65292;&#22522;&#20110;&#27169;&#26495;&#21644;&#38750;&#27169;&#26495;&#26041;&#27861;&#36890;&#24120;&#25490;&#21517;&#19981;&#21516;&#12290;2&#65289;&#19982;&#24179;&#34892;&#30340;&#38750;&#27169;&#26495;&#21644;&#27169;&#26495;&#25552;&#31034;&#30456;&#27604;&#65292;&#20934;&#30830;&#24230;&#19979;&#38477;&#20102;&#26368;&#22810;42%&#12290;3&#65289;&#22312;&#38750;&#27169;&#26495;&#26041;&#27861;&#20013;&#65292;&#22256;&#24785;&#24230;&#19982;&#20934;&#30830;&#24230;&#21576;&#36127;&#30456;&#20851;&#65292;&#20294;&#26159;&#22312;&#22522;&#20110;&#27169;&#26495;&#30340;&#25506;&#27979;&#20013;&#65292;&#23427;&#20204;&#21576;&#27491;&#30456;&#20851;&#65292;&#36825;&#19982;&#30452;&#35273;&#30456;&#21453;&#12290;4&#65289;&#27169;&#22411;&#20542;&#21521;&#20110;&#39044;&#27979;&#30456;&#21516;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
The differences between cloze-task language model (LM) probing with 1) expert-made templates and 2) naturally-occurring text have often been overlooked. Here, we evaluate 16 different LMs on 10 probing English datasets -- 4 template-based and 6 template-free -- in general and biomedical domains to answer the following research questions: (RQ1) Do model rankings differ between the two approaches? (RQ2) Do models' absolute scores differ between the two approaches? (RQ3) Do the answers to RQ1 and RQ2 differ between general and domain-specific models? Our findings are: 1) Template-free and template-based approaches often rank models differently, except for the top domain-specific models. 2) Scores decrease by up to 42% Acc@1 when comparing parallel template-free and template-based prompts. 3) Perplexity is negatively correlated with accuracy in the template-free approach, but, counter-intuitively, they are positively correlated for template-based probing. 4) Models tend to predict the same
&lt;/p&gt;</description></item><item><title>D-Nikud&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24076;&#20271;&#26469;&#35821;&#28857;&#38899;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;LSTM&#21644;BERT-based&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#29616;&#20195;&#25991;&#26412;&#21644;&#26356;&#35814;&#32454;&#30340;&#28857;&#38899;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.00075</link><description>&lt;p&gt;
D-Nikud: &#20351;&#29992;LSTM&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#24076;&#20271;&#26469;&#35821;&#30340;&#28857;&#38899;&#21270;
&lt;/p&gt;
&lt;p&gt;
D-Nikud: Enhancing Hebrew Diacritization with LSTM and Pretrained Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00075
&lt;/p&gt;
&lt;p&gt;
D-Nikud&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24076;&#20271;&#26469;&#35821;&#28857;&#38899;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;LSTM&#21644;BERT-based&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#29616;&#20195;&#25991;&#26412;&#21644;&#26356;&#35814;&#32454;&#30340;&#28857;&#38899;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
D-Nikud&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24076;&#20271;&#26469;&#35821;&#28857;&#38899;&#21270;&#26041;&#27861;&#65292;&#23427;&#23558;LSTM&#32593;&#32476;&#21644;&#22522;&#20110;BERT&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#12290;&#21463;Nakdimon&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;TavBERT&#39044;&#35757;&#32451;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#31995;&#32479;&#20013;&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;&#26550;&#26500;&#36873;&#25321;&#21644;&#22810;&#26679;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#24378;&#35843;&#29616;&#20195;&#25991;&#26412;&#21644;&#26356;&#35814;&#32454;&#30340;&#28857;&#38899;&#21270;&#65292;&#22914;&#24615;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
D-Nikud, a novel approach to Hebrew diacritization that integrates the strengths of LSTM networks and BERT-based (transformer) pre-trained model. Inspired by the methodologies employed in Nakdimon, we integrate it with the TavBERT pre-trained model, our system incorporates advanced architectural choices and diverse training data. Our experiments showcase state-of-the-art results on several benchmark datasets, with a particular emphasis on modern texts and more specified diacritization like gender.
&lt;/p&gt;</description></item><item><title>EvoMerge&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21644;&#21512;&#24182;&#30340;&#31995;&#32479;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#20132;&#21449;&#21644;&#24494;&#35843;&#36827;&#34892;&#26435;&#37325;&#21464;&#24322;&#65292;&#26088;&#22312;&#23558;&#27169;&#22411;&#25512;&#21521;&#36229;&#36234;&#20256;&#32479;&#24494;&#35843;&#38480;&#21046;&#30340;&#36827;&#21270;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.00070</link><description>&lt;p&gt;
EvoMerge:&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
EvoMerge: Neuroevolution for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00070
&lt;/p&gt;
&lt;p&gt;
EvoMerge&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21644;&#21512;&#24182;&#30340;&#31995;&#32479;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#20132;&#21449;&#21644;&#24494;&#35843;&#36827;&#34892;&#26435;&#37325;&#21464;&#24322;&#65292;&#26088;&#22312;&#23558;&#27169;&#22411;&#25512;&#21521;&#36229;&#36234;&#20256;&#32479;&#24494;&#35843;&#38480;&#21046;&#30340;&#36827;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#24494;&#35843;&#20013;&#65292;&#24182;&#19981;&#24635;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#24448;&#24448;&#27169;&#22411;&#26356;&#25797;&#38271;&#27169;&#20223;&#19968;&#31181;&#25968;&#25454;&#24418;&#24335;&#32780;&#19981;&#20855;&#22791;&#26356;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#29978;&#33267;&#21487;&#33021;&#22833;&#21435;&#19968;&#20123;&#26234;&#33021;&#12290;&#36825;&#37324;&#25105;&#20171;&#32461;&#20102;EvoMerge&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21644;&#21512;&#24182;&#30340;&#31995;&#32479;&#24615;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#26435;&#37325;&#20132;&#21449;&#21644;&#24494;&#35843;&#36827;&#34892;&#26435;&#37325;&#21464;&#24322;&#65292;EvoMerge&#24314;&#31435;&#20102;&#19968;&#20010;&#26088;&#22312;&#23558;&#27169;&#22411;&#25512;&#21521;&#36229;&#36234;&#20256;&#32479;&#24494;&#35843;&#38480;&#21046;&#30340;&#36827;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extensive fine-tuning on Large Language Models does not always yield better results. Oftentimes, models tend to get better at imitating one form of data without gaining greater reasoning ability and may even end up losing some intelligence. Here I introduce EvoMerge, a systematic approach to large language model training and merging. Leveraging model merging for weight crossover and fine-tuning for weight mutation, EvoMerge establishes an evolutionary process aimed at pushing models beyond the limits of conventional fine-tuning.
&lt;/p&gt;</description></item><item><title>LLaMA&#21644;ChatGPT&#27604;&#36739;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;SMILES&#23383;&#31526;&#20018;&#23884;&#20837;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#65292;LLaMA&#30456;&#23545;&#20110;ChatGPT&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.00024</link><description>&lt;p&gt;
LLaMA&#21644;ChatGPT&#23884;&#20837;&#22312;&#20998;&#23376;&#23884;&#20837;&#20013;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00024
&lt;/p&gt;
&lt;p&gt;
LLaMA&#21644;ChatGPT&#27604;&#36739;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;SMILES&#23383;&#31526;&#20018;&#23884;&#20837;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#65292;LLaMA&#30456;&#23545;&#20110;ChatGPT&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;ChatGPT&#21644;LLaMA&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#37325;&#35270;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#37322;Simplified Molecular Input Line Entry System (SMILES)&#26041;&#38754;&#12290;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;SMILES&#23383;&#31526;&#20018;&#35299;&#30721;&#20026;&#21521;&#37327;&#34920;&#31034;&#65292;&#20026;&#29702;&#35299;&#21270;&#23398;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#21644;LLaMA&#22312;&#23884;&#20837;SMILES&#23383;&#31526;&#20018;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#38598;&#20013;&#22312;&#20004;&#20010;&#20851;&#38190;&#24212;&#29992;&#39046;&#22495;&#65306;&#20998;&#23376;&#24615;&#36136;&#65288;MP&#65289;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDI&#65289;&#39044;&#27979;&#65292;&#36825;&#22312;&#33647;&#29289;&#24320;&#21457;&#21644;&#21307;&#30103;&#20445;&#20581;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;LLaMA&#29983;&#25104;&#30340;SMILES&#23884;&#20837;&#22312;MP&#21644;DDI&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22522;&#20110;LLaMA&#30340;SMILES&#23884;&#20837;&#22312;&#36825;&#20004;&#20010;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#32467;&#35770;&#65306;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20013;&#24212;&#29992;LLMs&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;SMILES&#36827;&#34892;&#23884;&#20837;&#26041;&#38754;&#65292;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Large Language Models (LLMs) like ChatGPT and LLaMA are increasingly recognized for their potential in the field of cheminformatics, particularly in interpreting Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures. These LLMs can decode SMILES strings into vector representations, providing a novel approach to understanding chemical graphs.   Methods: We investigate the performance of ChatGPT and LLaMA in embedding SMILES strings. Our evaluation focuses on two key applications: molecular property (MP) prediction and drug-drug interaction (DDI) prediction, both essential in drug development and healthcare.   Results: We find that SMILES embeddings generated using LLaMA outperform those from ChatGPT in both MP and DDI prediction tasks. Notably, LLaMA-based SMILES embeddings show results comparable to existing methods in both prediction tasks.   Conclusion: The application of LLMs in cheminformatics, particularly in utilizi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#20998;&#35299;&#20026;&#35745;&#21010;&#22120;&#12289;&#35843;&#29992;&#22120;&#21644;&#24635;&#32467;&#22120;&#27169;&#22359;&#65292;&#20197;&#20811;&#26381;&#23567;&#22411;&#27169;&#22411;&#24615;&#33021;&#38480;&#21046;&#21644;&#24037;&#20855;&#26356;&#26032;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.07324</link><description>&lt;p&gt;
&#23567;&#22411;LLMs&#26159;&#24369;&#24037;&#20855;&#23398;&#20064;&#32773;&#65306;&#22810;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Small LLMs Are Weak Tool Learners: A Multi-LLM Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#20998;&#35299;&#20026;&#35745;&#21010;&#22120;&#12289;&#35843;&#29992;&#22120;&#21644;&#24635;&#32467;&#22120;&#27169;&#22359;&#65292;&#20197;&#20811;&#26381;&#23567;&#22411;&#27169;&#22411;&#24615;&#33021;&#38480;&#21046;&#21644;&#24037;&#20855;&#26356;&#26032;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#22823;&#22823;&#25193;&#23637;&#20102;&#29420;&#31435;LLMs&#30340;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#19982;&#22806;&#37096;&#24037;&#20855;&#65288;&#20363;&#22914;API&#65292;&#20989;&#25968;&#65289;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#33258;&#20027;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;&#24037;&#20855;&#20351;&#29992;&#30340;&#25361;&#25112;&#35201;&#27714;LLMs&#19981;&#20165;&#33021;&#29702;&#35299;&#29992;&#25143;&#26597;&#35810;&#24182;&#29983;&#25104;&#31572;&#26696;&#65292;&#36824;&#35201;&#22312;&#20219;&#21153;&#35268;&#21010;&#12289;&#35760;&#24518;&#31649;&#29702;&#12289;&#24037;&#20855;&#35843;&#29992;&#21644;&#32467;&#26524;&#24635;&#32467;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#20256;&#32479;&#26041;&#27861;&#38598;&#20013;&#20110;&#35757;&#32451;&#21333;&#20010;&#20855;&#22791;&#25152;&#26377;&#36825;&#20123;&#21151;&#33021;&#30340;LLM&#65292;&#20294;&#22312;&#23567;&#22411;&#27169;&#22411;&#19978;&#20250;&#20986;&#29616;&#24615;&#33021;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#27492;&#22806;&#65292;&#24403;&#24037;&#20855;&#26356;&#26032;&#26102;&#65292;&#25972;&#20010;LLM&#21487;&#33021;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#19978;&#36848;&#33021;&#21147;&#20998;&#35299;&#20026;&#35745;&#21010;&#22120;&#12289;&#35843;&#29992;&#22120;&#21644;&#24635;&#32467;&#22120;&#12290;&#27599;&#20010;&#32452;&#20214;&#30001;&#19968;&#20010;&#21333;&#29420;&#30340;LLM&#23454;&#29616;&#65292;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#33021;&#21147;&#65292;&#24182;&#19982;&#20854;&#20182;&#32452;&#20214;&#21512;&#20316;&#23436;&#25104;&#20219;&#21153;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#26694;&#26550;&#20415;&#20110;&#36827;&#34892;&#20010;&#20307;&#26356;&#26032;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete complex tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers but also excel in task planning, memory management, tool invocation, and result summarization. While traditional approaches focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. Moreover, the entire LLM may require retraining when tools are updated. To overcome these challenges, we propose a novel strategy that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with other components to accomplish the task. This modular framework facilitates individual updates and t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;-&#35270;&#39057;&#23450;&#20301;&#20013;&#24120;&#35782;&#25512;&#29702;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;CORONET&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#24120;&#35782;&#36827;&#34892;&#35270;&#39057;&#21644;&#29983;&#25104;&#30340;&#20266;&#26597;&#35810;&#20043;&#38388;&#30340;&#26725;&#25509;&#12290;&#23454;&#39564;&#35777;&#26126;CORONET&#22312;&#38646;&#26679;&#26412;&#21644;&#20256;&#32479;NLVL&#26041;&#27861;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.17429</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#20013;&#30340;&#24120;&#35782;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Commonsense for Zero-Shot Natural Language Video Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;-&#35270;&#39057;&#23450;&#20301;&#20013;&#24120;&#35782;&#25512;&#29702;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;CORONET&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#24120;&#35782;&#36827;&#34892;&#35270;&#39057;&#21644;&#29983;&#25104;&#30340;&#20266;&#26597;&#35810;&#20043;&#38388;&#30340;&#26725;&#25509;&#12290;&#23454;&#39564;&#35777;&#26126;CORONET&#22312;&#38646;&#26679;&#26412;&#21644;&#20256;&#32479;NLVL&#26041;&#27861;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;-&#35270;&#39057;&#23450;&#20301;&#65288;NLVL&#65289;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#35270;&#39057;&#29255;&#27573;&#21644;&#20266;&#26597;&#35810;&#27880;&#37322;&#65292;&#22312;&#20165;&#29992;&#21407;&#22987;&#35270;&#39057;&#25968;&#25454;&#35757;&#32451;NLVL&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20266;&#26597;&#35810;&#32463;&#24120;&#32570;&#20047;&#23545;&#28304;&#35270;&#39057;&#30340;&#25166;&#23454;&#22522;&#30784;&#65292;&#23548;&#33268;&#20869;&#23481;&#19981;&#32467;&#26500;&#21270;&#21644;&#19981;&#36830;&#36143;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;NLVL&#20013;&#24120;&#35782;&#25512;&#29702;&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CORONET&#65292;&#19968;&#20010;&#38646;&#26679;&#26412;NLVL&#26694;&#26550;&#65292;&#36890;&#36807;&#24120;&#35782;&#22686;&#24378;&#27169;&#22359;&#26725;&#25509;&#35270;&#39057;&#21644;&#29983;&#25104;&#30340;&#20266;&#26597;&#35810;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;CORONET&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26469;&#32534;&#30721;&#20174;&#30693;&#35782;&#22270;&#20013;&#25552;&#21462;&#30340;&#24120;&#35782;&#20449;&#24687;&#65292;&#26465;&#20214;&#26159;&#35270;&#39057;&#65292;&#20197;&#21450;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#26469;&#22686;&#24378;&#32534;&#30721;&#35270;&#39057;&#21644;&#20266;&#26597;&#35810;&#34920;&#31034;&#20197;&#36827;&#34892;&#23450;&#20301;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;CORONET&#22312;&#38646;&#26679;&#26412;&#21644;&#20256;&#32479;NLVL&#26041;&#27861;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Natural Language-Video Localization (NLVL) methods have exhibited promising results in training NLVL models exclusively with raw video data by dynamically generating video segments and pseudo-query annotations. However, existing pseudo-queries often lack grounding in the source video, resulting in unstructured and disjointed content. In this paper, we investigate the effectiveness of commonsense reasoning in zero-shot NLVL. Specifically, we present CORONET, a zero-shot NLVL framework that leverages commonsense to bridge the gap between videos and generated pseudo-queries via a commonsense enhancement module. CORONET employs Graph Convolution Networks (GCN) to encode commonsense information extracted from a knowledge graph, conditioned on the video, and cross-attention mechanisms to enhance the encoded video and pseudo-query representations prior to localization. Through empirical evaluations on two benchmark datasets, we demonstrate that CORONET surpasses both zero-shot and w
&lt;/p&gt;</description></item><item><title>RLHF&#31639;&#27861;&#20013;&#30340;IIA&#20551;&#35774;&#23548;&#33268;&#20102;&#20498;&#32622;&#28608;&#21169;&#65292;&#38480;&#21046;&#20102;&#26597;&#35810;&#26684;&#24335;&#21644;&#23398;&#20064;&#31639;&#27861;&#30340;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2312.01057</link><description>&lt;p&gt;
RLHF&#21644;IIA&#65306;&#20498;&#32622;&#28608;&#21169;
&lt;/p&gt;
&lt;p&gt;
RLHF and IIA: Perverse Incentives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01057
&lt;/p&gt;
&lt;p&gt;
RLHF&#31639;&#27861;&#20013;&#30340;IIA&#20551;&#35774;&#23548;&#33268;&#20102;&#20498;&#32622;&#28608;&#21169;&#65292;&#38480;&#21046;&#20102;&#26597;&#35810;&#26684;&#24335;&#21644;&#23398;&#20064;&#31639;&#27861;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;RLHF&#65289;&#21487;&#20197;&#28608;&#21169;&#19982;&#20559;&#22909;&#19981;&#31526;&#30340;&#22238;&#24212;&#65292;&#22240;&#20026;&#23427;&#20204;&#22522;&#20110;&#20551;&#35774;&#26080;&#20851;&#27010;&#25324;&#30340;&#27169;&#22411;&#65288;IIA&#65289;&#12290;IIA&#24341;&#21457;&#30340;&#20498;&#32622;&#28608;&#21169;&#38459;&#30861;&#20102;&#26597;&#35810;&#26684;&#24335;&#21644;&#23398;&#20064;&#31639;&#27861;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing algorithms for reinforcement learning from human feedback (RLHF) can incentivize responses at odds with preferences because they are based on models that assume independence of irrelevant alternatives (IIA). The perverse incentives induced by IIA hinder innovations on query formats and learning algorithms.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#29702;&#24615;&#21270;&#33021;&#21147;&#26377;&#24453;&#25506;&#32034;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#32534;&#20889;&#30340;&#31034;&#20363;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#21463;&#27426;&#36814;&#30340;&#22522;&#20110;&#19990;&#30028;&#30693;&#35782;&#30340;&#29702;&#24615;&#21270;&#26041;&#24335;&#12290;&#36825;&#20123;&#29702;&#24615;&#21270;&#26041;&#24335;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#65292;&#22312;&#38169;&#35823;&#39044;&#27979;&#30340;&#29702;&#24615;&#21270;&#26041;&#38754;&#20250;&#25439;&#23475;&#20154;&#31867;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;</title><link>https://arxiv.org/abs/2311.05085</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23450;&#24615;&#20026;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#29702;&#24615;&#21270;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05085
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#29702;&#24615;&#21270;&#33021;&#21147;&#26377;&#24453;&#25506;&#32034;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#32534;&#20889;&#30340;&#31034;&#20363;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#21463;&#27426;&#36814;&#30340;&#22522;&#20110;&#19990;&#30028;&#30693;&#35782;&#30340;&#29702;&#24615;&#21270;&#26041;&#24335;&#12290;&#36825;&#20123;&#29702;&#24615;&#21270;&#26041;&#24335;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#65292;&#22312;&#38169;&#35823;&#39044;&#27979;&#30340;&#29702;&#24615;&#21270;&#26041;&#38754;&#20250;&#25439;&#23475;&#20154;&#31867;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20960;&#20046;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#29983;&#25104;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25552;&#20379;&#22522;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#20805;&#20998;&#29702;&#24615;&#25903;&#25345;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#31867;&#20219;&#21153;&#65292;&#27604;&#22914;&#24120;&#35782;&#24615;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#38656;&#35201;&#22522;&#20110;&#19990;&#30028;&#30693;&#35782;&#30340;&#29702;&#24615;&#26469;&#25903;&#25345;&#39044;&#27979;&#24182;&#25512;&#32763;&#22791;&#36873;&#36873;&#39033;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#32534;&#20889;&#30340;&#26679;&#20363;&#20197;&#23569;&#37327;&#26679;&#26412;&#30340;&#26041;&#24335;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#29983;&#25104;&#30693;&#35782;&#24341;&#23548;&#30340;&#29702;&#24615;&#21270;&#20219;&#21153;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#24037;&#20154;&#32676;&#20307;&#26356;&#21916;&#27426;&#22522;&#20110;&#30693;&#35782;&#30340;&#29702;&#24615;&#21270;&#26041;&#24335;&#65292;&#35748;&#20026;&#20854;&#20855;&#26377;&#20107;&#23454;&#24615;&#12289;&#20805;&#20998;&#24615;&#21644;&#20840;&#38754;&#24615;&#30340;&#21453;&#39539;&#12290;&#34429;&#28982;LLMs&#29983;&#25104;&#30340;&#29702;&#24615;&#21270;&#26041;&#24335;&#26356;&#21463;&#27426;&#36814;&#65292;&#20294;&#36824;&#38656;&#35201;&#22312;&#31616;&#27905;&#24615;&#21644;&#26032;&#39062;&#24615;&#26041;&#38754;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#22312;&#21478;&#19968;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38169;&#35823;&#27169;&#22411;&#39044;&#27979;&#30340;&#29702;&#24615;&#21270;&#22914;&#20309;&#20405;&#34432;&#20154;&#31867;&#23545;LLMs&#29983;&#25104;&#30340;&#29702;&#24615;&#21270;&#30340;&#20449;&#20219;&#12290;&#22312;&#36825;&#20123;&#35266;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are proficient at generating fluent text with minimal task-specific supervision. Yet, their ability to provide well-grounded rationalizations for knowledge-intensive tasks remains under-explored. Such tasks, like commonsense multiple-choice questions, require rationales based on world knowledge to support predictions and refute alternate options. We consider the task of generating knowledge-guided rationalization in natural language by using expert-written examples in a few-shot manner. Surprisingly, crowd-workers preferred knowledge-grounded rationales over crowdsourced rationalizations, citing their factuality, sufficiency, and comprehensive refutations. Although LLMs-generated rationales were preferable, further improvements in conciseness and novelty are required. In another study, we show how rationalization of incorrect model predictions erodes humans' trust in LLM-generated rationales. Motivated by these observations, we create a two-stage pipeline t
&lt;/p&gt;</description></item><item><title>InstructRetro&#26159;&#30446;&#21069;&#35268;&#27169;&#26368;&#22823;&#30340;&#20351;&#29992;&#26816;&#32034;&#39044;&#35757;&#32451;&#30340;LLM&#65292;&#25193;&#23637;&#20102;&#22522;&#30784;&#27169;&#22411;Retro 48B&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#20248;&#22312;&#21508;&#31181;&#38646;&#26679;&#20363;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2310.07713</link><description>&lt;p&gt;
InstructRetro: &#26816;&#32034;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#20013;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.07713
&lt;/p&gt;
&lt;p&gt;
InstructRetro&#26159;&#30446;&#21069;&#35268;&#27169;&#26368;&#22823;&#30340;&#20351;&#29992;&#26816;&#32034;&#39044;&#35757;&#32451;&#30340;LLM&#65292;&#25193;&#23637;&#20102;&#22522;&#30784;&#27169;&#22411;Retro 48B&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#20248;&#22312;&#21508;&#31181;&#38646;&#26679;&#20363;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#25216;&#26415;&#23545;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#22256;&#24785;&#24230;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#26816;&#32034;&#22686;&#24378;LLM&#30340;&#35268;&#27169;&#20173;&#28982;&#26377;&#38480;&#65288;&#22914;Retro&#20855;&#26377;75&#20159;&#20010;&#21442;&#25968;&#65289;&#65292;&#36825;&#38480;&#21046;&#20102;&#25351;&#20196;&#35843;&#20248;&#21644;&#38646;&#26679;&#20363;&#27867;&#21270;&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Retro 48B&#65292;&#36825;&#26159;&#30446;&#21069;&#35268;&#27169;&#26368;&#22823;&#30340;&#20351;&#29992;&#26816;&#32034;&#39044;&#35757;&#32451;&#30340;LLM&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#26816;&#32034;&#25216;&#26415;&#20174;1.2&#19975;&#20159;&#20010;&#26631;&#35760;&#20013;&#32487;&#32493;&#39044;&#35757;&#32451;&#19968;&#20010;43B&#30340;GPT&#27169;&#22411;&#65292;&#24182;&#20511;&#21161;Retro&#26041;&#27861;&#23558;&#20854;&#25193;&#23637;&#21040;4800&#20159;&#20010;&#21442;&#25968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#24471;&#21040;&#30340;&#22522;&#30784;&#27169;&#22411;Retro 48B&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20165;&#20351;&#29992;1.2&#19975;&#20159;&#20010;&#26631;&#35760;&#36827;&#34892;&#35757;&#32451;&#30340;43B GPT&#27169;&#22411;&#65292;&#19988;&#21482;&#22686;&#21152;&#20102;2.58%&#30340;GPU&#20351;&#29992;&#26102;&#38388;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26174;&#33879;&#25193;&#23637;&#28508;&#21147;&#12290;&#22312;&#23545;Retro&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;&#21518;&#65292;InstructRetro&#22312;&#21508;&#31181;&#38646;&#26679;&#20363;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining auto-regressive large language models (LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval. Specifically, we continue to pretrain a 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. Notably, the obtained foundation model, Retro 48B, largely outperforms the counterpart GPT 43B trained on 1.2T tokens in terms of perplexity with only 2.58% additional GPU hours, demonstrating the significant scaling potential of the method. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on a wide range of zero-shot tasks. Spe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32858;&#31867;&#31639;&#27861;&#65292;&#20197;&#23558;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;K-Means&#30340;&#27010;&#24565;&#21457;&#29616;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#24182;&#20445;&#25345;&#20102;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2308.10263</link><description>&lt;p&gt;
&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#28508;&#22312;&#27010;&#24565;&#30340;&#25193;&#23637;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Scaling up Discovery of Latent Concepts in Deep NLP Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32858;&#31867;&#31639;&#27861;&#65292;&#20197;&#23558;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;K-Means&#30340;&#27010;&#24565;&#21457;&#29616;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#24182;&#20445;&#25345;&#20102;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#24341;&#36215;&#20102;&#19968;&#22330;&#38761;&#21629;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#26159;&#40657;&#21283;&#23376;&#65292;&#38656;&#35201;&#30740;&#31350;&#26469;&#29702;&#35299;&#23427;&#20204;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;Dalvi&#31561;&#20154;&#65288;2022&#24180;&#65289;&#26368;&#36817;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PLMs&#65289;&#20013;&#30340;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#32858;&#31867;&#20998;&#26512;&#65292;&#24320;&#23637;&#20102;&#34920;&#31034;&#20998;&#26512;&#65292;&#20294;&#30001;&#20110;&#36816;&#34892;&#32858;&#21512;&#23618;&#27425;&#32858;&#31867;&#30340;&#39640;&#25104;&#26412;&#65292;&#35813;&#26041;&#27861;&#22312;&#23567;&#35268;&#27169;&#19978;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#32858;&#31867;&#31639;&#27861;&#65292;&#20197;&#20415;&#23558;PLM&#34920;&#31034;&#20013;&#32534;&#30721;&#30340;&#27010;&#24565;&#21457;&#29616;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#21457;&#29616;&#30340;&#28508;&#22312;&#27010;&#24565;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25351;&#26631;&#26469;&#27604;&#36739;&#25152;&#30740;&#31350;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;K-Means&#30340;&#27010;&#24565;&#21457;&#29616;&#22312;&#20445;&#25345;&#25152;&#33719;&#24471;&#27010;&#24565;&#36136;&#37327;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#28508;&#22312;&#27010;&#24565;&#21457;&#29616;&#25193;&#23637;&#21040;LLMs&#21644;&#30701;&#35821;&#27010;&#24565;&#20013;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26032;&#21457;&#29616;&#30340;&#25928;&#29575;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the revolution caused by deep NLP models, they remain black boxes, necessitating research to understand their decision-making processes. A recent work by Dalvi et al. (2022) carried out representation analysis through the lens of clustering latent spaces within pre-trained models (PLMs), but that approach is limited to small scale due to the high cost of running Agglomerative hierarchical clustering. This paper studies clustering algorithms in order to scale the discovery of encoded concepts in PLM representations to larger datasets and models. We propose metrics for assessing the quality of discovered latent concepts and use them to compare the studied clustering algorithms. We found that K-Means-based concept discovery significantly enhances efficiency while maintaining the quality of the obtained concepts. Furthermore, we demonstrate the practicality of this newfound efficiency by scaling latent concept discovery to LLMs and phrasal concepts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#31995;&#32479;&#20013;&#30340;&#20027;&#35859;&#20851;&#31995;&#23558;&#20107;&#20214;&#35302;&#21457;&#22120;&#22312;&#19981;&#21516;&#39046;&#22495;&#38388;&#36827;&#34892;&#32806;&#21512;&#65292;&#20197;&#25552;&#21319;&#20107;&#20214;&#35302;&#21457;&#35782;&#21035;&#30340;&#39046;&#22495;&#36716;&#31227;&#24615;&#33021;&#12290;&#22312;&#20174;&#39640;&#36164;&#28304;&#21040;&#20302;&#36164;&#28304;&#39046;&#22495;&#30340;&#36716;&#31227;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#20943;&#36731;&#24615;&#33021;&#19979;&#38477;&#65292;&#29305;&#21035;&#26159;&#22312;&#20174;&#32500;&#22522;&#30334;&#31185;&#21040;&#26032;&#38395;&#39046;&#22495;&#30340;&#36716;&#31227;&#20013;&#25928;&#26524;&#26174;&#33879;&#12290;&#21516;&#26102;&#32467;&#21512;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#33021;&#36827;&#19968;&#27493;&#22686;&#24378;&#36716;&#31227;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2305.14163</link><description>&lt;p&gt;
&#21033;&#29992;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#26469;&#22686;&#24378;&#20107;&#20214;&#35302;&#21457;&#35782;&#21035;&#30340;&#39046;&#22495;&#36716;&#31227;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Open Information Extraction for More Robust Domain Transfer of Event Trigger Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#31995;&#32479;&#20013;&#30340;&#20027;&#35859;&#20851;&#31995;&#23558;&#20107;&#20214;&#35302;&#21457;&#22120;&#22312;&#19981;&#21516;&#39046;&#22495;&#38388;&#36827;&#34892;&#32806;&#21512;&#65292;&#20197;&#25552;&#21319;&#20107;&#20214;&#35302;&#21457;&#35782;&#21035;&#30340;&#39046;&#22495;&#36716;&#31227;&#24615;&#33021;&#12290;&#22312;&#20174;&#39640;&#36164;&#28304;&#21040;&#20302;&#36164;&#28304;&#39046;&#22495;&#30340;&#36716;&#31227;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#20943;&#36731;&#24615;&#33021;&#19979;&#38477;&#65292;&#29305;&#21035;&#26159;&#22312;&#20174;&#32500;&#22522;&#30334;&#31185;&#21040;&#26032;&#38395;&#39046;&#22495;&#30340;&#36716;&#31227;&#20013;&#25928;&#26524;&#26174;&#33879;&#12290;&#21516;&#26102;&#32467;&#21512;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#33021;&#36827;&#19968;&#27493;&#22686;&#24378;&#36716;&#31227;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#35302;&#21457;&#35782;&#21035;&#26159;&#35768;&#22810;&#39046;&#22495;&#20013;&#20851;&#38190;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#65292;&#22914;&#32500;&#22522;&#30334;&#31185;&#25110;&#26032;&#38395;&#12290;&#35813;&#20219;&#21153;&#36890;&#24120;&#20381;&#36182;&#20110;&#35302;&#21457;&#35782;&#21035;&#65288;TD&#65289;&#8212;&#35782;&#21035;&#25991;&#26412;&#20013;&#24341;&#36215;&#29305;&#23450;&#20107;&#20214;&#30340;&#26631;&#35760;&#33539;&#22260;&#12290;&#23613;&#31649;&#35302;&#21457;&#22120;&#30340;&#27010;&#24565;&#24212;&#29702;&#24819;&#22320;&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#65292;&#20294;&#20174;&#39640;&#36164;&#28304;&#39046;&#22495;&#21040;&#20302;&#36164;&#28304;&#39046;&#22495;&#30340;TD&#39046;&#22495;&#36716;&#31227;&#20250;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#65288;OIE&#65289;&#31995;&#32479;&#33719;&#21462;&#30340;&#20027;&#35859;&#20851;&#31995;&#23558;&#35302;&#21457;&#22120;&#22312;&#39046;&#22495;&#20043;&#38388;&#36827;&#34892;&#32806;&#21512;&#26469;&#35299;&#20915;TD&#20013;&#30340;&#36127;&#36716;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#27880;&#20837;&#30340;OIE&#20851;&#31995;&#21487;&#20197;&#20316;&#20026;&#19981;&#21516;&#39046;&#22495;&#35302;&#21457;&#22120;&#20043;&#38388;&#30340;&#20013;&#20171;&#65292;&#22686;&#24378;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;TD&#39046;&#22495;&#36716;&#31227;&#65292;&#24182;&#20943;&#23569;&#24615;&#33021;&#19979;&#38477;&#65292;&#29305;&#21035;&#26159;&#20174;&#39640;&#36164;&#28304;&#28304;&#39046;&#22495;&#65288;&#32500;&#22522;&#30334;&#31185;&#65289;&#36716;&#31227;&#21040;&#20302;&#36164;&#28304;&#30446;&#26631;&#39046;&#22495;&#65288;&#26032;&#38395;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25913;&#36827;&#30340;&#36716;&#31227;&#19982;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event detection is a crucial information extraction task in many domains, such as Wikipedia or news. The task typically relies on trigger detection (TD) -- identifying token spans in the text that evoke specific events. While the notion of triggers should ideally be universal across domains, domain transfer for TD from high- to low-resource domains results in significant performance drops. We address the problem of negative transfer in TD by coupling triggers between domains using subject-object relations obtained from a rule-based open information extraction (OIE) system. We demonstrate that OIE relations injected through multi-task training can act as mediators between triggers in different domains, enhancing zero- and few-shot TD domain transfer and reducing performance drops, in particular when transferring from a high-resource source domain (Wikipedia) to a low(er)-resource target domain (news). Additionally, we combine this improved transfer with masked language modeling on the t
&lt;/p&gt;</description></item><item><title>Atinuke&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#30340;&#23618;&#19982;&#27880;&#24847;&#26426;&#21046;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#65292;&#20174;&#32780;&#20248;&#21270;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16736</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Engineering A Large Language Model From Scratch. (arXiv:2401.16736v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16736
&lt;/p&gt;
&lt;p&gt;
Atinuke&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#30340;&#23618;&#19982;&#27880;&#24847;&#26426;&#21046;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#65292;&#20174;&#32780;&#20248;&#21270;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#30340;&#21019;&#26032;&#25216;&#26415;&#30340;&#24320;&#21457;&#21644;&#21457;&#24067;&#12290;Atinuke&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#29420;&#29305;&#30340;&#37197;&#32622;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#20248;&#21270;&#24615;&#33021;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#30340;&#23618;&#19982;&#27880;&#24847;&#26426;&#21046;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#20174;&#32780;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#24314;&#31435;&#26377;&#24847;&#20041;&#30340;&#20851;&#32852;&#12290;&#30001;&#20110;&#20854;&#25299;&#25169;&#32467;&#26500;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#37197;&#32622;&#65292;&#23427;&#21487;&#20197;&#25552;&#21462;&#29305;&#24449;&#24182;&#23398;&#20064;&#22797;&#26434;&#30340;&#26144;&#23556;&#65292;&#20174;&#32780;&#27169;&#20223;&#20154;&#31867;&#35821;&#35328;&#12290;Atinuke&#26159;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#30340;&#65292;&#24182;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#26080;&#32541;&#38598;&#25104;&#12290;softmax&#12289;&#23884;&#20837;&#21644;&#22810;&#22836;&#27880;&#24847;&#21147;&#31561;&#39640;&#32423;&#30697;&#38453;&#25805;&#20316;&#20351;&#24471;&#23545;&#25991;&#26412;&#12289;&#22768;&#38899;&#21644;&#35270;&#35273;&#20449;&#21495;&#30340;&#32454;&#33268;&#22788;&#29702;&#25104;&#20026;&#21487;&#33021;&#12290;&#36890;&#36807;&#23558;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#19982;&#36719;&#20214;&#35774;&#35745;&#21407;&#21017;&#21644;&#25968;&#23398;&#26041;&#27861;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
The proliferation of deep learning in natural language processing (NLP) has led to the development and release of innovative technologies capable of understanding and generating human language with remarkable proficiency. Atinuke, a Transformer-based neural network, optimises performance across various language tasks by utilising a unique configuration. The architecture interweaves layers for processing sequential data with attention mechanisms to draw meaningful affinities between inputs and outputs. Due to the configuration of its topology and hyperparameter tuning, it can emulate human-like language by extracting features and learning complex mappings. Atinuke is modular, extensible, and integrates seamlessly with existing machine learning pipelines. Advanced matrix operations like softmax, embeddings, and multi-head attention enable nuanced handling of textual, acoustic, and visual signals. By unifying modern deep learning techniques with software design principles and mathematical
&lt;/p&gt;</description></item><item><title>UNSEE&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#38750;&#23545;&#27604;&#24230;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#32593;&#32476;&#35299;&#20915;&#20102;&#34920;&#31034;&#22349;&#22604;&#38382;&#39064;&#65292;&#36798;&#21040;&#20102;&#19982;&#23545;&#27604;&#30446;&#26631;&#30456;&#24403;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.15316</link><description>&lt;p&gt;
UNSEE: &#26080;&#30417;&#30563;&#30340;&#38750;&#23545;&#27604;&#24230;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
UNSEE: Unsupervised Non-contrastive Sentence Embeddings. (arXiv:2401.15316v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15316
&lt;/p&gt;
&lt;p&gt;
UNSEE&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#38750;&#23545;&#27604;&#24230;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#32593;&#32476;&#35299;&#20915;&#20102;&#34920;&#31034;&#22349;&#22604;&#38382;&#39064;&#65292;&#36798;&#21040;&#20102;&#19982;&#23545;&#27604;&#30446;&#26631;&#30456;&#24403;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UNSEE&#65288;Unsupervised Non-Contrastive Sentence Embeddings&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;SimCSE&#12290;&#25105;&#20204;&#39318;&#20808;&#35299;&#20915;&#20102;SimCSE&#20013;&#26367;&#25442;&#23545;&#27604;&#30446;&#26631;&#20026;&#38750;&#23545;&#27604;&#30446;&#26631;&#26102;&#20986;&#29616;&#30340;&#34920;&#31034;&#22349;&#22604;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30446;&#26631;&#32593;&#32476;&#30340;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#34920;&#31034;&#22349;&#22604;&#12290;&#30446;&#26631;&#32593;&#32476;&#30340;&#24341;&#20837;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#38750;&#23545;&#27604;&#30446;&#26631;&#65292;&#22312;&#20445;&#25345;&#35757;&#32451;&#31283;&#23450;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#19982;&#23545;&#27604;&#30446;&#26631;&#30456;&#24403;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36890;&#36807;&#31934;&#24515;&#35843;&#25972;&#21644;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38750;&#23545;&#27604;&#24230;&#21477;&#23376;&#23884;&#20837;&#19978;&#36798;&#21040;&#20102;&#24005;&#23792;&#24615;&#33021;&#12290;&#36825;&#19968;&#20840;&#38754;&#30340;&#21162;&#21147;&#20135;&#29983;&#20102;&#20986;&#33394;&#30340;&#21477;&#23376;&#34920;&#31034;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present UNSEE: Unsupervised Non-Contrastive Sentence Embeddings, a novel approach that outperforms SimCSE in the Massive Text Embedding benchmark. Our exploration begins by addressing the challenge of representation collapse, a phenomenon observed when contrastive objectives in SimCSE are replaced with non-contrastive objectives. To counter this issue, we propose a straightforward solution known as the target network, effectively mitigating representation collapse. The introduction of the target network allows us to leverage non-contrastive objectives, maintaining training stability while achieving performance improvements comparable to contrastive objectives. Our method has achieved peak performance in non-contrastive sentence embeddings through meticulous fine-tuning and optimization. This comprehensive effort has yielded superior sentence representation models, showcasing the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TraCo&#30340;&#26032;&#23618;&#27425;&#20027;&#39064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20256;&#36755;&#35745;&#21010;&#20381;&#36182;&#26041;&#27861;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#35299;&#32544;&#30721;&#22120;&#65292;&#25913;&#21892;&#20102;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#30340;&#20146;&#21644;&#24615;&#12289;&#21512;&#29702;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14113</link><description>&lt;p&gt;
&#20851;&#20110;&#23618;&#27425;&#20027;&#39064;&#24314;&#27169;&#30340;&#20146;&#21644;&#24615;&#12289;&#21512;&#29702;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Affinity, Rationality, and Diversity of Hierarchical Topic Modeling. (arXiv:2401.14113v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TraCo&#30340;&#26032;&#23618;&#27425;&#20027;&#39064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20256;&#36755;&#35745;&#21010;&#20381;&#36182;&#26041;&#27861;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#35299;&#32544;&#30721;&#22120;&#65292;&#25913;&#21892;&#20102;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#30340;&#20146;&#21644;&#24615;&#12289;&#21512;&#29702;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#20027;&#39064;&#24314;&#27169;&#26088;&#22312;&#20174;&#35821;&#26009;&#24211;&#20013;&#21457;&#29616;&#28508;&#22312;&#20027;&#39064;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#32455;&#25104;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#65292;&#20197;&#20415;&#29702;&#35299;&#20855;&#26377;&#26399;&#26395;&#35821;&#20041;&#31890;&#24230;&#30340;&#25991;&#26723;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#22312;&#20135;&#29983;&#20302;&#20146;&#21644;&#24615;&#12289;&#21512;&#29702;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#20027;&#39064;&#23618;&#27425;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#38459;&#30861;&#20102;&#25991;&#26723;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20256;&#36755;&#35745;&#21010;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#23618;&#27425;&#20027;&#39064;&#27169;&#22411;&#65288;TraCo&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36755;&#35745;&#21010;&#20381;&#36182;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#20043;&#21069;&#31616;&#21333;&#30340;&#20027;&#39064;&#20381;&#36182;&#26041;&#27861;&#12290;&#23427;&#38480;&#21046;&#20381;&#36182;&#20851;&#31995;&#20197;&#30830;&#20445;&#23427;&#20204;&#30340;&#31232;&#30095;&#24615;&#21644;&#24179;&#34913;&#24615;&#65292;&#24182;&#36890;&#36807;&#23427;&#20204;&#23545;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#36825;&#25913;&#21892;&#20102;&#23618;&#27425;&#32467;&#26500;&#30340;&#20146;&#21644;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#35299;&#32544;&#30721;&#22120;&#12290;&#23427;&#36890;&#36807;&#35299;&#32544;&#32534;&#30721;&#23558;&#19981;&#21516;&#30340;&#35821;&#20041;&#31890;&#24230;&#20998;&#37197;&#32473;&#19981;&#21516;&#23618;&#27425;&#30340;&#20027;&#39064;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#23618;&#27425;&#32467;&#26500;&#30340;&#21512;&#29702;&#24615;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical topic modeling aims to discover latent topics from a corpus and organize them into a hierarchy to understand documents with desirable semantic granularity. However, existing work struggles with producing topic hierarchies of low affinity, rationality, and diversity, which hampers document understanding. To overcome these challenges, we in this paper propose Transport Plan and Context-aware Hierarchical Topic Model (TraCo). Instead of early simple topic dependencies, we propose a transport plan dependency method. It constrains dependencies to ensure their sparsity and balance, and also regularizes topic hierarchy building with them. This improves affinity and diversity of hierarchies. We further propose a context-aware disentangled decoder. Rather than previously entangled decoding, it distributes different semantic granularity to topics at different levels by disentangled decoding. This facilitates the rationality of hierarchies. Experiments on benchmark datasets demonstra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#21644;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#21644;&#20351;&#29992;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EoTD&#21644;ETD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11864</link><description>&lt;p&gt;
&#36890;&#36807;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation. (arXiv:2401.11864v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#21644;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#21644;&#20351;&#29992;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EoTD&#21644;ETD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#23558;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#21387;&#32553;&#21040;&#20855;&#26377;&#23567;&#20110;&#21313;&#20159;&#21442;&#25968;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#65292;&#23558;&#25512;&#29702;&#36807;&#31243;&#23553;&#35013;&#20026;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;EoTD&#25968;&#25454;&#38598;&#26469;&#23545;SLMs&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#20197;&#25552;&#21319;SLMs&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#36825;&#21253;&#25324;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#65288;&#21253;&#25324;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#31243;&#24207;&#21644;&#24605;&#32500;&#26041;&#31243;&#65289;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;EoTD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;ETD&#20351;&#36825;&#20123;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Ensemble Thoughts Distillation (ETD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for fine-tuning. Our experimental findings demonstrate that EoTD significantly boosts the reasoning abilities of SLMs, while ETD enables these models to achieve state-of-the-art reasoning performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#38646;&#26679;&#26412;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#24615;&#32508;&#36848;&#33258;&#21160;&#31579;&#36873;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#25351;&#23548;&#24494;&#35843;&#21644;&#26657;&#20934;&#25216;&#26415;&#22312;&#31579;&#36873;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#19988;&#19982;&#38646;&#26679;&#26412;&#27169;&#22411;&#30340;&#38598;&#25104;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#33410;&#30465;&#31579;&#36873;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.06320</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#31995;&#32479;&#24615;&#32508;&#36848;&#31579;&#36873;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Generative Large Language Models for Systematic Review Screening Automation. (arXiv:2401.06320v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#38646;&#26679;&#26412;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#24615;&#32508;&#36848;&#33258;&#21160;&#31579;&#36873;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#25351;&#23548;&#24494;&#35843;&#21644;&#26657;&#20934;&#25216;&#26415;&#22312;&#31579;&#36873;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#19988;&#19982;&#38646;&#26679;&#26412;&#27169;&#22411;&#30340;&#38598;&#25104;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#33410;&#30465;&#31579;&#36873;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#24615;&#32508;&#36848;&#23545;&#20110;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#23398;&#38750;&#24120;&#37325;&#35201;&#65292;&#23427;&#20204;&#32508;&#21512;&#20998;&#26512;&#20102;&#29305;&#23450;&#38382;&#39064;&#30340;&#24050;&#21457;&#34920;&#30740;&#31350;&#32467;&#26524;&#12290;&#36827;&#34892;&#27492;&#31867;&#32508;&#36848;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#36164;&#28304;&#21644;&#26102;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#31579;&#36873;&#38454;&#27573;&#65292;&#38656;&#35201;&#35780;&#20272;&#20986;&#29256;&#29289;&#25688;&#35201;&#26159;&#21542;&#24212;&#21253;&#25324;&#22312;&#32508;&#36848;&#20013;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#38646;&#26679;&#26412;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#33258;&#21160;&#31579;&#36873;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20843;&#31181;&#19981;&#21516;&#30340;LLM&#30340;&#25928;&#26524;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#21484;&#22238;&#38408;&#20540;&#30340;&#26657;&#20934;&#25216;&#26415;&#65292;&#29992;&#20110;&#30830;&#23450;&#26159;&#21542;&#24212;&#23558;&#20986;&#29256;&#29289;&#21253;&#25324;&#22312;&#31995;&#32479;&#24615;&#32508;&#36848;&#20013;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35780;&#20272;&#20351;&#29992;&#20102;&#20116;&#20010;&#26631;&#20934;&#27979;&#35797;&#38598;&#65292;&#32467;&#26524;&#26174;&#31034;&#25351;&#23548;&#24494;&#35843;&#22312;&#31579;&#36873;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#26657;&#20934;&#20351;LLMs&#22312;&#23454;&#29616;&#30446;&#26631;&#21484;&#22238;&#26041;&#38754;&#26356;&#23454;&#29992;&#65292;&#24182;&#19988;&#23558;&#36825;&#20004;&#32773;&#19982;&#38646;&#26679;&#26412;&#27169;&#22411;&#30340;&#38598;&#25104;&#30456;&#32467;&#21512;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#33410;&#30465;&#20102;&#22823;&#37327;&#31579;&#36873;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models~(LLMs) for automatic screening. We evaluate the effectiveness of eight different LLMs and investigate a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Our comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders LLMs practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>LinguAlchemy&#26159;&#19968;&#31181;&#23558;&#35821;&#35328;&#31867;&#22411;&#23398;&#21644;&#22320;&#29702;&#20803;&#32032;&#34701;&#21512;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#26410;&#35265;&#35821;&#35328;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06034</link><description>&lt;p&gt;
LinguAlchemy: &#23558;&#35821;&#35328;&#31867;&#22411;&#23398;&#21644;&#22320;&#29702;&#20803;&#32032;&#34701;&#21512;&#20197;&#23454;&#29616;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization. (arXiv:2401.06034v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06034
&lt;/p&gt;
&lt;p&gt;
LinguAlchemy&#26159;&#19968;&#31181;&#23558;&#35821;&#35328;&#31867;&#22411;&#23398;&#21644;&#22320;&#29702;&#20803;&#32032;&#34701;&#21512;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#26410;&#35265;&#35821;&#35328;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#35821;&#35328;&#19978;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#65292;PLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#23548;&#33268;&#35821;&#35328;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#65292;&#29978;&#33267;&#29983;&#25104;&#30340;&#22238;&#24212;&#19982;&#38543;&#26426;&#22522;&#20934;&#30456;&#24403;&#33618;&#21776;&#12290;&#36825;&#19968;&#38480;&#21046;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;PLMs&#30340;&#19968;&#20010;&#38271;&#26399;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#30340;&#22810;&#26679;&#24615;&#21644;&#24179;&#31561;&#33719;&#21462;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;LinguAlchemy&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#36825;&#26159;&#19968;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23558;&#35821;&#35328;&#30340;&#21508;&#20010;&#26041;&#38754;&#65288;&#21253;&#25324;&#31867;&#22411;&#23398;&#12289;&#22320;&#29702;&#21644;&#35821;&#31995;&#65289;&#32435;&#20837;PLMs&#30340;&#34920;&#31034;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#24449;&#30456;&#24212;&#30340;&#35821;&#35328;&#32422;&#26463;&#12290;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;LinguAlchemy&#26174;&#33879;&#25552;&#39640;&#20102;mBERT&#21644;XLM-R&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#20934;&#30830;&#24615;&#32489;&#25928;&#65292;&#20998;&#21035;&#25552;&#39640;&#20102;&#32422;18%&#21644;&#32422;2%&#65292;&#23637;&#29616;&#20986;&#36739;&#39640;&#30340;&#26410;&#35265;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (PLMs) have shown remarkable generalization toward multiple tasks and languages. Nonetheless, the generalization of PLMs towards unseen languages is poor, resulting in significantly worse language performance, or even generating nonsensical responses that are comparable to a random baseline. This limitation has been a longstanding problem of PLMs raising the problem of diversity and equal access to language modeling technology. In this work, we solve this limitation by introducing LinguAlchemy, a regularization technique that incorporates various aspects of languages covering typological, geographical, and phylogenetic constraining the resulting representation of PLMs to better characterize the corresponding linguistics constraints. LinguAlchemy significantly improves the accuracy performance of mBERT and XLM-R on unseen languages by ~18% and ~2%, respectively compared to fully finetuned models and displaying a high degree of unseen language generalization. W
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;Stack Overflow&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01472</link><description>&lt;p&gt;
Stack Overflow&#22238;&#31572;&#20013;&#20449;&#24687;&#39640;&#20142;&#30340;&#21021;&#25506;
&lt;/p&gt;
&lt;p&gt;
A First Look at Information Highlighting in Stack Overflow Answers. (arXiv:2401.01472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;Stack Overflow&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#27983;&#35272;Stack Overflow&#65288;SO&#65289;&#30340;&#30693;&#35782;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20351;&#24086;&#23376;&#23545;&#29992;&#25143;&#26356;&#29983;&#21160;&#65292;SO&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;Markdown&#25110;HTML&#32534;&#20889;&#21644;&#32534;&#36753;&#24086;&#23376;&#65292;&#20197;&#20415;&#29992;&#25143;&#21487;&#20197;&#21033;&#29992;&#21508;&#31181;&#26684;&#24335;&#21270;&#26679;&#24335;&#65288;&#20363;&#22914;&#31895;&#20307;&#12289;&#26012;&#20307;&#21644;&#20195;&#30721;&#65289;&#26469;&#31361;&#20986;&#37325;&#35201;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#31361;&#20986;&#20449;&#24687;&#30340;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#30446;&#26631;&#65306;&#25105;&#20204;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;SO&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#20026;&#20102;&#25193;&#23637;&#25105;&#20204;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#24102;&#26377;&#26684;&#24335;&#21270;&#26679;&#24335;&#30340;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#26412;&#25991;&#30740;&#31350;&#20102;Stack Overflow&#30340;31,169,429&#20010;&#22238;&#31572;&#12290;&#20026;&#20102;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;CNN&#21644;BERT&#27169;&#22411;&#65292;&#38024;&#23545;&#27599;&#31181;&#26684;&#24335;&#21270;&#31867;&#22411;&#65288;&#21363;&#31895;&#20307;&#12289;&#26012;&#20307;&#12289;&#20195;&#30721;&#21644;&#26631;&#39064;&#65289;&#20351;&#29992;&#25105;&#20204;&#20174;SO&#22238;&#31572;&#25910;&#38598;&#30340;&#31361;&#20986;&#20449;&#24687;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Navigating the knowledge of Stack Overflow (SO) remains challenging. To make the posts vivid to users, SO allows users to write and edit posts with Markdown or HTML so that users can leverage various formatting styles (e.g., bold, italic, and code) to highlight the important information. Nonetheless, there have been limited studies on the highlighted information. Objective: We carried out the first large-scale exploratory study on the information highlighted in SO answers in our recent study. To extend our previous study, we develop approaches to automatically recommend highlighted content with formatting styles using neural network architectures initially designed for the Named Entity Recognition task. Method: In this paper, we studied 31,169,429 answers of Stack Overflow. For training recommendation models, we choose CNN and BERT models for each type of formatting (i.e., Bold, Italic, Code, and Heading) using the information highlighting dataset we collected from SO answers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20803;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#37325;&#22609;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24378;&#35843;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#20803;&#25552;&#31034;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#23569;&#26679;&#26412;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.11482</link><description>&lt;p&gt;
AGI&#31995;&#32479;&#30340;&#20803;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Meta Prompting for AGI Systems. (arXiv:2311.11482v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20803;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#37325;&#22609;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24378;&#35843;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#20803;&#25552;&#31034;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#23569;&#26679;&#26412;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20803;&#25552;&#31034;(meta prompting)&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#25216;&#26415;&#65292;&#37325;&#26032;&#22609;&#36896;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12289;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#21033;&#29992;&#12290;&#22522;&#20110;&#31867;&#22411;&#29702;&#35770;&#21644;&#33539;&#30068;&#35770;&#65292;&#20803;&#25552;&#31034;&#27880;&#37325;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#20197;&#20869;&#23481;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20803;&#25552;&#31034;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#23558;&#20854;&#19982;&#23569;&#26679;&#26412;&#25552;&#31034;(few-shot prompting)&#21306;&#20998;&#24320;&#26469;&#65292;&#24182;&#24378;&#35843;&#20854;&#22312;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#37325;&#28857;&#20851;&#27880;&#23558;&#20803;&#25552;&#31034;&#25193;&#23637;&#21040;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;&#23637;&#31034;&#22914;&#20309;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#20998;&#25104;&#36739;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20196;&#29260;&#25928;&#29575;&#65292;&#24182;&#20351;&#38382;&#39064;&#27714;&#35299;&#30340;&#27604;&#36739;&#26356;&#21152;&#20844;&#24179;&#65292;&#23588;&#20854;&#26159;&#19982;&#23569;&#26679;&#26412;&#31034;&#20363;&#26041;&#27861;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#25552;&#31034;&#20219;&#21153;&#65292;&#20801;&#35768;LLMs&#20197;&#36845;&#20195;&#30340;&#20803;&#32534;&#31243;&#24418;&#24335;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive study of Meta Prompting, an innovative technique reshaping the utilization of large language models (LLMs), multi-modal foundation models, and AI systems in problem-solving and data interpretation. Grounded in type theory and category theory, Meta Prompting emphasizes the structure and syntax of information over traditional content-centric methods. The paper explores the formal definitions of Meta Prompting (MP), sets it apart from Few-Shot Prompting, and underlines its effectiveness in various AI applications. A key focus is on extending Meta Prompting to complex reasoning tasks, showing how it effectively deconstructs intricate problems into simpler sub-problems, enhancing token efficiency and enabling more equitable problem-solving comparisons, especially against few-shot example methods. Additionally, the paper introduces Meta Prompting for Prompting Tasks, allowing LLMs to self-generate new prompts in an iterative, metaprogramming-like manner. T
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10688</link><description>&lt;p&gt;
&#19968;&#31181;&#20165;&#35299;&#30721;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24320;&#31665;&#21363;&#29992;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#25509;&#36817;&#27599;&#20010;&#20010;&#21035;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#22312;&#22823;&#22411;&#26102;&#38388;&#24207;&#21015;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#39044;&#27979;&#21382;&#21490;&#38271;&#24230;&#12289;&#39044;&#27979;&#38271;&#24230;&#21644;&#26102;&#38388;&#31890;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
&lt;/p&gt;</description></item><item><title>SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00533</link><description>&lt;p&gt;
SELF&#65306;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
SELF: Language-Driven Self-Evolution for Large Language Model. (arXiv:2310.00533v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00533
&lt;/p&gt;
&lt;p&gt;
SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#21331;&#36234;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23398;&#20064;&#21644;&#25512;&#21160;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#8212;&#8212;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;&#30340;&#36335;&#24452;&#20173;&#28982;&#26410;&#30693;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;"SELF"&#65288;&#24102;&#26377;&#35821;&#35328;&#21453;&#39304;&#30340;&#33258;&#20027;&#36827;&#21270;&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLM&#33021;&#22815;&#19981;&#26029;&#22320;&#33258;&#25105;&#36827;&#21270;&#12290;&#27492;&#22806;&#65292;SELF&#21033;&#29992;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#12289;&#20840;&#38754;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#31934;&#30830;&#23450;&#20301;&#21709;&#24212;&#25913;&#36827;&#30340;&#39046;&#22495;&#65292;&#24182;&#25552;&#39640;&#33258;&#20027;&#36827;&#21270;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;SELF&#39318;&#20808;&#36827;&#34892;&#20803;&#25216;&#33021;&#23398;&#20064;&#65292;&#19987;&#27880;&#20110;&#33258;&#25105;&#21453;&#39304;&#21644;&#33258;&#25105;&#31934;&#28860;&#12290;&#36825;&#20123;&#20803;&#25216;&#33021;&#26159;&#20851;&#38190;&#65292;&#24341;&#23548;&#27169;&#22411;&#22312;&#33258;&#21046;&#25968;&#25454;&#30340;&#25345;&#32493;&#35757;&#32451;&#21608;&#26399;&#20013;&#36827;&#34892;&#21518;&#32493;&#30340;&#33258;&#25105;&#36827;&#21270;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#20869;&#22312;&#33021;&#21147;&#12290;&#22312;&#32473;&#23450;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;SELF&#20351;&#27169;&#22411;&#20855;&#22791;&#20102;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased remarkable versatility across diverse domains. However, the pathway toward autonomous model development, a cornerstone for achieving human-level learning and advancing autonomous AI, remains largely uncharted. We introduce an innovative approach, termed "SELF" (Self-Evolution with Language Feedback). This methodology empowers LLMs to undergo continual self-evolution. Furthermore, SELF employs language-based feedback as a versatile and comprehensive evaluative tool, pinpointing areas for response refinement and bolstering the stability of self-evolutionary training. Initiating with meta-skill learning, SELF acquires foundational meta-skills with a focus on self-feedback and self-refinement. These meta-skills are critical, guiding the model's subsequent self-evolution through a cycle of perpetual training with self-curated data, thereby enhancing its intrinsic abilities. Given unlabeled instructions, SELF equips the model with the capability to
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20998;&#26512;&#21644;&#39564;&#35777;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25163;&#21160;&#25110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26631;&#27880;&#26041;&#27861;&#22312;&#22823;&#22411;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.13063</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#12289;&#39564;&#35777;&#21644;&#24212;&#29992;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies. (arXiv:2309.13063v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13063
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20998;&#26512;&#21644;&#39564;&#35777;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25163;&#21160;&#25110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26631;&#27880;&#26041;&#27861;&#22312;&#22823;&#22411;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#24535;&#25968;&#25454;&#21487;&#20197;&#25581;&#31034;&#29992;&#25143;&#19982;&#32593;&#32476;&#25628;&#32034;&#26381;&#21153;&#30340;&#20132;&#20114;&#26041;&#24335;&#12289;&#29992;&#25143;&#30340;&#38656;&#27714;&#20197;&#21450;&#28385;&#24847;&#31243;&#24230;&#31561;&#23453;&#36149;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#24182;&#19981;&#23481;&#26131;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#26032;&#30340;&#32593;&#32476;&#25628;&#32034;&#24418;&#24335;&#65292;&#22914;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#32842;&#22825;&#12290;&#20026;&#20102;&#29702;&#35299;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#29992;&#26377;&#24847;&#20041;&#30340;&#20998;&#31867;&#26041;&#24335;&#26631;&#35760;&#23427;&#20204;&#30340;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#20854;&#22810;&#26679;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#25110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26631;&#27880;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#19988;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#32780;&#35328;&#65292;&#35201;&#20040;&#20195;&#20215;&#39640;&#26114;&#35201;&#20040;&#19981;&#22815;&#28789;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#20016;&#23500;&#19988;&#30456;&#20851;&#30340;&#27010;&#24565;&#12289;&#25551;&#36848;&#21644;&#31034;&#20363;&#26469;&#34920;&#31034;&#29992;&#25143;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;LLM&#29983;&#25104;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26085;&#24535;&#20998;&#26512;&#21487;&#33021;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#36825;&#26679;&#30340;&#20998;&#31867;&#24471;&#19981;&#21040;&#22806;&#37096;&#39564;&#35777;&#65292;&#24182;&#19988;&#21487;&#33021;&#23384;&#22312;&#19981;&#33391;&#30340;&#21453;&#39304;&#22238;&#36335;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#24037;&#19987;&#23478;&#21644;&#35780;&#20272;&#32773;&#26469;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Log data can reveal valuable information about how users interact with web search services, what they want, and how satisfied they are. However, analyzing user intents in log data is not easy, especially for new forms of web search such as AI-driven chat. To understand user intents from log data, we need a way to label them with meaningful categories that capture their diversity and dynamics. Existing methods rely on manual or ML-based labeling, which are either expensive or inflexible for large and changing datasets. We propose a novel solution using large language models (LLMs), which can generate rich and relevant concepts, descriptions, and examples for user intents. However, using LLMs to generate a user intent taxonomy and apply it to do log analysis can be problematic for two main reasons: such a taxonomy is not externally validated, and there may be an undesirable feedback loop. To overcome these issues, we propose a new methodology with human experts and assessors to verify th
&lt;/p&gt;</description></item><item><title>Code Llama&#26159;&#19968;&#31995;&#21015;&#29992;&#20110;&#20195;&#30721;&#30340;&#24320;&#25918;&#22522;&#30784;&#27169;&#22411;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#22635;&#20805;&#21151;&#33021;&#65292;&#25903;&#25345;&#22823;&#22411;&#36755;&#20837;&#19978;&#19979;&#25991;&#21644;&#38646;-shot&#25351;&#20196;&#36319;&#36394;&#33021;&#21147;&#12290;&#22312;&#22810;&#20010;&#20195;&#30721;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;Code Llama&#36798;&#21040;&#24320;&#25918;&#27169;&#22411;&#20013;&#26368;&#39640;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;Python&#19987;&#38376;&#21270;&#27169;&#22411;&#22312;&#26576;&#20123;&#27979;&#35797;&#19978;&#36229;&#36234;&#20102;Llama 2&#30340;70B&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.12950</link><description>&lt;p&gt;
Code Llama: &#29992;&#20110;&#20195;&#30721;&#30340;&#24320;&#25918;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Code Llama: Open Foundation Models for Code. (arXiv:2308.12950v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12950
&lt;/p&gt;
&lt;p&gt;
Code Llama&#26159;&#19968;&#31995;&#21015;&#29992;&#20110;&#20195;&#30721;&#30340;&#24320;&#25918;&#22522;&#30784;&#27169;&#22411;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#22635;&#20805;&#21151;&#33021;&#65292;&#25903;&#25345;&#22823;&#22411;&#36755;&#20837;&#19978;&#19979;&#25991;&#21644;&#38646;-shot&#25351;&#20196;&#36319;&#36394;&#33021;&#21147;&#12290;&#22312;&#22810;&#20010;&#20195;&#30721;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;Code Llama&#36798;&#21040;&#24320;&#25918;&#27169;&#22411;&#20013;&#26368;&#39640;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;Python&#19987;&#38376;&#21270;&#27169;&#22411;&#22312;&#26576;&#20123;&#27979;&#35797;&#19978;&#36229;&#36234;&#20102;Llama 2&#30340;70B&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#24067;&#20102;Code Llama&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#22522;&#20110;Llama 2&#30340;&#29992;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24320;&#25918;&#27169;&#22411;&#20013;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22635;&#20805;&#21151;&#33021;&#65292;&#25903;&#25345;&#22823;&#22411;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#33021;&#22815;&#36827;&#34892;&#38646;-shot&#25351;&#20196;&#36319;&#36394;&#32534;&#31243;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20379;&#22810;&#31181;&#29256;&#26412;&#20197;&#35206;&#30422;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#65306;&#22522;&#30784;&#27169;&#22411;&#65288;Code Llama&#65289;&#65292;Python&#19987;&#38376;&#21270;&#27169;&#22411;&#65288;Code Llama-Python&#65289;&#65292;&#20197;&#21450;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;&#65288;Code Llama-Instruct&#65289;&#65292;&#27599;&#20010;&#27169;&#22411;&#21442;&#25968;&#20998;&#21035;&#20026;7B&#12289;13B&#21644;34B&#12290;&#25152;&#26377;&#27169;&#22411;&#37117;&#26159;&#22312;16k&#26631;&#35760;&#24207;&#21015;&#19978;&#35757;&#32451;&#30340;&#65292;&#21487;&#20197;&#25913;&#21892;&#38271;&#24230;&#19981;&#36229;&#36807;100k&#26631;&#35760;&#30340;&#36755;&#20837;&#12290;7B&#21644;13B&#30340;Code Llama&#21644;Code Llama-Instruct&#21464;&#31181;&#20250;&#26681;&#25454;&#21608;&#22260;&#20869;&#23481;&#36827;&#34892;&#22635;&#20805;&#12290;Code Llama&#22312;&#20960;&#20010;&#20195;&#30721;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#24320;&#25918;&#27169;&#22411;&#20013;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;HumanEval&#21644;MBPP&#20998;&#21035;&#36798;&#21040;&#20102;53%&#21644;55%&#30340;&#20998;&#25968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Code Llama-Python 7B&#22312;HumanEval&#21644;MBPP&#19978;&#20248;&#20110;Llama 2 70B&#65292;&#32780;&#25105;&#20204;&#30340;&#25152;&#26377;&#27169;&#22411;&#37117;&#20248;&#20110;&#20854;&#20182;&#20219;&#20309;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every othe
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21033;&#29992;&#23545;&#35805;&#20013;&#30340;&#38544;&#24335;&#21453;&#39304;&#26469;&#25913;&#36827;&#31038;&#20132;&#23545;&#35805;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36890;&#36807;&#25910;&#38598;&#29992;&#25143;&#21709;&#24212;&#38271;&#24230;&#12289;&#24773;&#24863;&#21644;&#21453;&#24212;&#31561;&#20449;&#21495;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#29983;&#25104;&#35805;&#35821;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.14117</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#20013;&#21033;&#29992;&#26469;&#33258;&#37096;&#32626;&#25968;&#25454;&#30340;&#38544;&#24335;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Leveraging Implicit Feedback from Deployment Data in Dialogue. (arXiv:2307.14117v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14117
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21033;&#29992;&#23545;&#35805;&#20013;&#30340;&#38544;&#24335;&#21453;&#39304;&#26469;&#25913;&#36827;&#31038;&#20132;&#23545;&#35805;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36890;&#36807;&#25910;&#38598;&#29992;&#25143;&#21709;&#24212;&#38271;&#24230;&#12289;&#24773;&#24863;&#21644;&#21453;&#24212;&#31561;&#20449;&#21495;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#29983;&#25104;&#35805;&#35821;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#36890;&#36807;&#23398;&#20064;&#29992;&#25143;&#19982;&#37096;&#32626;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#28982;&#23545;&#35805;&#26469;&#25913;&#36827;&#31038;&#20132;&#23545;&#35805;&#31995;&#32479;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#27880;&#37322;&#12290;&#20026;&#20102;&#38544;&#24335;&#34913;&#37327;&#26426;&#22120;&#29983;&#25104;&#35805;&#35821;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#21033;&#29992;&#25910;&#38598;&#23545;&#35805;&#20013;&#26410;&#26469;&#20154;&#31867;&#35805;&#35821;&#30340;&#29992;&#25143;&#21709;&#24212;&#38271;&#24230;&#12289;&#24773;&#24863;&#21644;&#21453;&#24212;&#31561;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#20102;BlenderBot&#65288;Xu&#31561;&#65292;2023&#24180;&#65289;&#20844;&#24320;&#21457;&#24067;&#30340;&#37096;&#32626;&#25968;&#25454;&#12290;&#20154;&#31867;&#35780;&#20272;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#26032;&#27169;&#22411;&#27604;&#22522;&#32447;&#22238;&#22797;&#26377;&#25152;&#25913;&#36827;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#20195;&#29702;&#20449;&#21495;&#20063;&#21487;&#33021;&#23548;&#33268;&#20986;&#29616;&#19981;&#33391;&#23646;&#24615;&#30340;&#29983;&#25104;&#12290;&#20363;&#22914;&#65292;&#20248;&#21270;&#23545;&#35805;&#38271;&#24230;&#21487;&#33021;&#23548;&#33268;&#19982;&#22522;&#32447;&#30456;&#27604;&#26356;&#20855;&#20105;&#35758;&#24615;&#25110;&#19981;&#21451;&#22909;&#30340;&#29983;&#25104;&#65292;&#32780;&#20248;&#21270;&#31215;&#26497;&#24773;&#24863;&#25110;&#21453;&#24212;&#21017;&#21487;&#20197;&#20943;&#23569;&#36825;&#20123;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study improving social conversational agents by learning from natural dialogue between users and a deployed model, without extra annotations. To implicitly measure the quality of a machine-generated utterance, we leverage signals like user response length, sentiment and reaction of the future human utterances in the collected dialogue episodes. Our experiments use the publicly released deployment data from BlenderBot (Xu et al., 2023). Human evaluation indicates improvements in our new models over baseline responses; however, we find that some proxy signals can lead to more generations with undesirable properties as well. For example, optimizing for conversation length can lead to more controversial or unfriendly generations compared to the baseline, whereas optimizing for positive sentiment or reaction can decrease these behaviors.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#22312;&#31354;&#38388;&#35268;&#21010;&#21644;&#23548;&#33322;&#20132;&#21449;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#35299;&#26512;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25191;&#34892;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.11865</link><description>&lt;p&gt;
CARTIER: &#38754;&#21521;&#26426;&#22120;&#20154;&#25351;&#20196;&#25191;&#34892;&#30340;&#22320;&#22270;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots. (arXiv:2307.11865v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#22312;&#31354;&#38388;&#35268;&#21010;&#21644;&#23548;&#33322;&#20132;&#21449;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#35299;&#26512;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25191;&#34892;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#31354;&#38388;&#35268;&#21010;&#21644;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#19982;&#23548;&#33322;&#20132;&#21449;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#36981;&#24490;&#30456;&#23545;&#22797;&#26434;&#30340;&#25351;&#20196;&#65292;&#36825;&#20123;&#25351;&#20196;&#26356;&#31867;&#20284;&#20110;&#33258;&#28982;&#23545;&#35805;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#26174;&#24335;&#36807;&#31243;&#25351;&#20196;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#22312;&#37027;&#20123;&#23548;&#33322;&#25351;&#20196;&#34987;&#25552;&#20379;&#20026;&#21629;&#20196;&#24335;&#25351;&#20196;&#65288;&#20363;&#22914;&#65292;&#21435;&#20912;&#31665;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#35805;&#20132;&#20114;&#20013;&#30340;&#38544;&#24335;&#25351;&#20196;&#12290;&#25105;&#20204;&#21033;&#29992;3D&#27169;&#25311;&#22120;AI2Thor&#21019;&#24314;&#22797;&#26434;&#19988;&#21487;&#37325;&#22797;&#30340;&#22330;&#26223;&#65292;&#24182;&#36890;&#36807;&#20026;40&#31181;&#23545;&#35937;&#31867;&#22411;&#28155;&#21152;&#22797;&#26434;&#30340;&#35821;&#35328;&#26597;&#35810;&#26469;&#22686;&#24378;&#23427;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#23558;&#29992;&#25143;&#20132;&#20114;&#35299;&#37322;&#20026;&#22330;&#26223;&#20013;&#23545;&#35937;&#21015;&#34920;&#30340;&#19978;&#19979;&#25991;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#26512;&#25551;&#36848;&#24615;&#35821;&#35328;&#26597;&#35810;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work explores the capacity of large language models (LLMs) to address problems at the intersection of spatial planning and natural language interfaces for navigation.Our focus is on following relatively complex instructions that are more akin to natural conversation than traditional explicit procedural directives seen in robotics. Unlike most prior work, where navigation directives are provided as imperative commands (e.g., go to the fridge), we examine implicit directives within conversational interactions. We leverage the 3D simulator AI2Thor to create complex and repeatable scenarios at scale, and augment it by adding complex language queries for 40 object types. We demonstrate that a robot can better parse descriptive language queries than existing methods by using an LLM to interpret the user interaction in the context of a list of the objects in the scene.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;VisualGPTScore&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01879</link><description>&lt;p&gt;
VisualGPTScore: &#22810;&#27169;&#24577;&#29983;&#25104;&#39044;&#35757;&#32451;&#20998;&#25968;&#30340;&#35270;&#35273;&#35821;&#20041;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores. (arXiv:2306.01879v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01879
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VisualGPTScore&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; VisualGPTScore &#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#26469;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#19978;&#36816;&#31639;&#12290;&#19982;&#20256;&#32479;&#35266;&#28857;&#35748;&#20026;&#30340;VLM&#21482;&#26159;&#26080;&#24847;&#20041;&#30340;&#21333;&#35789;&#34955;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340; VisualGPTScore &#22312; ARO &#21644; Crepe &#31561;&#26368;&#36817;&#25552;&#20986;&#30340;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20102;&#39030;&#23574;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) discriminatively pre-trained with contrastive image-text matching losses such as $P(\text{match}|\text{text}, \text{image})$ have been criticized for lacking compositional understanding. This means they might output similar scores even if the original caption is rearranged into a different semantic statement. To address this, we propose to use the ${\bf V}$isual ${\bf G}$enerative ${\bf P}$re-${\bf T}$raining Score (${\bf VisualGPTScore}$) of $P(\text{text}|\text{image})$, a $\textit{multimodal generative}$ score that captures the likelihood of a text caption conditioned on an image using an image-conditioned language model. Contrary to the belief that VLMs are mere bag-of-words models, our off-the-shelf VisualGPTScore demonstrates top-tier performance on recently proposed image-text retrieval benchmarks like ARO and Crepe that assess compositional reasoning. Furthermore, we factorize VisualGPTScore into a product of the $\textit{marginal}$ P(text) and the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#35821;&#35328;&#27169;&#22411;&#37325;&#20889;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25913;&#21892;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13514</link><description>&lt;p&gt;
&#23567;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#37325;&#20889;&#20854;&#36755;&#20986;&#26469;&#25552;&#39640;&#24040;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Small Language Models Improve Giants by Rewriting Their Outputs. (arXiv:2305.13514v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#35821;&#35328;&#27169;&#22411;&#37325;&#20889;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25913;&#21892;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#36890;&#24120;&#19981;&#22914;&#24494;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#24040;&#22823;&#20307;&#31215;&#21644;&#36890;&#36807;API&#30340;&#21463;&#38480;&#35775;&#38382;&#20351;&#24471;&#38024;&#23545;&#20219;&#21153;&#30340;&#24494;&#35843;&#19981;&#20999;&#23454;&#38469;&#12290;&#32780;&#19988;&#65292;LLMs&#23545;&#25552;&#31034;&#30340;&#19981;&#21516;&#26041;&#38754;&#65288;&#20363;&#22914;&#65292;&#28436;&#31034;&#30340;&#36873;&#25321;&#21644;&#39034;&#24207;&#65289;&#24456;&#25935;&#24863;&#65292;&#22240;&#27492;&#21487;&#33021;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#20854;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#32416;&#27491;LLM&#30340;&#36755;&#20986;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23569;&#26679;&#26412;&#25552;&#31034;LLM&#29983;&#25104;&#19968;&#20010;&#20505;&#36873;&#27744;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26356;&#23567;&#30340;&#27169;&#22411;&#65292;LM-corrector&#65288;LMCor&#65289;&#26469;&#25913;&#36827;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;LMCor&#34987;&#35757;&#32451;&#29992;&#20110;&#23545;&#20505;&#36873;&#32773;&#36827;&#34892;&#25490;&#21517;&#12289;&#32452;&#21512;&#21644;&#37325;&#20889;&#65292;&#20197;&#20135;&#29983;&#26368;&#32456;&#30340;&#30446;&#26631;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#19968;&#20010;&#23567;&#30340;LMCor&#27169;&#22411;&#65288;250M&#65289;&#65292;&#20063;&#21487;&#20197;&#26174;&#30528;&#25913;&#21892;LLMs&#65288;62B&#65289;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;LMCor&#34920;&#29616;&#20986;&#23545;&#25552;&#31034;&#21464;&#21270;&#30340;&#25913;&#36827;&#40065;&#26834;&#24615;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#25913;&#21892;LLMs&#23454;&#38469;&#21487;&#29992;&#24615;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive few-shot learning capabilities, but they often underperform compared to fine-tuned models on challenging tasks. Furthermore, their large size and restricted access only through APIs make task-specific fine-tuning impractical. Moreover, LLMs are sensitive to different aspects of prompts (e.g., the selection and order of demonstrations) and can thus require time-consuming prompt engineering. In this light, we propose a method to correct LLM outputs without relying on their weights. First, we generate a pool of candidates by few-shot prompting an LLM. Second, we refine the LLM-generated outputs using a smaller model, the LM-corrector (LMCor), which is trained to rank, combine and rewrite the candidates to produce the final target output. Our experiments demonstrate that even a small LMCor model (250M) substantially improves the few-shot performance of LLMs (62B) across diverse tasks. Moreover, we illustrate that the LMCor exhibits 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.07303</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#20998;&#24067;&#20449;&#24687;&#30340;&#31070;&#32463;&#35789;&#21521;&#37327;&#19968;&#30452;&#20197;&#26469;&#37117;&#33021;&#20026;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#30340;&#21547;&#20041;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#38590;&#20197;&#35299;&#37322;&#21644;&#25511;&#21046;&#30340;&#34920;&#31034;&#12290;&#30456;&#21453;&#65292;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20855;&#26377;&#36882;&#24402;&#30340;&#65292;&#33258;&#35828;&#26126;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#21487;&#20197;&#25903;&#25345;&#33021;&#22815;&#20445;&#30041;&#21521;&#37327;&#31354;&#38388;&#20013;&#26174;&#24335;&#27010;&#24565;&#20851;&#31995;&#21644;&#32422;&#26463;&#30340;&#26032;&#22411;&#34920;&#31034;&#23398;&#20064;&#33539; paradigm&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#12289;&#22810;&#20851;&#31995;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#26144;&#23556;&#23450;&#20041;&#21644;&#23450;&#20041;&#26415;&#35821;&#21450;&#20854;&#30456;&#24212;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#20165;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#35789;&#21521;&#37327;&#12290;&#36890;&#36807;&#33258;&#21160;&#20174;&#23450;&#20041;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#32763;&#35793;&#30446;&#26631;&#35268;&#33539;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26694;&#26550;&#19987;&#38376;&#35774;&#23450;&#20026;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#25429;&#33719;&#30001;&#23450;&#20041;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-based word embeddings using solely distributional information have consistently produced useful meaning representations for downstream tasks. However, existing approaches often result in representations that are hard to interpret and control. Natural language definitions, on the other side, possess a recursive, self-explanatory semantic structure that can support novel representation learning paradigms able to preserve explicit conceptual relations and constraints in the vector space.  This paper proposes a neuro-symbolic, multi-relational framework to learn word embeddings exclusively from natural language definitions by jointly mapping defined and defining terms along with their corresponding semantic relations. By automatically extracting the relations from definitions corpora and formalising the learning problem via a translational objective, we specialise the framework in hyperbolic space to capture the hierarchical and multi-resolution structure induced by the definitions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26723;&#32423;&#31471;&#21040;&#31471;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#65292;&#23454;&#29616;&#26041;&#38754;&#26816;&#27979;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#35780;&#20998;&#39044;&#27979;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01710</link><description>&lt;p&gt;
&#26143;&#36784;&#21363;&#20320;&#25152;&#38656;&#65306;&#29992;&#36828;&#31243;&#30417;&#30563;&#37329;&#23383;&#22612;&#32593;&#32476;&#36827;&#34892;&#25991;&#26723;&#32423;&#31471;&#21040;&#31471;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Stars Are All You Need: A Distantly Supervised Pyramid Network for Document-Level End-to-End Sentiment Analysis. (arXiv:2305.01710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26723;&#32423;&#31471;&#21040;&#31471;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#65292;&#23454;&#29616;&#26041;&#38754;&#26816;&#27979;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#35780;&#20998;&#39044;&#27979;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25991;&#26723;&#32423;&#31471;&#21040;&#31471;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#23545;&#22312;&#32447;&#35780;&#35770;&#20013;&#34920;&#36798;&#30340;&#26041;&#38754;&#21644;&#35780;&#35770;&#24773;&#24863;&#36827;&#34892;&#26377;&#25928;&#30340;&#32479;&#19968;&#20998;&#26512;&#12290;&#25105;&#20204;&#20551;&#35774;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#26159;&#35780;&#35770;&#20013;&#21508;&#26041;&#38754;&#35780;&#20998;&#30340;&#8220;&#31895;&#31890;&#24230;&#32508;&#21512;&#8221;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#30417;&#30563;&#30340;&#37329;&#23383;&#22612;&#32593;&#32476;&#65288;DSPN&#65289;&#65292;&#21482;&#29992;&#25991;&#26723;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#65292;&#21363;&#21487;&#26377;&#25928;&#22320;&#25191;&#34892;&#26041;&#38754;-&#31867;&#21035;&#26816;&#27979;&#12289;&#26041;&#38754;-&#31867;&#21035;&#24773;&#24863;&#20998;&#26512;&#21644;&#35780;&#20998;&#39044;&#27979;&#12290;&#36890;&#36807;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#25191;&#34892;&#36825;&#19977;&#20010;&#30456;&#20851;&#30340;&#24773;&#24863;&#23376;&#20219;&#21153;&#65292;DSPN&#21487;&#20197;&#25552;&#21462;&#35780;&#35770;&#20013;&#25552;&#21040;&#30340;&#26041;&#38754;&#65292;&#30830;&#23450;&#30456;&#24212;&#30340;&#24773;&#24863;&#65292;&#24182;&#39044;&#27979;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;&#33521;&#25991;&#21644;&#27721;&#35821;&#22810;&#26041;&#38754;&#35780;&#35770;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;DSPN&#65292;&#21457;&#29616;&#20165;&#20351;&#29992;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#36827;&#34892;&#30417;&#30563;&#65292;DSPN&#30340;&#24615;&#33021;&#19982;&#21508;&#31181;&#22522;&#20934;&#27169;&#22411;&#30456;&#24403;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;DSPN&#22312;&#35780;&#35770;&#19978;&#30340;&#21487;&#35299;&#37322;&#24615;&#36755;&#20986;&#65292;&#20197;&#35828;&#26126;&#37329;&#23383;&#22612;&#32593;&#32476;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose document-level end-to-end sentiment analysis to efficiently understand aspect and review sentiment expressed in online reviews in a unified manner. In particular, we assume that star rating labels are a "coarse-grained synthesis" of aspect ratings across in the review. We propose a Distantly Supervised Pyramid Network (DSPN) to efficiently perform Aspect-Category Detection, Aspect-Category Sentiment Analysis, and Rating Prediction using only document star rating labels for training. By performing these three related sentiment subtasks in an end-to-end manner, DSPN can extract aspects mentioned in the review, identify the corresponding sentiments, and predict the star rating labels. We evaluate DSPN on multi-aspect review datasets in English and Chinese and find that with only star rating labels for supervision, DSPN can perform comparably well to a variety of benchmark models. We also demonstrate the interpretability of DSPN's outputs on reviews to show the py
&lt;/p&gt;</description></item></channel></rss>