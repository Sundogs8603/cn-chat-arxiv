<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#26159;&#23548;&#33268;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#36827;&#34892;&#21452;&#21521;&#25512;&#29702;&#30340;&#26681;&#26412;&#21407;&#22240;&#20043;&#19968;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#30340;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00758</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36870;&#36716;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Mitigating Reversal Curse via Semantic-aware Permutation Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00758
&lt;/p&gt;
&lt;p&gt;
&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#26159;&#23548;&#33268;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#36827;&#34892;&#21452;&#21521;&#25512;&#29702;&#30340;&#26681;&#26412;&#21407;&#22240;&#20043;&#19968;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#30340;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#28982;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22240;&#26524;&#20851;&#31995;&#30340;LLM&#36973;&#36935;&#20102;&#8220;&#36870;&#36716;&#35781;&#21650;&#8221;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#65292;&#27169;&#22411;&#30693;&#36947;&#8220;A&#30340;&#29238;&#20146;&#26159;B&#8221;&#65292;&#20294;&#26080;&#27861;&#25512;&#29702;&#20986;&#8220;B&#30340;&#23401;&#23376;&#26159;A&#8221;&#12290;&#36825;&#19968;&#23616;&#38480;&#24615;&#23545;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#36827;&#23637;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#26263;&#31034;&#20102;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#24212;&#29992;&#21452;&#21521;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#25991;&#39318;&#20808;&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#65292;&#24182;&#30830;&#23450;&#20102;&#36870;&#36716;&#35781;&#21650;&#30340;&#26681;&#26412;&#21407;&#22240;&#22312;&#20110;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#20043;&#38388;&#30340;&#35789;&#24207;&#19981;&#21516;&#65292;&#21363;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#39044;&#27979;&#20808;&#34892;&#35789;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#32771;&#34385;&#21040;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#25490;&#21015;&#21487;&#20197;&#34987;&#35270;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#20351;&#27169;&#22411;&#39044;&#27979;&#20808;&#34892;&#35789;&#25110;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#25490;&#21015;&#26041;&#27861;&#21487;&#33021;&#21463;&#21040;&#25130;&#26029;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00758v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the "reversal curse". It is a typical example that the model knows "A's father is B", but is unable to reason "B's child is A". This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional reasoning. In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may dis
&lt;/p&gt;</description></item><item><title>AtP*&#26159;&#19968;&#31181;&#23558;LLM&#34892;&#20026;&#20934;&#30830;&#23450;&#20301;&#21040;&#32452;&#20214;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;Attribution Patching&#23384;&#22312;&#30340;&#26174;&#33879;&#20551;&#38452;&#24615;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#26174;&#33879;&#25913;&#36827;&#20197;&#21450;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.00745</link><description>&lt;p&gt;
AtP*&#65306;&#19968;&#31181;&#23558;LLM&#34892;&#20026;&#23450;&#20301;&#21040;&#32452;&#20214;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AtP*: An efficient and scalable method for localizing LLM behaviour to components
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00745
&lt;/p&gt;
&lt;p&gt;
AtP*&#26159;&#19968;&#31181;&#23558;LLM&#34892;&#20026;&#20934;&#30830;&#23450;&#20301;&#21040;&#32452;&#20214;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;Attribution Patching&#23384;&#22312;&#30340;&#26174;&#33879;&#20551;&#38452;&#24615;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#26174;&#33879;&#25913;&#36827;&#20197;&#21450;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Activation Patching&#26159;&#19968;&#31181;&#30452;&#25509;&#35745;&#31639;&#34892;&#20026;&#22240;&#26524;&#24402;&#22240;&#20110;&#27169;&#22411;&#32452;&#20214;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35201;&#20840;&#38754;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#38656;&#35201;&#36827;&#34892;&#19968;&#27425;&#25104;&#26412;&#38543;&#27169;&#22411;&#32452;&#20214;&#25968;&#37327;&#32447;&#24615;&#22686;&#21152;&#30340;&#25195;&#25551;&#65292;&#36825;&#21487;&#33021;&#23545;SoTA&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35828;&#25104;&#26412;&#36807;&#39640;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;Attribution Patching&#65288;AtP&#65289;&#65292;&#36825;&#26159;&#23545;Activation Patching&#30340;&#19968;&#31181;&#24555;&#36895;&#22522;&#20110;&#26799;&#24230;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#20004;&#31867;&#23548;&#33268;AtP&#20986;&#29616;&#26174;&#33879;&#20551;&#38452;&#24615;&#30340;&#25925;&#38556;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AtP*&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#20004;&#31181;&#25913;&#21464;&#26469;&#35299;&#20915;&#36825;&#20123;&#25925;&#38556;&#27169;&#24335;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;AtP&#21450;&#20854;&#20182;&#24555;&#36895;&#28608;&#27963;&#20462;&#34917;&#26041;&#27861;&#30340;&#23545;&#27604;&#65292;&#24182;&#34920;&#26126;AtP&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#30740;&#31350;&#26041;&#27861;&#65292;&#32780;AtP*&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#38480;&#21046;AtP*&#20272;&#35745;&#30340;&#20551;&#38452;&#24615;&#21097;&#20313;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00745v1 Announce Type: cross  Abstract: Activation Patching is a method of directly computing causal attributions of behavior to model components. However, applying it exhaustively requires a sweep with cost scaling linearly in the number of model components, which can be prohibitively expensive for SoTA Large Language Models (LLMs). We investigate Attribution Patching (AtP), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives. We propose a variant of AtP called AtP*, with two changes to address these failure modes while retaining scalability. We present the first systematic study of AtP and alternative methods for faster activation patching and show that AtP significantly outperforms all other investigated methods, with AtP* providing further significant improvement. Finally, we provide a method to bound the probability of remaining false negatives of AtP* estimates.
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#23545;&#38750;&#35028;&#32654;&#22269;&#33521;&#35821;&#35828;&#35805;&#32773;&#25345;&#26377;&#36127;&#38754;&#30340;&#28508;&#22312;&#21051;&#26495;&#21360;&#35937;&#65292;&#23637;&#29616;&#20102;&#26041;&#35328;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.00742</link><description>&lt;p&gt;
&#26041;&#35328;&#20559;&#35265;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#29289;&#24615;&#26684;&#12289;&#23601;&#19994;&#33021;&#21147;&#21644;&#29359;&#32618;&#20542;&#21521;&#30340;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Dialect prejudice predicts AI decisions about people's character, employability, and criminality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00742
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23545;&#38750;&#35028;&#32654;&#22269;&#33521;&#35821;&#35828;&#35805;&#32773;&#25345;&#26377;&#36127;&#38754;&#30340;&#28508;&#22312;&#21051;&#26495;&#21360;&#35937;&#65292;&#23637;&#29616;&#20102;&#26041;&#35328;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20159;&#20154;&#29616;&#22312;&#19982;&#35821;&#35328;&#27169;&#22411;&#20114;&#21160;&#65292;&#20854;&#29992;&#36884;&#20174;&#20316;&#20026;&#20889;&#20316;&#36741;&#21161;&#21040;&#24433;&#21709;&#25307;&#32856;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#20250;&#20256;&#25773;&#31995;&#32479;&#24615;&#31181;&#26063;&#20559;&#35265;&#65292;&#20351;&#23427;&#20204;&#23545;&#20687;&#38750;&#27954;&#35028;&#32654;&#22269;&#20154;&#36825;&#26679;&#30340;&#32676;&#20307;&#20570;&#20986;&#26377;&#38382;&#39064;&#30340;&#20559;&#35265;&#21028;&#26029;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20307;&#29616;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#31181;&#26063;&#25991;&#21270;&#20559;&#35265;&#65292;&#21363;&#26041;&#35328;&#20559;&#35265;&#65306;&#25105;&#20204;&#25193;&#23637;&#20102;&#20851;&#20110;&#32654;&#22269;&#20154;&#23545;&#38750;&#35028;&#32654;&#22269;&#33521;&#35821;&#35828;&#35805;&#32773;&#25345;&#26377;&#31181;&#26063;&#35821;&#35328;&#21051;&#26495;&#21360;&#35937;&#30340;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#20063;&#26377;&#21516;&#26679;&#30340;&#20559;&#35265;&#65292;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#36825;&#31181;&#21051;&#26495;&#21360;&#35937;&#27604;&#23454;&#39564;&#20013;&#35760;&#24405;&#30340;&#20219;&#20309;&#20154;&#31867;&#20851;&#20110;&#38750;&#35028;&#32654;&#22269;&#20154;&#30340;&#21051;&#26495;&#21360;&#35937;&#37117;&#26356;&#20026;&#36127;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00742v1 Announce Type: cross  Abstract: Hundreds of millions of people now interact with language models, with uses ranging from serving as a writing aid to informing hiring decisions. Yet these language models are known to perpetuate systematic racial prejudices, making their judgments biased in problematic ways about groups like African Americans. While prior research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice: we extend research showing that Americans hold raciolinguistic stereotypes about speakers of African American English and find that language models have the same prejudice, exhibiting covert stereotypes that are more negative than any human stereotypes about African Americans ever experimentally recorde
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#35821;&#20041;&#20449;&#24687;&#32852;&#21512;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;(MFS-HVE)&#12290;</title><link>https://arxiv.org/abs/2403.00724</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#35270;&#35273;&#35777;&#25454;&#30340;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Relation Extraction with Hybrid Visual Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00724
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#35821;&#20041;&#20449;&#24687;&#32852;&#21512;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;(MFS-HVE)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#30340;&#30446;&#26631;&#26159;&#22312;&#21482;&#26377;&#23569;&#37327;&#26631;&#35760;&#23454;&#20363;&#21487;&#20379;&#35757;&#32451;&#26102;&#65292;&#39044;&#27979;&#21477;&#23376;&#20013;&#21629;&#21517;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21333;&#19968;&#27169;&#24577;&#20449;&#24687;&#65292;&#20363;&#22914;&#20165;&#25991;&#26412;&#12290;&#36825;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#24403;&#25991;&#26412;&#20013;&#25551;&#36848;&#30340;&#21629;&#21517;&#23454;&#20307;&#20043;&#38388;&#27809;&#26377;&#26126;&#30830;&#30340;&#19978;&#19979;&#25991;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;(MFS-HVE)&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#35821;&#20041;&#20449;&#24687;&#20849;&#21516;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;MFS-HVE&#21253;&#25324;&#35821;&#20041;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#32452;&#20214;&#12290;MFS-HVE&#35821;&#20041;&#29305;&#24449;&#25552;&#21462;&#22120;&#34987;&#35774;&#35745;&#29992;&#20110;&#25552;&#21462;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#12290;&#35270;&#35273;&#29305;&#24449;&#21253;&#25324;&#20840;&#23616;&#22270;&#20687;&#29305;&#24449;&#21644;&#22270;&#20687;&#20869;&#30340;&#23616;&#37096;&#29289;&#20307;&#29305;&#24449;&#12290;MFS-HVE&#22810;&#27169;&#24577;&#34701;&#21512;&#21333;&#20803;&#21033;&#29992;&#22270;&#20687;&#24341;&#23548;&#27880;&#24847;&#21147;&#21644;&#30446;&#26631;&#24341;&#23548;&#27880;&#24847;&#21147;&#38598;&#25104;&#26469;&#33258;&#21508;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00724v1 Announce Type: new  Abstract: The goal of few-shot relation extraction is to predict relations between name entities in a sentence when only a few labeled instances are available for training. Existing few-shot relation extraction methods focus on uni-modal information such as text only. This reduces performance when there are no clear contexts between the name entities described in text. We propose a multi-modal few-shot relation extraction model (MFS-HVE) that leverages both textual and visual semantic information to learn a multi-modal representation jointly. The MFS-HVE includes semantic feature extractors and multi-modal fusion components. The MFS-HVE semantic feature extractors are developed to extract both textual and visual features. The visual features include global image features and local object features within the image. The MFS-HVE multi-modal fusion unit integrates information from various modalities using image-guided attention, object-guided attentio
&lt;/p&gt;</description></item><item><title>&#23558;&#33258;&#27965;&#35299;&#30721;&#26041;&#27861;&#25193;&#23637;&#21040;&#24320;&#25918;&#21709;&#24212;&#29983;&#25104;&#65292;&#36890;&#36807;&#25972;&#21512;&#25237;&#31080;&#65292;&#24182;&#22312;NLI&#35780;&#20272;&#20013;&#23637;&#31034;&#20102;&#8220;&#26679;&#26412;&#19982;&#36873;&#25321;&#8221;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#35299;&#30721;&#22120;&#21487;&#20197;&#25552;&#39640;30%&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00696</link><description>&lt;p&gt;
&#33258;&#27965;&#35299;&#30721;&#20197;&#33719;&#24471;&#26356;&#30495;&#23454;&#30340;&#24320;&#25918;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Self-Consistent Decoding for More Factual Open Responses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00696
&lt;/p&gt;
&lt;p&gt;
&#23558;&#33258;&#27965;&#35299;&#30721;&#26041;&#27861;&#25193;&#23637;&#21040;&#24320;&#25918;&#21709;&#24212;&#29983;&#25104;&#65292;&#36890;&#36807;&#25972;&#21512;&#25237;&#31080;&#65292;&#24182;&#22312;NLI&#35780;&#20272;&#20013;&#23637;&#31034;&#20102;&#8220;&#26679;&#26412;&#19982;&#36873;&#25321;&#8221;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#35299;&#30721;&#22120;&#21487;&#20197;&#25552;&#39640;30%&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27965;&#24615;&#24050;&#32463;&#25104;&#20026;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20934;&#30830;&#30701;&#31572;&#26696;&#33021;&#21147;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#22312;&#20197;&#21069;&#30340;&#23450;&#20041;&#20013;&#65292;&#23427;&#21482;&#20851;&#27880;&#20174;&#29983;&#25104;&#25991;&#26412;&#20013;&#35299;&#26512;&#20986;&#30340;&#26368;&#32456;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#23558;&#36825;&#19968;&#24605;&#24819;&#25193;&#23637;&#21040;&#24320;&#25918;&#21709;&#24212;&#29983;&#25104;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#26041;&#27861;&#20013;&#25972;&#21512;&#25237;&#31080;&#26469;&#23454;&#29616;&#12290;&#27599;&#20010;&#36755;&#20986;&#21477;&#23376;&#26159;&#20174;&#22810;&#20010;&#26679;&#26412;&#20013;&#36873;&#25321;&#30340;&#65292;&#22522;&#20110;&#31616;&#21333;&#30340;&#20196;&#29260;&#37325;&#21472;&#24471;&#20998;&#65292;&#24182;&#19988;&#26465;&#20214;&#26159;&#20808;&#21069;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#8220;&#26679;&#26412;&#19982;&#36873;&#25321;&#8221;&#26041;&#27861;&#19982;&#36138;&#23146;&#35299;&#30721;&#12289;&#26463;&#25628;&#32034;&#12289;&#26680;&#37319;&#26679;&#20197;&#21450;DoLA&#12289;P-CRR&#21644;S-CRR&#31561;&#26368;&#36817;&#24341;&#20837;&#30340;&#36991;&#20813;&#20135;&#29983;&#24187;&#35273;&#30340;&#35299;&#30721;&#22120;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#26679;&#26412;&#19982;&#36873;&#25321;&#8221;&#26041;&#27861;&#22312;&#22522;&#20110;NLI&#30340;&#35780;&#20272;&#20013;&#27604;&#36825;&#20123;&#35299;&#30721;&#22120;&#22312;CNN/DM&#21644;XSum&#23376;&#38598;&#19978;&#25552;&#39640;&#20102;30%&#30340;&#30495;&#23454;&#24615;&#65292;&#22312;&#32500;&#25345;&#19982;&#21442;&#32771;&#25688;&#35201;&#30456;&#24403;&#30340;ROUGE-1 F1&#20998;&#25968;&#30340;&#21516;&#26102;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#31867;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00696v1 Announce Type: new  Abstract: Self-consistency has emerged as a powerful method for improving the accuracy of short answers generated by large language models. As previously defined, it only concerns the accuracy of a final answer parsed from generated text. In this work, we extend the idea to open response generation, by integrating voting into the decoding method. Each output sentence is selected from among multiple samples, conditioning on the previous selections, based on a simple token overlap score. We compare this "Sample &amp; Select" method to greedy decoding, beam search, nucleus sampling, and the recently introduced hallucination avoiding decoders of DoLA, P-CRR, and S-CRR. We show that Sample &amp; Select improves factuality by a 30% relative margin against these decoders in NLI-based evaluation on the subsets of CNN/DM and XSum used in the FRANK benchmark, while maintaining comparable ROUGE-1 F1 scores against reference summaries. We collect human verifications 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23450;&#20041;&#23383;&#33410;&#28322;&#20215;&#21450;&#24320;&#21457;&#24037;&#20855;&#65292;&#24110;&#21161;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#20419;&#36827;&#22810;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#21644;&#25968;&#25454;&#23454;&#36341;&#30340;&#20844;&#24179;&#24615;</title><link>https://arxiv.org/abs/2403.00686</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#25968;&#25454;&#38598;&#22823;&#23567;&#24046;&#24322;&#20013;&#30340;&#24230;&#37327;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Bit of a Problem: Measurement Disparities in Dataset Sizes Across Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00686
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23450;&#20041;&#23383;&#33410;&#28322;&#20215;&#21450;&#24320;&#21457;&#24037;&#20855;&#65292;&#24110;&#21161;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#20419;&#36827;&#22810;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#21644;&#25968;&#25454;&#23454;&#36341;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#20004;&#31181;&#35821;&#35328;&#20043;&#38388;&#30340;&#23383;&#33410;&#28322;&#20215;&#23450;&#20041;&#20026;&#29992;&#20110;&#32534;&#30721;&#36825;&#20123;&#35821;&#35328;&#20013;&#21305;&#37197;&#20869;&#23481;&#30340;&#25991;&#26412;&#25152;&#38656;&#23383;&#33410;&#30340;&#27604;&#29575;&#12290;&#25105;&#20204;&#35745;&#31639;&#20102;1155&#31181;&#35821;&#35328;&#30340;&#23383;&#33410;&#28322;&#20215;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#26469;&#20272;&#35745;&#20854;&#20182;&#35821;&#35328;&#30340;&#23383;&#33410;&#28322;&#20215;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#24037;&#20855;&#65292;&#21487;&#20197;&#33719;&#21462;&#20219;&#24847;&#20004;&#31181;&#35821;&#35328;&#30340;&#23383;&#33410;&#28322;&#20215;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#36328;&#35821;&#35328;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#27604;&#36739;&#65292;&#20197;&#20415;&#26356;&#20844;&#24179;&#22320;&#24320;&#21457;&#22810;&#35821;&#35328;&#27169;&#22411;&#21644;&#25968;&#25454;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00686v1 Announce Type: new  Abstract: How should text dataset sizes be compared across languages? Even for content-matched (parallel) corpora, UTF-8 encoded text can require a dramatically different number of bytes for different languages. In our work, we define the byte premium between two languages as the ratio of bytes used to encode content-matched text in those languages. We compute byte premiums for 1155 languages, and we use linear regressions to estimate byte premiums for other languages. We release a tool to obtain byte premiums for any two languages, enabling comparisons of dataset sizes across languages for more equitable multilingual model development and data practices.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35299;&#37322;&#23545;&#35805;&#30340;&#36136;&#37327;&#24314;&#27169;&#65292;&#36890;&#36807;&#30740;&#31350;&#35299;&#37322;&#32773;&#21644;&#34987;&#35299;&#37322;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#19982;&#35299;&#37322;&#36136;&#37327;&#30456;&#20851;&#32852;&#65292;&#20197;&#30830;&#20445;&#34987;&#35299;&#37322;&#32773;&#25104;&#21151;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.00662</link><description>&lt;p&gt;
&#24314;&#27169;&#23545;&#35805;&#24335;&#35299;&#37322;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Modeling the Quality of Dialogical Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35299;&#37322;&#23545;&#35805;&#30340;&#36136;&#37327;&#24314;&#27169;&#65292;&#36890;&#36807;&#30740;&#31350;&#35299;&#37322;&#32773;&#21644;&#34987;&#35299;&#37322;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#19982;&#35299;&#37322;&#36136;&#37327;&#30456;&#20851;&#32852;&#65292;&#20197;&#30830;&#20445;&#34987;&#35299;&#37322;&#32773;&#25104;&#21151;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#22312;&#25105;&#20204;&#30340;&#29983;&#27963;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#35299;&#37322;&#20197;&#23545;&#35805;&#24418;&#24335;&#20986;&#29616;&#65292;&#35299;&#37322;&#32773;&#19982;&#34987;&#35299;&#37322;&#32773;&#35752;&#35770;&#24863;&#20852;&#36259;&#30340;&#27010;&#24565;&#25110;&#29616;&#35937;&#12290;&#30001;&#20110;&#20004;&#20301;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#35753;&#34987;&#35299;&#37322;&#32773;&#29702;&#35299;&#24182;&#19981;&#26159;&#19968;&#20214;&#31616;&#21333;&#30340;&#20107;&#24773;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20851;&#27880;&#20102;&#19987;&#23478;&#35299;&#37322;&#32773;&#25104;&#21151;&#23545;&#35805;&#20013;&#30340;&#35299;&#37322;&#27493;&#39588;&#12289;&#23545;&#35805;&#34892;&#20026;&#21644;&#20027;&#39064;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#35299;&#37322;&#32463;&#24120;&#22833;&#36133;&#65292;&#36825;&#24102;&#26469;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#20160;&#20040;&#20250;&#20351;&#23545;&#35805;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#35299;&#37322;&#23545;&#35805;&#65292;&#20851;&#27880;&#35299;&#37322;&#32773;&#21644;&#34987;&#35299;&#37322;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#19982;&#35299;&#37322;&#36136;&#37327;&#30456;&#20851;&#32852;&#65292;&#20197;&#30830;&#20445;&#34987;&#35299;&#37322;&#32773;&#25104;&#21151;&#29702;&#35299;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#26469;&#33258;Reddit&#35770;&#22363;&#8220;&#20687;&#25105;&#20116;&#23681;&#19968;&#26679;&#35299;&#37322;&#8221;&#20013;&#30340;399&#20010;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20132;&#20114;&#27969;&#31243;&#21644;&#35299;&#37322;&#36136;&#37327;&#30340;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00662v1 Announce Type: new  Abstract: Explanations are pervasive in our lives. Mostly, they occur in dialogical form where an {\em explainer} discusses a concept or phenomenon of interest with an {\em explainee}. Leaving the explainee with a clear understanding is not straightforward due to the knowledge gap between the two participants. Previous research looked at the interaction of explanation moves, dialogue acts, and topics in successful dialogues with expert explainers. However, daily-life explanations often fail, raising the question of what makes a dialogue successful. In this work, we study explanation dialogues in terms of the interactions between the explainer and explainee and how they correlate with the quality of explanations in terms of a successful understanding on the explainee's side. In particular, we first construct a corpus of 399 dialogues from the Reddit forum {\em Explain Like I am Five} and annotate it for interaction flows and explanation quality. We
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34913;&#37327;&#25991;&#26412;&#22810;&#26679;&#24615;&#30340;&#26631;&#20934;&#20998;&#25968;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#21387;&#32553;&#31639;&#27861;&#21487;&#20197;&#25429;&#25417;&#31867;&#20284;&#20110;$n$-gram&#37325;&#21472;&#21516;&#36136;&#24615;&#24471;&#20998;&#30340;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#22810;&#31181;&#24230;&#37327;&#26041;&#27861;&#26469;&#25253;&#21578;&#20998;&#25968;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#26412;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.00553</link><description>&lt;p&gt;
&#35268;&#33539;&#25991;&#26412;&#22810;&#26679;&#24615;&#30340;&#27979;&#37327;&#65306;&#19968;&#20010;&#24037;&#20855;&#21644;&#23545;&#20998;&#25968;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34913;&#37327;&#25991;&#26412;&#22810;&#26679;&#24615;&#30340;&#26631;&#20934;&#20998;&#25968;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#21387;&#32553;&#31639;&#27861;&#21487;&#20197;&#25429;&#25417;&#31867;&#20284;&#20110;$n$-gram&#37325;&#21472;&#21516;&#36136;&#24615;&#24471;&#20998;&#30340;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#22810;&#31181;&#24230;&#37327;&#26041;&#27861;&#26469;&#25253;&#21578;&#20998;&#25968;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#26412;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#22609;&#36896;&#20102;&#20154;&#20204;&#23545;&#20854;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#30340;&#30475;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#33521;&#35821;&#25991;&#26412;&#30340;&#22810;&#26679;&#24615;&#24471;&#20998;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#21387;&#32553;&#31639;&#27861;&#25429;&#25417;&#21040;&#19982;$n$-gram&#30340;&#37325;&#21472;&#21516;&#36136;&#24615;&#24471;&#20998;&#25152;&#34913;&#37327;&#30340;&#20449;&#24687;&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;&#32467;&#21512;&#22810;&#31181;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#21387;&#32553;&#27604;&#12289;&#38271;$n$-gram&#30340;&#33258;&#37325;&#22797;&#12289;Self-BLEU&#21644;BERTScore&#8212;&#8212;&#36275;&#20197;&#25253;&#21578;&#65292;&#22240;&#20026;&#23427;&#20204;&#24444;&#27492;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#36739;&#20302;&#12290;&#36825;&#20123;&#20998;&#25968;&#30340;&#36866;&#29992;&#24615;&#36229;&#20986;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#26512;&#65307;&#20363;&#22914;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#22312;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#22810;&#26679;&#24615;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00553v1 Announce Type: new  Abstract: The diversity across outputs generated by large language models shapes the perception of their quality and utility. Prompt leaks, templated answer structure, and canned responses across different interactions are readily noticed by people, but there is no standard score to measure this aspect of model behavior. In this work we empirically investigate diversity scores on English texts. We find that computationally efficient compression algorithms capture information similar to what is measured by slow to compute $n$-gram overlap homogeneity scores. Further, a combination of measures -- compression ratios, self-repetition of long $n$-grams and Self-BLEU and BERTScore -- are sufficient to report, as they have low mutual correlation with each other. The applicability of scores extends beyond analysis of generative models; for example, we highlight applications on instruction-tuning datasets and human-produced texts. We release a diversity sc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35299;&#30721;&#22120;&#27169;&#22411;&#26469;&#29983;&#25104;&#22320;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#33258;&#21160;&#26657;&#27491;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#25340;&#20889;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2403.00528</link><description>&lt;p&gt;
&#29992;&#20110;&#21516;&#26102;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#25552;&#21462;&#21644;&#25340;&#20889;&#26657;&#27491;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Simultaneous Named Entity Extraction and Spelling Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35299;&#30721;&#22120;&#27169;&#22411;&#26469;&#29983;&#25104;&#22320;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#33258;&#21160;&#26657;&#27491;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#25340;&#20889;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22914;BERT&#24050;&#34987;&#35777;&#26126;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#65288;NE&#65289;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;BERT LM&#36890;&#24120;&#34987;&#29992;&#20316;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#23558;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#21333;&#20010;&#26631;&#35760;&#20998;&#31867;&#65292;&#25110;&#23558;&#26631;&#35760;&#33539;&#22260;&#20998;&#31867;&#20026;&#21487;&#33021;&#30340;NE&#31867;&#21035;&#20043;&#19968;&#12290;&#26412;&#25991;&#20551;&#35774;&#20165;&#35299;&#30721;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20063;&#21487;&#20197;&#34987;&#29983;&#25104;&#24615;&#22320;&#29992;&#20110;&#25552;&#21462;NE&#20197;&#21450;&#21487;&#33021;&#24674;&#22797;NE&#30340;&#27491;&#30830;&#34920;&#38754;&#24418;&#24335;&#65292;&#20854;&#20013;&#36755;&#20837;&#25991;&#26412;&#20013;&#23384;&#22312;&#30340;&#20219;&#20309;&#25340;&#20889;&#38169;&#35823;&#37117;&#23558;&#34987;&#33258;&#21160;&#32416;&#27491;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;BERT LMs&#36827;&#34892;&#24494;&#35843;&#20316;&#20026;&#22522;&#32447;&#65292;&#20197;&#21450;&#20843;&#20010;&#24320;&#28304;LLMs&#65292;&#22312;&#36890;&#36807;&#23558;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#24212;&#29992;&#20110;&#26085;&#26412;&#21830;&#24215;&#25910;&#25454;&#22270;&#20687;&#32780;&#33719;&#24471;&#30340;&#25991;&#26412;&#19978;&#36827;&#34892;NE&#29983;&#25104;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19981;&#23581;&#35797;&#25214;&#21040;&#25110;&#35780;&#20272;&#25991;&#26412;&#20013;NE&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#34920;&#26126;&#26368;&#20248;&#24494;&#35843;&#30340;LLM&#30340;&#34920;&#29616;&#22914;&#20309;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00528v1 Announce Type: new  Abstract: Language Models (LMs) such as BERT, have been shown to perform well on the task of identifying Named Entities (NE) in text. A BERT LM is typically used as a classifier to classify individual tokens in the input text, or to classify spans of tokens, as belonging to one of a set of possible NE categories.   In this paper, we hypothesise that decoder-only Large Language Models (LLMs) can also be used generatively to extract both the NE, as well as potentially recover the correct surface form of the NE, where any spelling errors that were present in the input text get automatically corrected.   We fine-tune two BERT LMs as baselines, as well as eight open-source LLMs, on the task of producing NEs from text that was obtained by applying Optical Character Recognition (OCR) to images of Japanese shop receipts; in this work, we do not attempt to find or evaluate the location of NEs in the text.   We show that the best fine-tuned LLM performs as 
&lt;/p&gt;</description></item><item><title>ROME&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#35760;&#24518;&#21644;&#38750;&#35760;&#24518;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#21270;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#19981;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20102;&#35299;&#27169;&#22411;&#35760;&#24518;&#30340;&#27934;&#23519;&#21644;&#24433;&#21709;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.00510</link><description>&lt;p&gt;
ROME: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25991;&#26412;&#12289;&#27010;&#29575;&#21644;&#38544;&#34255;&#29366;&#24577;&#30340;&#35760;&#24518;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
ROME: Memorization Insights from Text, Probability and Hidden State in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00510
&lt;/p&gt;
&lt;p&gt;
ROME&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#35760;&#24518;&#21644;&#38750;&#35760;&#24518;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#21270;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#19981;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20102;&#35299;&#27169;&#22411;&#35760;&#24518;&#30340;&#27934;&#23519;&#21644;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#21270;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24314;&#31435;&#20102;&#29992;&#20110;&#37327;&#21270;&#35760;&#24518;&#30340;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#24433;&#21709;&#22240;&#32032;&#65292;&#22914;&#25968;&#25454;&#22797;&#21046;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#25552;&#31034;&#38271;&#24230;&#65292;&#24182;&#36890;&#36807;&#23558;&#27169;&#22411;&#36755;&#20986;&#19982;&#35757;&#32451;&#35821;&#26009;&#24211;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#35760;&#24518;&#21270;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#35821;&#26009;&#24211;&#35268;&#27169;&#24040;&#22823;&#19988;&#20854;&#39044;&#22788;&#29702;&#32791;&#26102;&#12290;&#20026;&#20102;&#22312;&#19981;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#35760;&#24518;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROME&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#27604;&#36739;&#35760;&#24518;&#21270;&#21644;&#38750;&#35760;&#24518;&#21270;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25506;&#32034;&#35760;&#24518;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#27169;&#22411;&#39318;&#20808;&#23558;&#36873;&#23450;&#30340;&#26679;&#26412;&#20998;&#20026;&#35760;&#24518;&#21270;&#21644;&#38750;&#35760;&#24518;&#21270;&#32452;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#12289;&#27010;&#29575;&#21644;&#38544;&#34255;&#29366;&#24577;&#30340;&#35265;&#35299;&#27604;&#36739;&#36825;&#20004;&#32452;&#20013;&#30340;&#28436;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#21253;&#25324;&#35789;&#38271;&#12289;&#35789;&#24615;&#12289;&#35789;&#39057;&#12289;&#22343;&#20540;&#21644;&#26041;&#24046;&#22312;&#20869;&#30340;&#22240;&#32032;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00510v1 Announce Type: cross  Abstract: Probing the memorization of large language models holds significant importance. Previous works have established metrics for quantifying memorization, explored various influencing factors, such as data duplication, model size, and prompt length, and evaluated memorization by comparing model outputs with training corpora. However, the training corpora are of enormous scale and its pre-processing is time-consuming. To explore memorization without accessing training data, we propose a novel approach, named ROME, wherein memorization is explored by comparing disparities across memorized and non-memorized. Specifically, models firstly categorize the selected samples into memorized and non-memorized groups, and then comparing the demonstrations in the two groups from the insights of text, probability, and hidden state. Experimental findings show the disparities in factors including word length, part-of-speech, word frequency, mean and varianc
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#19978;&#19979;&#25991;&#26500;&#24314;&#34920;&#31034;&#65288;CCR&#65289;&#30340;&#21382;&#21490;&#24515;&#29702;&#25991;&#26412;&#20998;&#26512;&#27969;&#31243;&#32467;&#21512;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#21476;&#20856;&#20013;&#25991;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#24515;&#29702;&#26500;&#24314;&#65292;&#37319;&#29992;&#38388;&#25509;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00509</link><description>&lt;p&gt;
&#35843;&#26597;&#27515;&#20129;&#24605;&#32500;&#65306;&#22522;&#20110;&#19978;&#19979;&#25991;&#26500;&#24314;&#34920;&#31034;&#65288;CCR&#65289;&#30340;&#21382;&#21490;&#24515;&#29702;&#25991;&#26412;&#20998;&#26512;&#23545;&#20110;&#21476;&#20856;&#20013;&#25991;
&lt;/p&gt;
&lt;p&gt;
Surveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00509
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#26500;&#24314;&#34920;&#31034;&#65288;CCR&#65289;&#30340;&#21382;&#21490;&#24515;&#29702;&#25991;&#26412;&#20998;&#26512;&#27969;&#31243;&#32467;&#21512;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#21476;&#20856;&#20013;&#25991;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#24515;&#29702;&#26500;&#24314;&#65292;&#37319;&#29992;&#38388;&#25509;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#21476;&#20856;&#20013;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#21382;&#21490;&#24515;&#29702;&#25991;&#26412;&#20998;&#26512;&#27969;&#31243;&#12290;&#25968;&#21315;&#24180;&#26469;&#65292;&#20154;&#31867;&#29992;&#21508;&#31181;&#35821;&#35328;&#21019;&#20316;&#25991;&#26412;&#65307;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#35745;&#31639;&#25991;&#29486;&#37117;&#38598;&#20013;&#22312;&#24403;&#20195;&#35821;&#35328;&#21644;&#35821;&#26009;&#24211;&#19978;&#12290;&#21382;&#21490;&#24515;&#29702;&#23398;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#20381;&#36182;&#35745;&#31639;&#25216;&#26415;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#24320;&#21457;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#21382;&#21490;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#24515;&#29702;&#23398;&#26041;&#38754;&#30340;&#20869;&#23481;&#12290;&#21517;&#20026;&#19978;&#19979;&#25991;&#21270;&#32467;&#26500;&#34920;&#24449;&#65288;CCR&#65289;&#30340;&#24403;&#21069;&#27969;&#31243;&#65292;&#32467;&#21512;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#65288;&#21363;&#24515;&#29702;&#35843;&#26597;&#65289;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36890;&#36807;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#20197;&#27979;&#37327;&#21476;&#20856;&#20013;&#25991;&#35821;&#26009;&#24211;&#20013;&#30340;&#20256;&#32479;&#20027;&#20041;&#12289;&#35268;&#33539;&#21147;&#37327;&#21644;&#38598;&#20307;&#20027;&#20041;&#31561;&#24515;&#29702;&#26500;&#24314;&#12290;&#37492;&#20110;&#21487;&#29992;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38388;&#25509;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#20013;&#25991;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00509v1 Announce Type: cross  Abstract: In this work, we develop a pipeline for historical-psychological text analysis in classical Chinese. Humans have produced texts in various languages for thousands of years; however, most of the computational literature is focused on contemporary languages and corpora. The emerging field of historical psychology relies on computational techniques to extract aspects of psychology from historical corpora using new methods developed in natural language processing (NLP). The present pipeline, called Contextualized Construct Representations (CCR), combines expert knowledge in psychometrics (i.e., psychological surveys) with text representations generated via transformer-based language models to measure psychological constructs such as traditionalism, norm strength, and collectivism in classical Chinese corpora. Considering the scarcity of available data, we propose an indirect supervised contrastive learning approach and build the first Chin
&lt;/p&gt;</description></item><item><title>PoTeC&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#39046;&#22495;&#19987;&#23478;&#21644;&#26032;&#25163;&#30524;&#21160;&#25968;&#25454;&#30340;&#33258;&#28982;&#38405;&#35835;&#35821;&#26009;&#24211;&#65292;&#24182;&#19988;&#37319;&#29992;&#20102;&#23436;&#20840;&#20132;&#20114;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#30740;&#31350;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.00506</link><description>&lt;p&gt;
PoTeC&#65306;&#19968;&#20010;&#24503;&#35821;&#33258;&#28982;&#30524;&#21160;&#38405;&#35835;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
PoTeC: A German Naturalistic Eye-tracking-while-reading Corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00506
&lt;/p&gt;
&lt;p&gt;
PoTeC&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#39046;&#22495;&#19987;&#23478;&#21644;&#26032;&#25163;&#30524;&#21160;&#25968;&#25454;&#30340;&#33258;&#28982;&#38405;&#35835;&#35821;&#26009;&#24211;&#65292;&#24182;&#19988;&#37319;&#29992;&#20102;&#23436;&#20840;&#20132;&#20114;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#30740;&#31350;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00506v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#27874;&#33576;&#22374;&#25945;&#31185;&#20070;&#35821;&#26009;&#24211;&#65288;PoTeC&#65289;&#26159;&#19968;&#20010;&#33258;&#28982;&#30524;&#21160;&#38405;&#35835;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#20102;75&#20301;&#21442;&#19982;&#32773;&#38405;&#35835;12&#31687;&#31185;&#23398;&#25991;&#31456;&#30340;&#25968;&#25454;&#12290; PoTeC&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#39046;&#22495;&#19987;&#23478;&#21644;&#26032;&#25163;&#30524;&#21160;&#25968;&#25454;&#30340;&#33258;&#28982;&#38405;&#35835;&#35821;&#26009;&#24211;&#65292;&#37319;&#29992;&#20102;2x2x2&#23436;&#20840;&#20132;&#20114;&#35774;&#35745;&#65292;&#21253;&#25324;&#21442;&#19982;&#32773;&#30340;&#23398;&#20064;&#27700;&#24179;&#21644;&#23398;&#20064;&#23398;&#31185;&#20316;&#20026;&#21463;&#35797;&#32773;&#38388;&#22240;&#32032;&#65292;&#25991;&#26412;&#39046;&#22495;&#20316;&#20026;&#21463;&#35797;&#32773;&#20869;&#22240;&#32032;&#12290; &#21442;&#19982;&#32773;&#30340;&#38405;&#35835;&#29702;&#35299;&#36890;&#36807;&#19968;&#31995;&#21015;&#25991;&#26412;&#29702;&#35299;&#38382;&#39064;&#36827;&#34892;&#35780;&#20272;&#65292;&#20182;&#20204;&#30340;&#39046;&#22495;&#30693;&#35782;&#36890;&#36807;&#27599;&#31687;&#25991;&#26412;&#30340;&#25991;&#26412;&#29420;&#31435;&#32972;&#26223;&#38382;&#39064;&#36827;&#34892;&#27979;&#35797;&#12290; &#36825;&#20123;&#26448;&#26009;&#38024;&#23545;&#19981;&#21516;&#32423;&#21035;&#30340;&#22810;&#31181;&#35821;&#35328;&#29305;&#24449;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290; &#25105;&#20204;&#35774;&#24819;PoTeC&#21487;&#29992;&#20110;&#21508;&#31181;&#30740;&#31350;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#23545;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#37325;&#26032;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00506v1 Announce Type: new  Abstract: The Potsdam Textbook Corpus (PoTeC) is a naturalistic eye-tracking-while-reading corpus containing data from 75 participants reading 12 scientific texts. PoTeC is the first naturalistic eye-tracking-while-reading corpus that contains eye-movements from domain-experts as well as novices in a within-participant manipulation: It is based on a 2x2x2 fully-crossed factorial design which includes the participants' level of study and the participants' discipline of study as between-subject factors and the text domain as a within-subject factor. The participants' reading comprehension was assessed by a series of text comprehension questions and their domain knowledge was tested by text-independent background questions for each of the texts. The materials are annotated for a variety of linguistic features at different levels. We envision PoTeC to be used for a wide range of studies including but not limited to analyses of expert and non-expert re
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#23398;&#27966;&#23545;&#20110;&#26426;&#22120;&#26159;&#21542;&#29702;&#35299;&#25991;&#26412;&#23384;&#22312;&#20998;&#27495;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#26126;&#30830;&#23450;&#20041;&#30340;&#29702;&#35299;&#24037;&#20316;&#23450;&#20041;&#65292;&#26126;&#30830;&#25215;&#35748;&#20102;&#24847;&#35782;&#30340;&#38382;&#39064;&#65292;&#24182;&#19982;&#21746;&#23398;&#12289;&#24515;&#29702;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#31561;&#20016;&#23500;&#25991;&#29486;&#24314;&#31435;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.00499</link><description>&lt;p&gt;
&#20725;&#23608;&#25026;&#21527;&#65311;&#19968;&#31181;&#36873;&#25321;&#33258;&#24049;&#20882;&#38505;&#25506;&#32034;&#26426;&#22120;&#35748;&#30693;&#30340;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of Machine Cognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00499
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#23398;&#27966;&#23545;&#20110;&#26426;&#22120;&#26159;&#21542;&#29702;&#35299;&#25991;&#26412;&#23384;&#22312;&#20998;&#27495;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#26126;&#30830;&#23450;&#20041;&#30340;&#29702;&#35299;&#24037;&#20316;&#23450;&#20041;&#65292;&#26126;&#30830;&#25215;&#35748;&#20102;&#24847;&#35782;&#30340;&#38382;&#39064;&#65292;&#24182;&#19982;&#21746;&#23398;&#12289;&#24515;&#29702;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#31561;&#20016;&#23500;&#25991;&#29486;&#24314;&#31435;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;LLMs&#30340;&#36827;&#23637;&#24341;&#21457;&#20102;&#19968;&#20010;&#20851;&#20110;&#23427;&#20204;&#26159;&#21542;&#29702;&#35299;&#25991;&#26412;&#30340;&#36777;&#35770;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#36825;&#22330;&#36777;&#35770;&#20013;&#65292;&#21453;&#23545;&#27966;&#23545;&#20110;&#29702;&#35299;&#26377;&#19981;&#21516;&#30340;&#23450;&#20041;&#65292;&#23588;&#20854;&#22312;&#24847;&#35782;&#30340;&#20316;&#29992;&#19978;&#23384;&#22312;&#20998;&#27495;&#12290;&#20026;&#20102;&#35777;&#23454;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#24605;&#24819;&#23454;&#39564;&#65292;&#28041;&#21450;&#19968;&#20010;&#22312;&#27599;&#20010;&#21487;&#33021;&#30340;&#22522;&#20934;&#19978;&#37117;&#34920;&#29616;&#20986;&#33394;&#30340;&#24320;&#28304;&#32842;&#22825;&#26426;&#22120;&#20154; $Z$&#65292;&#30475;&#20284;&#27809;&#26377;&#20027;&#35266;&#32463;&#39564;&#12290;&#25105;&#20204;&#38382;$Z$&#26159;&#21542;&#26377;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#22312;AI&#30740;&#31350;&#20013;&#30340;&#19981;&#21516;&#23398;&#27966;&#20284;&#20046;&#20197;&#19981;&#21516;&#26041;&#24335;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#26415;&#35821;&#19978;&#30340;&#20998;&#27495;&#12290;&#22312;&#26410;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26126;&#30830;&#23450;&#20041;&#30340;&#29702;&#35299;&#24037;&#20316;&#23450;&#20041;&#65292;&#26126;&#30830;&#25215;&#35748;&#20102;&#24847;&#35782;&#30340;&#38382;&#39064;&#65292;&#24182;&#19982;&#21746;&#23398;&#12289;&#24515;&#29702;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#31561;&#20016;&#23500;&#25991;&#29486;&#24314;&#31435;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00499v1 Announce Type: new  Abstract: Recent advances in LLMs have sparked a debate on whether they understand text. In this position paper, we argue that opponents in this debate hold different definitions for understanding, and particularly differ in their view on the role of consciousness. To substantiate this claim, we propose a thought experiment involving an open-source chatbot $Z$ which excels on every possible benchmark, seemingly without subjective experience. We ask whether $Z$ is capable of understanding, and show that different schools of thought within seminal AI research seem to answer this question differently, uncovering their terminological disagreement. Moving forward, we propose two distinct working definitions for understanding which explicitly acknowledge the question of consciousness, and draw connections with a rich literature in philosophy, psychology and neuroscience.
&lt;/p&gt;</description></item><item><title>LUCID&#26088;&#22312;&#36890;&#36807;&#39640;&#36136;&#37327;&#21644;&#35821;&#35328;&#22797;&#26434;&#30340;&#25968;&#25454;&#65292;&#20197;&#21450;&#39640;&#24230;&#33258;&#21160;&#21270;&#30340;LLM&#27169;&#22411;&#65292;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#35206;&#30422;&#26377;&#38480;&#12289;&#23545;&#35805;&#29616;&#35937;&#26377;&#38480;&#12289;&#26410;&#26631;&#35760;&#30340;&#29305;&#28857;&#65292;&#20197;&#21450;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#25237;&#20837;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00462</link><description>&lt;p&gt;
LUCID: &#30001;LLM&#29983;&#25104;&#30340;&#22797;&#26434;&#19988;&#26377;&#36259;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00462
&lt;/p&gt;
&lt;p&gt;
LUCID&#26088;&#22312;&#36890;&#36807;&#39640;&#36136;&#37327;&#21644;&#35821;&#35328;&#22797;&#26434;&#30340;&#25968;&#25454;&#65292;&#20197;&#21450;&#39640;&#24230;&#33258;&#21160;&#21270;&#30340;LLM&#27169;&#22411;&#65292;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#35206;&#30422;&#26377;&#38480;&#12289;&#23545;&#35805;&#29616;&#35937;&#26377;&#38480;&#12289;&#26410;&#26631;&#35760;&#30340;&#29305;&#28857;&#65292;&#20197;&#21450;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#25237;&#20837;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00462v1 &#20844;&#21578;&#31867;&#22411;:&#26032; &#25688;&#35201;:&#34394;&#25311;&#21161;&#25163;&#22312;&#23545;&#35805;&#33021;&#21147;&#26041;&#38754;&#21363;&#23558;&#36808;&#21521;&#19968;&#20010;&#37325;&#35201;&#36827;&#27493;&#65292;&#36825;&#24471;&#30410;&#20110;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#30495;&#27491;&#38761;&#21629;&#24615;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#33021;&#21147;&#30340;&#20027;&#35201;&#29942;&#39048;&#20173;&#28982;&#26159;&#39640;&#36136;&#37327;&#21644;&#35821;&#35328;&#22797;&#26434;&#30340;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#19978;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#39046;&#22495;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#65292;&#21253;&#21547;&#23525;&#23525;&#26080;&#20960;&#30340;&#30495;&#27491;&#20855;&#25361;&#25112;&#24615;&#30340;&#23545;&#35805;&#29616;&#35937;&#65307;&#36825;&#20123;&#29616;&#35937;&#36890;&#24120;&#26410;&#26631;&#35760;&#65292;&#36825;&#20351;&#24471;&#22312;&#27809;&#26377;&#36153;&#26102;&#36153;&#21147;&#30340;&#20154;&#31867;&#35780;&#20272;&#30340;&#24773;&#20917;&#19979;&#38590;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#27492;&#22806;&#65292;&#30452;&#21040;&#29616;&#22312;&#65292;&#21019;&#24314;&#39640;&#36136;&#37327;&#23545;&#35805;&#25968;&#25454;&#20173;&#38656;&#35201;&#30456;&#24403;&#22823;&#37327;&#30340;&#20154;&#21147;&#25237;&#20837;&#65292;&#36825;&#38480;&#21046;&#20102;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#20197;&#21450;&#24555;&#36895;&#20026;&#26032;&#30446;&#26631;&#39046;&#22495;&#30340;&#25968;&#25454;&#22686;&#37327;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;LUCID&#26469;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#19988;&#39640;&#24230;&#33258;&#21160;&#21270;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00462v1 Announce Type: new  Abstract: Virtual assistants are poised to take a dramatic leap forward in terms of their dialogue capabilities, spurred by recent advances in transformer-based Large Language Models (LLMs). Yet a major bottleneck to achieving genuinely transformative task-oriented dialogue capabilities remains the scarcity of high quality and linguistically sophisticated data. Existing datasets, while impressive in scale, have limited domain coverage and contain few genuinely challenging conversational phenomena; those which are present are typically unlabelled, making it difficult to assess the strengths and weaknesses of models without time-consuming and costly human evaluation. Moreover, creating high quality dialogue data has until now required considerable human input, limiting both the scale of these datasets and the ability to rapidly bootstrap data for a new target domain. We aim to overcome these issues with LUCID, a modularised and highly automated LLM-
&lt;/p&gt;</description></item><item><title>PRIMATE&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#25233;&#37057;&#27700;&#24179;&#35780;&#20272;&#30340;&#27880;&#37322;&#36136;&#37327;&#38382;&#39064;&#65292;&#36890;&#36807;&#31934;&#31070;&#20581;&#24247;&#19987;&#19994;&#20154;&#22763;&#37325;&#26032;&#27880;&#37322;&#65292;&#25552;&#39640;&#20102;&#26080;&#27442;&#26395;&#30151;&#29366;&#26816;&#27979;&#30340;&#27979;&#35797;&#38598;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.00438</link><description>&lt;p&gt;
&#24744;&#30340;&#27169;&#22411;&#26410;&#33021;&#26377;&#25928;&#39044;&#27979;&#25233;&#37057;&#65292;&#21407;&#22240;&#20309;&#22312;&#65306;PRIMATE&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Your Model Is Not Predicting Depression Well And That Is Why: A Case Study of PRIMATE Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00438
&lt;/p&gt;
&lt;p&gt;
PRIMATE&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#25233;&#37057;&#27700;&#24179;&#35780;&#20272;&#30340;&#27880;&#37322;&#36136;&#37327;&#38382;&#39064;&#65292;&#36890;&#36807;&#31934;&#31070;&#20581;&#24247;&#19987;&#19994;&#20154;&#22763;&#37325;&#26032;&#27880;&#37322;&#65292;&#25552;&#39640;&#20102;&#26080;&#27442;&#26395;&#30151;&#29366;&#26816;&#27979;&#30340;&#27979;&#35797;&#38598;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#29992;&#20110;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#36827;&#34892;&#25233;&#37057;&#27700;&#24179;&#20272;&#35745;&#30340;&#31934;&#31070;&#20581;&#24247;&#25968;&#25454;&#38598;&#20013;&#27880;&#37322;&#30340;&#36136;&#37327;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#29992;&#20108;&#36827;&#21046;&#31867;&#21035;&#27880;&#37322;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#65292;&#21363;&#25233;&#37057;&#25110;&#38750;&#25233;&#37057;&#65292;&#20294;&#26368;&#36817;&#30340;&#25968;&#25454;&#38598;&#65288;&#22914;D2S&#21644;PRIMATE&#65289;&#26088;&#22312;&#20351;&#29992;PHQ-9&#30151;&#29366;&#36827;&#34892;&#26356;&#32454;&#33268;&#30340;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#25968;&#25454;&#38598;&#20381;&#36182;&#20110;&#27809;&#26377;&#39046;&#22495;&#30693;&#35782;&#30340;&#20247;&#21253;&#24037;&#20316;&#32773;&#36827;&#34892;&#27880;&#37322;&#12290;&#38024;&#23545;PRIMATE&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#38024;&#23545;&#27880;&#37322;&#26377;&#25928;&#24615;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#32570;&#20047;&#20852;&#36259;&#25110;&#24841;&#24742;&#30151;&#29366;&#12290;&#36890;&#36807;&#31934;&#31070;&#20581;&#24247;&#19987;&#19994;&#20154;&#22763;&#30340;&#37325;&#26032;&#27880;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26356;&#31934;&#32454;&#30340;&#26631;&#31614;&#21644;&#25991;&#26412;&#33539;&#22260;&#20316;&#20026;&#35777;&#25454;&#65292;&#35782;&#21035;&#20986;&#30456;&#24403;&#25968;&#37327;&#30340;&#35823;&#25253;&#12290;&#25105;&#20204;&#30340;&#31934;&#32454;&#27880;&#37322;&#23558;&#22312;&#25968;&#25454;&#20351;&#29992;&#21327;&#35758;&#19979;&#21457;&#24067;&#65292;&#20026;&#26080;&#27442;&#26395;&#30151;&#29366;&#26816;&#27979;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#27979;&#35797;&#38598;&#12290;&#26412;&#30740;&#31350;&#20984;&#26174;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00438v1 Announce Type: new  Abstract: This paper addresses the quality of annotations in mental health datasets used for NLP-based depression level estimation from social media texts. While previous research relies on social media-based datasets annotated with binary categories, i.e. depressed or non-depressed, recent datasets such as D2S and PRIMATE aim for nuanced annotations using PHQ-9 symptoms. However, most of these datasets rely on crowd workers without the domain knowledge for annotation. Focusing on the PRIMATE dataset, our study reveals concerns regarding annotation validity, particularly for the lack of interest or pleasure symptom. Through reannotation by a mental health professional, we introduce finer labels and textual spans as evidence, identifying a notable number of false positives. Our refined annotations, to be released under a Data Use Agreement, offer a higher-quality test set for anhedonia detection. This study underscores the necessity of addressing a
&lt;/p&gt;</description></item><item><title>HIRO &#26159;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#25277;&#35937;&#24847;&#35265;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32034;&#24341;&#32467;&#26500;&#26469;&#25552;&#21462;&#36755;&#20837;&#35780;&#35770;&#20013;&#27969;&#34892;&#24847;&#35265;&#30340;&#21477;&#23376;&#31751;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30456;&#20851;&#30340;&#25688;&#35201;&#65292;&#24471;&#21040;&#26356;&#20855;&#35821;&#20041;&#32467;&#26500;&#30340;&#32534;&#30721;&#31354;&#38388;&#21644;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#25688;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.00435</link><description>&lt;p&gt;
&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#24847;&#35265;&#25688;&#35201;&#30340;&#20998;&#23618;&#32034;&#24341;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Indexing for Retrieval-Augmented Opinion Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00435
&lt;/p&gt;
&lt;p&gt;
HIRO &#26159;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#25277;&#35937;&#24847;&#35265;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32034;&#24341;&#32467;&#26500;&#26469;&#25552;&#21462;&#36755;&#20837;&#35780;&#35770;&#20013;&#27969;&#34892;&#24847;&#35265;&#30340;&#21477;&#23376;&#31751;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30456;&#20851;&#30340;&#25688;&#35201;&#65292;&#24471;&#21040;&#26356;&#20855;&#35821;&#20041;&#32467;&#26500;&#30340;&#32534;&#30721;&#31354;&#38388;&#21644;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#25277;&#35937;&#24847;&#35265;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#25277;&#21462;&#26041;&#27861;&#30340;&#21487;&#24402;&#22240;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36830;&#36143;&#24615;&#21644;&#27969;&#30021;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;HIRO&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#23558;&#21477;&#23376;&#26144;&#23556;&#21040;&#36890;&#36807;&#35821;&#20041;&#32452;&#32455;&#30340;&#31163;&#25955;&#23618;&#27425;&#32467;&#26500;&#36335;&#24452;&#30340;&#32034;&#24341;&#32467;&#26500;&#12290;&#22312;&#25512;&#26029;&#26102;&#65292;&#25105;&#20204;&#22635;&#20805;&#32034;&#24341;&#24182;&#20351;&#29992;&#23427;&#26469;&#35782;&#21035;&#21644;&#26816;&#32034;&#21253;&#21547;&#36755;&#20837;&#35780;&#35770;&#20013;&#27969;&#34892;&#24847;&#35265;&#30340;&#21477;&#23376;&#31751;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;LLM&#29983;&#25104;&#19968;&#20010;&#22522;&#20110;&#36825;&#20123;&#25552;&#21462;&#30340;&#35777;&#25454;&#31751;&#30340;&#21487;&#35835;&#25688;&#35201;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#27169;&#22359;&#21270;&#24615;&#20801;&#35768;&#25105;&#20204;&#22312;&#27599;&#20010;&#38454;&#27573;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;HIRO&#23398;&#20064;&#20102;&#27604;&#20808;&#21069;&#24037;&#20316;&#26356;&#20855;&#35821;&#20041;&#32467;&#26500;&#30340;&#32534;&#30721;&#31354;&#38388;&#65292;&#24182;&#29983;&#25104;&#20102;&#26356;&#31526;&#21512;&#36755;&#20837;&#35780;&#35770;&#20013;&#24847;&#35265;&#30340;&#25688;&#35201;&#12290;&#20154;&#31867;&#35780;&#20272;&#35777;&#23454;&#65292;HIRO&#29983;&#25104;&#30340;&#25688;&#35201;&#26356;&#36830;&#36143;&#12289;&#35814;&#32454;&#21644;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00435v1 Announce Type: new  Abstract: We propose a method for unsupervised abstractive opinion summarization, that combines the attributability and scalability of extractive approaches with the coherence and fluency of Large Language Models (LLMs). Our method, HIRO, learns an index structure that maps sentences to a path through a semantically organized discrete hierarchy. At inference time, we populate the index and use it to identify and retrieve clusters of sentences containing popular opinions from input reviews. Then, we use a pretrained LLM to generate a readable summary that is grounded in these extracted evidential clusters. The modularity of our approach allows us to evaluate its efficacy at each stage. We show that HIRO learns an encoding space that is more semantically structured than prior work, and generates summaries that are more representative of the opinions in the input reviews. Human evaluation confirms that HIRO generates more coherent, detailed and accur
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#25506;&#32034;LLMs&#22312;&#26032;&#38395;&#26631;&#39064;&#26377;&#38024;&#23545;&#24615;&#24773;&#24863;&#20998;&#26512;&#20013;&#19981;&#21516;&#32423;&#21035;&#25552;&#31034;&#35268;&#33539;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00418</link><description>&lt;p&gt;
LLMs&#29992;&#20110;&#26032;&#38395;&#26631;&#39064;&#30340;&#26377;&#38024;&#23545;&#24615;&#24773;&#24863;&#20998;&#26512;&#65306;&#25506;&#32034;&#19981;&#21516;&#32423;&#21035;&#30340;&#25552;&#31034;&#35268;&#33539;&#21270;
&lt;/p&gt;
&lt;p&gt;
LLMs for Targeted Sentiment in News Headlines: Exploring Different Levels of Prompt Prescriptiveness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00418
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#25506;&#32034;LLMs&#22312;&#26032;&#38395;&#26631;&#39064;&#26377;&#38024;&#23545;&#24615;&#24773;&#24863;&#20998;&#26512;&#20013;&#19981;&#21516;&#32423;&#21035;&#25552;&#31034;&#35268;&#33539;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#26631;&#39064;&#24120;&#24120;&#36890;&#36807;&#26377;&#24847;&#35782;&#22320;&#20197;&#29305;&#23450;&#26041;&#24335;&#25551;&#32472;&#23454;&#20307;&#26469;&#24341;&#21457;&#24773;&#24863;&#65292;&#36825;&#20351;&#24471;&#26032;&#38395;&#26631;&#39064;&#30340;&#26377;&#38024;&#23545;&#24615;&#24773;&#24863;&#20998;&#26512;(TSA)&#25104;&#20026;&#19968;&#39033;&#20540;&#24471;&#20570;&#20294;&#20855;&#26377;&#25361;&#25112;&#30340;&#20219;&#21153;&#12290;&#24494;&#35843;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#23637;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;TSA&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#30340;&#32972;&#26223;&#30693;&#35782;&#26377;&#38480;&#65292;&#38656;&#35201;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;LLMs&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#35821;&#35328;&#21644;&#19990;&#30028;&#30693;&#35782;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20026;TSA&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#25552;&#31034;&#35774;&#35745;&#30340;&#24433;&#21709;&#24456;&#22823;&#12290;&#36890;&#36807;&#19982;&#20027;&#35266;&#20219;&#21153;&#30340;&#27880;&#37322;&#33539;&#24335;&#36827;&#34892;&#31867;&#27604;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25552;&#31034;&#35774;&#35745;&#23545;LLMs&#22312;&#26032;&#38395;&#26631;&#39064;TSA&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20351;&#29992;&#19981;&#21516;&#32423;&#21035;&#30340;&#35268;&#33539;&#25552;&#31034;&#65288;&#20174;&#32431;&#31929;&#30340;&#38646;&#26679;&#26412;&#21040;&#31526;&#21512;&#27880;&#37322;&#25351;&#21335;&#30340;&#31934;&#24515;&#20934;&#22791;&#30340;&#23569;&#26679;&#26412;&#25552;&#31034;&#65289;&#30340;&#26368;&#20808;&#36827;LLMs&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#35748;&#35782;&#21040;TSA&#30340;&#20027;&#35266;&#24615;&#36136;&#65292;&#25105;&#20204;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00418v1 Announce Type: new  Abstract: News headlines often evoke sentiment by intentionally portraying entities in particular ways, making targeted sentiment analysis (TSA) of headlines a worthwhile but difficult task. Fine-tuned encoder models show satisfactory TSA performance, but their background knowledge is limited, and they require a labeled dataset. LLMs offer a potentially universal solution for TSA due to their broad linguistic and world knowledge along with in-context learning abilities, yet their performance is heavily influenced by prompt design. Drawing parallels with annotation paradigms for subjective tasks, we explore the influence of prompt design on the performance of LLMs for TSA of news headlines. We evaluate the predictive accuracy of state-of-the-art LLMs using prompts with different levels of prescriptiveness, ranging from plain zero-shot to elaborate few-shot prompts matching annotation guidelines. Recognizing the subjective nature of TSA, we evaluate
&lt;/p&gt;</description></item><item><title>&#26631;&#35760;&#21270;&#26174;&#33879;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26412;&#30740;&#31350;&#20174;&#21333;&#35789;&#32423;&#21035;&#21040;&#23376;&#35789;&#32423;&#21035;&#36861;&#28335;&#20102;&#26631;&#35760;&#21270;&#22120;&#30340;&#28436;&#21464;&#24182;&#25552;&#20986;&#20102;&#20174;&#35748;&#30693;&#31185;&#23398;&#20013;&#30340;&#8220;&#26368;&#23567;&#21162;&#21147;&#21407;&#21017;&#8221;&#33719;&#24471;&#21551;&#31034;&#65292;&#21161;&#21147;&#26631;&#35760;&#21270;&#22120;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.00417</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#26631;&#35760;&#21270;&#65306;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25171;&#36896;&#26356;&#22909;&#30340;&#26631;&#35760;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Rethinking Tokenization: Crafting Better Tokenizers for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00417
&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#21270;&#26174;&#33879;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26412;&#30740;&#31350;&#20174;&#21333;&#35789;&#32423;&#21035;&#21040;&#23376;&#35789;&#32423;&#21035;&#36861;&#28335;&#20102;&#26631;&#35760;&#21270;&#22120;&#30340;&#28436;&#21464;&#24182;&#25552;&#20986;&#20102;&#20174;&#35748;&#30693;&#31185;&#23398;&#20013;&#30340;&#8220;&#26368;&#23567;&#21162;&#21147;&#21407;&#21017;&#8221;&#33719;&#24471;&#21551;&#31034;&#65292;&#21161;&#21147;&#26631;&#35760;&#21270;&#22120;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#21270;&#26174;&#33879;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#36861;&#28335;&#20102;&#26631;&#35760;&#21270;&#22120;&#20174;&#21333;&#35789;&#32423;&#21035;&#21040;&#23376;&#35789;&#32423;&#21035;&#30340;&#28436;&#21464;&#65292;&#20998;&#26512;&#23427;&#20204;&#22914;&#20309;&#24179;&#34913;&#26631;&#35760;&#21644;&#31867;&#22411;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#21516;&#26102;&#25511;&#21046;&#22797;&#26434;&#24615;&#12290;&#23613;&#31649;&#20687;&#23383;&#33410;&#23545;&#32534;&#30721;(BPE)&#36825;&#26679;&#30340;&#23376;&#35789;&#26631;&#35760;&#22120;&#20811;&#26381;&#20102;&#35768;&#22810;&#21333;&#35789;&#26631;&#35760;&#22120;&#30340;&#23616;&#38480;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#38750;&#25289;&#19969;&#35821;&#35328;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#25484;&#25569;&#22810;&#35789;&#34920;&#36798;&#30340;&#24494;&#22937;&#20043;&#22788;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#26631;&#35760;&#21270;&#22120;&#19981;&#20165;&#20165;&#26159;&#25216;&#26415;&#24037;&#20855;&#65292;&#36824;&#24212;&#35813;&#20174;&#20851;&#20110;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#30340;&#35748;&#30693;&#31185;&#23398;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#35813;&#30740;&#31350;&#38543;&#21518;&#20171;&#32461;&#20102;&#35748;&#30693;&#31185;&#23398;&#20013;&#30340;&#8220;&#26368;&#23567;&#21162;&#21147;&#21407;&#21017;&#8221;&#65292;&#21363;&#20154;&#31867;&#33258;&#28982;&#23547;&#27714;&#20943;&#23569;&#35748;&#30693;&#21162;&#21147;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#21407;&#21017;&#23545;&#26631;&#35760;&#22120;&#21457;&#23637;&#30340;&#30410;&#22788;&#12290;&#22522;&#20110;&#36825;&#19968;&#21407;&#21017;&#65292;&#26412;&#25991;&#25552;&#20986;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00417v1 Announce Type: new  Abstract: Tokenization significantly influences language models(LMs)' performance. This paper traces the evolution of tokenizers from word-level to subword-level, analyzing how they balance tokens and types to enhance model adaptability while controlling complexity. Despite subword tokenizers like Byte Pair Encoding (BPE) overcoming many word tokenizer limitations, they encounter difficulties in handling non-Latin languages and depend heavily on extensive training data and computational resources to grasp the nuances of multiword expressions (MWEs). This article argues that tokenizers, more than mere technical tools, should drawing inspiration from the cognitive science about human language processing. This study then introduces the "Principle of Least Effort" from cognitive science, that humans naturally seek to reduce cognitive effort, and discusses the benefits of this principle for tokenizer development. Based on this principle, the paper prop
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FCTR&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#33521;&#35821;&#20197;&#22806;&#35821;&#35328;&#65292;&#23588;&#20854;&#26159;&#22303;&#32819;&#20854;&#35821;&#65292;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#25968;&#25454;&#38598;&#28508;&#21147;&#25512;&#21160;&#22303;&#32819;&#20854;&#35821;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.00411</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#23398;&#20064;&#19982;&#20302;&#36164;&#28304;&#24494;&#35843;&#65306;&#20197;&#22303;&#32819;&#20854;&#20107;&#23454;&#26816;&#26597;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with Fact-Checking in Turkish
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00411
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FCTR&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#33521;&#35821;&#20197;&#22806;&#35821;&#35328;&#65292;&#23588;&#20854;&#26159;&#22303;&#32819;&#20854;&#35821;&#65292;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#25968;&#25454;&#38598;&#28508;&#21147;&#25512;&#21160;&#22303;&#32819;&#20854;&#35821;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#36805;&#36895;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20854;&#23545;&#20844;&#20247;&#33286;&#35770;&#30340;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#38169;&#35823;&#20449;&#24687;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#35813;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#12290;&#22240;&#27492;&#65292;&#20854;&#20182;&#35821;&#35328;&#65292;&#21253;&#25324;&#22303;&#32819;&#20854;&#35821;&#65292;&#30340;&#25968;&#25454;&#38598;&#31232;&#32570;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;3238&#20010;&#30495;&#23454;&#22768;&#26126;&#32452;&#25104;&#30340;FCTR&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#65292;&#24182;&#25972;&#21512;&#20102;&#19977;&#23478;&#22303;&#32819;&#20854;&#20107;&#23454;&#26816;&#26597;&#32452;&#32455;&#25910;&#38598;&#30340;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#22303;&#32819;&#20854;&#35821;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;&#38646;&#27425;&#21644;&#23569;&#27425;&#65289;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25968;&#25454;&#38598;&#26377;&#25512;&#21160;&#22303;&#32819;&#20854;&#35821;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00411v1 Announce Type: new  Abstract: The rapid spread of misinformation through social media platforms has raised concerns regarding its impact on public opinion. While misinformation is prevalent in other languages, the majority of research in this field has concentrated on the English language. Hence, there is a scarcity of datasets for other languages, including Turkish. To address this concern, we have introduced the FCTR dataset, consisting of 3238 real-world claims. This dataset spans multiple domains and incorporates evidence collected from three Turkish fact-checking organizations. Additionally, we aim to assess the effectiveness of cross-lingual transfer learning for low-resource languages, with a particular focus on Turkish. We demonstrate in-context learning (zero-shot and few-shot) performance of large language models in this context. The experimental results indicate that the dataset has the potential to advance research in the Turkish language.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#38543;&#26426;&#20559;&#22909;&#32763;&#36716;&#30340;&#31574;&#30053;&#20248;&#21270;&#36890;&#29992;&#26694;&#26550;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#23384;&#22312;&#22024;&#26434;&#21453;&#39304;&#26102;&#30340;DPO&#31639;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#20154;&#31867;&#20852;&#36259;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.00409</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;DPO: &#29992;&#26377;&#22122;&#21453;&#39304;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Provably Robust DPO: Aligning Language Models with Noisy Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00409
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#38543;&#26426;&#20559;&#22909;&#32763;&#36716;&#30340;&#31574;&#30053;&#20248;&#21270;&#36890;&#29992;&#26694;&#26550;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#23384;&#22312;&#22024;&#26434;&#21453;&#39304;&#26102;&#30340;DPO&#31639;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#20154;&#31867;&#20852;&#36259;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20174;&#22522;&#20110;&#21916;&#22909;&#21453;&#39304;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#19982;&#20154;&#31867;&#20852;&#36259;&#23545;&#40784;&#30340;&#26377;&#21069;&#26223;&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#40784;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23545;&#39640;&#36136;&#37327;&#20154;&#31867;&#21916;&#22909;&#25968;&#25454;&#30340;&#20381;&#36182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26500;&#25104;&#20102;&#29942;&#39048;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25968;&#25454;&#38598;&#20013;&#26377;&#22122;&#65288;&#19981;&#27491;&#30830;&#21644;&#27169;&#31946;&#65289;&#30340;&#20559;&#22909;&#23545;&#21487;&#33021;&#20250;&#38480;&#21046;&#35821;&#35328;&#27169;&#22411;&#20934;&#30830;&#25429;&#25417;&#20154;&#31867;&#24847;&#22270;&#12290;&#34429;&#28982;&#20174;&#19994;&#32773;&#26368;&#36817;&#25552;&#20986;&#20102;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#20943;&#36731;&#22122;&#22768;&#20559;&#22909;&#30340;&#24433;&#21709;&#65292;&#20294;&#23545;&#23427;&#20204;&#30340;&#24037;&#20316;&#23436;&#25972;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#38754;&#21521;&#22312;&#38543;&#26426;&#20559;&#22909;&#32763;&#36716;&#23384;&#22312;&#30340;&#31574;&#30053;&#20248;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#31639;&#27861;&#65292;&#22240;&#20026;&#23427;&#20551;&#35774;&#20559;&#22909;&#36981;&#24490; Bradley-Te
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00409v1 Announce Type: cross  Abstract: Learning from preference-based feedback has recently gained traction as a promising approach to align language models with human interests. While these aligned generative models have demonstrated impressive capabilities across various tasks, their dependence on high-quality human preference data poses a bottleneck in practical applications. Specifically, noisy (incorrect and ambiguous) preference pairs in the dataset might restrict the language models from capturing human intent accurately. While practitioners have recently proposed heuristics to mitigate the effect of noisy preferences, a complete theoretical understanding of their workings remain elusive.   In this work, we aim to bridge this gap by by introducing a general framework for policy optimization in the presence of random preference flips. We focus on the direct preference optimization (DPO) algorithm in particular since it assumes that preferences adhere to the Bradley-Te
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31169;&#20154;&#22522;&#20934;&#35774;&#23450;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#27979;&#35797;&#25968;&#25454;&#30340;&#31169;&#23494;&#24615;&#65292;&#20351;&#27169;&#22411;&#22312;&#19981;&#25581;&#38706;&#27979;&#35797;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22522;&#20934;&#35774;&#23450;&#20013;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00393</link><description>&lt;p&gt;
&#38450;&#27490;&#27745;&#26579;&#21644;&#25552;&#39640;LLM&#27604;&#36739;&#35780;&#20272;&#30340;&#31169;&#20154;&#22522;&#20934;&#35774;&#23450;
&lt;/p&gt;
&lt;p&gt;
Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31169;&#20154;&#22522;&#20934;&#35774;&#23450;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#27979;&#35797;&#25968;&#25454;&#30340;&#31169;&#23494;&#24615;&#65292;&#20351;&#27169;&#22411;&#22312;&#19981;&#25581;&#38706;&#27979;&#35797;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22522;&#20934;&#35774;&#23450;&#20013;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20934;&#35774;&#23450;&#26159;&#35780;&#20272;LLM&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#22240;&#20026;&#23427;&#36895;&#24230;&#24555;&#12289;&#21487;&#22797;&#21046;&#19988;&#25104;&#26412;&#20302;&#24265;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#20170;&#22825;&#22823;&#22810;&#25968;&#24320;&#28304;&#22522;&#20934;&#35774;&#23450;&#24050;&#32463;&#34987;&#27745;&#26579;&#25110;&#27844;&#38706;&#21040;LLM&#20013;&#65292;&#36825;&#24847;&#21619;&#30528;LLM&#22312;&#39044;&#35757;&#32451;&#21644;/&#25110;&#24494;&#35843;&#26399;&#38388;&#21487;&#20197;&#35775;&#38382;&#27979;&#35797;&#25968;&#25454;&#12290;&#36825;&#23545;&#36804;&#20170;&#20026;&#27490;&#36827;&#34892;&#30340;&#22522;&#20934;&#30740;&#31350;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#26410;&#26469;&#20351;&#29992;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#25552;&#20986;&#20102;&#20005;&#37325;&#20851;&#20999;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Private Benchmarking&#65292;&#36825;&#26159;&#19968;&#20010;&#26041;&#26696;&#65292;&#20854;&#20013;&#27979;&#35797;&#25968;&#25454;&#38598;&#20445;&#25345;&#31169;&#23494;&#65292;&#27169;&#22411;&#22312;&#19981;&#21521;&#27169;&#22411;&#36879;&#38706;&#27979;&#35797;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#21508;&#31181;&#22330;&#26223;&#65288;&#21462;&#20915;&#20110;&#23545;&#27169;&#22411;&#25152;&#26377;&#32773;&#25110;&#25968;&#25454;&#38598;&#25152;&#26377;&#32773;&#30340;&#20449;&#20219;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#31169;&#20154;&#22522;&#20934;&#35774;&#23450;&#36991;&#20813;&#25968;&#25454;&#27745;&#26579;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#38656;&#35201;&#20445;&#25252;&#27169;&#22411;&#26435;&#37325;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#26469;&#33258;&#26426;&#23494;&#35745;&#31639;&#21644;&#23494;&#30721;&#23398;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00393v1 Announce Type: cross  Abstract: Benchmarking is the de-facto standard for evaluating LLMs, due to its speed, replicability and low cost. However, recent work has pointed out that the majority of the open source benchmarks available today have been contaminated or leaked into LLMs, meaning that LLMs have access to test data during pretraining and/or fine-tuning. This raises serious concerns about the validity of benchmarking studies conducted so far and the future of evaluation using benchmarks. To solve this problem, we propose Private Benchmarking, a solution where test datasets are kept private and models are evaluated without revealing the test data to the model. We describe various scenarios (depending on the trust placed on model owners or dataset owners), and present solutions to avoid data contamination using private benchmarking. For scenarios where the model weights need to be kept private, we describe solutions from confidential computing and cryptography t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#35299;&#30721;&#22120;&#20559;&#32622;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#21253;&#21547;&#22823;&#37327;&#39046;&#22495;&#29305;&#23450;&#31232;&#26377;&#21333;&#35789;&#30340;&#24773;&#20917;&#65292;&#21516;&#26102;&#25512;&#20986;&#20102; MED-IT &#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#23398;&#26415;&#30028;&#32570;&#20047;&#30693;&#35782;&#23494;&#38598;&#22411;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00370</link><description>&lt;p&gt;
&#21518;&#35299;&#30721;&#22120;&#20559;&#32622;&#29992;&#20110;&#31471;&#21040;&#31471;&#22810;&#36718;&#21307;&#23398;&#35775;&#35848;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Post-decoder Biasing for End-to-End Speech Recognition of Multi-turn Medical Interview
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00370
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#35299;&#30721;&#22120;&#20559;&#32622;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#21253;&#21547;&#22823;&#37327;&#39046;&#22495;&#29305;&#23450;&#31232;&#26377;&#21333;&#35789;&#30340;&#24773;&#20917;&#65292;&#21516;&#26102;&#25512;&#20986;&#20102; MED-IT &#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#23398;&#26415;&#30028;&#32570;&#20047;&#30693;&#35782;&#23494;&#38598;&#22411;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#26041;&#27861;&#27491;&#36880;&#28176;&#21462;&#20195;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;E2E&#27169;&#22411;&#30340;&#20248;&#21270;&#32570;&#20047;&#22788;&#29702;&#35299;&#30721;&#20559;&#31227;&#30340;&#30452;&#35266;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#22823;&#37327;&#39046;&#22495;&#29305;&#23450;&#31232;&#26377;&#21333;&#35789;&#19988;&#20855;&#26377;&#29305;&#23450;&#37325;&#35201;&#21547;&#20041;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#23398;&#26415;&#30028;&#32570;&#20047;&#30693;&#35782;&#23494;&#38598;&#22411;&#35821;&#38899;&#25968;&#25454;&#38598;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38480;&#21046;&#22240;&#32032;&#65292;&#36890;&#24120;&#20351;&#29992;&#30340;&#35821;&#38899;&#35821;&#26009;&#24211;&#19982;&#29616;&#23454;&#23545;&#35805;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Medical Interview&#65288;MED-IT&#65289;&#65292;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#30693;&#35782;&#23494;&#38598;&#22411;&#21629;&#21517;&#23454;&#20307;&#30340;&#22810;&#36718;&#20250;&#35786;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#22686;&#24378;E2E&#27169;&#22411;&#23545;&#31232;&#26377;&#21333;&#35789;&#35782;&#21035;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#21518;&#35299;&#30721;&#22120;&#20559;&#32622;&#65292;&#23427;&#22522;&#20110;&#35757;&#32451;&#36716;&#24405;&#30340;&#20998;&#24067;&#26500;&#24314;&#20102;&#19968;&#20010;&#36716;&#25442;&#27010;&#29575;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00370v1 Announce Type: new  Abstract: End-to-end (E2E) approach is gradually replacing hybrid models for automatic speech recognition (ASR) tasks. However, the optimization of E2E models lacks an intuitive method for handling decoding shifts, especially in scenarios with a large number of domain-specific rare words that hold specific important meanings. Furthermore, the absence of knowledge-intensive speech datasets in academia has been a significant limiting factor, and the commonly used speech corpora exhibit significant disparities with realistic conversation. To address these challenges, we present Medical Interview (MED-IT), a multi-turn consultation speech dataset that contains a substantial number of knowledge-intensive named entities. We also explore methods to enhance the recognition performance of rare words for E2E models. We propose a novel approach, post-decoder biasing, which constructs a transform probability matrix based on the distribution of training transc
&lt;/p&gt;</description></item><item><title>SCRAP&#37319;&#29992;&#33258;&#27965;&#25512;&#29702;&#21644;&#25552;&#21462;-&#20998;&#37197;&#31574;&#30053;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#21644;&#27491;&#30830;&#39044;&#27979;&#22235;&#20803;&#32452;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;ASQP&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00354</link><description>&lt;p&gt;
&#33258;&#27965;&#25512;&#29702;&#22522;&#20110;&#25552;&#21462;-&#20998;&#37197;&#31574;&#30053;&#30340;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Consistent Reasoning-based Aspect-Sentiment Quad Prediction with Extract-Then-Assign Strategy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00354
&lt;/p&gt;
&lt;p&gt;
SCRAP&#37319;&#29992;&#33258;&#27965;&#25512;&#29702;&#21644;&#25552;&#21462;-&#20998;&#37197;&#31574;&#30053;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#21644;&#27491;&#30830;&#39044;&#27979;&#22235;&#20803;&#32452;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;ASQP&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;&#65288;ASQP&#65289;&#20219;&#21153;&#20013;&#65292;&#29992;&#20110;&#39044;&#27979;&#24773;&#24863;&#22235;&#20803;&#30340;&#29983;&#25104;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#19981;&#31934;&#30830;&#39044;&#27979;&#21644;&#26377;&#38480;&#21487;&#35299;&#37322;&#24615;&#30340;&#22256;&#25200;&#65292;&#36825;&#26159;&#30001;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#22235;&#20803;&#32452;&#21512;&#25104;&#36807;&#31243;&#24314;&#27169;&#19981;&#36275;&#24341;&#36215;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#27965;&#25512;&#29702;-&#22522;&#20110;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;&#65288;SCRAP&#65289;&#65292;&#20248;&#21270;&#20854;&#27169;&#22411;&#20197;&#25353;&#39034;&#24207;&#29983;&#25104;&#25512;&#29702;&#21644;&#30456;&#24212;&#30340;&#24773;&#24863;&#22235;&#20803;&#12290; SCRAP&#37319;&#29992;&#25552;&#21462;-&#28982;&#21518;-&#20998;&#37197;&#25512;&#29702;&#31574;&#30053;&#65292;&#32039;&#23494;&#27169;&#20223;&#20154;&#31867;&#35748;&#30693;&#12290;&#26368;&#21518;&#65292;SCRAP&#36890;&#36807;&#19968;&#33268;&#24615;&#25237;&#31080;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20197;&#21450;&#36890;&#36807;&#19968;&#33268;&#24615;&#25237;&#31080;&#27491;&#30830;&#39044;&#27979;&#22235;&#20803;&#32452;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;ASQP&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00354v1 Announce Type: new  Abstract: In the task of aspect sentiment quad prediction (ASQP), generative methods for predicting sentiment quads have shown promising results. However, they still suffer from imprecise predictions and limited interpretability, caused by data scarcity and inadequate modeling of the quadruplet composition process. In this paper, we propose Self-Consistent Reasoning-based Aspect-sentiment quadruple Prediction (SCRAP), optimizing its model to generate reasonings and the corresponding sentiment quadruplets in sequence. SCRAP adopts the Extract-Then-Assign reasoning strategy, which closely mimics human cognition. In the end, SCRAP significantly improves the model's ability to handle complex reasoning tasks and correctly predict quadruplets through consistency voting, resulting in enhanced interpretability and accuracy in ASQP.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Semi-Instruct&#30340;&#26041;&#27861;&#65292;&#26725;&#25509;&#20102;&#33258;&#28982;&#25351;&#23548;&#21644;&#33258;&#25105;&#25351;&#23548;&#20004;&#31181;&#33539;&#24335;&#65292;&#33021;&#22815;&#23558;&#33258;&#28982;&#25351;&#23548;&#20013;&#30340;&#22810;&#26679;&#20294;&#19981;&#24403;&#30340;&#20195;&#30721;&#36716;&#25442;&#20026;&#36866;&#24403;&#30340;&#25351;&#20196;-&#20195;&#30721;&#37197;&#23545;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27979;&#35797;&#29992;&#20363;&#26500;&#24314;&#26041;&#27861;&#20197;&#39564;&#35777;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;</title><link>https://arxiv.org/abs/2403.00338</link><description>&lt;p&gt;
Semi-Instruct: &#26725;&#25509;&#33258;&#28982;&#25351;&#23548;&#19982;&#33258;&#25105;&#25351;&#23548;&#20197;&#24212;&#29992;&#20110;&#22823;&#22411;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Semi-Instruct: Bridging Natural-Instruct and Self-Instruct for Code Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00338
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Semi-Instruct&#30340;&#26041;&#27861;&#65292;&#26725;&#25509;&#20102;&#33258;&#28982;&#25351;&#23548;&#21644;&#33258;&#25105;&#25351;&#23548;&#20004;&#31181;&#33539;&#24335;&#65292;&#33021;&#22815;&#23558;&#33258;&#28982;&#25351;&#23548;&#20013;&#30340;&#22810;&#26679;&#20294;&#19981;&#24403;&#30340;&#20195;&#30721;&#36716;&#25442;&#20026;&#36866;&#24403;&#30340;&#25351;&#20196;-&#20195;&#30721;&#37197;&#23545;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27979;&#35797;&#29992;&#20363;&#26500;&#24314;&#26041;&#27861;&#20197;&#39564;&#35777;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00338v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#25351;&#23548;&#35843;&#25972;&#22312;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Code LLMs&#65289;&#20013;&#30340;&#31243;&#24207;&#21512;&#25104;&#20219;&#21153;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30446;&#21069;&#65292;&#25910;&#38598;&#35843;&#25972;&#25968;&#25454;&#30340;&#20004;&#31181;&#20027;&#35201;&#33539;&#24335;&#26159;&#33258;&#28982;&#25351;&#20196;&#65288;&#20154;&#24037;&#32534;&#20889;&#65289;&#21644;&#33258;&#25105;&#25351;&#20196;&#65288;&#33258;&#21160;&#29983;&#25104;&#65289;&#12290;&#33258;&#28982;&#25351;&#20196;&#21253;&#21547;&#22810;&#26679;&#19988;&#27491;&#30830;&#30340;&#20195;&#30721;&#65292;&#20294;&#32570;&#20047;&#25351;&#20196;-&#20195;&#30721;&#37197;&#23545;&#65292;&#24182;&#23384;&#22312;&#19981;&#24403;&#30340;&#23884;&#22871;&#21333;&#34892;&#20195;&#30721;&#26684;&#24335;&#12290;&#30456;&#21453;&#65292;&#33258;&#25105;&#25351;&#20196;&#33258;&#21160;&#29983;&#25104;&#36866;&#24403;&#37197;&#23545;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29983;&#25104;&#37325;&#22797;&#20869;&#23481;&#65292;&#23384;&#22312;&#22810;&#26679;&#24615;&#20302;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#26080;&#27861;&#30830;&#20445;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;&#20026;&#20102;&#26725;&#25509;&#36825;&#20004;&#20010;&#33539;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Semi-Instruct&#12290;&#23427;&#39318;&#20808;&#36890;&#36807;&#31867;&#20284;&#20110;&#33258;&#25105;&#25351;&#20196;&#30340;&#26041;&#27861;&#65292;&#23558;&#33258;&#28982;&#25351;&#20196;&#20013;&#30340;&#22810;&#26679;&#20294;&#19981;&#24403;&#30340;&#20195;&#30721;&#36716;&#25442;&#20026;&#36866;&#24403;&#30340;&#25351;&#20196;-&#20195;&#30721;&#37197;&#23545;&#12290;&#20026;&#20102;&#39564;&#35777;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27979;&#35797;&#29992;&#20363;&#26500;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#29992;&#20363;&#36755;&#20837;&#24182;&#25191;&#34892;&#27491;&#30830;&#20195;&#30721;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00338v1 Announce Type: new  Abstract: Instruction tuning plays a pivotal role in Code Large Language Models (Code LLMs) for the task of program synthesis. Presently, two dominant paradigms for collecting tuning data are natural-instruct (human-written) and self-instruct (automatically generated). Natural-instruct includes diverse and correct codes but lacks instruction-code pairs, and exists improper code formats like nested single-line codes. In contrast, self-instruct automatically generates proper paired data. However, it suffers from low diversity due to generating duplicates and cannot ensure the correctness of codes. To bridge the both paradigms, we propose \textbf{Semi-Instruct}. It first converts diverse but improper codes from natural-instruct into proper instruction-code pairs through a method similar to self-instruct. To verify the correctness of generated codes, we design a novel way to construct test cases by generating cases' inputs and executing correct codes 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Auto-regressive Selective Replacement Ascent (ASRA)&#31639;&#27861;&#65292;&#25105;&#20204;&#25104;&#21151;&#24341;&#23548;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#27602;&#20869;&#23481;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.00292</link><description>&lt;p&gt;
&#22522;&#20110;DPP&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#25628;&#32034;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DPP-Based Adversarial Prompt Searching for Lanugage Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00292
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Auto-regressive Selective Replacement Ascent (ASRA)&#31639;&#27861;&#65292;&#25105;&#20204;&#25104;&#21151;&#24341;&#23548;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#27602;&#20869;&#23481;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#29983;&#25104;&#27627;&#26080;&#24847;&#20041;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#30340;&#39118;&#38505;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#20204;&#30340;&#23433;&#20840;&#37096;&#32626;&#12290;&#22240;&#27492;&#65292;&#22312;&#37096;&#32626;&#20043;&#21069;&#21457;&#29616;&#24182;&#20462;&#25913;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#28508;&#22312;&#30340;&#26377;&#27602;&#36755;&#20986;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#25628;&#32034;&#25552;&#31034;&#26469;&#24341;&#23548;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#30446;&#26631;&#36755;&#20986;&#30340;&#26377;&#27602;&#20869;&#23481;&#12290;&#35813;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25991;&#26412;&#25968;&#25454;&#30340;&#31163;&#25955;&#24615;&#20197;&#21450;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#25152;&#38656;&#30340;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#22238;&#24402;&#36873;&#25321;&#26367;&#20195;&#19978;&#21319;&#65288;ASRA&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#30340;&#36873;&#25321;&#25552;&#31034;&#30340;&#31163;&#25955;&#20248;&#21270;&#31639;&#27861;&#12290;&#23545;&#20845;&#31181;&#19981;&#21516;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ASRA&#23545;&#24341;&#21457;&#26377;&#27602;&#20869;&#23481;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00292v1 Announce Type: new  Abstract: Language models risk generating mindless and offensive content, which hinders their safe deployment. Therefore, it is crucial to discover and modify potential toxic outputs of pre-trained language models before deployment. In this work, we elicit toxic content by automatically searching for a prompt that directs pre-trained language models towards the generation of a specific target output. The problem is challenging due to the discrete nature of textual data and the considerable computational resources required for a single forward pass of the language model. To combat these challenges, we introduce Auto-regressive Selective Replacement Ascent (ASRA), a discrete optimization algorithm that selects prompts based on both quality and similarity with determinantal point process (DPP). Experimental results on six different pre-trained language models demonstrate the efficacy of ASRA for eliciting toxic content. Furthermore, our analysis reve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19981;&#21516;&#35821;&#31181;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#36755;&#20986;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20351;&#29992;&#20102;&#19977;&#31181;&#27979;&#37327;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.00277</link><description>&lt;p&gt;
&#22810;&#35821;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Gender Bias in Large Language Models across Multiple Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19981;&#21516;&#35821;&#31181;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#36755;&#20986;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20351;&#29992;&#20102;&#19977;&#31181;&#27979;&#37327;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#19981;&#26029;&#37096;&#32626;&#65292;&#35780;&#20272;LLMs&#20013;&#23884;&#20837;&#30340;&#24615;&#21035;&#20559;&#35265;&#30340;&#24433;&#21709;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#20027;&#39064;&#24050;&#32463;&#21463;&#21040;&#30456;&#24403;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#33521;&#35821;&#29615;&#22659;&#19979;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#30340;&#35843;&#26597;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#19988;&#20998;&#26512;&#19981;&#36275;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#19981;&#21516;&#35821;&#35328;&#30340;LLMs&#29983;&#25104;&#30340;&#36755;&#20986;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#27979;&#37327;&#26041;&#27861;&#65306;1&#65289;&#22312;&#32473;&#23450;&#24615;&#21035;&#30456;&#20851;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#25551;&#36848;&#24615;&#35789;&#27719;&#26102;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;2&#65289;&#22312;&#32473;&#23450;&#25551;&#36848;&#24615;&#35789;&#27719;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#24615;&#21035;&#30456;&#20851;&#20195;&#35789;&#65288;&#22905;/&#20182;&#65289;&#26102;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;3&#65289;&#22312;LLM&#29983;&#25104;&#30340;&#23545;&#35805;&#20027;&#39064;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#19977;&#31181;&#27979;&#37327;&#26041;&#27861;&#30740;&#31350;&#20102;&#21508;&#35821;&#31181;&#20013;GPT&#31995;&#21015;LLMs&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00277v1 Announce Type: new  Abstract: With the growing deployment of large language models (LLMs) across various applications, assessing the influence of gender biases embedded in LLMs becomes crucial. The topic of gender bias within the realm of natural language processing (NLP) has gained considerable focus, particularly in the context of English. Nonetheless, the investigation of gender bias in languages other than English is still relatively under-explored and insufficiently analyzed. In this work, We examine gender bias in LLMs-generated outputs for different languages. We use three measurements: 1) gender bias in selecting descriptive words given the gender-related context. 2) gender bias in selecting gender-related pronouns (she/he) given the descriptive words. 3) gender bias in the topics of LLM-generated dialogues. We investigate the outputs of the GPT series of LLMs in various languages using our three measurement methods. Our findings revealed significant gender b
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#20840;&#25991;&#26448;&#26009;&#31185;&#23398;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#32858;&#21512;&#29289;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;&#65288;PNCs&#65289;&#26679;&#26412;&#21015;&#34920;&#65292;&#24341;&#20837;&#26032;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#25216;&#26415;&#65292;&#30740;&#31350;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#20197;&#21450;&#33258;&#19968;&#33268;&#24615;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#20808;&#36827;&#30340;LLMs&#20063;&#38590;&#20197;&#23436;&#20840;&#25552;&#21462;&#25152;&#26377;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.00260</link><description>&lt;p&gt;
&#20174;&#20840;&#25991;&#26723;&#26696;&#20013;&#25552;&#21462;&#32858;&#21512;&#29289;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Extracting Polymer Nanocomposite Samples from Full-Length Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00260
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#20840;&#25991;&#26448;&#26009;&#31185;&#23398;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#32858;&#21512;&#29289;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;&#65288;PNCs&#65289;&#26679;&#26412;&#21015;&#34920;&#65292;&#24341;&#20837;&#26032;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#25216;&#26415;&#65292;&#30740;&#31350;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#20197;&#21450;&#33258;&#19968;&#33268;&#24615;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#20808;&#36827;&#30340;LLMs&#20063;&#38590;&#20197;&#23436;&#20840;&#25552;&#21462;&#25152;&#26377;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#20840;&#25991;&#26448;&#26009;&#31185;&#23398;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#32858;&#21512;&#29289;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;&#65288;PNCs&#65289;&#26679;&#26412;&#21015;&#34920;&#30340;&#26041;&#27861;&#12290;&#25361;&#25112;&#22312;&#20110;PNC&#26679;&#26412;&#30340;&#22797;&#26434;&#24615;&#65292;&#20854;&#23646;&#24615;&#25955;&#24067;&#22312;&#25991;&#26412;&#20013;&#12290;&#26631;&#35760;PNCs&#35814;&#32454;&#20449;&#24687;&#30340;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#20351;&#24471;&#30001;&#20110;&#38590;&#20197;&#21019;&#24314;&#20840;&#38754;&#30340;&#21629;&#21517;&#23454;&#20307;&#36328;&#24230;&#27880;&#37322;&#65292;&#20256;&#32479;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#25216;&#26415;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#25216;&#26415;&#65292;&#24182;&#25506;&#35752;&#20102;&#38646;-shot&#26041;&#24335;&#19979;&#30340;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#23558;&#33258;&#19968;&#33268;&#24615;&#34701;&#20837;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#20808;&#36827;&#30340;LLMs&#20063;&#24456;&#38590;&#20174;&#25991;&#31456;&#20013;&#25552;&#21462;&#25152;&#26377;&#26679;&#26412;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#19968;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#19977;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00260v1 Announce Type: new  Abstract: This paper investigates the use of large language models (LLMs) for extracting sample lists of polymer nanocomposites (PNCs) from full-length materials science research papers. The challenge lies in the complex nature of PNC samples, which have numerous attributes scattered throughout the text. The complexity of annotating detailed information on PNCs limits the availability of data, making conventional document-level relation extraction techniques impractical due to the challenge in creating comprehensive named entity span annotations. To address this, we introduce a new benchmark and an evaluation technique for this task and explore different prompting strategies in a zero-shot manner. We also incorporate self-consistency to improve the performance. Our findings show that even advanced LLMs struggle to extract all of the samples from an article. Finally, we analyze the errors encountered in this process, categorizing them into three ma
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;EUROPA&#65292;&#21253;&#21547;&#25152;&#26377;24&#31181;&#27431;&#30431;&#23448;&#26041;&#35821;&#35328;&#65292;&#34920;&#26126;&#22312;&#29305;&#23450;&#39046;&#22495;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.00252</link><description>&lt;p&gt;
EUROPA&#65306;&#19968;&#20010;&#27861;&#24459;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
EUROPA: A Legal Multilingual Keyphrase Generation Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00252
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;EUROPA&#65292;&#21253;&#21547;&#25152;&#26377;24&#31181;&#27431;&#30431;&#23448;&#26041;&#35821;&#35328;&#65292;&#34920;&#26126;&#22312;&#29305;&#23450;&#39046;&#22495;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#29983;&#25104;&#20027;&#35201;&#22312;&#23398;&#26415;&#30740;&#31350;&#25991;&#31456;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#25506;&#32034;&#65292;&#29305;&#21035;&#20391;&#37325;&#20110;&#31185;&#23398;&#39046;&#22495;&#21644;&#33521;&#35821;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EUROPA&#65292;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290; &#23427;&#28304;&#33258;&#27431;&#27954;&#27861;&#38498;&#30340;&#27861;&#24459;&#21028;&#20915;&#65292;&#24182;&#21253;&#21547;&#20102;&#25152;&#26377;24&#31181;&#27431;&#30431;&#23448;&#26041;&#35821;&#35328;&#20013;&#30340;&#23454;&#20363;&#12290; &#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#19978;&#36816;&#34892;&#22810;&#35821;&#35328;&#27169;&#22411;&#24182;&#20998;&#26512;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#22312;&#20687;&#25105;&#20204;&#25552;&#20986;&#30340;&#29305;&#23450;&#39046;&#22495;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00252v1 Announce Type: cross  Abstract: Keyphrase generation has primarily been explored within the context of academic research articles, with a particular focus on scientific domains and the English language. In this work, we present EUROPA, a dataset for multilingual keyphrase generation in the legal domain. It is derived from legal judgments from the Court of Justice of the European Union (EU), and contains instances in all 24 EU official languages. We run multilingual models on our corpus and analyze the results, showing room for improvement on a domain-specific multilingual corpus such as the one we present.
&lt;/p&gt;</description></item><item><title>CASIMIR&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#20316;&#32773;&#32508;&#21512;&#20462;&#35746;&#30340;&#31185;&#23398;&#25991;&#31456;&#35821;&#26009;&#24211;&#65292;&#20197;&#20419;&#36827;&#23545;&#31185;&#23398;&#25991;&#31456;&#20889;&#20316;&#20462;&#35746;&#27493;&#39588;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.00241</link><description>&lt;p&gt;
CASIMIR&#65306;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#20316;&#32773;&#32508;&#21512;&#20462;&#35746;&#30340;&#31185;&#23398;&#25991;&#31456;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
CASIMIR: A Corpus of Scientific Articles enhanced with Multiple Author-Integrated Revisions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00241
&lt;/p&gt;
&lt;p&gt;
CASIMIR&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#20316;&#32773;&#32508;&#21512;&#20462;&#35746;&#30340;&#31185;&#23398;&#25991;&#31456;&#35821;&#26009;&#24211;&#65292;&#20197;&#20419;&#36827;&#23545;&#31185;&#23398;&#25991;&#31456;&#20889;&#20316;&#20462;&#35746;&#27493;&#39588;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#31185;&#23398;&#25991;&#31456;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#26159;&#19968;&#31181;&#39640;&#24230;&#35268;&#33539;&#21270;&#21644;&#20855;&#20307;&#30340;&#20307;&#35009;&#65292;&#22240;&#27492;&#31934;&#36890;&#20070;&#38754;&#20132;&#27969;&#23545;&#26377;&#25928;&#20256;&#36798;&#30740;&#31350;&#21457;&#29616;&#21644;&#35266;&#28857;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#31185;&#23398;&#25991;&#31456;&#20889;&#20316;&#36807;&#31243;&#20013;&#20462;&#35746;&#27493;&#39588;&#30340;&#21407;&#22987;&#25991;&#26412;&#36164;&#28304;&#12290;&#36825;&#20010;&#21517;&#20026;CASIMIR&#30340;&#26032;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;OpenReview&#30340;15,646&#31687;&#31185;&#23398;&#25991;&#31456;&#30340;&#22810;&#20010;&#20462;&#35746;&#29256;&#26412;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#21516;&#34892;&#35780;&#23457;&#12290;&#25991;&#31456;&#30340;&#36830;&#32493;&#29256;&#26412;&#23545;&#22312;&#21477;&#23376;&#32423;&#21035;&#23545;&#40784;&#65292;&#21516;&#26102;&#20445;&#30041;&#27573;&#33853;&#20301;&#32622;&#20449;&#24687;&#20316;&#20026;&#25903;&#25345;&#26410;&#26469;&#20462;&#35746;&#30740;&#31350;&#30340;&#20803;&#25968;&#25454;&#22312;&#35821;&#31687;&#32423;&#21035;&#12290;&#27599;&#19968;&#23545;&#20462;&#35746;&#21518;&#30340;&#21477;&#23376;&#37117;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#30340;&#32534;&#36753;&#21644;&#30456;&#20851;&#20462;&#35746;&#24847;&#22270;&#36827;&#34892;&#20102;&#20016;&#23500;&#12290;&#20026;&#20102;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#21021;&#22987;&#36136;&#37327;&#65292;&#25105;&#20204;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#20462;&#35746;&#26041;&#27861;&#36827;&#34892;&#20102;&#23450;&#24615;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00241v1 Announce Type: new  Abstract: Writing a scientific article is a challenging task as it is a highly codified and specific genre, consequently proficiency in written communication is essential for effectively conveying research findings and ideas. In this article, we propose an original textual resource on the revision step of the writing process of scientific articles. This new dataset, called CASIMIR, contains the multiple revised versions of 15,646 scientific articles from OpenReview, along with their peer reviews. Pairs of consecutive versions of an article are aligned at sentence-level while keeping paragraph location information as metadata for supporting future revision studies at the discourse level. Each pair of revised sentences is enriched with automatically extracted edits and associated revision intention. To assess the initial quality on the dataset, we conducted a qualitative study of several state-of-the-art text revision approaches and compared various
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;FlanT5-XXL&#21644;SemEval 2016&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#38646;-shot&#31435;&#22330;&#26816;&#27979;&#22312;&#25512;&#29305;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#21450;&#20854;&#23545;&#25552;&#31034;&#21644;&#35299;&#30721;&#31574;&#30053;&#30340;&#25935;&#24863;&#24615;&#65292;&#25581;&#31034;&#20102;&#20854;&#33021;&#22815;&#21305;&#25932;&#25110;&#36229;&#36234;&#26368;&#20808;&#36827;&#22522;&#20934;&#27979;&#35797;&#30340;&#33021;&#21147;&#65292;&#24182;&#35782;&#21035;&#20102;&#20854;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.00236</link><description>&lt;p&gt;
&#20351;&#29992;FlanT5-XXL&#36827;&#34892;&#38646;-shot&#31435;&#22330;&#26816;&#27979;&#30340;&#22522;&#20934;&#27979;&#35797;&#65306;&#20174;&#35757;&#32451;&#25968;&#25454;&#12289;&#25552;&#31034;&#21644;&#35299;&#30721;&#31574;&#30053;&#20013;&#25506;&#35752;&#20854;&#25509;&#36817;SOTA&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Benchmarking zero-shot stance detection with FlanT5-XXL: Insights from training data, prompting, and decoding strategies into its near-SoTA performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00236
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;FlanT5-XXL&#21644;SemEval 2016&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#38646;-shot&#31435;&#22330;&#26816;&#27979;&#22312;&#25512;&#29305;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#21450;&#20854;&#23545;&#25552;&#31034;&#21644;&#35299;&#30721;&#31574;&#30053;&#30340;&#25935;&#24863;&#24615;&#65292;&#25581;&#31034;&#20102;&#20854;&#33021;&#22815;&#21305;&#25932;&#25110;&#36229;&#36234;&#26368;&#20808;&#36827;&#22522;&#20934;&#27979;&#35797;&#30340;&#33021;&#21147;&#65292;&#24182;&#35782;&#21035;&#20102;&#20854;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;LLM&#30340;&#38646;-shot&#31435;&#22330;&#26816;&#27979;&#22312;&#25512;&#29305;&#19978;&#30340;&#34920;&#29616;&#12290;&#20351;&#29992;FlanT5-XXL&#65292;&#19968;&#20010;&#32463;&#36807;&#35843;&#25972;&#25351;&#20196;&#30340;&#24320;&#28304;LLM&#65292;&#22312;SemEval 2016&#20219;&#21153;6A&#12289;6B&#21644;P-Stance&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#25552;&#31034;&#21644;&#35299;&#30721;&#31574;&#30053;&#19979;&#30340;&#34920;&#29616;&#21450;&#20854;&#21464;&#21270;&#65292;&#20197;&#21450;&#27169;&#22411;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#38646;-shot&#26041;&#27861;&#21487;&#20197;&#21305;&#25932;&#29978;&#33267;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20854;&#34920;&#29616;&#30340;&#21508;&#31181;&#35265;&#35299;&#65292;&#21253;&#25324;&#23545;&#25351;&#20196;&#21644;&#25552;&#31034;&#30340;&#25935;&#24863;&#24615;&#65292;&#35299;&#30721;&#31574;&#30053;&#65292;&#25552;&#31034;&#30340;&#22256;&#24785;&#24230;&#65292;&#20197;&#21450;&#25552;&#31034;&#20013;&#23384;&#22312;&#30340;&#21542;&#23450;&#21644;&#21453;&#23545;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#20445;LLM&#27809;&#26377;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#31181;&#21487;&#33021;&#37096;&#20998;&#35299;&#37322;&#35299;&#30721;&#31574;&#30053;&#20043;&#38388;&#34920;&#29616;&#24046;&#24322;&#30340;&#31215;&#26497;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00236v1 Announce Type: cross  Abstract: We investigate the performance of LLM-based zero-shot stance detection on tweets. Using FlanT5-XXL, an instruction-tuned open-source LLM, with the SemEval 2016 Tasks 6A, 6B, and P-Stance datasets, we study the performance and its variations under different prompts and decoding strategies, as well as the potential biases of the model. We show that the zero-shot approach can match or outperform state-of-the-art benchmarks, including fine-tuned models. We provide various insights into its performance including the sensitivity to instructions and prompts, the decoding strategies, the perplexity of the prompts, and to negations and oppositions present in prompts. Finally, we ensure that the LLM has not been trained on test datasets, and identify a positivity bias which may partially explain the performance differences across decoding strategie
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Multimodal ArXiv&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;ArXivCap&#21644;ArXivQA&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#31185;&#23398;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;ArXivQA&#36890;&#36807;&#31185;&#23398;&#22270;&#29983;&#25104;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.00231</link><description>&lt;p&gt;
Multimodal ArXiv: &#29992;&#20110;&#25552;&#21319;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#31185;&#23398;&#29702;&#35299;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00231
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Multimodal ArXiv&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;ArXivCap&#21644;ArXivQA&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#31185;&#23398;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;ArXivQA&#36890;&#36807;&#31185;&#23398;&#22270;&#29983;&#25104;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#65292;&#20197;GPT-4V&#20026;&#20363;&#65292;&#22312;&#28041;&#21450;&#33258;&#28982;&#22330;&#26223;&#20013;&#30340;&#20855;&#20307;&#22270;&#20687;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31185;&#23398;&#39046;&#22495;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#65292;&#23427;&#20204;&#22312;&#35299;&#37322;&#25277;&#35937;&#22270;&#24418;&#65288;&#20363;&#22914;&#20960;&#20309;&#24418;&#29366;&#21644;&#31185;&#23398;&#22270;&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Multimodal ArXiv&#65292;&#21253;&#25324;ArXivCap&#21644;ArXivQA&#65292;&#20197;&#22686;&#24378;LVLMs&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;ArXivCap&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#28085;&#30422;&#21508;&#31181;&#31185;&#23398;&#39046;&#22495;&#30340;572K&#20221;ArXiv&#35770;&#25991;&#30340;6.4M&#24352;&#22270;&#20687;&#21644;3.9M&#20010;&#26631;&#39064;&#30340;&#22270;&#20687;&#26631;&#39064;&#25968;&#25454;&#38598;&#12290;&#20511;&#37492;ArXivCap&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ArXivQA&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#25552;&#31034;GPT-4V&#29983;&#25104;&#30340;&#22522;&#20110;&#31185;&#23398;&#22270;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;ArXivQA&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;LVLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#22810;&#27169;&#24577;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;10.4%&#30340;&#32477;&#23545;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;ArXivCap&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22235;&#20010;&#20174;&#35270;&#35273;&#21040;&#25991;&#26412;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00231v1 Announce Type: cross  Abstract: Large vision-language models (LVLMs), exemplified by GPT-4V, excel across diverse tasks involving concrete images from natural scenes. However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains. To fill this gap, we introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA, for enhancing LVLMs scientific comprehension. ArXivCap is a figure-caption dataset comprising 6.4M images and 3.9M captions sourced from 572K ArXiv papers spanning various scientific domains. Drawing from ArXivCap, we introduce ArXivQA, a question-answering dataset generated by prompting GPT-4V based on scientific figures. ArXivQA greatly enhances LVLMs' mathematical reasoning capabilities, achieving a 10.4% absolute accuracy gain on a multimodal mathematical reasoning benchmark. Furthermore, employing ArXivCap, we devise four vision-to-text tas
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#30340;&#35821;&#20041;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#26041;&#27861;&#21644;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#35821;&#35328;&#20013;&#20248;&#20110;&#20197;&#24448;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.00226</link><description>&lt;p&gt;
&#29992;&#20110;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#30340;&#35821;&#20041;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Semantic Distance Metric Learning approach for Lexical Semantic Change Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00226
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#30340;&#35821;&#20041;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#26041;&#27861;&#21644;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#35821;&#35328;&#20013;&#20248;&#20110;&#20197;&#24448;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#35789;&#27719;&#30340;&#26102;&#38388;&#35821;&#20041;&#21464;&#21270;&#26159;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#24517;&#39035;&#23545;&#26102;&#38388;&#25935;&#24863;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#65288;SCD&#65289;&#20219;&#21153;&#32771;&#34385;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;$C_1$&#21644;$C_2$&#20043;&#38388;&#39044;&#27979;&#32473;&#23450;&#30446;&#26631;&#35789;$w$&#26159;&#21542;&#25913;&#21464;&#20102;&#21547;&#20041;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29616;&#26377;&#30340;Word-in-Context&#65288;WiC&#65289;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#20004;&#38454;&#27573;SCD&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#23545;&#20110;&#30446;&#26631;&#35789;$w$&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#20004;&#20010;&#24863;&#30693;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#34920;&#31034;&#32473;&#23450;&#35821;&#26009;&#24211;&#20013;&#25152;&#36873;&#21477;&#23376;&#20013;$w$&#30340;&#21547;&#20041;&#12290;&#25509;&#19979;&#26469;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#31181;&#24863;&#30693;&#24863;&#30693;&#36317;&#31163;&#24230;&#37327;&#65292;&#27604;&#36739;&#30446;&#26631;&#35789;&#22312;$C_1$&#21644;$C_2$&#20013;&#30340;&#25152;&#26377;&#20986;&#29616;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#23545;&#22810;&#20010;SCD&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#25152;&#26377;&#20808;&#21069;&#25552;&#20986;&#30340;&#22810;&#31181;&#35821;&#35328;&#30340;SCD&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00226v1 Announce Type: new  Abstract: Detecting temporal semantic changes of words is an important task for various NLP applications that must make time-sensitive predictions. Lexical Semantic Change Detection (SCD) task considers the problem of predicting whether a given target word, $w$, changes its meaning between two different text corpora, $C_1$ and $C_2$. For this purpose, we propose a supervised two-staged SCD method that uses existing Word-in-Context (WiC) datasets. In the first stage, for a target word $w$, we learn two sense-aware encoder that represents the meaning of $w$ in a given sentence selected from a corpus. Next, in the second stage, we learn a sense-aware distance metric that compares the semantic representations of a target word across all of its occurrences in $C_1$ and $C_2$. Experimental results on multiple benchmark datasets for SCD show that our proposed method consistently outperforms all previously proposed SCD methods for multiple languages, esta
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;XLSR Wav2Vec2&#21644;mBART&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#35270;&#39057;&#20869;&#23481;&#30340;&#22810;&#35821;&#35328;&#36716;&#24405;&#21644;&#32763;&#35793;&#65292;&#20026;&#20010;&#24615;&#21270;&#35821;&#38899;&#25552;&#20379;&#20102;&#21487;&#35775;&#38382;&#30340;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2403.00212</link><description>&lt;p&gt;
&#20351;&#29992;&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;XLSR Wav2Vec2&#21644;mBART&#36827;&#34892;&#35270;&#39057;&#30340;&#36716;&#24405;&#21644;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00212
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;XLSR Wav2Vec2&#21644;mBART&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#35270;&#39057;&#20869;&#23481;&#30340;&#22810;&#35821;&#35328;&#36716;&#24405;&#21644;&#32763;&#35793;&#65292;&#20026;&#20010;&#24615;&#21270;&#35821;&#38899;&#25552;&#20379;&#20102;&#21487;&#35775;&#38382;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#20351;&#29992;&#26497;&#23569;&#25968;&#25454;&#35757;&#32451;&#20010;&#24615;&#21270;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#20165;&#21033;&#29992;&#26469;&#33258;YouTube&#35270;&#39057;&#30340;14&#20998;&#38047;&#33258;&#23450;&#20041;&#38899;&#39057;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#26816;&#32034;&#30340;&#35821;&#38899;&#36716;&#25442;&#65288;RVC&#65289;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#23450;&#20041;&#30340;Common Voice 16.0&#35821;&#26009;&#24211;&#12290;&#38543;&#21518;&#65292;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#19968;&#20010;&#36328;&#35821;&#35328;&#33258;&#30417;&#30563;&#34920;&#31034;&#65288;XLSR&#65289;Wav2Vec2&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#24320;&#21457;&#30340;&#22522;&#20110;Web&#30340;GUI&#21487;&#20197;&#39640;&#25928;&#22320;&#36716;&#24405;&#21644;&#32763;&#35793;&#36755;&#20837;&#30340;&#21360;&#22320;&#35821;&#35270;&#39057;&#12290;&#36890;&#36807;&#38598;&#25104;XLSR Wav2Vec2&#21644;mBART&#65292;&#35813;&#31995;&#32479;&#23558;&#32763;&#35793;&#21518;&#30340;&#25991;&#26412;&#19982;&#35270;&#39057;&#26102;&#38388;&#36724;&#23545;&#40784;&#65292;&#20026;&#20010;&#24615;&#21270;&#35821;&#38899;&#30340;&#22810;&#35821;&#35328;&#35270;&#39057;&#20869;&#23481;&#36716;&#24405;&#21644;&#32763;&#35793;&#25552;&#20379;&#20102;&#21487;&#35775;&#38382;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00212v1 Announce Type: new  Abstract: This research addresses the challenge of training an ASR model for personalized voices with minimal data. Utilizing just 14 minutes of custom audio from a YouTube video, we employ Retrieval-Based Voice Conversion (RVC) to create a custom Common Voice 16.0 corpus. Subsequently, a Cross-lingual Self-supervised Representations (XLSR) Wav2Vec2 model is fine-tuned on this dataset. The developed web-based GUI efficiently transcribes and translates input Hindi videos. By integrating XLSR Wav2Vec2 and mBART, the system aligns the translated text with the video timeline, delivering an accessible solution for multilingual video content transcription and translation for personalized voice.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#65292;&#25913;&#36827;&#20102;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;&#26041;&#27861;&#65292;&#20943;&#36731;&#25945;&#24072;&#32321;&#37325;&#30340;&#24037;&#20316;&#37327;&#65292;&#38450;&#27490;&#29983;&#25104;&#26080;&#25928;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00199</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#25913;&#36827;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Socratic Question Generation using Data Augmentation and Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00199
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#65292;&#25913;&#36827;&#20102;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;&#26041;&#27861;&#65292;&#20943;&#36731;&#25945;&#24072;&#32321;&#37325;&#30340;&#24037;&#20316;&#37327;&#65292;&#38450;&#27490;&#29983;&#25104;&#26080;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#26159;&#19968;&#31181;&#24341;&#23548;&#23398;&#29983;&#29420;&#31435;&#35299;&#20915;&#38382;&#39064;&#32780;&#19981;&#30452;&#25509;&#25581;&#31034;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#25913;&#36827;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#33487;&#26684;&#25289;&#24213;&#38382;&#39064;&#65292;&#20197;&#20943;&#36731;&#25945;&#24072;&#30340;&#32321;&#37325;&#24037;&#20316;&#37327;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#28041;&#21450;&#25552;&#31034;&#36825;&#20123;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26377;&#26102;&#20250;&#20135;&#29983;&#26080;&#25928;&#30340;&#36755;&#20986;&#65292;&#20363;&#22914;&#30452;&#25509;&#25581;&#31034;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#25110;&#25552;&#20379;&#26080;&#20851;&#25110;&#36807;&#26089;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#39318;&#20808;&#25552;&#20986;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#20016;&#23500;&#29616;&#26377;&#30340;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#25968;&#25454;&#38598;&#65307;&#20854;&#27425;&#65292;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#24320;&#28304;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;LLama 2&#65292;&#20197;&#26356;&#20542;&#21521;&#20110;&#22320;&#38754;&#30495;&#20540;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00199v1 Announce Type: new  Abstract: The Socratic method is a way of guiding students toward solving a problem independently without directly revealing the solution to the problem. Although this method has been shown to significantly improve student learning outcomes, it remains a complex labor-intensive task for instructors. Large language models (LLMs) can be used to augment human effort by automatically generating Socratic questions for students. However, existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions. To alleviate this problem, inspired by reinforcement learning with AI feedback (RLAIF), we first propose a data augmentation method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways. Next, we propose a method to optimize open-source LLMs such as LLama 2 to prefer ground-truth questio
&lt;/p&gt;</description></item><item><title>AXOLOTL&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#19977;&#27493;&#36807;&#31243;&#65292;&#35782;&#21035;&#21644;&#35299;&#20915;&#20559;&#35265;&#65292;&#25351;&#23548;&#27169;&#22411;&#33258;&#25105;&#21435;&#20559;&#35265;&#20854;&#36755;&#20986;&#65292;&#20174;&#32780;&#23454;&#29616;&#20844;&#24179;&#24615;&#24182;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00198</link><description>&lt;p&gt;
AXOLOTL&#65306;&#36890;&#36807;&#36741;&#21161;&#33258;&#25105;&#21435;&#20559;&#35265;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#23454;&#29616;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00198
&lt;/p&gt;
&lt;p&gt;
AXOLOTL&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#19977;&#27493;&#36807;&#31243;&#65292;&#35782;&#21035;&#21644;&#35299;&#20915;&#20559;&#35265;&#65292;&#25351;&#23548;&#27169;&#22411;&#33258;&#25105;&#21435;&#20559;&#35265;&#20854;&#36755;&#20986;&#65292;&#20174;&#32780;&#23454;&#29616;&#20844;&#24179;&#24615;&#24182;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#20294;&#23481;&#26131;&#21463;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#24433;&#21709;&#65292;&#23548;&#33268;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20986;&#29616;&#19981;&#20844;&#24179;&#32467;&#26524;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#31574;&#30053;&#26469;&#20943;&#36731;&#20559;&#35265;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AXOLOTL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#26694;&#26550;&#65292;&#33021;&#22815;&#29420;&#31435;&#20110;&#20219;&#21153;&#21644;&#27169;&#22411;&#36816;&#34892;&#65292;&#22312;&#19981;&#30452;&#25509;&#35775;&#38382;&#20869;&#37096;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#20844;&#20849;API&#19982;LLMs&#20132;&#20114;&#12290;&#36890;&#36807;&#31867;&#20284;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#19977;&#27493;&#36807;&#31243;&#65292;AXOLOTL&#35782;&#21035;&#20559;&#35265;&#65292;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25351;&#23548;&#27169;&#22411;&#33258;&#25105;&#21435;&#20559;&#35265;&#20854;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#27861;&#26368;&#23567;&#21270;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#20445;&#25345;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#20351;AXOLOTL&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#26131;&#29992;&#24615;&#30340;&#21435;&#20559;&#35265;LLM&#36755;&#20986;&#30340;&#26377;&#21069;&#26223;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00198v1 Announce Type: cross  Abstract: Pre-trained Large Language Models (LLMs) have significantly advanced natural language processing capabilities but are susceptible to biases present in their training data, leading to unfair outcomes in various applications. While numerous strategies have been proposed to mitigate bias, they often require extensive computational resources and may compromise model performance. In this work, we introduce AXOLOTL, a novel post-processing framework, which operates agnostically across tasks and models, leveraging public APIs to interact with LLMs without direct access to internal parameters. Through a three-step process resembling zero-shot learning, AXOLOTL identifies biases, proposes resolutions, and guides the model to self-debias its outputs. This approach minimizes computational costs and preserves model performance, making AXOLOTL a promising tool for debiasing LLM outputs with broad applicability and ease of use.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#20013;&#20559;&#35265;&#25918;&#22823;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;Seesaw-CF&#65292;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#20102;&#26435;&#37325;&#32534;&#36753;&#26041;&#27861;&#23545;&#27169;&#22411;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.00180</link><description>&lt;p&gt;
"Flex Tape&#19981;&#33021;&#20462;&#22797;&#36825;&#20010;": &#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#21644;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
"Flex Tape Can't Fix That": Bias and Misinformation in Edited Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00180
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#20013;&#20559;&#35265;&#25918;&#22823;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;Seesaw-CF&#65292;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#20102;&#26435;&#37325;&#32534;&#36753;&#26041;&#27861;&#23545;&#27169;&#22411;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#24050;&#32463;&#25104;&#20026;&#26356;&#26032;&#23384;&#20648;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#30340;&#19968;&#31181;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#32534;&#36753;&#24212;&#29992;&#21518;&#65292;&#27169;&#22411;&#32534;&#36753;&#21487;&#33021;&#20250;&#20135;&#29983;&#24847;&#24819;&#19981;&#21040;&#30340;&#21518;&#26524;&#65306;&#19982;&#32534;&#36753;&#26080;&#20851;&#30340;&#20449;&#24687;&#20063;&#21487;&#33021;&#34987;&#26356;&#25913;&#65292;&#24182;&#19988;&#27169;&#22411;&#30340;&#20854;&#20182;&#19968;&#33324;&#34892;&#20026;&#21487;&#33021;&#34987;&#38169;&#35823;&#22320;&#25913;&#21464;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22914;&#20309;&#24847;&#22806;&#22320;&#21152;&#21095;&#20102;&#27169;&#22411;&#21518;&#32534;&#36753;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;Seesaw-CF&#65292;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#32534;&#36753;&#30340;&#20559;&#35265;&#30456;&#20851;&#20260;&#23475;&#65292;&#24182;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#19981;&#21516;&#26435;&#37325;&#32534;&#36753;&#26041;&#27861;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#19982;&#31181;&#26063;&#12289;&#22320;&#29702;&#26469;&#28304;&#21644;&#24615;&#21035;&#31561;&#20154;&#21475;&#23646;&#24615;&#30456;&#20851;&#30340;&#20559;&#35265;&#65292;&#20197;&#21450;&#30001;&#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38271;&#25991;&#26412;&#20013;&#30340;&#23450;&#24615;&#32570;&#38519;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32534;&#36753;&#27169;&#22411;&#22312;&#21464;&#24471;&#23545;&#20122;&#27954;&#12289;&#38750;&#27954;&#31561;&#23646;&#24615;&#30340;&#23646;&#24615;&#19981;&#30830;&#23450;&#24230;&#24840;&#39640;&#26102;&#34920;&#29616;&#20986;&#19981;&#21516;&#31243;&#24230;&#30340;&#26356;&#20026;&#20559;&#35265;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00180v1 Announce Type: new  Abstract: Model editing has emerged as a cost-effective strategy to update knowledge stored in language models. However, model editing can have unintended consequences after edits are applied: information unrelated to the edits can also be changed, and other general behaviors of the model can be wrongly altered. In this work, we investigate how model editing methods unexpectedly amplify model biases post-edit. We introduce a novel benchmark dataset, Seesaw-CF, for measuring bias-related harms of model editing and conduct the first in-depth investigation of how different weight-editing methods impact model bias. Specifically, we focus on biases with respect to demographic attributes such as race, geographic origin, and gender, as well as qualitative flaws in long-form texts generated by edited language models. We find that edited models exhibit, to various degrees, more biased behavior as they become less confident in attributes for Asian, African,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#30417;&#30563;&#30340;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#21807;&#19968;&#31867;&#21517;&#20316;&#20026;&#21807;&#19968;&#30417;&#30563;&#65292;&#21516;&#26102;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00165</link><description>&lt;p&gt;
TELEClass: &#31246;&#21153;&#23398;&#20016;&#23500;&#21644;LLM&#22686;&#24378;&#30340;&#26368;&#23567;&#30417;&#30563;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#30417;&#30563;&#30340;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#21807;&#19968;&#31867;&#21517;&#20316;&#20026;&#21807;&#19968;&#30417;&#30563;&#65292;&#21516;&#26102;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26088;&#22312;&#23558;&#27599;&#20010;&#25991;&#26723;&#20998;&#31867;&#20026;&#26631;&#31614;Taxonomy&#20013;&#30340;&#19968;&#32452;&#31867;&#21035;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20351;&#29992;&#26368;&#23569;&#30417;&#30563;&#65306;&#20165;&#20351;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#21807;&#19968;&#31867;&#21517;&#20316;&#20026;&#30417;&#30563;&#26469;&#36827;&#34892;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#38646;&#25552;&#31034;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#23618;&#35774;&#32622;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#22240;&#20026;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#22823;&#32780;&#32467;&#26500;&#21270;&#30340;&#26631;&#31614;&#31354;&#38388;&#26159;&#26080;&#25928;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20197;&#21069;&#30340;&#24369;&#30417;&#30563;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20165;&#21033;&#29992;&#21407;&#22987;&#30340;Taxonomy&#39592;&#26550;&#65292;&#24573;&#30053;&#20102;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#38544;&#34255;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#29992;&#20316;&#39069;&#22806;&#30340;&#31867;&#21035;&#25351;&#31034;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00165v1 Announce Type: new  Abstract: Hierarchical text classification aims to categorize each document into a set of classes in a label taxonomy. Most earlier works focus on fully or semi-supervised methods that require a large amount of human annotated data which is costly and time-consuming to acquire. To alleviate human efforts, in this paper, we work on hierarchical text classification with the minimal amount of supervision: using the sole class name of each node as the only supervision. Recently, large language models (LLM) show competitive performance on various tasks through zero-shot prompting, but this method performs poorly in the hierarchical setting, because it is ineffective to include the large and structured label space in a prompt. On the other hand, previous weakly-supervised hierarchical text classification methods only utilize the raw taxonomy skeleton and ignore the rich information hidden in the text corpus that can serve as additional class-indicative 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;EBBS&#65292;&#37197;&#21512;&#26032;&#39062;&#30340;&#21452;&#23618;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#20248;&#20110;&#30452;&#25509;&#21644;&#36890;&#36807;&#31532;&#19977;&#35821;&#35328;&#36827;&#34892;&#30340;&#32763;&#35793;&#65292;&#24182;&#23454;&#29616;&#30693;&#35782;&#33976;&#39311;&#26469;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.00144</link><description>&lt;p&gt;
EBBS: &#19968;&#20010;&#20855;&#26377;&#21452;&#23618;&#26463;&#25628;&#32034;&#30340;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#38646;&#32763;&#35793;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
EBBS: An Ensemble with Bi-Level Beam Search for Zero-Shot Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00144
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;EBBS&#65292;&#37197;&#21512;&#26032;&#39062;&#30340;&#21452;&#23618;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#20248;&#20110;&#30452;&#25509;&#21644;&#36890;&#36807;&#31532;&#19977;&#35821;&#35328;&#36827;&#34892;&#30340;&#32763;&#35793;&#65292;&#24182;&#23454;&#29616;&#30693;&#35782;&#33976;&#39311;&#26469;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25105;&#20204;&#29992;&#29305;&#23450;&#30340;&#32763;&#35793;&#26041;&#21521;&#35757;&#32451;&#22810;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#38646;&#32763;&#35793;&#30340;&#33021;&#21147;&#23601;&#20250;&#20986;&#29616;&#65307;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#22312;&#26410;&#35265;&#36807;&#30340;&#26041;&#21521;&#36827;&#34892;&#32763;&#35793;&#12290;&#21478;&#22806;&#65292;&#38646;&#32763;&#35793;&#20063;&#21487;&#20197;&#36890;&#36807;&#31532;&#19977;&#31181;&#35821;&#35328;&#65288;&#20363;&#22914;&#33521;&#35821;&#65289;&#26469;&#23454;&#29616;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#30452;&#25509;&#21644;&#36890;&#36807;&#31532;&#19977;&#31181;&#35821;&#35328;&#36827;&#34892;&#30340;&#32763;&#35793;&#37117;&#23384;&#22312;&#22122;&#38899;&#65292;&#24182;&#19988;&#34920;&#29616;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EBBS&#65292;&#19968;&#20010;&#20855;&#26377;&#26032;&#39062;&#30340;&#21452;&#23618;&#26463;&#25628;&#32034;&#31639;&#27861;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#38598;&#25104;&#32452;&#20214;&#22312;&#19979;&#23618;&#36880;&#27493;&#25506;&#32034;&#33258;&#24049;&#30340;&#39044;&#27979;&#65292;&#20294;&#23427;&#20204;&#36890;&#36807;&#19978;&#23618;&#30340;&#8220;&#36719;&#25237;&#31080;&#8221;&#26426;&#21046;&#36827;&#34892;&#21516;&#27493;&#12290;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;EBBS&#22987;&#32456;&#20248;&#20110;&#30452;&#25509;&#21644;&#36890;&#36807;&#31532;&#19977;&#31181;&#35821;&#35328;&#36827;&#34892;&#30340;&#32763;&#35793;&#65292;&#20197;&#21450;&#29616;&#26377;&#30340;&#38598;&#25104;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#38598;&#25104;&#30340;&#30693;&#35782;&#20256;&#22238;&#21040;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#65307;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;E
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00144v1 Announce Type: cross  Abstract: The ability of zero-shot translation emerges when we train a multilingual model with certain translation directions; the model can then directly translate in unseen directions. Alternatively, zero-shot translation can be accomplished by pivoting through a third language (e.g., English). In our work, we observe that both direct and pivot translations are noisy and achieve less satisfactory performance. We propose EBBS, an ensemble method with a novel bi-level beam search algorithm, where each ensemble component explores its own prediction step by step at the lower level but they are synchronized by a "soft voting" mechanism at the upper level. Results on two popular multilingual translation datasets show that EBBS consistently outperforms direct and pivot translations as well as existing ensemble techniques. Further, we can distill the ensemble's knowledge back to the multilingual model to improve inference efficiency; profoundly, our E
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26641;&#24179;&#22343;&#27861;&#26500;&#24314;&#38598;&#25104;&#35299;&#26512;&#22120;&#65292;&#31283;&#23450;&#24182;&#25552;&#21319;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#22522;&#20934;&#32447;</title><link>https://arxiv.org/abs/2403.00143</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#25104;&#30340;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#65306;&#26641;&#24179;&#22343;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ensemble-Based Unsupervised Discontinuous Constituency Parsing by Tree Averaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00143
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26641;&#24179;&#22343;&#27861;&#26500;&#24314;&#38598;&#25104;&#35299;&#26512;&#22120;&#65292;&#31283;&#23450;&#24182;&#25552;&#21319;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#30340;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#25105;&#20204;&#35266;&#23519;&#21040;&#20808;&#21069;&#21807;&#19968;&#27169;&#22411;&#30340;&#24615;&#33021;&#23384;&#22312;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23545;&#29616;&#26377;&#19981;&#36830;&#32493;&#35299;&#26512;&#22120;&#30340;&#19981;&#21516;&#36816;&#34892;&#26500;&#24314;&#19968;&#20010;&#38598;&#25104;&#65292;&#24182;&#36890;&#36807;&#24179;&#22343;&#39044;&#27979;&#26641;&#26469;&#31283;&#23450;&#21644;&#25552;&#21319;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38024;&#23545;&#19981;&#21516;&#30340;&#20108;&#20803;&#24615;&#21644;&#36830;&#32493;&#24615;&#35774;&#32622;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#26641;&#24179;&#22343;&#35745;&#31639;&#22797;&#26434;&#24230;&#20998;&#26512;&#65288;&#20197;P&#21644;NP&#23436;&#20840;&#20026;&#21333;&#20301;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#30830;&#31639;&#27861;&#26469;&#22788;&#29702;&#36825;&#19968;&#20219;&#21153;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#23545;&#25152;&#26377;&#26679;&#26412;&#36816;&#34892;&#26102;&#38388;&#22343;&#21512;&#29702;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#32447;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00143v1 Announce Type: cross  Abstract: We address unsupervised discontinuous constituency parsing, where we observe a high variance in the performance of the only previous model. We propose to build an ensemble of different runs of the existing discontinuous parser by averaging the predicted trees, to stabilize and boost performance. To begin with, we provide comprehensive computational complexity analysis (in terms of P and NP-complete) for tree averaging under different setups of binarity and continuity. We then develop an efficient exact algorithm to tackle the task, which runs in a reasonable time for all samples in our experiments. Results on three datasets show our method outperforms all baselines in all metrics; we also provide in-depth analyses of our approach.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#21463;&#25511;&#25277;&#35937;&#25688;&#35201;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EROS&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#26174;&#33879;&#25913;&#21892;&#38544;&#31169;&#25919;&#31574;&#25991;&#20214;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#35835;&#24615;&#65292;&#24378;&#35843;&#20102;&#21253;&#21547;&#20851;&#38190;&#38544;&#31169;&#30456;&#20851;&#23454;&#20307;&#21644;&#32452;&#32455;&#29702;&#30001;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00141</link><description>&lt;p&gt;
EROS&#65306;&#22522;&#20110;&#23454;&#20307;&#30340;&#21463;&#25511;&#25919;&#31574;&#25991;&#20214;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
EROS: Entity-Driven Controlled Policy Document Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00141
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21463;&#25511;&#25277;&#35937;&#25688;&#35201;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EROS&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#26174;&#33879;&#25913;&#21892;&#38544;&#31169;&#25919;&#31574;&#25991;&#20214;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#35835;&#24615;&#65292;&#24378;&#35843;&#20102;&#21253;&#21547;&#20851;&#38190;&#38544;&#31169;&#30456;&#20851;&#23454;&#20307;&#21644;&#32452;&#32455;&#29702;&#30001;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#25919;&#31574;&#25991;&#20214;&#22312;&#21521;&#20010;&#20154;&#20171;&#32461;&#32452;&#32455;&#23545;&#29992;&#25143;&#20010;&#20154;&#25968;&#25454;&#30340;&#25910;&#38598;&#12289;&#20351;&#29992;&#21644;&#20445;&#25252;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20197;&#38271;&#31687;&#12289;&#22797;&#26434;&#21644;&#26214;&#28073;&#30340;&#35821;&#35328;&#32780;&#38395;&#21517;&#65292;&#23588;&#20854;&#28041;&#21450;&#19982;&#38544;&#31169;&#30456;&#20851;&#30340;&#23454;&#20307;&#12290;&#22240;&#27492;&#65292;&#23545;&#35797;&#22270;&#29702;&#35299;&#32452;&#32455;&#25968;&#25454;&#20351;&#29992;&#25919;&#31574;&#30340;&#29992;&#25143;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20351;&#29992;&#21463;&#25511;&#25277;&#35937;&#25688;&#35201;&#26469;&#22686;&#24378;&#25919;&#31574;&#25991;&#20214;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#35835;&#24615; -- &#25105;&#20204;&#24378;&#21046;&#29983;&#25104;&#30340;&#25688;&#35201;&#21253;&#25324;&#20851;&#38190;&#30340;&#38544;&#31169;&#30456;&#20851;&#23454;&#20307;&#65288;&#22914;&#25968;&#25454;&#21644;&#23186;&#20307;&#65289;&#20197;&#21450;&#32452;&#32455;&#30340;&#29702;&#30001;&#65288;&#22914;&#30446;&#26631;&#21644;&#21407;&#22240;&#65289;&#22312;&#25910;&#38598;&#36825;&#20123;&#23454;&#20307;&#26102;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;PD-Sum&#65292;&#19968;&#20010;&#24102;&#26377;&#26631;&#35760;&#30340;&#38544;&#31169;&#30456;&#20851;&#23454;&#20307;&#26631;&#31614;&#30340;&#25919;&#31574;&#25991;&#20214;&#25688;&#35201;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;EROS&#36890;&#36807;&#22522;&#20110;&#36328;&#24230;&#30340;&#23454;&#20307;&#25552;&#21462;&#27169;&#22411;&#35782;&#21035;&#20851;&#38190;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00141v1 Announce Type: cross  Abstract: Privacy policy documents have a crucial role in educating individuals about the collection, usage, and protection of users' personal data by organizations. However, they are notorious for their lengthy, complex, and convoluted language especially involving privacy-related entities. Hence, they pose a significant challenge to users who attempt to comprehend organization's data usage policy. In this paper, we propose to enhance the interpretability and readability of policy documents by using controlled abstractive summarization -- we enforce the generated summaries to include critical privacy-related entities (e.g., data and medium) and organization's rationale (e.g.,target and reason) in collecting those entities. To achieve this, we develop PD-Sum, a policy-document summarization dataset with marked privacy-related entity labels. Our proposed model, EROS, identifies critical entities through a span-based entity extraction model and em
&lt;/p&gt;</description></item><item><title>&#35752;&#35770;&#22312;ChatGPT&#20013;&#23558;&#32763;&#35793;&#31616;&#35201;&#21644;&#32763;&#35793;&#32773;/&#20316;&#32773;&#20154;&#29289;&#35282;&#33394;&#34701;&#20837;&#25552;&#31034;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#34429;&#28982;&#26377;&#21161;&#20110;&#20419;&#36827;&#20154;&#31867;&#20043;&#38388;&#30340;&#32763;&#35793;&#36890;&#20449;&#65292;&#20294;&#23545;&#20110;&#25913;&#21892;ChatGPT&#32763;&#35793;&#36136;&#37327;&#30340;&#25928;&#26524;&#26377;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.00127</link><description>&lt;p&gt;
&#20351;&#29992;&#25552;&#31034;ChatGPT&#36827;&#34892;&#32763;&#35793;&#65306;&#32763;&#35793;&#31616;&#35201;&#21644;&#20154;&#29289;&#35282;&#33394;&#25552;&#31034;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Prompting ChatGPT for Translation: A Comparative Analysis of Translation Brief and Persona Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00127
&lt;/p&gt;
&lt;p&gt;
&#35752;&#35770;&#22312;ChatGPT&#20013;&#23558;&#32763;&#35793;&#31616;&#35201;&#21644;&#32763;&#35793;&#32773;/&#20316;&#32773;&#20154;&#29289;&#35282;&#33394;&#34701;&#20837;&#25552;&#31034;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#34429;&#28982;&#26377;&#21161;&#20110;&#20419;&#36827;&#20154;&#31867;&#20043;&#38388;&#30340;&#32763;&#35793;&#36890;&#20449;&#65292;&#20294;&#23545;&#20110;&#25913;&#21892;ChatGPT&#32763;&#35793;&#36136;&#37327;&#30340;&#25928;&#26524;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#20013;&#30340;&#25552;&#31034;&#24037;&#31243;&#24050;&#26174;&#31034;&#20986;&#25913;&#21892;&#32763;&#35793;&#36136;&#37327;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#25552;&#31034;&#35774;&#35745;&#20013;&#34701;&#20837;&#32763;&#35793;&#27010;&#24565;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;ChatGPT&#20013;&#20026;&#32763;&#35793;&#20219;&#21153;&#35774;&#35745;&#25552;&#31034;&#26102;&#23558;&#32763;&#35793;&#31616;&#35201;&#27010;&#24565;&#21644;&#32763;&#35793;&#32773;&#21450;&#20316;&#32773;&#30340;&#20154;&#29289;&#35282;&#33394;&#32467;&#21512;&#36215;&#26469;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#26576;&#20123;&#20803;&#32032;&#26377;&#21161;&#20110;&#20419;&#36827;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#32763;&#35793;&#36890;&#20449;&#65292;&#20294;&#23427;&#20204;&#22312;&#25552;&#39640;ChatGPT&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#30340;&#25928;&#26524;&#26377;&#38480;&#12290;&#36825;&#20984;&#26174;&#20102;&#38656;&#35201;&#26356;&#22810;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#32763;&#35793;&#29702;&#35770;&#23478;&#21644;&#23454;&#36341;&#32773;&#22914;&#20309;&#24320;&#21457;&#30446;&#21069;&#26681;&#26893;&#20110;&#20154;&#19982;&#20154;&#20132;&#27969;&#33539;&#24335;&#30340;&#27010;&#24565;&#24037;&#20855;&#38598;&#65292;&#20197;&#24212;&#29992;&#20110;&#28041;&#21450;&#20154;&#26426;&#20132;&#20114;&#30340;&#26032;&#20852;&#24037;&#20316;&#27969;&#20013;&#30340;&#32763;&#35793;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00127v1 Announce Type: new  Abstract: Prompt engineering in LLMs has shown potential for improving translation quality. However, the potential of incorporating translation concepts in prompt design remains largely underexplored. Against this backdrop, this paper discusses the effectiveness of incorporating the conceptual tool of translation brief and the personas of translator and author into prompt design for translation tasks in ChatGPT. Findings suggest that, although certain elements are constructive in facilitating human to human communication for translation tasks, their effectiveness is limited for improving translation quality in ChatGPT. This accentuates the need for more explorative research on how translation theorists and practitioners can develop the current set of conceptual tools rooted in the human to human communication paradigm for translation purposes in this emerging workflow involving human machine interaction.
&lt;/p&gt;</description></item><item><title>FAC$^2$E&#26694;&#26550;&#36890;&#36807;&#20998;&#31163;&#35821;&#35328;&#21644;&#35748;&#30693;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#22810;&#32500;&#21644;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#26041;&#24335;&#65292;&#24182;&#23558;LLMs&#24212;&#29992;&#33021;&#21147;&#20998;&#35299;&#20026;&#22238;&#24518;&#30693;&#35782;&#12289;&#21033;&#29992;&#30693;&#35782;&#21644;&#35299;&#20915;&#38382;&#39064;&#19977;&#20010;&#23376;&#27493;&#39588;&#65292;&#20174;&#32780;&#20026;LLMs&#25552;&#20379;&#20102;&#21452;&#37325;&#35786;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.00126</link><description>&lt;p&gt;
FAC$^2$E: &#36890;&#36807;&#20998;&#31163;&#35821;&#35328;&#21644;&#35748;&#30693;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
FAC$^2$E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00126
&lt;/p&gt;
&lt;p&gt;
FAC$^2$E&#26694;&#26550;&#36890;&#36807;&#20998;&#31163;&#35821;&#35328;&#21644;&#35748;&#30693;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#22810;&#32500;&#21644;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#26041;&#24335;&#65292;&#24182;&#23558;LLMs&#24212;&#29992;&#33021;&#21147;&#20998;&#35299;&#20026;&#22238;&#24518;&#30693;&#35782;&#12289;&#21033;&#29992;&#30693;&#35782;&#21644;&#35299;&#20915;&#38382;&#39064;&#19977;&#20010;&#23376;&#27493;&#39588;&#65292;&#20174;&#32780;&#20026;LLMs&#25552;&#20379;&#20102;&#21452;&#37325;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20027;&#35201;&#36890;&#36807;&#22312;&#21508;&#31181;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#25972;&#20307;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#26410;&#33021;&#20840;&#38754;&#21306;&#20998;&#32454;&#31890;&#24230;&#30340;&#35821;&#35328;&#21644;&#35748;&#30693;&#25216;&#33021;&#65292;&#23548;&#33268;&#23545;LLMs&#33021;&#21147;&#30340;&#35299;&#37322;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FAC$^2$E&#65292;&#19968;&#31181;&#29992;&#20110;&#32454;&#31890;&#24230;&#21644;&#22522;&#20110;&#35748;&#30693;&#30340;LLMs&#33021;&#21147;&#35780;&#20272;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#31163;&#19982;&#35821;&#35328;&#30456;&#20851;&#30340;&#33021;&#21147;&#21644;&#35748;&#30693;&#30456;&#20851;&#30340;&#33021;&#21147;&#65292;&#20197;&#22810;&#32500;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26469;&#21046;&#23450;LLMs&#30340;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20174;LLMs&#20013;&#25552;&#21462;&#20013;&#38388;&#25512;&#29702;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#24212;&#29992;&#29305;&#23450;&#33021;&#21147;&#30340;&#36807;&#31243;&#20998;&#35299;&#20026;&#19977;&#20010;&#23376;&#27493;&#39588;&#65306;&#22238;&#24518;&#30456;&#20851;&#30693;&#35782;&#12289;&#21033;&#29992;&#30693;&#35782;&#21644;&#35299;&#20915;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;FAC$^2$E&#35780;&#20272;&#27599;&#20010;&#32454;&#31890;&#24230;&#33021;&#21147;&#30340;&#27599;&#20010;&#23376;&#27493;&#39588;&#65292;&#20026;LLMs&#25552;&#20379;&#20102;&#21452;&#37325;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00126v1 Announce Type: new  Abstract: Large language models (LLMs) are primarily evaluated by overall performance on various text understanding and generation tasks. However, such a paradigm fails to comprehensively differentiate the fine-grained language and cognitive skills, rendering the lack of sufficient interpretation to LLMs' capabilities. In this paper, we present FAC$^2$E, a framework for Fine-grAined and Cognition-grounded LLMs' Capability Evaluation. Specifically, we formulate LLMs' evaluation in a multi-dimensional and explainable manner by dissociating the language-related capabilities and the cognition-related ones. Besides, through extracting the intermediate reasoning from LLMs, we further break down the process of applying a specific capability into three sub-steps: recalling relevant knowledge, utilizing knowledge, and solving problems. Finally, FAC$^2$E evaluates each sub-step of each fine-grained capability, providing a two-faceted diagnosis for LLMs. Uti
&lt;/p&gt;</description></item><item><title>LoRA&#20316;&#20026;&#25915;&#20987;&#32773;&#28183;&#36879;LLM&#23433;&#20840;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20849;&#20139;&#19982;&#29609;&#32781;&#22330;&#26223;&#19979;&#21487;&#33021;&#23454;&#29616;&#30340;&#25915;&#20987;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2403.00108</link><description>&lt;p&gt;
&#23558;LoRA&#20316;&#20026;&#25915;&#20987;&#65281;&#22312;Share-and-Play&#22330;&#26223;&#19979;&#31359;&#36879;LLM&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00108
&lt;/p&gt;
&lt;p&gt;
LoRA&#20316;&#20026;&#25915;&#20987;&#32773;&#28183;&#36879;LLM&#23433;&#20840;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20849;&#20139;&#19982;&#29609;&#32781;&#22330;&#26223;&#19979;&#21487;&#33021;&#23454;&#29616;&#30340;&#25915;&#20987;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#23545;&#20110;&#22686;&#24378;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#24182;&#30830;&#20445;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;&#20013;&#65292;LoRA&#22240;&#20854;&#25928;&#29575;&#21644;&#26131;&#29992;&#24615;&#32780;&#22791;&#21463;&#25512;&#23815;&#65292;&#20801;&#35768;&#26368;&#32456;&#29992;&#25143;&#36731;&#26494;&#22312;&#24320;&#28304;&#24179;&#21488;&#19978;&#21457;&#24067;&#21644;&#37319;&#29992;&#36731;&#37327;&#30340;LoRA&#27169;&#22359;&#65292;&#20197;&#23450;&#21046;&#20854;&#27169;&#22411;&#20197;&#36866;&#24212;&#19981;&#21516;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#20415;&#30340;&#20849;&#20139;&#19982;&#29609;&#32781;&#35774;&#32622;&#25171;&#24320;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;LoRA&#20316;&#20026;&#25915;&#20987;&#32773;&#65292;&#20363;&#22914;&#32972;&#38376;&#27880;&#20837;&#65292;&#24182;&#24191;&#27867;&#20998;&#21457;&#23545;&#25239;&#24615;LoRA&#32473;&#31038;&#21306;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#21033;&#30340;&#21518;&#26524;&#12290;&#23613;&#31649;&#20849;&#20139;LoRA&#27169;&#22359;&#23384;&#22312;&#24040;&#22823;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#20294;&#36825;&#19968;&#26041;&#38754;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#20849;&#20139;&#19982;&#29609;&#32781;&#22330;&#26223;&#20013;&#21487;&#33021;&#20570;&#20986;&#30340;&#25915;&#20987;&#26426;&#20250;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#21518;&#38376;&#27880;&#20837;LoRA&#27169;&#22359;&#24182;&#28145;&#20837;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00108v1 Announce Type: cross  Abstract: Fine-tuning LLMs is crucial to enhancing their task-specific performance and ensuring model behaviors are aligned with human preferences. Among various fine-tuning methods, LoRA is popular for its efficiency and ease to use, allowing end-users to easily post and adopt lightweight LoRA modules on open-source platforms to tailor their model for different customization. However, such a handy share-and-play setting opens up new attack surfaces, that the attacker can render LoRA as an attacker, such as backdoor injection, and widely distribute the adversarial LoRA to the community easily. This can result in detrimental outcomes. Despite the huge potential risks of sharing LoRA modules, this aspect however has not been fully explored. To fill the gap, in this study we thoroughly investigate the attack opportunities enabled in the growing share-and-play scenario. Specifically, we study how to inject backdoor into the LoRA module and dive deep
&lt;/p&gt;</description></item><item><title>Proc2PDDL&#26159;&#21253;&#21547;&#24320;&#25918;&#39046;&#22495;&#31243;&#24207;&#24615;&#25991;&#26412;&#21644;&#19987;&#23478;&#27880;&#37322;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#23545;&#20110;&#21160;&#20316;&#30340;&#20808;&#20915;&#26465;&#20214;&#21644;&#25928;&#26524;&#30340;&#23450;&#20041;&#22312;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#20026;&#26410;&#26469;&#25972;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#24418;&#24335;&#35268;&#21010;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.00092</link><description>&lt;p&gt;
PROC2PDDL&#65306;&#26469;&#33258;&#25991;&#26412;&#30340;&#24320;&#25918;&#39046;&#22495;&#35268;&#21010;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
PROC2PDDL: Open-Domain Planning Representations from Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00092
&lt;/p&gt;
&lt;p&gt;
Proc2PDDL&#26159;&#21253;&#21547;&#24320;&#25918;&#39046;&#22495;&#31243;&#24207;&#24615;&#25991;&#26412;&#21644;&#19987;&#23478;&#27880;&#37322;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#23545;&#20110;&#21160;&#20316;&#30340;&#20808;&#20915;&#26465;&#20214;&#21644;&#25928;&#26524;&#30340;&#23450;&#20041;&#22312;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#20026;&#26410;&#26469;&#25972;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#24418;&#24335;&#35268;&#21010;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#35268;&#21010;&#20173;&#28982;&#26159;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#39044;&#27979;&#35268;&#21010;&#39046;&#22495;&#23450;&#20041;&#65288;&#20363;&#22914;&#65292;PDDL&#65289;&#65292;&#20294;&#20165;&#22312;&#23553;&#38381;&#39046;&#22495;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Proc2PDDL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#24320;&#25918;&#39046;&#22495;&#31243;&#24207;&#24615;&#25991;&#26412;&#21644;&#19987;&#23478;&#27880;&#37322;&#30340;PDDL&#34920;&#31034;&#30340;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#23450;&#20041;&#21160;&#20316;&#30340;&#20808;&#20915;&#26465;&#20214;&#21644;&#25928;&#26524;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;Proc2PDDL&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;GPT-3.5 &#30340;&#25104;&#21151;&#29575;&#25509;&#36817;&#20110;0%&#65292;&#32780; GPT-4 &#30340;&#25104;&#21151;&#29575;&#32422;&#20026;35%&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#20986;&#35821;&#27861;&#21644;&#35821;&#20041;&#38169;&#35823;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;&#31243;&#24207;&#21644;&#25512;&#29702;&#20107;&#20214;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#20998;&#26512;&#21644;&#25968;&#25454;&#38598;&#26377;&#21161;&#20110;&#26410;&#26469;&#22312;&#25972;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#24418;&#24335;&#35268;&#21010;&#30340;&#26368;&#20339;&#26041;&#38754;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00092v1 Announce Type: new  Abstract: Planning in a text-based environment continues to be a major challenge for AI systems. Recent approaches have used language models to predict a planning domain definition (e.g., PDDL) but have only been evaluated in closed-domain simulated environments. To address this, we present Proc2PDDL , the first dataset containing open-domain procedural texts paired with expert-annotated PDDL representations. Using this dataset, we evaluate state-of-the-art models on defining the preconditions and effects of actions. We show that Proc2PDDL is highly challenging, with GPT-3.5's success rate close to 0% and GPT-4's around 35%. Our analysis shows both syntactic and semantic errors, indicating LMs' deficiency in both generating domain-specific prgorams and reasoning about events. We hope this analysis and dataset helps future progress towards integrating the best of LMs and formal planning.
&lt;/p&gt;</description></item><item><title>Resonance RoPE&#26159;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;RoPE&#29305;&#24449;&#30340;&#25554;&#20540;&#26469;&#32553;&#23567;&#35757;&#32451;&#30701;-&#27979;&#35797;&#38271;&#22330;&#26223;&#19979;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#22312;&#32447;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00071</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#27867;&#21270;&#33021;&#21147;&#65306;&#20849;&#25391; RoPE
&lt;/p&gt;
&lt;p&gt;
Resonance RoPE: Improving Context Length Generalization of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00071
&lt;/p&gt;
&lt;p&gt;
Resonance RoPE&#26159;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;RoPE&#29305;&#24449;&#30340;&#25554;&#20540;&#26469;&#32553;&#23567;&#35757;&#32451;&#30701;-&#27979;&#35797;&#38271;&#22330;&#26223;&#19979;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#22312;&#32447;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#35757;&#32451;&#30701;-&#27979;&#35797;&#38271;&#65288;TSTL&#65289;&#22330;&#26223;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;Rotary Position Embedding&#65288;RoPE&#65289;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#22312;&#36739;&#30701;&#24207;&#21015;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#36739;&#38271;&#24207;&#21015;&#20013;&#36935;&#21040;&#20301;&#32622;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Resonance RoPE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;RoPE&#29305;&#24449;&#30340;&#25554;&#20540;&#26469;&#32553;&#23567;TSTL&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#22312;&#32447;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PosGen&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#21512;&#25104;&#22522;&#20934;&#65292;&#19987;&#38376;&#38024;&#23545;TSTL&#22330;&#26223;&#20013;&#30340;&#31934;&#32454;&#34892;&#20026;&#20998;&#26512;&#65292;&#26088;&#22312;&#20174;&#38271;&#19978;&#19979;&#25991;&#20013;&#19981;&#26029;&#22686;&#21152;&#30340;&#20196;&#29260;&#29983;&#25104;&#22256;&#38590;&#21644;&#35782;&#21035;&#26032;&#20196;&#29260;&#20301;&#32622;&#30340;&#25361;&#25112;&#20013;&#20998;&#31163;&#20986;&#26469;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24212;&#29992;Resonance RoPE&#21518;&#65292;Transformer&#27169;&#22411;&#21487;&#20197;&#35782;&#21035;OOD&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00071v1 Announce Type: cross  Abstract: This paper addresses the challenge of train-short-test-long (TSTL) scenarios in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE), where models pre-trained on shorter sequences face difficulty with out-of-distribution (OOD) token positions in longer sequences. We introduce Resonance RoPE, a novel approach designed to narrow the generalization gap in TSTL scenarios by refining the interpolation of RoPE features for OOD positions, significantly improving the model performance without additional online computational costs. Furthermore, we present PosGen, a new synthetic benchmark specifically designed for fine-grained behavior analysis in TSTL scenarios, aiming to isolate the constantly increasing difficulty of token generation on long contexts from the challenges of recognizing new token positions. Our experiments on synthetic tasks show that after applying Resonance RoPE, Transformers recognize OOD position bet
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23558;&#30456;&#21516;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#26597;&#35810;&#32452;&#21512;&#20026;&#21333;&#20010;&#25552;&#31034;&#65292;&#20197;&#26368;&#23567;&#21270;&#37325;&#22797;&#35843;&#29992;&#26469;&#20248;&#21270;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20250;&#35758;&#25688;&#35201;&#20013;&#30340;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.00067</link><description>&lt;p&gt;
Query-OPT&#65306;&#36890;&#36807;&#22810;&#26597;&#35810;&#25351;&#20196;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20250;&#35758;&#25688;&#35201;&#20013;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23558;&#30456;&#21516;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#26597;&#35810;&#32452;&#21512;&#20026;&#21333;&#20010;&#25552;&#31034;&#65292;&#20197;&#26368;&#23567;&#21270;&#37325;&#22797;&#35843;&#29992;&#26469;&#20248;&#21270;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20250;&#35758;&#25688;&#35201;&#20013;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20851;&#27880;&#22522;&#20110;&#26597;&#35810;&#30340;&#20250;&#35758;&#25688;&#35201;&#20219;&#21153;&#65292;&#22312;&#27492;&#20219;&#21153;&#20013;&#65292;&#38024;&#23545;&#29305;&#23450;&#26597;&#35810;&#23545;&#19978;&#19979;&#25991;&#65288;&#20250;&#35758;&#35760;&#24405;&#65289;&#29983;&#25104;&#25688;&#35201;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#27492;&#20219;&#21153;&#26102;&#65292;&#21363;&#20351;&#19978;&#19979;&#25991;&#20445;&#25345;&#19981;&#21464;&#65292;&#27599;&#20010;&#26032;&#26597;&#35810;&#20063;&#38656;&#35201;&#23545;LLM&#25512;&#29702;&#31471;&#28857;/API&#36827;&#34892;&#19968;&#27425;&#26032;&#35843;&#29992;&#12290;&#28982;&#32780;&#65292;&#21453;&#22797;&#35843;&#29992;LLM&#25512;&#29702;&#31471;&#28857;&#20250;&#26174;&#33879;&#22686;&#21152;&#22312;&#29983;&#20135;&#20013;&#20351;&#29992;&#23427;&#20204;&#30340;&#25104;&#26412;&#65292;&#36825;&#20351;&#24471;&#35768;&#22810;&#23454;&#38469;&#29992;&#20363;&#20013;LLMs&#37117;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#25104;&#21151;&#22320;&#23558;&#30456;&#21516;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#26597;&#35810;&#32452;&#21512;&#20026;&#21333;&#20010;&#25552;&#31034;&#20197;&#26368;&#23567;&#21270;&#37325;&#22797;&#35843;&#29992;&#65292;&#22312;&#20250;&#35758;&#25688;&#35201;&#20013;&#20351;&#29992;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#21508;&#31181;&#27969;&#34892;&#30340;LLM&#65288;GPT-4&#12289;PaLM-2&#12289;LLaMA-2&#12289;Mistral&#21644;FLAN-T5&#65289;&#22312;&#21333;&#26597;&#35810;&#21644;&#22810;&#26597;&#35810;&#35774;&#32622;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00067v1 Announce Type: new  Abstract: This work focuses on the task of query-based meeting summarization in which the summary of a context (meeting transcript) is generated in response to a specific query. When using Large Language Models (LLMs) for this task, a new call to the LLM inference endpoint/API is required for each new query even if the context stays the same. However, repeated calls to the LLM inference endpoints would significantly increase the costs of using them in production, making LLMs impractical for many real-world use cases. To address this problem, in this paper, we investigate whether combining the queries for the same input context in a single prompt to minimize repeated calls can be successfully used in meeting summarization. In this regard, we conduct extensive experiments by comparing the performance of various popular LLMs: GPT-4, PaLM-2, LLaMA-2, Mistral, and FLAN-T5 in single-query and multi-query settings. We observe that while most LLMs tend to
&lt;/p&gt;</description></item><item><title>SEED&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sample-Efficient adaptation with Error-Driven learning&#30340;&#26032;&#39062;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#20316;&#20026;&#23398;&#20064;&#26426;&#20250;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.00046</link><description>&lt;p&gt;
&#20351;&#29992;&#26679;&#26412;&#39640;&#25928;&#36866;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#23450;&#20041;&#20197;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SEED: Customize Large Language Models with Sample-Efficient Adaptation for Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00046
&lt;/p&gt;
&lt;p&gt;
SEED&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sample-Efficient adaptation with Error-Driven learning&#30340;&#26032;&#39062;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#20316;&#20026;&#23398;&#20064;&#26426;&#20250;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#38656;&#35201;&#35843;&#25972;LLMs&#20197;&#28385;&#36275;&#29305;&#23450;&#38656;&#27714;&#65292;&#20294;&#23454;&#38469;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#65292;&#23548;&#33268;&#20195;&#30721;&#29983;&#25104;&#24615;&#33021;&#36739;&#24046;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#35843;&#25972;LLMs&#20197;&#36866;&#24212;&#26032;&#22330;&#26223;&#24182;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#26159;&#24403;&#21069;&#20195;&#30721;&#29983;&#25104;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEED&#30340;&#26032;&#39062;&#36866;&#24212;&#26041;&#27861;&#65292;&#21363;Sample-Efficient adaptation with Error-Driven learning for code generation&#12290;SEED&#21033;&#29992;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#20316;&#20026;&#23398;&#20064;&#26426;&#20250;&#65292;&#21033;&#29992;&#38169;&#35823;&#20462;&#35746;&#26469;&#20811;&#26381;&#33258;&#36523;&#32570;&#28857;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SEED&#28041;&#21450;&#35782;&#21035;LLMs&#29983;&#25104;&#30340;&#38169;&#35823;&#20195;&#30721;&#65292;&#20351;&#29992;Self-revise&#36827;&#34892;&#20195;&#30721;&#20462;&#35746;&#65292;&#20248;&#21270;&#27169;&#22411;&#24182;&#36845;&#20195;&#22320;&#36827;&#34892;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00046v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have made significant progress in code generation, they still struggle with code generation tasks in specific scenarios. These scenarios usually necessitate the adaptation of LLMs to fulfill specific needs, but the limited training data available in practice leads to poor code generation performance. How to effectively adapt LLMs to new scenarios with fewer training samples is a major challenge for current code generation. In this paper, we propose a novel adaptation approach named SEED, which stands for Sample-Efficient adaptation with Error-Driven learning for code generation. SEED leverages the errors made by LLMs as learning opportunities, using error revision to overcome its own shortcomings, thus achieving efficient learning. Specifically, SEED involves identifying error code generated by LLMs, employing Self-revise for code revision, optimizing the model with revised code, and iteratively ad
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#26410;&#30693;&#20107;&#20214;&#30340;&#36866;&#24212;&#24615;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;FADE&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22686;&#24378;&#21644;&#22270;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30446;&#26631;&#39044;&#27979;&#22120;&#65292;&#21516;&#26102;&#29420;&#31435;&#35757;&#32451;&#20107;&#20214;&#39044;&#27979;&#22120;&#65292;&#26368;&#32456;&#20943;&#36731;&#20107;&#20214;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.00037</link><description>&lt;p&gt;
&#26410;&#26469;&#21457;&#23637;&#65306;&#31038;&#20132;&#23186;&#20307;&#19978;&#30475;&#19981;&#35265;&#20107;&#20214;&#30340;&#36866;&#24212;&#24615;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Evolving to the Future: Unseen Event Adaptive Fake News Detection on Social Media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00037
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#26410;&#30693;&#20107;&#20214;&#30340;&#36866;&#24212;&#24615;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;FADE&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22686;&#24378;&#21644;&#22270;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30446;&#26631;&#39044;&#27979;&#22120;&#65292;&#21516;&#26102;&#29420;&#31435;&#35757;&#32451;&#20107;&#20214;&#39044;&#27979;&#22120;&#65292;&#26368;&#32456;&#20943;&#36731;&#20107;&#20214;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20551;&#26032;&#38395;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24191;&#27867;&#20256;&#25773;&#26085;&#30410;&#23041;&#32961;&#20010;&#20154;&#21644;&#31038;&#20250;&#12290;&#22312;&#31038;&#20132;&#23186;&#20307;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#20551;&#26032;&#38395;&#26816;&#27979;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#26032;&#38395;&#25253;&#36947;&#36807;&#21435;&#20107;&#20214;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#30446;&#26631;&#26159;&#39044;&#27979;&#21644;&#35782;&#21035;&#26377;&#20851;&#26410;&#26469;&#20107;&#20214;&#30340;&#20551;&#26032;&#38395;&#65292;&#36825;&#20123;&#20107;&#20214;&#36890;&#24120;&#19982;&#36807;&#21435;&#23436;&#20840;&#19981;&#21516;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#23384;&#22312;&#40065;&#26834;&#24615;&#19981;&#36275;&#65292;&#26080;&#27861;&#27867;&#21270;&#21040;&#30475;&#19981;&#35265;&#30340;&#20107;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#26410;&#26469;&#33258;&#36866;&#24212;&#20107;&#20214;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#65288;FADE&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#36866;&#24212;&#22686;&#24378;&#31574;&#30053;&#21644;&#22270;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30446;&#26631;&#39044;&#27979;&#22120;&#65292;&#20197;&#36827;&#34892;&#26356;&#31283;&#20581;&#30340;&#25972;&#20307;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#29420;&#31435;&#35757;&#32451;&#19968;&#20010;&#20165;&#20107;&#20214;&#30340;&#39044;&#27979;&#22120;&#20197;&#33719;&#24471;&#26377;&#20559;&#35265;&#30340;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#33719;&#24471;&#26368;&#32456;&#39044;&#27979;&#26469;&#36827;&#19968;&#27493;&#20943;&#36731;&#20107;&#20214;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00037v1 Announce Type: cross  Abstract: With the rapid development of social media, the wide dissemination of fake news on social media is increasingly threatening both individuals and society. In the dynamic landscape of social media, fake news detection aims to develop a model trained on news reporting past events. The objective is to predict and identify fake news about future events, which often relate to subjects entirely different from those in the past. However, existing fake detection methods exhibit a lack of robustness and cannot generalize to unseen events. To address this, we introduce Future ADaptive Event-based Fake news Detection (FADE) framework. Specifically, we train a target predictor through an adaptive augmentation strategy and graph contrastive learning to make more robust overall predictions. Simultaneously, we independently train an event-only predictor to obtain biased predictions. Then we further mitigate event bias by obtaining the final prediction
&lt;/p&gt;</description></item><item><title>TV-TREES&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19467</link><description>&lt;p&gt;
TV-TREES&#65306;&#29992;&#20110;&#31070;&#32463;&#31526;&#21495;&#35270;&#39057;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;
&lt;/p&gt;
&lt;p&gt;
TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19467
&lt;/p&gt;
&lt;p&gt;
TV-TREES&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#30005;&#35270;&#21098;&#36753;&#31561;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20869;&#23481;&#36827;&#34892;&#38382;&#31572;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#37096;&#20998;&#26159;&#22240;&#20026;&#24403;&#21069;&#30340;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20381;&#36182;&#20110;&#21333;&#27169;&#24577;&#25512;&#29702;&#65292;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TV-TREES&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#12290;TV-TREES&#20316;&#20026;&#19968;&#31181;&#20419;&#36827;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#30340;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#20219;&#21153;&#26469;&#35780;&#20272;&#27492;&#31867;&#26041;&#27861;&#30340;&#25512;&#29702;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#30340;&#12289;&#20855;&#26377;&#26368;&#20808;&#36827;&#38646;-shot&#24615;&#33021;&#30340;&#23436;&#25972;&#35270;&#39057;&#21098;&#36753;&#65292;&#23637;&#31034;&#20102;&#19982;&#40657;&#30418;&#26041;&#27861;&#30456;&#27604;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19467v1 Announce Type: cross  Abstract: It is challenging to perform question-answering over complex, multimodal content such as television clips. This is in part because current video-language models rely on single-modality reasoning, have lowered performance on long inputs, and lack interpetability. We propose TV-TREES, the first multimodal entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by producing trees of entailment relationships between simple premises directly entailed by the videos and higher-level conclusions. We then introduce the task of multimodal entailment tree generation to evaluate the reasoning quality of such methods. Our method's experimental results on the challenging TVQA dataset demonstrate intepretable, state-of-the-art zero-shot performance on full video clips, illustrating a best of both worlds contrast to black-box methods.
&lt;/p&gt;</description></item><item><title>$\texttt{COSMIC}$&#26159;&#19968;&#31181;&#20197;&#30456;&#20114;&#20449;&#24687;&#20026;&#22522;&#30784;&#30340;&#26032;&#30340;&#25688;&#35201;&#35780;&#20272;&#26041;&#27861;&#65292;&#26377;&#25928;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#24378;&#12290;&#31454;&#20105;&#24615;&#33021;&#20248;&#20110;$\texttt{BERTScore}$&#21644;$\texttt{ROUGE}$&#12290;</title><link>https://arxiv.org/abs/2402.19457</link><description>&lt;p&gt;
$\texttt{COSMIC}$: &#30456;&#20114;&#20449;&#24687;&#29992;&#20110;&#20219;&#21153;&#26080;&#20851;&#25688;&#35201;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
$\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19457
&lt;/p&gt;
&lt;p&gt;
$\texttt{COSMIC}$&#26159;&#19968;&#31181;&#20197;&#30456;&#20114;&#20449;&#24687;&#20026;&#22522;&#30784;&#30340;&#26032;&#30340;&#25688;&#35201;&#35780;&#20272;&#26041;&#27861;&#65292;&#26377;&#25928;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#24378;&#12290;&#31454;&#20105;&#24615;&#33021;&#20248;&#20110;$\texttt{BERTScore}$&#21644;$\texttt{ROUGE}$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#24635;&#32467;&#36136;&#37327;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#26681;&#25454;&#24635;&#32467;&#22120;&#29983;&#25104;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#29992;&#19988;&#20445;&#30041;&#20219;&#21153;&#32467;&#26524;&#30340;&#25688;&#35201;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#24314;&#31435;&#20102;&#36825;&#20123;&#20219;&#21153;&#30340;&#32467;&#26524;&#38169;&#35823;&#27010;&#29575;&#19982;&#28304;&#25991;&#26412;&#21644;&#29983;&#25104;&#25688;&#35201;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;$\texttt{COSMIC}$&#20316;&#20026;&#36825;&#19968;&#24230;&#37327;&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#23637;&#31034;&#20102;&#23427;&#19982;&#22522;&#20110;&#20154;&#31867;&#21028;&#26029;&#30340;&#24230;&#37327;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#23427;&#22312;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23545;&#24050;&#24314;&#31435;&#30340;&#24230;&#37327;&#22914;$\texttt{BERTScore}$&#21644;$\texttt{ROUGE}$&#30340;&#27604;&#36739;&#20998;&#26512;&#20984;&#26174;&#20102;$\texttt{COSMIC}$&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19457v1 Announce Type: cross  Abstract: Assessing the quality of summarizers poses significant challenges. In response, we propose a novel task-oriented evaluation approach that assesses summarizers based on their capacity to produce summaries that are useful for downstream tasks, while preserving task outcomes. We theoretically establish a direct relationship between the resulting error probability of these tasks and the mutual information between source texts and generated summaries. We introduce $\texttt{COSMIC}$ as a practical implementation of this metric, demonstrating its strong correlation with human judgment-based metrics and its effectiveness in predicting downstream task performance. Comparative analyses against established metrics like $\texttt{BERTScore}$ and $\texttt{ROUGE}$ highlight the competitive performance of $\texttt{COSMIC}$.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#65292;&#20174;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#30340;&#35282;&#24230;&#36830;&#25509;&#36755;&#20837;&#25991;&#27573;&#21644;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.19350</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#30340;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#65292;&#20174;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#30340;&#35282;&#24230;&#36830;&#25509;&#36755;&#20837;&#25991;&#27573;&#21644;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21033;&#29992;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#27169;&#25311;&#20154;&#31867;&#25512;&#29702;&#21644;&#25512;&#26029;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#36339;QA&#26041;&#38754;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#26102;&#65292;PLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#20154;&#31867;&#20043;&#38388;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#24515;&#29702;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#65292;&#36755;&#20837;&#25991;&#27573;&#20013;&#30340;&#26174;&#24335;&#20449;&#24687;&#19982;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#20043;&#38388;&#23384;&#22312;&#37325;&#35201;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#26410;&#33021;&#20805;&#20998;&#20851;&#27880;&#20174;&#20154;&#31867;&#35748;&#30693;&#30740;&#31350;&#30340;&#35282;&#24230;&#38142;&#25509;&#36755;&#20837;&#25991;&#27573;&#21644;&#22522;&#20110;PLMs&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#65288;PEI&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#25552;&#31034;&#36830;&#25509;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#65292;&#19982;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#23545;&#40784;&#65292;&#29992;&#20110;&#22810;&#36339;QA&#12290;&#25105;&#20204;&#23558;&#36755;&#20837;&#25991;&#27573;&#35270;&#20026;&#26174;&#24335;&#30693;&#35782;&#65292;&#21033;&#29992;&#23427;&#20204;&#36890;&#36807;&#32479;&#19968;&#25552;&#31034;&#25512;&#23548;&#38544;&#24335;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19350v1 Announce Type: new  Abstract: Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to simulate human reasoning and inference processes, achieving proficient performance in multi-hop QA. However, a gap persists between PLMs' reasoning abilities and those of humans when tackling complex problems. Psychological studies suggest a vital connection between explicit information in passages and human prior knowledge during reading. Nevertheless, current research has given insufficient attention to linking input passages and PLMs' pre-training-based knowledge from the perspective of human cognition studies. In this study, we introduce a \textbf{P}rompting \textbf{E}xplicit and \textbf{I}mplicit knowledge (PEI) framework, which uses prompts to connect explicit and implicit knowledge, aligning with human reading process for multi-hop QA. We consider the input passages as explicit knowledge, employing them to elicit implicit knowledge through unified prompt reason
&lt;/p&gt;</description></item><item><title>WanJuan-CC&#26159;&#19968;&#20010;&#23433;&#20840;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;Common Crawl&#25968;&#25454;&#24182;&#32463;&#36807;&#22810;&#39033;&#31579;&#36873;&#21644;&#36807;&#28388;&#27493;&#39588;&#24471;&#21040;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.19282</link><description>&lt;p&gt;
WanJuan-CC&#65306;&#19968;&#20010;&#23433;&#20840;&#19988;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19282
&lt;/p&gt;
&lt;p&gt;
WanJuan-CC&#26159;&#19968;&#20010;&#23433;&#20840;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;Common Crawl&#25968;&#25454;&#24182;&#32463;&#36807;&#22810;&#39033;&#31579;&#36873;&#21644;&#36807;&#28388;&#27493;&#39588;&#24471;&#21040;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; WanJuan-CC&#65292;&#36825;&#26159;&#19968;&#20010;&#23433;&#20840;&#19988;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#26469;&#28304;&#20110;Common Crawl&#25968;&#25454;&#12290;&#30740;&#31350;&#35299;&#20915;&#20102;&#20026;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#25968;&#25454;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#27969;&#31243;&#26469;&#22788;&#29702;Common Crawl&#25968;&#25454;&#65292;&#21253;&#25324;&#25552;&#21462;&#12289;&#21551;&#21457;&#24335;&#35268;&#21017;&#36807;&#28388;&#12289;&#27169;&#31946;&#21435;&#37325;&#12289;&#20869;&#23481;&#23433;&#20840;&#36807;&#28388;&#21644;&#25968;&#25454;&#36136;&#37327;&#36807;&#28388;&#12290;&#20174;&#22823;&#32422;680&#20159;&#20010;&#21407;&#22987;&#33521;&#25991;&#25991;&#26723;&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;22&#19975;&#20159;&#26631;&#35760;&#30340;&#23433;&#20840;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#36873;&#20986;&#20102;10&#19975;&#20159;&#26631;&#35760;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#20316;&#20026;WanJuan-CC&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#24050;&#32463;&#24320;&#28304;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;3000&#20159;&#26631;&#35760;&#12290;&#35813;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#19982;&#25968;&#25454;&#36136;&#37327;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#36873;&#25321;&#36866;&#24403;&#30340;&#25968;&#25454;&#12290;&#20026;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;WanJuan-CC&#35757;&#32451;&#20102;10&#20159;&#21442;&#25968;&#21644;30&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19282v1 Announce Type: new  Abstract: This paper presents WanJuan-CC, a safe and high-quality open-sourced English webtext dataset derived from Common Crawl data. The study addresses the challenges of constructing large-scale pre-training datasets for language models, which require vast amounts of high-quality data. A comprehensive process was designed to handle Common Crawl data, including extraction, heuristic rule filtering, fuzzy deduplication, content safety filtering, and data quality filtering. From approximately 68 billion original English documents, we obtained 2.22T Tokens of safe data and selected 1.0T Tokens of high-quality data as part of WanJuan-CC. We have open-sourced 300B Tokens from this dataset. The paper also provides statistical information related to data quality, enabling users to select appropriate data according to their needs. To evaluate the quality and utility of the dataset, we trained 1B-parameter and 3B-parameter models using WanJuan-CC and ano
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#24207;&#30340;&#25935;&#24863;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#35789;&#24207;&#20449;&#24687;&#37327;&#65292;&#21457;&#29616;&#22312;&#35821;&#35328;&#25552;&#31034;&#25552;&#20379;&#20887;&#20313;&#20449;&#24687;&#26102;&#65292;&#27169;&#22411;&#23545;&#35789;&#24207;&#21464;&#21270;&#19981;&#25935;&#24863;&#65292;&#19988;&#19981;&#21516;&#20219;&#21153;&#38388;&#30340;&#19981;&#25935;&#24863;&#31243;&#24230;&#26377;&#25152;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.18838</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#35789;&#24207;&#37325;&#35201;&#65292;&#20160;&#20040;&#26102;&#20505;&#19981;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
When does word order matter and when doesn't it?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#24207;&#30340;&#25935;&#24863;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#35789;&#24207;&#20449;&#24687;&#37327;&#65292;&#21457;&#29616;&#22312;&#35821;&#35328;&#25552;&#31034;&#25552;&#20379;&#20887;&#20313;&#20449;&#24687;&#26102;&#65292;&#27169;&#22411;&#23545;&#35789;&#24207;&#21464;&#21270;&#19981;&#25935;&#24863;&#65292;&#19988;&#19981;&#21516;&#20219;&#21153;&#38388;&#30340;&#19981;&#25935;&#24863;&#31243;&#24230;&#26377;&#25152;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#33021;&#23545;&#35789;&#24207;&#21464;&#21270;&#19981;&#25935;&#24863;&#12290;&#26412;&#25991;&#25552;&#20986;&#35821;&#35328;&#20887;&#20313;&#24615;&#21487;&#20197;&#35299;&#37322;&#36825;&#19968;&#29616;&#35937;&#65292;&#21363;&#35789;&#24207;&#21644;&#20854;&#20182;&#35821;&#35328;&#25552;&#31034;&#65288;&#22914;&#26684;&#26631;&#65289;&#25552;&#20379;&#37325;&#21472;&#19988;&#20887;&#20313;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#24403;&#39034;&#24207;&#25552;&#20379;&#20887;&#20313;&#20449;&#24687;&#26102;&#65292;&#27169;&#22411;&#23545;&#35789;&#24207;&#30340;&#19981;&#25935;&#24863;&#34920;&#29616;&#65292;&#32780;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#25935;&#24863;&#31243;&#24230;&#21508;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#20351;&#29992;&#26080;&#24207;&#21644;&#25171;&#20081;&#39034;&#24207;&#30340;&#21477;&#23376;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#26469;&#37327;&#21270;&#35789;&#24207;&#30340;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#35789;&#24207;&#20449;&#24687;&#36234;&#19981;&#20855;&#20449;&#24687;&#37327;&#65292;&#27169;&#22411;&#22312;&#26080;&#24207;&#21644;&#25171;&#20081;&#39034;&#24207;&#30340;&#21477;&#23376;&#20043;&#38388;&#30340;&#39044;&#27979;&#36234;&#19968;&#33268;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#36825;&#31181;&#24433;&#21709;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65306;&#23545;&#20110;&#19968;&#20123;&#20219;&#21153;&#65292;&#22914;SST-2&#65292;LM&#30340;&#39044;&#27979;&#20960;&#20046;&#24635;&#26159;&#19982;&#21407;&#22987;&#32467;&#26524;&#19968;&#33268;&#65292;&#21363;&#20351;&#28857;&#38388;&#20114;&#20449;&#24687;&#65288;PMI&#65289;&#21457;&#29983;&#21464;&#21270;&#65292;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18838v1 Announce Type: new  Abstract: Language models (LMs) may appear insensitive to word order changes in natural language understanding (NLU) tasks. In this paper, we propose that linguistic redundancy can explain this phenomenon, whereby word order and other linguistic cues such as case markers provide overlapping and thus redundant information. Our hypothesis is that models exhibit insensitivity to word order when the order provides redundant information, and the degree of insensitivity varies across tasks. We quantify how informative word order is using mutual information (MI) between unscrambled and scrambled sentences. Our results show the effect that the less informative word order is, the more consistent the model's predictions are between unscrambled and scrambled sentences. We also find that the effect varies across tasks: for some tasks, like SST-2, LMs' prediction is almost always consistent with the original one even if the Pointwise-MI (PMI) changes, while fo
&lt;/p&gt;</description></item><item><title>RORA &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#23545;&#26631;&#31614;&#30340;&#26032;&#20449;&#24687;&#36129;&#29486;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18678</link><description>&lt;p&gt;
RORA&#65306;&#24378;&#22823;&#30340;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RORA: Robust Free-Text Rationale Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18678
&lt;/p&gt;
&lt;p&gt;
RORA &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#23545;&#26631;&#31614;&#30340;&#26032;&#20449;&#24687;&#36129;&#29486;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#22312;&#21487;&#35299;&#37322;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24357;&#21512;&#20102;&#27169;&#22411;&#20915;&#31574;&#32972;&#21518;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28508;&#22312;&#25512;&#29702;&#36335;&#24452;&#30340;&#22810;&#26679;&#24615;&#21450;&#30456;&#24212;&#32570;&#20047;&#26126;&#30830;&#30340;&#30495;&#30456;&#22522;&#30784;&#65292;&#20854;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#20381;&#36182;&#20110;&#29702;&#30001;&#25903;&#25345;&#30446;&#26631;&#26631;&#31614;&#30340;&#31243;&#24230;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25351;&#26631;&#22312;&#35780;&#20272;&#24847;&#22806;&#27844;&#28431;&#26631;&#31614;&#30340;&#29702;&#30001;&#26102;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RORA&#65292;&#19968;&#31181;&#38024;&#23545;&#26631;&#31614;&#27844;&#28431;&#30340;&#24378;&#22823;&#30340;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#35780;&#20272;&#26041;&#27861;&#12290;RORA&#37327;&#21270;&#20102;&#29702;&#30001;&#20026;&#35777;&#26126;&#26631;&#31614;&#25552;&#20379;&#30340;&#26032;&#20449;&#24687;&#12290;&#36825;&#26159;&#36890;&#36807;&#35780;&#20272;&#19982;&#27844;&#28431;&#29305;&#24449;&#25239;&#24178;&#25200;&#30340;&#39044;&#27979;&#24615;&#23478;&#26063;&#26465;&#20214;V&#20449;&#24687;&#23454;&#29616;&#30340;&#12290;RORA&#22312;&#35780;&#20272;&#20154;&#24037;&#25776;&#20889;&#12289;&#21512;&#25104;&#25110;&#27169;&#22411;&#29983;&#25104;&#30340;&#29702;&#30001;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18678v1 Announce Type: new  Abstract: Free-text rationales play a pivotal role in explainable NLP, bridging the knowledge and reasoning gaps behind a model's decision-making. However, due to the diversity of potential reasoning paths and a corresponding lack of definitive ground truth, their evaluation remains a challenge. Existing evaluation metrics rely on the degree to which a rationale supports a target label, but we find these fall short in evaluating rationales that inadvertently leak the labels. To address this problem, we propose RORA, a Robust free-text Rationale evaluation against label leakage. RORA quantifies the new information supplied by a rationale to justify the label. This is achieved by assessing the conditional V-information \citep{hewitt-etal-2021-conditional} with a predictive family robust against leaky features that can be exploited by a small model. RORA consistently outperforms existing approaches in evaluating human-written, synthetic, or model-gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;LLMs&#36328;&#35821;&#35328;&#21644;&#22320;&#29702;&#21306;&#22495;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#24182;&#21457;&#29616;&#33521;&#35821;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#29983;&#25104;&#20107;&#23454;&#25968;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#35821;&#35328;&#65292;&#21516;&#26102;&#22810;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#23545;&#26469;&#33258;&#35199;&#26041;&#22823;&#38470;&#20107;&#23454;&#20449;&#24687;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.18045</link><description>&lt;p&gt;
Multi-FAct: &#20351;&#29992;FActScore&#35780;&#20272;&#22810;&#35821;&#35328;LLM&#30340;&#22810;&#21306;&#22495;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;LLMs&#36328;&#35821;&#35328;&#21644;&#22320;&#29702;&#21306;&#22495;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#24182;&#21457;&#29616;&#33521;&#35821;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#29983;&#25104;&#20107;&#23454;&#25968;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#35821;&#35328;&#65292;&#21516;&#26102;&#22810;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#23545;&#26469;&#33258;&#35199;&#26041;&#22823;&#38470;&#20107;&#23454;&#20449;&#24687;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#20986;&#29616;&#20107;&#23454;&#19978;&#30340;&#24187;&#35273;&#65292;&#29983;&#25104;&#19982;&#24050;&#30693;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#25991;&#26412;&#12290;&#23613;&#31649;&#24191;&#27867;&#30740;&#31350;&#20102;&#33521;&#35821;&#20013;&#30340;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#22810;&#35821;&#35328;LLMs&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#31995;&#32479;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;LLMs&#36328;&#35821;&#35328;&#21644;&#22320;&#29702;&#21306;&#22495;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#35821;&#35328;&#20107;&#23454;&#35780;&#20272;&#27969;&#31243;&#65292;&#23558;FActScore&#65288;Min&#31561;&#65292;2023&#65289;&#25913;&#32534;&#20026;&#22810;&#26679;&#21270;&#35821;&#35328;&#12290;&#25105;&#20204;&#22312;&#20061;&#31181;&#35821;&#35328;&#19978;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#33521;&#35821;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#29983;&#25104;&#20107;&#23454;&#25968;&#37327;&#26041;&#38754;&#22987;&#32456;&#34920;&#29616;&#20248;&#24322;&#12290;&#27492;&#22806;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#23545;&#26469;&#33258;&#35199;&#26041;&#22823;&#38470;&#30340;&#20107;&#23454;&#20449;&#24687;&#30340;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#23545;&#25913;&#36827;&#22810;&#35821;&#35328;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#38656;&#27714;&#65292;&#24182;&#24378;&#35843;&#20102;LLMs&#30340;&#20107;&#23454;&#29983;&#25104;&#20013;&#30340;&#22320;&#29702;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18045v1 Announce Type: new  Abstract: Large Language Models (LLMs) are prone to factuality hallucination, generating text that contradicts established knowledge. While extensive research has addressed this in English, little is known about multilingual LLMs. This paper systematically evaluates multilingual LLMs' factual accuracy across languages and geographic regions. We introduce a novel pipeline for multilingual factuality evaluation, adapting FActScore(Min et al., 2023) for diverse languages. Our analysis across nine languages reveals that English consistently outperforms others in factual accuracy and quantity of generated facts. Furthermore, multilingual models demonstrate a bias towards factual information from Western continents. These findings highlight the need for improved multilingual factuality assessment and underscore geographical biases in LLMs' fact generation.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20851;&#38190;&#25216;&#26415;&#12289;&#25351;&#26631;&#12289;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.17944</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;--&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models on Tabular Data -- A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17944
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20851;&#38190;&#25216;&#26415;&#12289;&#25351;&#26631;&#12289;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#26041;&#38754;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#65292;&#21253;&#25324;&#39044;&#27979;&#12289;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#12289;&#38382;&#31572;&#21644;&#34920;&#26684;&#29702;&#35299;&#31561;&#22810;&#31181;&#20219;&#21153;&#12290;&#27599;&#20010;&#20219;&#21153;&#37117;&#24102;&#26469;&#29420;&#29305;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#23545;&#35813;&#30740;&#31350;&#39046;&#22495;&#20013;&#20851;&#38190;&#25216;&#26415;&#12289;&#25351;&#26631;&#12289;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#20248;&#21270;&#26041;&#27861;&#30340;&#20840;&#38754;&#23457;&#26597;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24635;&#32467;&#24182;&#27604;&#36739;&#36825;&#20123;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20379;&#23545;&#25968;&#25454;&#38598;&#12289;&#25351;&#26631;&#21644;&#26041;&#27861;&#35770;&#30340;&#20840;&#38754;&#35843;&#26597;&#21644;&#20998;&#31867;&#12290;&#23427;&#35782;&#21035;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#20248;&#21183;&#12289;&#23616;&#38480;&#24615;&#12289;&#26410;&#24320;&#21457;&#39046;&#22495;&#21644;&#31354;&#30333;&#65292;&#21516;&#26102;&#20026;&#36825;&#19968;&#37325;&#35201;&#19988;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#24341;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17944v1 Announce Type: new  Abstract: Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehen
&lt;/p&gt;</description></item><item><title>&#23558;&#24694;&#24847;&#25552;&#31034;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#25552;&#31034;&#20351;&#24471;LLM&#36234;&#29425;&#25915;&#20987;&#26356;&#38590;&#34987;&#26816;&#27979;</title><link>https://arxiv.org/abs/2402.16914</link><description>&lt;p&gt;
DrAttack: &#25552;&#31034;&#20998;&#35299;&#21644;&#37325;&#26500;&#20351;&#24378;&#22823;&#30340;LLM&#36234;&#29425;&#32773;
&lt;/p&gt;
&lt;p&gt;
DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16914
&lt;/p&gt;
&lt;p&gt;
&#23558;&#24694;&#24847;&#25552;&#31034;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#25552;&#31034;&#20351;&#24471;LLM&#36234;&#29425;&#25915;&#20987;&#26356;&#38590;&#34987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#23558;&#24694;&#24847;&#25552;&#31034;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#25552;&#31034;&#33021;&#22815;&#26377;&#25928;&#27169;&#31946;&#20854;&#28508;&#22312;&#30340;&#24694;&#24847;&#24847;&#22270;&#65292;&#20351;&#20043;&#20197;&#29255;&#27573;&#21270;&#12289;&#19981;&#26131;&#26816;&#27979;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#36234;&#29425;&#25915;&#20987;&#30340;&#33258;&#21160;&#25552;&#31034;&#20998;&#35299;&#21644;&#37325;&#26500;&#26694;&#26550;&#65288;DrAttack&#65289;&#12290;DrAttack&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;(a) &#23558;&#21407;&#22987;&#25552;&#31034;&#36827;&#34892;&#8220;&#20998;&#35299;&#8221;&#20026;&#23376;&#25552;&#31034;&#65292;(b) &#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#35821;&#20041;&#19978;&#30456;&#20284;&#20294;&#38544;&#21547;&#30340;&#8220;&#37325;&#26500;&#8221;&#36825;&#20123;&#23376;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16914v1 Announce Type: cross  Abstract: The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks, which adversarially trigger LLMs to output harmful content. However, current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned LLMs. This paper discovers that decomposing a malicious prompt into separated sub-prompts can effectively obscure its underlying malicious intent by presenting it in a fragmented, less detectable form, thereby addressing these limitations. We introduce an automatic prompt \textbf{D}ecomposition and \textbf{R}econstruction framework for jailbreak \textbf{Attack} (DrAttack). DrAttack includes three key components: (a) `Decomposition' of the original prompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly by in-context learning with semantically similar but h
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35780;&#20272;&#22120;&#20013;&#30340;&#20284;&#28982;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#36825;&#31181;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15987</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20559;&#24046;&#30340;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Likelihood-based Mitigation of Evaluation Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15987
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35780;&#20272;&#22120;&#20013;&#30340;&#20284;&#28982;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#36825;&#31181;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#20284;&#28982;&#20316;&#20026;&#34913;&#37327;LLM&#23545;&#21477;&#23376;&#21487;&#20449;&#24230;&#30340;&#25351;&#26631;&#65292;&#21487;&#33021;&#20250;&#22240;&#21477;&#23376;&#34920;&#38754;&#24046;&#24322;&#65288;&#22914;&#35789;&#24207;&#21644;&#21477;&#23376;&#32467;&#26500;&#65289;&#32780;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#23558;LLMs&#29992;&#20110;&#35780;&#20272;&#65292;&#21487;&#33021;&#23384;&#22312;&#20284;&#28982;&#20559;&#24046;&#65306;&#23427;&#20204;&#21487;&#33021;&#20250;&#39640;&#20272;&#20855;&#26377;&#36739;&#39640;&#20284;&#28982;&#24615;&#30340;&#21477;&#23376;&#65292;&#32780;&#20302;&#20272;&#20855;&#26377;&#36739;&#20302;&#20284;&#28982;&#24615;&#30340;&#21477;&#23376;&#12290;&#26412;&#25991;&#23545;LLM&#35780;&#20272;&#22120;&#20013;&#20284;&#28982;&#20559;&#24046;&#30340;&#23384;&#22312;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#20284;&#28982;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39640;&#24230;&#20559;&#32622;&#30340;&#23454;&#20363;&#20316;&#20026;&#23569;&#26679;&#26412;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#35780;&#20272;&#25968;&#25454;&#21040;&#25991;&#26412;&#21644;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#20219;&#21153;&#26102;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#27979;&#35797;&#30340;&#20960;&#31181;LLMs&#26174;&#31034;&#20986;&#20284;&#28982;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#20943;&#36731;&#20102;&#36825;&#31181;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15987v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are widely used to evaluate natural language generation tasks as automated metrics. However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods. In this paper, we investigate the presence and impact of likelihood bias in LLM-based evaluators. We also propose a method to mitigate the likelihood bias. Our method utilizes highly biased instances as few-shot examples for in-context learning. Our experiments in evaluating the data-to-text and grammatical error correction tasks reveal that several LLMs we test display a likelihood bias. Furthermore, our proposed method successfully mitigates this bias, also impr
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;MATHWELL&#27169;&#22411;&#29983;&#25104;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#33521;&#25991;&#25968;&#23398;&#24212;&#29992;&#39064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20,490&#20010;&#38382;&#39064;&#65292;&#32463;&#39046;&#22495;&#19987;&#23478;&#35780;&#20998;&#32467;&#26524;&#26174;&#31034;&#65292;MATHWELL&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#21487;&#25191;&#34892;&#35299;&#20915;&#26041;&#26696;&#24182;&#31526;&#21512;&#25152;&#26377;&#26631;&#20934;&#30340;&#20221;&#39069;&#27604;&#20854;&#20182;&#36873;&#25321;&#39640;&#20986;40%&#65292;&#20854;&#20013;74%&#30340;&#21487;&#35299;&#20915;&#38382;&#39064;&#21516;&#26102;&#20570;&#21040;&#20102;&#20934;&#30830;&#21644;&#36866;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.15861</link><description>&lt;p&gt;
MATHWELL: &#22312;&#35268;&#27169;&#19978;&#29983;&#25104;&#25945;&#32946;&#25968;&#23398;&#24212;&#29992;&#39064;
&lt;/p&gt;
&lt;p&gt;
MATHWELL: Generating Educational Math Word Problems at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15861
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;MATHWELL&#27169;&#22411;&#29983;&#25104;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#33521;&#25991;&#25968;&#23398;&#24212;&#29992;&#39064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20,490&#20010;&#38382;&#39064;&#65292;&#32463;&#39046;&#22495;&#19987;&#23478;&#35780;&#20998;&#32467;&#26524;&#26174;&#31034;&#65292;MATHWELL&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#21487;&#25191;&#34892;&#35299;&#20915;&#26041;&#26696;&#24182;&#31526;&#21512;&#25152;&#26377;&#26631;&#20934;&#30340;&#20221;&#39069;&#27604;&#20854;&#20182;&#36873;&#25321;&#39640;&#20986;40%&#65292;&#20854;&#20013;74%&#30340;&#21487;&#35299;&#20915;&#38382;&#39064;&#21516;&#26102;&#20570;&#21040;&#20102;&#20934;&#30830;&#21644;&#36866;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#24212;&#29992;&#39064;&#22312;K-8&#25945;&#32946;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#32534;&#20889;&#23427;&#20204;&#32791;&#26102;&#19988;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#35268;&#27169;&#21270;&#38382;&#39064;&#26469;&#25903;&#25345;K-8&#25968;&#23398;&#25945;&#32946;&#12290;&#20026;&#20102;&#25945;&#32946;&#24615;&#65292;&#29983;&#25104;&#30340;&#38382;&#39064;&#24517;&#39035;&#26159;1&#65289;&#21487;&#35299;&#20915;&#30340;&#65292;2&#65289;&#20934;&#30830;&#30340;&#65292;3&#65289;&#36866;&#24403;&#30340;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#26410;&#26631;&#35760;&#36825;&#20123;&#26631;&#20934;&#65292;&#22240;&#27492;&#19981;&#36866;&#21512;&#35757;&#32451;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MATHWELL&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#36845;&#20195;&#24494;&#35843;&#30340;70B Llama-2&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;K-8&#25968;&#23398;&#24212;&#29992;&#39064;&#12290;&#20511;&#21161;MATHWELL&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#33521;&#25991;&#24212;&#29992;&#39064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20,490&#20010;&#38382;&#39064;&#12290;&#32463;&#39046;&#22495;&#19987;&#23478;&#35780;&#20998;&#30340;3,484&#20010;&#38382;&#39064;&#21457;&#29616;&#65292;MATHWELL&#25317;&#26377;&#27604;&#20854;&#20182;&#36873;&#25321;&#26356;&#39640;&#30340;&#21487;&#25191;&#34892;&#35299;&#20915;&#26041;&#26696;&#21644;&#28385;&#36275;&#25152;&#26377;&#26631;&#20934;&#30340;&#38382;&#39064;&#20221;&#39069;&#39640;&#20986;40&#65285;&#65292;&#20854;&#20013;74&#65285;&#30340;&#38382;&#39064;&#20855;&#26377;&#21487;&#35299;&#30340;&#12289;&#20934;&#30830;&#30340;&#21644;&#36866;&#24403;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15861v1 Announce Type: new  Abstract: Math word problems are critical K-8 educational tools, but writing them is time-consuming and requires domain expertise. We suggest that language models can support K-8 math education by automatically generating problems at scale. To be educational, generated problems must be 1) solvable, 2) accurate, and 3) appropriate. Existing datasets are unlabeled for these criteria, making them ill-suited for training problem generators. We introduce MATHWELL, a Llama-2 (70B) model iteratively finetuned to generate K-8 math word problems using data from expert annotation. Using MATHWELL, we generate the largest English word problem dataset to date, containing 20,490 problems. 3,484 are scored by domain experts who find MATHWELL has a 40% higher share of problems that have executable solutions and meet all criteria than alternatives, with 74% of its problems with executable solutions being solvable, accurate, and appropriate.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Prejudice-Caprice Framework&#65288;PCF&#65289;&#26469;&#20840;&#38754;&#34913;&#37327;LLMs&#20013;&#30340;&#27495;&#35270;&#65292;&#32771;&#34385;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#19968;&#36143;&#20559;&#35265;&#20559;&#22909;&#21644;&#20559;&#22909;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.15481</link><description>&lt;p&gt;
&#20559;&#35265;&#21644;&#21453;&#22797;&#26080;&#24120;&#65306;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31038;&#20250;&#27495;&#35270;&#30340;&#32479;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15481
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Prejudice-Caprice Framework&#65288;PCF&#65289;&#26469;&#20840;&#38754;&#34913;&#37327;LLMs&#20013;&#30340;&#27495;&#35270;&#65292;&#32771;&#34385;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#19968;&#36143;&#20559;&#35265;&#20559;&#22909;&#21644;&#20559;&#22909;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15481v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31038;&#20250;&#36816;&#33829;&#20013;&#30340;&#26085;&#30410;&#34701;&#21512;&#21152;&#21095;&#20102;&#23427;&#20204;&#23545;&#32463;&#27982;&#12289;&#27861;&#24459;&#12289;&#25945;&#32946;&#21644;&#21307;&#30103;&#31561;&#37325;&#35201;&#39046;&#22495;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#24341;&#21457;&#20102;&#20844;&#20247;&#23545;&#36825;&#20123;&#27169;&#22411;&#28041;&#21450;&#27495;&#35270;&#23433;&#20840;&#21644;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#27495;&#35270;&#27979;&#37327;&#26694;&#26550;&#20165;&#35780;&#20272;LLMs&#30340;&#24179;&#22343;&#27495;&#35270;&#34892;&#20026;&#65292;&#24448;&#24448;&#30001;&#20110;&#24573;&#35270;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#23548;&#33268;&#27495;&#35270;&#30340;&#22240;&#32032;&#65292;&#21363;LLMs&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#39044;&#27979;&#21464;&#21270;&#32780;&#21464;&#24471;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prejudice-Caprice Framework&#65288;PCF&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;LLMs&#30340;&#19968;&#36143;&#20559;&#35265;&#20559;&#22909;&#21644;&#22312;&#22810;&#26679;&#19978;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15481v1 Announce Type: new  Abstract: The growing integration of large language models (LLMs) into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models' discrimination-related safety and reliability. However, prior discrimination measuring frameworks solely assess the average discriminatory behavior of LLMs, often proving inadequate due to the overlook of an additional discrimination-leading factor, i.e., the LLMs' prediction variation across diverse contexts. In this work, we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in LLMs by considering both their consistently biased preference and preference variation across diverse contexts. Specifically, we mathematically dissect the aggregated contextualized discrimination risk of LLMs into prejudice risk, originating from LLMs' persistent prejudice, and caprice risk, stemmin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25351;&#20196;&#20013;&#24515;&#21709;&#24212;&#30340;&#23481;&#24525;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22797;&#26434;&#26597;&#35810;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25581;&#31034;&#35302;&#21457;&#19981;&#36947;&#24503;&#21709;&#24212;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15302</link><description>&lt;p&gt;
&#26377;&#20851;LLMs&#25351;&#20196;&#20013;&#24515;&#21709;&#24212;&#30340;&#65288;&#19981;&#36947;&#24503;&#65289;&#31243;&#24230;&#26377;&#22810;&#39640;&#65311;&#25581;&#31034;&#23433;&#20840;&#38450;&#25252;&#26639;&#23545;&#26377;&#23475;&#26597;&#35810;&#30340;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25351;&#20196;&#20013;&#24515;&#21709;&#24212;&#30340;&#23481;&#24525;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22797;&#26434;&#26597;&#35810;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25581;&#31034;&#35302;&#21457;&#19981;&#36947;&#24503;&#21709;&#24212;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#22260;&#32469;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23433;&#20840;&#21644;&#36947;&#24503;&#20351;&#29992;&#26085;&#30410;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20250;&#34987;&#21508;&#31181;&#22797;&#26434;&#30340;&#26041;&#27861;&#27450;&#39575;&#65292;&#20135;&#29983;&#26377;&#23475;&#25110;&#19981;&#36947;&#24503;&#20869;&#23481;&#65292;&#21253;&#25324;&#8220;&#36234;&#29425;&#8221;&#25216;&#26415;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25805;&#32437;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#19978;&#65306;LLMs&#22312;&#35201;&#27714;&#23427;&#20204;&#29983;&#25104;&#20197;&#20266;&#20195;&#30721;&#12289;&#31243;&#24207;&#25110;&#36719;&#20214;&#29255;&#27573;&#20026;&#20013;&#24515;&#30340;&#21709;&#24212;&#26102;&#65292;&#26377;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#33021;&#20250;&#34987;&#35823;&#23548;&#65292;&#32780;&#19981;&#26159;&#29983;&#25104;&#26222;&#36890;&#25991;&#26412;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TechHazardQA&#65292;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#24212;&#20197;&#25991;&#26412;&#21644;&#20197;&#25351;&#20196;&#20026;&#20013;&#24515;&#26684;&#24335;&#65288;&#20363;&#22914;&#20266;&#20195;&#30721;&#65289;&#22238;&#31572;&#30340;&#22797;&#26434;&#26597;&#35810;&#65292;&#26088;&#22312;&#35782;&#21035;&#19981;&#36947;&#24503;&#21709;&#24212;&#30340;&#35302;&#21457;&#22120;&#12290;&#25105;&#20204;&#26597;&#35810;&#20102;&#19968;&#31995;&#21015;LLMs-- Llama-2-13b&#65292;Llama-2-7b&#65292;Mistral-V2&#21644;Mistral 8X7B--&#24182;&#35201;&#27714;&#23427;&#20204;&#29983;&#25104;&#25991;&#26412;&#21644;&#25351;&#20196;&#20026;&#20013;&#24515;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15302v1 Announce Type: new  Abstract: In this study, we tackle a growing concern around the safety and ethical use of large language models (LLMs). Despite their potential, these models can be tricked into producing harmful or unethical content through various sophisticated methods, including 'jailbreaking' techniques and targeted manipulation. Our work zeroes in on a specific issue: to what extent LLMs can be led astray by asking them to generate responses that are instruction-centric such as a pseudocode, a program or a software snippet as opposed to vanilla text. To investigate this question, we introduce TechHazardQA, a dataset containing complex queries which should be answered in both text and instruction-centric formats (e.g., pseudocodes), aimed at identifying triggers for unethical responses. We query a series of LLMs -- Llama-2-13b, Llama-2-7b, Mistral-V2 and Mistral 8X7B -- and ask them to generate both text and instruction-centric responses. For evaluation we rep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#27169;&#22411;&#32534;&#30721;&#32452;&#21512;&#24615;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#22312;&#32452;&#21512;&#24615;&#22522;&#20934;&#19978;&#21462;&#24471;&#36229;&#36807;10% &#30340;&#32477;&#23545;&#25913;&#36827;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#22312;&#26631;&#20934;&#23545;&#35937;&#35782;&#21035;&#21644;&#26816;&#32034;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15021</link><description>&lt;p&gt;
CLoVe: &#22312;&#23545;&#27604;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#32452;&#21512;&#24615;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#27169;&#22411;&#32534;&#30721;&#32452;&#21512;&#24615;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#22312;&#32452;&#21512;&#24615;&#22522;&#20934;&#19978;&#21462;&#24471;&#36229;&#36807;10% &#30340;&#32477;&#23545;&#25913;&#36827;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#22312;&#26631;&#20934;&#23545;&#35937;&#35782;&#21035;&#21644;&#26816;&#32034;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#30340;&#34920;&#29616;&#26174;&#33879;&#25552;&#21319;&#12290;&#22522;&#30784;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22914;CLIP&#24050;&#22312;&#22810;&#20010;&#35774;&#32622;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20960;&#20010;&#20219;&#21153;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#25797;&#38271;&#20110;&#23545;&#35937;&#20013;&#24515;&#35782;&#21035;&#65292;&#20294;&#23398;&#20064;&#30340;&#25991;&#26412;&#34920;&#31034;&#20284;&#20046;&#23545;&#35789;&#24207;&#19981;&#21464;&#65292;&#26410;&#33021;&#20197;&#26032;&#39062;&#26041;&#24335;&#32452;&#25104;&#24050;&#30693;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#20219;&#20309;VLM&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#21333;&#27969;&#27169;&#22411;&#22914;GPT-4V&#65292;&#25104;&#21151;&#35782;&#21035;&#32452;&#21512;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#29616;&#26377;&#27169;&#22411;&#32534;&#30721;&#32452;&#21512;&#24615;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#22312;&#32452;&#21512;&#24615;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#36229;&#36807;10% &#30340;&#32477;&#23545;&#25913;&#36827;&#65292;&#21516;&#26102;&#22312;&#26631;&#20934;&#23545;&#35937;&#35782;&#21035;&#21644;&#26816;&#32034;&#22522;&#20934;&#19978;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/netflix/&#22788;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15021v1 Announce Type: cross  Abstract: Recent years have witnessed a significant increase in the performance of Vision and Language tasks. Foundational Vision-Language Models (VLMs), such as CLIP, have been leveraged in multiple settings and demonstrated remarkable performance across several tasks. Such models excel at object-centric recognition yet learn text representations that seem invariant to word order, failing to compose known concepts in novel ways. However, no evidence exists that any VLM, including large-scale single-stream models such as GPT-4V, identifies compositions successfully. In this paper, we introduce a framework to significantly improve the ability of existing models to encode compositional language, with over 10% absolute improvement on compositionality benchmarks, while maintaining or improving the performance on standard object-recognition and retrieval benchmarks. Our code and pre-trained models are publicly available at https://github.com/netflix/
&lt;/p&gt;</description></item><item><title>&#35843;&#26597;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;&#65292;&#23588;&#20854;&#23545;&#19982;&#40657;&#20154;&#22899;&#24615;&#30456;&#20851;&#30340;&#21517;&#23383;&#34920;&#29616;&#26368;&#19981;&#21033;&#12290;&#23457;&#35745;&#22312;&#27169;&#22411;&#37096;&#32626;&#21644;&#23454;&#26045;&#26102;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#24378;&#35843;&#12290;</title><link>https://arxiv.org/abs/2402.14875</link><description>&lt;p&gt;
&#21517;&#23383;&#30340;&#21547;&#20041;&#26159;&#20160;&#20040;&#65311;&#23457;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
What's in a Name? Auditing Large Language Models for Race and Gender Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14875
&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;&#65292;&#23588;&#20854;&#23545;&#19982;&#40657;&#20154;&#22899;&#24615;&#30456;&#20851;&#30340;&#21517;&#23383;&#34920;&#29616;&#26368;&#19981;&#21033;&#12290;&#23457;&#35745;&#22312;&#27169;&#22411;&#37096;&#32626;&#21644;&#23454;&#26045;&#26102;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#24378;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37319;&#29992;&#23457;&#35745;&#35774;&#35745;&#26469;&#35843;&#26597;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#21253;&#25324;GPT-4&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#21457;&#27169;&#22411;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#20026;&#20010;&#20154;&#25552;&#20379;&#24314;&#35758;&#65292;&#27604;&#22914;&#22312;&#36141;&#36710;&#35848;&#21028;&#25110;&#36873;&#20030;&#32467;&#26524;&#39044;&#27979;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#35813;&#24314;&#35758;&#31995;&#32479;&#24615;&#22320;&#23545;&#19982;&#31181;&#26063;&#23569;&#25968;&#32676;&#20307;&#21644;&#22899;&#24615;&#24120;&#35265;&#30456;&#20851;&#30340;&#21517;&#23383;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#19982;&#40657;&#20154;&#22899;&#24615;&#30456;&#20851;&#30340;&#21517;&#23383;&#24471;&#21040;&#30340;&#32467;&#26524;&#26368;&#19981;&#21033;&#12290;&#36825;&#20123;&#20559;&#35265;&#22312;42&#20010;&#25552;&#31034;&#27169;&#26495;&#21644;&#22810;&#20010;&#27169;&#22411;&#20013;&#37117;&#26159;&#19968;&#33268;&#30340;&#65292;&#34920;&#26126;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#24615;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#23396;&#31435;&#20107;&#20214;&#12290;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#25968;&#20540;&#12289;&#19982;&#20915;&#31574;&#30456;&#20851;&#30340;&#38170;&#28857;&#21487;&#20197;&#25104;&#21151;&#25269;&#28040;&#20559;&#35265;&#65292;&#32780;&#23450;&#24615;&#32454;&#33410;&#30340;&#24433;&#21709;&#24182;&#19981;&#19968;&#33268;&#65292;&#29978;&#33267;&#21487;&#33021;&#20250;&#21152;&#21095;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#37096;&#32626;&#21644;&#23454;&#26045;&#26102;&#36827;&#34892;&#23457;&#35745;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#20943;&#36731;&#20854;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14875v1 Announce Type: cross  Abstract: We employ an audit design to investigate biases in state-of-the-art large language models, including GPT-4. In our study, we elicit prompt the models for advice regarding an individual across a variety of scenarios, such as during car purchase negotiations or election outcome predictions. We find that the advice systematically disadvantages names that are commonly associated with racial minorities and women. Names associated with Black women receive the least advantageous outcomes. The biases are consistent across 42 prompt templates and several models, indicating a systemic issue rather than isolated incidents. While providing numerical, decision-relevant anchors in the prompt can successfully counteract the biases, qualitative details have inconsistent effects and may even increase disparities. Our findings underscore the importance of conducting audits at the point of LLM deployment and implementation to mitigate their potential for
&lt;/p&gt;</description></item><item><title>&#20316;&#32773;&#35843;&#26597;&#20102;&#31070;&#32463;LM&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#26469;&#35299;&#30721;&#34164;&#28085;&#21028;&#26029;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#36828;&#39640;&#20110;&#38543;&#26426;&#20960;&#29575;&#22320;&#35299;&#30721;&#33258;&#28982;&#21477;&#23376;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#65292;&#26263;&#31034;LM&#38544;&#21547;&#22320;&#27169;&#25311;&#20102;&#35821;&#20041;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.13956</link><description>&lt;p&gt;
&#20320;&#33021;&#36890;&#36807;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#23398;&#20064;&#35821;&#20041;&#21527;&#65311;&#20197;&#34164;&#28085;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Can You Learn Semantics Through Next-Word Prediction? The Case of Entailment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13956
&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#35843;&#26597;&#20102;&#31070;&#32463;LM&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#26469;&#35299;&#30721;&#34164;&#28085;&#21028;&#26029;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#36828;&#39640;&#20110;&#38543;&#26426;&#20960;&#29575;&#22320;&#35299;&#30721;&#33258;&#28982;&#21477;&#23376;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#65292;&#26263;&#31034;LM&#38544;&#21547;&#22320;&#27169;&#25311;&#20102;&#35821;&#20041;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Merrill&#31561;&#20154;&#65288;2022&#65289;&#35748;&#20026;&#65292;&#22312;&#29702;&#35770;&#19978;&#65292;&#26368;&#20248;LM&#39044;&#27979;&#30340;&#27010;&#29575;&#32534;&#30721;&#20102;&#20851;&#20110;&#34164;&#28085;&#20851;&#31995;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20294;&#26159;&#30001;&#20110;Merrill&#31561;&#20154;&#25552;&#20986;&#30340;&#24378;&#28872;&#29702;&#24819;&#21270;&#20551;&#35774;&#65292;&#19981;&#28165;&#26970;&#31070;&#32463;LM&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#19978;&#26159;&#21542;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#23398;&#20064;&#34164;&#28085;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20182;&#20204;&#30340;&#29702;&#35770;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#20174;&#31070;&#32463;LM&#20013;&#35299;&#30721;&#34164;&#28085;&#21028;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#31867;&#20284;&#20110;&#20182;&#20204;&#30340;&#27979;&#35797;&#21487;&#20197;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#21644;LM&#20013;&#35299;&#30721;&#33258;&#28982;&#21477;&#23376;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#65292;&#36828;&#36828;&#36229;&#36807;&#38543;&#26426;&#26426;&#20250;&#65292;&#23613;&#31649;&#19981;&#26159;&#23436;&#32654;&#30340;&#12290;&#36825;&#34920;&#26126;LM&#38544;&#21547;&#22320;&#27169;&#25311;&#20102;&#35821;&#20041;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#20197;&#39044;&#27979;&#21477;&#23376;&#20849;&#29616;&#27169;&#24335;&#19978;&#30340;&#35821;&#20041;&#25928;&#24212;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#23454;&#38469;&#19978;&#39044;&#27979;&#34164;&#28085;&#30340;&#27979;&#35797;&#19982;&#29702;&#35770;&#27979;&#35797;&#30340;&#26041;&#21521;&#30456;&#21453;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#28508;&#22312;&#30340;&#29702;&#35770;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13956v1 Announce Type: new  Abstract: Do LMs infer the semantics of text from co-occurrence patterns in their training data? Merrill et al. (2022) argue that, in theory, probabilities predicted by an optimal LM encode semantic information about entailment relations, but it is unclear whether neural LMs trained on corpora learn entailment in this way because of strong idealizing assumptions made by Merrill et al. In this work, we investigate whether their theory can be used to decode entailment judgments from neural LMs. We find that a test similar to theirs can decode entailment relations between natural sentences, well above random chance, though not perfectly, across many datasets and LMs. This suggests LMs implicitly model aspects of semantics to predict semantic effects on sentence co-occurrence patterns. However, we find the test that predicts entailment in practice works in the opposite direction to the theoretical test. We thus revisit the assumptions underlying the o
&lt;/p&gt;</description></item><item><title>Neeko&#21033;&#29992;&#21160;&#24577;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#31574;&#30053;&#65292;&#26377;&#25928;&#22788;&#29702;&#22810;&#35282;&#33394;&#25198;&#28436;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#25552;&#21319;&#20102;&#23545;&#19981;&#21516;&#23646;&#24615;&#12289;&#20010;&#24615;&#21644;&#35828;&#35805;&#27169;&#24335;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13717</link><description>&lt;p&gt;
Neeko&#65306;&#21033;&#29992;&#21160;&#24577;LoRA&#23454;&#29616;&#39640;&#25928;&#22810;&#35282;&#33394;&#25198;&#28436;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13717
&lt;/p&gt;
&lt;p&gt;
Neeko&#21033;&#29992;&#21160;&#24577;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#31574;&#30053;&#65292;&#26377;&#25928;&#22788;&#29702;&#22810;&#35282;&#33394;&#25198;&#28436;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#25552;&#21319;&#20102;&#23545;&#19981;&#21516;&#23646;&#24615;&#12289;&#20010;&#24615;&#21644;&#35828;&#35805;&#27169;&#24335;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20195;&#29702;&#31243;&#24207;&#20013;&#36215;&#30528;&#38761;&#21629;&#24615;&#20316;&#29992;&#65292;&#20294;&#22312;&#22810;&#35282;&#33394;&#25198;&#28436;&#65288;MCRP&#65289;&#22330;&#26223;&#20013;&#36935;&#21040;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Neeko&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#39640;&#25928;&#27169;&#20223;&#22810;&#20010;&#35282;&#33394;&#32780;&#35774;&#35745;&#30340;&#21019;&#26032;&#26694;&#26550;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;Neeko&#37319;&#29992;&#21160;&#24577;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#31574;&#30053;&#65292;&#20351;&#20854;&#33021;&#22815;&#26080;&#32541;&#36866;&#24212;&#19981;&#21516;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#35282;&#33394;&#25198;&#28436;&#36807;&#31243;&#20998;&#35299;&#20026;&#20195;&#29702;&#39044;&#35757;&#32451;&#12289;&#22810;&#20010;&#35282;&#33394;&#25198;&#28436;&#21644;&#35282;&#33394;&#22686;&#37327;&#23398;&#20064;&#65292;&#26377;&#25928;&#22788;&#29702;&#24050;&#30693;&#21644;&#26410;&#30693;&#35282;&#33394;&#12290;&#36825;&#31181;&#21160;&#24577;&#26041;&#27861;&#65292;&#32467;&#21512;&#20026;&#27599;&#20010;&#35282;&#33394;&#35774;&#35745;&#30340;&#29420;&#29305;LoRA&#22359;&#65292;&#22686;&#24378;&#20102;Neeko&#23545;&#29420;&#29305;&#23646;&#24615;&#12289;&#20010;&#24615;&#21644;&#35828;&#35805;&#27169;&#24335;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;Neeko&#22312;MCRP&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#26356;&#20855;&#21560;&#24341;&#21147;&#21644;&#22810;&#26679;&#21270;&#30340;&#20114;&#21160;&#20307;&#39564;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;&#65288;&#38142;&#25509;&#20013;&#25552;&#20379;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13717v1 Announce Type: new  Abstract: Large Language Models (LLMs) have revolutionized open-domain dialogue agents but encounter challenges in multi-character role-playing (MCRP) scenarios. To address the issue, we present Neeko, an innovative framework designed for efficient multiple characters imitation. Unlike existing methods, Neeko employs a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to diverse characters. Our framework breaks down the role-playing process into agent pre-training, multiple characters playing, and character incremental learning, effectively handling both seen and unseen roles. This dynamic approach, coupled with distinct LoRA blocks for each character, enhances Neeko's adaptability to unique attributes, personalities, and speaking patterns. As a result, Neeko demonstrates superior performance in MCRP over most existing methods, offering more engaging and versatile user interaction experiences. Code and data are available at
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#20013;&#22269;&#26368;&#22823;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24494;&#21338;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#20013;&#25991;&#22810;&#27169;&#24577;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65288;CMNER&#65289;&#65292;&#21253;&#21547;5,000&#26465;&#24494;&#21338;&#24086;&#23376;&#21644;18,326&#24352;&#23545;&#24212;&#22270;&#29255;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#22270;&#29255;&#32435;&#20837;NER&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13693</link><description>&lt;p&gt;
&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#30340;&#20013;&#25991;&#22810;&#27169;&#24577;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;CMNER
&lt;/p&gt;
&lt;p&gt;
CMNER: A Chinese Multimodal NER Dataset based on Social Media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#20013;&#22269;&#26368;&#22823;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24494;&#21338;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#20013;&#25991;&#22810;&#27169;&#24577;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65288;CMNER&#65289;&#65292;&#21253;&#21547;5,000&#26465;&#24494;&#21338;&#24086;&#23376;&#21644;18,326&#24352;&#23545;&#24212;&#22270;&#29255;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#22270;&#29255;&#32435;&#20837;NER&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;MNER&#65289;&#26159;&#19968;&#39033;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#20854;&#36890;&#36807;&#30456;&#20851;&#22270;&#29255;&#25552;&#20379;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;&#20013;&#25991;MNER&#39046;&#22495;&#65292;&#25968;&#25454;&#26126;&#26174;&#19981;&#36275;&#65292;&#20005;&#37325;&#38459;&#30861;&#20102;&#35813;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#24494;&#21338;&#30340;&#25968;&#25454;&#32534;&#21046;&#20102;&#19968;&#20010;&#20013;&#25991;&#22810;&#27169;&#24577;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65288;CMNER&#65289;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;5,000&#26465;&#24494;&#21338;&#24086;&#23376;&#65292;&#37197;&#23545;18,326&#24352;&#23545;&#24212;&#30340;&#22270;&#29255;&#12290;&#23454;&#20307;&#34987;&#20998;&#31867;&#20026;&#22235;&#20010;&#19981;&#21516;&#31867;&#21035;&#65306;&#20154;&#29289;&#12289;&#22320;&#28857;&#12289;&#32452;&#32455;&#21644;&#26434;&#39033;&#12290;&#25105;&#20204;&#22312;CMNER&#19978;&#36827;&#34892;&#20102;&#22522;&#32447;&#23454;&#39564;&#65292;&#32467;&#26524;&#24378;&#35843;&#20102;&#23558;&#22270;&#29255;&#32435;&#20837;NER&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20844;&#24320;&#30340;&#33521;&#25991;MNER&#25968;&#25454;&#38598;&#65288;Twitter2015&#65289;&#19978;&#36827;&#34892;&#20102;&#36328;&#35821;&#35328;&#23454;&#39564;&#65292;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#21363;&#20013;&#25991;&#21644;&#33521;&#25991;&#20043;&#38388;&#23384;&#22312;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13693v1 Announce Type: new  Abstract: Multimodal Named Entity Recognition (MNER) is a pivotal task designed to extract named entities from text with the support of pertinent images. Nonetheless, a notable paucity of data for Chinese MNER has considerably impeded the progress of this natural language processing task within the Chinese domain. Consequently, in this study, we compile a Chinese Multimodal NER dataset (CMNER) utilizing data sourced from Weibo, China's largest social media platform. Our dataset encompasses 5,000 Weibo posts paired with 18,326 corresponding images. The entities are classified into four distinct categories: person, location, organization, and miscellaneous. We perform baseline experiments on CMNER, and the outcomes underscore the effectiveness of incorporating images for NER. Furthermore, we conduct cross-lingual experiments on the publicly available English MNER dataset (Twitter2015), and the results substantiate our hypothesis that Chinese and Eng
&lt;/p&gt;</description></item><item><title>DoRA&#26159;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#26435;&#37325;&#20998;&#35299;&#20026;&#24133;&#24230;&#21644;&#26041;&#21521;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;LoRA&#36827;&#34892;&#26041;&#21521;&#26356;&#26032;&#30340;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#23398;&#20064;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26469;&#25552;&#39640;&#23545;LLaMA&#65292;LLaVA&#21644;VL-B&#30340;&#24494;&#35843;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09353</link><description>&lt;p&gt;
DoRA: &#20998;&#35299;&#26435;&#37325;&#30340;&#20302;&#31209;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
DoRA: Weight-Decomposed Low-Rank Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09353
&lt;/p&gt;
&lt;p&gt;
DoRA&#26159;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#26435;&#37325;&#20998;&#35299;&#20026;&#24133;&#24230;&#21644;&#26041;&#21521;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;LoRA&#36827;&#34892;&#26041;&#21521;&#26356;&#26032;&#30340;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#23398;&#20064;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26469;&#25552;&#39640;&#23545;LLaMA&#65292;LLaVA&#21644;VL-B&#30340;&#24494;&#35843;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PEFT&#65289;&#26041;&#27861;&#20013;&#65292;&#30001;&#20110;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#25512;&#29702;&#25104;&#26412;&#65292;LoRA&#21450;&#20854;&#21464;&#31181;&#26041;&#27861;&#22240;&#27492;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#23436;&#20840;&#24494;&#35843;&#65288;FT&#65289;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#31934;&#24230;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26435;&#37325;&#20998;&#35299;&#20998;&#26512;&#26041;&#27861;&#26469;&#30740;&#31350;FT&#21644;LoRA&#20043;&#38388;&#30340;&#20869;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#27169;&#25311;FT&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DoRA&#30340;&#26435;&#37325;&#20998;&#35299;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#12290;DoRA&#23558;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#20998;&#35299;&#20026;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24133;&#24230;&#21644;&#26041;&#21521;&#65292;&#24182;&#19988;&#20855;&#20307;&#20351;&#29992;LoRA&#36827;&#34892;&#26041;&#21521;&#26356;&#26032;&#65292;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;DoRA&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;LoRA&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#20219;&#20309;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#22312;&#24494;&#35843;LLaMA&#65292;LLaVA&#21644;VL-B&#19978;&#65292;DoRA&#22987;&#32456;&#20248;&#20110;LoRA&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09353v1 Announce Type: new Abstract: Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-B
&lt;/p&gt;</description></item><item><title>UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07939</link><description>&lt;p&gt;
UFO: &#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#20132;&#20114;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
UFO: A UI-Focused Agent for Windows OS Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07939
&lt;/p&gt;
&lt;p&gt;
UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;UFO&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#20102;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;UFO&#37319;&#29992;&#21452;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#31934;&#30830;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#21644;&#25511;&#21046;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#26234;&#33021;&#20307;&#21487;&#20197;&#26080;&#32541;&#22320;&#22312;&#21333;&#20010;&#24212;&#29992;&#31243;&#24207;&#20869;&#20197;&#21450;&#36328;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#23548;&#33322;&#21644;&#25805;&#20316;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#21363;&#20351;&#28041;&#21450;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#65292;&#23454;&#29616;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#30340;&#21160;&#20316;&#36830;&#25509;&#65292;&#24182;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#12290;&#22240;&#27492;&#65292;UFO&#23558;&#33392;&#24040;&#32780;&#32791;&#26102;&#30340;&#36807;&#31243;&#36716;&#21464;&#20026;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#23601;&#21487;&#20197;&#23436;&#25104;&#30340;&#31616;&#21333;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;9&#20010;&#27969;&#34892;&#30340;Windows&#24212;&#29992;&#31243;&#24207;&#19978;&#23545;UFO&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#21453;&#26144;&#29992;&#25143;&#26085;&#24120;&#20351;&#29992;&#24773;&#26223;&#30340;&#21508;&#31181;&#24773;&#20917;&#12290;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#21644;&#30495;&#23454;&#26696;&#20363;&#30740;&#31350;&#24471;&#20986;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;UFO&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users' daily usage. The results, derived from both quantitative metrics and real-case studies, underscore t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OAIF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#21453;&#39304;&#26469;&#25913;&#21892;DAP&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;LLM&#20316;&#20026;&#26631;&#27880;&#22120;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20248;&#20110;&#31163;&#32447;DAP&#21644;RLHF&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.04792</link><description>&lt;p&gt;
&#26469;&#33258;&#22312;&#32447;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#30340;&#30452;&#25509;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Direct Language Model Alignment from Online AI Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OAIF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#21453;&#39304;&#26469;&#25913;&#21892;DAP&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;LLM&#20316;&#20026;&#26631;&#27880;&#22120;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20248;&#20110;&#31163;&#32447;DAP&#21644;RLHF&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30452;&#25509;&#23545;&#40784;&#20559;&#22909;&#65288;DAP&#65289;&#26041;&#27861;&#22914;DPO&#24050;&#25104;&#20026;&#23545;&#20110;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#30340;&#39640;&#25928;&#26367;&#20195;&#26041;&#27861;&#65292;&#19981;&#35201;&#27714;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;DAP&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#20559;&#22909;&#25968;&#25454;&#38598;&#36890;&#24120;&#22312;&#35757;&#32451;&#20043;&#21069;&#25910;&#38598;&#65292;&#24182;&#19988;&#20174;&#19981;&#26356;&#26032;&#65292;&#22240;&#27492;&#21453;&#39304;&#32431;&#31929;&#26159;&#31163;&#32447;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#30340;&#22238;&#24212;&#36890;&#24120;&#26159;&#20174;&#19968;&#20010;&#19982;&#34987;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#26679;&#30340;&#65292;&#30001;&#20110;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20250;&#21464;&#21270;&#65292;&#23545;&#40784;&#38454;&#27573;&#24517;&#28982;&#26159;&#38750;&#31574;&#30053;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#32447;&#21453;&#39304;&#26159;&#20851;&#38190;&#65292;&#21487;&#20197;&#25913;&#21892;DAP&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#32447;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#65288;OAIF&#65289;&#65292;&#20351;&#29992;LLM&#20316;&#20026;&#26631;&#27880;&#22120;&#65306;&#22312;&#27599;&#20010;&#35757;&#32451;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#20174;&#24403;&#21069;&#27169;&#22411;&#20013;&#37319;&#26679;&#20004;&#20010;&#22238;&#24212;&#65292;&#24182;&#25552;&#31034;LLM&#26631;&#27880;&#22120;&#36873;&#25321;&#21738;&#20010;&#26356;&#21463;&#27426;&#36814;&#65292;&#20174;&#32780;&#25552;&#20379;&#22312;&#32447;&#21453;&#39304;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#20294;&#36890;&#36807;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;OAIF&#20248;&#20110;&#31163;&#32447;DAP&#21644;RLHF
&lt;/p&gt;
&lt;p&gt;
Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#26367;MOOCs&#20013;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#20013;&#35780;&#20272;&#23398;&#29983;&#20889;&#20316;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03776</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;MOOCs&#35780;&#20998;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As MOOCs Graders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03776
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#26367;MOOCs&#20013;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#20013;&#35780;&#20272;&#23398;&#29983;&#20889;&#20316;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#65288;MOOCs&#65289;&#20026;&#25317;&#26377;&#30005;&#33041;&#21644;&#20114;&#32852;&#32593;&#35775;&#38382;&#26435;&#38480;&#30340;&#20840;&#29699;&#20219;&#20309;&#20154;&#25552;&#20379;&#20813;&#36153;&#25945;&#32946;&#30340;&#26426;&#20250;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36825;&#20123;&#35838;&#31243;&#30340;&#22823;&#35268;&#27169;&#27880;&#20876;&#24847;&#21619;&#30528;&#19968;&#20301;&#25945;&#24072;&#20960;&#20046;&#19981;&#21487;&#33021;&#35780;&#20272;&#27599;&#20010;&#23398;&#29983;&#30340;&#20889;&#20316;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#21516;&#20276;&#35780;&#20998;&#36890;&#24120;&#26159;&#39318;&#36873;&#26041;&#27861;&#65292;&#36890;&#24120;&#30001;&#31616;&#21333;&#26126;&#20102;&#30340;&#35780;&#20998;&#26631;&#20934;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#21516;&#20276;&#35780;&#20998;&#22312;&#21487;&#38752;&#24230;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;18&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#65292;&#25506;&#32034;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26367;&#20195;MOOCs&#20013;&#30340;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;LLMs&#65306;GPT-4&#21644;GPT-3.5&#65292;&#24182;&#28085;&#30422;&#19977;&#38376;&#19981;&#21516;&#30340;&#35838;&#31243;&#65306;&#20837;&#38376;&#22825;&#25991;&#23398;&#65292;&#22825;&#20307;&#29983;&#29289;&#23398;&#20197;&#21450;&#22825;&#25991;&#23398;&#30340;&#21382;&#21490;&#19982;&#21746;&#23398;&#12290;&#20026;&#20102;&#35757;&#32451;LLMs&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;&#38646;-shot&#36830;&#32493;&#24605;&#32771;&#65288;Zero-shot-CoT&#65289;&#25552;&#31034;&#25216;&#26415;&#30340;&#21464;&#31181;&#30340;&#19977;&#20010;&#19981;&#21516;&#25552;&#31034;&#65306;&#32467;&#21512;Zero-shot-CoT&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive open online courses (MOOCs) unlock the doors to free education for anyone around the globe with access to a computer and the internet. Despite this democratization of learning, the massive enrollment in these courses means it is almost impossible for one instructor to assess every student's writing assignment. As a result, peer grading, often guided by a straightforward rubric, is the method of choice. While convenient, peer grading often falls short in terms of reliability and validity. In this study, using 18 distinct settings, we explore the feasibility of leveraging large language models (LLMs) to replace peer grading in MOOCs. Specifically, we focus on two state-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses: Introductory Astronomy, Astrobiology, and the History and Philosophy of Astronomy. To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#32858;&#21512;&#38388;&#25509;&#25512;&#29702;&#36335;&#24452;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03268</link><description>&lt;p&gt;
&#20174;&#25512;&#29702;&#36335;&#24452;&#32858;&#21512;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#32858;&#21512;&#38388;&#25509;&#25512;&#29702;&#36335;&#24452;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#26126;&#30830;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#12290;&#20026;&#20102;&#29702;&#35299;&#39044;&#35757;&#32451;&#19982;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#30340;&#20851;&#31995;&#22914;&#20309;&#20419;&#20351;&#25512;&#29702;&#33021;&#21147;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#21487;&#20197;&#23558;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#22312;&#39044;&#35757;&#32451;&#26102;&#36890;&#36807;&#32858;&#21512;&#38388;&#25509;&#30340;&#25512;&#29702;&#36335;&#24452;&#26469;&#24471;&#20986;&#26032;&#32467;&#35770;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20010;&#35270;&#35282;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;&#25968;&#23398;&#25512;&#29702;&#31561;&#20851;&#38190;&#24773;&#20917;&#19979;&#38750;&#24120;&#26377;&#25928;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25512;&#29702;&#36335;&#24452;&#24418;&#24335;&#21270;&#20026;&#22312;&#30693;&#35782;/&#25512;&#29702;&#22270;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#12290;&#23545;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#24067;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#30456;&#20851;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#27010;&#29575;&#30340;&#21152;&#26435;&#21644;&#26159;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21512;&#29702;&#26041;&#24335;&#12290;&#23545;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#35757;&#32451;&#23545;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#29616;&#23454;&#19990;&#30028;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs). More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38142;&#24335;&#21453;&#39304;&#65288;CoF&#65289;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#20013;&#20986;&#29616;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#36882;&#24402;&#38142;&#24335;&#21453;&#39304;&#65288;R-CoF&#65289;&#26041;&#27861;&#65292;&#24182;&#25552;&#21040;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02648</link><description>&lt;p&gt;
&#38142;&#24335;&#21453;&#39304;&#65306;&#32531;&#35299;&#22238;&#31572;&#19981;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38142;&#24335;&#21453;&#39304;&#65288;CoF&#65289;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#20013;&#20986;&#29616;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#36882;&#24402;&#38142;&#24335;&#21453;&#39304;&#65288;R-CoF&#65289;&#26041;&#27861;&#65292;&#24182;&#25552;&#21040;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#30693;&#35782;&#23494;&#38598;&#22411;&#38382;&#39064;&#26102;&#32463;&#24120;&#20986;&#29616;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#65292;&#21363;&#20351;&#36755;&#20837;&#30456;&#21516;&#65292;&#20063;&#20250;&#25552;&#20379;&#19981;&#21516;&#30340;&#36755;&#20986;&#12290;&#24403;&#29992;&#25143;&#34920;&#36798;&#22362;&#20915;&#30456;&#21453;&#30340;&#31435;&#22330;&#26102;&#65292;LLMs&#35843;&#25972;&#20854;&#22238;&#31572;&#30340;&#36136;&#37327;&#20250;&#21464;&#24046;&#65292;&#23613;&#31649;&#21021;&#22987;&#22238;&#31572;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#20123;&#34892;&#20026;&#38477;&#20302;&#20102;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#30340;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#65306;1&#65289;&#36890;&#36807;&#23637;&#31034;&#38142;&#24335;&#21453;&#39304;&#65288;CoF&#65289;&#22914;&#20309;&#23548;&#33268;LLMs&#26356;&#21152;&#20559;&#31163;&#23454;&#38469;&#31572;&#26696;&#65292;&#24341;&#36215;&#36807;&#24230;&#20381;&#36182;ChatGPT&#31561;AI&#20195;&#29702;&#24102;&#26469;&#30340;&#22266;&#26377;&#39118;&#38505;&#65307;2&#65289;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36882;&#24402;&#38142;&#24335;&#21453;&#39304;&#65288;R-CoF&#65289;&#65292;&#25105;&#20204;&#27491;&#22312;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;CoF&#31995;&#32479;&#25509;&#25910;&#19968;&#20010;&#24320;&#25918;&#24335;&#22810;&#27493;&#38382;&#39064;&#65292;&#28982;&#21518;&#25105;&#20204;&#37325;&#22797;&#25552;&#20379;&#26080;&#24847;&#20041;&#30340;&#21453;&#39304;&#65292;&#35201;&#27714;&#20877;&#27425;&#23581;&#35797;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#21453;&#39304;&#21482;&#20250;&#38477;&#20302;&#22238;&#31572;&#30340;&#36136;&#37327;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) frequently suffer from knowledge-intensive questions, often being inconsistent by providing different outputs despite given the same input. The response quality worsens when the user expresses a firm opposing stance which causes the LLMs to adjust its response despite the correct initial one. These behaviors decrease the reliability and validity of the responses provided by these models. In this paper, we attempt to 1) raise awareness of the inherent risks that follow from overly relying on AI agents like ChatGPT by showing how Chain-of-Feedback (CoF) triggers LLMs to deviate more from the actual answer and 2) suggest a novel prompting method, Recursive Chain of Feedback (R-CoF), that we are conducting further study. The CoF system takes in an open-ended multi-step question. Then, we repetitively provide meaningless feedback requesting another attempt. Our preliminary experiments show that such feedback only decreases the quality of the response. On the oth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SGE&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#36947;&#24503;&#24773;&#26223;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;SGE&#22312;&#20116;&#20010;LLMs&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20026;&#30740;&#31350;LLM&#19981;&#19968;&#33268;&#24615;&#30340;&#26681;&#26412;&#21407;&#22240;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.01719</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#37327;&#36947;&#24503;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Measuring Moral Inconsistencies in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SGE&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#36947;&#24503;&#24773;&#26223;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;SGE&#22312;&#20116;&#20010;LLMs&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20026;&#30740;&#31350;LLM&#19981;&#19968;&#33268;&#24615;&#30340;&#26681;&#26412;&#21407;&#22240;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#35821;&#20041;&#31561;&#20215;&#30340;&#25552;&#31034;&#20135;&#29983;&#35821;&#20041;&#31561;&#20215;&#30340;&#21709;&#24212;&#65292;&#37027;&#20040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#34987;&#35748;&#20026;&#26159;&#19968;&#33268;&#30340;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;LLMs&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#29983;&#25104;&#26041;&#38754;&#20063;&#23384;&#22312;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#65292;&#36825;&#23545;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#20934;&#30830;&#24230;&#26469;&#34913;&#37327;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#27809;&#26377;&#8220;&#27491;&#30830;&#8221;&#31572;&#26696;&#30340;&#36947;&#24503;&#24773;&#26223;&#65288;&#20363;&#22914;&#65292;&#36947;&#36335;&#20132;&#36816;&#38382;&#39064;&#65289;&#26159;&#19981;&#21512;&#36866;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SGE&#65289;&#65292;&#26469;&#34913;&#37327;LLM&#22312;&#36947;&#24503;&#24773;&#26223;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#8220;&#32463;&#39564;&#27861;&#21017;&#8221;&#65288;RoTs&#65289;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#25105;&#20204;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;SGE&#19982;&#20154;&#31867;&#21028;&#26029;&#22312;&#20116;&#20010;LLMs&#19978;&#26356;&#22909;&#22320;&#30456;&#20851;&#12290;&#22312;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;LLM&#19981;&#19968;&#33268;&#24615;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Large Language Model~(LLM) is considered consistent if semantically equivalent prompts produce semantically equivalent responses. Despite recent advancements showcasing the impressive capabilities of LLMs in conversational systems, we show that even state-of-the-art LLMs are highly inconsistent in their generations, questioning their reliability. Prior research has tried to measure this with task-specific accuracies. However, this approach is unsuitable for moral scenarios, such as the trolley problem, with no ``correct'' answer. To address this issue, we propose a novel information-theoretic measure called Semantic Graph Entropy~(SGE) to measure the consistency of an LLM in moral scenarios. We leverage ``Rules of Thumb''~(RoTs) to explain a model's decision-making strategies and further enhance our metric. Compared to existing consistency metrics, SGE correlates better with human judgments across five LLMs. In the future, we aim to investigate the root causes of LLM inconsistencies 
&lt;/p&gt;</description></item><item><title>LatestEval&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21644;&#26102;&#38388;&#25935;&#24863;&#30340;&#27979;&#35797;&#26500;&#24314;&#19981;&#21463;&#25968;&#25454;&#27745;&#26579;&#30340;&#38405;&#35835;&#29702;&#35299;&#35780;&#20272;&#65292;&#36991;&#20813;&#20102;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#20174;&#32780;&#40723;&#21169;&#27169;&#22411;&#26356;&#22909;&#22320;&#25512;&#26029;&#31572;&#26696;&#12290;</title><link>https://arxiv.org/abs/2312.12343</link><description>&lt;p&gt;
LatestEval: &#36890;&#36807;&#21160;&#24577;&#21644;&#26102;&#38388;&#25935;&#24863;&#30340;&#27979;&#35797;&#26500;&#24314;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12343
&lt;/p&gt;
&lt;p&gt;
LatestEval&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21644;&#26102;&#38388;&#25935;&#24863;&#30340;&#27979;&#35797;&#26500;&#24314;&#19981;&#21463;&#25968;&#25454;&#27745;&#26579;&#30340;&#38405;&#35835;&#29702;&#35299;&#35780;&#20272;&#65292;&#36991;&#20813;&#20102;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#20174;&#32780;&#40723;&#21169;&#27169;&#22411;&#26356;&#22909;&#22320;&#25512;&#26029;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#20808;&#22312;&#36229;&#22823;&#35268;&#27169;&#33258;&#21160;&#25235;&#21462;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#35780;&#20272;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#36825;&#20010;&#38382;&#39064;&#23548;&#33268;&#22312;&#20934;&#30830;&#35780;&#20272;&#27169;&#22411;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LatestEval&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#36817;&#30340;&#25991;&#26412;&#21019;&#24314;&#19981;&#21463;&#27745;&#26579;&#30340;&#38405;&#35835;&#29702;&#35299;&#35780;&#20272;&#12290;LatestEval&#36890;&#36807;&#20165;&#20351;&#29992;&#22312;&#26368;&#36817;&#26102;&#38388;&#31383;&#21475;&#20869;&#21457;&#24067;&#30340;&#25991;&#26412;&#26469;&#36991;&#20813;&#25968;&#25454;&#27745;&#26579;&#65292;&#30830;&#20445;&#19981;&#20250;&#19982;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#37325;&#21472;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;LatestEval&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;1&#65289;&#25910;&#38598;&#26368;&#26032;&#25991;&#26412;&#65307;2&#65289;&#35782;&#21035;&#20851;&#38190;&#20449;&#24687;&#65292;&#20197;&#21450;3&#65289;&#26500;&#24314;&#38024;&#23545;&#35813;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20174;&#19978;&#19979;&#25991;&#20013;&#21024;&#38500;&#29616;&#26377;&#31572;&#26696;&#12290;&#36825;&#40723;&#21169;&#27169;&#22411;&#26681;&#25454;&#21097;&#20313;&#19978;&#19979;&#25991;&#25512;&#26029;&#31572;&#26696;&#65292;&#32780;&#19981;&#20165;&#26159;&#22797;&#21046;&#31896;&#36148;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12343v3 Announce Type: replace-cross  Abstract: Data contamination in evaluation is getting increasingly prevalent with the emergence of language models pre-trained on super large, automatically crawled corpora. This problem leads to significant challenges in the accurate assessment of model capabilities and generalisations. In this paper, we propose LatestEval, an automatic method that leverages the most recent texts to create uncontaminated reading comprehension evaluations. LatestEval avoids data contamination by only using texts published within a recent time window, ensuring no overlap with the training corpora of pre-trained language models. We develop the LatestEval automated pipeline to 1) gather the latest texts; 2) identify key information, and 3) construct questions targeting the information while removing the existing answers from the context. This encourages models to infer the answers themselves based on the remaining context, rather than just copy-paste. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#27880;&#20998;&#37197;&#30340;&#27874;&#24418;&#27169;&#24335;&#23545;&#20854;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#35282;&#24230;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2312.04455</link><description>&lt;p&gt;
&#24378;&#21270;&#20851;&#27880;&#21147;&#20013;&#26368;&#30701;&#30340;&#25903;&#26609;&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#24847;&#35782;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#27880;&#20998;&#37197;&#30340;&#27874;&#24418;&#27169;&#24335;&#23545;&#20854;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#35282;&#24230;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#20851;&#27880;&#20998;&#37197;&#20013;&#30340;&#20869;&#22312;&#27874;&#24418;&#27169;&#24335;&#26174;&#33879;&#24433;&#21709;&#23427;&#20204;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#21033;&#29992;LLMs&#36827;&#34892;&#24037;&#20855;&#20351;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#20851;&#38190;&#20449;&#24687;&#22312;&#19978;&#19979;&#25991;&#20013;&#20301;&#20110;&#20851;&#27880;&#27874;&#24418;&#30340;&#20302;&#35895;&#21306;&#22495;&#26102;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#24573;&#35270;&#35813;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#26032;&#22411;&#25512;&#29702;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;LLMs&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#22788;&#29702;&#36755;&#20837;&#12290;&#27599;&#20010;&#36807;&#31243;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#20934;&#35282;&#24230;&#36827;&#34892;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65292;&#20174;&#32780;&#21019;&#24314;&#20986;&#19968;&#20010;&#29420;&#29305;&#30340;&#20851;&#27880;&#27874;&#24418;&#12290;&#36890;&#36807;&#29992;&#19968;&#20010;&#36807;&#31243;&#30340;&#20851;&#27880;&#20302;&#35895;&#34917;&#20607;&#21478;&#19968;&#20010;&#36807;&#31243;&#30340;&#20851;&#27880;&#39640;&#23792;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;LLM&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named Attention Buckets. It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchm
&lt;/p&gt;</description></item><item><title>&#27979;&#35797;&#20102;&#22810;&#31181;&#31616;&#21333;&#30340;&#22810;LLM&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#21457;&#29616;&#20004;&#20010;LLM&#20195;&#29702;&#25353;&#39034;&#24207;&#20351;&#29992;&#30340;&#31616;&#21333;&#32467;&#26500;&#34920;&#29616;&#26368;&#20339;&#65292;&#19968;&#20010;&#32534;&#36753;&#36890;&#29992;&#25805;&#20316;&#27969;&#31243;&#65292;&#21478;&#19968;&#20010;&#39564;&#35777;&#21487;&#25191;&#34892;&#24615;&#65292;&#22312;&#23450;&#21046;&#25805;&#20316;&#27969;&#31243;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#65292;&#26377;&#26395;&#36827;&#19968;&#27493;&#25506;&#32034;&#22810;&#20195;&#29702;&#32534;&#36753;&#20307;&#31995;&#32467;&#26500;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2311.09510</link><description>&lt;p&gt;
&#19968;&#20992;&#20999;&#24182;&#38750;&#38134;&#24377;&#65306;&#23450;&#21046;&#24320;&#25918;&#39046;&#22495;&#30340;&#25805;&#20316;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
One Size Does Not Fit All: Customizing Open-Domain Procedures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09510
&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#20102;&#22810;&#31181;&#31616;&#21333;&#30340;&#22810;LLM&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#21457;&#29616;&#20004;&#20010;LLM&#20195;&#29702;&#25353;&#39034;&#24207;&#20351;&#29992;&#30340;&#31616;&#21333;&#32467;&#26500;&#34920;&#29616;&#26368;&#20339;&#65292;&#19968;&#20010;&#32534;&#36753;&#36890;&#29992;&#25805;&#20316;&#27969;&#31243;&#65292;&#21478;&#19968;&#20010;&#39564;&#35777;&#21487;&#25191;&#34892;&#24615;&#65292;&#22312;&#23450;&#21046;&#25805;&#20316;&#27969;&#31243;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#65292;&#26377;&#26395;&#36827;&#19968;&#27493;&#25506;&#32034;&#22810;&#20195;&#29702;&#32534;&#36753;&#20307;&#31995;&#32467;&#26500;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#25805;&#20316;&#30340;&#27969;&#31243;&#65292;&#27604;&#22914;&#22914;&#20309;&#31181;&#26893;&#33457;&#22253;&#65292;&#29616;&#22312;&#34987;&#25968;&#30334;&#19975;&#29992;&#25143;&#20351;&#29992;&#65292;&#20294;&#26377;&#26102;&#38656;&#35201;&#33258;&#23450;&#20041;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#29305;&#23450;&#38656;&#27714;&#65292;&#20363;&#22914;&#19981;&#20351;&#29992;&#26432;&#34411;&#21058;&#31181;&#26893;&#33457;&#22253;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#34913;&#37327;&#21644;&#25913;&#36827;&#19968;&#20010;LLM&#25191;&#34892;&#27492;&#31867;&#23450;&#21046;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#27979;&#35797;&#20960;&#31181;&#31616;&#21333;&#30340;&#22810;LLM&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#26032;&#30340;&#35780;&#20272;&#38598;CustomPlans&#23545;&#20854;&#36827;&#34892;&#23450;&#21046;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;200&#22810;&#20010;WikiHow&#25805;&#20316;&#27969;&#31243;&#32452;&#25104;&#65292;&#27599;&#20010;&#27969;&#31243;&#37117;&#26377;&#29305;&#23450;&#30340;&#23450;&#21046;&#38656;&#27714;&#12290;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#20855;&#26377;&#20004;&#20010;LLM&#20195;&#29702;&#30340;&#20307;&#31995;&#32467;&#26500;&#34920;&#29616;&#26368;&#20339;&#65292;&#23427;&#20204;&#25353;&#39034;&#24207;&#20351;&#29992;&#65292;&#19968;&#20010;&#32534;&#36753;&#36890;&#29992;&#25805;&#20316;&#27969;&#31243;&#65292;&#21478;&#19968;&#20010;&#39564;&#35777;&#20854;&#21487;&#25191;&#34892;&#24615;&#65292;&#26126;&#26174;&#20248;&#20110;&#31471;&#21040;&#31471;&#25552;&#31034;&#30340;LLM&#65288;10.5&#65285;&#32477;&#23545;&#65289;&#12290;&#36825;&#34920;&#26126;LLM&#21487;&#20197;&#34987;&#21512;&#29702;&#26377;&#25928;&#22320;&#37197;&#32622;&#20197;&#36827;&#34892;&#25805;&#20316;&#27969;&#31243;&#30340;&#23450;&#21046;&#12290;&#36825;&#20063;&#34920;&#26126;&#65292;&#22810;&#20195;&#29702;&#32534;&#36753;&#20307;&#31995;&#32467;&#26500;&#21487;&#33021;&#20540;&#24471;&#36827;&#19968;&#27493;&#25506;&#35752;&#65292;&#20197;&#29992;&#20110;&#20854;&#20182;&#31867;&#22411;&#30340;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09510v2 Announce Type: replace  Abstract: How-to procedures, such as how to plant a garden, are now used by millions of users, but sometimes need customizing to meet a user's specific needs, e.g., planting a garden without pesticides. Our goal is to measure and improve an LLM's ability to perform such customization. Our approach is to test several simple multi-LLM-agent architectures for customization, as well as an end-to-end LLM, using a new evaluation set, called CustomPlans, of over 200 WikiHow procedures each with a customization need. We find that a simple architecture with two LLM agents used sequentially performs best, one that edits a generic how-to procedure and one that verifies its executability, significantly outperforming (10.5% absolute) an end-to-end prompted LLM. This suggests that LLMs can be configured reasonably effectively for procedure customization. This also suggests that multi-agent editing architectures may be worth exploring further for other custo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#20132;&#20114;&#24335;&#35752;&#35770;&#65292;SAIE&#26694;&#26550;&#20419;&#36827;&#20102;&#23398;&#20064;&#27169;&#22411;&#19982;&#20249;&#20276;&#27169;&#22411;&#20043;&#38388;&#30340;&#25903;&#25345;&#24615;&#21644;&#23545;&#25239;&#24615;&#23545;&#35805;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.08107</link><description>&lt;p&gt;
SAIE&#26694;&#26550;&#65306;&#21333;&#32431;&#25903;&#25345;&#24182;&#19981;&#36275;&#22815;-&#36890;&#36807;&#23545;&#25239;&#24615;&#35780;&#27880;&#25512;&#36827;LLM&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
SAIE Framework: Support Alone Isn't Enough -- Advancing LLM Training with Adversarial Remarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08107
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#20132;&#20114;&#24335;&#35752;&#35770;&#65292;SAIE&#26694;&#26550;&#20419;&#36827;&#20102;&#23398;&#20064;&#27169;&#22411;&#19982;&#20249;&#20276;&#27169;&#22411;&#20043;&#38388;&#30340;&#25903;&#25345;&#24615;&#21644;&#23545;&#25239;&#24615;&#23545;&#35805;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#19982;&#20854;&#20182;&#27169;&#22411;&#25110;&#20154;&#31867;&#35752;&#35770;&#26469;&#35777;&#26126;&#25110;&#25209;&#35780;&#20854;&#39044;&#27979;&#32467;&#26524;&#65292;&#20174;&#32780;&#20016;&#23500;&#23427;&#20204;&#23545;&#23454;&#20363;&#30340;&#20869;&#22312;&#29702;&#35299;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#25512;&#29702;&#38454;&#27573;&#30340;&#20027;&#21160;&#35752;&#35770;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#36825;&#31867;&#20132;&#20114;&#22312;&#35757;&#32451;&#38454;&#27573;&#24182;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#25105;&#20204;&#20551;&#35774;&#23558;&#20132;&#20114;&#24335;&#35752;&#35770;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#25913;&#36827;&#23427;&#20204;&#30340;&#25512;&#29702;&#21644;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SAIE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20419;&#36827;&#20102;&#23398;&#20064;&#27169;&#22411;&#19982;&#20249;&#20276;&#27169;&#22411;&#20043;&#38388;&#30340;&#25903;&#25345;&#24615;&#21644;&#23545;&#25239;&#24615;&#35752;&#35770;&#12290;&#23398;&#20064;&#27169;&#22411;&#25509;&#25910;&#20249;&#20276;&#30340;&#22238;&#22797;&#65292;&#28982;&#21518;&#26681;&#25454;&#27492;&#35752;&#35770;&#26356;&#26032;&#20854;&#21442;&#25968;&#12290;&#36825;&#31181;&#21160;&#24577;&#35843;&#25972;&#36807;&#31243;&#22312;&#25972;&#20010;&#35757;&#32451;&#38454;&#27573;&#25345;&#32493;&#36827;&#34892;&#65292;&#20197;&#21709;&#24212;&#23398;&#20064;&#27169;&#22411;&#19981;&#26029;&#21457;&#23637;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08107v2 Announce Type: replace  Abstract: Large Language Models (LLMs) can justify or critique their predictions through discussions with other models or humans, thereby enriching their intrinsic understanding of instances. While proactive discussions in the inference phase have been shown to boost performance, such interactions have not been extensively explored during the training phase. We hypothesize that incorporating interactive discussions into the training process can enhance the models' understanding and improve their reasoning and verbal expression abilities during inference. This work introduces the SAIE framework, which facilitates supportive and adversarial discussions between learner and partner models. The learner model receives responses from the partner, and its parameters are then updated based on this discussion. This dynamic adjustment process continues throughout the training phase, responding to the evolving outputs of the learner model. Our empirical e
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;QLoRA&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24503;&#22269;&#35758;&#20250;&#36777;&#35770;&#20013;&#33258;&#21160;&#21270;&#21457;&#35328;&#20154;&#24402;&#23646;&#65292;&#20026;&#35745;&#31639;&#20998;&#26512;&#25919;&#27835;&#35805;&#35821;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2309.09902</link><description>&lt;p&gt;
&#20351;&#29992;QLoRA&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24503;&#22269;&#35758;&#20250;&#36777;&#35770;&#20013;&#30340;&#21457;&#35328;&#20154;&#24402;&#23646;
&lt;/p&gt;
&lt;p&gt;
Speaker attribution in German parliamentary debates with QLoRA-adapted large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09902
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;QLoRA&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24503;&#22269;&#35758;&#20250;&#36777;&#35770;&#20013;&#33258;&#21160;&#21270;&#21457;&#35328;&#20154;&#24402;&#23646;&#65292;&#20026;&#35745;&#31639;&#20998;&#26512;&#25919;&#27835;&#35805;&#35821;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#26029;&#22686;&#38271;&#30340;&#25919;&#27835;&#25991;&#26412;&#20026;&#28145;&#20837;&#20102;&#35299;&#25919;&#27835;&#21160;&#24577;&#21644;&#24847;&#35782;&#24418;&#24577;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#25163;&#21160;&#20998;&#26512;&#30340;&#24037;&#20316;&#37327;&#12290;&#33258;&#21160;&#21457;&#35328;&#20154;&#24402;&#23646;&#21487;&#20197;&#26816;&#27979;&#35328;&#36766;&#20107;&#20214;&#20013;&#35841;&#23545;&#35841;&#35828;&#20102;&#20160;&#20040;&#65292;&#24182;&#19982;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#23494;&#20999;&#30456;&#20851;&#65292;&#26159;&#35745;&#31639;&#25991;&#26412;&#20998;&#26512;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;LLama 2&#22312;2017-2021&#24180;&#24503;&#22269;&#35758;&#20250;&#36777;&#35770;&#20013;&#33258;&#21160;&#21270;&#21457;&#35328;&#20154;&#24402;&#23646;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;QLoRA&#23545;Llama 2&#36827;&#34892;&#24494;&#35843;&#65292;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;GermEval 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#21270;&#21457;&#35328;&#20154;&#24402;&#23646;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20026;&#25919;&#27835;&#35805;&#35821;&#30340;&#35745;&#31639;&#20998;&#26512;&#21644;&#21457;&#23637;&#24320;&#36767;&#20102;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.09902v2 Announce Type: replace  Abstract: The growing body of political texts opens up new opportunities for rich insights into political dynamics and ideologies but also increases the workload for manual analysis. Automated speaker attribution, which detects who said what to whom in a speech event and is closely related to semantic role labeling, is an important processing step for computational text analysis. We study the potential of the large language model family Llama 2 to automate speaker attribution in German parliamentary debates from 2017-2021. We fine-tune Llama 2 with QLoRA, an efficient training strategy, and observe our approach to achieve competitive performance in the GermEval 2023 Shared Task On Speaker Attribution in German News Articles and Parliamentary Debates. Our results shed light on the capabilities of large language models in automating speaker attribution, revealing a promising avenue for computational analysis of political discourse and the develo
&lt;/p&gt;</description></item><item><title>MuLTI&#26159;&#19968;&#31181;&#39640;&#31934;&#24230;&#39640;&#25928;&#30340;&#35270;&#39057;&#19982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;&#22810;&#36884;&#37319;&#26679;&#22120;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#26377;&#25928;&#30340;&#29305;&#24449;&#34701;&#21512;&#21644;&#24555;&#36895;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;</title><link>https://arxiv.org/abs/2303.05707</link><description>&lt;p&gt;
MuLTI&#65306;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;&#22810;&#36884;&#37319;&#26679;&#22120;&#21644;&#22810;&#36873;&#27169;&#22411;&#30340;&#39640;&#25928;&#35270;&#39057;&#19982;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
MuLTI: Efficient Video-and-Language Understanding with Text-Guided MultiWay-Sampler and Multiple Choice Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.05707
&lt;/p&gt;
&lt;p&gt;
MuLTI&#26159;&#19968;&#31181;&#39640;&#31934;&#24230;&#39640;&#25928;&#30340;&#35270;&#39057;&#19982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;&#22810;&#36884;&#37319;&#26679;&#22120;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#26377;&#25928;&#30340;&#29305;&#24449;&#34701;&#21512;&#21644;&#24555;&#36895;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#19982;&#35821;&#35328;&#29702;&#35299;&#22312;&#34892;&#19994;&#20013;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#22914;&#35270;&#39057;&#38382;&#31572;&#12289;&#25991;&#26412;&#35270;&#39057;&#26816;&#32034;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#29616;&#26377;&#30340;&#35270;&#39057;&#19982;&#35821;&#35328;&#29702;&#35299;&#26041;&#27861;&#19968;&#33324;&#37319;&#29992;&#32321;&#37325;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#28040;&#32791;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MuLTI&#65292;&#19968;&#31181;&#39640;&#31934;&#24230;&#39640;&#25928;&#30340;&#35270;&#39057;&#19982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#26377;&#25928;&#30340;&#29305;&#24449;&#34701;&#21512;&#21644;&#24555;&#36895;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#27744;&#21270;&#27531;&#24046;&#26144;&#23556;&#21644;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#25991;&#26412;&#24341;&#23548;&#22810;&#36884;&#37319;&#26679;&#22120;&#65292;&#29992;&#20110;&#37319;&#26679;&#38271;&#24207;&#21015;&#21644;&#34701;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#20043;&#21069;&#37319;&#26679;&#22120;&#23548;&#33268;&#30340;&#24615;&#33021;&#38477;&#32423;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.05707v2 Announce Type: replace-cross  Abstract: Video-and-language understanding has a variety of applications in the industry, such as video question answering, text-video retrieval, and multi-label classification. Existing video-and-language understanding methods generally adopt heavy multi-modal encoders and feature fusion modules, which consume high computational costs. Specially, they have difficulty dealing with dense video frames or long text prevalent in industrial applications. This paper proposes MuLTI, a highly accurate and efficient video-and-language understanding model that achieves efficient and effective feature fusion and rapid adaptation to downstream tasks. Specifically, we design a Text-Guided MultiWay-Sampler based on adapt-pooling residual mapping and self-attention modules to sample long sequences and fuse multi-modal features, which reduces the computational costs and addresses performance degradation caused by previous samplers. Therefore, MuLTI can 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26694;&#26550;InceptionXML&#65292;&#36890;&#36807;&#22312;embedding&#32500;&#24230;&#19978;&#37325;&#26032;&#20998;&#37197;&#21367;&#31215;&#25805;&#20316;&#65292;&#24212;&#23545;&#30701;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#21333;&#35789;&#39034;&#24207;&#32570;&#22833;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;InceptionXML+&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#27493;&#26631;&#31614;&#31579;&#36873;&#22120;&#21644;&#26497;&#31471;&#20998;&#31867;&#22120;&#65292;&#25913;&#36827;&#20102;&#21160;&#24577;&#30828;&#36127;&#37319;&#26679;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2109.07319</link><description>&lt;p&gt;
InceptionXML&#65306;&#19968;&#31181;&#24102;&#26377;&#21516;&#27493;&#36127;&#37319;&#26679;&#30340;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#29992;&#20110;&#30701;&#25991;&#26412;&#26497;&#31471;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
InceptionXML: A Lightweight Framework with Synchronized Negative Sampling for Short Text Extreme Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.07319
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26694;&#26550;InceptionXML&#65292;&#36890;&#36807;&#22312;embedding&#32500;&#24230;&#19978;&#37325;&#26032;&#20998;&#37197;&#21367;&#31215;&#25805;&#20316;&#65292;&#24212;&#23545;&#30701;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#21333;&#35789;&#39034;&#24207;&#32570;&#22833;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;InceptionXML+&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#27493;&#26631;&#31614;&#31579;&#36873;&#22120;&#21644;&#26497;&#31471;&#20998;&#31867;&#22120;&#65292;&#25913;&#36827;&#20102;&#21160;&#24577;&#30828;&#36127;&#37319;&#26679;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#25991;&#26412;&#25968;&#25454;&#23545;&#22823;&#37327;&#30446;&#26631;&#26631;&#31614;&#36827;&#34892;&#33258;&#21160;&#27880;&#37322;&#65292;&#34987;&#31216;&#20026;&#30701;&#25991;&#26412;&#26497;&#31471;&#20998;&#31867;&#65292;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#21253;&#25324;&#30456;&#20851;&#25628;&#32034;&#39044;&#27979;&#21644;&#20135;&#21697;&#25512;&#33616;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#26550;&#26500;InceptionXML&#65292;&#20854;&#36731;&#37327;&#20294;&#21151;&#33021;&#24378;&#22823;&#65292;&#24182;&#19988;&#33021;&#22815;&#24212;&#23545;&#25628;&#32034;&#21644;&#25512;&#33616;&#20219;&#21153;&#20013;&#30701;&#25991;&#26412;&#26597;&#35810;&#20013;&#22266;&#26377;&#30340;&#32570;&#20047;&#21333;&#35789;&#39034;&#24207;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#21367;&#31215;&#30340;&#25805;&#20316;&#27839;&#30528;&#23884;&#20837;&#32500;&#24230;&#37325;&#26032;&#26500;&#24314;&#65292;&#32780;&#19981;&#26159;&#20687;&#20256;&#32479;CNNs&#19968;&#26679;&#27839;&#30528;&#21333;&#35789;&#32500;&#24230;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#35777;&#26126;&#20102;&#24212;&#29992;&#21367;&#31215;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#25193;&#23637;&#21040;&#20855;&#26377;&#25968;&#30334;&#19975;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;InceptionXML+&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#27493;&#26631;&#31614;&#31579;&#36873;&#22120;&#21644;&#26497;&#31471;&#20998;&#31867;&#22120;&#65292;&#25913;&#36827;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#21160;&#24577;&#30828;&#36127;&#37319;&#26679;&#25216;&#26415;&#22312;&#26631;&#31614;&#31579;&#36873;&#20013;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.07319v3 Announce Type: replace-cross  Abstract: Automatic annotation of short-text data to a large number of target labels, referred to as Short Text Extreme Classification, has found numerous applications including prediction of related searches and product recommendation tasks. In this paper, we propose a convolutional architecture InceptionXML which is light-weight, yet powerful, and robust to the inherent lack of word-order in short-text queries encountered in search and recommendation tasks. We demonstrate the efficacy of applying convolutions by recasting the operation along the embedding dimension instead of the word dimension as applied in conventional CNNs for text classification. Towards scaling our model to datasets with millions of labels, we also propose InceptionXML+ framework which improves upon the shortcomings of the recently proposed dynamic hard-negative mining technique for label shortlisting by synchronizing the label-shortlister and extreme classifier. 
&lt;/p&gt;</description></item><item><title>CFMatch&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#23558;&#33258;&#21160;&#31572;&#26696;&#31561;&#20215;&#35780;&#20272;&#19982;&#20154;&#24037;&#19987;&#23478;&#21028;&#26029;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#26126;&#30830;&#19968;&#33268;&#30340;&#35780;&#20272;&#25351;&#21335;&#24182;&#24341;&#20837;&#39640;&#25928;&#12289;&#31283;&#20581;&#19988;&#36731;&#37327;&#32423;&#30340;&#21028;&#21035;&#24335;AE&#20998;&#31867;&#22120;&#21305;&#37197;&#26041;&#27861;&#26469;&#35299;&#20915;&#24403;&#21069;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13170</link><description>&lt;p&gt;
CFMatch: &#23558;&#33258;&#21160;&#31572;&#26696;&#31561;&#20215;&#35780;&#20272;&#19982;&#20154;&#24037;&#19987;&#23478;&#21028;&#26029;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering. (arXiv:2401.13170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13170
&lt;/p&gt;
&lt;p&gt;
CFMatch&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#23558;&#33258;&#21160;&#31572;&#26696;&#31561;&#20215;&#35780;&#20272;&#19982;&#20154;&#24037;&#19987;&#23478;&#21028;&#26029;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#26126;&#30830;&#19968;&#33268;&#30340;&#35780;&#20272;&#25351;&#21335;&#24182;&#24341;&#20837;&#39640;&#25928;&#12289;&#31283;&#20581;&#19988;&#36731;&#37327;&#32423;&#30340;&#21028;&#21035;&#24335;AE&#20998;&#31867;&#22120;&#21305;&#37197;&#26041;&#27861;&#26469;&#35299;&#20915;&#24403;&#21069;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#31995;&#32479;&#21482;&#26377;&#22312;&#25105;&#20204;&#30693;&#36947;&#31572;&#26696;&#26159;&#21542;&#27491;&#30830;&#30340;&#24773;&#20917;&#19979;&#25165;&#33021;&#21462;&#24471;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#35768;&#22810;&#26368;&#20855;&#25361;&#25112;&#21644;&#26377;&#36259;&#30340;&#38382;&#31572;&#31034;&#20363;&#65292;&#24403;&#21069;&#29992;&#20110;&#30830;&#23450;&#31572;&#26696;&#31561;&#20215;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#36890;&#24120;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#65292;&#23588;&#20854;&#26159;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26356;&#20887;&#38271;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#31572;&#26696;&#12290;&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#65306;&#32570;&#20047;&#25968;&#25454;&#21644;&#27169;&#22411;&#36807;&#22823;&#65306;&#22522;&#20110;LLM&#30340;&#35780;&#20998;&#22120;&#21487;&#20197;&#26356;&#22909;&#22320;&#19982;&#20154;&#24037;&#35780;&#21028;&#21592;&#30456;&#20851;&#32852;&#65292;&#20294;&#36825;&#20010;&#20219;&#21153;&#21482;&#22312;&#26377;&#38480;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21363;&#20351;&#21487;&#29992;&#65292;&#23545;&#27169;&#22411;&#30340;&#26356;&#26032;&#20063;&#26377;&#38480;&#65292;&#22240;&#20026;LLM&#36807;&#22823;&#19988;&#24448;&#24448;&#26114;&#36149;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#26126;&#30830;&#19968;&#33268;&#30340;&#25351;&#21335;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#25351;&#21335;&#29992;&#20110;&#20174;&#19987;&#19994;&#20154;&#24037;&#38382;&#31572;&#27604;&#36187;&#20013;&#37319;&#32435;&#26426;&#22120;&#38382;&#31572;&#22312;&#31572;&#26696;&#31561;&#20215;&#24615;&#35780;&#20272;&#26041;&#38754;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26631;&#20934;&#35780;&#20272;&#21644;&#19968;&#31181;&#26356;&#39640;&#25928;&#12289;&#31283;&#20581;&#19988;&#36731;&#37327;&#32423;&#30340;&#21028;&#21035;&#24335;AE&#20998;&#31867;&#22120;&#21305;&#37197;&#26041;&#27861;&#65288;CFMatch&#65292;&#22823;&#23567;&#23567;&#20110;1MB&#65289;&#65292;&#32463;&#36807;&#35757;&#32451;&#21644;&#39564;&#35777;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#31572;&#26696;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current evaluation metrics to determine answer equivalence (AE) often do not align with human judgments, particularly more verbose, free-form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big: LLM-based scorers can correlate better with human judges, but this task has only been tested on limited QA datasets, and even when available, update of the model is limited because LLMs are large and often expensive. We rectify both of these issues by providing clear and consistent guidelines for evaluating AE in machine QA adopted from professional human QA contests. We also introduce a combination of standard evaluation and a more efficient, robust, and lightweight discriminate AE classifier-based matching method (CFMatch, smaller than 1 MB), trained and validated to more accurately evalu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#35780;&#20272;&#21508;&#31181;LMMs&#22312;&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01523</link><description>&lt;p&gt;
GOAT-Bench: &#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse. (arXiv:2401.01523v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01523
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#35780;&#20272;&#21508;&#31181;LMMs&#22312;&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#28145;&#21051;&#25913;&#21464;&#20102;&#20449;&#24687;&#30340;&#21019;&#36896;&#12289;&#20256;&#25773;&#21644;&#21560;&#25910;&#26041;&#24335;&#65292;&#22312;&#25968;&#23383;&#26102;&#20195;&#20135;&#29983;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24433;&#21709;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#36825;&#20010;&#29190;&#28856;&#20063;&#23548;&#33268;&#20102;&#32593;&#32476;&#36855;&#22240;&#30340;&#28389;&#29992;&#25968;&#37327;&#26174;&#33879;&#22686;&#21152;&#12290;&#35780;&#20272;&#36855;&#22240;&#30340;&#36127;&#38754;&#24433;&#21709;&#26159;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#24494;&#22937;&#21644;&#38544;&#26214;&#30340;&#21547;&#20041;&#65292;&#36825;&#20123;&#21547;&#20041;&#19981;&#33021;&#30452;&#25509;&#36890;&#36807;&#26174;&#24615;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#20256;&#36798;&#20986;&#26469;&#12290;&#37492;&#20110;&#27492;&#65292;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#20316;&#20026;&#22788;&#29702;&#22810;&#26679;&#21270;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21331;&#36234;&#33021;&#21147;&#30340;&#28966;&#28857;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#12290;&#38024;&#23545;&#36825;&#19968;&#21457;&#23637;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#28145;&#20837;&#30740;&#31350;&#21508;&#31181;LMMs(&#22914;GPT-4V)&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;6K&#20010;&#22810;&#26679;&#30340;&#36855;&#22240;&#65292;&#28085;&#30422;&#30340;&#20027;&#39064;&#21253;&#25324;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#12289;&#24615;&#21035;&#27495;&#35270;&#21644;&#32593;&#32476;&#27450;&#20940;&#31561;&#12290;&#21033;&#29992;GOAT-Be
&lt;/p&gt;
&lt;p&gt;
The exponential growth of social media has profoundly transformed how information is created, disseminated, and absorbed, exceeding any precedent in the digital age. Regrettably, this explosion has also spawned a significant increase in the online abuse of memes. Evaluating the negative impact of memes is notably challenging, owing to their often subtle and implicit meanings, which are not directly conveyed through the overt text and imagery. In light of this, large multimodal models (LMMs) have emerged as a focal point of interest due to their remarkable capabilities in handling diverse multimodal tasks. In response to this development, our paper aims to thoroughly examine the capacity of various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of social abuse manifested in memes. We introduce the comprehensive meme benchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes such as implicit hate speech, sexism, and cyberbullying, etc. Utilizing GOAT-Be
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#26126;&#26377;&#24207;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#19988;&#20154;&#31867;&#21270;&#22320;&#32452;&#32455;&#24605;&#32500;&#65292;&#20197;&#25552;&#39640;&#28436;&#32462;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03309</link><description>&lt;p&gt;
&#31616;&#26126;&#26377;&#24207;&#30340;&#24863;&#30693;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning. (arXiv:2310.03309v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03309
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#26126;&#26377;&#24207;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#19988;&#20154;&#31867;&#21270;&#22320;&#32452;&#32455;&#24605;&#32500;&#65292;&#20197;&#25552;&#39640;&#28436;&#32462;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#28436;&#32462;&#25512;&#29702;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#22797;&#26434;&#30340;&#28436;&#32462;&#38382;&#39064;&#20013;&#20173;&#28982;&#24456;&#38590;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#36825;&#31867;&#38382;&#39064;&#20855;&#26377;&#22823;&#37327;&#21069;&#25552;&#65288;&#21363;&#20107;&#23454;&#25110;&#35268;&#21017;&#65289;&#65292;&#20854;&#20013;&#28041;&#21450;&#23454;&#20307;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#38656;&#35201;&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#12290;&#19968;&#31181;&#30452;&#35266;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#21407;&#22987;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#20197;&#21069;&#21521;&#65288;&#20363;&#22914;&#36873;&#25321;-&#25512;&#29702;&#65289;&#25110;&#21453;&#21521;&#65288;&#20363;&#22914;LAMBADA&#65289;&#26041;&#24335;&#23558;&#22810;&#20010;&#22240;&#26524;&#25512;&#29702;&#27493;&#39588;&#36830;&#25509;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#19981;&#21487;&#36991;&#20813;&#22320;&#38656;&#35201;&#22823;&#37327;&#30340;&#24635;&#20307;&#38454;&#27573;&#65292;&#23548;&#33268;&#35745;&#31639;&#24320;&#38144;&#22823;&#65292;&#24182;&#19988;&#26377;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#20135;&#29983;&#35823;&#23548;&#24615;&#30340;&#27493;&#39588;&#12290;&#38500;&#20102;&#36880;&#38454;&#27573;&#20998;&#35299;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#20154;&#31867;&#38382;&#39064;&#35299;&#20915;&#30340;&#21478;&#19968;&#20010;&#26041;&#38754;&#33719;&#24471;&#20102;&#21551;&#21457;&#12290;&#20154;&#31867;&#20542;&#21521;&#20110;&#25552;&#28860;&#20986;&#26368;&#30456;&#20851;&#30340;&#20449;&#24687;&#24182;&#26377;&#24207;&#22320;&#32452;&#32455;&#24605;&#32500;&#65288;&#20363;&#22914;&#21019;&#24314;&#24605;&#32500;&#23548;&#22270;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#20182;&#20204;&#23545;&#38382;&#39064;&#36827;&#34892;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploiting large language models (LLMs) to tackle deductive reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex deductive problems, characterized by plenty of premises (i.e., facts or rules) entailing intricate relationships among entities and requiring multi-hop reasoning. One intuitive solution is to decompose the original task into smaller sub-tasks, and then chain the multiple casual reasoning steps together in a forward (e.g., Selection-Inference) or backward (e.g., LAMBADA) direction. However, these techniques inevitably necessitate a large number of overall stages, leading to computationally expensive operations and a higher possibility of making misleading steps. In addition to stage-by-stage decomposition, we draw inspiration from another aspect of human problem-solving. Humans tend to distill the most relevant information and organize their thoughts systematically (e.g., creating mind maps), which assists th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65292;&#36890;&#36807;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#29992;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#65292;&#20174;&#32780;&#29983;&#25104;&#24615;&#33021;&#26356;&#22909;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2310.02304</link><description>&lt;p&gt;
&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65306;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation. (arXiv:2310.02304v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65292;&#36890;&#36807;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#29992;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#65292;&#20174;&#32780;&#29983;&#25104;&#24615;&#33021;&#26356;&#22909;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;&#20363;&#22914;&#24605;&#32500;&#26641;&#21644;&#31243;&#24207;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#65289;&#21462;&#24471;&#20102;&#19968;&#20123;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#8220;&#33050;&#25163;&#26550;&#8221;&#31243;&#24207;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#35813;&#31243;&#24207;&#26500;&#24314;&#20102;&#22810;&#27425;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#36755;&#20986;&#12290;&#33050;&#25163;&#26550;&#31243;&#24207;&#36890;&#24120;&#20351;&#29992;Python&#31561;&#32534;&#31243;&#35821;&#35328;&#32534;&#20889;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#31181;&#23376;&#8220;&#25913;&#36827;&#22120;&#8221;&#24320;&#22987;&#65292;&#36890;&#36807;&#22810;&#27425;&#26597;&#35810;&#35821;&#35328;&#27169;&#22411;&#24182;&#36820;&#22238;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#26681;&#25454;&#32473;&#23450;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#25913;&#36827;&#36755;&#20837;&#31243;&#24207;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36816;&#34892;&#36825;&#20010;&#31181;&#23376;&#25913;&#36827;&#22120;&#26469;&#25913;&#36827;&#33258;&#36523;&#12290;&#22312;&#19968;&#31995;&#21015;&#32454;&#20998;&#20219;&#21153;&#20013;&#65292;&#24471;&#21040;&#30340;&#25913;&#36827;&#25913;&#36827;&#22120;&#29983;&#25104;&#30340;&#31243;&#24207;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#31181;&#23376;&#25913;&#36827;&#22120;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#30340;&#21508;&#31181;&#33258;&#25105;&#25913;&#36827;&#31574;&#30053;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21253;&#25324;&#27874;&#26463;&#25628;&#32034;&#12289;&#36951;&#20256;&#31639;&#27861;&#21644;&#27169;&#25311;&#36864;&#28779;&#12290;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#27809;&#26377;&#25913;&#21464;&#65292;&#36825;&#24182;&#19981;&#26159;&#19968;&#31181;&#22686;&#38271;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent advances in AI systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a "scaffolding" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed "improver" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. Afterward, we analyze the variety of self-improvement strategies proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EVE&#30340;&#39640;&#25928;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#36974;&#34109;&#20449;&#21495;&#24314;&#27169;&#21644;&#27169;&#24577;&#24863;&#30693;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;Transformer&#32593;&#32476;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36827;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11971</link><description>&lt;p&gt;
EVE: &#20351;&#29992;&#36974;&#34109;&#39044;&#27979;&#21644;&#27169;&#24577;&#24863;&#30693;&#30340;&#39640;&#25928;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE. (arXiv:2308.11971v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EVE&#30340;&#39640;&#25928;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#36974;&#34109;&#20449;&#21495;&#24314;&#27169;&#21644;&#27169;&#24577;&#24863;&#30693;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;Transformer&#32593;&#32476;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36827;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EVE&#30340;&#39640;&#25928;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#23427;&#26159;&#30001;&#19968;&#31181;&#32479;&#19968;&#30340;Transformer&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;EVE&#36890;&#36807;&#22312;&#22270;&#20687;-&#25991;&#26412;&#23545;&#19978;&#36827;&#34892;&#36974;&#34109;&#20449;&#21495;&#24314;&#27169;&#26469;&#32479;&#19968;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#37325;&#24314;&#21487;&#35265;&#20449;&#21495;&#65292;&#21363;&#22270;&#20687;&#20687;&#32032;&#21644;&#25991;&#26412;&#26631;&#35760;&#12290;&#36890;&#36807;&#38598;&#25104;&#27169;&#24577;&#24863;&#30693;&#30340;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#27169;&#22359;&#65292;EVE&#22312;&#19968;&#20010;&#20849;&#20139;&#30340;Transformer&#32593;&#32476;&#20013;&#32534;&#30721;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20999;&#25442;&#21040;&#19981;&#21516;&#30340;&#19987;&#23478;&#26469;&#25429;&#25417;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building scalable vision-language models to learn from diverse, multimodal data remains an open challenge. In this paper, we introduce an Efficient Vision-languagE foundation model, namely EVE, which is one unified multimodal Transformer pre-trained solely by one unified pre-training task. Specifically, EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts. To unify pre-training tasks of vision and language, EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 3.5x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing to the combination of the unified architecture and pre-training task, EVE is easy t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#35777;&#35843;&#26597;&#65292;&#25506;&#32034;&#20102;&#20998;&#24067;&#22806;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#21457;&#29616;&#20102;LLM&#22312;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#24182;&#37319;&#29992;&#20102;&#26032;&#30340;&#29983;&#25104;&#24335;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.10261</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#38754;&#26377;&#22810;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Good Are Large Language Models at Out-of-Distribution Detection?. (arXiv:2308.10261v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#35777;&#35843;&#26597;&#65292;&#25506;&#32034;&#20102;&#20998;&#24067;&#22806;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#21457;&#29616;&#20102;LLM&#22312;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#24182;&#37319;&#29992;&#20102;&#26032;&#30340;&#29983;&#25104;&#24335;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#26816;&#27979;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#22312;ML&#31038;&#21306;&#24341;&#36215;&#20102;&#33539;&#24335;&#36716;&#21464;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#30740;&#31350;&#24050;&#32463;&#20197;BERT&#12289;RoBERTa&#21644;GPT-2&#31561;&#30456;&#23545;&#23567;&#35268;&#27169;&#30340;Transformer&#27169;&#22411;&#25506;&#32034;&#20102;OOD&#26816;&#27979;&#65292;&#20294;&#22312;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#25512;&#29702;&#33539;&#24335;&#26041;&#38754;&#30340;&#26126;&#26174;&#24046;&#24322;&#24341;&#21457;&#20102;&#23545;&#36825;&#20123;&#21457;&#29616;&#22312;LLM&#20013;&#30340;&#36866;&#29992;&#24615;&#30340;&#36136;&#30097;&#12290;&#26412;&#25991;&#22312;LLMs&#39046;&#22495;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#37325;&#28857;&#20851;&#27880;7B&#21040;65B&#22823;&#23567;&#30340;LLaMA&#31995;&#21015;&#12290;&#25105;&#20204;&#23545;&#24120;&#29992;&#30340;OOD&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23457;&#26597;&#20102;&#23427;&#20204;&#22312;&#38646;&#26799;&#24230;&#21644;&#24494;&#35843;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#20043;&#21069;&#30340;&#21028;&#21035;&#24335;&#30340;&#20869;&#37096;&#20998;&#24067;&#24494;&#35843;&#25913;&#20026;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20351;LLM&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#19982;&#20043;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection plays a vital role in enhancing the reliability of machine learning (ML) models. The emergence of large language models (LLMs) has catalyzed a paradigm shift within the ML community, showcasing their exceptional capabilities across diverse natural language processing tasks. While existing research has probed OOD detection with relative small-scale Transformers like BERT, RoBERTa and GPT-2, the stark differences in scales, pre-training objectives, and inference paradigms call into question the applicability of these findings to LLMs. This paper embarks on a pioneering empirical investigation of OOD detection in the domain of LLMs, focusing on LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate commonly-used OOD detectors, scrutinizing their performance in both zero-grad and fine-tuning scenarios. Notably, we alter previous discriminative in-distribution fine-tuning into generative fine-tuning, aligning the pre-training objective of LLM
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#25351;&#31216;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20511;&#37492;&#35821;&#35328;&#21746;&#23398;&#22806;&#37096;&#20027;&#20041;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#25552;&#20986;&#20102;LMs&#21487;&#20197;&#25351;&#31216;&#30340;&#29702;&#30001;&#12290;</title><link>http://arxiv.org/abs/2308.05576</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#25351;&#31216;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Language Models Refer?. (arXiv:2308.05576v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05576
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#25351;&#31216;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20511;&#37492;&#35821;&#35328;&#21746;&#23398;&#22806;&#37096;&#20027;&#20041;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#25552;&#20986;&#20102;LMs&#21487;&#20197;&#25351;&#31216;&#30340;&#29702;&#30001;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#29992;&#35821;&#35328;&#20570;&#20160;&#20040;&#65311;&#22823;&#23478;&#37117;&#21516;&#24847;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#65288;&#22823;&#37096;&#20998;&#65289;&#36830;&#36143;&#30340;&#21477;&#23376;&#12290;&#20294;&#26159;&#23427;&#20204;&#29992;&#36825;&#20123;&#23383;&#31526;&#20018;&#34920;&#36798;&#20102;&#20160;&#20040;&#65292;&#36824;&#26159;&#21482;&#26159;&#20197;&#19968;&#31181;&#20196;&#20154;&#20449;&#26381;&#30340;&#35821;&#35328;&#36816;&#29992;&#30340;&#27169;&#25311;&#20013;&#32993;&#35328;&#20081;&#35821;&#65311;&#36825;&#26159;&#19968;&#20010;&#27169;&#31946;&#30340;&#38382;&#39064;&#65292;&#26377;&#35768;&#22810;&#26041;&#27861;&#21487;&#20197;&#20351;&#20854;&#26126;&#30830;&#21270;&#12290;&#36825;&#37324;&#25105;&#20204;&#23558;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#21363;&#65292;LMs&#30340;&#35789;&#35821;&#26159;&#21542;&#25351;&#31216;&#65306;&#21363;&#65292;LMs&#30340;&#36755;&#20986;&#26159;&#21542;&#23454;&#29616;&#20102;&#8220;&#35789;&#35821;-&#19990;&#30028;&#8221;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#26377;&#21021;&#27493;&#30340;&#29702;&#30001;&#35748;&#20026;&#23427;&#20204;&#19981;&#20855;&#22791;&#25351;&#31216;&#33021;&#21147;&#65292;&#22240;&#20026;LMs&#27809;&#26377;&#20687;&#26222;&#36890;&#35821;&#35328;&#29992;&#25143;&#37027;&#26679;&#19982;&#19990;&#30028;&#20114;&#21160;&#12290;&#20511;&#37492;&#35821;&#35328;&#21746;&#23398;&#30340;&#22806;&#37096;&#20027;&#20041;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#35748;&#20026;&#34920;&#35937;&#26159;&#35823;&#23548;&#30340;&#65292;&#26377;&#20805;&#20998;&#30340;&#29702;&#30001;&#35748;&#20026;LMs&#21487;&#20197;&#25351;&#31216;&#12290;
&lt;/p&gt;
&lt;p&gt;
What do language models (LMs) do with language? Everyone agrees that they produce sequences of (mostly) coherent sentences. But are they saying anything with those strings or simply babbling in a convincing simulacrum of language use? This is a vague question, and there are many ways of making it precise. Here we will address one aspect of the question, namely, whether LMs' words refer: that is, whether the outputs of LMs achieve "word-to-world" connections. There is prima facie reason to think they do not since LMs do not interact with the world in the way that ordinary language users do. Drawing on insights from the externalist tradition in philosophy of language, we argue that appearances are misleading and that there is good reason to think that LMs can refer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.08158</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#23398;&#20064;&#21040;&#38750;&#39044;&#26399;&#30340;&#20559;&#35265;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#35770;&#25991;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20559;&#35265;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#21361;&#23475;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#20511;&#37492;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20559;&#35265;&#31867;&#22411;&#12289;&#37327;&#21270;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#12290;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#23545;&#20110;&#37327;&#21270;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#20559;&#35265;&#24230;&#37327;&#24182;&#19981;&#28041;&#21450;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20559;&#35265;&#65292;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#26159;&#34920;&#38754;&#30340;&#65292;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;Twitter&#19978;&#20998;&#20139;&#30340;&#22270;&#20687;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;CLIP&#21069;&#32512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#22270;&#20687;&#21644;&#25512;&#25991;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#29983;&#25104;&#20851;&#20110;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26367;&#20195;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.14779</link><description>&lt;p&gt;
Twitter&#22270;&#20687;&#30340;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text Conditional Alt-Text Generation for Twitter Images. (arXiv:2305.14779v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;Twitter&#19978;&#20998;&#20139;&#30340;&#22270;&#20687;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;CLIP&#21069;&#32512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#22270;&#20687;&#21644;&#25512;&#25991;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#29983;&#25104;&#20851;&#20110;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26367;&#20195;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31038;&#20132;&#23186;&#20307;&#29305;&#21035;&#26159;Twitter&#19978;&#20998;&#20139;&#30340;&#22270;&#20687;&#29983;&#25104;&#26367;&#20195;&#25991;&#26412;&#65288;&#25110;alt-text&#65289;&#25551;&#36848;&#30340;&#26041;&#27861;&#12290;&#19982;&#22270;&#20687;&#30340;&#23383;&#24149;&#19981;&#21516;&#65292;&#25991;&#26412;&#26367;&#25442;&#25991;&#26412;&#26356;&#21152;&#30452;&#30333;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#29305;&#23450;&#12290;&#27492;&#22806;&#65292;&#20851;&#38190;&#26159;&#65292;&#21457;&#24067;&#21040;Twitter&#19978;&#30340;&#22270;&#20687;&#36890;&#24120;&#26159;&#30001;&#29992;&#25143;&#32534;&#20889;&#30340;&#25991;&#26412;&#38468;&#21152;&#30340;&#65292;&#23613;&#31649;&#36825;&#20123;&#25991;&#26412;&#19981;&#19968;&#23450;&#25551;&#36848;&#22270;&#20687;&#65292;&#20294;&#21487;&#33021;&#25552;&#20379;&#26377;&#29992;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#22914;&#26524;&#27491;&#30830;&#21033;&#29992;&#21487;&#20197;&#25552;&#20379;&#20449;&#24687;&#65292;&#20363;&#22914;&#25512;&#25991;&#21487;&#33021;&#20250;&#21629;&#21517;&#22270;&#29255;&#20013;&#27169;&#22411;&#20043;&#21069;&#27809;&#26377;&#35265;&#36807;&#30340;&#19981;&#24120;&#35265;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;CLIP&#21069;&#32512;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#25552;&#21462;&#22270;&#20687;&#30340;&#23884;&#20837;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#26144;&#23556;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36755;&#20986;&#21333;&#35789;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#30701;&#24207;&#21015;&#65292;&#25110;&#31216;&#20026;&#8220;&#21069;&#32512;&#8221;&#65292;&#25105;&#20204;&#23558;&#25512;&#25991;&#26412;&#36523;&#30340;&#25991;&#26412;&#20063;&#36830;&#25509;&#21040;&#20854;&#20013;&#12290;&#36825;&#26679;&#65292;&#27169;&#22411;&#23601;&#21487;&#20197;&#22312;&#25991;&#31456;&#20013;&#26465;&#20214;&#21270;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#28982;&#21518;&#23558;&#21512;&#24182;&#30340;&#22810;&#27169;&#24335;&#21069;&#32512;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we present an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter. This task is more than just a special case of image captioning, as alt-text is both more literally descriptive and context-specific. Also critically, images posted to Twitter are often accompanied by user-written text that despite not necessarily describing the image may provide useful context that if properly leveraged can be informative -- e.g. the tweet may name an uncommon object in the image that the model has not previously seen. We address this with a CLIP prefix model that extracts an embedding of the image and passes it to a mapping network that outputs a short sequence in word embedding space, or a ``prefix'', to which we also concatenate the text from the tweet itself. This lets the model condition on both visual and textual information from the post. The combined multimodal prefix is then fed as a prompt to a pretrained lang
&lt;/p&gt;</description></item><item><title>GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08774</link><description>&lt;p&gt;
GPT-4&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Technical Report. (arXiv:2303.08774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08774
&lt;/p&gt;
&lt;p&gt;
GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;GPT-4&#30340;&#24320;&#21457;&#65292;&#23427;&#26159;&#19968;&#20010;&#21487;&#20197;&#25509;&#21463;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#19981;&#22914;&#20154;&#31867;&#65292;&#20294;GPT-4&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#65292;&#25104;&#32489;&#25490;&#21517;&#22312;&#21069;10&#65285;&#24038;&#21491;&#12290;GPT-4&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#25991;&#26723;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#21518;&#35757;&#32451;&#23545;&#40784;&#36807;&#31243;&#25552;&#39640;&#20102;&#20107;&#23454;&#24615;&#21644;&#31526;&#21512;&#26399;&#26395;&#34892;&#20026;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;GPT-4&#30340;&#26576;&#20123;&#24615;&#33021;&#26041;&#38754;&#65292;&#32780;&#36825;&#20123;&#24615;&#33021;&#26159;&#22522;&#20110;&#20351;&#29992;&#19981;&#36229;&#36807;GPT-4&#35745;&#31639;&#33021;&#21147;&#30340;1/1,000&#30340;&#27169;&#22411;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt&#21644;&#35821;&#20041;&#30693;&#35782;&#22686;&#24378;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#35782;&#21035;&#26410;&#35265;&#20851;&#31995;&#12290;&#37319;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#21333;&#35789;&#32423;&#21035;&#30340;&#31867;&#27604;&#25512;&#29702;&#26041;&#27861;&#65292;&#29983;&#25104;&#20102;&#20855;&#26377;&#26410;&#35265;&#20851;&#31995;&#30340;&#22686;&#24378;&#23454;&#20363;&#12290;&#35774;&#35745;&#20102;&#22522;&#20110;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#30340;&#25552;&#31034;&#65292;&#22686;&#21152;&#20102;&#24050;&#35265;&#20851;&#31995;&#30340;&#35821;&#20041;&#30693;&#35782;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2112.04539</link><description>&lt;p&gt;
&#22522;&#20110;Prompt&#21644;&#35821;&#20041;&#30693;&#35782;&#22686;&#24378;&#30340;&#38646;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Prompt-based Zero-shot Relation Extraction with Semantic Knowledge Augmentation. (arXiv:2112.04539v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.04539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt&#21644;&#35821;&#20041;&#30693;&#35782;&#22686;&#24378;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#35782;&#21035;&#26410;&#35265;&#20851;&#31995;&#12290;&#37319;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#21333;&#35789;&#32423;&#21035;&#30340;&#31867;&#27604;&#25512;&#29702;&#26041;&#27861;&#65292;&#29983;&#25104;&#20102;&#20855;&#26377;&#26410;&#35265;&#20851;&#31995;&#30340;&#22686;&#24378;&#23454;&#20363;&#12290;&#35774;&#35745;&#20102;&#22522;&#20110;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#30340;&#25552;&#31034;&#65292;&#22686;&#21152;&#20102;&#24050;&#35265;&#20851;&#31995;&#30340;&#35821;&#20041;&#30693;&#35782;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;(RTE)&#20219;&#21153;&#20013;&#65292;&#35782;&#21035;&#27809;&#26377;&#35757;&#32451;&#23454;&#20363;&#30340;&#26410;&#35265;&#20851;&#31995;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#24050;&#32463;&#23581;&#35797;&#20351;&#29992;&#38382;&#31572;&#27169;&#22411;&#25110;&#20851;&#31995;&#25551;&#36848;&#26469;&#35782;&#21035;&#26410;&#35265;&#20851;&#31995;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#26377;&#20851;&#24050;&#35265;&#21644;&#26410;&#35265;&#20851;&#31995;&#20043;&#38388;&#32852;&#31995;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt&#21644;&#35821;&#20041;&#30693;&#35782;&#22686;&#24378;&#30340;&#27169;&#22411;&#65288;ZS-SKA&#65289;&#65292;&#29992;&#20110;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#35782;&#21035;&#26410;&#35265;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31867;&#27604;&#25512;&#29702;&#30340;&#21333;&#35789;&#32423;&#21477;&#23376;&#32763;&#35793;&#35268;&#21017;&#65292;&#24182;&#20351;&#29992;&#35813;&#35268;&#21017;&#20174;&#20855;&#26377;&#24050;&#35265;&#20851;&#31995;&#30340;&#23454;&#20363;&#20013;&#29983;&#25104;&#20855;&#26377;&#26410;&#35265;&#20851;&#31995;&#30340;&#22686;&#24378;&#23454;&#20363;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#30340;&#21152;&#26435;&#34394;&#25311;&#26631;&#31614;&#26500;&#24314;&#30340;&#25552;&#31034;&#20449;&#24687;&#65292;&#20197;&#25972;&#21512;&#20174;&#24050;&#35265;&#20851;&#31995;&#20013;&#23398;&#21040;&#30340;&#35821;&#20041;&#30693;&#35782;&#20449;&#24687;&#12290;&#25105;&#20204;&#26500;&#36896;&#20102;&#21152;&#26435;&#34394;&#25311;&#26631;&#31614;&#35789;&#65292;&#32780;&#19981;&#26159;&#22312;&#25552;&#31034;&#27169;&#26495;&#20013;&#20351;&#29992;&#23454;&#38469;&#30340;&#26631;&#31614;&#38598;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;RTE&#20219;&#21153;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;
&lt;/p&gt;
&lt;p&gt;
In relation triplet extraction (RTE), recognizing unseen (new) relations for which there are no training instances is a challenging task. Efforts have been made to recognize unseen relations based on question-answering models or relation descriptions. However, these approaches miss the semantic information about connections between seen and unseen relations. In this paper, We propose a prompt-based model with semantic knowledge augmentation (ZS-SKA) to recognize unseen relations under the zero-shot setting. We present a new word-level analogy-based sentence translation rule and generate augmented instances with unseen relations from instances with seen relations using that new rule. We design prompts with weighted virtual label construction based on an external knowledge graph to integrate semantic knowledge information learned from seen relations. Instead of using the actual label sets in the prompt template, we construct weighted virtual label words. We learn the representations of b
&lt;/p&gt;</description></item></channel></rss>