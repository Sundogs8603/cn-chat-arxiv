<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#21644;&#25968;&#25454;&#37325;&#22797;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;D4&#31639;&#27861;&#21487;&#20197;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#21152;&#36895;&#35757;&#32451;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.12284</link><description>&lt;p&gt;
D4&#65306;&#36890;&#36807;&#25991;&#26723;&#21435;&#37325;&#19982;&#22810;&#26679;&#21270;&#25913;&#36827;LLM&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
D4: Improving LLM Pretraining via Document De-Duplication and Diversification. (arXiv:2308.12284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12284
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#21644;&#25968;&#25454;&#37325;&#22797;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;D4&#31639;&#27861;&#21487;&#20197;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#21152;&#36895;&#35757;&#32451;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#34987;&#29992;&#20110;&#35757;&#32451;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#36890;&#24120;&#26159;&#36890;&#36807;&#23545;&#26469;&#33258;&#22823;&#35268;&#27169;&#32593;&#32476;&#35821;&#26009;&#24211;&#20013;&#38543;&#26426;&#36873;&#25321;&#30340;&#23613;&#21487;&#33021;&#22810;&#30340;&#26631;&#35760;&#36827;&#34892;&#19968;&#27425;&#23398;&#20064;&#12290;&#34429;&#28982;&#22312;&#36234;&#26469;&#36234;&#22823;&#30340;&#20114;&#32852;&#32593;&#23616;&#37096;&#19978;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#19981;&#26029;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#25913;&#36827;&#30340;&#35268;&#27169;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#32780;&#20943;&#23567;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#32034;&#25968;&#25454;&#36873;&#25321;&#23545;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#38500;&#20102;MinHash&#31561;&#31616;&#21333;&#30340;&#21435;&#37325;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#23637;&#31034;&#20102;&#36890;&#36807;&#35880;&#24910;&#30340;&#25968;&#25454;&#36873;&#25321;(&#22312;&#21435;&#37325;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;)&#21487;&#20197;&#21152;&#24555;&#35757;&#32451;(&#25552;&#39640;&#20102;20%&#30340;&#25928;&#29575;)&#24182;&#19988;&#22312;16&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24179;&#22343;&#19979;&#28216;&#20934;&#30830;&#29575;&#19978;&#26377;&#25152;&#25552;&#21319;(&#39640;&#36798;2%)&#65292;&#22312;6.7B&#27169;&#22411;&#35268;&#27169;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26234;&#33021;&#37325;&#22797;&#25968;&#25454;&#30340;&#34920;&#29616;&#24635;&#26159;&#20248;&#20110;&#22522;&#32447;&#35757;&#32451;(&#32780;&#37325;&#22797;&#38543;&#26426;&#25968;&#25454;&#30340;&#34920;&#29616;&#27604;&#22522;&#32447;&#35757;&#32451;&#26356;&#24046;)&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32874;&#26126;&#30340;&#25968;&#25454;&#22788;&#29702;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;LLM&#30340;&#35757;&#32451;&#25928;&#26524;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over recent years, an increasing amount of compute and data has been poured into training large language models (LLMs), usually by doing one-pass learning on as many tokens as possible randomly selected from large-scale web corpora. While training on ever-larger portions of the internet leads to consistent performance improvements, the size of these improvements diminishes with scale, and there has been little work exploring the effect of data selection on pre-training and downstream performance beyond simple de-duplication methods such as MinHash. Here, we show that careful data selection (on top of de-duplicated data) via pre-trained model embeddings can speed up training (20% efficiency gains) and improves average downstream accuracy on 16 NLP tasks (up to 2%) at the 6.7B model scale. Furthermore, we show that repeating data intelligently consistently outperforms baseline training (while repeating random data performs worse than baseline training). Our results indicate that clever d
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26816;&#39564;&#20102;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;(FLMs)&#21450;&#20854;&#38598;&#25104;&#22312;&#22522;&#20934;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38598;&#25104;&#21487;&#20197;&#24433;&#21709;FLMs&#30340;&#20010;&#20307;&#20851;&#27880;&#65292;&#24182;&#23637;&#31034;&#19981;&#21516;FLMs&#20043;&#38388;&#30340;&#21327;&#35843;&#21644;&#21512;&#20316;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.12272</link><description>&lt;p&gt;
&#31616;&#21333;&#21363;&#26159;&#26356;&#22909;&#65292;&#22823;&#24182;&#19981;&#36275;&#22815;&#65306;&#36208;&#21521;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#30340;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models. (arXiv:2308.12272v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12272
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26816;&#39564;&#20102;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;(FLMs)&#21450;&#20854;&#38598;&#25104;&#22312;&#22522;&#20934;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38598;&#25104;&#21487;&#20197;&#24433;&#21709;FLMs&#30340;&#20010;&#20307;&#20851;&#27880;&#65292;&#24182;&#23637;&#31034;&#19981;&#21516;FLMs&#20043;&#38388;&#30340;&#21327;&#35843;&#21644;&#21512;&#20316;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;(FLMs)&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#32773;&#27491;&#22312;&#24320;&#21457;&#26356;&#22823;&#30340;FLMs&#65288;&#20363;&#22914;&#65292;XLNet&#12289;T5&#65289;&#20197;&#23454;&#29616;&#19978;&#19979;&#25991;&#21270;&#30340;&#35821;&#35328;&#34920;&#31034;&#12289;&#20998;&#31867;&#21644;&#29983;&#25104;&#12290;&#34429;&#28982;&#24320;&#21457;&#26356;&#22823;&#30340;FLMs&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#20294;&#20063;&#23384;&#22312;&#34394;&#26500;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#39118;&#38505;&#12290;&#20174;&#26681;&#26412;&#19978;&#35828;&#65292;&#26356;&#22823;&#30340;FLMs&#24314;&#31435;&#22312;&#36739;&#23567;&#30340;FLMs&#65288;&#20363;&#22914;&#65292;BERT&#65289;&#30340;&#22522;&#30784;&#20043;&#19978;&#65307;&#22240;&#27492;&#65292;&#20154;&#20204;&#24517;&#39035;&#35748;&#35782;&#21040;&#36739;&#23567;&#30340;FLMs&#30340;&#28508;&#21147;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#38598;&#25104;&#26469;&#23454;&#29616;&#12290;&#22312;&#24403;&#21069;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;FLMs&#21450;&#20854;&#38598;&#25104;&#36827;&#34892;&#20102;&#23454;&#38469;&#26816;&#39564;&#12290;&#25105;&#20204;&#20551;&#35774;FLMs&#30340;&#38598;&#25104;&#21487;&#20197;&#24433;&#21709;&#20854;&#20010;&#20307;&#20851;&#27880;&#65292;&#24182;&#25581;&#31034;&#19981;&#21516;FLMs&#20043;&#38388;&#30340;&#21327;&#35843;&#21644;&#21512;&#20316;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#21033;&#29992;BERT&#24182;&#23450;&#20041;&#20102;&#19977;&#31181;&#20854;&#20182;&#30340;&#38598;&#25104;&#25216;&#26415;&#65306;{&#27973;&#23618;&#12289;&#21322;&#28145;&#23618;&#21644;&#28145;&#23618;}&#65292;&#20854;&#20013;&#28145;&#23618;&#38598;&#25104;&#24341;&#20837;&#20102;&#19968;&#20010;&#30693;&#35782;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundational Language Models (FLMs) have advanced natural language processing (NLP) research. Current researchers are developing larger FLMs (e.g., XLNet, T5) to enable contextualized language representation, classification, and generation. While developing larger FLMs has been of significant advantage, it is also a liability concerning hallucination and predictive uncertainty. Fundamentally, larger FLMs are built on the same foundations as smaller FLMs (e.g., BERT); hence, one must recognize the potential of smaller FLMs which can be realized through an ensemble. In the current research, we perform a reality check on FLMs and their ensemble on benchmark and real-world datasets. We hypothesize that the ensembling of FLMs can influence the individualistic attention of FLMs and unravel the strength of coordination and cooperation of different FLMs. We utilize BERT and define three other ensemble techniques: {Shallow, Semi, and Deep}, wherein the Deep-Ensemble introduces a knowledge-guide
&lt;/p&gt;</description></item><item><title>Prompt2Model&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20219;&#21153;&#26469;&#35757;&#32451;&#36866;&#21512;&#37096;&#32626;&#30340;&#27169;&#22411;&#65292;&#30456;&#27604;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#20204;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12261</link><description>&lt;p&gt;
Prompt2Model&#65306;&#20174;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#29983;&#25104;&#21487;&#37096;&#32626;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prompt2Model: Generating Deployable Models from Natural Language Instructions. (arXiv:2308.12261v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12261
&lt;/p&gt;
&lt;p&gt;
Prompt2Model&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20219;&#21153;&#26469;&#35757;&#32451;&#36866;&#21512;&#37096;&#32626;&#30340;&#27169;&#22411;&#65292;&#30456;&#27604;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#20204;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20351;&#24471;&#31995;&#32479;&#26500;&#24314;&#32773;&#33021;&#22815;&#36890;&#36807;&#25552;&#31034;&#26469;&#21019;&#24314;&#33021;&#32988;&#20219;&#30340;NLP&#31995;&#32479;&#65292;&#20182;&#20204;&#21482;&#38656;&#35201;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20219;&#21153;&#24182;&#25552;&#20379;&#19968;&#20123;&#31034;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#20854;&#20182;&#26041;&#38754;&#65292;LLM&#26159;&#20256;&#32479;&#19987;&#29992;NLP&#27169;&#22411;&#30340;&#19968;&#31181;&#36864;&#27493;&#65307;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#37096;&#32626;&#65292;&#24182;&#21487;&#33021;&#34987;API&#23553;&#38145;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Prompt2Model&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#23427;&#25509;&#25910;&#31867;&#20284;LLMs&#25552;&#20379;&#30340;&#20219;&#21153;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#36755;&#20837;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#35757;&#32451;&#19968;&#20010;&#36866;&#21512;&#37096;&#32626;&#30340;&#19987;&#29992;&#27169;&#22411;&#12290;&#36825;&#36890;&#36807;&#22810;&#27493;&#39588;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26816;&#32034;&#12289;&#20351;&#29992;LLMs&#29983;&#25104;&#25968;&#25454;&#38598;&#20197;&#21450;&#36825;&#20123;&#26816;&#32034;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#30417;&#30563;&#24494;&#35843;&#26469;&#23436;&#25104;&#12290;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#32473;&#23450;&#30456;&#21516;&#30340;&#23569;&#26679;&#26412;&#25552;&#31034;&#20316;&#20026;&#36755;&#20837;&#24773;&#20917;&#19979;&#65292;Prompt2Model&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#30456;&#27604;&#24378;&#22823;&#30340;LLM - gpt-3.5-turbo&#24179;&#22343;&#24615;&#33021;&#25552;&#21319;20%&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22312;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#36991;&#20813;&#29983;&#25104;&#29256;&#26435;&#25968;&#25454;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21644;&#20248;&#21270;&#35270;&#20026;softmax&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#19968;&#31181;&#39640;&#25928;&#36827;&#34892;softmax&#22238;&#24402;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.12247</link><description>&lt;p&gt;
&#22914;&#20309;&#22312;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20445;&#25252;&#29256;&#26435;&#25968;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Protect Copyright Data in Optimization of Large Language Models?. (arXiv:2308.12247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22312;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#36991;&#20813;&#29983;&#25104;&#29256;&#26435;&#25968;&#25454;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21644;&#20248;&#21270;&#35270;&#20026;softmax&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#19968;&#31181;&#39640;&#25928;&#36827;&#34892;softmax&#22238;&#24402;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#35745;&#31639;&#26426;&#30740;&#31350;&#21644;&#24212;&#29992;&#20013;&#21457;&#25381;&#20102;&#21464;&#38761;&#24615;&#30340;&#20316;&#29992;&#12290;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#36755;&#20986;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#25968;&#25454;&#24341;&#21457;&#20102;&#20105;&#35758;&#65292;&#36825;&#21487;&#33021;&#21457;&#29983;&#22312;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#26412;&#36523;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#24773;&#20917;&#19979;&#12290;LLM&#26159;&#24314;&#31435;&#22312;Transformer&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19978;&#30340;&#65292;&#32780;Transformer&#20381;&#36182;&#19968;&#31181;&#31216;&#20026;Attention&#30340;&#25968;&#23398;&#35745;&#31639;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;softmax&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#20248;&#21270;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;softmax&#22238;&#24402;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#39640;&#25928;&#36827;&#34892;softmax&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#20197;&#38450;&#27490;&#22238;&#24402;&#20989;&#25968;&#29983;&#25104;&#29256;&#26435;&#25968;&#25454;&#12290;&#36825;&#20026;&#22312;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#36991;&#20813;&#29983;&#25104;&#29256;&#26435;&#25968;&#25454;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function.  In this paper, we show that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12219</link><description>&lt;p&gt;
&#25193;&#23637;&#24615;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23436;&#25104;&#22810;&#31181;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. (arXiv:2308.12219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36215;&#24471;&#30410;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#35299;&#20915;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#26041;&#38754;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#25105;&#20204;&#36890;&#36807;&#20808;&#36890;&#36807;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#39044;&#35757;&#32451;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#20877;&#36890;&#36807;&#25193;&#25955;&#36866;&#24212;&#23558;&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20026;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#21644;&#25351;&#23548;&#35843;&#20248;&#26469;&#21457;&#25496;&#20854;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#20013;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream langua
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20026;&#20363;&#65292;&#26816;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#20013;&#23398;&#26415;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#65292;&#24182;&#21457;&#29616;&#20102;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20219;&#21153;&#19981;&#31526;&#21512;&#22312;&#32447;&#26381;&#21153;&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#19981;&#30495;&#23454;&#12289;&#35780;&#20272;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#31561;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.12215</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#26041;&#38754;&#30340;&#25361;&#25112;&#65306;&#19968;&#20010;&#38024;&#23545;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection. (arXiv:2308.12215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20026;&#20363;&#65292;&#26816;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#20013;&#23398;&#26415;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#65292;&#24182;&#21457;&#29616;&#20102;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20219;&#21153;&#19981;&#31526;&#21512;&#22312;&#32447;&#26381;&#21153;&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#19981;&#30495;&#23454;&#12289;&#35780;&#20272;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#31561;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#26816;&#26597;&#20102;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#19978;&#23398;&#26415;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#12290;&#25105;&#20204;&#23545;&#35813;&#39046;&#22495;&#20013;270&#31687;&#24191;&#21463;&#24341;&#29992;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#33258;&#21160;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#30340;&#25991;&#29486;&#31995;&#32479;&#21270;&#65292;&#24182;&#23545;&#23376;&#38598;&#20013;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#25968;&#25454;&#21644;&#20195;&#30721;&#30340;&#21487;&#29992;&#24615;&#12289;&#35774;&#35745;&#22833;&#35823;&#12289;&#21487;&#22797;&#29616;&#24615;&#21644;&#27867;&#21270;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#25991;&#29486;&#20013;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#36825;&#23545;&#25152;&#22768;&#31216;&#30340;&#24615;&#33021;&#21644;&#23454;&#29992;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#26816;&#27979;&#20219;&#21153;&#36890;&#24120;&#19982;&#22312;&#32447;&#26381;&#21153;&#30495;&#27491;&#38754;&#20020;&#30340;&#25361;&#25112;&#26377;&#26412;&#36136;&#19978;&#30340;&#21306;&#21035;&#12290;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#36890;&#24120;&#19981;&#20195;&#34920;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#26223;&#65292;&#32780;&#19988;&#35780;&#20272;&#24448;&#24448;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#30340;&#21487;&#29992;&#24615;&#24456;&#24046;&#12290;&#27169;&#22411;&#22312;&#39046;&#22495;&#22806;&#30340;&#25968;&#25454;&#19978;&#27867;&#21270;&#33021;&#21147;&#19981;&#24378;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. We find significant shortcomings in the literature that call into question claimed performance and practicality. Detection tasks are often meaningfully distinct from the challenges that online services actually face. Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. Data and code availability is poor. Models do not generalize well to out-of-domain data. Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. Our aim is fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#20309;&#35838;&#31243;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#26377;&#38480;&#30340;&#25104;&#21151;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#23558;&#35838;&#31243;&#19982;Adam&#20248;&#21270;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#23427;&#20204;&#23398;&#20064;&#36866;&#24212;&#20102;&#27425;&#20248;&#21270;&#21442;&#25968;&#65292;&#23548;&#33268;&#20854;&#33030;&#24369;&#24615;&#22686;&#21152;</title><link>http://arxiv.org/abs/2308.12202</link><description>&lt;p&gt;
&#20351;&#29992;Adam&#36827;&#34892;&#35838;&#31243;&#23398;&#20064;&#65306;&#39764;&#39740;&#22312;&#20110;&#38169;&#35823;&#30340;&#32454;&#33410;
&lt;/p&gt;
&lt;p&gt;
Curriculum Learning with Adam: The Devil Is in the Wrong Details. (arXiv:2308.12202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#20309;&#35838;&#31243;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#26377;&#38480;&#30340;&#25104;&#21151;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#23558;&#35838;&#31243;&#19982;Adam&#20248;&#21270;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#23427;&#20204;&#23398;&#20064;&#36866;&#24212;&#20102;&#27425;&#20248;&#21270;&#21442;&#25968;&#65292;&#23548;&#33268;&#20854;&#33030;&#24369;&#24615;&#22686;&#21152;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35838;&#31243;&#23398;&#20064;&#65288;CL&#65289;&#35748;&#20026;&#65292;&#19982;&#20154;&#31867;&#31867;&#20284;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#26356;&#26377;&#25928;&#22320;&#20174;&#19982;&#20854;&#24403;&#21069;&#23398;&#20064;&#36827;&#23637;&#30456;&#21305;&#37197;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;CL&#26041;&#27861;&#20173;&#28982;&#34987;&#24456;&#23569;&#20102;&#35299;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#23588;&#20854;&#22914;&#27492;&#65292;&#20854;&#21462;&#24471;&#30340;&#25104;&#26524;&#20063;&#26377;&#38480;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20854;&#20013;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#23581;&#35797;&#22797;&#29616;&#21644;&#25193;&#23637;&#19968;&#20123;&#26368;&#36817;&#30340;&#35838;&#31243;&#26041;&#27861;&#65292;&#20294;&#21457;&#29616;&#24403;&#24212;&#29992;&#20110;NLP&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#32467;&#26524;&#20986;&#22855;&#22320;&#33030;&#24369;&#12290;&#23545;&#26576;&#20123;&#24773;&#20917;&#19979;&#35838;&#31243;&#25928;&#26524;&#30340;&#28145;&#20837;&#30740;&#31350;&#21521;&#25105;&#20204;&#23637;&#31034;&#20102;&#21407;&#22240;&#65306;&#24403;&#23558;&#35838;&#31243;&#19982;&#24191;&#21463;&#27426;&#36814;&#30340;Adam&#20248;&#21270;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#23398;&#20064;&#36866;&#24212;&#27492;&#31639;&#27861;&#30340;&#27425;&#20248;&#21270;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#19981;&#21516;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#28041;&#21450;&#24120;&#35265;&#30340;&#25163;&#24037;&#21046;&#20316;&#21644;&#33258;&#21160;&#21270;CL&#26041;&#27861;&#65292;&#20197;&#35828;&#26126;&#36825;&#31181;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20854;&#20013;&#27809;&#26377;&#19968;&#20010;&#33021;&#22815;&#32988;&#36807;&#20165;&#20351;&#29992;&#31934;&#24515;&#36873;&#25321;&#30340;Adam&#36827;&#34892;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Curriculum learning (CL) posits that machine learning models -- similar to humans -- may learn more efficiently from data that match their current learning progress. However, CL methods are still poorly understood and, in particular for natural language processing (NLP), have achieved only limited success. In this paper, we explore why. Starting from an attempt to replicate and extend a number of recent curriculum methods, we find that their results are surprisingly brittle when applied to NLP. A deep dive into the (in)effectiveness of the curricula in some scenarios shows us why: when curricula are employed in combination with the popular Adam optimisation algorithm, they oftentimes learn to adapt to suboptimally chosen optimisation parameters for this algorithm. We present a number of different case studies with different common hand-crafted and automated CL approaches to illustrate this phenomenon, and we find that none of them outperforms optimisation with only Adam with well-chose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#24544;&#23454;&#24615;&#65292;&#36890;&#36807;&#35745;&#31639;&#30001;&#35821;&#22659;&#25903;&#25345;&#30340;&#20027;&#24352;&#20013;&#26368;&#38271;&#30340;&#38750;&#36830;&#32493;&#23376;&#23383;&#31526;&#20018;&#65292;&#31216;&#20043;&#20026;&#26368;&#38271;&#25903;&#25345;&#23376;&#24207;&#21015;&#65288;LSS&#65289;&#12290;&#35777;&#26126;&#20102;&#24403;&#20351;&#29992;LSS&#26102;&#65292;&#36825;&#31181;&#24230;&#37327;&#19982;&#20154;&#31867;&#35780;&#20998;&#26356;&#30456;&#20851;&#65292;&#24182;&#19988;&#22312;&#24544;&#23454;&#24615;&#35780;&#20272;&#19978;&#30456;&#36739;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#24230;&#37327;&#26377;18%&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.12157</link><description>&lt;p&gt;
&#29992;&#26368;&#38271;&#25903;&#25345;&#23376;&#24207;&#21015;&#35780;&#20272;&#24544;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Faithfulness Using the Longest Supported Subsequence. (arXiv:2308.12157v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#24544;&#23454;&#24615;&#65292;&#36890;&#36807;&#35745;&#31639;&#30001;&#35821;&#22659;&#25903;&#25345;&#30340;&#20027;&#24352;&#20013;&#26368;&#38271;&#30340;&#38750;&#36830;&#32493;&#23376;&#23383;&#31526;&#20018;&#65292;&#31216;&#20043;&#20026;&#26368;&#38271;&#25903;&#25345;&#23376;&#24207;&#21015;&#65288;LSS&#65289;&#12290;&#35777;&#26126;&#20102;&#24403;&#20351;&#29992;LSS&#26102;&#65292;&#36825;&#31181;&#24230;&#37327;&#19982;&#20154;&#31867;&#35780;&#20998;&#26356;&#30456;&#20851;&#65292;&#24182;&#19988;&#22312;&#24544;&#23454;&#24615;&#35780;&#20272;&#19978;&#30456;&#36739;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#24230;&#37327;&#26377;18%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26085;&#30410;&#22797;&#26434;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#30340;&#21487;&#20449;&#24230;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#24635;&#32467;&#21644;&#38382;&#31572;&#31561;&#20219;&#21153;&#20013;&#12290;&#30001;&#20110;&#35821;&#35328;&#22810;&#26679;&#24615;&#21644;&#21487;&#33021;&#30340;&#31572;&#26696;&#31181;&#31867;&#32321;&#22810;&#65292;&#30830;&#20445;&#23427;&#20204;&#30340;&#21709;&#24212;&#22312;&#35821;&#22659;&#20013;&#26377;&#26681;&#25454;&#12289;&#24544;&#23454;&#21487;&#20449;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#24544;&#23454;&#24615;&#65292;&#36890;&#36807;&#35745;&#31639;&#30001;&#35821;&#22659;&#25903;&#25345;&#30340;&#20027;&#24352;&#20013;&#26368;&#38271;&#30340;&#38750;&#36830;&#32493;&#23376;&#23383;&#31526;&#20018;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#26368;&#38271;&#25903;&#25345;&#23376;&#24207;&#21015;&#65288;LSS&#65289;&#12290;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24494;&#35843;&#20102;&#19968;&#20010;&#27169;&#22411;&#20197;&#29983;&#25104;LSS&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#24403;&#20351;&#29992;LSS&#26102;&#65292;&#36825;&#20123;&#24230;&#37327;&#19982;&#20154;&#31867;&#35780;&#20998;&#26356;&#30456;&#20851;&#65292;&#30456;&#23545;&#20110;&#19981;&#20351;&#29992;LSS&#26102;&#12290;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#24230;&#37327;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#24544;&#23454;&#24615;&#30340;&#26368;&#20808;&#36827;&#24230;&#37327;&#30340;&#25913;&#36827;&#36798;&#21040;&#20102;18%&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#24635;&#32467;&#20219;&#21153;&#19978;&#19968;&#30452;&#20248;&#20110;&#20854;&#20182;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
As increasingly sophisticated language models emerge, their trustworthiness becomes a pivotal issue, especially in tasks such as summarization and question-answering. Ensuring their responses are contextually grounded and faithful is challenging due to the linguistic diversity and the myriad of possible answers. In this paper, we introduce a novel approach to evaluate faithfulness of machine-generated text by computing the longest noncontinuous substring of the claim that is supported by the context, which we refer to as the Longest Supported Subsequence (LSS). Using a new human-annotated dataset, we finetune a model to generate LSS. We introduce a new method of evaluation and demonstrate that these metrics correlate better with human ratings when LSS is employed, as opposed to when it is not. Our proposed metric demonstrates an 18% enhancement over the prevailing state-of-the-art metric for faithfulness on our dataset. Our metric consistently outperforms other metrics on a summarizati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#32599;&#39532;&#23612;&#20122;&#35821;&#30340;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#23454;&#38469;&#25968;&#25454;&#38598;&#25214;&#20986;&#20102;&#24433;&#21709;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#30340;&#26368;&#37325;&#35201;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.12131</link><description>&lt;p&gt;
&#38754;&#21521;&#32599;&#39532;&#23612;&#20122;&#35821;&#30340;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semantic Change Detection for the Romanian Language. (arXiv:2308.12131v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#32599;&#39532;&#23612;&#20122;&#35821;&#30340;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#23454;&#38469;&#25968;&#25454;&#38598;&#25214;&#20986;&#20102;&#24433;&#21709;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#30340;&#26368;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#20041;&#21464;&#21270;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#30740;&#31350;&#26102;&#26399;&#35821;&#26009;&#24211;&#20013;&#21333;&#35789;&#30340;&#29992;&#27861;&#26469;&#35782;&#21035;&#21333;&#35789;&#24847;&#20041;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#22312;&#30495;&#23454;&#30340;&#33521;&#35821;&#21644;&#32599;&#39532;&#23612;&#20122;&#35821;&#25968;&#25454;&#38598;&#19978;&#20998;&#26512;&#20102;&#21019;&#24314;&#38745;&#24577;&#21644;&#19978;&#19979;&#25991;&#21333;&#35789;&#23884;&#20837;&#27169;&#22411;&#30340;&#19981;&#21516;&#31574;&#30053;&#65292;&#21363;Word2Vec&#21644;ELMo&#12290;&#20026;&#20102;&#27979;&#35797;&#25105;&#20204;&#30340;&#27969;&#31243;&#24182;&#30830;&#23450;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#20010;&#33521;&#35821;&#25968;&#25454;&#38598;(SEMEVAL-CCOHA)&#19978;&#35780;&#20272;&#20102;&#36825;&#20004;&#31181;&#21333;&#35789;&#23884;&#20837;&#27169;&#22411;&#12290;&#27492;&#21518;&#65292;&#25105;&#20204;&#23558;&#23454;&#39564;&#37325;&#28857;&#25918;&#22312;&#32599;&#39532;&#23612;&#20122;&#35821;&#25968;&#25454;&#38598;&#19978;&#65292;&#24182;&#24378;&#35843;&#20102;&#36825;&#31181;&#36164;&#28304;&#26377;&#38480;&#35821;&#35328;&#20013;&#35821;&#20041;&#21464;&#21270;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#22914;&#24847;&#20041;&#30340;&#33719;&#21462;&#21644;&#20002;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26681;&#25454;&#35821;&#26009;&#24211;&#30340;&#19981;&#21516;&#65292;&#36873;&#25321;&#27169;&#22411;&#21644;&#35745;&#31639;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#20998;&#25968;&#30340;&#36317;&#31163;&#26159;&#32771;&#34385;&#30340;&#26368;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic semantic change methods try to identify the changes that appear over time in the meaning of words by analyzing their usage in diachronic corpora. In this paper, we analyze different strategies to create static and contextual word embedding models, i.e., Word2Vec and ELMo, on real-world English and Romanian datasets. To test our pipeline and determine the performance of our models, we first evaluate both word embedding models on an English dataset (SEMEVAL-CCOHA). Afterward, we focus our experiments on a Romanian dataset, and we underline different aspects of semantic changes in this low-resource language, such as meaning acquisition and loss. The experimental results show that, depending on the corpus, the most important factors to consider are the choice of model and the distance to calculate a score for detecting semantic change.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20219;&#21153;&#25351;&#20196;&#30340;&#20301;&#32622;&#65292;&#20174;&#32780;&#24357;&#34917;&#25351;&#20196;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#35268;&#27169;&#21644;&#38271;&#24230;&#30340;&#24207;&#21015;&#19978;&#22343;&#20248;&#20110;&#20256;&#32479;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2308.12097</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24207;&#21015;&#29983;&#25104;&#20013;&#65292;&#25351;&#20196;&#20301;&#32622;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Instruction Position Matters in Sequence Generation with Large Language Models. (arXiv:2308.12097v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20219;&#21153;&#25351;&#20196;&#30340;&#20301;&#32622;&#65292;&#20174;&#32780;&#24357;&#34917;&#25351;&#20196;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#35268;&#27169;&#21644;&#38271;&#24230;&#30340;&#24207;&#21015;&#19978;&#22343;&#20248;&#20110;&#20256;&#32479;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#25191;&#34892;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#65292;&#22914;&#32763;&#35793;&#25110;&#25688;&#35201;&#12290;&#24494;&#35843;&#25968;&#25454;&#36890;&#24120;&#26159;&#20174;&#29305;&#23450;&#20219;&#21153;&#25351;&#20196;&#12289;&#36755;&#20837;&#21477;&#23376;&#21644;&#23545;&#24212;&#22238;&#24212;&#20018;&#32852;&#32780;&#25104;&#12290;&#32771;&#34385;&#21040;LLMs&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#24314;&#27169;&#30340;&#23616;&#37096;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20026;&#38271;&#36755;&#20837;&#21477;&#23376;&#29983;&#25104;&#22238;&#24212;&#26102;&#38754;&#20020;&#25351;&#20196;&#36951;&#24536;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23558;&#20219;&#21153;&#25351;&#20196;&#30340;&#20301;&#32622;&#31227;&#21160;&#21040;&#36755;&#20837;&#21477;&#23376;&#20043;&#21518;&#65292;&#22686;&#24378;LLMs&#30340;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#26041;&#27861;&#21487;&#20197;&#25913;&#21464;&#27169;&#22411;&#30340;&#23398;&#20064;&#37325;&#28857;&#65292;&#20174;&#32780;&#24378;&#35843;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#30340;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#35268;&#27169;&#65288;1B / 7B / 13B&#65289;&#21644;&#19981;&#21516;&#24207;&#21015;&#38271;&#24230;&#19979;&#22987;&#32456;&#20248;&#20110;&#20256;&#32479;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are capable of performing conditional sequence generation tasks, such as translation or summarization, through instruction fine-tuning. The fine-tuning data is generally sequentially concatenated from a specific task instruction, an input sentence, and the corresponding response. Considering the locality modeled by the self-attention mechanism of LLMs, these models face the risk of instruction forgetting when generating responses for long input sentences. To mitigate this issue, we propose enhancing the instruction-following capability of LLMs by shifting the position of task instructions after the input sentences. Theoretical analysis suggests that our straightforward method can alter the model's learning focus, thereby emphasizing the training of instruction-following capabilities. Concurrently, experimental results demonstrate that our approach consistently outperforms traditional settings across various model scales (1B / 7B / 13B) and different sequenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#29615;&#22659;&#65292;&#20316;&#20026;&#25915;&#20987;&#20195;&#29702;&#20154;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#12290;&#35813;&#35774;&#35745;&#34920;&#26126;LLMs&#22312;&#39640;&#25928;&#24212;&#23545;&#22797;&#26434;&#20915;&#31574;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19982;&#32463;&#36807;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#20195;&#29702;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12086</link><description>&lt;p&gt;
&#36208;&#20986;&#31548;&#23376;&#65306;&#38543;&#26426;&#40550;&#40521;&#22312;&#32593;&#32476;&#23433;&#20840;&#29615;&#22659;&#20013;&#30340;&#32988;&#21033;
&lt;/p&gt;
&lt;p&gt;
Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments. (arXiv:2308.12086v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#29615;&#22659;&#65292;&#20316;&#20026;&#25915;&#20987;&#20195;&#29702;&#20154;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#12290;&#35813;&#35774;&#35745;&#34920;&#26126;LLMs&#22312;&#39640;&#25928;&#24212;&#23545;&#22797;&#26434;&#20915;&#31574;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19982;&#32463;&#36807;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#20195;&#29702;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#28041;&#21450;&#25991;&#26412;&#29983;&#25104;&#12289;&#25688;&#35201;&#21644;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#19981;&#21516;&#39046;&#22495;&#20013;&#24191;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#23384;&#22312;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#22522;&#20110;LLM&#30340;&#35774;&#35745;&#22312;&#35268;&#21010;&#21644;&#23548;&#33322;&#24320;&#25918;&#19990;&#30028;&#22330;&#26223;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#23558;&#39044;&#35757;&#32451;&#30340;LLMs&#29992;&#20316;&#32593;&#32476;&#23433;&#20840;&#29615;&#22659;&#20013;&#30340;&#20195;&#29702;&#20154;&#30340;&#26032;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#22312;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#20316;&#20026;&#20004;&#20010;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#25915;&#20987;&#20195;&#29702;&#12290;&#22312;&#22823;&#22810;&#25968;&#22330;&#26223;&#21644;&#37197;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20195;&#29702;&#22312;&#34920;&#29616;&#19978;&#19982;&#32463;&#36807;&#25968;&#21315;&#27425;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#20195;&#29702;&#30456;&#20284;&#25110;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#26368;&#20339;LLM&#20195;&#29702;&#22312;&#27809;&#26377;&#20219;&#20309;&#39069;&#22806;&#35757;&#32451;&#36807;&#31243;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19982;&#29615;&#22659;&#30340;&#20154;&#31867;&#27979;&#35797;&#32773;&#31867;&#20284;&#12290;&#36825;&#31181;&#35774;&#35745;&#31361;&#26174;&#20102;LLMs&#22312;&#39640;&#25928;&#24212;&#23545;&#22797;&#26434;&#20915;&#31574;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have gained widespread popularity across diverse domains involving text generation, summarization, and various natural language processing tasks. Despite their inherent limitations, LLM-based designs have shown promising capabilities in planning and navigating open-world scenarios. This paper introduces a novel application of pre-trained LLMs as agents within cybersecurity network environments, focusing on their utility for sequential decision-making processes.  We present an approach wherein pre-trained LLMs are leveraged as attacking agents in two reinforcement learning environments. Our proposed agents demonstrate similar or better performance against state-of-the-art agents trained for thousands of episodes in most scenarios and configurations. In addition, the best LLM agents perform similarly to human testers of the environment without any additional training process. This design highlights the potential of LLMs to efficiently address complex decision
&lt;/p&gt;</description></item><item><title>InstructionGPT-4&#36890;&#36807;&#20165;&#20351;&#29992;200&#20010;&#20363;&#23376;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#36136;&#37327;&#24230;&#37327;&#21644;&#36873;&#25321;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#20219;&#21153;&#20013;&#20248;&#20110;&#21407;&#22987;&#30340;MiniGPT-4&#12290;</title><link>http://arxiv.org/abs/2308.12067</link><description>&lt;p&gt;
InstructionGPT-4: &#19968;&#20010;200&#25351;&#20196;&#33539;&#24335;&#29992;&#20110;&#24494;&#35843;MiniGPT-4
&lt;/p&gt;
&lt;p&gt;
InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4. (arXiv:2308.12067v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12067
&lt;/p&gt;
&lt;p&gt;
InstructionGPT-4&#36890;&#36807;&#20165;&#20351;&#29992;200&#20010;&#20363;&#23376;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#36136;&#37327;&#24230;&#37327;&#21644;&#36873;&#25321;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#20219;&#21153;&#20013;&#20248;&#20110;&#21407;&#22987;&#30340;MiniGPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#36807;&#31243;&#33719;&#21462;&#20854;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65306;&#22312;&#22270;&#20687;-&#25991;&#26412;&#23545;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30417;&#30563;&#24335;&#35270;&#35273;-&#35821;&#35328;&#25351;&#20196;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#26377;&#38480;&#37327;&#30340;&#39640;&#36136;&#37327;&#36981;&#24490;&#25351;&#20196;&#25968;&#25454;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;InstructionGPT-4&#65292;&#23427;&#32463;&#36807;&#24494;&#35843;&#30340;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;200&#20010;&#20363;&#23376;&#65292;&#32422;&#21344;MiniGPT-4&#23545;&#40784;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#30340;&#36981;&#24490;&#25351;&#20196;&#25968;&#25454;&#30340;6%&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#20960;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#36136;&#37327;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;&#22522;&#20110;&#36825;&#20123;&#24230;&#37327;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#36873;&#25321;&#22120;&#65292;&#33258;&#21160;&#35782;&#21035;&#21644;&#36807;&#28388;&#20302;&#36136;&#37327;&#30340;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#12290;&#36890;&#36807;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;InstructionGPT-4&#22312;&#21508;&#31181;&#35780;&#20272;&#65288;&#22914;&#35270;&#35273;&#38382;&#31572;&#12289;GPT-4&#20559;&#22909;&#65289;&#19978;&#20248;&#20110;&#21407;&#22987;&#30340;MiniGPT-4&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findi
&lt;/p&gt;</description></item><item><title>FlexKBQA&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#31243;&#24207;&#32763;&#35793;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#31639;&#27861;&#20174;&#30693;&#35782;&#24211;&#20013;&#25277;&#26679;&#22810;&#26679;&#30340;&#31243;&#24207;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#20174;&#32780;&#35299;&#20915;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#31572;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12060</link><description>&lt;p&gt;
FlexKBQA&#65306;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#28789;&#27963;LLM&#39537;&#21160;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering. (arXiv:2308.12060v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12060
&lt;/p&gt;
&lt;p&gt;
FlexKBQA&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#31243;&#24207;&#32763;&#35793;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#31639;&#27861;&#20174;&#30693;&#35782;&#24211;&#20013;&#25277;&#26679;&#22810;&#26679;&#30340;&#31243;&#24207;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#20174;&#32780;&#35299;&#20915;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#31572;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#26159;&#19968;&#20010;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#30693;&#35782;&#24211;&#20013;&#30340;&#23454;&#20307;&#25968;&#37327;&#24222;&#22823;&#65292;&#24182;&#19988;&#29992;&#25143;&#25552;&#20986;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#22810;&#26679;&#21270;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;KBQA&#27169;&#22411;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#22240;&#20026;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#19981;&#36275;&#12290;&#20026;&#20102;&#20943;&#36731;&#25163;&#21160;&#27880;&#37322;&#30340;&#36127;&#25285;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#31243;&#24207;&#32763;&#35793;&#22120;&#65292;&#20171;&#32461;&#20102;FlexKBQA&#26469;&#35299;&#20915;&#23569;&#26679;&#26412;KBQA&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FlexKBQA&#21033;&#29992;&#33258;&#21160;&#21270;&#31639;&#27861;&#20174;&#30693;&#35782;&#24211;&#20013;&#25277;&#26679;&#22810;&#26679;&#30340;&#31243;&#24207;&#65288;&#22914;SPARQL&#26597;&#35810;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;LLMs&#23558;&#20854;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#36825;&#20010;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#26377;&#21161;&#20110;&#35757;&#32451;&#19968;&#20010;&#19987;&#38376;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#26469;&#22788;&#29702;&#30693;&#35782;&#24211;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#29992;&#25143;&#38382;&#39064;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#30340;&#38556;&#30861;&#65292;FlexKBQA&#24341;&#20837;&#20102;&#19968;&#20010;&#25191;&#34892;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge base question answering (KBQA) is a critical yet challenging task due to the vast number of entities within knowledge bases and the diversity of natural language questions posed by users. Unfortunately, the performance of most KBQA models tends to decline significantly in real-world scenarios where high-quality annotated data is insufficient. To mitigate the burden associated with manual annotation, we introduce FlexKBQA by utilizing Large Language Models (LLMs) as program translators for addressing the challenges inherent in the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms to sample diverse programs, such as SPARQL queries, from the knowledge base, which are subsequently converted into natural language questions via LLMs. This synthetic dataset facilitates training a specialized lightweight model for the KB. Additionally, to reduce the barriers of distribution shift between synthetic data and real user questions, FlexKBQA introduces an executiong
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#29983;&#25104;&#30340;&#26679;&#26412;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12289;&#22870;&#21169;&#21152;&#26435;&#22238;&#24402;&#21644;&#20915;&#31574;Transformer&#31561;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31867;&#20284;&#20110;&#30417;&#30563;&#24494;&#35843;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2308.12050</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aligning Language Models with Offline Reinforcement Learning from Human Feedback. (arXiv:2308.12050v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#29983;&#25104;&#30340;&#26679;&#26412;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12289;&#22870;&#21169;&#21152;&#26435;&#22238;&#24402;&#21644;&#20915;&#31574;Transformer&#31561;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31867;&#20284;&#20110;&#30417;&#30563;&#24494;&#35843;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#26377;&#25928;&#22320;&#28385;&#36275;&#20154;&#31867;&#38656;&#27714;&#21644;&#31038;&#20250;&#20215;&#20540;&#35266;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#26469;&#36981;&#24490;&#25351;&#20196;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25216;&#26415;&#65292;&#22914;Proximal Policy Optimization&#65288;PPO&#65289;&#65292;&#36825;&#20123;&#25216;&#26415;&#34987;&#35777;&#26126;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#19981;&#31283;&#23450;&#19988;&#38590;&#20197;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;PPO&#38656;&#35201;&#22797;&#26434;&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#23454;&#29616;&#65292;&#24433;&#21709;&#20102;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#25928;&#29575;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#29983;&#25104;&#30340;&#26679;&#26412;&#32780;&#19981;&#26159;&#19982;RL&#29615;&#22659;&#20132;&#20114;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#19982;&#36807;&#28388;&#12289;&#22870;&#21169;&#21152;&#26435;&#22238;&#24402;&#65288;RWR&#65289;&#20197;&#21450;&#20915;&#31574;Transformer&#65288;DT&#65289;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#12290;&#36890;&#36807;&#37319;&#29992;&#31867;&#20284;&#20110;&#30417;&#30563;&#24494;&#35843;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human preferences is crucial for language models (LMs) to effectively cater to human needs and societal values. Previous research has made notable progress by leveraging human feedback to follow instructions. However, these approaches rely primarily on online reinforcement learning (RL) techniques like Proximal Policy Optimization (PPO), which have been proven unstable and challenging to tune for language models. Moreover, PPO requires complex distributed system implementation, hindering the efficiency of large-scale distributed training. In this study, we propose an offline reinforcement learning from human feedback (RLHF) framework to align LMs using pre-generated samples without interacting with RL environments. Specifically, we explore maximum likelihood estimation (MLE) with filtering, reward-weighted regression (RWR), and Decision Transformer (DT) to align language models to human preferences. By employing a loss function similar to supervised fine-tuning, our metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CgT-GAN&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;&#20026;&#27169;&#22411;&#25552;&#20379;&#35270;&#35273;&#20449;&#24687;&#65292;&#32467;&#21512;&#23545;&#25239;&#35757;&#32451;&#21644;&#22522;&#20110;CLIP&#30340;&#22870;&#21169;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22270;&#20687;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.12045</link><description>&lt;p&gt;
CgT-GAN: CLIP&#24341;&#23548;&#30340;&#25991;&#26412;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
CgT-GAN: CLIP-guided Text GAN for Image Captioning. (arXiv:2308.12045v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CgT-GAN&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;&#20026;&#27169;&#22411;&#25552;&#20379;&#35270;&#35273;&#20449;&#24687;&#65292;&#32467;&#21512;&#23545;&#25239;&#35757;&#32451;&#21644;&#22522;&#20110;CLIP&#30340;&#22870;&#21169;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22270;&#20687;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;CLIP&#22312;&#27809;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#22270;&#20687;&#25551;&#36848;&#22330;&#26223;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#22270;&#20687;&#25551;&#36848;&#30340;&#25928;&#26524;&#12290;&#26368;&#36817;&#30340;CLIP-based&#22270;&#20687;&#25551;&#36848;&#26041;&#27861;&#37319;&#29992;&#20102;&#32431;&#25991;&#26412;&#35757;&#32451;&#33539;&#24335;&#65292;&#21363;&#20174;&#20849;&#20139;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#37325;&#24314;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#24046;&#36317;&#25110;&#32773;&#25991;&#26412;&#23884;&#20837;&#30340;&#23384;&#20648;&#38656;&#27714;&#19978;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#12290;&#32771;&#34385;&#21040;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#33719;&#21462;&#22270;&#20687;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#30340;&#25991;&#26412;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CgT-GAN&#65289;&#65292;&#23558;&#22270;&#20687;&#21152;&#20837;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#8220;&#30475;&#21040;&#8221;&#30495;&#23454;&#30340;&#35270;&#35273;&#27169;&#24577;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#20351;&#24471;CgT-GAN&#33021;&#22815;&#27169;&#20223;&#22806;&#37096;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#30701;&#35821;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;CLIP&#30340;&#22870;&#21169;&#25552;&#20379;&#35821;&#20041;&#24341;&#23548;&#12290;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#22120;&#36890;&#36807;GAN&#30340;&#37492;&#21035;&#22120;&#35745;&#31639;&#30340;&#33258;&#28982;&#24230;&#21644;&#22522;&#20110;&#20154;&#31867;&#35821;&#35328;&#30340;&#35821;&#20041;&#20174;&#32780;&#33719;&#24471;&#32852;&#21512;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large-scale visual-language pre-trained model, Contrastive Language-Image Pre-training (CLIP), has significantly improved image captioning for scenarios without human-annotated image-caption pairs. Recent advanced CLIP-based image captioning without human annotations follows a text-only training paradigm, i.e., reconstructing text from shared embedding space. Nevertheless, these approaches are limited by the training/inference gap or huge storage requirements for text embeddings. Given that it is trivial to obtain images in the real world, we propose CLIP-guided text GAN (CgT-GAN), which incorporates images into the training process to enable the model to "see" real visual modality. Particularly, we use adversarial training to teach CgT-GAN to mimic the phrases of an external text corpus and CLIP-based reward to provide semantic guidance. The caption generator is jointly rewarded based on the caption naturalness to human language calculated from the GAN's discriminator and the sema
&lt;/p&gt;</description></item><item><title>IncreLoRA&#26159;&#19968;&#31181;&#22686;&#37327;&#21442;&#25968;&#20998;&#37197;&#26041;&#27861;&#65292;&#26681;&#25454;&#27169;&#22359;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#28155;&#21152;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#12290;</title><link>http://arxiv.org/abs/2308.12043</link><description>&lt;p&gt;
IncreLoRA: &#22686;&#37327;&#21442;&#25968;&#20998;&#37197;&#26041;&#27861;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning. (arXiv:2308.12043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12043
&lt;/p&gt;
&lt;p&gt;
IncreLoRA&#26159;&#19968;&#31181;&#22686;&#37327;&#21442;&#25968;&#20998;&#37197;&#26041;&#27861;&#65292;&#26681;&#25454;&#27169;&#22359;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#28155;&#21152;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#22823;&#23567;&#19981;&#26029;&#22686;&#21152;&#65292;&#23545;&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#24182;&#19981;&#39640;&#25928;&#65292;&#29305;&#21035;&#26159;&#24403;&#23384;&#22312;&#22823;&#37327;&#30340;&#19979;&#28216;&#20219;&#21153;&#26102;&#65292;&#36825;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#35757;&#32451;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#24050;&#26377;&#35768;&#22810;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#65288;PEFT&#65289;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#20854;&#20013;&#65292;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#23558;&#21487;&#35757;&#32451;&#30340;&#31209;&#20998;&#35299;&#30697;&#38453;&#27880;&#20837;&#27599;&#20010;&#30446;&#26631;&#27169;&#22359;&#30340;&#20856;&#22411;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;LoRA&#24573;&#35270;&#20102;&#19981;&#21516;&#27169;&#22359;&#20013;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#21098;&#26525;LoRA&#30340;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#26465;&#20214;&#19979;&#65292;&#20462;&#21098;&#21442;&#25968;&#30697;&#38453;&#30340;&#31209;&#30340;&#19978;&#30028;&#20173;&#28982;&#21463;&#21040;&#39044;&#35774;&#20540;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IncreLoRA&#65292;&#19968;&#31181;&#22686;&#37327;&#21442;&#25968;&#20998;&#37197;&#26041;&#27861;&#65292;&#26681;&#25454;&#27599;&#20010;&#27169;&#22359;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#28155;&#21152;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#20462;&#21098;&#26041;&#27861;&#19981;&#21516;&#65292;&#22240;&#20026;&#23427;&#20174;&#22686;&#21152;&#21442;&#25968;&#30340;&#35282;&#24230;&#26469;&#20248;&#21270;&#35843;&#20248;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing size of pre-trained language models (PLMs), fine-tuning all the parameters in the model is not efficient, especially when there are a large number of downstream tasks, which incur significant training and storage costs. Many parameter-efficient fine-tuning (PEFT) approaches have been proposed, among which, Low-Rank Adaptation (LoRA) is a representative approach that injects trainable rank decomposition matrices into every target module. Yet LoRA ignores the importance of parameters in different modules. To address this problem, many works have been proposed to prune the parameters of LoRA. However, under limited training conditions, the upper bound of the rank of the pruned parameter matrix is still affected by the preset values. We, therefore, propose IncreLoRA, an incremental parameter allocation method that adaptively adds trainable parameters during training based on the importance scores of each module. This approach is different from the pruning method as it i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312; TREC 2022 &#28145;&#24230;&#23398;&#20064;&#36187;&#36947;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#30340;&#28151;&#21512;&#25991;&#26412;&#26816;&#32034;&#21644;&#22810;&#38454;&#27573;&#25991;&#26412;&#25490;&#21517;&#26041;&#27861;&#65292;&#22312;&#26816;&#32034;&#38454;&#27573;&#32467;&#21512;&#20102;&#20256;&#32479;&#31232;&#30095;&#26816;&#32034;&#21644;&#31070;&#32463;&#31264;&#23494;&#26816;&#32034;&#30340;&#20004;&#31181;&#32467;&#26500;&#65292;&#22312;&#25490;&#21517;&#38454;&#27573;&#26500;&#24314;&#20102;&#20840;&#20132;&#20114;&#24335;&#25490;&#21517;&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#23376;&#25490;&#21517;&#27169;&#22359;&#65292;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12039</link><description>&lt;p&gt;
TREC 2022&#28145;&#24230;&#23398;&#20064;&#36187;&#36947;&#30340;&#28151;&#21512;&#26816;&#32034;&#21644;&#22810;&#38454;&#27573;&#25991;&#26412;&#25490;&#21517;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep Learning Track. (arXiv:2308.12039v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312; TREC 2022 &#28145;&#24230;&#23398;&#20064;&#36187;&#36947;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#30340;&#28151;&#21512;&#25991;&#26412;&#26816;&#32034;&#21644;&#22810;&#38454;&#27573;&#25991;&#26412;&#25490;&#21517;&#26041;&#27861;&#65292;&#22312;&#26816;&#32034;&#38454;&#27573;&#32467;&#21512;&#20102;&#20256;&#32479;&#31232;&#30095;&#26816;&#32034;&#21644;&#31070;&#32463;&#31264;&#23494;&#26816;&#32034;&#30340;&#20004;&#31181;&#32467;&#26500;&#65292;&#22312;&#25490;&#21517;&#38454;&#27573;&#26500;&#24314;&#20102;&#20840;&#20132;&#20114;&#24335;&#25490;&#21517;&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#23376;&#25490;&#21517;&#27169;&#22359;&#65292;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#26816;&#32034;&#25216;&#26415;&#22312;&#21508;&#31181;&#23454;&#38469;&#19994;&#21153;&#22330;&#26223;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;TREC 2022&#28145;&#24230;&#23398;&#20064;&#36187;&#36947;&#20013;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25105;&#20204;&#35299;&#20915;&#26041;&#26696;&#20013;&#37319;&#29992;&#30340;&#28151;&#21512;&#25991;&#26412;&#26816;&#32034;&#21644;&#22810;&#38454;&#27573;&#25991;&#26412;&#25490;&#21517;&#26041;&#27861;&#12290;&#26816;&#32034;&#38454;&#27573;&#32467;&#21512;&#20102;&#20256;&#32479;&#31232;&#30095;&#26816;&#32034;&#21644;&#31070;&#32463;&#31264;&#23494;&#26816;&#32034;&#30340;&#20004;&#31181;&#32467;&#26500;&#12290;&#22312;&#25490;&#21517;&#38454;&#27573;&#65292;&#38500;&#20102;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#20840;&#20132;&#20114;&#24335;&#25490;&#21517;&#27169;&#22411;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#23376;&#25490;&#21517;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#26368;&#32456;&#30340;&#25991;&#26412;&#25490;&#21517;&#24615;&#33021;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27573;&#33853;&#25490;&#21517;&#21644;&#25991;&#26723;&#25490;&#21517;&#30340;&#27979;&#35797;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;&#31532;1&#21517;&#21644;&#31532;4&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale text retrieval technology has been widely used in various practical business scenarios. This paper presents our systems for the TREC 2022 Deep Learning Track. We explain the hybrid text retrieval and multi-stage text ranking method adopted in our solution. The retrieval stage combined the two structures of traditional sparse retrieval and neural dense retrieval. In the ranking stage, in addition to the full interaction-based ranking model built on large pre-trained language model, we also proposes a lightweight sub-ranking module to further enhance the final text ranking performance. Evaluation results demonstrate the effectiveness of our proposed approach. Our models achieve the 1st and 4th rank on the test set of passage ranking and document ranking respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#35757;&#32451;&#22823;&#22411;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20102;&#36328;&#35821;&#31181;&#38646;&#26679;&#26412;&#22810;&#27169;&#24335;&#23398;&#20064;&#65292;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12038</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#31181;&#38646;&#26679;&#26412;&#22810;&#27169;&#24335;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages. (arXiv:2308.12038v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#35757;&#32451;&#22823;&#22411;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20102;&#36328;&#35821;&#31181;&#38646;&#26679;&#26412;&#22810;&#27169;&#24335;&#23398;&#20064;&#65292;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#26041;&#38754;&#65292;&#22810;&#27169;&#24335;&#23398;&#20064;&#20986;&#29616;&#20102;&#26174;&#33879;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#25104;&#21151;&#36890;&#24120;&#20165;&#38480;&#20110;&#33521;&#35821;&#65292;&#20854;&#20182;&#35821;&#35328;&#21017;&#30456;&#23545;&#33853;&#21518;&#12290;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#26500;&#24314;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#23545;&#24212;&#29289;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#38750;&#33521;&#35821;&#22810;&#27169;&#24335;&#25968;&#25454;&#20855;&#26377;&#20302;&#36164;&#28304;&#29305;&#24615;&#65288;&#21363;&#32570;&#20047;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MPM&#65292;&#19968;&#31181;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#35757;&#32451;&#22823;&#22411;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#26377;&#25928;&#35757;&#32451;&#33539;&#20363;&#12290;MPM&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#36328;&#35821;&#31181;&#38646;&#26679;&#26412;&#22810;&#27169;&#24335;&#23398;&#20064;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#20165;&#22312;&#33521;&#35821;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24335;&#27169;&#22411;&#21487;&#20197;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#20854;&#20182;&#35821;&#35328;&#65292;&#29992;&#20110;&#22270;&#20687;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#65292;&#29978;&#33267;&#36229;&#36807;&#22312;&#26412;&#22320;&#35821;&#35328;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#20197;&#20013;&#25991;&#20316;&#20026;MPM&#23454;&#36341;&#30340;&#19968;&#20010;&#32451;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data). In this work, we propose MPM, an effective training paradigm for training large multimodal models in low-resource languages. MPM demonstrates that Multilingual language models can Pivot zero-shot Multimodal learning across languages. Specifically, based on a strong multilingual large language model, multimodal models pretrained on English-only image-text data can well generalize to other languages in a zero-shot manner for both image-to-text and text-to-image generation, even surpassing models trained on image-text data in native languages. Taking Chinese as a practice of MP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PREFER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#39304;&#26426;&#21046;&#21644;&#33258;&#21160;&#21512;&#25104;&#26032;&#30340;&#25552;&#31034;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12033</link><description>&lt;p&gt;
PREFER: &#36890;&#36807;&#21453;&#39304;-&#21453;&#24605;-&#20248;&#21270;&#30340;&#25552;&#31034;&#38598;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine. (arXiv:2308.12033v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PREFER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#39304;&#26426;&#21046;&#21644;&#33258;&#21160;&#21512;&#25104;&#26032;&#30340;&#25552;&#31034;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#24378;&#22823;&#33021;&#21147;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#25552;&#31034;&#26368;&#36817;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25552;&#31034;&#38598;&#25104;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20852;&#36259;&#65292;&#20197;&#35299;&#20915; LLMs &#30340;&#24187;&#35273;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#30340;&#33539;&#24335;&#65292;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#20934;&#22791;&#30340;&#25552;&#31034;&#38598;&#21512;&#65292;&#24182;&#19988;&#26080;&#27861;&#38024;&#23545;&#19981;&#21516;&#30340;&#24369;&#23398;&#20064;&#22120;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#20248;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#36890;&#29992;&#12289;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026; PREFER (&#36890;&#36807;&#21453;&#39304;-&#21453;&#24605;-&#20248;&#21270;&#30340;&#25552;&#31034;&#38598;&#25104;&#23398;&#20064;)&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32771;&#34385;&#21040;&#24369;&#23398;&#20064;&#22120;&#24212;&#35813;&#20851;&#27880;&#25552;&#21319;&#36807;&#31243;&#20013;&#30340;&#22256;&#38590;&#26679;&#26412;&#65292;PREFER &#26500;&#24314;&#20102;&#19968;&#20010;&#21453;&#39304;&#26426;&#21046;&#65292;&#29992;&#20110;&#21453;&#24605;&#29616;&#26377;&#24369;&#23398;&#20064;&#22120;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#22522;&#20110;&#27492;&#65292;LLM &#38656;&#35201;&#33258;&#21160;&#21512;&#25104;&#26032;&#30340;&#25552;&#31034;&#26469;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an effective tool for eliciting the power of Large Language Models (LLMs), prompting has recently demonstrated unprecedented abilities across a variety of complex tasks. To further improve the performance, prompt ensemble has attracted substantial interest for tackling the hallucination and instability of LLMs. However, existing methods usually adopt a two-stage paradigm, which requires a pre-prepared set of prompts with substantial manual effort, and is unable to perform directed optimization for different weak learners. In this paper, we propose a simple, universal, and automatic method named PREFER (Pompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations. Specifically, given the fact that weak learners are supposed to focus on hard examples during boosting, PREFER builds a feedback mechanism for reflecting on the inadequacies of existing weak learners. Based on this, the LLM is required to automatically synthesize new prompts for iterative refinemen
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#25105;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#35753;LLM&#33021;&#22815;&#33258;&#20027;&#22320;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#36981;&#24490;&#38590;&#24230;&#25351;&#26631;&#65288;IFD&#65289;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#22312;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#20248;&#20110;&#20256;&#32479;&#25968;&#25454;&#36755;&#20837;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12032</link><description>&lt;p&gt;
&#20174;&#25968;&#37327;&#21040;&#36136;&#37327;&#65306;&#21033;&#29992;&#33258;&#25105;&#24341;&#23548;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#25552;&#21319;LLM&#24615;&#33021;&#20197;&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning. (arXiv:2308.12032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12032
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#25105;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#35753;LLM&#33021;&#22815;&#33258;&#20027;&#22320;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#36981;&#24490;&#38590;&#24230;&#25351;&#26631;&#65288;IFD&#65289;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#22312;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#20248;&#20110;&#20256;&#32479;&#25968;&#25454;&#36755;&#20837;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#65292;&#25351;&#20196;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#24050;&#25104;&#20026;&#19968;&#20010;&#28966;&#28857;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#25105;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#35753;LLM&#33021;&#22815;&#33258;&#20027;&#22320;&#35782;&#21035;&#21644;&#36873;&#25321;&#22823;&#35268;&#27169;&#24320;&#28304;&#25968;&#25454;&#38598;&#20013;&#30340;&#31934;&#36873;&#26679;&#26412;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#25163;&#21160;&#31579;&#36873;&#21644;&#28508;&#22312;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#25351;&#20196;&#36981;&#24490;&#38590;&#24230;&#65288;IFD&#65289;&#25351;&#26631;&#65292;&#23427;&#25104;&#20026;&#20102;&#19968;&#20010;&#20915;&#23450;&#24615;&#24037;&#20855;&#65292;&#29992;&#20110;&#35782;&#21035;&#27169;&#22411;&#26399;&#26395;&#21709;&#24212;&#21644;&#33258;&#20027;&#29983;&#25104;&#33021;&#21147;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#28789;&#27963;&#24212;&#29992;IFD&#65292;&#25105;&#20204;&#33021;&#22815;&#25214;&#21040;&#31934;&#36873;&#26679;&#26412;&#65292;&#20174;&#32780;&#22823;&#24133;&#25552;&#21319;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#12290;&#22312;Alpaca&#21644;WizardLM&#31561;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#39564;&#35777;&#25903;&#25345;&#25105;&#20204;&#30340;&#21457;&#29616;&#65307;&#20165;&#20351;&#29992;&#20256;&#32479;&#25968;&#25454;&#36755;&#20837;&#30340;10%&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#33258;&#25105;&#24341;&#23548;&#25361;&#36873;&#21644;IFD&#25351;&#26631;&#30340;&#32508;&#21512;&#24847;&#21619;&#30528;LLM&#20248;&#21270;&#30340;&#19968;&#20010;&#21464;&#38761;&#24615;&#39134;&#36291;&#65292;&#26377;&#26395;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#38477;&#20302;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of Large Language Models, the balance between instruction data quality and quantity has become a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from vast open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal tool to identify discrepancies between a model's expected responses and its autonomous generation prowess. Through the adept application of IFD, cherry samples are pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on renowned datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of conventional data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the optimization of LLMs, promising both
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22870;&#21169;&#27169;&#22411;&#26469;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#24182;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.12030</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#19982;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompt-Based Length Controlled Generation with Reinforcement Learning. (arXiv:2308.12030v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12030
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22870;&#21169;&#27169;&#22411;&#26469;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#24182;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;GPT-4&#22240;&#20854;&#24778;&#20154;&#30340;&#25913;&#36827;&#21644;&#24615;&#33021;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#25104;&#20026;LLM&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#35805;&#39064;&#65292;&#23427;&#36824;&#20351;&#29992;&#25143;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#22312;&#26356;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#29983;&#25104;&#25152;&#38656;&#38271;&#24230;&#30340;&#21512;&#36866;&#31572;&#26696;&#25110;&#25991;&#31456;&#12290;&#27492;&#22806;&#65292;LLM&#20013;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#38750;&#24120;&#32791;&#26102;&#65292;&#32780;&#25511;&#21046;&#29983;&#25104;&#38271;&#24230;&#30340;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#38480;&#21046;&#38271;&#24230;&#20219;&#24847;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#65292;&#20174;&#32780;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#25511;&#21046;&#26041;&#27861;&#26469;&#23454;&#29616;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#20110;&#31867;&#20284;GPT&#30340;LLM&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#25110;&#22522;&#20110;&#35268;&#21017;&#30340;&#22870;&#21169;&#27169;&#22411;&#25552;&#20379;&#22870;&#21169;&#20449;&#21495;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#23545;&#39044;&#23450;&#20041;&#30446;&#26631;&#38271;&#24230;&#36827;&#34892;&#22870;&#21169;&#26469;&#24433;&#21709;LLM&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) like ChatGPT and GPT-4 have attracted great attention given their surprising improvement and performance. Length controlled generation of LLMs emerges as an important topic, which also enables users to fully leverage the capability of LLMs in more real-world scenarios like generating a proper answer or essay of a desired length. In addition, the autoregressive generation in LLMs is extremely time-consuming, while the ability of controlling this generated length can arbitrarily reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show th
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#27880;&#20837;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#22788;&#29702;&#20013;&#25991;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35268;&#33539;&#21270;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26377;&#25928;&#32534;&#30721;&#21307;&#23398;&#23454;&#20307;&#20013;&#30340;&#30693;&#35782;&#39033;&#65292;&#24182;&#23558;&#23427;&#20204;&#27880;&#20837;&#21040;&#27169;&#22411;&#20013;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12025</link><description>&lt;p&gt;
&#20013;&#25991;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35268;&#33539;&#21270;&#30340;&#30693;&#35782;&#27880;&#20837;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Knowledge-injected Prompt Learning for Chinese Biomedical Entity Normalization. (arXiv:2308.12025v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12025
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#27880;&#20837;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#22788;&#29702;&#20013;&#25991;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35268;&#33539;&#21270;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26377;&#25928;&#32534;&#30721;&#21307;&#23398;&#23454;&#20307;&#20013;&#30340;&#30693;&#35782;&#39033;&#65292;&#24182;&#23558;&#23427;&#20204;&#27880;&#20837;&#21040;&#27169;&#22411;&#20013;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35268;&#33539;&#21270;&#65288;BEN&#65289;&#20219;&#21153;&#26088;&#22312;&#23558;&#21407;&#22987;&#30340;&#38750;&#32467;&#26500;&#21270;&#21307;&#23398;&#23454;&#20307;&#23545;&#40784;&#21040;&#26631;&#20934;&#23454;&#20307;&#65292;&#20174;&#32780;&#20419;&#36827;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#24182;&#20415;&#20110;&#26356;&#22909;&#30340;&#19979;&#28216;&#21307;&#23398;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20013;&#25991;BEN&#20219;&#21153;&#19978;&#23384;&#22312;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#38480;&#30340;&#21307;&#23398;&#25968;&#25454;&#19979;&#30340;&#23569;&#26679;&#26412;&#24773;&#20917;&#65292;&#22806;&#37096;&#21307;&#23398;&#30693;&#35782;&#24211;&#30340;&#24040;&#22823;&#28508;&#21147;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#21033;&#29992;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#27880;&#20837;&#25552;&#31034;&#23398;&#20064;&#65288;PL-Knowledge&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20116;&#20010;&#38454;&#27573;&#65306;&#20505;&#36873;&#23454;&#20307;&#21305;&#37197;&#12289;&#30693;&#35782;&#25552;&#21462;&#12289;&#30693;&#35782;&#32534;&#30721;&#12289;&#30693;&#35782;&#27880;&#20837;&#21644;&#39044;&#27979;&#36755;&#20986;&#12290;&#36890;&#36807;&#26377;&#25928;&#22320;&#32534;&#30721;&#21307;&#23398;&#23454;&#20307;&#20013;&#21253;&#21547;&#30340;&#30693;&#35782;&#39033;&#65292;&#24182;&#23558;&#23427;&#20204;&#25972;&#21512;&#21040;&#25105;&#20204;&#30340;&#23450;&#21046;&#30693;&#35782;&#27880;&#20837;&#27169;&#26495;&#20013;&#65292;&#39069;&#22806;&#30340;&#30693;&#35782;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Biomedical Entity Normalization (BEN) task aims to align raw, unstructured medical entities to standard entities, thus promoting data coherence and facilitating better downstream medical applications. Recently, prompt learning methods have shown promising results in this task. However, existing research falls short in tackling the more complex Chinese BEN task, especially in the few-shot scenario with limited medical data, and the vast potential of the external medical knowledge base has yet to be fully harnessed. To address these challenges, we propose a novel Knowledge-injected Prompt Learning (PL-Knowledge) method. Specifically, our approach consists of five stages: candidate entity matching, knowledge extraction, knowledge encoding, knowledge injection, and prediction output. By effectively encoding the knowledge items contained in medical entities and incorporating them into our tailor-made knowledge-injected templates, the additional knowledge enhances the model's ability to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21015;&#34920;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#31895;&#21040;&#32454;&#31070;&#32463;&#26816;&#32034;&#22120;&#26469;&#37325;&#26032;&#25490;&#24207;&#27573;&#33853;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20854;&#20182;&#20505;&#36873;&#21477;&#23376;&#30340;&#21015;&#34920;&#19978;&#19979;&#25991;&#20449;&#24687;&#32435;&#20837;&#27573;&#33853;&#34920;&#31034;&#20013;&#65292;&#22686;&#24378;&#20102;&#27573;&#33853;&#34920;&#31034;&#12290;&#32780;&#19988;&#65292;&#35813;&#26041;&#27861;&#23558;&#21015;&#34920;&#19978;&#19979;&#25991;&#24314;&#27169;&#36807;&#31243;&#20998;&#20026;&#20004;&#20010;&#23376;&#36807;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27573;&#33853;&#27880;&#24847;&#26426;&#21046;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#20801;&#35768;&#39640;&#25928;&#32534;&#30721;&#22823;&#37327;&#20505;&#36873;&#31572;&#26696;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.12022</link><description>&lt;p&gt;
&#21033;&#29992;&#21015;&#34920;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#31895;&#21040;&#32454;&#31070;&#32463;&#26816;&#32034;&#22120;&#23545;&#27573;&#33853;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Reranking Passages with Coarse-to-Fine Neural Retriever using List-Context Information. (arXiv:2308.12022v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21015;&#34920;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#31895;&#21040;&#32454;&#31070;&#32463;&#26816;&#32034;&#22120;&#26469;&#37325;&#26032;&#25490;&#24207;&#27573;&#33853;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20854;&#20182;&#20505;&#36873;&#21477;&#23376;&#30340;&#21015;&#34920;&#19978;&#19979;&#25991;&#20449;&#24687;&#32435;&#20837;&#27573;&#33853;&#34920;&#31034;&#20013;&#65292;&#22686;&#24378;&#20102;&#27573;&#33853;&#34920;&#31034;&#12290;&#32780;&#19988;&#65292;&#35813;&#26041;&#27861;&#23558;&#21015;&#34920;&#19978;&#19979;&#25991;&#24314;&#27169;&#36807;&#31243;&#20998;&#20026;&#20004;&#20010;&#23376;&#36807;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27573;&#33853;&#27880;&#24847;&#26426;&#21046;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#20801;&#35768;&#39640;&#25928;&#32534;&#30721;&#22823;&#37327;&#20505;&#36873;&#31572;&#26696;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#26159;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25991;&#26723;&#26102;&#12290;&#20256;&#32479;&#30340;&#31070;&#32463;&#26550;&#26500;&#22312;&#20026;&#38382;&#39064;&#26816;&#32034;&#26368;&#20339;&#27573;&#33853;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#23558;&#38382;&#39064;&#19982;&#27599;&#20010;&#27573;&#33853;&#20998;&#24320;&#21305;&#37197;&#65292;&#24456;&#23569;&#32771;&#34385;&#20854;&#20182;&#27573;&#33853;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#32780;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#25552;&#20379;&#27604;&#36739;&#21644;&#21442;&#32771;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21015;&#34920;&#19978;&#19979;&#25991;&#27880;&#24847;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;&#20854;&#20182;&#20505;&#36873;&#21477;&#23376;&#30340;&#21015;&#34920;&#19978;&#19979;&#25991;&#20449;&#24687;&#32435;&#20837;&#27573;&#33853;&#34920;&#31034;&#20013;&#26469;&#22686;&#24378;&#27573;&#33853;&#34920;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#31895;&#21040;&#32454;&#65288;C2F&#65289;&#31070;&#32463;&#26816;&#32034;&#22120;&#36890;&#36807;&#23558;&#21015;&#34920;&#19978;&#19979;&#25991;&#24314;&#27169;&#36807;&#31243;&#20998;&#20026;&#20004;&#20010;&#23376;&#36807;&#31243;&#26469;&#35299;&#20915;&#27573;&#33853;&#27880;&#24847;&#26426;&#21046;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#20174;&#32780;&#20801;&#35768;&#23545;&#22823;&#37327;&#20505;&#36873;&#31572;&#26696;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#36827;&#34892;&#39640;&#25928;&#32534;&#30721;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#29992;&#20110;&#19968;&#27425;&#24615;&#32534;&#30721;&#20219;&#24847;&#25968;&#37327;&#30340;&#20505;&#36873;&#31572;&#26696;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Passage reranking is a crucial task in many applications, particularly when dealing with large-scale documents. Traditional neural architectures are limited in retrieving the best passage for a question because they usually match the question to each passage separately, seldom considering contextual information in other passages that can provide comparison and reference information. This paper presents a list-context attention mechanism to augment the passage representation by incorporating the list-context information from other candidates. The proposed coarse-to-fine (C2F) neural retriever addresses the out-of-memory limitation of the passage attention mechanism by dividing the list-context modeling process into two sub-processes, allowing for efficient encoding of context information from a large number of candidate answers. This method can be generally used to encode context information from any number of candidate answers in one pass. Different from most multi-stage information re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#27169;&#22411;&#23545;&#40784;&#30446;&#26631;&#30340;&#19981;&#21516;&#35266;&#28857;&#65292;&#24182;&#36861;&#36394;&#20854;&#28436;&#21270;&#36335;&#24452;&#65292;&#26088;&#22312;&#24110;&#21161;&#30830;&#23450;&#26368;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.12014</link><description>&lt;p&gt;
&#20174;&#25351;&#20196;&#21040;&#20869;&#22312;&#20154;&#31867;&#20215;&#20540; - &#22823;&#27169;&#22411;&#23545;&#40784;&#30446;&#26631;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models. (arXiv:2308.12014v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#27169;&#22411;&#23545;&#40784;&#30446;&#26631;&#30340;&#19981;&#21516;&#35266;&#28857;&#65292;&#24182;&#36861;&#36394;&#20854;&#28436;&#21270;&#36335;&#24452;&#65292;&#26088;&#22312;&#24110;&#21161;&#30830;&#23450;&#26368;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27169;&#22411;&#65292;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#36890;&#24120;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#30001;&#22823;&#37327;&#21442;&#25968;&#32452;&#25104;&#65292;&#19981;&#20165;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#33719;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#36824;&#21576;&#29616;&#20986;&#36739;&#23567;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#26032;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#27169;&#22411;&#19982;&#26085;&#24120;&#29983;&#27963;&#30340;&#26085;&#30410;&#20132;&#32455;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#21487;&#33021;&#36896;&#25104;&#20005;&#37325;&#30340;&#31038;&#20250;&#21361;&#23475;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#21162;&#21147;&#24050;&#32463;&#36827;&#34892;&#20102;&#65292;&#20197;&#20351;LLM&#19982;&#20154;&#31867;&#23545;&#40784;&#65292;&#20197;&#20351;&#23427;&#20204;&#26356;&#22909;&#22320;&#36981;&#24490;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#28385;&#36275;&#20154;&#31867;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#8220;&#19982;&#20309;&#23545;&#40784;&#8221;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#35752;&#35770;&#65292;&#19981;&#24403;&#30340;&#23545;&#40784;&#30446;&#26631;&#29978;&#33267;&#21487;&#33021;&#36866;&#24471;&#20854;&#21453;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#24037;&#20316;&#20013;&#30340;&#19981;&#21516;&#23545;&#40784;&#30446;&#26631;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#65292;&#24182;&#36861;&#36394;&#23427;&#20204;&#30340;&#28436;&#21270;&#36335;&#24452;&#65292;&#20197;&#24110;&#21161;&#30830;&#23450;&#26368;&#22522;&#26412;&#30340;&#30446;&#26631;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20174;&#23545;&#40784;&#30446;&#26631;&#30340;&#23450;&#20041;&#21644;&#23545;&#40784;&#35780;&#20272;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#30456;&#20851;&#24037;&#20316;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Big models, exemplified by Large Language Models (LLMs), are models typically pre-trained on massive data and comprised of enormous parameters, which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. However, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. Therefore, many efforts have been made to align LLMs with humans to make them better follow user instructions and satisfy human preferences. Nevertheless, `what to align with' has not been fully discussed, and inappropriate alignment goals might even backfire. In this paper, we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. Particularly, we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation. Our analysis encompasses
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#21476;&#20856;&#25991;&#23398;&#30340;&#19977;&#35821;&#21477;&#23376;RoBERTa&#27169;&#22411;SPhilBERTa&#65292;&#33021;&#22815;&#20248;&#31168;&#22320;&#36827;&#34892;&#36328;&#35821;&#35328;&#35821;&#20041;&#29702;&#35299;&#21644;&#35782;&#21035;&#21476;&#24076;&#33098;&#35821;&#12289;&#25289;&#19969;&#35821;&#21644;&#33521;&#35821;&#20043;&#38388;&#30456;&#21516;&#21477;&#23376;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#32763;&#35793;&#29983;&#25104;&#20102;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;SPhilBERTa&#22312;&#33258;&#21160;&#26816;&#27979;&#31867;&#38469;&#23545;&#24212;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12008</link><description>&lt;p&gt;
&#34987;&#24449;&#26381;&#30340;&#26684;&#37324;&#35199;&#20122;&#25112;&#32988;&#20102;&#37326;&#34542;&#30340;&#32988;&#21033;&#32773;&#12290;&#26816;&#27979;&#25289;&#19969;&#21476;&#24076;&#33098;&#25991;&#23398;&#30340;&#31867;&#38469;&#25351;&#28041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graecia capta ferum victorem cepit. Detecting Latin Allusions to Ancient Greek Literature. (arXiv:2308.12008v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12008
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#21476;&#20856;&#25991;&#23398;&#30340;&#19977;&#35821;&#21477;&#23376;RoBERTa&#27169;&#22411;SPhilBERTa&#65292;&#33021;&#22815;&#20248;&#31168;&#22320;&#36827;&#34892;&#36328;&#35821;&#35328;&#35821;&#20041;&#29702;&#35299;&#21644;&#35782;&#21035;&#21476;&#24076;&#33098;&#35821;&#12289;&#25289;&#19969;&#35821;&#21644;&#33521;&#35821;&#20043;&#38388;&#30456;&#21516;&#21477;&#23376;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#32763;&#35793;&#29983;&#25104;&#20102;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;SPhilBERTa&#22312;&#33258;&#21160;&#26816;&#27979;&#31867;&#38469;&#23545;&#24212;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#38469;&#25351;&#28041;&#22312;&#21476;&#20856;&#25991;&#23398;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#25289;&#19969;&#20316;&#23478;&#32463;&#24120;&#24341;&#29992;&#21476;&#24076;&#33098;&#25991;&#26412;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#33258;&#21160;&#35782;&#21035;&#36825;&#20123;&#31867;&#38469;&#24341;&#29992;&#30340;&#26041;&#27861;&#23616;&#38480;&#20110;&#21333;&#35821;&#35328;&#26041;&#27861;&#65292;&#20165;&#22312;&#25289;&#19969;&#25991;&#25110;&#24076;&#33098;&#25991;&#26412;&#20013;&#23547;&#25214;&#31867;&#20284;&#20043;&#22788;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SPhilBERTa&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21476;&#20856;&#25991;&#23398;&#30340;&#19977;&#35821;&#21477;&#23376;RoBERTa&#27169;&#22411;&#65292;&#20248;&#20110;&#36328;&#35821;&#35328;&#35821;&#20041;&#29702;&#35299;&#21644;&#35782;&#21035;&#22312;&#21476;&#24076;&#33098;&#35821;&#12289;&#25289;&#19969;&#35821;&#21644;&#33521;&#35821;&#20043;&#38388;&#30456;&#21516;&#21477;&#23376;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#23558;&#33521;&#35821;&#25991;&#26412;&#32763;&#35793;&#25104;&#21476;&#24076;&#33098;&#35821;&#26469;&#29983;&#25104;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;SPhilBERTa&#22312;&#33258;&#21160;&#26816;&#27979;&#31867;&#38469;&#23545;&#24212;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#36164;&#28304;&#21487;&#22312;https://github.com/Heidelberg-NLP/ancient-language-models&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intertextual allusions hold a pivotal role in Classical Philology, with Latin authors frequently referencing Ancient Greek texts. Until now, the automatic identification of these intertextual references has been constrained to monolingual approaches, seeking parallels solely within Latin or Greek texts. In this study, we introduce SPhilBERTa, a trilingual Sentence-RoBERTa model tailored for Classical Philology, which excels at cross-lingual semantic comprehension and identification of identical sentences across Ancient Greek, Latin, and English. We generate new training data by automatically translating English texts into Ancient Greek. Further, we present a case study, demonstrating SPhilBERTa's capability to facilitate automated detection of intertextual parallels. Our models and resources are available at https://github.com/Heidelberg-NLP/ancient-language-models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;Topical-Chat&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#30340;&#20154;-&#20154;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25512;&#21160;&#24320;&#25918;&#22495;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#12290;&#30740;&#31350;&#32773;&#35757;&#32451;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.11995</link><description>&lt;p&gt;
Topical-Chat: &#36827;&#23637;&#20013;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations. (arXiv:2308.11995v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11995
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;Topical-Chat&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#30340;&#20154;-&#20154;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25512;&#21160;&#24320;&#25918;&#22495;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#12290;&#30740;&#31350;&#32773;&#35757;&#32451;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#31038;&#20132;&#26426;&#22120;&#20154;&#33021;&#22815;&#19982;&#20154;&#31867;&#36827;&#34892;&#28145;&#20837;&#12289;&#26377;&#36259;&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#39033;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26426;&#22120;&#20154;&#22312;&#19982;&#25317;&#26377;&#33258;&#24049;&#19990;&#30028;&#30693;&#35782;&#30340;&#20154;&#31867;&#36827;&#34892;&#23545;&#35805;&#26102;&#65292;&#38656;&#35201;&#26377;&#25928;&#22320;&#21033;&#29992;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#20027;&#35201;&#26159;&#20855;&#26377;&#26126;&#30830;&#35282;&#33394;&#30340;&#26679;&#24335;&#21270;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#36824;&#27809;&#26377;&#25506;&#32034;&#23545;&#35805;&#20013;&#30340;&#20027;&#39064;&#28085;&#30422;&#30340;&#28145;&#24230;&#21644;&#24191;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Topical-Chat&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#30340;&#20154;-&#20154;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#30340;&#30693;&#35782;&#28085;&#30422;&#20102;8&#20010;&#24191;&#27867;&#30340;&#20027;&#39064;&#65292;&#24182;&#19988;&#23545;&#35805;&#21442;&#19982;&#32773;&#27809;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#35282;&#33394;&#65292;&#20197;&#36827;&#19968;&#27493;&#20419;&#36827;&#24320;&#25918;&#22495;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#22312;Topical-Chat&#19978;&#35757;&#32451;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#35805;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#20197;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building socialbots that can have deep, engaging open-domain conversations with humans is one of the grand challenges of artificial intelligence (AI). To this end, bots need to be able to leverage world knowledge spanning several domains effectively when conversing with humans who have their own world knowledge. Existing knowledge-grounded conversation datasets are primarily stylized with explicit roles for conversation partners. These datasets also do not explore depth or breadth of topical coverage with transitions in conversations. We introduce Topical-Chat, a knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don't have explicitly defined roles, to help further research in open-domain conversational AI. We also train several state-of-the-art encoder-decoder conversational models on Topical-Chat and perform automated and human evaluation for benchmarking.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EVE&#30340;&#39640;&#25928;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#36974;&#34109;&#20449;&#21495;&#24314;&#27169;&#21644;&#27169;&#24577;&#24863;&#30693;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;Transformer&#32593;&#32476;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36827;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11971</link><description>&lt;p&gt;
EVE: &#20351;&#29992;&#36974;&#34109;&#39044;&#27979;&#21644;&#27169;&#24577;&#24863;&#30693;&#30340;&#39640;&#25928;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE. (arXiv:2308.11971v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EVE&#30340;&#39640;&#25928;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#36974;&#34109;&#20449;&#21495;&#24314;&#27169;&#21644;&#27169;&#24577;&#24863;&#30693;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;Transformer&#32593;&#32476;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36827;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EVE&#30340;&#39640;&#25928;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#23427;&#26159;&#30001;&#19968;&#31181;&#32479;&#19968;&#30340;Transformer&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;EVE&#36890;&#36807;&#22312;&#22270;&#20687;-&#25991;&#26412;&#23545;&#19978;&#36827;&#34892;&#36974;&#34109;&#20449;&#21495;&#24314;&#27169;&#26469;&#32479;&#19968;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#37325;&#24314;&#21487;&#35265;&#20449;&#21495;&#65292;&#21363;&#22270;&#20687;&#20687;&#32032;&#21644;&#25991;&#26412;&#26631;&#35760;&#12290;&#36890;&#36807;&#38598;&#25104;&#27169;&#24577;&#24863;&#30693;&#30340;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#27169;&#22359;&#65292;EVE&#22312;&#19968;&#20010;&#20849;&#20139;&#30340;Transformer&#32593;&#32476;&#20013;&#32534;&#30721;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20999;&#25442;&#21040;&#19981;&#21516;&#30340;&#19987;&#23478;&#26469;&#25429;&#25417;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building scalable vision-language models to learn from diverse, multimodal data remains an open challenge. In this paper, we introduce an Efficient Vision-languagE foundation model, namely EVE, which is one unified multimodal Transformer pre-trained solely by one unified pre-training task. Specifically, EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts. To unify pre-training tasks of vision and language, EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 3.5x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing to the combination of the unified architecture and pre-training task, EVE is easy t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#39118;&#26684;&#31561;&#39069;&#22806;&#26465;&#20214;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20316;&#32773;&#25972;&#21512;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.11940</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Audio Generation with Multiple Conditional Diffusion Model. (arXiv:2308.11940v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#39118;&#26684;&#31561;&#39069;&#22806;&#26465;&#20214;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20316;&#32773;&#25972;&#21512;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#38899;&#39057;&#29983;&#25104;&#27169;&#22411;&#26377;&#20854;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#21253;&#21547;&#38899;&#39057;&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#20165;&#20381;&#38752;&#25991;&#26412;&#20250;&#23548;&#33268;&#21463;&#25511;&#24615;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#26465;&#20214;&#65288;&#21253;&#25324;&#20869;&#23481;&#65288;&#26102;&#38388;&#25139;&#65289;&#21644;&#39118;&#26684;&#65288;&#38899;&#39640;&#26354;&#32447;&#21644;&#33021;&#37327;&#26354;&#32447;&#65289;&#65289;&#20316;&#20026;&#25991;&#26412;&#30340;&#34917;&#20805;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#30340;&#31934;&#32454;&#25511;&#21046;&#12290;&#20026;&#20102;&#20445;&#25345;&#29983;&#25104;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#25511;&#21046;&#26465;&#20214;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#30001;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#34701;&#21512;&#32593;&#32476;&#26469;&#32534;&#30721;&#21644;&#34701;&#21512;&#39069;&#22806;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#25968;&#25454;&#38598;&#25972;&#21512;&#20026;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#38899;&#39057;&#21644;&#30456;&#24212;&#30340;&#26465;&#20214;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#25511;&#21046;&#26465;&#20214;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#30001;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#34701;&#21512;&#32593;&#32476;&#26469;&#32534;&#30721;&#21644;&#34701;&#21512;&#39069;&#22806;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38899;&#39057;&#24046;&#24322;&#23383;&#24149;&#29983;&#25104;&#65288;ADC&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#25193;&#23637;&#20219;&#21153;&#65292;&#29992;&#20110;&#25551;&#36848;&#31867;&#20284;&#20294;&#30053;&#26377;&#24046;&#24322;&#30340;&#38899;&#39057;&#29255;&#27573;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#12290;&#36890;&#36807;&#24341;&#20837;&#20132;&#21449;&#27880;&#24847;&#21147;&#38598;&#20013;&#30340;Transformer&#32534;&#30721;&#22120;&#21644;&#30456;&#20284;&#24615;-&#24046;&#24322;&#35299;&#32544;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#20013;&#30340;&#24046;&#24322;&#25551;&#36848;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#21487;&#35270;&#21270;&#26469;&#25913;&#21892;&#27880;&#24847;&#21147;&#26435;&#37325;&#20197;&#25552;&#21462;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.11923</link><description>&lt;p&gt;
&#21033;&#29992;&#30456;&#20284;&#24615;-&#24046;&#24322;&#35299;&#32544;&#26469;&#36827;&#34892;&#38899;&#39057;&#24046;&#24322;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement. (arXiv:2308.11923v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11923
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38899;&#39057;&#24046;&#24322;&#23383;&#24149;&#29983;&#25104;&#65288;ADC&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#25193;&#23637;&#20219;&#21153;&#65292;&#29992;&#20110;&#25551;&#36848;&#31867;&#20284;&#20294;&#30053;&#26377;&#24046;&#24322;&#30340;&#38899;&#39057;&#29255;&#27573;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#12290;&#36890;&#36807;&#24341;&#20837;&#20132;&#21449;&#27880;&#24847;&#21147;&#38598;&#20013;&#30340;Transformer&#32534;&#30721;&#22120;&#21644;&#30456;&#20284;&#24615;-&#24046;&#24322;&#35299;&#32544;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#20013;&#30340;&#24046;&#24322;&#25551;&#36848;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#21487;&#35270;&#21270;&#26469;&#25913;&#21892;&#27880;&#24847;&#21147;&#26435;&#37325;&#20197;&#25552;&#21462;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#38899;&#39057;&#24046;&#24322;&#23383;&#24149;&#29983;&#25104;&#65288;ADC&#65289;&#20316;&#20026;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#30340;&#26032;&#25193;&#23637;&#20219;&#21153;&#65292;&#29992;&#20110;&#25551;&#36848;&#31867;&#20284;&#20294;&#30053;&#26377;&#24046;&#24322;&#30340;&#38899;&#39057;&#29255;&#27573;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#12290;ADC&#35299;&#20915;&#20102;&#20256;&#32479;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#20013;&#65292;&#23545;&#20110;&#30456;&#20284;&#38899;&#39057;&#29255;&#27573;&#29983;&#25104;&#31867;&#20284;&#23383;&#24149;&#30340;&#38382;&#39064;&#65292;&#26080;&#27861;&#25551;&#36848;&#20869;&#23481;&#24046;&#24322;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#21449;&#27880;&#24847;&#21147;&#38598;&#20013;&#30340;Transformer&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#27604;&#36739;&#19968;&#23545;&#38899;&#39057;&#29255;&#27573;&#21644;&#19968;&#31181;&#30456;&#20284;&#24615;-&#24046;&#24322;&#35299;&#32544;&#26469;&#24378;&#35843;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;AudioDiffCaps&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#31867;&#20284;&#20294;&#30053;&#26377;&#24046;&#24322;&#30340;&#38899;&#39057;&#29255;&#27573;&#23545;&#20197;&#21450;&#20154;&#24037;&#26631;&#27880;&#30340;&#23427;&#20204;&#20043;&#38388;&#24046;&#24322;&#30340;&#25551;&#36848;&#12290;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;ADC&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#22312;Transformer&#32534;&#30721;&#22120;&#20013;&#23545;&#20854;&#36827;&#34892;&#21487;&#35270;&#21270;&#26469;&#25913;&#21892;&#27880;&#24847;&#21147;&#26435;&#37325;&#20197;&#25552;&#21462;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We proposed Audio Difference Captioning (ADC) as a new extension task of audio captioning for describing the semantic differences between input pairs of similar but slightly different audio clips. The ADC solves the problem that conventional audio captioning sometimes generates similar captions for similar audio clips, failing to describe the difference in content. We also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space. To evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences. The experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#34920;&#26684;&#24207;&#21015;&#21270;&#27169;&#22359;&#21644;&#32416;&#27491;&#26426;&#21046;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#20173;&#26377;&#24046;&#36317;&#65292;&#20294;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.11891</link><description>&lt;p&gt;
&#32447;&#24615;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#20998;&#26512;&#34920;&#26684;&#25968;&#25454;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap: Deciphering Tabular Data Using Large Language Model. (arXiv:2308.11891v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#34920;&#26684;&#24207;&#21015;&#21270;&#27169;&#22359;&#21644;&#32416;&#27491;&#26426;&#21046;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#20173;&#26377;&#24046;&#36317;&#65292;&#20294;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#19968;&#30452;&#26159;&#23398;&#26415;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#38543;&#30528;&#35832;&#22914;ChatGPT&#20043;&#31867;&#30340;&#24222;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25506;&#32034;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#22788;&#29702;&#19982;&#34920;&#26684;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#34920;&#26684;&#32467;&#26500;&#21644;&#20869;&#23481;&#19978;&#30340;&#33021;&#21147;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#23558;&#34920;&#26684;&#24207;&#21015;&#21270;&#30340;&#27169;&#22359;&#65292;&#24182;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#32416;&#27491;&#26426;&#21046;&#26469;&#20462;&#27491;&#28508;&#22312;&#30340;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#20173;&#26377;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of natural language processing, the understanding of tabular data has perpetually stood as a focal point of scholarly inquiry. The emergence of expansive language models, exemplified by the likes of ChatGPT, has ushered in a wave of endeavors wherein researchers aim to harness these models for tasks related to table-based question answering. Central to our investigative pursuits is the elucidation of methodologies that amplify the aptitude of such large language models in discerning both the structural intricacies and inherent content of tables, ultimately facilitating their capacity to provide informed responses to pertinent queries. To this end, we have architected a distinctive module dedicated to the serialization of tables for seamless integration with expansive language models. Additionally, we've instituted a corrective mechanism within the model to rectify potential inaccuracies. Experimental results indicate that, although our proposed method trails the SOTA by ap
&lt;/p&gt;</description></item><item><title>Cabrita&#26159;&#19968;&#31181;&#35299;&#20915;&#24615;&#33021;&#21644;&#39640;&#25928;&#26631;&#35760;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#21487;&#25215;&#21463;&#30340;&#25104;&#26412;&#35299;&#20915;&#20102;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.11878</link><description>&lt;p&gt;
Cabrita: &#24357;&#21512;&#22806;&#35821;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Cabrita: closing the gap for foreign languages. (arXiv:2308.11878v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11878
&lt;/p&gt;
&lt;p&gt;
Cabrita&#26159;&#19968;&#31181;&#35299;&#20915;&#24615;&#33021;&#21644;&#39640;&#25928;&#26631;&#35760;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#21487;&#25215;&#21463;&#30340;&#25104;&#26412;&#35299;&#20915;&#20102;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#22312;&#29305;&#23450;&#35821;&#35328;&#25110;&#39046;&#22495;&#20013;&#26377;&#20004;&#20010;&#37325;&#35201;&#30446;&#30340;&#65306;i)&#22686;&#24378;&#22312;&#29305;&#23450;&#35821;&#35328;&#25110;&#39046;&#22495;&#32972;&#26223;&#19979;&#30340;&#24615;&#33021;&#65292;ii)&#30830;&#20445;&#26377;&#25928;&#30340;&#26631;&#35760;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#30456;&#20851;&#25104;&#26412;&#65292;&#36825;&#20123;&#25104;&#26412;&#21487;&#33021;&#36798;&#21040;&#20845;&#20301;&#25968;&#29978;&#33267;&#19971;&#20301;&#25968;&#30340;&#32654;&#20803;&#37329;&#39069;&#65292;&#36825;&#21462;&#20915;&#20110;&#27169;&#22411;&#22823;&#23567;&#21644;&#28041;&#21450;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25104;&#26412;&#25361;&#25112;&#65292;&#20027;&#35201;&#35299;&#20915;&#26041;&#26696;&#26159;&#20381;&#36182;&#21487;&#29992;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23613;&#31649;&#26368;&#36817;&#20986;&#29616;&#20102;&#20687;LLaMA&#21644;LLaMA-2&#27169;&#22411;&#36825;&#26679;&#30340;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#26576;&#20123;&#29305;&#23450;&#39046;&#22495;&#38382;&#39064;&#20173;&#28982;&#34920;&#29616;&#20302;&#25928;&#65292;&#25110;&#32773;&#22312;&#28041;&#21450;&#23545;&#35805;&#24335;&#35760;&#24518;&#36164;&#28304;&#30340;&#22330;&#26223;&#20013;&#26080;&#25928;&#65292;&#22240;&#20026;&#34920;&#31034;&#25991;&#26412;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#24040;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cabrita&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#65292;&#23427;&#25104;&#21151;&#35299;&#20915;&#20102;&#24615;&#33021;&#21644;&#39640;&#25928;&#26631;&#35760;&#21270;&#30340;&#38382;&#39064;&#65292;&#32780;&#19988;&#25104;&#26412;&#21487;&#25215;&#21463;&#12290;
&lt;/p&gt;
&lt;p&gt;
The strategy of training the model from scratch in a specific language or domain serves two essential purposes: i) enhancing performance in the particular linguistic or domain context, and ii) ensuring effective tokenization. The main limitation inherent to this approach lies in the associated cost, which can reach six to seven-digit dollar values, depending on the model size and the number of parameters involved.  The main solution to overcome the cost challenge is to rely on available pre-trained models, which, despite recent advancements such as the LLaMA and LLaMA-2 models, still demonstrate inefficiency for certain specific domain problems or prove ineffective in scenarios involving conversational memory resources, given the large number of tokens required to represent text.  To overcome this issue, we present a methodology named Cabrita, which, as our research demonstrates, successfully addresses the performance and efficient tokenization problem, all at an affordable cost. We be
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#20449;&#24687;&#28304;&#30340;&#19978;&#19979;&#25991;&#65292;&#35753;GPT&#27169;&#22411;&#33021;&#22815;&#22238;&#31572;&#32771;&#35797;&#39064;&#30446;&#12290;&#22312;&#20351;&#29992;&#21152;&#21033;&#31119;&#23612;&#20122;&#39550;&#39542;&#25163;&#20876;&#20316;&#20026;&#20449;&#24687;&#28304;&#30340;&#27979;&#35797;&#20013;&#65292;GPT-3&#27169;&#22411;&#21462;&#24471;&#20102;96%&#30340;&#21450;&#26684;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.11827</link><description>&lt;p&gt;
&#25506;&#32034;GPT&#27169;&#22411;&#22312;&#32771;&#35797;&#20013;&#30340;&#25928;&#26524;&#65306;&#39550;&#39542;&#25191;&#29031;&#30693;&#35782;&#27979;&#35797;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver's License Knowledge Test. (arXiv:2308.11827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#20449;&#24687;&#28304;&#30340;&#19978;&#19979;&#25991;&#65292;&#35753;GPT&#27169;&#22411;&#33021;&#22815;&#22238;&#31572;&#32771;&#35797;&#39064;&#30446;&#12290;&#22312;&#20351;&#29992;&#21152;&#21033;&#31119;&#23612;&#20122;&#39550;&#39542;&#25163;&#20876;&#20316;&#20026;&#20449;&#24687;&#28304;&#30340;&#27979;&#35797;&#20013;&#65292;GPT-3&#27169;&#22411;&#21462;&#24471;&#20102;96%&#30340;&#21450;&#26684;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;Open AI&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#65292;&#25797;&#38271;&#22238;&#31572;&#38382;&#39064;&#65292;&#20294;&#20854;&#30693;&#35782;&#20165;&#38480;&#20110;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#38480;&#21046;&#20351;&#24471;&#24403;&#38754;&#20020;&#26377;&#20851;&#26368;&#26032;&#21457;&#23637;&#25110;&#38750;&#20844;&#24320;&#25991;&#20214;&#30340;&#38382;&#39064;&#26102;&#65292;&#23427;&#20204;&#21464;&#24471;&#26080;&#25928;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20043;&#21069;&#26410;&#21253;&#21547;&#22312;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#28304;&#30340;&#19978;&#19979;&#25991;&#26469;&#20351;GPT&#27169;&#22411;&#33021;&#22815;&#22238;&#31572;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#39044;&#22788;&#29702;&#12289;&#19978;&#19979;&#25991;&#21644;&#26597;&#35810;&#30340;&#23884;&#20837;&#12289;&#36890;&#36807;&#25972;&#21512;&#19978;&#19979;&#25991;&#23884;&#20837;&#26500;&#24314;&#25552;&#31034;&#20197;&#21450;&#20351;&#29992;GPT&#27169;&#22411;&#29983;&#25104;&#31572;&#26696;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#21463;&#25511;&#27979;&#35797;&#22330;&#26223;&#65292;&#20351;&#29992;&#21152;&#21033;&#31119;&#23612;&#20122;&#39550;&#39542;&#25163;&#20876;&#20316;&#20026;&#20449;&#24687;&#28304;&#12290;GPT-3&#27169;&#22411;&#22312;&#19968;&#22871;50&#36947;&#26679;&#26412;&#39550;&#39542;&#30693;&#35782;&#27979;&#35797;&#39064;&#19978;&#21462;&#24471;&#20102;96%&#30340;&#21450;&#26684;&#20998;&#25968;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#27809;&#26377;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#30340;&#21450;&#26684;&#20998;&#25968;&#19979;&#38477;&#21040;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large language models such as Open AI's Generative Pre-trained Transformer (GPT) models are proficient at answering questions, but their knowledge is confined to the information present in their training data. This limitation renders them ineffective when confronted with questions about recent developments or non-public documents. Our research proposes a method that enables GPT models to answer questions by employing context from an information source not previously included in their training data. The methodology includes preprocessing of contextual information, the embedding of contexts and queries, constructing prompt through the integration of context embeddings, and generating answers using GPT models. We applied this method in a controlled test scenario using the California Driver's Handbook as the information source. The GPT-3 model achieved a 96% passing score on a set of 50 sample driving knowledge test questions. In contrast, without context, the model's passing score fell to
&lt;/p&gt;</description></item><item><title>&#32473;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35774;&#22791;&#30340;&#25991;&#26412;&#37325;&#20889;&#20195;&#29702;&#30340;&#26500;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#33021;&#22815;&#22312;&#20445;&#25345;&#27169;&#22411;&#33021;&#21147;&#30340;&#21069;&#25552;&#19979;&#65292;&#23454;&#29616;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#25991;&#26412;&#37325;&#20889;&#12290;</title><link>http://arxiv.org/abs/2308.11807</link><description>&lt;p&gt;
&#38754;&#21521;&#35774;&#22791;&#30340;&#25991;&#26412;&#37325;&#20889;&#20195;&#29702;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards an On-device Agent for Text Rewriting. (arXiv:2308.11807v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11807
&lt;/p&gt;
&lt;p&gt;
&#32473;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35774;&#22791;&#30340;&#25991;&#26412;&#37325;&#20889;&#20195;&#29702;&#30340;&#26500;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#33021;&#22815;&#22312;&#20445;&#25345;&#27169;&#22411;&#33021;&#21147;&#30340;&#21069;&#25552;&#19979;&#65292;&#23454;&#29616;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#25991;&#26412;&#37325;&#20889;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#37325;&#20889;&#26041;&#38754;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#20307;&#31215;&#24222;&#22823;&#20351;&#24471;&#23427;&#20204;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#25512;&#29702;&#21464;&#24471;&#19981;&#23454;&#38469;&#65292;&#32780;&#21518;&#32773;&#26412;&#21487;&#33021;&#25552;&#20379;&#22686;&#24378;&#38544;&#31169;&#21644;&#32463;&#27982;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26500;&#24314;&#20197;&#31227;&#21160;&#35774;&#22791;&#20026;&#20013;&#24515;&#30340;&#25991;&#26412;&#37325;&#20889;&#27169;&#22411;&#30340;&#26032;&#22411;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#20559;&#22909;&#25968;&#25454;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#24357;&#34917;&#19982;&#26356;&#22823;&#30340;&#26381;&#21153;&#22120;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive capabilities for text rewriting. Nonetheless, the large sizes of these models make them impractical for on-device inference, which would otherwise allow for enhanced privacy and economical inference. Creating a smaller yet potent language model for text rewriting presents a formidable challenge because it requires balancing the need for a small size with the need to retain the emergent capabilities of the LLM, that requires costly data collection. To address the above challenge, we introduce a new instruction tuning approach for building a mobile-centric text rewriting model. Our strategies enable the generation of high quality training data without any human labeling. In addition, we propose a heuristic reinforcement learning framework which substantially enhances performance without requiring preference data. To further bridge the performance gap with the larger server-side model, we propose an effective approach that combines
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26694;&#26550;FATE&#65292;&#23427;&#36890;&#36807;&#31163;&#32676;&#23398;&#20064;&#26126;&#30830;&#22320;&#23398;&#20064;&#25991;&#26412;&#20013;&#30340;&#24322;&#24120;&#24471;&#20998;&#65292;&#24182;&#21033;&#29992;&#20808;&#21069;&#24050;&#30693;&#30340;&#23569;&#37327;&#24322;&#24120;&#31034;&#20363;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#24182;&#20248;&#21270;&#20102;&#24322;&#24120;&#24471;&#20998;&#30340;&#31934;&#30830;&#24230;&#21644;&#25968;&#25454;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.11780</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#20013;&#20351;&#29992;&#31163;&#32676;&#23398;&#20064;&#36827;&#34892;&#23569;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-shot Anomaly Detection in Text with Deviation Learning. (arXiv:2308.11780v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26694;&#26550;FATE&#65292;&#23427;&#36890;&#36807;&#31163;&#32676;&#23398;&#20064;&#26126;&#30830;&#22320;&#23398;&#20064;&#25991;&#26412;&#20013;&#30340;&#24322;&#24120;&#24471;&#20998;&#65292;&#24182;&#21033;&#29992;&#20808;&#21069;&#24050;&#30693;&#30340;&#23569;&#37327;&#24322;&#24120;&#31034;&#20363;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#24182;&#20248;&#21270;&#20102;&#24322;&#24120;&#24471;&#20998;&#30340;&#31934;&#30830;&#24230;&#21644;&#25968;&#25454;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#25991;&#26412;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#26500;&#24314;&#20165;&#20381;&#36182;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#27169;&#22411;&#19978;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;&#27809;&#26377;&#21487;&#29992;&#30340;&#26631;&#35760;&#24322;&#24120;&#31034;&#20363;&#30340;&#20551;&#35774;&#36816;&#34892;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#20123;&#24322;&#24120;&#36890;&#24120;&#20197;&#23567;&#25968;&#37327;&#23384;&#22312;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#21033;&#29992;&#20808;&#21069;&#24050;&#30693;&#30340;&#24322;&#24120;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#26356;&#27880;&#37325;&#23398;&#20064;&#29305;&#24449;&#23884;&#20837;&#32780;&#19981;&#26159;&#30452;&#25509;&#20248;&#21270;&#24322;&#24120;&#24471;&#20998;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#24322;&#24120;&#24471;&#20998;&#21644;&#23398;&#20064;&#36807;&#31243;&#20013;&#25968;&#25454;&#30340;&#20302;&#25928;&#21033;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FATE&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#26377;&#38480;&#30340;&#24322;&#24120;&#31034;&#20363;&#65292;&#24182;&#20351;&#29992;&#31163;&#32676;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#26126;&#30830;&#22320;&#23398;&#20064;&#24322;&#24120;&#24471;&#20998;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#23558;&#27491;&#24120;&#31034;&#20363;&#30340;&#24322;&#24120;&#24471;&#20998;&#35843;&#25972;&#20026;&#19982;&#20808;&#21069;&#20998;&#24067;&#33719;&#24471;&#30340;&#21442;&#32771;&#24471;&#20998;&#30456;&#20284;&#12290;&#30456;&#21453;&#65292;&#24322;&#24120;&#26679;&#26412;&#34987;&#36843;&#20855;&#26377;&#26126;&#26174;&#20559;&#31163;&#30340;&#24322;&#24120;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most current methods for detecting anomalies in text concentrate on constructing models solely relying on unlabeled data. These models operate on the presumption that no labeled anomalous examples are available, which prevents them from utilizing prior knowledge of anomalies that are typically present in small numbers in many real-world applications. Furthermore, these models prioritize learning feature embeddings rather than optimizing anomaly scores directly, which could lead to suboptimal anomaly scoring and inefficient use of data during the learning process. In this paper, we introduce FATE, a deep few-shot learning-based framework that leverages limited anomaly examples and learns anomaly scores explicitly in an end-to-end method using deviation learning. In this approach, the anomaly scores of normal examples are adjusted to closely resemble reference scores obtained from a prior distribution. Conversely, anomaly samples are forced to have anomalous scores that considerably devi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21644;&#28145;&#24230;&#23398;&#20064;&#20027;&#39064;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#26234;&#33021;&#25163;&#26426;&#37319;&#38598;&#30340;&#35821;&#38899;&#24405;&#38899;&#20013;&#35782;&#21035;&#20986;&#19982;&#25233;&#37057;&#30456;&#20851;&#30340;29&#20010;&#20027;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#20854;&#20013;6&#20010;&#20027;&#39064;&#20316;&#20026;&#25233;&#37057;&#30340;&#39118;&#38505;&#20027;&#39064;&#12290;&#27492;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#38271;&#26399;&#30417;&#27979;&#35821;&#35328;&#20351;&#29992;&#65292;&#21487;&#20197;&#20102;&#35299;&#20027;&#39064;&#30340;&#20986;&#29616;&#19982;&#25233;&#37057;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2308.11773</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21644;&#28145;&#24230;&#23398;&#20064;&#20027;&#39064;&#27169;&#22411;&#22312;&#26234;&#33021;&#25163;&#26426;&#37319;&#38598;&#30340;&#33258;&#30001;&#22238;&#31572;&#35821;&#38899;&#24405;&#38899;&#20013;&#35782;&#21035;&#19982;&#25233;&#37057;&#30456;&#20851;&#30340;&#20027;&#39064;
&lt;/p&gt;
&lt;p&gt;
Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model. (arXiv:2308.11773v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11773
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21644;&#28145;&#24230;&#23398;&#20064;&#20027;&#39064;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#26234;&#33021;&#25163;&#26426;&#37319;&#38598;&#30340;&#35821;&#38899;&#24405;&#38899;&#20013;&#35782;&#21035;&#20986;&#19982;&#25233;&#37057;&#30456;&#20851;&#30340;29&#20010;&#20027;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#20854;&#20013;6&#20010;&#20027;&#39064;&#20316;&#20026;&#25233;&#37057;&#30340;&#39118;&#38505;&#20027;&#39064;&#12290;&#27492;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#38271;&#26399;&#30417;&#27979;&#35821;&#35328;&#20351;&#29992;&#65292;&#21487;&#20197;&#20102;&#35299;&#20027;&#39064;&#30340;&#20986;&#29616;&#19982;&#25233;&#37057;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20351;&#29992;&#24050;&#34987;&#35777;&#26126;&#19982;&#25233;&#37057;&#30456;&#20851;&#65292;&#20294;&#38656;&#35201;&#22823;&#35268;&#27169;&#39564;&#35777;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#35786;&#25152;&#30740;&#31350;&#36153;&#29992;&#39640;&#26114;&#12290;&#22240;&#27492;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24050;&#34987;&#24212;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#19978;&#39044;&#27979;&#25233;&#37057;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;-&#32570;&#20047;&#39564;&#35777;&#26631;&#31614;&#12289;&#26679;&#26412;&#20559;&#24046;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20351;&#29992;Whisper&#24037;&#20855;&#21644;BERTopic&#27169;&#22411;&#22312;&#26469;&#33258;265&#21517;&#21442;&#19982;&#32773;&#30340;3919&#20010;&#26234;&#33021;&#25163;&#26426;&#24405;&#38899;&#20013;&#35782;&#21035;&#20986;29&#20010;&#20027;&#39064;&#12290;&#20854;&#20013;6&#20010;&#20027;&#39064;&#30340;PHQ-8&#20013;&#20301;&#25968;&#22823;&#20110;&#25110;&#31561;&#20110;10&#34987;&#35270;&#20026;&#25233;&#37057;&#30340;&#39118;&#38505;&#20027;&#39064;&#65306;&#27809;&#26377;&#26399;&#26395;&#12289;&#30561;&#30496;&#12289;&#24515;&#29702;&#27835;&#30103;&#12289;&#29702;&#21457;&#12289;&#23398;&#20064;&#21644;&#35838;&#31243;&#12290;&#20026;&#20102;&#38416;&#26126;&#20027;&#39064;&#30340;&#20986;&#29616;&#21644;&#19982;&#25233;&#37057;&#30340;&#20851;&#32852;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#35782;&#21035;&#20986;&#30340;&#20027;&#39064;&#22312;&#34892;&#20026;&#65288;&#26469;&#33258;&#21487;&#31359;&#25140;&#35774;&#22791;&#65289;&#21644;&#35821;&#35328;&#29305;&#24449;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#36824;&#23545;&#20027;&#39064;&#36716;&#21464;&#21644;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#25233;&#37057;&#20005;&#37325;&#31243;&#24230;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#34920;&#26126;&#20102;&#38271;&#26399;&#30417;&#27979;&#35821;&#35328;&#20351;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Language use has been shown to correlate with depression, but large-scale validation is needed. Traditional methods like clinic studies are expensive. So, natural language processing has been employed on social media to predict depression, but limitations remain-lack of validated labels, biased user samples, and no context. Our study identified 29 topics in 3919 smartphone-collected speech recordings from 265 participants using the Whisper tool and BERTopic model. Six topics with a median PHQ-8 greater than or equal to 10 were regarded as risk topics for depression: No Expectations, Sleep, Mental Therapy, Haircut, Studying, and Coursework. To elucidate the topic emergence and associations with depression, we compared behavioral (from wearables) and linguistic characteristics across identified topics. The correlation between topic shifts and changes in depression severity over time was also investigated, indicating the importance of longitudinally monitoring language use. We also tested
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#39640;&#23545;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#36827;&#34892;&#26816;&#27979;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21306;&#20998;&#24320;&#26469;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#26041;&#38754;&#19982;&#30495;&#23454;&#31185;&#23398;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#31639;&#27861;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.11767</link><description>&lt;p&gt;
&#25552;&#39640;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#26816;&#27979;&#30340;&#26041;&#27861;&#65306;&#24341;&#20837;xFakeBibs&#30417;&#30563;&#23398;&#20064;&#32593;&#32476;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm. (arXiv:2308.11767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#39640;&#23545;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#36827;&#34892;&#26816;&#27979;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21306;&#20998;&#24320;&#26469;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#26041;&#38754;&#19982;&#30495;&#23454;&#31185;&#23398;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#31639;&#27861;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#27491;&#22312;&#25104;&#20026;&#29616;&#23454;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21306;&#20998;ChatGPT&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21644;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;100&#20010;&#30495;&#23454;&#20986;&#29256;&#29289;&#25688;&#35201;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#37319;&#29992;10&#20493;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#25509;&#21463;&#33539;&#22260;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;&#19982;ChatGPT&#20869;&#23481;&#36827;&#34892;&#27604;&#36739;&#65292;&#26126;&#26174;&#21487;&#35265;ChatGPT&#20165;&#36129;&#29486;&#20102;23\%&#30340;&#20108;&#20803;&#32452;&#20869;&#23481;&#65292;&#36825;&#27604;&#20854;&#20182;10&#20010;&#20132;&#21449;&#39564;&#35777;&#20013;&#30340;&#20219;&#20309;&#19968;&#20010;&#37117;&#23569;50\%&#12290;&#36825;&#20010;&#20998;&#26512;&#20984;&#26174;&#20102;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#19978;&#19982;&#30495;&#23454;&#31185;&#23398;&#30340;&#26126;&#26174;&#24046;&#24322;&#12290;&#22312;&#23545;&#27599;&#31687;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;xFakeBibs&#31639;&#27861;&#20934;&#30830;&#22320;&#23558;98&#31687;&#20986;&#29256;&#29289;&#35782;&#21035;&#20026;&#20551;&#30340;&#65292;&#26377;2&#31687;&#25991;&#29486;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;&#30495;&#23454;&#20986;&#29256;&#29289;&#12290;&#23613;&#31649;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#31639;&#27861;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is becoming a new reality. In this paper, we show how to distinguish ChatGPT-generated publications from counterparts produced by scientists. Using a newly designed supervised Machine Learning algorithm, we demonstrate how to detect machine-generated publications from those produced by scientists. The algorithm was trained using 100 real publication abstracts, followed by a 10-fold calibration approach to establish a lower-upper bound range of acceptance. In the comparison with ChatGPT content, it was evident that ChatGPT contributed merely 23\% of the bigram content, which is less than 50\% of any of the other 10 calibrating folds. This analysis highlights a significant disparity in technical terms where ChatGPT fell short of matching real science. When categorizing the individual articles, the xFakeBibs algorithm accurately identified 98 out of 100 publications as fake, with 2 articles incorrectly classified as real publications. Though this work introduced an algorithmic app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#26469;&#20943;&#36731;&#20302;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#24471;&#21040;&#20102;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2308.11764</link><description>&lt;p&gt;
Halo&#65306;&#35780;&#20272;&#21644;&#38477;&#20302;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models. (arXiv:2308.11764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#26469;&#20943;&#36731;&#20302;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#24471;&#21040;&#20102;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#12290;&#34429;&#28982;&#23545;&#20110;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#26041;&#20415;&#65292;&#20294;&#26159;&#19982;&#20854;&#26356;&#22823;&#35268;&#27169;&#30340;&#23545;&#24212;&#27169;&#22411;&#30456;&#27604;&#65292;&#24320;&#28304;&#30340;&#21442;&#25968;&#36739;&#23569;&#30340;LLMs&#32463;&#24120;&#20986;&#29616;&#20005;&#37325;&#24187;&#35273;&#38382;&#39064;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#27979;&#37327;&#21644;&#20943;&#23569;BLOOM 7B&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#26159;&#20844;&#24320;&#25552;&#20379;&#32473;&#30740;&#31350;&#21644;&#21830;&#19994;&#24212;&#29992;&#30340;&#24369;&#24320;&#28304;LLMs&#30340;&#20195;&#34920;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;HaloCheck&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26080;&#38656;&#30693;&#35782;&#30340;&#40657;&#30418;&#23376;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;LLMs&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#20302;&#21442;&#25968;LLMs&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;LLMs&#30340;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP). Although convenient for research and practical applications, open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts. This paper focuses on measuring and reducing hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs that are publicly available for research and commercial applications. We introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed to quantify the severity of hallucinations in LLMs. Additionally, we explore techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter LLMs. Our experiments effectively demonstrate the reduction of hallucinations in challenging domains for these LLMs.
&lt;/p&gt;</description></item><item><title>KnowledGPT&#26159;&#19968;&#20010;&#20840;&#38754;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#24211;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#30693;&#35782;&#30340;&#26816;&#32034;&#21644;&#23384;&#20648;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;KnowledGPT&#33021;&#22815;&#26356;&#20840;&#38754;&#21644;&#20934;&#30830;&#22320;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11761</link><description>&lt;p&gt;
KnowledGPT&#65306;&#21033;&#29992;&#26816;&#32034;&#21644;&#23384;&#20648;&#35775;&#38382;&#30693;&#35782;&#24211;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases. (arXiv:2308.11761v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11761
&lt;/p&gt;
&lt;p&gt;
KnowledGPT&#26159;&#19968;&#20010;&#20840;&#38754;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#24211;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#30693;&#35782;&#30340;&#26816;&#32034;&#21644;&#23384;&#20648;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;KnowledGPT&#33021;&#22815;&#26356;&#20840;&#38754;&#21644;&#20934;&#30830;&#22320;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#23637;&#29616;&#20986;&#24778;&#20154;&#30340;&#24433;&#21709;&#21147;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#22914;&#23436;&#25972;&#24615;&#12289;&#21450;&#26102;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#23558;LLMs&#19982;&#22806;&#37096;&#30693;&#35782;&#28304;&#36830;&#25509;&#36215;&#26469;&#65292;&#20294;&#26159;&#30693;&#35782;&#24211;(KBs)&#30340;&#25972;&#21512;&#20173;&#28982;&#40092;&#20026;&#20154;&#30693;&#19988;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;KnowledGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;LLMs&#19982;&#21508;&#31181;&#30693;&#35782;&#24211;&#36830;&#25509;&#36215;&#26469;&#65292;&#26041;&#20415;&#30693;&#35782;&#30340;&#26816;&#32034;&#21644;&#23384;&#20648;&#12290;&#26816;&#32034;&#36807;&#31243;&#37319;&#29992;&#24605;&#32500;&#21551;&#21457;&#31243;&#24207;&#65292;&#29983;&#25104;&#29992;&#20110;KB&#30340;&#25628;&#32034;&#35821;&#35328;&#30340;&#20195;&#30721;&#65292;&#20854;&#20013;&#39044;&#23450;&#20041;&#20102;KB&#25805;&#20316;&#30340;&#20989;&#25968;&#12290;&#38500;&#20102;&#26816;&#32034;&#22806;&#65292;KnowledGPT&#36824;&#25552;&#20379;&#20102;&#23558;&#30693;&#35782;&#23384;&#20648;&#22312;&#20010;&#24615;&#21270;&#30693;&#35782;&#24211;&#20013;&#30340;&#33021;&#21147;&#65292;&#20197;&#28385;&#36275;&#20010;&#20307;&#29992;&#25143;&#38656;&#27714;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#23558;LLMs&#19982;KBs&#38598;&#25104;&#65292;KnowledGPT&#33021;&#22815;&#27491;&#30830;&#22238;&#31572;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive impact in the field of natural language processing, but they still struggle with several issues regarding, such as completeness, timeliness, faithfulness and adaptability. While recent efforts have focuses on connecting LLMs with external knowledge sources, the integration of knowledge bases (KBs) remains understudied and faces several challenges. In this paper, we introduce KnowledGPT, a comprehensive framework to bridge LLMs with various knowledge bases, facilitating both the retrieval and storage of knowledge. The retrieval process employs the program of thought prompting, which generates search language for KBs in code format with pre-defined functions for KB operations. Besides retrieval, KnowledGPT offers the capability to store knowledge in a personalized KB, catering to individual user demands. With extensive experiments, we show that by integrating LLMs with KBs, KnowledGPT properly answers a broader range of questions 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.11730</link><description>&lt;p&gt;
&#22810;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11730
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#8220;&#39044;&#35757;&#32451;&#12289;&#25552;&#31034;&#12289;&#39044;&#27979;&#8221;&#33539;&#24335;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;OD-QA&#65289;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#65288;MD-QA&#65289;&#22330;&#26223;&#19979;&#25506;&#32034;&#36825;&#20010;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#20010;&#35201;&#27714;&#23545;&#19981;&#21516;&#25991;&#26723;&#30340;&#20869;&#23481;&#21644;&#32467;&#26500;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#32852;&#26377;&#28145;&#20837;&#29702;&#35299;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#37325;&#35201;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#65288;KGP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;MD-QA&#20013;&#20026;LLMs&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#22270;&#26500;&#24314;&#27169;&#22359;&#21644;&#22270;&#36941;&#21382;&#27169;&#22359;&#12290;&#23545;&#20110;&#22270;&#26500;&#24314;&#65292;&#25105;&#20204;&#20351;&#29992;&#33410;&#28857;&#26469;&#34920;&#31034;&#25991;&#27573;&#25110;&#25991;&#26723;&#32467;&#26500;&#65288;&#20363;&#22914;&#65292;&#39029;&#38754;/&#34920;&#26684;&#65289;&#65292;&#32780;&#20351;&#29992;&#36793;&#26469;&#34920;&#31034;&#25991;&#27573;&#20043;&#38388;&#30340;&#35821;&#20041;/&#35789;&#27719;&#30456;&#20284;&#24615;&#25110;&#32773;&#25991;&#26723;&#20869;&#30340;&#32467;&#26500;&#20851;&#31995;&#12290;&#23545;&#20110;&#22270;&#36941;&#21382;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;LM&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#23427;&#22312;&#33410;&#28857;&#20043;&#38388;&#23548;&#33322;&#24182;&#25910;&#38598;&#25903;&#25345;&#24615;&#30340;&#25991;&#27573;&#65292;&#20197;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38598;&#21512;&#25193;&#23637;&#21644;&#20195;&#34920;&#24615;&#31034;&#20363;&#30340;&#35821;&#35328;&#25506;&#27979;&#26041;&#27861;&#26469;&#25512;&#36827;&#20851;&#31995;&#25552;&#21462;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#31867;&#21035;&#25490;&#24207;&#65292;&#25552;&#39640;&#20102;&#20851;&#31995;&#20998;&#31867;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#23545;&#27604;&#31867;&#20043;&#38388;&#30340;&#28151;&#28102;&#12290;&#32463;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#20851;&#31995;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11720</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#38598;&#21512;&#25193;&#23637;&#30340;&#31034;&#20363;&#36827;&#34892;&#35821;&#35328;&#25506;&#27979;&#26469;&#25512;&#36827;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Advancing Relation Extraction through Language Probing with Exemplars from Set Co-Expansion. (arXiv:2308.11720v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38598;&#21512;&#25193;&#23637;&#21644;&#20195;&#34920;&#24615;&#31034;&#20363;&#30340;&#35821;&#35328;&#25506;&#27979;&#26041;&#27861;&#26469;&#25512;&#36827;&#20851;&#31995;&#25552;&#21462;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#31867;&#21035;&#25490;&#24207;&#65292;&#25552;&#39640;&#20102;&#20851;&#31995;&#20998;&#31867;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#23545;&#27604;&#31867;&#20043;&#38388;&#30340;&#28151;&#28102;&#12290;&#32463;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#20851;&#31995;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#26159;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20195;&#34920;&#24615;&#31034;&#20363;&#21644;&#38598;&#21512;&#25193;&#23637;&#26469;&#25552;&#39640;&#20851;&#31995;&#20998;&#31867;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#23545;&#27604;&#31867;&#20043;&#38388;&#30340;&#28151;&#28102;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#20195;&#34920;&#24615;&#31034;&#20363;&#20026;&#27599;&#20010;&#20851;&#31995;&#31867;&#25552;&#20379;&#31181;&#23376;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#30340;&#38598;&#21512;&#25193;&#23637;&#31639;&#27861;&#36890;&#36807;&#23558;&#30446;&#26631;&#23545;&#21644;&#30446;&#26631;&#31867;&#30340;&#20195;&#34920;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#32435;&#20837;&#35757;&#32451;&#30446;&#26631;&#26469;&#20016;&#23500;&#35757;&#32451;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#38598;&#21512;&#25193;&#23637;&#36807;&#31243;&#36824;&#28041;&#21450;&#19968;&#20010;&#32771;&#34385;&#23545;&#27604;&#31867;&#31034;&#20363;&#30340;&#31867;&#21035;&#25490;&#24207;&#36807;&#31243;&#12290;&#21033;&#29992;&#26080;&#19978;&#19979;&#25991;&#30340;Hearst&#27169;&#24335;&#21033;&#29992;&#20851;&#31995;&#25552;&#21450;&#30340;&#19978;&#19979;&#25991;&#32454;&#33410;&#26469;&#30830;&#23450;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#12290;&#32463;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#38598;&#21512;&#25193;&#23637;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#39640;&#20102;&#20851;&#31995;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation Extraction (RE) is a pivotal task in automatically extracting structured information from unstructured text. In this paper, we present a multi-faceted approach that integrates representative examples and through co-set expansion. The primary goal of our method is to enhance relation classification accuracy and mitigating confusion between contrastive classes.  Our approach begins by seeding each relationship class with representative examples. Subsequently, our co-set expansion algorithm enriches training objectives by incorporating similarity measures between target pairs and representative pairs from the target class. Moreover, the co-set expansion process involves a class ranking procedure that takes into account exemplars from contrastive classes. Contextual details encompassing relation mentions are harnessed via context-free Hearst patterns to ascertain contextual similarity.  Empirical evaluation demonstrates the efficacy of our co-set expansion approach, resulting in a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.11696</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#22686;&#21152;&#23548;&#33268;&#20102;&#19968;&#31867;&#20840;&#38754;&#35780;&#20272;&#24191;&#27867;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#19982;&#22823;&#35268;&#27169;&#35745;&#31639;&#25104;&#26412;&#30456;&#20851;&#65292;&#27599;&#20010;&#27169;&#22411;&#38656;&#35201;&#25968;&#21315;&#20010;GPU&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35780;&#20272;&#25928;&#29575;&#26041;&#38754;&#30340;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#35752;&#35770;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#19981;&#25439;&#23475;&#21487;&#38752;&#24615;&#30340;&#24773;&#20917;&#19979;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;HELM&#22522;&#20934;&#27979;&#35797;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#35745;&#31639;-&#21487;&#38752;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#36890;&#36807;&#20174;&#22522;&#20934;&#27979;&#35797;&#20013;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#65292;&#24403;&#21069;&#22312;HELM&#19978;&#30340;&#39046;&#20808;&#32773;&#21487;&#33021;&#20250;&#25913;&#21464;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#31034;&#20363;&#21363;&#21487;&#33719;&#24471;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#23454;&#26102;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#30340;&#21457;&#23637;&#65292;&#20351;&#29992;&#21160;&#24577;&#35821;&#27861;&#21644;CHILDES&#35821;&#26009;&#24211;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#27010;&#29575;&#30340;&#36880;&#27493;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;78%&#30340;&#24773;&#20917;&#19979;&#33021;&#23436;&#20840;&#21305;&#37197;&#26368;&#20339;&#20505;&#36873;&#36755;&#20986;&#65292;&#19988;&#20855;&#22791;&#33258;&#25105;&#20462;&#22797;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.11683</link><description>&lt;p&gt;
&#23454;&#26102;&#23398;&#20064;&#29983;&#25104;&#21644;&#20462;&#22797;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Learning to generate and corr- uh I mean repair language in real-time. (arXiv:2308.11683v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23454;&#26102;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#30340;&#21457;&#23637;&#65292;&#20351;&#29992;&#21160;&#24577;&#35821;&#27861;&#21644;CHILDES&#35821;&#26009;&#24211;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#27010;&#29575;&#30340;&#36880;&#27493;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;78%&#30340;&#24773;&#20917;&#19979;&#33021;&#23436;&#20840;&#21305;&#37197;&#26368;&#20339;&#20505;&#36873;&#36755;&#20986;&#65292;&#19988;&#20855;&#22791;&#33258;&#25105;&#20462;&#22797;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#20013;&#65292;&#21457;&#35328;&#32773;&#36880;&#23383;&#36880;&#21477;&#22320;&#20135;&#29983;&#35821;&#35328;&#65292;&#24182;&#19981;&#26029;&#30417;&#25511;&#33258;&#24049;&#30340;&#36129;&#29486;&#26159;&#21542;&#36866;&#24403;&#65292;&#21516;&#26102;&#21160;&#24577;&#22320;&#36866;&#24212;&#23545;&#35805;&#30340;&#29615;&#22659;&#65292;&#36825;&#32463;&#24120;&#23548;&#33268;&#20182;&#20204;&#22312;&#35828;&#35805;&#36807;&#31243;&#20013;&#21363;&#26102;&#20462;&#22797;&#33258;&#24049;&#30340;&#35805;&#35821;&#12290;&#36825;&#31181;&#23454;&#26102;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#23545;&#20110;&#27969;&#21033;&#21644;&#33258;&#28982;&#30340;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20808;&#21069;&#23398;&#20064;&#30340;&#21160;&#24577;&#35821;&#27861;&#21644;CHILDES&#35821;&#26009;&#24211;&#65292;&#24320;&#21457;&#12289;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;&#19968;&#20010;&#22522;&#20110;&#27010;&#29575;&#30340;&#36880;&#27493;&#29983;&#25104;&#27169;&#22411;&#65292;&#27169;&#22411;&#30340;&#36755;&#20837;&#26159;&#19968;&#20010;&#32431;&#31929;&#30340;&#35821;&#20041;&#29983;&#25104;&#30446;&#26631;&#27010;&#24565;&#65292;&#20351;&#29992;&#31867;&#22411;&#29702;&#35770;&#19982;&#35760;&#24405;&#65288;TTR&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#22312;78%&#30340;&#24773;&#20917;&#19979;&#19982;&#26368;&#20339;&#20505;&#36873;&#36755;&#20986;&#23436;&#20840;&#21305;&#37197;&#65292;ROUGE-l&#24471;&#20998;&#20026;0.86&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#21516;&#19968;&#27169;&#22411;&#29983;&#25104;&#22312;&#35805;&#35821;&#36807;&#31243;&#20013;&#29983;&#25104;&#30446;&#26631;&#21457;&#29983;&#21464;&#21270;&#26102;&#30340;&#33258;&#25105;&#20462;&#22797;&#33021;&#21147;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#35780;&#20272;&#12290;&#33258;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#33258;&#25105;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conversation, speakers produce language incrementally, word by word, while continuously monitoring the appropriateness of their own contribution in the dynamically unfolding context of the conversation; and this often leads them to repair their own utterance on the fly. This real-time language processing capacity is furthermore crucial to the development of fluent and natural conversational AI. In this paper, we use a previously learned Dynamic Syntax grammar and the CHILDES corpus to develop, train and evaluate a probabilistic model for incremental generation where input to the model is a purely semantic generation goal concept in Type Theory with Records (TTR). We show that the model's output exactly matches the gold candidate in 78% of cases with a ROUGE-l score of 0.86. We further do a zero-shot evaluation of the ability of the same model to generate self-repairs when the generation goal changes mid-utterance. Automatic evaluation shows that the model can generate self-repairs c
&lt;/p&gt;</description></item><item><title>Tryage&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36335;&#30001;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#23545;&#20010;&#20307;&#36755;&#20837;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#20174;&#27169;&#22411;&#24211;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#20197;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#21644;&#23450;&#21046;&#21270;&#30340;&#36127;&#25285;&#65292;&#37322;&#25918;&#24222;&#22823;&#30340;&#26032;&#20852;&#27169;&#22411;&#24211;&#30340;&#24040;&#22823;&#23041;&#21147;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2308.11601</link><description>&lt;p&gt;
Tryage: &#23454;&#26102;&#26234;&#33021;&#36335;&#30001;&#29992;&#25143;&#25552;&#31034;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model. (arXiv:2308.11601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11601
&lt;/p&gt;
&lt;p&gt;
Tryage&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36335;&#30001;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#23545;&#20010;&#20307;&#36755;&#20837;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#20174;&#27169;&#22411;&#24211;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#20197;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#21644;&#23450;&#21046;&#21270;&#30340;&#36127;&#25285;&#65292;&#37322;&#25918;&#24222;&#22823;&#30340;&#26032;&#20852;&#27169;&#22411;&#24211;&#30340;&#24040;&#22823;&#23041;&#21147;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#26550;&#26500;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#24341;&#20837;&#23548;&#33268;&#20102;&#22312;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#21644;&#25968;&#25454;&#39046;&#22495;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#12290;&#22312;Hugging Face&#29983;&#24577;&#31995;&#32479;&#20013;&#26377;&#36229;&#36807;200,000&#20010;&#27169;&#22411;&#65292;&#29992;&#25143;&#22312;&#36873;&#25321;&#21644;&#20248;&#21270;&#27169;&#22411;&#20197;&#36866;&#24212;&#22810;&#26041;&#38754;&#30340;&#24037;&#20316;&#27969;&#31243;&#21644;&#25968;&#25454;&#39046;&#22495;&#30340;&#21516;&#26102;&#65292;&#36824;&#35201;&#35299;&#20915;&#35745;&#31639;&#12289;&#23433;&#20840;&#21644;&#26102;&#25928;&#24615;&#31561;&#38382;&#39064;&#12290;&#36843;&#20999;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#26469;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#21644;&#23450;&#21046;&#21270;&#30340;&#36127;&#25285;&#65292;&#24182;&#37322;&#25918;&#24222;&#22823;&#30340;&#26032;&#20852;&#27169;&#22411;&#24211;&#30340;&#24040;&#22823;&#23041;&#21147;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36335;&#30001;&#31995;&#32479;Tryage&#65292;&#23427;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36335;&#30001;&#22120;&#26681;&#25454;&#23545;&#20010;&#20307;&#36755;&#20837;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#20174;&#27169;&#22411;&#24211;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19987;&#23478;&#27169;&#22411;&#12290;&#21463;&#22823;&#33041;&#20013;&#30340;&#19992;&#33041;&#36335;&#30001;&#22120;&#21551;&#21457;&#65292;Tryage&#37319;&#29992;&#24863;&#30693;&#36335;&#30001;&#22120;&#26469;&#39044;&#27979;&#19979;&#28216;&#27169;&#22411;&#22312;&#25552;&#31034;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20570;&#20986;&#36335;&#30001;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11534</link><description>&lt;p&gt;
&#20316;&#20026;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as a User Simulator. (arXiv:2308.11534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38381;&#28304;ChatGPT&#30340;&#21331;&#36234;&#24615;&#33021;&#24341;&#21457;&#20102;&#23545;&#20854;&#27665;&#20027;&#21270;&#30340;&#21162;&#21147;&#65292;&#20511;&#21161;&#30495;&#23454;&#29992;&#25143;&#21644;ChatGPT&#23545;&#35805;&#30340;&#21162;&#21147;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;Vicuna&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#20363;&#23376;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;Baize&#21644;UltraChat&#31561;&#21162;&#21147;&#20027;&#35201;&#20381;&#38752;ChatGPT&#26681;&#25454;&#25351;&#20196;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#65292;&#32780;&#19981;&#26159;&#30495;&#23454;&#30340;&#20154;&#31867;&#23398;&#20064;&#65292;&#23548;&#33268;&#33539;&#22260;&#26377;&#38480;&#65292;&#22810;&#26679;&#24615;&#20943;&#24369;&#65292;&#32570;&#20047;&#30495;&#27491;&#30340;&#22810;&#36718;&#23545;&#35805;&#21160;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#25226;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#12290;&#38543;&#21518;&#65292;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#25105;&#20204;&#30340;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;Vicuna-Bench&#21644;MT-Bench&#20013;&#22343;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT conversations, as evidenced by Vicuna. However, while current endeavors like Baize and UltraChat aim to auto-generate conversational data due to challenges in gathering human participation, they primarily rely on ChatGPT to simulate human behaviors based on directives rather than genuine human learning. This results in a limited scope, diminished diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we innovatively target human questions extracted from genuine human-machine conversations as a learning goal and train a user simulator, UserGPT, to produce a high-quality human-centric synthetic conversation dataset, RealChat. Subsequently, this dataset trains our assistant model, ReaLM. Experimentally, ReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;SONAR&#65292;&#19968;&#20010;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#30340;&#21477;&#23376;&#23884;&#20837;&#31354;&#38388;&#65292;&#36890;&#36807;&#21333;&#19968;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#30456;&#20284;&#24615;&#25628;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20110;200&#31181;&#35821;&#35328;&#30340;&#25991;&#26412;&#35299;&#30721;&#22120;&#65292;&#21487;&#20197;&#36827;&#34892;&#25991;&#26412;&#21040;&#25991;&#26412;&#21644;&#35821;&#38899;&#21040;&#25991;&#26412;&#30340;&#26426;&#22120;&#32763;&#35793;&#12290;&#36825;&#20123;&#32467;&#26524;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#23545;&#35821;&#38899;&#21040;&#25991;&#26412;&#27169;&#22411;&#20063;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11466</link><description>&lt;p&gt;
&#21477;&#23376;&#32423;&#22810;&#27169;&#24577;&#21644;&#35821;&#35328;&#26080;&#20851;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Sentence-Level Multimodal and Language-Agnostic Representations. (arXiv:2308.11466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11466
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;SONAR&#65292;&#19968;&#20010;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#30340;&#21477;&#23376;&#23884;&#20837;&#31354;&#38388;&#65292;&#36890;&#36807;&#21333;&#19968;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#30456;&#20284;&#24615;&#25628;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20110;200&#31181;&#35821;&#35328;&#30340;&#25991;&#26412;&#35299;&#30721;&#22120;&#65292;&#21487;&#20197;&#36827;&#34892;&#25991;&#26412;&#21040;&#25991;&#26412;&#21644;&#35821;&#38899;&#21040;&#25991;&#26412;&#30340;&#26426;&#22120;&#32763;&#35793;&#12290;&#36825;&#20123;&#32467;&#26524;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#23545;&#35821;&#38899;&#21040;&#25991;&#26412;&#27169;&#22411;&#20063;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SONAR&#65292;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#30340;&#23450;&#38271;&#21477;&#23376;&#23884;&#20837;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#21333;&#19968;&#25991;&#26412;&#32534;&#30721;&#22120;&#35206;&#30422;&#20102;200&#31181;&#35821;&#35328;&#65292;&#22312;xsim&#21644;xsim++&#22810;&#35821;&#35328;&#30456;&#20284;&#24615;&#25628;&#32034;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;LASER3&#21644;LabSE&#12290;&#20351;&#29992;&#29305;&#23450;&#35821;&#35328;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#22312;&#24072;&#29983;&#35774;&#32622;&#19979;&#35757;&#32451;&#35821;&#38899;&#36716;&#24405;&#25968;&#25454;&#21518;&#65292;&#35821;&#38899;&#29255;&#27573;&#21487;&#20197;&#22312;&#21516;&#19968;SONAR&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#32534;&#30721;&#22120;&#22312;&#30456;&#20284;&#24615;&#25628;&#32034;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;200&#31181;&#35821;&#35328;&#30340;&#25991;&#26412;&#35299;&#30721;&#22120;&#65292;&#21487;&#20197;&#36827;&#34892;&#25991;&#26412;&#21040;&#25991;&#26412;&#21644;&#35821;&#38899;&#21040;&#25991;&#26412;&#30340;&#26426;&#22120;&#32763;&#35793;&#65292;&#21253;&#25324;&#38646;&#32763;&#35793;&#35821;&#35328;&#21644;&#27169;&#24577;&#32452;&#21512;&#12290;&#23613;&#31649;&#23384;&#22312;&#23450;&#38271;&#29942;&#39048;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#32467;&#26524;&#22312;&#19982;&#26368;&#20808;&#36827;&#30340;NLLB~1B&#27169;&#22411;&#30456;&#27604;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#30340;&#38646;&#32763;&#35793;&#35821;&#38899;&#21040;&#25991;&#26412;&#32467;&#26524;&#19982;&#24378;&#26377;&#21147;&#30340;&#30417;&#30563;&#22522;&#32447;&#27169;&#22411;Whisper&#30456;&#27604;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SONAR, a new multilingual and multimodal fixed-size sentence embedding space. Our single text encoder, covering 200 languages, substantially outperforms existing sentence embeddings such as LASER3 and LabSE on the xsim and xsim++ multilingual similarity search tasks. Speech segments can be embedded in the same SONAR embedding space using language-specific speech encoders trained in a teacher-student setting on speech transcription data. Our encoders outperform existing speech encoders on similarity search tasks. We also provide a text decoder for 200 languages, which allows us to perform text-to-text and speech-to-text machine translation, including for zero-shot language and modality combinations. Our text-to-text results are competitive compared to the state-of-the-art NLLB~1B model, despite the fixed-size bottleneck representation. Our zero-shot speech-to-text translation results compare favorably with strong supervised baselines such as Whisper.
&lt;/p&gt;</description></item><item><title>BAN-PL&#26159;&#27874;&#20848;&#35821;&#30340;&#31532;&#19968;&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;Wykop&#36825;&#20010;&#31867;&#20284;"&#27874;&#20848;&#29256;Reddit"&#30340;&#31038;&#20132;&#32593;&#32476;&#26381;&#21153;&#30340;&#34987;&#26631;&#35760;&#20026;&#26377;&#23475;&#24182;&#21024;&#38500;&#30340;&#20869;&#23481;&#65292;&#23558;&#26377;&#21161;&#20110;&#25913;&#36827;&#33258;&#21160;&#26816;&#27979;&#20114;&#32852;&#32593;&#19978;&#30340;&#20882;&#29359;&#24615;&#35821;&#35328;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2308.10592</link><description>&lt;p&gt;
BAN-PL: &#19968;&#20221;&#26469;&#33258;Wykop.pl&#32593;&#31449;&#30340;&#31105;&#27490;&#26377;&#23475;&#21644;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#26032;&#27874;&#20848;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BAN-PL: a Novel Polish Dataset of Banned Harmful and Offensive Content from Wykop.pl web service. (arXiv:2308.10592v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10592
&lt;/p&gt;
&lt;p&gt;
BAN-PL&#26159;&#27874;&#20848;&#35821;&#30340;&#31532;&#19968;&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;Wykop&#36825;&#20010;&#31867;&#20284;"&#27874;&#20848;&#29256;Reddit"&#30340;&#31038;&#20132;&#32593;&#32476;&#26381;&#21153;&#30340;&#34987;&#26631;&#35760;&#20026;&#26377;&#23475;&#24182;&#21024;&#38500;&#30340;&#20869;&#23481;&#65292;&#23558;&#26377;&#21161;&#20110;&#25913;&#36827;&#33258;&#21160;&#26816;&#27979;&#20114;&#32852;&#32593;&#19978;&#30340;&#20882;&#29359;&#24615;&#35821;&#35328;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#26816;&#27979;&#20114;&#32852;&#32593;&#19978;&#30340;&#20882;&#29359;&#24615;&#35821;&#35328;&#12289;&#20167;&#24680;&#35328;&#35770;&#21644;&#32593;&#32476;&#27450;&#20940;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#38656;&#35201;&#25913;&#36827;&#23545;&#21253;&#21547;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#35775;&#38382;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BAN-PL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20197;&#27874;&#20848;&#35821;&#25552;&#20379;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#23427;&#21253;&#21547;&#20102;&#34987;&#19987;&#19994;&#23457;&#26597;&#21592;&#26631;&#35760;&#20026;&#26377;&#23475;&#24182;&#38543;&#21518;&#34987;&#21024;&#38500;&#30340;&#25991;&#26412;&#12290;&#25968;&#25454;&#38598;&#20849;&#21253;&#21547;&#26469;&#33258;Wykop&#36825;&#20010;&#39047;&#21463;&#27426;&#36814;&#30340;&#31038;&#20132;&#32593;&#32476;&#26381;&#21153;&#30340;691,662&#26465;&#20869;&#23481;&#65292;&#20854;&#20013;&#21253;&#25324;&#24086;&#23376;&#21644;&#35780;&#35770;&#65292;&#24182;&#19988;&#24179;&#22343;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65306;&#8220;&#26377;&#23475;&#8221;&#21644;&#8220;&#20013;&#31435;&#8221;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#35814;&#32454;&#35828;&#26126;&#65292;&#24182;&#24378;&#35843;&#20102;&#25968;&#25454;&#30340;&#35821;&#35328;&#29305;&#27530;&#24615;&#12290;BAN-PL&#25968;&#25454;&#38598;&#20197;&#21450;&#29992;&#20110;&#39044;&#22788;&#29702;&#33039;&#35805;&#30340;&#39640;&#32423;&#33050;&#26412;&#23558;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in automated detection of offensive language online, including hate speech and cyberbullying, require improved access to publicly available datasets comprising social media content. In this paper, we introduce BAN-PL, the first open dataset in the Polish language that encompasses texts flagged as harmful and subsequently removed by professional moderators. The dataset encompasses a total of 691,662 pieces of content from a popular social networking service, Wykop, often referred to as the "Polish Reddit", including both posts and comments, and is evenly distributed into two distinct classes: "harmful" and "neutral". We provide a comprehensive description of the data collection and preprocessing procedures, as well as highlight the linguistic specificity of the data. The BAN-PL dataset, along with advanced preprocessing scripts for, i.a., unmasking profanities, will be publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#23545;&#35805;&#26041;&#27861;&#26469;&#35299;&#20915;&#20010;&#24615;&#21270;&#33021;&#28304;&#30456;&#20851;&#38382;&#39064;&#30340;&#26032;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#35268;&#33539;&#33258;&#21160;&#32763;&#35793;&#20026;&#20248;&#21270;&#23454;&#20363;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#21644;&#21709;&#24212;&#29992;&#25143;&#35268;&#33539;&#21644;&#20559;&#22909;&#65292;&#24182;&#25552;&#20379;&#38750;&#32447;&#24615;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#19968;&#26041;&#27861;&#31361;&#30772;&#20102;&#24403;&#21069;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#23454;&#20363;&#30456;&#20851;&#30340;&#33021;&#28304;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.10380</link><description>&lt;p&gt;
&#20154;&#22312;&#24490;&#29615;&#20013;&#30340;&#21487;&#25345;&#32493;&#24615;&#20248;&#21270;&#33258;&#21160;&#24418;&#24335;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Human-on-the-Loop Optimization Autoformalism Approach for Sustainability. (arXiv:2308.10380v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#23545;&#35805;&#26041;&#27861;&#26469;&#35299;&#20915;&#20010;&#24615;&#21270;&#33021;&#28304;&#30456;&#20851;&#38382;&#39064;&#30340;&#26032;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#35268;&#33539;&#33258;&#21160;&#32763;&#35793;&#20026;&#20248;&#21270;&#23454;&#20363;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#21644;&#21709;&#24212;&#29992;&#25143;&#35268;&#33539;&#21644;&#20559;&#22909;&#65292;&#24182;&#25552;&#20379;&#38750;&#32447;&#24615;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#19968;&#26041;&#27861;&#31361;&#30772;&#20102;&#24403;&#21069;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#23454;&#20363;&#30456;&#20851;&#30340;&#33021;&#28304;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#33258;&#28982;&#23545;&#35805;&#30340;&#26041;&#24335;&#35299;&#20915;&#20010;&#24615;&#21270;&#33021;&#28304;&#30456;&#20851;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#21487;&#23450;&#21046;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#38656;&#35201;&#21453;&#22797;&#35299;&#20915;&#65292;&#24182;&#19988;&#22312;&#24314;&#27169;&#19978;&#26377;&#36731;&#24494;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#26159;&#29992;&#25143;&#29305;&#23450;&#30340;&#65292;&#22240;&#27492;&#23545;&#20110;&#21046;&#23450;&#19968;&#31181;&#36866;&#21512;&#25152;&#26377;&#29992;&#25143;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#23558;&#20248;&#21270;&#27714;&#35299;&#22120;&#19982;LLM&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20854;&#29702;&#35299;&#21644;&#21709;&#24212;&#29992;&#25143;&#35268;&#33539;&#21644;&#20559;&#22909;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20379;&#38750;&#32447;&#24615;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24320;&#21019;&#20102;&#20154;&#23548;&#21521;&#30340;&#20248;&#21270;&#33258;&#21160;&#24418;&#24335;&#21270;&#30340;&#26032;&#27010;&#24565;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#35268;&#33539;&#33258;&#21160;&#32763;&#35793;&#20026;&#20248;&#21270;&#23454;&#20363;&#12290;&#36825;&#20351;&#24471;LLM&#33021;&#22815;&#20998;&#26512;&#12289;&#35299;&#37322;&#21644;&#35299;&#20915;&#21508;&#31181;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#33021;&#28304;&#38382;&#39064;&#65292;&#31361;&#30772;&#20102;&#24403;&#21069;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#33021;&#28304;&#39046;&#22495;&#30340;&#21508;&#31181;&#24120;&#35265;&#20219;&#21153;&#65292;&#20174;&#30005;&#21147;&#21040;...
&lt;/p&gt;
&lt;p&gt;
This paper outlines a natural conversational approach to solving personalized energy-related problems using large language models (LLMs). We focus on customizable optimization problems that necessitate repeated solving with slight variations in modeling and are user-specific, hence posing a challenge to devising a one-size-fits-all model. We put forward a strategy that augments an LLM with an optimization solver, enhancing its proficiency in understanding and responding to user specifications and preferences while providing nonlinear reasoning capabilities. Our approach pioneers the novel concept of human-guided optimization autoformalism, translating a natural language task specification automatically into an optimization instance. This enables LLMs to analyze, explain, and tackle a variety of instance-specific energy-related problems, pushing beyond the limits of current prompt-based techniques.  Our research encompasses various commonplace tasks in the energy sector, from electric v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#35777;&#35843;&#26597;&#65292;&#25506;&#32034;&#20102;&#20998;&#24067;&#22806;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#21457;&#29616;&#20102;LLM&#22312;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#24182;&#37319;&#29992;&#20102;&#26032;&#30340;&#29983;&#25104;&#24335;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.10261</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#38754;&#26377;&#22810;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Good Are Large Language Models at Out-of-Distribution Detection?. (arXiv:2308.10261v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#35777;&#35843;&#26597;&#65292;&#25506;&#32034;&#20102;&#20998;&#24067;&#22806;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#21457;&#29616;&#20102;LLM&#22312;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#24182;&#37319;&#29992;&#20102;&#26032;&#30340;&#29983;&#25104;&#24335;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#26816;&#27979;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#22312;ML&#31038;&#21306;&#24341;&#36215;&#20102;&#33539;&#24335;&#36716;&#21464;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#30740;&#31350;&#24050;&#32463;&#20197;BERT&#12289;RoBERTa&#21644;GPT-2&#31561;&#30456;&#23545;&#23567;&#35268;&#27169;&#30340;Transformer&#27169;&#22411;&#25506;&#32034;&#20102;OOD&#26816;&#27979;&#65292;&#20294;&#22312;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#25512;&#29702;&#33539;&#24335;&#26041;&#38754;&#30340;&#26126;&#26174;&#24046;&#24322;&#24341;&#21457;&#20102;&#23545;&#36825;&#20123;&#21457;&#29616;&#22312;LLM&#20013;&#30340;&#36866;&#29992;&#24615;&#30340;&#36136;&#30097;&#12290;&#26412;&#25991;&#22312;LLMs&#39046;&#22495;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#37325;&#28857;&#20851;&#27880;7B&#21040;65B&#22823;&#23567;&#30340;LLaMA&#31995;&#21015;&#12290;&#25105;&#20204;&#23545;&#24120;&#29992;&#30340;OOD&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23457;&#26597;&#20102;&#23427;&#20204;&#22312;&#38646;&#26799;&#24230;&#21644;&#24494;&#35843;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#20043;&#21069;&#30340;&#21028;&#21035;&#24335;&#30340;&#20869;&#37096;&#20998;&#24067;&#24494;&#35843;&#25913;&#20026;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20351;LLM&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#19982;&#20043;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection plays a vital role in enhancing the reliability of machine learning (ML) models. The emergence of large language models (LLMs) has catalyzed a paradigm shift within the ML community, showcasing their exceptional capabilities across diverse natural language processing tasks. While existing research has probed OOD detection with relative small-scale Transformers like BERT, RoBERTa and GPT-2, the stark differences in scales, pre-training objectives, and inference paradigms call into question the applicability of these findings to LLMs. This paper embarks on a pioneering empirical investigation of OOD detection in the domain of LLMs, focusing on LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate commonly-used OOD detectors, scrutinizing their performance in both zero-grad and fine-tuning scenarios. Notably, we alter previous discriminative in-distribution fine-tuning into generative fine-tuning, aligning the pre-training objective of LLM
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20197;&#32418;&#38431;&#35780;&#20272;&#30340;&#26041;&#24335;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#35780;&#20272;&#65292;&#21457;&#29616;&#21363;&#20415;&#26159;&#24191;&#27867;&#37096;&#32626;&#30340;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#36830;&#32493;&#21457;&#35328;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#36829;&#21453;&#20262;&#29702;&#22320;&#23545;&#26377;&#23475;&#26597;&#35810;&#20570;&#20986;&#22238;&#24212;&#12290;&#36890;&#36807;&#32418;&#38431;&#35780;&#20272;&#23581;&#35797;&#65292;&#21457;&#29616;&#22810;&#25968;&#24320;&#28304;LLM&#20063;&#20250;&#29983;&#25104;&#26377;&#23475;&#22238;&#24212;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;LLM&#23433;&#20840;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.09662</link><description>&lt;p&gt;
&#20351;&#29992;&#36830;&#32493;&#21457;&#35328;&#30340;&#26041;&#24335;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32418;&#38431;&#35780;&#20272;&#20197;&#23454;&#29616;&#23433;&#20840;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment. (arXiv:2308.09662v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09662
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20197;&#32418;&#38431;&#35780;&#20272;&#30340;&#26041;&#24335;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#35780;&#20272;&#65292;&#21457;&#29616;&#21363;&#20415;&#26159;&#24191;&#27867;&#37096;&#32626;&#30340;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#36830;&#32493;&#21457;&#35328;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#36829;&#21453;&#20262;&#29702;&#22320;&#23545;&#26377;&#23475;&#26597;&#35810;&#20570;&#20986;&#22238;&#24212;&#12290;&#36890;&#36807;&#32418;&#38431;&#35780;&#20272;&#23581;&#35797;&#65292;&#21457;&#29616;&#22810;&#25968;&#24320;&#28304;LLM&#20063;&#20250;&#29983;&#25104;&#26377;&#23475;&#22238;&#24212;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;LLM&#23433;&#20840;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#20248;&#21270;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#30446;&#26631;&#65292;&#20197;&#20854;&#24040;&#22823;&#30340;&#22810;&#20219;&#21153;&#33021;&#21147;&#38663;&#25788;&#19990;&#30028;&#12290;&#38543;&#30528;&#23427;&#20204;&#30340;&#23646;&#24615;&#21644;&#32534;&#30721;&#30693;&#35782;&#30340;&#20986;&#29616;&#65292;LLMs&#20135;&#29983;&#26377;&#23475;&#36755;&#20986;&#30340;&#39118;&#38505;&#22686;&#21152;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#21512;&#21487;&#25193;&#23637;&#22320;&#37096;&#32626;&#32473;&#20844;&#20247;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23433;&#20840;&#35780;&#20272;&#22522;&#20934;RED-EVAL&#65292;&#36827;&#34892;&#32418;&#38431;&#35780;&#20272;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20415;&#26159;&#24191;&#27867;&#37096;&#32626;&#30340;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#22522;&#20110;&#36830;&#32493;&#21457;&#35328;&#30340;(CoU)&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#20351;&#22522;&#20110;GPT-4&#21644;ChatGPT&#30340;&#38381;&#28304;LLM&#31995;&#32479;&#36829;&#21453;&#20262;&#29702;&#22320;&#23545;&#36229;&#36807;65%&#21644;73%&#30340;&#26377;&#23475;&#26597;&#35810;&#20570;&#20986;&#22238;&#24212;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;RED-EVAL&#22312;8&#20010;&#24320;&#28304;LLM&#20013;&#30340;&#19968;&#33268;&#24615;&#65292;&#36890;&#36807;&#32418;&#38431;&#35780;&#20272;&#23581;&#35797;&#29983;&#25104;86%&#20197;&#19978;&#30340;&#26377;&#23475;&#22238;&#24212;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RED-INSTRUCT--&#19968;&#31181;&#29992;&#20110;LLM&#23433;&#20840;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;1&#65289;HARMFULQA&#25968;&#25454;&#25910;&#38598;&#65306;&#21033;&#29992;CoU&#25552;&#31034;,
&lt;/p&gt;
&lt;p&gt;
Larger language models (LLMs) have taken the world by storm with their massive multi-tasking capabilities simply by optimizing over a next-word prediction objective. With the emergence of their properties and encoded knowledge, the risk of LLMs producing harmful outputs increases, making them unfit for scalable deployment for the public. In this work, we propose a new safety evaluation benchmark RED-EVAL that carries out red-teaming. We show that even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of harmful queries. We also demonstrate the consistency of the RED-EVAL across 8 open-source LLMs in generating harmful responses in more than 86% of the red-teaming attempts. Next, we propose RED-INSTRUCT--An approach for the safety alignment of LLMs. It constitutes two phases: 1) HARMFULQA data collection: Leveraging CoU prompting, 
&lt;/p&gt;</description></item><item><title>MemoChat&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35843;&#20248;&#25351;&#20196;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#22791;&#24536;&#24405;&#26469;&#20445;&#25345;&#23545;&#35805;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08239</link><description>&lt;p&gt;
MemoChat: &#36890;&#36807;&#35843;&#25972;LLMs&#20351;&#29992;&#22791;&#24536;&#24405;&#20197;&#20445;&#25345;&#19968;&#33268;&#24615;&#30340;&#38271;&#36317;&#31163;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation. (arXiv:2308.08239v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08239
&lt;/p&gt;
&lt;p&gt;
MemoChat&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35843;&#20248;&#25351;&#20196;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#22791;&#24536;&#24405;&#26469;&#20445;&#25345;&#23545;&#35805;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MemoChat&#65292;&#19968;&#20010;&#29992;&#20110;&#20248;&#21270;&#25351;&#20196;&#30340;&#27969;&#27700;&#32447;&#65292;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26377;&#25928;&#22320;&#20351;&#29992;&#33258;&#34892;&#32452;&#32455;&#30340;&#22791;&#24536;&#24405;&#26469;&#20445;&#25345;&#19968;&#33268;&#30340;&#38271;&#36317;&#31163;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#12290;&#25105;&#20204;&#36890;&#36807;&#36845;&#20195;&#30340;&#8220;&#35760;&#24518;-&#26816;&#32034;-&#21709;&#24212;&#8221;&#24490;&#29615;&#23637;&#31034;&#20102;&#19968;&#20010;&#38271;&#36317;&#31163;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#12290;&#36825;&#35201;&#27714;&#25105;&#20204;&#20026;&#27599;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#31934;&#24515;&#35774;&#35745;&#23450;&#21046;&#30340;&#35843;&#20248;&#25351;&#20196;&#12290;&#36825;&#20123;&#25351;&#20196;&#26159;&#20174;&#19968;&#31995;&#21015;&#20844;&#20849;&#25968;&#25454;&#38598;&#20013;&#37325;&#24314;&#30340;&#65292;&#20197;&#25945;&#23548;LLMs&#35760;&#24518;&#21644;&#26816;&#32034;&#36807;&#21435;&#30340;&#23545;&#35805;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#22791;&#24536;&#24405;&#25552;&#39640;&#26410;&#26469;&#23545;&#35805;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36992;&#35831;&#19987;&#23478;&#25163;&#21160;&#27880;&#37322;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38271;&#36317;&#31163;&#23545;&#35805;&#19968;&#33268;&#24615;&#30340;&#27979;&#35797;&#38598;&#12290;&#22312;&#28041;&#21450;&#24320;&#28304;&#21644;&#21487;&#35775;&#38382;API&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#19977;&#31181;&#27979;&#35797;&#22330;&#26223;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;MemoChat&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#36229;&#36234;&#20102;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose MemoChat, a pipeline for refining instructions that enables large language models (LLMs) to effectively employ self-composed memos for maintaining consistent long-range open-domain conversations. We demonstrate a long-range open-domain conversation through iterative "memorization-retrieval-response" cycles. This requires us to carefully design tailored tuning instructions for each distinct stage. The instructions are reconstructed from a collection of public datasets to teach the LLMs to memorize and retrieve past dialogues with structured memos, leading to enhanced consistency when participating in future conversations. We invite experts to manually annotate a test set designed to evaluate the consistency of long-range conversations questions. Experiments on three testing scenarios involving both open-source and API-accessible chatbots at scale verify the efficacy of MemoChat, which outperforms strong baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07758</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#65288;Chain-of-Though, CoT&#65289;&#25552;&#31034;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;Self-Consistency&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#37319;&#26679;&#19968;&#32452;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#36825;&#20123;&#38142;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#28982;&#21518;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#26102;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#26495;&#65292;&#21363;``&#22914;&#26524;&#25105;&#20204;&#30693;&#36947;&#19978;&#36848;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#20505;&#36873;&#31572;&#26696;&#65292;&#37027;&#20040;&#26410;&#30693;&#21464;&#37327;x&#30340;&#20540;&#26159;&#22810;&#23569;&#65311;''&#65292;&#23558;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#23631;&#34109;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#22914;&#26524;&#25552;&#20379;&#30340;&#20505;&#36873;&#31572;&#26696;&#26159;&#27491;&#30830;&#30340;&#65292;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#25104;&#21151;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;FOBAR&#26041;&#27861;&#65292;&#23558;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.16680</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#26223;&#35266;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#39046;&#20808;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23545;&#20154;&#31867;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#20063;&#26292;&#38706;&#20986;&#22266;&#26377;&#30340;&#39118;&#38505;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#30340;&#21452;&#37325;&#24615;&#36136;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#21487;&#20449;&#24230;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#25991;&#29486;&#65292;&#20294;&#38024;&#23545;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#21487;&#20449;&#24230;&#30340;&#32508;&#21512;&#35843;&#26597;&#20173;&#28982;&#24456;&#23569;&#35265;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#35843;&#26597;&#20102;&#28041;&#21450;&#36825;&#20123;&#27169;&#22411;&#30340;&#38271;&#26399;&#21644;&#26032;&#20852;&#23041;&#32961;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36131;&#20219;&#36825;&#22235;&#20010;&#22522;&#26412;&#32500;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#24352;&#35814;&#23613;&#30340;&#22320;&#22270;&#65292;&#27010;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20123;&#21162;&#21147;&#23545;&#20110;&#20419;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ulti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#20998;&#31867;&#21644;&#20998;&#26512;&#20102;ACL Anthology&#20013;&#30340;&#30740;&#31350;&#35770;&#25991;&#65292;&#25552;&#20379;&#20102;&#23545;&#30740;&#31350;&#39046;&#22495;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#21644;NLP&#39046;&#22495;&#30340;&#20998;&#31867;&#23398;&#12290;&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;NLP&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.10652</link><description>&lt;p&gt;
&#25506;&#35752;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#39046;&#22495;&#30340;&#21457;&#23637;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Exploring the Landscape of Natural Language Processing Research. (arXiv:2307.10652v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10652
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#20998;&#31867;&#21644;&#20998;&#26512;&#20102;ACL Anthology&#20013;&#30340;&#30740;&#31350;&#35770;&#25991;&#65292;&#25552;&#20379;&#20102;&#23545;&#30740;&#31350;&#39046;&#22495;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#21644;NLP&#39046;&#22495;&#30340;&#20998;&#31867;&#23398;&#12290;&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;NLP&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20316;&#20026;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#30340;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#65292;&#22312;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#24555;&#36895;&#20256;&#25773;&#21644;&#24191;&#27867;&#24212;&#29992;&#12290;&#37492;&#20110;&#35813;&#39046;&#22495;&#30740;&#31350;&#24037;&#20316;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#30740;&#31350;&#30028;&#24050;&#23545;&#25968;&#20010;&#19982;NLP&#30456;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20173;&#32570;&#23569;&#19968;&#39033;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#23545;&#24050;&#24314;&#31435;&#30340;&#20027;&#39064;&#36827;&#34892;&#20998;&#31867;&#12289;&#35782;&#21035;&#36235;&#21183;&#24182;&#27010;&#25324;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;ACL Anthology&#20013;&#21253;&#21547;&#30340;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#31867;&#21644;&#20998;&#26512;&#12290;&#32467;&#26524;&#21576;&#29616;&#20102;&#30740;&#31350;&#39046;&#22495;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#65292;&#20026;NLP&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20998;&#31867;&#23398;&#65292;&#20998;&#26512;&#20102;NLP&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24635;&#32467;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an efficient approach to understand, generate, and process natural language texts, research in natural language processing (NLP) has exhibited a rapid spread and wide adoption in recent years. Given the increasing amount of research work in this area, several NLP-related approaches have been surveyed in the research community. However, a comprehensive study that categorizes established topics, identifies trends, and outlines areas for future research remains absent to this day. Contributing to closing this gap, we have systematically classified and analyzed research papers included in the ACL Anthology. As a result, we present a structured overview of the research landscape, provide a taxonomy of fields-of-study in NLP, analyze recent developments in NLP, summarize our findings, and highlight directions for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#33258;&#27965;&#24615;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#20174;&#19968;&#20010;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#26469;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06857</link><description>&lt;p&gt;
&#33258;&#27965;&#24615;&#26041;&#27861;&#29992;&#20110;&#26080;&#38480;&#29983;&#25104;&#38382;&#39064;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Self-consistency for open-ended generations. (arXiv:2307.06857v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#33258;&#27965;&#24615;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#20174;&#19968;&#20010;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#26469;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#30340;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#33258;&#27965;&#24615;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#20855;&#26377;&#22266;&#23450;&#31572;&#26696;&#30340;&#25552;&#31034;&#65292;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25512;&#24191;&#30340;&#33258;&#27965;&#24615;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;&#20854;&#36866;&#29992;&#24615;&#65292;&#36229;&#36234;&#20102;&#22266;&#23450;&#31572;&#26696;&#38382;&#39064;&#30340;&#33539;&#22260;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20174;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#21363;&#20351;&#27809;&#26377;&#35775;&#38382;&#21040;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#20063;&#33021;&#22312;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#26174;&#33879;&#21644;&#19968;&#33268;&#22320;&#25913;&#36827;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#27809;&#26377;&#35745;&#31639;&#24320;&#38144;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20877;&#25490;&#24207;&#27169;&#22411;&#25110;&#23545;&#29616;&#26377;&#27169;&#22411;&#30340;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for improving the quality and consistency of generated outputs from large-scale pre-trained language models (LLMs). Self-consistency has emerged as an effective approach for prompts with fixed answers, selecting the answer with the highest number of votes. In this paper, we introduce a generalized framework for self-consistency that extends its applicability beyond problems that have fixed-answer answers. Through extensive simulations, we demonstrate that our approach consistently recovers the optimal or near-optimal generation from a set of candidates. We also propose lightweight parameter-free similarity functions that show significant and consistent improvements across code generation, autoformalization, and summarization tasks, even without access to token log probabilities. Our method incurs minimal computational overhead, requiring no auxiliary reranker models or modifications to the existing model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20998;&#37197;&#21644;&#20998;&#31867;&#36719;&#20214;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#35821;&#35328;&#29305;&#24449;&#21644;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#20998;&#37197;&#32473;&#26368;&#30456;&#20851;&#30340;&#22242;&#38431;&#25104;&#21592;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20197;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00009</link><description>&lt;p&gt;
&#33258;&#21160;&#20998;&#37197;&#21644;&#20998;&#31867;&#36719;&#20214;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Automated Assignment and Classification of Software Issues. (arXiv:2307.00009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20998;&#37197;&#21644;&#20998;&#31867;&#36719;&#20214;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#35821;&#35328;&#29305;&#24449;&#21644;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#20998;&#37197;&#32473;&#26368;&#30456;&#20851;&#30340;&#22242;&#38431;&#25104;&#21592;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20197;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#38382;&#39064;&#21253;&#21547;&#20462;&#22797;&#12289;&#25913;&#36827;&#25110;&#21019;&#24314;&#26032;&#32447;&#31243;&#30340;&#24037;&#20316;&#21333;&#20803;&#65292;&#22312;&#24320;&#21457;&#36807;&#31243;&#20013;&#20419;&#36827;&#22242;&#38431;&#25104;&#21592;&#20043;&#38388;&#30340;&#27807;&#36890;&#12290;&#23558;&#38382;&#39064;&#20998;&#37197;&#32473;&#26368;&#30456;&#20851;&#30340;&#22242;&#38431;&#25104;&#21592;&#24182;&#30830;&#23450;&#38382;&#39064;&#30340;&#31867;&#21035;&#26159;&#19968;&#39033;&#32321;&#29712;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#38169;&#35823;&#30340;&#20998;&#31867;&#20250;&#23548;&#33268;&#39033;&#30446;&#24310;&#36831;&#21644;&#37325;&#26032;&#24037;&#20316;&#65292;&#32473;&#22242;&#38431;&#25104;&#21592;&#24102;&#26469;&#40635;&#28902;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#29992;&#20110;&#27973;&#23618;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#35821;&#35328;&#29305;&#24449;&#65292;&#24182;&#23558;&#27973;&#23618;&#26041;&#27861;&#21644;&#38598;&#25104;&#26041;&#27861;&#19982;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#20998;&#37197;&#32473;&#22235;&#31181;&#35282;&#33394;&#65288;&#35774;&#35745;&#24072;&#12289;&#24320;&#21457;&#20154;&#21592;&#12289;&#27979;&#35797;&#20154;&#21592;&#21644;&#39046;&#23548;&#32773;&#65289;&#65292;&#32780;&#19981;&#26159;&#29305;&#23450;&#30340;&#20010;&#20154;&#25110;&#22242;&#38431;&#65292;&#20197;&#20419;&#36827;&#25105;&#20204;&#35299;&#20915;&#26041;&#26696;&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#24320;&#21457;&#20154;&#21592;&#30340;&#32463;&#39564;&#27700;&#24179;&#65292;&#20197;&#21453;&#26144;&#25105;&#20204;&#35299;&#20915;&#26041;&#26696;&#30340;&#24037;&#19994;&#23454;&#36341;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#31867;&#26041;&#27861;&#23558;&#38382;&#39064;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#21253;&#25324;&#38169;&#35823;&#12289;&#26032;&#21151;&#33021;&#12289;&#25913;&#36827;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software issues contain units of work to fix, improve or create new threads during the development and facilitate communication among the team members. Assigning an issue to the most relevant team member and determining a category of an issue is a tedious and challenging task. Wrong classifications cause delays and rework in the project and trouble among the team members. This thesis proposes a set of carefully curated linguistic features for shallow machine learning methods and compares the performance of shallow and ensemble methods with deep language models. Unlike the state-of-the-art, we assign issues to four roles (designer, developer, tester, and leader) rather than to specific individuals or teams to contribute to the generality of our solution. We also consider the level of experience of the developers to reflect the industrial practices in our solution formulation. We employ a classification approach to categorize issues into distinct classes, namely bug, new feature, improve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#36716;&#21270;&#20026;&#26356;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#22810;&#27169;&#24577;&#20851;&#31995;&#25277;&#21462;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.14122</link><description>&lt;p&gt;
&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#25552;&#21462;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#21644;&#22810;&#27169;&#24577;&#20851;&#31995;&#25277;&#21462;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Prompt Distillation for Multimodal Named Entity and Multimodal Relation Extraction. (arXiv:2306.14122v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#36716;&#21270;&#20026;&#26356;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#22810;&#27169;&#24577;&#20851;&#31995;&#25277;&#21462;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;MNER&#65289;&#21644;&#22810;&#27169;&#24577;&#20851;&#31995;&#25277;&#21462;&#65288;MRE&#65289;&#38656;&#35201;&#22788;&#29702;&#22797;&#26434;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#22522;&#26412;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#25552;&#28860;&#20026;&#26356;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#31995;&#21015;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26469;&#23454;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#28085;&#30422;&#22810;&#31890;&#24230;&#65288;&#21517;&#35789;&#12289;&#21477;&#23376;&#12289;&#22810;&#27169;&#24577;&#65289;&#21644;&#25968;&#25454;&#22686;&#24378;&#65288;&#26679;&#24335;&#12289;&#23454;&#20307;&#12289;&#22270;&#20687;&#65289;&#32500;&#24230;&#30340;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#65292;&#23637;&#31034;&#20102;&#20174;LLMs&#20013;&#24341;&#23548;&#27492;&#31867;&#25512;&#29702;&#33021;&#21147;&#30340;&#31034;&#20363;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#25552;&#31034;&#25552;&#21462;&#26041;&#27861;&#65292;&#20197;&#21560;&#25910;LLMs&#20013;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#22312;&#22788;&#29702;&#20165;&#25991;&#26412;&#36755;&#20837;&#26102;&#30340;&#23454;&#29992;&#24615;&#65292;&#32780;&#26080;&#38656;&#28155;&#21152;&#22270;&#20687;&#21644;&#38142;&#24335;&#24605;&#32500;&#30693;&#35782;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction (MRE) necessitate the fundamental reasoning capacity for intricate linguistic and multimodal comprehension. In this study, we explore distilling the reasoning ability of large language models (LLMs) into a more compact student model by generating a \textit{chain of thought} (CoT) -- a sequence of intermediate reasoning steps. Specifically, we commence by exemplifying the elicitation of such reasoning ability from LLMs through CoT prompts covering multi-grain (noun, sentence, multimodality) and data-augmentation (style, entity, image) dimensions. Subsequently, we present a novel conditional prompt distillation method to assimilate the commonsense reasoning ability from LLMs, thereby enhancing the utility of the student model in addressing text-only inputs without the requisite addition of image and CoT knowledge. Extensive experiments reveal that our approach attains state-of-the-art accuracy and manifests a p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#39046;&#22495;&#29305;&#23450;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#30340;&#36923;&#36753;&#32534;&#31243;&#35821;&#35328;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#65292;&#23454;&#29616;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#26469;&#35299;&#20915;KGQA&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#35782;&#21035;&#25152;&#26377;&#27979;&#35797;&#38382;&#39064;&#30340;&#27491;&#30830;&#31572;&#26696;&#23454;&#20307;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#39640;&#25928;&#26524;&#65292;&#21363;&#20351;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#35299;&#20915;&#39046;&#22495;&#29305;&#23450;&#22270;&#35889;&#19978;&#30340;&#38382;&#39064;&#22238;&#31572;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.02206</link><description>&lt;p&gt;
&#20351;&#29992;&#36923;&#36753;&#32534;&#31243;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#12290; (arXiv:2303.02206v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Domain Specific Question Answering Over Knowledge Graphs Using Logical Programming and Large Language Models. (arXiv:2303.02206v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02206
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#39046;&#22495;&#29305;&#23450;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#30340;&#36923;&#36753;&#32534;&#31243;&#35821;&#35328;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#65292;&#23454;&#29616;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#26469;&#35299;&#20915;KGQA&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#35782;&#21035;&#25152;&#26377;&#27979;&#35797;&#38382;&#39064;&#30340;&#27491;&#30830;&#31572;&#26696;&#23454;&#20307;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#39640;&#25928;&#26524;&#65292;&#21363;&#20351;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#35299;&#20915;&#39046;&#22495;&#29305;&#23450;&#22270;&#35889;&#19978;&#30340;&#38382;&#39064;&#22238;&#31572;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39046;&#22495;&#29305;&#23450;&#30340;&#22270;&#35889;&#19978;&#22238;&#31572;&#38382;&#39064;&#38656;&#35201;&#19968;&#31181;&#23450;&#21046;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#20851;&#31995;&#30340;&#25968;&#37327;&#26377;&#38480;&#65292;&#24182;&#19988;&#39046;&#22495;&#30340;&#29305;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#32463;&#20856;&#30340;&#36923;&#36753;&#32534;&#31243;&#35821;&#35328;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#20854;&#33021;&#22815;&#21033;&#29992;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#26469;&#35299;&#20915;KGQA&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#34920;&#31034;&#20026;Prolog&#26597;&#35810;&#65292;&#25105;&#20204;&#21487;&#20197;&#23481;&#26131;&#22320;&#29983;&#25104;&#31243;&#24207;&#21270;&#29983;&#25104;&#30340;&#31572;&#26696;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#33879;&#21517;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;MetaQA&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#20102;&#19968;&#23567;&#37096;&#20998;&#27880;&#37322;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#33021;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#25152;&#26377;&#27979;&#35797;&#38382;&#39064;&#30340;&#27491;&#30830;&#31572;&#26696;&#23454;&#20307;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#39046;&#22495;&#29305;&#23450;&#22270;&#35889;&#19978;&#30340;&#38382;&#39064;&#22238;&#31572;&#65292;&#36890;&#36807;&#25972;&#21512;&#36923;&#36753;&#32534;&#31243;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#21644;&#20581;&#22766;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering questions over domain-specific graphs requires a tailored approach due to the limited number of relations and the specific nature of the domain. Our approach integrates classic logical programming languages into large language models (LLMs), enabling the utilization of logical reasoning capabilities to tackle the KGQA task. By representing the questions as Prolog queries, which are readable and near close to natural language in representation, we facilitate the generation of programmatically derived answers. To validate the effectiveness of our approach, we evaluate it using a well-known benchmark dataset, MetaQA. Our experimental results demonstrate that our method achieves accurate identification of correct answer entities for all test questions, even when trained on a small fraction of annotated data. Overall, our work presents a promising approach to addressing question answering over domain-specific graphs, offering an explainable and robust solution by incorporating log
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;NLP&#20316;&#20026;&#38236;&#22836;&#65292;&#36890;&#36807;&#23450;&#20301;&#22240;&#26524;&#20851;&#31995;&#21644;&#24863;&#30693;&#25366;&#25496;&#24515;&#29702;&#20581;&#24247;&#65292;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#19978;&#29992;&#25143;&#30340;&#24515;&#29702;&#35821;&#35328;&#36164;&#28304;&#65292;&#25552;&#20986;&#20102;&#20851;&#38190;&#32500;&#24230;&#21644;&#39046;&#22495;&#65292;&#20197;&#20419;&#36827;&#20020;&#24202;&#24515;&#29702;&#23398;&#23454;&#36341;&#21644;&#20010;&#24615;&#21270;&#24515;&#29702;&#20445;&#20581;&#12290;</title><link>http://arxiv.org/abs/2301.11004</link><description>&lt;p&gt;
NLP&#20316;&#20026;&#23450;&#20301;&#22240;&#26524;&#20851;&#31995;&#21644;&#24863;&#30693;&#25366;&#25496;&#24515;&#29702;&#20581;&#24247;&#30340;&#31038;&#20132;&#23186;&#20307;&#30340;&#38236;&#22836;
&lt;/p&gt;
&lt;p&gt;
NLP as a Lens for Causal Analysis and Perception Mining to Infer Mental Health on Social Media. (arXiv:2301.11004v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;NLP&#20316;&#20026;&#38236;&#22836;&#65292;&#36890;&#36807;&#23450;&#20301;&#22240;&#26524;&#20851;&#31995;&#21644;&#24863;&#30693;&#25366;&#25496;&#24515;&#29702;&#20581;&#24247;&#65292;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#19978;&#29992;&#25143;&#30340;&#24515;&#29702;&#35821;&#35328;&#36164;&#28304;&#65292;&#25552;&#20986;&#20102;&#20851;&#38190;&#32500;&#24230;&#21644;&#39046;&#22495;&#65292;&#20197;&#20419;&#36827;&#20020;&#24202;&#24515;&#29702;&#23398;&#23454;&#36341;&#21644;&#20010;&#24615;&#21270;&#24515;&#29702;&#20445;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#65292;&#20154;&#20204;&#20043;&#38388;&#30340;&#20114;&#21160;&#24448;&#24448;&#20256;&#36798;&#20102;&#20182;&#20204;&#34892;&#20026;&#32972;&#21518;&#30340;&#24847;&#22270;&#65292;&#20026;&#22312;&#32447;&#29992;&#25143;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#25552;&#20379;&#20102;&#24515;&#29702;&#35821;&#35328;&#36164;&#28304;&#12290;&#35745;&#31639;&#26234;&#33021;&#25216;&#26415;&#22312;&#20174;&#36825;&#20123;&#31038;&#20132;&#23186;&#20307;&#36164;&#28304;&#20013;&#25512;&#26029;&#24515;&#29702;&#30142;&#30149;&#26041;&#38754;&#30340;&#25104;&#21151;&#65292;&#34920;&#26126;NLP&#21487;&#20197;&#20316;&#20026;&#23450;&#20301;&#22240;&#26524;&#20851;&#31995;&#21644;&#24863;&#30693;&#25366;&#25496;&#30340;&#38236;&#22836;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#38656;&#35201;&#26356;&#22810;&#26377;&#24847;&#20041;&#21644;&#21487;&#35299;&#37322;&#30340;&#30740;&#31350;&#65292;&#20197;&#22312;&#20020;&#24202;&#24515;&#29702;&#23398;&#23454;&#36341;&#21644;&#20010;&#24615;&#21270;&#24515;&#29702;&#20445;&#20581;&#26041;&#38754;&#20135;&#29983;&#26368;&#20339;&#24433;&#21709;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#37325;&#35201;&#32500;&#24230;&#65306;&#65288;1&#65289;&#23450;&#20301;&#22240;&#26524;&#20851;&#31995;&#20197;&#35828;&#26126;&#29992;&#25143;&#33258;&#21160;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#65307;&#65288;2&#65289;&#24863;&#30693;&#25366;&#25496;&#20197;&#25512;&#26029;&#31038;&#20132;&#25928;&#24212;&#23545;&#22312;&#32447;&#29992;&#25143;&#24847;&#22270;&#30340;&#24515;&#29702;&#35266;&#28857;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#19982;&#36825;&#20004;&#20010;&#32500;&#24230;&#30456;&#20851;&#30340;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#26368;&#36817;&#22312;&#35805;&#35821;&#20998;&#26512;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions among humans on social media often convey intentions behind their actions, yielding a psychological language resource for Mental Health Analysis (MHA) of online users. The success of Computational Intelligence Techniques (CIT) for inferring mental illness from such social media resources points to NLP as a lens for causal analysis and perception mining. However, we argue that more consequential and explainable research is required for optimal impact on clinical psychology practice and personalized mental healthcare. To bridge this gap, we posit two significant dimensions: (1) Causal analysis to illustrate a cause and effect relationship in the user generated text; (2) Perception mining to infer psychological perspectives of social effects on online users intentions. Within the scope of Natural Language Processing (NLP), we further explore critical areas of inquiry associated with these two dimensions, specifically through recent advancements in discourse analysis. This pos
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#30340;&#20316;&#32773;&#39118;&#26684;&#36716;&#31227;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26159;&#19968;&#31867;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20316;&#32773;&#39118;&#26684;&#36716;&#31227;&#65292;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#30446;&#26631;&#20316;&#32773;&#39118;&#26684;&#25991;&#26412;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#38750;&#30693;&#21517;&#20316;&#32773;&#30340;&#39118;&#26684;&#36716;&#31227;&#23578;&#26410;&#26377;&#20805;&#20998;&#30340;&#30740;&#31350;&#65292;&#32780;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#36866;&#29992;&#20110;&#24050;&#21457;&#34920;&#30340;&#20316;&#23478;&#12289;&#25919;&#27835;&#23478;&#25110;&#20854;&#20182;&#30693;&#21517;&#20154;&#22763;&#21644;&#20316;&#32773;&#39118;&#26684;&#12290;</title><link>http://arxiv.org/abs/2212.08986</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#30340;&#20316;&#32773;&#39118;&#26684;&#36716;&#31227;&#65306;&#38750;&#30693;&#21517;&#20316;&#32773;&#33021;&#22815;&#34987;&#27169;&#20223;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Low-Resource Authorship Style Transfer: Can Non-Famous Authors Be Imitated?. (arXiv:2212.08986v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08986
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#30340;&#20316;&#32773;&#39118;&#26684;&#36716;&#31227;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26159;&#19968;&#31867;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20316;&#32773;&#39118;&#26684;&#36716;&#31227;&#65292;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#30446;&#26631;&#20316;&#32773;&#39118;&#26684;&#25991;&#26412;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#38750;&#30693;&#21517;&#20316;&#32773;&#30340;&#39118;&#26684;&#36716;&#31227;&#23578;&#26410;&#26377;&#20805;&#20998;&#30340;&#30740;&#31350;&#65292;&#32780;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#36866;&#29992;&#20110;&#24050;&#21457;&#34920;&#30340;&#20316;&#23478;&#12289;&#25919;&#27835;&#23478;&#25110;&#20854;&#20182;&#30693;&#21517;&#20154;&#22763;&#21644;&#20316;&#32773;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#39118;&#26684;&#36716;&#31227;&#26159;&#25351;&#23558;&#25991;&#26412;&#25913;&#20889;&#25104;&#30446;&#26631;&#20316;&#32773;&#30340;&#39118;&#26684;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#24847;&#24605;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#22823;&#22810;&#19987;&#27880;&#20110;&#23558;&#39118;&#26684;&#36716;&#31227;&#21040;&#22312;&#20070;&#31821;&#12289;&#28436;&#35762;&#25110;&#20854;&#20182;&#24050;&#21457;&#34920;&#20316;&#21697;&#20013;&#20855;&#26377;&#35768;&#22810;&#31034;&#20363;&#30340;&#30446;&#26631;&#20316;&#32773;&#36523;&#19978;&#12290;&#36825;&#31181;&#39640;&#36164;&#28304;&#30340;&#35757;&#32451;&#25968;&#25454;&#35201;&#27714;&#65288;&#36890;&#24120;&#22823;&#20110;10&#19975;&#20010;&#35789;&#65289;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#36866;&#29992;&#20110;&#23558;&#39118;&#26684;&#36716;&#31227;&#21040;&#24050;&#21457;&#34920;&#30340;&#20316;&#23478;&#12289;&#25919;&#27835;&#23478;&#25110;&#20854;&#20182;&#30693;&#21517;&#20154;&#22763;&#21644;&#20316;&#32773;&#39118;&#26684;&#19978;&#65292;&#32780;&#36716;&#31227;&#21040;&#38750;&#30693;&#21517;&#20316;&#32773;&#30340;&#39118;&#26684;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#30340;&#20316;&#32773;&#39118;&#26684;&#36716;&#31227;&#8221;&#20219;&#21153;&#65292;&#36825;&#26159;&#19968;&#31867;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20316;&#32773;&#39118;&#26684;&#36716;&#31227;&#65292;&#20165;&#23384;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#30446;&#26631;&#20316;&#32773;&#39118;&#26684;&#25991;&#26412;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#36873;&#25321;&#20102;Reddit&#19978;&#30340;&#28304;&#20316;&#32773;&#21644;&#30446;&#26631;&#20316;&#32773;&#65292;&#24182;&#23545;&#20182;&#20204;&#30340;Reddit&#24086;&#23376;&#36827;&#34892;&#39118;&#26684;&#36716;&#31227;&#65292;&#38480;&#21046;&#33258;&#24049;&#20165;&#20351;&#29992;&#20102;16&#31687;&#24086;&#23376;&#65288;&#24179;&#22343;&#32422;500&#20010;&#35789;&#65289;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Authorship style transfer involves altering text to match the style of a target author whilst preserving the original meaning. Existing unsupervised approaches like STRAP have largely focused on style transfer to target authors with many examples of their writing style in books, speeches, or other published works. This high-resource training data requirement (often greater than 100,000 words) makes these approaches primarily useful for style transfer to published authors, politicians, or other well-known figures and authorship styles, while style transfer to non-famous authors has not been well-studied. We introduce the \textit{low-resource authorship style transfer} task, a more challenging class of authorship style transfer where only a limited amount of text in the target author's style may exist. In our experiments, we specifically choose source and target authors from Reddit and style transfer their Reddit posts, limiting ourselves to just 16 posts (on average ~500 words) of the t
&lt;/p&gt;</description></item><item><title>PyABSA&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;&#21487;&#22797;&#29616;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#25903;&#25345;&#22810;&#20010;ABSA&#23376;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#28789;&#27963;&#30340;&#25193;&#23637;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.01368</link><description>&lt;p&gt;
PyABSA: &#19968;&#20010;&#29992;&#20110;&#21487;&#22797;&#29616;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PyABSA: A Modularized Framework for Reproducible Aspect-based Sentiment Analysis. (arXiv:2208.01368v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01368
&lt;/p&gt;
&lt;p&gt;
PyABSA&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;&#21487;&#22797;&#29616;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#25903;&#25345;&#22810;&#20010;ABSA&#23376;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#28789;&#27963;&#30340;&#25193;&#23637;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#30340;&#21457;&#23637;&#65292;&#24613;&#38656;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#22797;&#21046;&#26368;&#26032;&#25216;&#26415;&#30340;ABSA&#24615;&#33021;&#30340;&#38590;&#24230;&#65292;&#23588;&#20854;&#26159;&#23545;&#21021;&#23398;&#32773;&#26469;&#35828;&#12290;&#20026;&#20102;&#28385;&#36275;&#38656;&#27714;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PyABSA&#65292;&#19968;&#20010;&#22522;&#20110;PyTorch&#26500;&#24314;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#22797;&#29616;&#30340;ABSA&#12290;&#20026;&#20102;&#20419;&#36827;ABSA&#30740;&#31350;&#65292;PyABSA&#25903;&#25345;&#22810;&#20010;ABSA&#23376;&#20219;&#21153;&#65292;&#21253;&#25324;&#26041;&#38754;&#26415;&#35821;&#25552;&#21462;&#12289;&#26041;&#38754;&#24773;&#24863;&#20998;&#31867;&#21644;&#31471;&#21040;&#31471;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PyABSA&#38598;&#25104;&#20102;29&#20010;&#27169;&#22411;&#21644;26&#20010;&#25968;&#25454;&#38598;&#12290;&#21482;&#38656;&#20960;&#34892;&#20195;&#30721;&#65292;&#23601;&#21487;&#20197;&#22797;&#29616;&#27169;&#22411;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;PyABSA&#36824;&#21487;&#20197;&#28789;&#27963;&#25193;&#23637;&#21040;&#32771;&#34385;&#30340;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;PyABSA&#31361;&#20986;&#20102;&#20854;&#25968;&#25454;&#22686;&#24378;&#21644;&#26631;&#27880;&#21151;&#33021;&#65292;&#26174;&#33879;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#27426;&#36814;&#22823;&#23478;&#22312;\url{https://github.com/yangheng95/PyABSA}&#19978;&#23581;&#35797;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of aspect-based sentiment analysis (ABSA) has urged the lack of a user-friendly framework that can largely lower the difficulty of reproducing state-of-the-art ABSA performance, especially for beginners. To meet the demand, we present \our, a modularized framework built on PyTorch for reproducible ABSA. To facilitate ABSA research, PyABSA supports several ABSA subtasks, including aspect term extraction, aspect sentiment classification, and end-to-end aspect-based sentiment analysis. Concretely, PyABSA integrates 29 models and 26 datasets. With just a few lines of code, the result of a model on a specific dataset can be reproduced. With a modularized design, PyABSA can also be flexibly extended to considered models, datasets, and other related tasks. Besides, PyABSA highlights its data augmentation and annotation features, which significantly address data scarcity. All are welcome to have a try at \url{https://github.com/yangheng95/PyABSA}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#38454;&#32447;&#24615;&#36923;&#36753;&#19982;&#25193;&#23637;&#24352;&#37327;&#31867;&#22411;&#28436;&#31639;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22266;&#26377;&#30340;&#28436;&#32462;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2206.08955</link><description>&lt;p&gt;
&#35753;&#19968;&#38454;&#32447;&#24615;&#36923;&#36753;&#25104;&#20026;&#29983;&#25104;&#35821;&#27861;
&lt;/p&gt;
&lt;p&gt;
Making first order linear logic a generating grammar. (arXiv:2206.08955v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#38454;&#32447;&#24615;&#36923;&#36753;&#19982;&#25193;&#23637;&#24352;&#37327;&#31867;&#22411;&#28436;&#31639;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22266;&#26377;&#30340;&#28436;&#32462;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#19981;&#21516;&#30340;&#33539;&#30068;&#35821;&#27861;&#22312;&#19968;&#38454;&#20056;&#27861;&#32447;&#24615;&#36923;&#36753;&#30340;&#19968;&#20010;&#29255;&#27573;&#20013;&#20855;&#26377;&#34920;&#38754;&#34920;&#31034;&#12290; &#25105;&#20204;&#34920;&#26126;&#65292;&#35813;&#29255;&#27573;&#31561;&#20215;&#20110;&#26368;&#36817;&#24341;&#20837;&#30340;&#25193;&#23637;&#24352;&#37327;&#31867;&#22411;&#28436;&#31639;&#12290; &#36825;&#19981;&#20165;&#20026;&#21069;&#32773;&#25552;&#20379;&#20102;&#19968;&#20123;&#26367;&#20195;&#30340;&#35821;&#27861;&#21644;&#30452;&#35266;&#30340;&#20960;&#20309;&#34920;&#31034;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#22266;&#26377;&#30340;&#28436;&#32462;&#31995;&#32479;&#65292;&#36825;&#26159;&#20197;&#21069;&#32570;&#23569;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is known that different categorial grammars have surface representation in a fragment of first order multiplicative linear logic. We show that the fragment of interest is equivalent to the recently introduced {\it extended tensor type calculus}. This provides the former not only with some alternative syntax and intuitive geometric representation, but also with an intrinsic deductive system, which has been absent.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35821;&#27861;&#30340;&#32467;&#26500;&#21270;&#36328;&#24230;&#36873;&#25321;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#36328;&#24230;&#32423;&#27880;&#37322;&#65292;&#25682;&#24323;&#20102;&#36138;&#24515;&#36328;&#24230;&#36873;&#25321;&#26041;&#26696;&#65292;&#20026;&#20849;&#25351;&#28040;&#35299;&#21644;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20219;&#21153;&#24102;&#26469;&#20102;&#23454;&#35777;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2205.03977</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#36328;&#24230;&#36873;&#25321;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Structured Span Selector. (arXiv:2205.03977v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03977
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35821;&#27861;&#30340;&#32467;&#26500;&#21270;&#36328;&#24230;&#36873;&#25321;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#36328;&#24230;&#32423;&#27880;&#37322;&#65292;&#25682;&#24323;&#20102;&#36138;&#24515;&#36328;&#24230;&#36873;&#25321;&#26041;&#26696;&#65292;&#20026;&#20849;&#25351;&#28040;&#35299;&#21644;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20219;&#21153;&#24102;&#26469;&#20102;&#23454;&#35777;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20363;&#22914;&#20849;&#25351;&#28040;&#35299;&#21644;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65292;&#38656;&#35201;&#36873;&#25321;&#25991;&#26412;&#36328;&#24230;&#24182;&#23545;&#20854;&#36827;&#34892;&#20915;&#31574;&#12290;&#36825;&#20123;&#20219;&#21153;&#30340;&#19968;&#31181;&#20856;&#22411;&#26041;&#27861;&#26159;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#36328;&#24230;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#36138;&#23146;&#22320;&#36873;&#25321;&#36328;&#24230;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#19979;&#28216;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#27809;&#26377;&#32467;&#21512;&#20219;&#20309;&#24402;&#32435;&#20559;&#32622;&#26469;&#30830;&#23450;&#24212;&#35813;&#36873;&#25321;&#21738;&#31181;&#36328;&#24230;&#65292;&#20363;&#22914;&#36873;&#25321;&#30340;&#36328;&#24230;&#24448;&#24448;&#26159;&#21477;&#27861;&#25104;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35821;&#27861;&#30340;&#32467;&#26500;&#21270;&#36328;&#24230;&#36873;&#25321;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#21033;&#29992;&#20026;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#30340;&#37096;&#20998;&#36328;&#24230;&#32423;&#27880;&#37322;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25682;&#24323;&#20102;&#21551;&#21457;&#24335;&#30340;&#36138;&#23146;&#36328;&#24230;&#36873;&#25321;&#26041;&#26696;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#26368;&#20339;&#19968;&#32452;&#36328;&#24230;&#19978;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#36328;&#24230;&#39044;&#27979;&#20219;&#21153;&#65306;&#20849;&#25351;&#28040;&#35299;&#21644;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23454;&#35777;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many natural language processing tasks, e.g., coreference resolution and semantic role labeling, require selecting text spans and making decisions about them. A typical approach to such tasks is to score all possible spans and greedily select spans for task-specific downstream processing. This approach, however, does not incorporate any inductive bias about what sort of spans ought to be selected, e.g., that selected spans tend to be syntactic constituents. In this paper, we propose a novel grammar-based structured span selection model which learns to make use of the partial span-level annotation provided for such problems. Compared to previous approaches, our approach gets rid of the heuristic greedy span selection scheme, allowing us to model the downstream task on an optimal set of spans. We evaluate our model on two popular span prediction tasks: coreference resolution and semantic role labeling. We show empirical improvements on both.
&lt;/p&gt;</description></item><item><title>&#12298;&#28145;&#20837;&#28145;&#24230;&#23398;&#20064;&#12299;&#26159;&#19968;&#26412;&#26088;&#22312;&#20351;&#28145;&#24230;&#23398;&#20064;&#26131;&#20110;&#29702;&#35299;&#30340;&#24320;&#28304;&#20070;&#31821;&#65292;&#25552;&#20379;&#20174;&#27010;&#24565;&#21040;&#20195;&#30721;&#30340;&#25945;&#23398;&#36164;&#28304;&#65292;&#26088;&#22312;&#25104;&#20026;&#25104;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31185;&#23398;&#23478;&#30340;&#36215;&#28857;&#65292;&#24182;&#20801;&#35768;&#31038;&#21306;&#24555;&#36895;&#26356;&#26032;&#21644;&#20114;&#21160;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2106.11342</link><description>&lt;p&gt;
&#28145;&#20837;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dive into Deep Learning. (arXiv:2106.11342v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11342
&lt;/p&gt;
&lt;p&gt;
&#12298;&#28145;&#20837;&#28145;&#24230;&#23398;&#20064;&#12299;&#26159;&#19968;&#26412;&#26088;&#22312;&#20351;&#28145;&#24230;&#23398;&#20064;&#26131;&#20110;&#29702;&#35299;&#30340;&#24320;&#28304;&#20070;&#31821;&#65292;&#25552;&#20379;&#20174;&#27010;&#24565;&#21040;&#20195;&#30721;&#30340;&#25945;&#23398;&#36164;&#28304;&#65292;&#26088;&#22312;&#25104;&#20026;&#25104;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31185;&#23398;&#23478;&#30340;&#36215;&#28857;&#65292;&#24182;&#20801;&#35768;&#31038;&#21306;&#24555;&#36895;&#26356;&#26032;&#21644;&#20114;&#21160;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26412;&#24320;&#28304;&#20070;&#26159;&#25105;&#20204;&#30340;&#21162;&#21147;&#65292;&#35753;&#28145;&#24230;&#23398;&#20064;&#21464;&#24471;&#26131;&#20110;&#29702;&#35299;&#65292;&#25945;&#35835;&#32773;&#27010;&#24565;&#12289;&#32972;&#26223;&#21644;&#20195;&#30721;&#12290;&#25972;&#26412;&#20070;&#37117;&#26159;&#22312;Jupyter&#31508;&#35760;&#26412;&#20013;&#36215;&#33609;&#30340;&#65292;&#19982;&#29420;&#31435;&#30340;&#20195;&#30721;&#26080;&#32541;&#38598;&#25104;&#20102;&#35828;&#26126;&#22270;&#12289;&#25968;&#23398;&#21644;&#20114;&#21160;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#36164;&#28304;&#65292;&#26082;&#21487;&#20197;&#33258;&#30001;&#20351;&#29992;&#65292;&#21448;&#21487;&#20197;&#25552;&#20379;&#36275;&#22815;&#30340;&#25216;&#26415;&#28145;&#24230;&#65292;&#20026;&#25104;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31185;&#23398;&#23478;&#30340;&#36215;&#28857;; &#21253;&#25324;&#21487;&#36816;&#34892;&#30340;&#20195;&#30721;&#65292;&#21521;&#35835;&#32773;&#23637;&#31034;&#22914;&#20309;&#23454;&#36341;&#35299;&#20915;&#38382;&#39064;; &#20801;&#35768;&#24555;&#36895;&#26356;&#26032;&#65292;&#19981;&#20165;&#30001;&#25105;&#20204;&#65292;&#36824;&#30001;&#25972;&#20010;&#31038;&#21306;&#26356;&#26032;; &#25509;&#21463;&#25216;&#26415;&#32454;&#33410;&#30340;&#20114;&#21160;&#35752;&#35770;&#21644;&#35299;&#31572;&#38382;&#39064;&#30340;&#35770;&#22363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This open-source book represents our attempt to make deep learning approachable, teaching readers the concepts, the context, and the code. The entire book is drafted in Jupyter notebooks, seamlessly integrating exposition figures, math, and interactive examples with self-contained code. Our goal is to offer a resource that could (i) be freely available for everyone; (ii) offer sufficient technical depth to provide a starting point on the path to actually becoming an applied machine learning scientist; (iii) include runnable code, showing readers how to solve problems in practice; (iv) allow for rapid updates, both by us and also by the community at large; (v) be complemented by a forum for interactive discussion of technical details and to answer questions.
&lt;/p&gt;</description></item></channel></rss>