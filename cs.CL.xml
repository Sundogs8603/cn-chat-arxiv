<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20934;&#27979;&#35797;MQuAKE&#65292;&#36890;&#36807;&#22810;&#36339;&#38382;&#39064;&#35780;&#20272;&#32534;&#36753;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#22238;&#31572;&#22240;&#32534;&#36753;&#20107;&#23454;&#32780;&#31572;&#26696;&#24212;&#35813;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#21484;&#22238;&#24050;&#32534;&#36753;&#30340;&#20107;&#23454;&#65292;&#20294;&#22312;&#22810;&#36339;&#38382;&#39064;&#19978;&#34920;&#29616;&#28798;&#38590;&#24615;&#22833;&#36133;&#12290;</title><link>http://arxiv.org/abs/2305.14795</link><description>&lt;p&gt;
MQuAKE&#65306;&#36890;&#36807;&#22810;&#36339;&#38382;&#39064;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions. (arXiv:2305.14795v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20934;&#27979;&#35797;MQuAKE&#65292;&#36890;&#36807;&#22810;&#36339;&#38382;&#39064;&#35780;&#20272;&#32534;&#36753;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#22238;&#31572;&#22240;&#32534;&#36753;&#20107;&#23454;&#32780;&#31572;&#26696;&#24212;&#35813;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#21484;&#22238;&#24050;&#32534;&#36753;&#30340;&#20107;&#23454;&#65292;&#20294;&#22312;&#22810;&#36339;&#38382;&#39064;&#19978;&#34920;&#29616;&#28798;&#38590;&#24615;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#23384;&#20648;&#30340;&#20449;&#24687;&#24456;&#24555;&#23601;&#20250;&#36807;&#26102;&#65292;&#37325;&#26032;&#35757;&#32451;&#24182;&#38750;&#24635;&#26159;&#21487;&#34892;&#30340;&#36873;&#25321;&#12290;&#36825;&#20419;&#20351;&#20154;&#20204;&#24320;&#21457;&#20102;&#36890;&#36807;&#26356;&#26032;&#27169;&#22411;&#26435;&#37325;&#27880;&#20837;&#26032;&#20107;&#23454;&#30340;&#19968;&#31995;&#21015;&#25216;&#26415;&#12290;&#24403;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#38750;&#24120;&#26377;&#38480;&#65292;&#20027;&#35201;&#39564;&#35777;&#32534;&#36753;&#20107;&#23454;&#30340;&#21484;&#22238;&#29575;&#65292;&#20294;&#26356;&#25913;&#19968;&#20010;&#20107;&#23454;&#24212;&#35813;&#20250;&#23545;&#27169;&#22411;&#30340;&#30456;&#20851;&#20449;&#24565;&#20135;&#29983;&#36830;&#38145;&#21453;&#24212;&#12290;&#22914;&#26524;&#25105;&#20204;&#32534;&#36753;&#33521;&#22269;&#39318;&#30456;&#20026;Rishi Sunak&#65292;&#37027;&#20040;&#23545;&#20110;&#8220;&#35841;&#26159;&#33521;&#22269;&#39318;&#30456;&#30340;&#37197;&#20598;&#8221;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24212;&#35813;&#24471;&#21040;&#19968;&#20010;&#19981;&#21516;&#30340;&#31572;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;MQuAKE&#65288;&#29992;&#20110;&#30693;&#35782;&#32534;&#36753;&#30340;&#22810;&#36339;&#38382;&#31572;&#65289;&#65292;&#21253;&#25324;&#22810;&#36339;&#38382;&#39064;&#65292;&#35780;&#20272;&#32534;&#36753;&#21518;&#30340;&#27169;&#22411;&#26159;&#21542;&#27491;&#30830;&#22238;&#31572;&#37027;&#20123;&#22240;&#32534;&#36753;&#20107;&#23454;&#32780;&#31572;&#26696;&#24212;&#35813;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#21484;&#22238;&#24050;&#32534;&#36753;&#30340;&#20107;&#23454;&#65292;&#20294;&#23427;&#20204;&#22312;&#26500;&#24314;&#30340;&#22810;&#36339;&#38382;&#39064;&#19978;&#36973;&#36935;&#20102;&#28798;&#38590;&#24615;&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#23545;LLMs&#30340;&#35780;&#20272;&#24517;&#39035;&#36229;&#36234;&#31616;&#21333;&#30340;&#20107;&#23454;&#21484;&#22238;&#65292;&#24182;&#32435;&#20837;&#26356;&#24494;&#22937;&#30340;&#30693;&#35782;&#32534;&#36753;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model's related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister? In this work, we present a benchmark MQuAKE (Multi-hop Question Answering for Knowledge Editing) comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions. We thus 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#31181;&#23376;&#21305;&#37197;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#21333;&#35789;&#21024;&#38500;&#26469;&#32531;&#35299;&#22240;&#35268;&#21017;&#27880;&#20837;&#30340;&#26631;&#31614;&#20559;&#35265;&#32780;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20854;&#24615;&#33021;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.14794</link><description>&lt;p&gt;
&#30465;&#24515;&#23398;&#20064;&#21464;&#24471;&#39046;&#20808;&#65306;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#31616;&#21333;&#31181;&#23376;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification. (arXiv:2305.14794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#31181;&#23376;&#21305;&#37197;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#21333;&#35789;&#21024;&#38500;&#26469;&#32531;&#35299;&#22240;&#35268;&#21017;&#27880;&#20837;&#30340;&#26631;&#31614;&#20559;&#35265;&#32780;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20854;&#24615;&#33021;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#22797;&#26434;&#30340;&#26041;&#27861;&#65292;&#23558;&#39640;&#23618;&#27425;&#30340;&#20154;&#31867;&#21551;&#21457;&#24335;&#26041;&#27861;&#36716;&#21270;&#20026;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#31181;&#23376;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#29983;&#25104;&#20266;&#26631;&#31614;&#30340;&#26368;&#31616;&#21333;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;&#25105;&#20204;&#34920;&#26126;&#31181;&#23376;&#21305;&#37197;&#30340;&#26377;&#38480;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#31181;&#23376;&#21305;&#37197;&#35268;&#21017;&#27880;&#20837;&#30340;&#26631;&#31614;&#20559;&#24046;&#65292;&#36825;&#20250;&#38459;&#27490;&#20998;&#31867;&#22120;&#23398;&#20064;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#31616;&#21333;&#22320;&#21024;&#38500;&#21305;&#37197;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#31181;&#23376;&#35789;&#21487;&#20197;&#32531;&#35299;&#26631;&#31614;&#20559;&#24046;&#24182;&#24110;&#21161;&#23398;&#20064;&#26356;&#22909;&#30340;&#32622;&#20449;&#24230;&#12290;&#38543;&#21518;&#65292;&#31181;&#23376;&#21305;&#37197;&#30340;&#24615;&#33021;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#65292;&#20351;&#23427;&#36798;&#21040;&#25110;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22788;&#29702;&#31181;&#23376;&#35789;&#19981;&#20026;&#20154;&#30693;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24314;&#35758;&#31616;&#21333;&#22320;&#21024;&#38500;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#21333;&#35789;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in weakly supervised text classification mostly focus on designing sophisticated methods to turn high-level human heuristics into quality pseudo-labels. In this paper, we revisit the seed matching-based method, which is arguably the simplest way to generate pseudo-labels, and show that its power was greatly underestimated. We show that the limited performance of seed matching is largely due to the label bias injected by the simple seed-match rule, which prevents the classifier from learning reliable confidence for selecting high-quality pseudo-labels. Interestingly, simply deleting the seed words present in the matched input texts can mitigate the label bias and help learn better confidence. Subsequently, the performance achieved by seed matching can be improved significantly, making it on par with or even better than the state-of-the-art. Furthermore, to handle the case when the seed words are not made known, we propose to simply delete the word tokens in the input tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#24490;&#29615;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#23569;&#37327;&#30417;&#30563;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#25991;&#26412;&#20219;&#21153;&#19982;&#20840;&#30417;&#30563;&#26041;&#27861;&#30456;&#36817;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#38750;&#22495;&#25968;&#25454;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14793</link><description>&lt;p&gt;
&#22522;&#20110;&#24490;&#29615;&#35757;&#32451;&#30340;&#20302;&#36164;&#28304;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#26041;&#27861;&#26469;&#33258;&#20110;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Faithful Low-Resource Data-to-Text Generation through Cycle Training. (arXiv:2305.14793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#24490;&#29615;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#23569;&#37327;&#30417;&#30563;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#25991;&#26412;&#20219;&#21153;&#19982;&#20840;&#30417;&#30563;&#26041;&#27861;&#30456;&#36817;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#38750;&#22495;&#25968;&#25454;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20027;&#35201;&#26159;&#36890;&#36807;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#19978;&#21487;&#33021;&#26080;&#27861;&#20135;&#29983;&#19982;&#36755;&#20837;&#25968;&#25454;&#30456;&#31526;&#30340;&#36755;&#20986;&#25991;&#26412;&#65292;&#23588;&#20854;&#26159;&#22312;&#22495;&#22806;&#25968;&#25454;&#19978;&#12290;&#30001;&#20110;&#32570;&#23569;&#29305;&#23450;&#39046;&#22495;&#30340;&#36275;&#22815;&#27880;&#37322;&#25968;&#25454;&#65292;&#22240;&#27492;&#25105;&#20204;&#23547;&#27714;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#36755;&#20986;&#25991;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#36890;&#36807;&#24490;&#29615;&#35757;&#32451;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20010;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#25991;&#26412;&#20043;&#38388;&#34920;&#31034;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#24490;&#29615;&#35757;&#32451;&#20351;&#29992;&#20004;&#20010;&#20114;&#20026;&#21453;&#20989;&#25968;&#30340;&#27169;&#22411;&#65306;&#19968;&#20010;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#65292;&#21478;&#19968;&#20010;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#29983;&#25104;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#23569;&#37327;&#30417;&#30563;&#25968;&#25454;&#65288;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;100&#20010;&#26679;&#26412;&#65289;&#30340;&#24773;&#20917;&#19979;&#21021;&#22987;&#21270;&#30340;&#24490;&#29615;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#20219;&#21153;&#19982;&#20840;&#30417;&#30563;&#26041;&#27861;&#30456;&#36817;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#38750;&#22495;&#25968;&#25454;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods to generate text from structured data have advanced significantly in recent years, primarily due to fine-tuning of pre-trained language models on large datasets. However, such models can fail to produce output faithful to the input data, particularly on out-of-domain data. Sufficient annotated data is often not available for specific domains, leading us to seek an unsupervised approach to improve the faithfulness of output text. Since the problem is fundamentally one of consistency between the representations of the structured data and text, we evaluate the effectiveness of cycle training in this work. Cycle training uses two models which are inverses of each other: one that generates text from structured data, and one which generates the structured data from natural language text. We show that cycle training, when initialized with a small amount of supervised data (100 samples in our case), achieves nearly the same performance as fully supervised approaches for the data-to-tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#21453;&#20107;&#23454;&#29983;&#25104;&#22120;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#23454;&#39564;&#21457;&#29616;&#23427;&#20204;&#22312;&#21508;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#20173;&#23384;&#22312;&#33258;&#25105;&#38480;&#21046;&#21644;&#32570;&#20047;&#36923;&#36753;&#25351;&#23548;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14791</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21453;&#20107;&#23454;&#29983;&#25104;&#22120;: &#20248;&#21183;&#21644;&#21155;&#21183;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Counterfactual Generator: Strengths and Weaknesses. (arXiv:2305.14791v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#21453;&#20107;&#23454;&#29983;&#25104;&#22120;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#23454;&#39564;&#21457;&#29616;&#23427;&#20204;&#22312;&#21508;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#20173;&#23384;&#22312;&#33258;&#25105;&#38480;&#21046;&#21644;&#32570;&#20047;&#36923;&#36753;&#25351;&#23548;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#29983;&#25104;&#21453;&#20107;&#23454;&#30340;&#33021;&#21147;&#65292;&#22914;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#21453;&#20107;&#23454;&#65292;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;LLMs&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#20998;&#26512;&#24433;&#21709;&#36825;&#31181;&#33021;&#21147;&#30340;&#22240;&#32032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#24773;&#24863;&#20998;&#26512;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#31561;&#22235;&#20010;&#20219;&#21153;&#20013;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#23454;&#39564;&#26469;&#35780;&#20272;LLMs&#22312;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#34429;&#28982;LLMs&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#37117;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#25913;&#36827;&#65292;&#20294;&#30001;&#20110;&#20854;&#33258;&#25105;&#38480;&#21046;&#21644;&#32570;&#20047;&#19982;&#24120;&#35782;&#30456;&#31526;&#30340;&#36923;&#36753;&#25351;&#23548;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#23427;&#20204;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20026;LLMs&#25552;&#20379;&#20934;&#30830;&#30340;&#20219;&#21153;&#23450;&#20041;&#21644;&#35814;&#32454;&#30340;&#36880;&#27493;&#25351;&#23548;&#22312;&#29983;&#25104;&#26377;&#25928;&#30340;&#21453;&#20107;&#23454;&#20013;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable performance in a range of natural language understanding and generation tasks. Yet, their ability to generate counterfactuals, which can be used for areas like data augmentation, remains under-explored. This study aims to investigate the counterfactual generation capabilities of LLMs and analysis factors that influence this ability. First, we evaluate how effective are LLMs in counterfactual generation through data augmentation experiments for small language models (SLMs) across four tasks: sentiment analysis, natural language inference, named entity recognition, and relation extraction. While LLMs show promising enhancements in various settings, they struggle in complex tasks due to their self-limitations and the lack of logical guidance to produce counterfactuals that align with commonsense. Second, our analysis reveals the pivotal role of providing accurate task definitions and detailed step-by-step instructions to LLMs in ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30340;&#27573;&#33853;&#32423;&#20013;&#25991;&#20027;&#39064;&#32467;&#26500;&#34920;&#31034;&#65292;&#20351;&#29992;&#21477;&#23376;&#32780;&#19981;&#26159;&#20851;&#38190;&#35789;&#26469;&#34920;&#31034;&#23376;&#20027;&#39064;&#65292;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#27573;&#33853;&#32423;&#20027;&#39064;&#32467;&#26500;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2305.14790</link><description>&lt;p&gt;
&#25552;&#21319;&#20013;&#25991;&#25991;&#26412;&#20027;&#39064;&#21010;&#20998;&#21644;&#32434;&#35201;&#29983;&#25104;&#65306;&#27573;&#33853;&#32423;&#20027;&#39064;&#34920;&#31034;&#65292;&#35821;&#26009;&#24211;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Advancing Topic Segmentation and Outline Generation in Chinese Texts: The Paragraph-level Topic Representation, Corpus, and Benchmark. (arXiv:2305.14790v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30340;&#27573;&#33853;&#32423;&#20013;&#25991;&#20027;&#39064;&#32467;&#26500;&#34920;&#31034;&#65292;&#20351;&#29992;&#21477;&#23376;&#32780;&#19981;&#26159;&#20851;&#38190;&#35789;&#26469;&#34920;&#31034;&#23376;&#20027;&#39064;&#65292;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#27573;&#33853;&#32423;&#20027;&#39064;&#32467;&#26500;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#21010;&#20998;&#21644;&#32434;&#35201;&#29983;&#25104;&#26088;&#22312;&#23558;&#19968;&#20010;&#25991;&#26723;&#20998;&#25104;&#36830;&#36143;&#30340;&#20027;&#39064;&#27573;&#33853;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#23376;&#26631;&#39064;&#12290;&#36825;&#20010;&#36807;&#31243;&#25581;&#31034;&#20102;&#19968;&#20010;&#25991;&#26723;&#30340;&#35805;&#39064;&#32467;&#26500;&#65292;&#26377;&#21161;&#20110;&#20174;&#26356;&#39640;&#30340;&#23618;&#27425;&#24555;&#36895;&#25226;&#25569;&#21644;&#29702;&#35299;&#25991;&#26723;&#30340;&#25972;&#20307;&#24773;&#22659;&#12290;&#28982;&#32780;&#65292;&#19982;&#33521;&#35821;&#39046;&#22495;&#21462;&#24471;&#30340;&#25104;&#21151;&#30456;&#27604;&#65292;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#27573;&#33853;&#32423;&#20027;&#39064;&#34920;&#31034;&#21644;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#35821;&#26009;&#24211;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#23618;&#30340;&#27573;&#33853;&#32423;&#20027;&#39064;&#32467;&#26500;&#34920;&#31034;&#65292;&#21253;&#25324;&#26631;&#39064;&#12289;&#23376;&#26631;&#39064;&#21644;&#27573;&#33853;&#65292;&#32508;&#21512;&#22320;&#27169;&#25311;&#20102;&#25991;&#26723;&#30340;&#35805;&#39064;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#21477;&#23376;&#32780;&#19981;&#26159;&#20851;&#38190;&#35789;&#26469;&#34920;&#31034;&#23376;&#20027;&#39064;&#65292;&#30830;&#20445;&#26356;&#20840;&#38754;&#22320;&#34920;&#31034;&#25991;&#26723;&#20869;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#26681;&#25454;&#36825;&#31181;&#34920;&#31034;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#26368;&#22823;&#30340;&#20013;&#25991;&#27573;&#33853;&#32423;&#20027;&#39064;&#32467;&#26500;&#35821;&#26009;&#24211;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic segmentation and outline generation strive to divide a document into coherent topic sections and generate corresponding subheadings. Such a process unveils the discourse topic structure of a document that benefits quickly grasping and understanding the overall context of the document from a higher level. However, research and applications in this field have been restrained due to the lack of proper paragraph-level topic representations and large-scale, high-quality corpora in Chinese compared to the success achieved in English. Addressing these issues, we introduce a hierarchical paragraph-level topic structure representation with title, subheading, and paragraph that comprehensively models the document discourse topic structure. In addition, we ensure a more holistic representation of topic distribution within the document by using sentences instead of keywords to represent sub-topics. Following this representation, we construct the largest Chinese Paragraph-level Topic Structur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20026;&#33258;&#21160;&#21387;&#32553;&#22120;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#38271;&#31687;&#25991;&#26412;&#21387;&#32553;&#25104;&#32039;&#20945;&#30340;&#25688;&#35201;&#21521;&#37327;&#65292;&#25552;&#39640;&#19978;&#19979;&#25991;&#30340;&#21033;&#29992;&#25928;&#29575;&#21644;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#31934;&#24230;&#24182;&#38477;&#20302;&#25512;&#26029;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.14788</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20026;&#33258;&#21160;&#21387;&#32553;&#22120;&#20197;&#25552;&#39640;&#27169;&#22411;&#19978;&#19979;&#25991;&#30340;&#21033;&#29992;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Adapting Language Models to Compress Contexts. (arXiv:2305.14788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20026;&#33258;&#21160;&#21387;&#32553;&#22120;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#38271;&#31687;&#25991;&#26412;&#21387;&#32553;&#25104;&#32039;&#20945;&#30340;&#25688;&#35201;&#21521;&#37327;&#65292;&#25552;&#39640;&#19978;&#19979;&#25991;&#30340;&#21033;&#29992;&#25928;&#29575;&#21644;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#31934;&#24230;&#24182;&#38477;&#20302;&#25512;&#26029;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21151;&#33021;&#24378;&#22823;&#19988;&#24191;&#27867;&#24212;&#29992;&#30340;&#24037;&#20855;&#65292;&#20294;&#20854;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#32422;&#26463;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20026;&#33258;&#21160;&#21387;&#32553;&#22120;&#65292;&#33021;&#22815;&#23558;&#38271;&#31687;&#25991;&#26412;&#21387;&#32553;&#25104;&#32039;&#20945;&#30340;&#25688;&#35201;&#21521;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#19978;&#19979;&#25991;&#30340;&#21033;&#29992;&#25928;&#29575;&#21644;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#25688;&#35201;&#21521;&#37327;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20316;&#20026;&#36719;&#25552;&#31034;&#34987;&#27169;&#22411;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments and summary vectors from all previous segments are used in language modeling. We fine-tune OPT models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations. We find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference cost. Finally, we explore the benefits of pre-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#35821;&#35328;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#20854;&#22312;&#31616;&#21333;&#35821;&#35328;&#25512;&#26029;&#20219;&#21153;&#20013;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21892;&#20854;&#23545;&#22522;&#26412;&#35821;&#35328;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.14785</link><description>&lt;p&gt;
ChatGPT&#21644;&#31616;&#21333;&#30340;&#35821;&#35328;&#25512;&#26029;&#65306;&#30450;&#28857;&#21644;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and Simple Linguistic Inferences: Blind Spots and Blinds. (arXiv:2305.14785v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#35821;&#35328;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#20854;&#22312;&#31616;&#21333;&#35821;&#35328;&#25512;&#26029;&#20219;&#21153;&#20013;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21892;&#20854;&#23545;&#22522;&#26412;&#35821;&#35328;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#29702;&#35299;&#33021;&#21147;&#38480;&#21046;&#65292;&#38024;&#23545;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#36890;&#24120;&#31616;&#21333;&#30340;&#25512;&#26029;&#20219;&#21153;&#65292;&#20294;&#36825;&#20123;&#20284;&#20046;&#23545;&#35813;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#38024;&#23545;(i)&#35821;&#27861;&#35268;&#23450;&#30340;&#34164;&#21547;&#65292;(ii)&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#35777;&#25454;&#21103;&#35789;&#30340;&#21069;&#25552;&#65292;&#20197;&#21450;(iii)&#21333;&#35843;&#34164;&#21547;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#20026;&#36825;&#20123;&#25512;&#29702;&#31867;&#22411;&#25552;&#20379;&#20102;&#19987;&#23478;&#35774;&#35745;&#30340;&#35780;&#20272;&#38598;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#36825;&#20123;&#25512;&#29702;&#31867;&#22411;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23637;&#31034;&#20013;&#31561;&#21040;&#20302;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;ChatGPT&#22312;&#30452;&#25509;&#25552;&#31034;&#19979;&#34920;&#29616;&#20986;&#23545;&#24213;&#23618;&#35821;&#35328;&#27010;&#24565;&#30340;&#20102;&#35299;&#65292;&#20294;&#23427;&#32463;&#24120;&#19981;&#33021;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#20316;&#20986;&#27491;&#30830;&#30340;&#25512;&#26029;&#12290;&#26356;&#26377;&#36259;&#30340;&#26159;&#65292;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#21069;&#25552;&#23884;&#20837;&#21069;&#25552;&#26465;&#20214;&#35302;&#21457;&#25110;&#38750;&#23454;&#38469;&#24615;&#21160;&#35789;&#20250;&#23548;&#33268;&#27169;&#22411;&#26356;&#39057;&#32321;&#22320;&#39044;&#27979;&#34164;&#21547;&#65292;&#32780;&#19981;&#32771;&#34385;&#27491;&#30830;&#30340;&#35821;&#20041;&#26631;&#31614;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#32487;&#32493;&#25913;&#21892;&#23427;&#20204;&#23545;&#22522;&#26412;&#35821;&#35328;&#27010;&#24565;&#30340;&#29702;&#35299;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper sheds light on the limitations of ChatGPT's understanding capabilities, focusing on simple inference tasks that are typically easy for humans but appear to be challenging for the model. Specifically, we target (i) grammatically-specified entailments, (ii) premises with evidential adverbs of uncertainty, and (iii) monotonicity entailments. We present expert-designed evaluation sets for these inference types and conduct experiments in a zero-shot setup. Our results show that the model struggles with these types of inferences, exhibiting moderate to low accuracy. Moreover, while ChatGPT demonstrates knowledge of the underlying linguistic concepts when prompted directly, it often fails to incorporate this knowledge to make correct inferences. Even more strikingly, further experiments show that embedding the premise under presupposition triggers or non-factive verbs causes the model to predict entailment more frequently {regardless} of the correct semantic label. Overall these re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#24320;&#20013;&#25991;&#25991;&#26412;&#20013;&#30340;&#35821;&#38899;&#23398;&#20449;&#24687;&#21644;&#23383;&#31526;&#20449;&#24687;&#65292;&#23454;&#29616;&#20004;&#32773;&#20043;&#38388;&#30340;&#30452;&#25509;&#20132;&#20114;&#65292;&#26377;&#25928;&#21033;&#29992;&#35821;&#38899;&#23398;&#20449;&#24687;&#25552;&#39640;&#20013;&#25991;&#25340;&#20889;&#32416;&#38169;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14783</link><description>&lt;p&gt;
&#20013;&#25991;&#25340;&#20889;&#32416;&#38169;&#20013;&#30340;&#35821;&#38899;&#23398;&#34920;&#31034;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Disentangled Phonetic Representation for Chinese Spelling Correction. (arXiv:2305.14783v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#24320;&#20013;&#25991;&#25991;&#26412;&#20013;&#30340;&#35821;&#38899;&#23398;&#20449;&#24687;&#21644;&#23383;&#31526;&#20449;&#24687;&#65292;&#23454;&#29616;&#20004;&#32773;&#20043;&#38388;&#30340;&#30452;&#25509;&#20132;&#20114;&#65292;&#26377;&#25928;&#21033;&#29992;&#35821;&#38899;&#23398;&#20449;&#24687;&#25552;&#39640;&#20013;&#25991;&#25340;&#20889;&#32416;&#38169;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#25340;&#20889;&#32416;&#38169;&#26088;&#22312;&#26816;&#27979;&#21644;&#32416;&#27491;&#20013;&#25991;&#25991;&#26412;&#20013;&#30340;&#38169;&#35823;&#23383;&#31526;&#12290;&#34429;&#28982;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#24341;&#20837;&#20102;&#35821;&#38899;&#23398;&#20449;&#24687;&#65288;&#27721;&#35821;&#25340;&#38899;&#65289;&#65292;&#20294;&#36890;&#24120;&#20250;&#23558;&#35821;&#38899;&#23398;&#34920;&#31034;&#19982;&#23383;&#31526;&#34920;&#31034;&#21512;&#24182;&#65292;&#36825;&#24448;&#24448;&#20250;&#21066;&#24369;&#27491;&#24120;&#25991;&#26412;&#30340;&#34920;&#31034;&#25928;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20004;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#35299;&#24320;&#20197;&#20801;&#35768;&#25991;&#26412;&#21644;&#35821;&#38899;&#20449;&#24687;&#20043;&#38388;&#30340;&#30452;&#25509;&#20132;&#20114;&#12290;&#20026;&#20102;&#23398;&#20064;&#26377;&#29992;&#30340;&#35821;&#38899;&#23398;&#34920;&#31034;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25340;&#38899;&#21040;&#23383;&#31526;&#30340;&#30446;&#26631;&#65292;&#35201;&#27714;&#27169;&#22411;&#20165;&#22522;&#20110;&#35821;&#38899;&#20449;&#24687;&#39044;&#27979;&#27491;&#30830;&#30340;&#23383;&#31526;&#65292;&#20854;&#20013;&#26045;&#21152;&#20102;&#19968;&#20010;&#20998;&#31163;&#25513;&#30721;&#20197;&#31105;&#29992;&#20174;&#35821;&#38899;&#36755;&#20837;&#21040;&#25991;&#26412;&#30340;&#27880;&#24847;&#21147;&#12290;&#20026;&#20102;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#35821;&#38899;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#25105;&#33976;&#39311;&#27169;&#22359;&#65292;&#20197;&#30830;&#20445;&#35821;&#20041;&#20449;&#24687;&#22312;&#39044;&#27979;&#20013;&#36215;&#20027;&#23548;&#20316;&#29992;&#12290;&#22312;&#19977;&#20010;&#20013;&#25991;&#25340;&#20889;&#32416;&#38169;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#35821;&#38899;&#23398;&#20449;&#24687;&#36827;&#34892;&#20013;&#25991;&#25340;&#20889;&#32416;&#38169;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chinese Spelling Correction (CSC) aims to detect and correct erroneous characters in Chinese texts. Although efforts have been made to introduce phonetic information (Hanyu Pinyin) in this task, they typically merge phonetic representations with character representations, which tends to weaken the representation effect of normal texts. In this work, we propose to disentangle the two types of features to allow for direct interaction between textual and phonetic information. To learn useful phonetic representations, we introduce a pinyin-to-character objective to ask the model to predict the correct characters based solely on phonetic information, where a separation mask is imposed to disable attention from phonetic input to text. To avoid overfitting the phonetics, we further design a self-distillation module to ensure that semantic information plays a major role in the prediction. Extensive experiments on three CSC benchmarks demonstrate the superiority of our method in using phonetic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;Twitter&#19978;&#20998;&#20139;&#30340;&#22270;&#20687;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;CLIP&#21069;&#32512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#22270;&#20687;&#21644;&#25512;&#25991;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#29983;&#25104;&#20851;&#20110;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26367;&#20195;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.14779</link><description>&lt;p&gt;
Twitter&#22270;&#20687;&#30340;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text Conditional Alt-Text Generation for Twitter Images. (arXiv:2305.14779v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;Twitter&#19978;&#20998;&#20139;&#30340;&#22270;&#20687;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;CLIP&#21069;&#32512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#22270;&#20687;&#21644;&#25512;&#25991;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#29983;&#25104;&#20851;&#20110;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26367;&#20195;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31038;&#20132;&#23186;&#20307;&#29305;&#21035;&#26159;Twitter&#19978;&#20998;&#20139;&#30340;&#22270;&#20687;&#29983;&#25104;&#26367;&#20195;&#25991;&#26412;&#65288;&#25110;alt-text&#65289;&#25551;&#36848;&#30340;&#26041;&#27861;&#12290;&#19982;&#22270;&#20687;&#30340;&#23383;&#24149;&#19981;&#21516;&#65292;&#25991;&#26412;&#26367;&#25442;&#25991;&#26412;&#26356;&#21152;&#30452;&#30333;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#29305;&#23450;&#12290;&#27492;&#22806;&#65292;&#20851;&#38190;&#26159;&#65292;&#21457;&#24067;&#21040;Twitter&#19978;&#30340;&#22270;&#20687;&#36890;&#24120;&#26159;&#30001;&#29992;&#25143;&#32534;&#20889;&#30340;&#25991;&#26412;&#38468;&#21152;&#30340;&#65292;&#23613;&#31649;&#36825;&#20123;&#25991;&#26412;&#19981;&#19968;&#23450;&#25551;&#36848;&#22270;&#20687;&#65292;&#20294;&#21487;&#33021;&#25552;&#20379;&#26377;&#29992;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#22914;&#26524;&#27491;&#30830;&#21033;&#29992;&#21487;&#20197;&#25552;&#20379;&#20449;&#24687;&#65292;&#20363;&#22914;&#25512;&#25991;&#21487;&#33021;&#20250;&#21629;&#21517;&#22270;&#29255;&#20013;&#27169;&#22411;&#20043;&#21069;&#27809;&#26377;&#35265;&#36807;&#30340;&#19981;&#24120;&#35265;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;CLIP&#21069;&#32512;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#25552;&#21462;&#22270;&#20687;&#30340;&#23884;&#20837;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#26144;&#23556;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36755;&#20986;&#21333;&#35789;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#30701;&#24207;&#21015;&#65292;&#25110;&#31216;&#20026;&#8220;&#21069;&#32512;&#8221;&#65292;&#25105;&#20204;&#23558;&#25512;&#25991;&#26412;&#36523;&#30340;&#25991;&#26412;&#20063;&#36830;&#25509;&#21040;&#20854;&#20013;&#12290;&#36825;&#26679;&#65292;&#27169;&#22411;&#23601;&#21487;&#20197;&#22312;&#25991;&#31456;&#20013;&#26465;&#20214;&#21270;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#28982;&#21518;&#23558;&#21512;&#24182;&#30340;&#22810;&#27169;&#24335;&#21069;&#32512;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we present an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter. This task is more than just a special case of image captioning, as alt-text is both more literally descriptive and context-specific. Also critically, images posted to Twitter are often accompanied by user-written text that despite not necessarily describing the image may provide useful context that if properly leveraged can be informative -- e.g. the tweet may name an uncommon object in the image that the model has not previously seen. We address this with a CLIP prefix model that extracts an embedding of the image and passes it to a mapping network that outputs a short sequence in word embedding space, or a ``prefix'', to which we also concatenate the text from the tweet itself. This lets the model condition on both visual and textual information from the post. The combined multimodal prefix is then fed as a prompt to a pretrained lang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;PLMs&#20013;&#21442;&#25968;&#21270;&#30693;&#35782;&#30340;&#21033;&#29992;&#65292;&#30740;&#31350;&#21457;&#29616;PLMs&#23384;&#22312;&#24050;&#33719;&#21462;&#30340;&#30693;&#35782;&#21644;&#21033;&#29992;&#30340;&#30693;&#35782;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#26377;&#38480;&#30340;&#40065;&#26834;&#24615;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#21487;&#20197;&#24357;&#34917;&#24050;&#33719;&#21462;&#30693;&#35782;&#30340;&#24046;&#36317;&#65292;&#20294;&#21033;&#29992;&#30693;&#35782;&#30340;&#24046;&#36317;&#20173;&#28982;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2305.14775</link><description>&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#37327;&#30693;&#35782;&#33719;&#21462;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models. (arXiv:2305.14775v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;PLMs&#20013;&#21442;&#25968;&#21270;&#30693;&#35782;&#30340;&#21033;&#29992;&#65292;&#30740;&#31350;&#21457;&#29616;PLMs&#23384;&#22312;&#24050;&#33719;&#21462;&#30340;&#30693;&#35782;&#21644;&#21033;&#29992;&#30340;&#30693;&#35782;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#26377;&#38480;&#30340;&#40065;&#26834;&#24615;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#21487;&#20197;&#24357;&#34917;&#24050;&#33719;&#21462;&#30693;&#35782;&#30340;&#24046;&#36317;&#65292;&#20294;&#21033;&#29992;&#30693;&#35782;&#30340;&#24046;&#36317;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#33719;&#21462;&#20102;&#22823;&#37327;&#30340;&#30693;&#35782;&#65292;&#20294;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#36825;&#20123;&#21442;&#25968;&#21270;&#30693;&#35782;&#20013;&#26377;&#22810;&#23569;&#23454;&#38469;&#21487;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;PLMs&#20013;&#21442;&#25968;&#21270;&#30693;&#35782;&#30340;&#21033;&#29992;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#39318;&#20808;&#20174;PLM&#21442;&#25968;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#38543;&#21518;&#22260;&#32469;&#36825;&#20123;&#25552;&#21462;&#30340;&#30693;&#35782;&#26500;&#24314;&#19979;&#28216;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#23436;&#20840;&#20381;&#36182;&#20110;&#21033;&#29992;&#27169;&#22411;&#25152;&#20855;&#22791;&#30340;&#30693;&#35782;&#65292;&#36991;&#20813;&#20102;&#19981;&#20805;&#20998;&#30340;&#20449;&#21495;&#31561;&#28151;&#28102;&#22240;&#32032;&#12290;&#20316;&#20026;&#19968;&#20010;&#31034;&#20363;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;PLMs&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#27979;&#37327;&#20102;125M&#21040;13B&#21442;&#25968;PLMs&#30340;&#21033;&#29992;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65306;&#65288;1&#65289;PLMs&#22312;&#24050;&#33719;&#21462;&#30340;&#30693;&#35782;&#21644;&#21033;&#29992;&#30340;&#30693;&#35782;&#20043;&#38388;&#23384;&#22312;&#20004;&#20010;&#24046;&#36317;&#65292;&#65288;2&#65289;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#65292;&#23427;&#20204;&#22312;&#21033;&#29992;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#40065;&#26834;&#24615;&#65292;&#65288;3&#65289;&#36739;&#22823;&#30340;&#27169;&#22411;&#21487;&#20197;&#24357;&#34917;&#24050;&#33719;&#21462;&#30693;&#35782;&#30340;&#24046;&#36317;&#65292;&#20294;&#21033;&#29992;&#30693;&#35782;&#30340;&#24046;&#36317;&#20173;&#28982;&#23384;&#22312;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#24403;&#21069;PLMs&#22312;&#21033;&#29992;&#24050;&#33719;&#21462;&#30693;&#35782;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
While pre-trained language models (PLMs) have shown evidence of acquiring vast amounts of knowledge, it remains unclear how much of this parametric knowledge is actually usable in performing downstream tasks. We propose a systematic framework to measure parametric knowledge utilization in PLMs. Our framework first extracts knowledge from a PLM's parameters and subsequently constructs a downstream task around this extracted knowledge. Performance on this task thus depends exclusively on utilizing the model's possessed knowledge, avoiding confounding factors like insufficient signal. As an instantiation, we study factual knowledge of PLMs and measure utilization across 125M to 13B parameter PLMs. We observe that: (1) PLMs exhibit two gaps in acquired vs. utilized knowledge, (2) they show limited robustness in utilizing knowledge under distribution shifts, and (3) larger models close the acquired knowledge gap but the utilized knowledge gap remains. Overall, our study provides insights 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38382;&#31572;&#30340;&#21435;&#25991;&#26412;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23637;&#31034;&#25552;&#21462;&#30340;&#25991;&#26412;&#25688;&#24405;&#12290;&#22312;&#38382;&#31572;&#21644;&#24341;&#35777;&#19978;&#30340;&#34920;&#29616;&#31867;&#20284;&#20110;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#21450;&#20559;&#22909;&#30340;&#21487;&#25511;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14772</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25511;&#30340;&#22522;&#20110;&#38382;&#31572;&#30340;&#21435;&#25991;&#26412;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Controllable QA-based Framework for Decontextualization. (arXiv:2305.14772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38382;&#31572;&#30340;&#21435;&#25991;&#26412;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23637;&#31034;&#25552;&#21462;&#30340;&#25991;&#26412;&#25688;&#24405;&#12290;&#22312;&#38382;&#31572;&#21644;&#24341;&#35777;&#19978;&#30340;&#34920;&#29616;&#31867;&#20284;&#20110;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#21450;&#20559;&#22909;&#30340;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#38656;&#35201;&#23558;&#25552;&#21462;&#30340;&#25688;&#24405;&#23637;&#31034;&#32473;&#29992;&#25143;&#65292;&#36825;&#20123;&#25688;&#24405;&#24448;&#24448;&#38656;&#35201;&#35299;&#32806;&#21407;&#26469;&#30340;&#25991;&#26412;&#25165;&#33021;&#26356;&#22909;&#22320;&#21576;&#29616;&#32473;&#29992;&#25143;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#38382;&#31572;&#21644;&#24341;&#35777;&#19978;&#30340;&#21435;&#25991;&#26412;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38382;&#31572;&#30340;&#21435;&#25991;&#26412;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26356;&#22909;&#22320;&#28385;&#36275;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#21450;&#20559;&#22909;&#65292;&#24182;&#19988;&#22312;&#32467;&#26524;&#19978;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;&#31471;&#21040;&#31471;&#26041;&#27861;&#30340;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#21516;&#26102;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#35813;&#26694;&#26550;&#23558;&#29992;&#25143;&#20559;&#22909;&#34701;&#20837;&#21040;&#31995;&#32479;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world applications require surfacing extracted snippets to users, whether motivated by assistive tools for literature surveys or document cross-referencing, or needs to mitigate and recover from model generated inaccuracies., Yet, these passages can be difficult to consume when divorced from their original document context. In this work, we explore the limits of LLMs to perform decontextualization of document snippets in user-facing scenarios, focusing on two real-world settings - question answering and citation context previews for scientific documents. We propose a question-answering framework for decontextualization that allows for better handling of user information needs and preferences when determining the scope of rewriting. We present results showing state-of-the-art LLMs under our framework remain competitive with end-to-end approaches. We also explore incorporating user preferences into the system, finding our framework allows for controllability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;SSD-2&#65292;&#23427;&#21487;&#20197;&#20174;0.4B&#25193;&#23637;&#21040;13B&#21442;&#25968;&#65292;&#24182;&#32463;&#36807;&#24494;&#35843;&#26469;&#36981;&#24490;&#25351;&#20196;&#12290;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#20351;&#29992;SSD-2&#21487;&#20197;&#24418;&#25104;&#26356;&#26377;&#25928;&#30340;&#27169;&#22411;&#21512;&#20316;&#65292;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14771</link><description>&lt;p&gt;
SSD-2&#65306;&#25193;&#23637;&#21644;&#25512;&#29702;&#26102;&#38388;&#34701;&#21512;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SSD-2: Scaling and Inference-time Fusion of Diffusion Language Models. (arXiv:2305.14771v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;SSD-2&#65292;&#23427;&#21487;&#20197;&#20174;0.4B&#25193;&#23637;&#21040;13B&#21442;&#25968;&#65292;&#24182;&#32463;&#36807;&#24494;&#35843;&#26469;&#36981;&#24490;&#25351;&#20196;&#12290;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#20351;&#29992;SSD-2&#21487;&#20197;&#24418;&#25104;&#26356;&#26377;&#25928;&#30340;&#27169;&#22411;&#21512;&#20316;&#65292;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#35328;&#27169;&#22411;&#34987;&#35777;&#26126;&#26159;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26131;&#20110;&#22312;&#25512;&#29702;&#26102;&#36827;&#34892;&#25511;&#21046;&#65292;&#24182;&#19988;&#26159;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#20165;&#22312;&#30456;&#23545;&#36739;&#23567;&#30340;&#35268;&#27169;&#19979;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#26368;&#36817;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;SSD-LM&#30340;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#23558;&#20854;&#20174;0.4B&#25193;&#23637;&#21040;13B&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#25913;&#36827;&#20854;&#35757;&#32451;&#21644;&#25512;&#29702;&#25928;&#29575;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#21629;&#21517;&#36825;&#20010;&#26032;&#27169;&#22411;&#20026;SSD-2&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36890;&#36807;&#24494;&#35843;&#26469;&#36981;&#24490;&#25351;&#20196;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SSD-2&#21487;&#20197;&#19982;100&#20493;&#26356;&#23567;&#30340;&#27169;&#22411;&#21512;&#20316;&#24418;&#25104;&#26032;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#30001;&#20010;&#20154;&#29992;&#25143;&#36827;&#34892;&#23450;&#21046;&#21644;&#37096;&#32626;&#12290;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#26356;&#21152;&#26377;&#25928;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based language models (LMs) have been shown to be competent generative models that are easy to control at inference and are a promising alternative to autoregressive LMs. While autoregressive LMs have benefited immensely from scaling and instruction-based learning, existing studies on diffusion LMs have been conducted on a relatively smaller scale. Starting with a recently proposed diffusion model SSD-LM, in this work we explore methods to scale it from 0.4B to 13B parameters, proposing several techniques to improve its training and inference efficiency. We call the new model SSD-2. We further show that this model can be easily finetuned to follow instructions. Finally, leveraging diffusion models' capability at inference-time control, we show that SSD-2 facilitates novel ensembles with 100x smaller models that can be customized and deployed by individual users. We find that compared to autoregressive models, the collaboration between diffusion models is more effective, leadi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#35843;&#25972;&#26631;&#27880;&#32773;&#20043;&#38388;&#23384;&#22312;&#30340;&#23610;&#24230;&#19981;&#19968;&#33268;&#65292;&#35299;&#20915;&#20102;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#26631;&#27880;&#32773;&#20043;&#38388;&#20998;&#27495;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14770</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#37325;&#26032;&#35843;&#25972;&#20154;&#31867;&#35780;&#20215;
&lt;/p&gt;
&lt;p&gt;
Using Natural Language Explanations to Rescale Human Judgments. (arXiv:2305.14770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#35843;&#25972;&#26631;&#27880;&#32773;&#20043;&#38388;&#23384;&#22312;&#30340;&#23610;&#24230;&#19981;&#19968;&#33268;&#65292;&#35299;&#20915;&#20102;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#26631;&#27880;&#32773;&#20043;&#38388;&#20998;&#27495;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#24102;&#26469;&#20102;&#38656;&#35201;&#39640;&#36136;&#37327;&#20154;&#26631;&#35760;&#25968;&#25454;&#30340;&#32039;&#36843;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20154;&#30340;&#21453;&#39304;&#21644;&#35780;&#20272;&#31561;&#36807;&#31243;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#36890;&#36807;&#22810;&#20010;&#20247;&#21253;&#24037;&#20316;&#32773;&#30340;&#20849;&#35782;&#26469;&#26631;&#27880;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#26631;&#27880;&#32773;&#21487;&#33021;&#23545;&#26631;&#27880;&#26041;&#26696;&#26377;&#19981;&#21516;&#30340;&#35299;&#37322;&#65292;&#38500;&#38750;&#25509;&#21463;&#20102;&#24191;&#27867;&#30340;&#22521;&#35757;&#65292;&#21542;&#21017;&#23545;&#20110;&#20027;&#35266;&#30340;NLP&#20219;&#21153;&#65292;&#29978;&#33267;&#21463;&#36807;&#35757;&#32451;&#30340;&#19987;&#23478;&#26631;&#27880;&#32773;&#20063;&#21487;&#33021;&#20250;&#20986;&#29616;&#24040;&#22823;&#30340;&#20998;&#27495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#32454;&#24494;&#24046;&#21035;&#21487;&#20197;&#36890;&#36807;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#36827;&#34892;&#25429;&#25417;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLM&#22312;&#23384;&#22312;&#20998;&#27495;&#26102;&#37325;&#26032;&#35843;&#25972;&#22823;&#23567;&#25490;&#24207;&#27880;&#37322;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;Likert&#35780;&#20998;&#21644;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#36755;&#20837;LLM&#65292;&#24182;&#25552;&#31034;&#23427;&#20135;&#29983;&#19968;&#20010;&#25968;&#23383;&#24471;&#20998;&#12290;&#36825;&#20010;&#24471;&#20998;&#24212;&#35813;&#21453;&#26144;&#27880;&#37322;&#32773;&#23545;&#31034;&#20363;&#30340;&#22522;&#26412;&#35780;&#20272;&#12290;&#35299;&#37322;&#30340;&#23384;&#22312;&#20351;LLM&#33021;&#22815;&#22312;&#23610;&#24230;&#20351;&#29992;&#24046;&#24322;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20351;&#35780;&#32423;&#22312;&#26631;&#27880;&#32773;&#20043;&#38388;&#21516;&#36136;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of large language models (LLMs) has brought a critical need for high-quality human-labeled data, particularly for processes like human feedback and evaluation. A common practice is to label data via consensus annotation over the judgments of multiple crowdworkers. However, different annotators may have different interpretations of labeling schemes unless given extensive training, and for subjective NLP tasks, even trained expert annotators can diverge heavily. We show that these nuances can be captured by high quality natural language explanations, and propose a method to rescale ordinal annotation in the presence of disagreement using LLMs. Specifically, we feed Likert ratings and corresponding natural language explanations into an LLM and prompt it to produce a numeric score. This score should reflect the underlying assessment of the example by the annotator. The presence of explanations allows the LLM to homogenize ratings across annotators in spite of scale usage differenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#34429;&#28982;&#27169;&#22411;&#34920;&#29616;&#20986;&#19968;&#23450;&#31243;&#24230;&#30340;N-ToM&#33021;&#21147;&#65292;&#20294;&#36828;&#38750;&#31283;&#20581;&#65292;&#23384;&#22312;&#23545;&#25239;&#24615;&#20363;&#23376;&#22256;&#38590;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14763</link><description>&lt;p&gt;
Clever Hans&#36824;&#26159;&#31070;&#32463;&#24515;&#26234;&#29702;&#35770;&#65311;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#31038;&#20132;&#25512;&#29702;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models. (arXiv:2305.14763v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#34429;&#28982;&#27169;&#22411;&#34920;&#29616;&#20986;&#19968;&#23450;&#31243;&#24230;&#30340;N-ToM&#33021;&#21147;&#65292;&#20294;&#36828;&#38750;&#31283;&#20581;&#65292;&#23384;&#22312;&#23545;&#25239;&#24615;&#20363;&#23376;&#22256;&#38590;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#33021;&#21147;&#30340;&#20105;&#35770;&#26085;&#30410;&#21319;&#32423;&#65292;&#38656;&#35201;&#24320;&#21457;&#21487;&#38752;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#26426;&#22120;&#30340;&#8220;&#26234;&#33021;&#8221;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#36726;&#20107;&#24615;&#30340;&#20363;&#23376;&#34987;&#29992;&#26469;&#26263;&#31034;&#20687;ChatGPT&#21644;GPT-4&#36825;&#26679;&#30340;&#26032;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#31070;&#32463;&#24515;&#26234;&#29702;&#35770;&#65288;N-ToM&#65289;;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#23545;&#36825;&#20123;&#33021;&#21147;&#24471;&#20986;&#20102;&#30456;&#20114;&#30683;&#30462;&#30340;&#32467;&#35770;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;6&#39033;&#20219;&#21153;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#26469;&#35843;&#26597;LLMs&#30340;N-ToM&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;LLMs&#34920;&#29616;&#20986;&#26576;&#20123;N-ToM&#33021;&#21147;&#65292;&#20294;&#36825;&#31181;&#34892;&#20026;&#36824;&#36828;&#38750;&#31283;&#20581;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26816;&#26597;&#24433;&#21709;N-ToM&#20219;&#21153;&#34920;&#29616;&#30340;&#22240;&#32032;&#65292;&#24182;&#21457;&#29616;LLMs&#22312;&#23545;&#25239;&#24615;&#20363;&#23376;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#34920;&#26126;&#20381;&#36182;&#20110;&#27973;&#23618;&#21551;&#21457;&#24335;&#32780;&#19981;&#26159;&#31283;&#20581;&#30340;ToM&#33021;&#21147;&#12290;&#25105;&#20204;&#35686;&#21578;&#19981;&#35201;&#20174;&#36726;&#20107;&#24615;&#30340;&#20363;&#23376;&#12289;&#26377;&#38480;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#20351;&#29992;&#20154;&#31867;&#35774;&#35745;&#30340;&#24515;&#29702;&#27979;&#35797;&#26469;&#35780;&#20272;&#27169;&#22411;&#20013;&#24471;&#20986;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The escalating debate on AI's capabilities warrants developing reliable metrics to assess machine "intelligence". Recently, many anecdotal examples were used to suggest that newer large language models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UniChart&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;UniChart&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;&#21508;&#31181;&#19981;&#21516;&#20027;&#39064;&#21644;&#35270;&#35273;&#39118;&#26684;&#30340;&#22823;&#37327;&#22270;&#34920;&#35821;&#26009;&#24211;&#65292;&#28982;&#21518;&#20351;&#29992;&#22270;&#34920;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#32534;&#30721;&#22270;&#34920;&#65292;&#24182;&#22312;&#20960;&#20010;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.14761</link><description>&lt;p&gt;
UniChart&#65306;&#38754;&#21521;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning. (arXiv:2305.14761v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UniChart&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;UniChart&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;&#21508;&#31181;&#19981;&#21516;&#20027;&#39064;&#21644;&#35270;&#35273;&#39118;&#26684;&#30340;&#22823;&#37327;&#22270;&#34920;&#35821;&#26009;&#24211;&#65292;&#28982;&#21518;&#20351;&#29992;&#22270;&#34920;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#32534;&#30721;&#22270;&#34920;&#65292;&#24182;&#22312;&#20960;&#20010;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#22312;&#25968;&#25454;&#20998;&#26512;&#12289;&#21487;&#35270;&#21270;&#37325;&#35201;&#35265;&#35299;&#21644;&#22238;&#31572;&#25968;&#25454;&#30340;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#27969;&#34892;&#12290;&#20026;&#20102;&#26041;&#20415;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#22522;&#20110;&#22270;&#34920;&#30340;&#25968;&#25454;&#20998;&#26512;&#65292;&#26368;&#36817;&#24341;&#20837;&#20102;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#20363;&#22914;&#22270;&#34920;&#38382;&#31572;&#21644;&#22270;&#34920;&#24635;&#32467;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#30340;&#26041;&#27861;&#37117;&#20351;&#29992;&#35821;&#35328;&#25110;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#65292;&#32780;&#19981;&#35797;&#22270;&#26126;&#30830;&#24314;&#27169;&#22270;&#34920;&#30340;&#32467;&#26500;&#65288;&#20363;&#22914;&#65292;&#22914;&#20309;&#35270;&#35273;&#32534;&#30721;&#25968;&#25454;&#20197;&#21450;&#22914;&#20309;&#23558;&#22270;&#34920;&#20803;&#32032;&#30456;&#20114;&#20851;&#32852;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;&#21508;&#31181;&#19981;&#21516;&#20027;&#39064;&#21644;&#35270;&#35273;&#39118;&#26684;&#30340;&#22823;&#37327;&#22270;&#34920;&#35821;&#26009;&#24211;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UniChart&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;UniChart&#23545;&#22270;&#34920;&#30340;&#30456;&#20851;&#25991;&#26412;&#12289;&#25968;&#25454;&#21644;&#35270;&#35273;&#20803;&#32032;&#36827;&#34892;&#32534;&#30721;&#65292;&#28982;&#21518;&#20351;&#29992;&#22522;&#20110;&#22270;&#34920;&#30340;&#25991;&#26412;&#35299;&#30721;&#22120;&#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39044;&#26399;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#38754;&#21521;&#22270;&#34920;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#20302;&#23618;&#27425;&#35270;&#35273;&#32534;&#30721;&#39044;&#27979;&#65292;&#65288;ii&#65289;&#22270;&#34920;&#20803;&#32032;&#20851;&#31995;&#39044;&#27979;&#21644;&#65288;iii&#65289;&#22270;&#34920;&#38382;&#39064;&#22238;&#31572;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;UniChart&#22312;&#20960;&#20010;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Charts are very popular for analyzing data, visualizing key insights and answering complex reasoning questions about data. To facilitate chart-based data analysis using natural language, several downstream tasks have been introduced recently such as chart question answering and chart summarization. However, most of the methods that solve these tasks use pretraining on language or vision-language tasks that do not attempt to explicitly model the structure of the charts (e.g., how data is visually encoded and how chart elements are related to each other). To address this, we first build a large corpus of charts covering a wide variety of topics and visual styles. We then present UniChart, a pretrained model for chart comprehension and reasoning. UniChart encodes the relevant text, data, and visual elements of charts and then uses a chart-grounded text decoder to generate the expected output in natural language. We propose several chart-specific pretraining tasks that include: (i) low-lev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24494;&#35843;&#31574;&#30053;Bi-Drop&#26469;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#24494;&#35843;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#32463;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#65292;Bi-Drop&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#23545;&#20110;&#22810;&#20219;&#21153;&#12289;&#22810;&#39046;&#22495;&#36716;&#31227;&#12289;&#25968;&#25454;&#19981;&#22343;&#34913;&#21644;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#20063;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14760</link><description>&lt;p&gt;
Bi-Drop: &#33258;&#36866;&#24212;&#23376;&#32593;&#32476;&#20248;&#21270;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#29992;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Bi-Drop: Generalizable Fine-tuning for Pre-trained Language Models via Adaptive Subnetwork Optimization. (arXiv:2305.14760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24494;&#35843;&#31574;&#30053;Bi-Drop&#26469;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#24494;&#35843;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#32463;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#65292;Bi-Drop&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#23545;&#20110;&#22810;&#20219;&#21153;&#12289;&#22810;&#39046;&#22495;&#36716;&#31227;&#12289;&#25968;&#25454;&#19981;&#22343;&#34913;&#21644;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#20063;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#24050;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#35757;&#32451;&#38598;&#26377;&#38480;&#65292;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#24494;&#35843;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#65292;&#20174;&#32780;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24494;&#35843;&#31574;&#30053;Bi-Drop&#26469;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32570;&#28857;&#12290;&#23427;&#21033;&#29992;dropout&#29983;&#25104;&#30340;&#21508;&#31181;&#23376;&#27169;&#22411;&#30340;&#26799;&#24230;&#20449;&#24687;&#26377;&#36873;&#25321;&#24615;&#22320;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Bi-Drop&#30340;&#24615;&#33021;&#20248;&#20110;&#20043;&#21069;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#23545;&#27604;&#32431;&#24494;&#35843;&#26356;&#24378;&#30340;&#31283;&#20581;&#24615;&#19982;&#36890;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20219;&#21153;&#12289;&#22810;&#39046;&#22495;&#36716;&#31227;&#12289;&#25968;&#25454;&#19981;&#22343;&#34913;&#21644;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#65292;Bi-Drop&#30340;&#34920;&#29616;&#24046;&#24322;&#26174;&#33879;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models have achieved remarkable success in a variety of natural language understanding tasks. Nevertheless, finetuning large pretrained models on downstream tasks is susceptible to overfitting if the training set is limited, which will lead to diminished performance. In this work, we propose a dynamic fine-tuning strategy for pretrained language models called Bi-Drop. It utilizes the gradient information of various sub-models generated by dropout to update the model parameters selectively. Experiments on the GLUE benchmark show that Bi-Drop outperforms previous fine-tuning methods by a considerable margin, and exhibits consistent superiority over vanilla fine-tuning across various pretrained models. Furthermore, empirical results indicate that Bi-Drop yields substantial improvements in the multiple task or domain transfer, data imbalance, and low-resource scenarios, demonstrating superb generalization ability and robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#24515;&#29702;&#23398;&#26500;&#36896;&#20013;&#25552;&#21462;&#30340;&#21487;&#35299;&#37322;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#24773;&#32490;&#29109;&#12289;&#35821;&#35328;&#39118;&#26684;&#21644;&#24773;&#24863;&#21305;&#37197;&#12289;&#23452;&#20154;&#24615;&#21644;&#20849;&#24773;&#31561;&#20116;&#20010;&#24230;&#37327;&#65292;&#36825;&#20123;&#20154;&#31867;&#24230;&#37327;&#26631;&#20934;&#19982;&#29616;&#26377;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#19981;&#30456;&#20851;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.14757</link><description>&lt;p&gt;
&#38754;&#21521;&#20154;&#31867;&#20013;&#24515;&#30340;&#24230;&#37327;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Human-Centered Metrics for Dialog System Evaluation. (arXiv:2305.14757v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#24515;&#29702;&#23398;&#26500;&#36896;&#20013;&#25552;&#21462;&#30340;&#21487;&#35299;&#37322;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#24773;&#32490;&#29109;&#12289;&#35821;&#35328;&#39118;&#26684;&#21644;&#24773;&#24863;&#21305;&#37197;&#12289;&#23452;&#20154;&#24615;&#21644;&#20849;&#24773;&#31561;&#20116;&#20010;&#24230;&#37327;&#65292;&#36825;&#20123;&#20154;&#31867;&#24230;&#37327;&#26631;&#20934;&#19982;&#29616;&#26377;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#19981;&#30456;&#20851;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24515;&#29702;&#23398;&#35282;&#24230;&#26469;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#30340;&#24230;&#37327;&#26041;&#27861;&#65306;&#23545;&#35805;&#20195;&#29702;&#20154;&#20687;&#20154;&#31867;&#19968;&#26679;&#34920;&#36798;&#20102;&#22810;&#31181;&#29366;&#24577;&#65288;&#30701;&#26399;&#22240;&#32032;&#65292;&#22914;&#24773;&#32490;&#65289;&#21644;&#29305;&#36136;&#65288;&#26356;&#38271;&#26399;&#22240;&#32032;&#65292;&#22914;&#20010;&#24615;&#65289;&#12290;&#36825;&#20123;&#21487;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#30001;&#26469;&#33258;&#24050;&#24314;&#31435;&#30340;&#24515;&#29702;&#23398;&#26500;&#36896;&#30340;&#20116;&#31181;&#24230;&#37327;&#32452;&#25104;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#23545;&#35805;&#21644;&#23545;&#35805;&#20013;&#30340;&#27599;&#20010;&#22238;&#21512;&#65306;&#24773;&#32490;&#29109;&#65292;&#35821;&#35328;&#39118;&#26684;&#21644;&#24773;&#24863;&#21305;&#37197;&#65292;&#20197;&#21450;&#23452;&#20154;&#24615;&#21644;&#20849;&#24773;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#20154;&#31867;&#24230;&#37327;&#26631;&#20934;&#19982;6&#31181;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65288;&#20363;&#22914;BARTScore&#21644;BLEURT&#65289;&#22312;7&#20010;&#26631;&#20934;&#23545;&#35805;&#31995;&#32479;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;Three Bot Dialog Evaluation Corpus&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;ChatGPT&#12289;GPT-3&#21644;BlenderBot&#30340;&#24050;&#27880;&#37322;&#23545;&#35805;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#20154;&#31867;&#24230;&#37327;&#26631;&#20934;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#20449;&#24687;&#65292;&#19982;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#19981;&#30456;&#20851;&#65292;&#24182;&#21487;&#22312;&#39044;&#27979;&#23545;&#35805;&#31995;&#32479;&#36136;&#37327;&#26102;&#36229;&#36234;&#29616;&#26377;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present metrics for evaluating dialog systems through a psychologically-grounded "human" lens: conversational agents express a diversity of both states (short-term factors like emotions) and traits (longer-term factors like personality) just as people do. These interpretable metrics consist of five measures from established psychology constructs that can be applied both across dialogs and on turns within dialogs: emotional entropy, linguistic style and emotion matching, as well as agreeableness and empathy. We compare these human metrics against 6 state-of-the-art automatic metrics (e.g. BARTScore and BLEURT) on 7 standard dialog system data sets. We also introduce a novel data set, the Three Bot Dialog Evaluation Corpus, which consists of annotated conversations from ChatGPT, GPT-3, and BlenderBot. We demonstrate the proposed human metrics offer novel information, are uncorrelated with automatic metrics, and lead to increased accuracy beyond existing automatic metrics for predictin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#25991;&#20307;&#25913;&#20889;&#30340;&#37325;&#20889;&#21644;&#35780;&#20272;&#38454;&#27573;&#25972;&#21512;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;few-shot prompting&#27604;&#36739;&#38750;&#19978;&#19979;&#25991;&#25913;&#20889;&#21644;&#19978;&#19979;&#25991;&#25913;&#20889;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#33258;&#21160;&#24230;&#37327;&#25351;&#26631;&#19981;&#19968;&#23450;&#33021;&#21453;&#26144;&#20986;&#20154;&#31867;&#30340;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.14755</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#27169;&#22411;&#21450;&#35780;&#20272;&#22312;&#25991;&#20307;&#25913;&#20889;&#20013;&#30340;&#24517;&#35201;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting. (arXiv:2305.14755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#25991;&#20307;&#25913;&#20889;&#30340;&#37325;&#20889;&#21644;&#35780;&#20272;&#38454;&#27573;&#25972;&#21512;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;few-shot prompting&#27604;&#36739;&#38750;&#19978;&#19979;&#25991;&#25913;&#20889;&#21644;&#19978;&#19979;&#25991;&#25913;&#20889;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#33258;&#21160;&#24230;&#37327;&#25351;&#26631;&#19981;&#19968;&#23450;&#33021;&#21453;&#26144;&#20986;&#20154;&#31867;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25991;&#20307;&#25913;&#20889;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#25805;&#20316;&#65292;&#20294;&#26159;&#24573;&#35270;&#25991;&#26412;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#21487;&#20197;&#23548;&#33268;&#25913;&#20889;&#32467;&#26524;&#26159;&#19968;&#33324;&#21270;&#12289;&#27495;&#20041;&#21644;&#19981;&#36830;&#36143;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#25972;&#21512;&#25991;&#26412;&#19978;&#19979;&#25991;&#21040;&#25991;&#20307;&#25913;&#20889;&#30340;&#37325;&#20889;&#21644;&#35780;&#20272;&#38454;&#27573;&#65292;&#37325;&#28857;&#20851;&#27880;&#24418;&#24335;&#12289;&#27602;&#24615;&#21644;&#24773;&#24863;&#36716;&#31227;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545; GPT-3.5 &#21644; GPT NeoX &#30340; few-shot &#25552;&#38382;&#27604;&#36739;&#37325;&#20889;&#30340;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#38750;&#19978;&#19979;&#25991;&#25913;&#20889;&#21644;&#19978;&#19979;&#25991;&#25913;&#20889;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20154;&#20204;&#36890;&#24120;&#26356;&#21916;&#27426;&#19978;&#19979;&#25991;&#25913;&#20889;&#65292;&#20294;&#33258;&#21160;&#24230;&#37327;&#25351;&#26631;&#65288;&#22914; BLEU&#65292;sBERT&#65289;&#19981;&#26159;&#36825;&#26679;&#30340;&#12290;&#20026;&#24357;&#21512;&#36825;&#31181;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#29992;&#33258;&#21160;&#24230;&#37327;&#25351;&#26631;&#30340;&#19978;&#19979;&#25991;&#34701;&#21512;&#29256;&#26412;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#26356;&#33021;&#21453;&#26144;&#20154;&#31867;&#20559;&#22909;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#25991;&#24378;&#35843;&#22312;&#25991;&#20307;&#25913;&#20889;&#30340;&#37325;&#20889;&#21644;&#35780;&#20272;&#38454;&#27573;&#25972;&#21512;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing stylistic text rewriting methods operate on a sentence level, but ignoring the broader context of the text can lead to generic, ambiguous, and incoherent rewrites. In this paper, we propose the integration of preceding textual context into both the rewriting and evaluation stages of stylistic text rewriting, focusing on formality, toxicity, and sentiment transfer tasks. We conduct a comparative evaluation of rewriting through few-shot prompting of GPT-3.5 and GPT NeoX, comparing non-contextual rewrites to contextual rewrites. Our experiments show that humans often prefer contextual rewrites over non-contextual ones, but automatic metrics (e.g., BLEU, sBERT) do not. To bridge this gap, we propose context-infused versions of common automatic metrics, and show that these better reflect human preferences. Overall, our paper highlights the importance of integrating preceding textual context into both the rewriting and evaluation stages of stylistic text rewriting.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DialogVCS&#65292;&#24314;&#31435;&#20102;4&#20010;&#25968;&#25454;&#38598;&#29992;&#20110;&#27979;&#35797;&#40065;&#26834;NLU&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#65292;&#26032;&#24847;&#22270;&#30340;&#20986;&#29616;&#21487;&#33021;&#20250;&#19982;&#29616;&#26377;&#24847;&#22270;&#22312;&#35821;&#20041;&#19978;&#23384;&#22312;&#20851;&#32852;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14751</link><description>&lt;p&gt;
DialogVCS&#65306;&#23545;&#35805;&#31995;&#32479;&#21319;&#32423;&#20013;&#30340;&#40065;&#26834;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
DialogVCS: Robust Natural Language Understanding in Dialogue System Upgrade. (arXiv:2305.14751v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DialogVCS&#65292;&#24314;&#31435;&#20102;4&#20010;&#25968;&#25454;&#38598;&#29992;&#20110;&#27979;&#35797;&#40065;&#26834;NLU&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#65292;&#26032;&#24847;&#22270;&#30340;&#20986;&#29616;&#21487;&#33021;&#20250;&#19982;&#29616;&#26377;&#24847;&#22270;&#22312;&#35821;&#20041;&#19978;&#23384;&#22312;&#20851;&#32852;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20135;&#21697;&#23545;&#35805;&#31995;&#32479;&#30340;&#19981;&#26029;&#26356;&#26032;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#27169;&#22411;&#65292;&#22240;&#20026;&#23454;&#38469;&#29992;&#25143;&#25968;&#25454;&#20250;&#19982;&#19978;&#19968;&#27425;&#26356;&#26032;&#32047;&#31215;&#30340;&#25968;&#25454;&#21512;&#24182;&#12290;&#26032;&#25968;&#25454;&#20013;&#20250;&#20986;&#29616;&#26032;&#30340;&#24847;&#22270;&#65292;&#24182;&#21487;&#33021;&#19982;&#29616;&#26377;&#24847;&#22270;&#22312;&#35821;&#20041;&#19978;&#23384;&#22312;&#32416;&#32544;&#65292;&#20363;&#22914;&#65292;&#35821;&#20041;&#36807;&#20110;&#29305;&#23450;&#25110;&#36890;&#29992;&#30340;&#26032;&#24847;&#22270;&#23454;&#38469;&#19978;&#26159;&#35821;&#20041;&#31354;&#38388;&#20013;&#26576;&#20123;&#29616;&#26377;&#24847;&#22270;&#30340;&#23376;&#38598;&#25110;&#36229;&#38598;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;NLU&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#20316;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#31532;&#19968;&#27425;&#23581;&#35797;&#65292;&#25105;&#20204;&#35774;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;4&#20010;&#23545;&#35805;&#29256;&#26412;&#25511;&#21046;&#25968;&#25454;&#38598;&#65288;DialogVCS&#65289;&#12290;&#25105;&#20204;&#23558;&#20855;&#26377;&#19981;&#23436;&#32654;&#25968;&#25454;&#30340;&#31995;&#32479;&#26356;&#26032;&#20013;&#30340;&#24847;&#22270;&#26816;&#27979;&#23450;&#20041;&#20026;&#20855;&#26377;&#27491;&#20294;&#26410;&#26631;&#35760;&#24847;&#22270;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#35201;&#27714;&#27169;&#22411;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#35782;&#21035;&#25152;&#26377;&#36866;&#24403;&#30340;&#24847;&#22270;&#65292;&#21253;&#25324;&#20855;&#26377;&#35821;&#20041;&#32416;&#32544;&#30340;&#24847;&#22270;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27169;&#22411;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the constant updates of the product dialogue systems, we need to retrain the natural language understanding (NLU) model as new data from the real users would be merged into the existent data accumulated in the last updates. Within the newly added data, new intents would emerge and might have semantic entanglement with the existing intents, e.g. new intents that are semantically too specific or generic are actually subset or superset of some existing intents in the semantic space, thus impairing the robustness of the NLU model. As the first attempt to solve this problem, we setup a new benchmark consisting of 4 Dialogue Version Control dataSets (DialogVCS). We formulate the intent detection with imperfect data in the system update as a multi-label classification task with positive but unlabeled intents, which asks the models to recognize all the proper intents, including the ones with semantic entanglement, in the inference. We also propose comprehensive baseline models and conduct i
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24120;&#24120;&#19981;&#33021;&#23436;&#20840;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#22522;&#20110;&#31572;&#26696;&#26029;&#35328;&#20998;&#35299;&#30340;&#32454;&#31890;&#24230;&#33258;&#25105;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#39564;&#35777;&#31572;&#26696;&#28385;&#36275;&#21738;&#20123;&#38382;&#39064;&#26631;&#20934;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#24110;&#21161;&#21457;&#29616;&#27169;&#22411;&#38169;&#35823;&#21644;&#30693;&#35782;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2305.14750</link><description>&lt;p&gt;
&#35299;&#20915;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#26102;&#30340;&#35780;&#20272;&#38382;&#39064;&#65306;&#22522;&#20110;&#31572;&#26696;&#26029;&#35328;&#20998;&#35299;&#30340;&#32454;&#31890;&#24230;&#33258;&#25105;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation. (arXiv:2305.14750v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14750
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24120;&#24120;&#19981;&#33021;&#23436;&#20840;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#22522;&#20110;&#31572;&#26696;&#26029;&#35328;&#20998;&#35299;&#30340;&#32454;&#31890;&#24230;&#33258;&#25105;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#39564;&#35777;&#31572;&#26696;&#28385;&#36275;&#21738;&#20123;&#38382;&#39064;&#26631;&#20934;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#24110;&#21161;&#21457;&#29616;&#27169;&#22411;&#38169;&#35823;&#21644;&#30693;&#35782;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#29983;&#25104;&#30340;&#31572;&#26696;&#19981;&#33021;&#28385;&#36275;&#38382;&#39064;&#30340;&#25152;&#26377;&#26631;&#20934;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#33258;&#25105;&#35780;&#20272;&#25216;&#26415;&#26088;&#22312;&#26816;&#27979;&#36825;&#20123;&#31572;&#26696;&#26159;&#21542;&#27491;&#30830;&#65292;&#20294;&#36825;&#20123;&#25216;&#26415;&#26080;&#27861;&#30830;&#23450;&#29983;&#25104;&#30340;&#31572;&#26696;&#28385;&#36275;&#38382;&#39064;&#30340;&#21738;&#20123;&#26631;&#20934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31572;&#26696;&#26029;&#35328;&#20998;&#35299;&#65288;ABCD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#21487;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#21487;&#20197;&#29992;&#26469;&#39564;&#35777;&#31572;&#26696;&#28385;&#36275;&#21738;&#20123;&#38382;&#39064;&#26631;&#20934;&#30340;&#30495;/&#20551;&#26029;&#35328;&#12290;&#20351;&#29992;&#20998;&#35299;&#30340;ABCD&#26029;&#35328;&#65292;&#25105;&#20204;&#25191;&#34892;&#20102;&#32454;&#31890;&#24230;&#30340;&#33258;&#25105;&#35780;&#20272;&#12290;&#36890;&#36807;&#23545;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#26032;&#25910;&#38598;&#30340;&#25361;&#25112;&#25968;&#25454;&#38598;ObscureQA&#65289;&#30340;&#21021;&#27493;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-3.5&#26377;&#19968;&#23450;&#33021;&#21147;&#30830;&#23450;&#20854;&#31572;&#26696;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#28385;&#36275;&#36755;&#20837;&#38382;&#39064;&#30340;&#26631;&#20934;&#65292;&#24182;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#27169;&#22411;&#38169;&#35823;&#21644;&#30693;&#35782;&#24046;&#36317;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
When answering complex questions, large language models (LLMs) may produce answers that do not satisfy all criteria of the question. While existing self-evaluation techniques aim to detect if such answers are correct, these techniques are unable to determine which criteria of the question are satisfied by the generated answers. To address this issue, we propose answer-based claim decomposition (ABCD), a prompting strategy that decomposes questions into a series of true/false claims that can be used to verify which criteria of the input question an answer satisfies. Using the decomposed ABCD claims, we perform fine-grained self-evaluation. Through preliminary experiments on three datasets, including a newly-collected challenge dataset ObscureQA, we find that GPT-3.5 has some ability to determine to what extent its answer satisfies the criteria of the input question, and can give insights into the errors and knowledge gaps of the model.
&lt;/p&gt;</description></item><item><title>ECHo&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#25512;&#29702;&#30340;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;CoT&#33539;&#24335;&#23545;&#40784;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#35780;&#20272;&#24403;&#21069;AI&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14740</link><description>&lt;p&gt;
ECHo: &#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#25512;&#29702;&#30340;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
ECHo: Event Causality Inference via Human-centric Reasoning. (arXiv:2305.14740v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14740
&lt;/p&gt;
&lt;p&gt;
ECHo&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#25512;&#29702;&#30340;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;CoT&#33539;&#24335;&#23545;&#40784;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#35780;&#20272;&#24403;&#21069;AI&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; ECHo&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#21644;&#35821;&#35328;&#31038;&#20132;&#24773;&#22659;&#30340;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;&#35786;&#26029;&#25968;&#25454;&#38598;&#12290; ECHo&#21033;&#29992;&#20174;&#29359;&#32618;&#21095;&#20013;&#25910;&#38598;&#30340;&#30495;&#23454;&#20154;&#31867;&#20013;&#24515;&#28436;&#32462;&#20449;&#24687;&#65292;&#36890;&#36807;&#28608;&#21457;&#20013;&#38388;&#24515;&#28789;&#29702;&#35770;&#65288;ToM&#65289;&#26469;&#24357;&#21512;&#22810;&#27169;&#24577;&#25512;&#29702;&#30340;&#40511;&#27807;&#65292;&#20174;&#32780;&#25552;&#39640;&#31038;&#20132;&#26234;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;Chain-of-Thought&#65288;CoT&#65289;&#33539;&#24335;&#23545;&#40784;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#24403;&#21069;AI&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20010;ToM&#22686;&#24378;&#30340;CoT&#31649;&#36947;&#21487;&#20197;&#22312; &#38646;-shot&#35270;&#35273;&#21644;&#35821;&#35328;&#29702;&#35299;&#20013;&#21253;&#23481;&#21644;&#25972;&#21512;&#21508;&#31181;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#20114;&#34917;&#30340;&#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#30340;ECHo&#20219;&#21153;&#26469;&#23457;&#26597;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;ECHo&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26292;&#38706;&#25512;&#29702;&#20013;&#30340;&#19981;&#23436;&#21892;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce ECHo, a diagnostic dataset of event causality inference grounded in visual-and-linguistic social scenarios. ECHo employs real-world human-centric deductive information collected from crime drama, bridging the gap in multimodal reasoning towards higher social intelligence through the elicitation of intermediate Theory-of-Mind (ToM). We propose a unified framework aligned with the Chain-of-Thought (CoT) paradigm to assess the reasoning capability of current AI systems. This ToM-enhanced CoT pipeline can accommodate and integrate various large foundation models in zero-shot visual-and-linguistic understanding. With this framework, we scrutinize the advanced large language and multimodal models via three complementary human-centric ECHo tasks. Further analysis demonstrates ECHo as a challenging dataset to expose imperfections and inconsistencies in reasoning.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#35299;&#30721; (CAD)&#65292;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19981;&#21516;LM&#31995;&#21015;&#30340;&#24544;&#23454;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#20915;&#30693;&#35782;&#20914;&#31361;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.14739</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#24863;&#30693;&#35299;&#30721;&#30340;&#24110;&#21161;&#19979;&#20449;&#20219;&#24744;&#30340;&#35777;&#25454;&#65306;&#26356;&#23569;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Trusting Your Evidence: Hallucinate Less with Context-aware Decoding. (arXiv:2305.14739v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14739
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#35299;&#30721; (CAD)&#65292;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19981;&#21516;LM&#31995;&#21015;&#30340;&#24544;&#23454;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#20915;&#30693;&#35782;&#20914;&#31361;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#38590;&#20197;&#20805;&#20998;&#20851;&#27880;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#24182;&#29983;&#25104;&#19981;&#24544;&#23454;&#25110;&#21253;&#21547;&#24187;&#35273;&#30340;&#25991;&#26412;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#35299;&#30721; (CAD)&#65292;&#23427;&#36981;&#24490;&#23545;&#27604;&#36755;&#20986;&#20998;&#24067;&#65292;&#21487;&#20197;&#25918;&#22823;&#20351;&#29992;&#27169;&#22411;&#26102;&#26377;&#26080;&#19978;&#19979;&#25991;&#26102;&#36755;&#20986;&#27010;&#29575;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CAD&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19981;&#21516;LM&#31995;&#21015;&#30340;&#24544;&#23454;&#24230;&#65292;&#21253;&#25324;OPT&#12289;GPT&#12289;LLaMA&#21644;FLAN-T5&#29992;&#20110;&#24635;&#32467;&#20219;&#21153; (&#20363;&#22914;&#65292;LLaMA&#22312;&#20107;&#23454;&#24230;&#24230;&#37327;&#26041;&#38754;&#33719;&#24471;&#20102;14.3%&#30340;&#22686;&#30410;)&#12290;&#27492;&#22806;&#65292;&#24403;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#19982;&#25152;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#20914;&#31361;&#26102;&#65292;CAD&#29305;&#21035;&#26377;&#25928;&#65292;&#20174;&#32780;&#22312;&#35299;&#20915;&#30693;&#35782;&#20914;&#31361;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#21040;&#20260;&#23475;&#30340;&#20154;&#32676;&#65292;&#21457;&#29616;&#23545;&#20110;&#36825;&#20123;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#65292;&#20182;&#20204;&#38754;&#20020;&#30340;&#27602;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.14735</link><description>&lt;p&gt;
&#36793;&#32536;&#32858;&#28966;&#65306;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#25439;&#20154;&#32676;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection. (arXiv:2305.14735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#21040;&#20260;&#23475;&#30340;&#20154;&#32676;&#65292;&#21457;&#29616;&#23545;&#20110;&#36825;&#20123;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#65292;&#20182;&#20204;&#38754;&#20020;&#30340;&#27602;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#23545;&#36793;&#32536;&#31038;&#21306;&#24433;&#21709;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#30830;&#23450;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#38024;&#23545;&#24369;&#21183;&#32676;&#20307;&#30340;&#20260;&#23475;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20250;&#25513;&#30422;&#30001;&#20132;&#21449;&#23376;&#32676;&#25110;&#36328;&#20154;&#21475;&#32676;&#20307;&#20849;&#20139;&#30340;&#20260;&#23475;&#27169;&#24335;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#8220;&#36793;&#32536;&#8221;&#23450;&#20041;&#20026;&#20855;&#26377;&#36828;&#31163;&#8220;&#24120;&#24577;&#8221; &#30340;&#20154;&#21475;&#23646;&#24615;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#24230;&#37327;&#38024;&#23545;&#36825;&#20123;&#24322;&#24120;&#20540;&#30340;&#20260;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#20307;&#30340;&#24615;&#33021;&#24046;&#24322;&#25351;&#25968;&#65288;GPDI&#65289;&#65292;&#20197;&#34913;&#37327;&#25968;&#25454;&#38598;&#32454;&#20998;&#20026;&#23376;&#32452;&#23545;&#38754;&#20020;&#22686;&#21152;&#30340;&#20260;&#23475;&#30340;&#35782;&#21035;&#31243;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26816;&#27979;&#27602;&#24615;&#26816;&#27979;&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#38024;&#23545;&#24322;&#24120;&#20540;&#30340;&#25991;&#26412;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#27602;&#24615;&#26816;&#39564;&#20013;&#27602;&#24615;&#26356;&#39640;&#65292;&#39640;&#36798;28&#65285;&#33267;86&#65285;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#23545;&#20110;&#20154;&#21475;&#23398;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#22987;&#32456;&#36739;&#24046;&#65292;&#24322;&#24120;&#20540;&#21644;&#38750;&#24322;&#24120;&#20540;&#20043;&#38388;&#30340;&#38169;&#35823;&#24046;&#36317;&#39640;&#36798;10&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
A standard method for measuring the impacts of AI on marginalized communities is to determine performance discrepancies between specified demographic groups. These approaches aim to address harms toward vulnerable groups, but they obscure harm patterns faced by intersectional subgroups or shared across demographic groups. We instead operationalize "the margins" as data points that are statistical outliers due to having demographic attributes distant from the "norm" and measure harms toward these outliers. We propose a Group-Based Performance Disparity Index (GPDI) that measures the extent to which a subdivision of a dataset into subgroups identifies those facing increased harms. We apply our approach to detecting disparities in toxicity detection and find that text targeting outliers is 28% to 86% more toxic for all types of toxicity examined. We also discover that model performance is consistently worse for demographic outliers, with disparities in error between outliers and non-outli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20102;&#20004;&#20010;&#39044;&#35757;&#32451;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#22312;&#38463;&#25289;&#20271;&#35821;GEC&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#19979;&#25991;&#31532;&#19968;&#38463;&#25289;&#20271;&#35821;&#22810;&#31867;&#35821;&#27861;&#38169;&#35823;&#26816;&#27979;&#32467;&#26524;&#12290;&#25991;&#31456;&#34920;&#26126;&#65292;&#20351;&#29992;GED&#20316;&#20026;GEC&#27169;&#22411;&#30340;&#36741;&#21161;&#36755;&#20837;&#26377;&#21161;&#20110;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#19978;&#19979;&#25991;&#24418;&#24577;&#39044;&#22788;&#29702;&#26377;&#21161;&#20110;&#25509;&#30528;GEC&#31995;&#32479;&#65292;&#25991;&#31456;&#22312;&#20004;&#20010;&#20849;&#20139;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;GEC&#32467;&#26524;&#20197;&#21450;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2305.14734</link><description>&lt;p&gt;
&#38463;&#25289;&#20271;&#35821;&#35821;&#27861;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#30340;&#36827;&#23637;&#65306;&#19968;&#39033;&#23454;&#35777;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Advancements in Arabic Grammatical Error Detection and Correction: An Empirical Investigation. (arXiv:2305.14734v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20102;&#20004;&#20010;&#39044;&#35757;&#32451;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#22312;&#38463;&#25289;&#20271;&#35821;GEC&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#19979;&#25991;&#31532;&#19968;&#38463;&#25289;&#20271;&#35821;&#22810;&#31867;&#35821;&#27861;&#38169;&#35823;&#26816;&#27979;&#32467;&#26524;&#12290;&#25991;&#31456;&#34920;&#26126;&#65292;&#20351;&#29992;GED&#20316;&#20026;GEC&#27169;&#22411;&#30340;&#36741;&#21161;&#36755;&#20837;&#26377;&#21161;&#20110;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#19978;&#19979;&#25991;&#24418;&#24577;&#39044;&#22788;&#29702;&#26377;&#21161;&#20110;&#25509;&#30528;GEC&#31995;&#32479;&#65292;&#25991;&#31456;&#22312;&#20004;&#20010;&#20849;&#20139;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;GEC&#32467;&#26524;&#20197;&#21450;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;(GEC)&#26159;&#19968;&#20010;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#33521;&#35821;&#38382;&#39064;&#65292;&#22312;&#35768;&#22810;&#24050;&#26377;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#31232;&#32570;&#21644;&#35821;&#35328;&#22797;&#26434;&#24615;&#31561;&#25361;&#25112;&#65292;&#23545;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#36827;&#34892;GEC&#30740;&#31350;&#30340;&#38480;&#21046;&#36739;&#22810;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#26032;&#24320;&#21457;&#30340;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#39318;&#27425;&#22312;&#38463;&#25289;&#20271;&#35821;GEC&#20013;&#33719;&#24471;&#20102;&#32467;&#26524;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#22810;&#31867;&#38463;&#25289;&#20271;&#35821;&#35821;&#27861;&#38169;&#35823;&#26816;&#27979;(GED)&#20219;&#21153;&#65292;&#24182;&#39318;&#27425;&#22312;&#22810;&#31867;&#38463;&#25289;&#20271;&#35821;GED&#19978;&#33719;&#24471;&#20102;&#32467;&#26524;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#36328;&#36234;&#19981;&#21516;&#31867;&#22411;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;GED&#20449;&#24687;&#20316;&#20026;&#36741;&#21161;&#36755;&#20837;&#30340;GEC&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;GEC&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#24418;&#24577;&#39044;&#22788;&#29702;&#22312;&#24110;&#21161;GEC&#31995;&#32479;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#38463;&#25289;&#20271;&#35821;GEC&#20849;&#20139;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#32467;&#26524;&#65292;&#24182;&#22312;&#26032;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grammatical error correction (GEC) is a well-explored problem in English with many existing models and datasets. However, research on GEC in morphologically rich languages has been limited due to challenges such as data scarcity and language complexity. In this paper, we present the first results on Arabic GEC by using two newly developed Transformer-based pretrained sequence-to-sequence models. We address the task of multi-class Arabic grammatical error detection (GED) and present the first results on multi-class Arabic GED. We show that using GED information as auxiliary input in GEC models improves GEC performance across three datasets spanning different genres. Moreover, we also investigate the use of contextual morphological preprocessing in aiding GEC systems. Our models achieve state-of-the-art results on two Arabic GEC shared tasks datasets and establish a strong benchmark on a newly created dataset.
&lt;/p&gt;</description></item><item><title>SenteCon&#26159;&#19968;&#31181;&#33021;&#22815;&#25552;&#20379;&#39640;&#32423;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25226;&#25991;&#26412;&#32534;&#30721;&#20026;&#21487;&#35299;&#37322;&#31867;&#21035;&#30340;&#23618;&#65292;&#21516;&#26102;&#19981;&#20250;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#39044;&#27979;&#24615;&#33021;&#36896;&#25104;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.14728</link><description>&lt;p&gt;
SenteCon: &#21033;&#29992;&#35789;&#27719;&#34920;&#23398;&#20064;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35821;&#35328;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations. (arXiv:2305.14728v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14728
&lt;/p&gt;
&lt;p&gt;
SenteCon&#26159;&#19968;&#31181;&#33021;&#22815;&#25552;&#20379;&#39640;&#32423;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25226;&#25991;&#26412;&#32534;&#30721;&#20026;&#21487;&#35299;&#37322;&#31867;&#21035;&#30340;&#23618;&#65292;&#21516;&#26102;&#19981;&#20250;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#39044;&#27979;&#24615;&#33021;&#36896;&#25104;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#35821;&#35328;&#34920;&#31034;&#24050;&#25104;&#20026;&#35821;&#35328;&#29305;&#24449;&#21270;&#30340;&#20027;&#35201;&#24418;&#24335;&#65292;&#20294;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20102;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#36825;&#19981;&#20165;&#38656;&#35201;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#36824;&#38656;&#35201;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#12290;&#29305;&#21035;&#26159;&#65292;&#35821;&#35328;&#24517;&#39035;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#29305;&#24449;&#21270;&#65292;&#21516;&#26102;&#20173;&#28982;&#24456;&#22909;&#22320;&#25551;&#36848;&#21407;&#22987;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SenteCon&#65292;&#19968;&#31181;&#22312;&#28145;&#24230;&#35821;&#35328;&#34920;&#31034;&#20013;&#24341;&#20837;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#25991;&#26412;&#27573;&#33853;&#65292;SenteCon&#23558;&#25991;&#26412;&#32534;&#30721;&#20026;&#21487;&#35299;&#37322;&#31867;&#21035;&#30340;&#23618;&#65292;&#20854;&#20013;&#27599;&#20010;&#32500;&#24230;&#23545;&#24212;&#20110;&#29305;&#23450;&#31867;&#21035;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#20351;&#29992;SenteCon&#23545;&#35821;&#35328;&#36827;&#34892;&#32534;&#30721;&#21487;&#20197;&#22312;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#39044;&#27979;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#39640;&#32423;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19979;&#28216;&#24615;&#33021;&#21644;&#21327;&#35758;&#26041;&#38754;&#65292;SenteCon&#20248;&#20110;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#35821;&#35328;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model's decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular, language must be featurized in a way that is interpretable while still characterizing the original text well. We present SenteCon, a method for introducing human interpretability in deep language representations. Given a passage of text, SenteCon encodes the text as a layer of interpretable categories in which each dimension corresponds to the relevance of a specific category. Our empirical evaluations indicate that encoding language with SenteCon provides high-level interpretability at little to no cost to predictive performance on downstream tasks. Moreover, we find that SenteCon outperforms existing interpretable language representations with respect to both its downstream performance and its agr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#29109;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#26469;&#25552;&#39640;&#38646;-shot&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14726</link><description>&lt;p&gt;
&#22522;&#20110;&#20132;&#21449;&#29109;&#24046;&#24322;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In-Context Demonstration Selection with Cross Entropy Difference. (arXiv:2305.14726v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#29109;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#26469;&#25552;&#39640;&#38646;-shot&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#21033;&#29992;&#19978;&#19979;&#25991;&#28436;&#31034;&#26469;&#25552;&#39640;&#38646;-shot&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#26368;&#20339;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27169;&#22411;&#24615;&#33021;&#21487;&#33021;&#22240;&#25152;&#36873;&#31034;&#20363;&#32780;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#29109;&#24046;&#24322;&#65288;CED&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#19978;&#19979;&#25991;&#28436;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#35266;&#23519;&#32467;&#26524;&#65306;&#22312;&#26377;&#38480;&#35843;&#25972;&#20102;&#36825;&#20123;&#28436;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#19978;&#19979;&#25991;&#28436;&#31034;&#30340;&#26377;&#25928;&#24615;&#19982;&#27979;&#35797;&#31034;&#20363;&#30340;&#22256;&#24785;&#24230;&#21576;&#36127;&#30456;&#20851;&#12290;&#25105;&#20204;&#21033;&#29992;&#21442;&#25968;&#26377;&#25928;&#30340;&#24494;&#35843;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#23567;&#22411;&#27169;&#22411;&#65292;&#20197;&#35745;&#31639;&#27979;&#35797;&#31034;&#20363;&#21644;&#27599;&#20010;&#20505;&#36873;&#19978;&#19979;&#25991;&#28436;&#31034;&#20043;&#38388;&#30340;&#20132;&#21449;&#29109;&#24046;&#24322;&#12290;&#35813;&#25351;&#26631;&#29992;&#20110;&#20026;&#27599;&#20010;&#27979;&#35797;&#36755;&#20837;&#29420;&#31435;&#22320;&#25490;&#21517;&#21644;&#36873;&#25321;&#19978;&#19979;&#25991;&#28436;&#31034;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#28151;&#21512;&#22495;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35813;&#25968;&#25454;&#38598;&#32467;&#21512;&#20102;8&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#20195;&#34920;4&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;CED&#22312;&#19978;&#19979;&#25991;&#28436;&#31034;&#36873;&#25321;&#26041;&#38754;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can use in-context demonstrations to improve performance on zero-shot tasks. However, selecting the best in-context examples is challenging because model performance can vary widely depending on the selected examples. We present a cross-entropy difference (CED) method for selecting in-context demonstrations. Our method is based on the observation that the effectiveness of in-context demonstrations negatively correlates with the perplexity of the test example by a language model that was finetuned on that demonstration. We utilize parameter efficient finetuning to train small models on training data that are used for computing the cross-entropy difference between a test example and every candidate in-context demonstration. This metric is used to rank and select in-context demonstrations independently for each test input. We evaluate our method on a mix-domain dataset that combines 8 benchmarks, representing 4 text generation tasks, showing that CED for in-co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;AMELI&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#23558;&#23646;&#24615;&#20449;&#24687;&#32435;&#20837;&#23454;&#20307;&#38142;&#25509;&#36807;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14725</link><description>&lt;p&gt;
AMELI:&#32454;&#31890;&#24230;&#23646;&#24615;&#22686;&#24378;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes. (arXiv:2305.14725v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14725
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;AMELI&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#23558;&#23646;&#24615;&#20449;&#24687;&#32435;&#20837;&#23454;&#20307;&#38142;&#25509;&#36807;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#65292;&#20854;&#20013;&#36755;&#20837;&#26159;&#19968;&#20010;&#30001;&#25991;&#26412;&#21644;&#22270;&#20687;&#25551;&#36848;&#30340;&#25552;&#21450;&#65292;&#30446;&#26631;&#26159;&#20174;&#19968;&#20010;&#22810;&#27169;&#24577;&#30693;&#35782;&#24211;&#20013;&#39044;&#27979;&#30456;&#24212;&#30340;&#30446;&#26631;&#23454;&#20307;&#65292;&#20854;&#20013;&#27599;&#20010;&#23454;&#20307;&#37117;&#26159;&#29992;&#25991;&#26412;&#25551;&#36848;&#12289;&#35270;&#35273;&#22270;&#20687;&#21644;&#19968;&#32452;&#23646;&#24615;&#20540;&#25551;&#36848;&#30340;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;AMELI&#65292;&#20854;&#20013;&#21253;&#21547;18,472&#20010;&#35780;&#35770;&#21644;35,598&#20010;&#20135;&#21697;&#12290;&#25105;&#20204;&#22312;AMELI&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#21644;&#25105;&#20204;&#22686;&#24378;&#30340;&#23646;&#24615;&#24863;&#30693;&#27169;&#22411;&#26469;&#24314;&#31435;&#22522;&#20934;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#23646;&#24615;&#20449;&#24687;&#32435;&#20837;&#23454;&#20307;&#38142;&#25509;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20026;&#23646;&#24615;&#24863;&#30693;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#24314;&#31435;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35299;&#20915;&#26041;&#26696;&#30340;&#22242;&#38431;&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#23558;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose attribute-aware multimodal entity linking, where the input is a mention described with a text and image, and the goal is to predict the corresponding target entity from a multimodal knowledge base (KB) where each entity is also described with a text description, a visual image and a set of attributes and values. To support this research, we construct AMELI, a large-scale dataset consisting of 18,472 reviews and 35,598 products. To establish baseline performance on AMELI, we experiment with the current state-of-the-art multimodal entity linking approaches and our enhanced attribute-aware model and demonstrate the importance of incorporating the attribute information into the entity linking process. To be best of our knowledge, we are the first to build benchmark dataset and solutions for the attribute-aware multimodal entity linking task. Datasets and codes will be made publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20174;&#35821;&#35328;&#38544;&#21947;&#29983;&#25104;&#35270;&#35273;&#38544;&#21947;&#65292;&#24182;&#19988;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#20849;&#21019;&#20986;&#20855;&#26377;&#35270;&#35273;&#20914;&#20987;&#21147;&#21644;&#35821;&#20041;&#21547;&#20041;&#30340;&#38544;&#21947;&#12290;</title><link>http://arxiv.org/abs/2305.14724</link><description>&lt;p&gt;
&#25105;&#23547;&#35269;&#19968;&#20010;&#38544;&#21947;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20849;&#21019;&#35270;&#35273;&#38544;&#21947;
&lt;/p&gt;
&lt;p&gt;
I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors. (arXiv:2305.14724v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20174;&#35821;&#35328;&#38544;&#21947;&#29983;&#25104;&#35270;&#35273;&#38544;&#21947;&#65292;&#24182;&#19988;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#20849;&#21019;&#20986;&#20855;&#26377;&#35270;&#35273;&#20914;&#20987;&#21147;&#21644;&#35821;&#20041;&#21547;&#20041;&#30340;&#38544;&#21947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38544;&#21947;&#26159;&#36890;&#36807;&#22270;&#20687;&#26469;&#35828;&#26381;&#25110;&#20256;&#36798;&#21019;&#24847;&#24819;&#27861;&#30340;&#24378;&#22823;&#20462;&#36766;&#25163;&#27861;&#12290;&#19982;&#35821;&#35328;&#38544;&#21947;&#31867;&#20284;&#65292;&#23427;&#20204;&#36890;&#36807;&#31526;&#21495;&#20027;&#20041;&#21644;&#31526;&#21495;&#30340;&#24182;&#32622;&#38544;&#21547;&#22320;&#20256;&#36798;&#21547;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#35821;&#35328;&#38544;&#21947;&#29983;&#25104;&#35270;&#35273;&#38544;&#21947;&#30340;&#26032;&#20219;&#21153;&#12290;&#36825;&#23545;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65288;&#22914;DALL $\cdot$ E 2&#65289;&#26469;&#35828;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#27169;&#25311;&#38544;&#21547;&#21547;&#20041;&#21644;&#32452;&#21512;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#37319;&#29992;&#20197;&#8220;&#20018;&#32852;&#24605;&#32500;&#8221;&#20026;&#25552;&#31034;&#30340;Instruct GPT-3&#65288;davinci-002&#65289;&#29983;&#25104;&#20195;&#34920;&#35821;&#35328;&#38544;&#21947;&#30340;&#35270;&#35273;&#38416;&#36848;&#30340;&#25991;&#26412;&#65292;&#20854;&#20013;&#21253;&#21547;&#38544;&#21547;&#21547;&#20041;&#21644;&#30456;&#20851;&#23545;&#35937;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#36890;&#36807;&#20154;&#26426;&#21327;&#20316;&#26694;&#26550;&#65292;&#20154;&#20204;&#19982;LLM&#21644;&#34920;&#29616;&#26368;&#20339;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#65292;&#21019;&#24314;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#38544;&#21947;&#21644;&#23427;&#20204;&#30340;&#35270;&#35273;&#23545;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LLMs&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#21487;&#20197;&#20849;&#21516;&#21019;&#36896;&#20986;&#20855;&#26377;&#35270;&#35273;&#20914;&#20987;&#21147;&#21644;&#35821;&#20041;&#21547;&#20041;&#30340;&#38544;&#21947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual metaphors are powerful rhetorical devices used to persuade or communicate creative ideas through images. Similar to linguistic metaphors, they convey meaning implicitly through symbolism and juxtaposition of the symbols. We propose a new task of generating visual metaphors from linguistic metaphors. This is a challenging task for diffusion-based text-to-image models, such as DALL$\cdot$E 2, since it requires the ability to model implicit meaning and compositionality. We propose to solve the task through the collaboration between Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3 (davinci-002) with Chain-of-Thought prompting generates text that represents a visual elaboration of the linguistic metaphor containing the implicit meaning and relevant objects, which is then used as input to the diffusion-based text-to-image models.Using a human-AI collaboration framework, where humans interact both with the LLM and the top-performing diffusion model, we create a high-qu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#35266;&#28857;&#20013;&#30340;&#35821;&#26009;&#24211;&#20877;&#35299;&#37322;&#21644;&#20803;&#35821;&#35328;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#27861;&#23448;&#20351;&#29992;&#30340;&#20803;&#35821;&#35328;&#31867;&#22411;&#30340;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.14719</link><description>&lt;p&gt;
CuRIAM: &#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#35266;&#28857;&#20013;&#30340;&#35821;&#26009;&#24211;&#20877;&#35299;&#37322;&#21644;&#20803;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
CuRIAM: Corpus re Interpretation and Metalanguage in U.S. Supreme Court Opinions. (arXiv:2305.14719v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#35266;&#28857;&#20013;&#30340;&#35821;&#26009;&#24211;&#20877;&#35299;&#37322;&#21644;&#20803;&#35821;&#35328;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#27861;&#23448;&#20351;&#29992;&#30340;&#20803;&#35821;&#35328;&#31867;&#22411;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#21496;&#27861;&#20915;&#23450;&#28041;&#21450;&#23545;&#27861;&#24459;&#25991;&#20214;&#30340;&#35299;&#37322;&#65307;&#22240;&#27492;&#65292;&#21496;&#27861;&#35266;&#28857;&#35201;&#20351;&#29992;&#35821;&#35328;&#20316;&#20026;&#19968;&#31181;&#20171;&#36136;&#26469;&#35780;&#35770;&#25110;&#24341;&#36215;&#23545;&#20854;&#20182;&#35821;&#35328;&#30340;&#20851;&#27880;&#12290;&#36825;&#26679;&#20351;&#29992;&#30340;&#35821;&#35328;&#31216;&#20026;&#20803;&#35821;&#35328;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27880;&#37322;&#27169;&#24335;&#26469;&#20998;&#31867;&#27861;&#24459;&#20803;&#35821;&#35328;&#30340;&#31867;&#22411;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#27169;&#24335;&#24212;&#29992;&#20110;&#19968;&#32452;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#30340;&#35266;&#28857;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#24635;&#20849;59k&#20196;&#29260;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#23545;&#27861;&#23448;&#20351;&#29992;&#30340;&#20803;&#35821;&#35328;&#31867;&#22411;&#35266;&#23519;&#21040;&#20102;&#20960;&#31181;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most judicial decisions involve the interpretation of legal texts; as such, judicial opinion requires the use of language as a medium to comment on or draw attention to other language. Language used this way is called metalanguage. We develop an annotation schema for categorizing types of legal metalanguage and apply our schema to a set of U.S. Supreme Court opinions, yielding a corpus totaling 59k tokens. We remark on several patterns observed in the kinds of metalanguage used by the justices.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35757;&#32451;&#31639;&#27861;Left-over Lunch RL &#65288;LoL-RL&#65289;&#65292;&#20351;&#29992;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#20219;&#20309;&#24207;&#21015;&#21040;&#24207;&#21015;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20248;&#21270;LM&#25928;&#29992;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14718</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#20248;&#21183;&#30340;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Language Models with Advantage-based Offline Policy Gradients. (arXiv:2305.14718v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35757;&#32451;&#31639;&#27861;Left-over Lunch RL &#65288;LoL-RL&#65289;&#65292;&#20351;&#29992;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#20219;&#20309;&#24207;&#21015;&#21040;&#24207;&#21015;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20248;&#21270;LM&#25928;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#29992;&#25143;&#23450;&#20041;&#30340;&#36136;&#37327;&#25110;&#39118;&#26684;&#38480;&#21046;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20856;&#22411;&#30340;&#26041;&#27861;&#21253;&#25324;&#23398;&#20064;&#39069;&#22806;&#30340;&#20154;&#24037;&#32534;&#20889;&#25968;&#25454;&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#36807;&#28388;&#8220;&#20302;&#36136;&#37327;&#8221;&#25968;&#25454;&#21644;/&#25110;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#20307;&#21453;&#39304;&#65288;RLHF&#65289;&#12290;&#28982;&#32780;&#65292;&#36807;&#28388;&#20250;&#21024;&#38500;&#26377;&#20215;&#20540;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#32780;&#25968;&#25454;&#25910;&#38598;&#21644;RLHF&#19981;&#26029;&#38656;&#35201;&#39069;&#22806;&#30340;&#20154;&#24037;&#32534;&#20889;&#25110;LM&#25506;&#32034;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#25104;&#26412;&#39640;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#8220;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;RL&#26469;&#20248;&#21270;&#29616;&#26377;&#30340;&#20247;&#21253;&#21644;&#20114;&#32852;&#32593;&#25968;&#25454;&#19978;&#30340;LM&#25928;&#29992;&#21527;&#65311;&#8221;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21097;&#20313;&#21320;&#39184;&#24378;&#21270;&#23398;&#20064;&#65288;LoL-RL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#29992;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#26469;&#23398;&#20064;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20316;&#20026;1&#27493;RL&#28216;&#25103;&#12290; LoL-RL&#21487;&#20197;&#24494;&#35843;LM&#65292;&#20197;&#20248;&#21270;&#20219;&#24847;&#22522;&#20110;&#20998;&#31867;&#22120;&#25110;&#20154;&#23450;&#20041;&#30340;&#25928;&#29992;&#20989;&#25968;&#30340;&#20219;&#20309;&#24207;&#21015;&#21040;&#24207;&#21015;&#25968;&#25454;&#12290;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#27169;&#22411;&#30340;&#20116;&#20010;&#19981;&#21516;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Improving language model generations according to some user-defined quality or style constraints is challenging. Typical approaches include learning on additional human-written data, filtering ``low-quality'' data using heuristics and/or using reinforcement learning with human feedback (RLHF). However, filtering can remove valuable training signals, whereas data collection and RLHF constantly require additional human-written or LM exploration data which can be costly to obtain. A natural question to ask is ``Can we leverage RL to optimize LM utility on existing crowd-sourced and internet data?''  To this end, we present Left-over Lunch RL (LoL-RL), a simple training algorithm that uses offline policy gradients for learning language generation tasks as a 1-step RL game. LoL-RL can finetune LMs to optimize arbitrary classifier-based or human-defined utility functions on any sequence-to-sequence data. Experiments with five different language generation tasks using models of varying sizes 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#37325;&#23450;&#20041;&#24314;&#27169;&#65288;MDM&#65289;&#20219;&#21153;&#65292;&#23558;&#30446;&#26631;&#35789;&#30340;&#25152;&#26377;&#19978;&#19979;&#25991;&#21644;&#23450;&#20041;&#27719;&#38598;&#22312;&#19968;&#36215;&#65292;&#30456;&#27604;&#21333;&#20010;&#23450;&#20041;&#24314;&#27169;&#65288;SDM&#65289;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#22312;&#39044;&#35757;&#32451;&#20219;&#21153;&#20013;&#21487;&#20197;&#25552;&#39640;SDM&#30340;&#24615;&#33021;&#65292;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#26377;&#21487;&#27604;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14717</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#37325;&#23450;&#20041;&#24314;&#27169;&#26469;&#25366;&#25496;&#19978;&#19979;&#25991;&#21644;&#23450;&#20041;&#20043;&#38388;&#30340;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Exploiting Correlations Between Contexts and Definitions with Multiple Definition Modeling. (arXiv:2305.14717v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#37325;&#23450;&#20041;&#24314;&#27169;&#65288;MDM&#65289;&#20219;&#21153;&#65292;&#23558;&#30446;&#26631;&#35789;&#30340;&#25152;&#26377;&#19978;&#19979;&#25991;&#21644;&#23450;&#20041;&#27719;&#38598;&#22312;&#19968;&#36215;&#65292;&#30456;&#27604;&#21333;&#20010;&#23450;&#20041;&#24314;&#27169;&#65288;SDM&#65289;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#22312;&#39044;&#35757;&#32451;&#20219;&#21153;&#20013;&#21487;&#20197;&#25552;&#39640;SDM&#30340;&#24615;&#33021;&#65292;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#26377;&#21487;&#27604;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#20041;&#24314;&#27169;&#26159;&#39640;&#32423;&#33258;&#28982;&#35821;&#35328;&#24212;&#29992;&#31243;&#24207;&#65288;&#20363;&#22914;&#29702;&#35299;&#21644;&#23545;&#35805;&#65289;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#30446;&#21069;&#65292;&#36825;&#31181;&#27169;&#22411;&#20391;&#37325;&#20110;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#20026;&#30446;&#26631;&#35789;&#25110;&#30701;&#35821;&#29983;&#25104;&#19968;&#20010;&#23450;&#20041;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#21333;&#20010;&#23450;&#20041;&#24314;&#27169;&#65288;SDM&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#33021;&#20805;&#20998;&#22320;&#27169;&#25311;&#21333;&#35789;&#30340;&#19981;&#21516;&#19978;&#19979;&#25991;&#21644;&#23450;&#20041;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#20026;SDM&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#38598;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#19987;&#19994;&#30693;&#35782;&#21644;&#21162;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#31216;&#20026;&#22810;&#37325;&#23450;&#20041;&#24314;&#27169;&#65288;MDM&#65289;&#30340;&#26032;&#20219;&#21153;&#65292;&#23558;&#30446;&#26631;&#35789;&#30340;&#25152;&#26377;&#19978;&#19979;&#25991;&#21644;&#23450;&#20041;&#27719;&#38598;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#21019;&#24314;&#27169;&#22411;&#20197;&#21450;&#22810;&#20010;&#35757;&#32451;&#38598;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#30340;&#20415;&#21033;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#24182;&#20998;&#26512;&#20102;MDM&#30340;&#22909;&#22788;&#65292;&#21253;&#25324;&#20351;&#29992;MDM&#20316;&#20026;&#39044;&#35757;&#32451;&#20219;&#21153;&#20197;&#25552;&#39640;SDM&#30340;&#24615;&#33021;&#21450;&#20854;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#21487;&#27604;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Definition modeling is an important task in advanced natural language applications such as understanding and conversation. Since its introduction, it focus on generating one definition for a target word or phrase in a given context, which we refer to as Single Definition Modeling (SDM). However, this approach does not adequately model the correlations and patterns among different contexts and definitions of words. In addition, the creation of a training dataset for SDM requires significant human expertise and effort. In this paper, we carefully design a new task called Multiple Definition Modeling (MDM) that pool together all contexts and definition of target words. We demonstrate the ease of creating a model as well as multiple training sets automatically. % In the experiments, we demonstrate and analyze the benefits of MDM, including improving SDM's performance by using MDM as the pretraining task and its comparable performance in the zero-shot setting.
&lt;/p&gt;</description></item><item><title>GlobalBench&#26159;&#19968;&#20010;&#21160;&#24577;&#36861;&#36394;&#25152;&#26377;&#35821;&#35328;NLP&#25968;&#25454;&#38598;&#36827;&#23637;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#25552;&#39640;&#20844;&#24179;&#35821;&#35328;&#25216;&#26415;&#30340;&#20840;&#29699;&#21457;&#23637;&#65292;&#24182;&#30830;&#23450;&#26368;&#38656;&#35201;&#26381;&#21153;&#30340;&#35821;&#35328;&#65292;&#30446;&#21069;&#36825;&#20123;&#35821;&#35328;&#20027;&#35201;&#26159;&#38750;&#27954;&#21644;&#32654;&#27954;&#21407;&#20303;&#27665;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2305.14716</link><description>&lt;p&gt;
GlobalBench&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20840;&#29699;&#36827;&#23637;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
GlobalBench: A Benchmark for Global Progress in Natural Language Processing. (arXiv:2305.14716v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14716
&lt;/p&gt;
&lt;p&gt;
GlobalBench&#26159;&#19968;&#20010;&#21160;&#24577;&#36861;&#36394;&#25152;&#26377;&#35821;&#35328;NLP&#25968;&#25454;&#38598;&#36827;&#23637;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#25552;&#39640;&#20844;&#24179;&#35821;&#35328;&#25216;&#26415;&#30340;&#20840;&#29699;&#21457;&#23637;&#65292;&#24182;&#30830;&#23450;&#26368;&#38656;&#35201;&#26381;&#21153;&#30340;&#35821;&#35328;&#65292;&#30446;&#21069;&#36825;&#20123;&#35821;&#35328;&#20027;&#35201;&#26159;&#38750;&#27954;&#21644;&#32654;&#27954;&#21407;&#20303;&#27665;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;NLP&#26377;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;NLP&#31995;&#32479;&#34920;&#29616;&#19978;&#20173;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;&#36825;&#20123;&#24046;&#24322;&#21487;&#33021;&#26159;&#30001;&#20110;&#36164;&#28304;&#20998;&#37197;&#19981;&#22343;&#21644;&#23545;&#19981;&#36275;&#36164;&#28304;&#35821;&#35328;&#30340;&#24037;&#20316;&#28608;&#21169;&#19981;&#22815;&#20248;&#21270;&#25152;&#23548;&#33268;&#30340;&#12290;&#20026;&#20102;&#36319;&#36394;&#24182;&#36827;&#19968;&#27493;&#28608;&#21169;&#20844;&#24179;&#35821;&#35328;&#25216;&#26415;&#30340;&#20840;&#29699;&#21457;&#23637;&#65292;&#25105;&#20204;&#20171;&#32461;GlobalBench&#12290;&#20808;&#21069;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;&#26159;&#38745;&#24577;&#30340;&#65292;&#24182;&#19988;&#20851;&#27880;&#30340;&#26159;&#23569;&#25968;&#20219;&#21153;&#21644;&#23569;&#25968;&#35821;&#35328;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;GlobalBench&#26159;&#19968;&#20010;&#19981;&#26029;&#25193;&#22823;&#30340;&#38598;&#21512;&#65292;&#26088;&#22312;&#21160;&#24577;&#36319;&#36394;&#25152;&#26377;&#35821;&#35328;&#30340;&#25152;&#26377;NLP&#25968;&#25454;&#38598;&#30340;&#36827;&#23637;&#12290;&#38500;&#20102;&#20165;&#20165;&#27979;&#37327;&#20934;&#30830;&#24230;&#22806;&#65292;GlobalBench&#36824;&#36319;&#36394;&#20102;&#25152;&#26377;&#35821;&#35328;&#30340;&#39044;&#35745;&#27599;&#20154;&#25928;&#29992;&#21644;&#25216;&#26415;&#20844;&#24179;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#35270;&#35282;&#65292;&#20102;&#35299;&#35821;&#35328;&#25216;&#26415;&#22914;&#20309;&#20026;&#19990;&#30028;&#21508;&#22320;&#30340;&#20154;&#26381;&#21153;&#12290;&#27492;&#22806;&#65292;GlobalBench&#26088;&#22312;&#35782;&#21035;&#26368;&#38656;&#35201;&#26381;&#21153;&#30340;&#35821;&#35328;&#65292;&#24182;&#22870;&#21169;&#38024;&#23545;&#36825;&#20123;&#35821;&#35328;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#30446;&#21069;&#65292;GlobalBench&#20013;&#26368;&#38656;&#35201;&#26381;&#21153;&#30340;&#35821;&#35328;&#20027;&#35201;&#26159;&#38750;&#27954;&#21644;&#32654;&#27954;&#21407;&#20303;&#27665;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#35780;&#20272;&#24230;&#37327;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#23545;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#26696;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14711</link><description>&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65306;&#22522;&#20110;&#22270;&#20687;&#23383;&#24149;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Gender Biases in Automatic Evaluation Metrics: A Case Study on Image Captioning. (arXiv:2305.14711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#35780;&#20272;&#24230;&#37327;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#23545;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#26696;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#35780;&#20272;&#24230;&#37327;&#22312;&#22270;&#20687;&#23383;&#24149;&#31561;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#24050;&#32463;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#28145;&#20837;&#25506;&#35752;&#8212;&#8212;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#20250;&#32534;&#30721;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35780;&#20272;&#30446;&#30340;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#34920;&#29616;&#24182;&#28508;&#22312;&#22320;&#25918;&#22823;&#20559;&#35265;&#12290;&#26412;&#25991;&#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#65292;&#23545;&#27169;&#22411;&#35780;&#20272;&#24230;&#37327;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#21644;&#37327;&#21270;&#20102;&#19981;&#21516;&#35780;&#20272;&#24230;&#37327;&#20013;&#20851;&#20110;&#32844;&#19994;&#12289;&#27963;&#21160;&#21644;&#29289;&#20307;&#27010;&#24565;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#36825;&#20123;&#26377;&#20559;&#35265;&#30340;&#24230;&#37327;&#24102;&#26469;&#30340;&#36127;&#38754;&#21518;&#26524;&#65292;&#27604;&#22914;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#20559;&#21521;&#26377;&#20559;&#35265;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21521;&#29983;&#25104;&#27169;&#22411;&#20256;&#25773;&#20559;&#35265;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained model-based evaluation metrics have demonstrated strong performance with high correlations with human judgments in various natural language generation tasks such as image captioning. Despite the impressive results, their impact on fairness is under-explored -- it is widely acknowledged that pretrained models can encode societal biases, and utilizing them for evaluation purposes may inadvertently manifest and potentially amplify biases. In this paper, we conduct a systematic study in gender biases of model-based evaluation metrics with a focus on image captioning tasks. Specifically, we first identify and quantify gender biases in different evaluation metrics regarding profession, activity, and object concepts. Then, we demonstrate the negative consequences of using these biased metrics, such as favoring biased generation models in deployment and propagating the biases to generation models through reinforcement learning. We also present a simple but effective alternative to r
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23384;&#22312;&#21518;&#38376;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21482;&#38656;&#27880;&#20837;&#26497;&#23569;&#37327;&#30340;&#24694;&#24847;&#25351;&#20196;&#20415;&#21487;&#27704;&#20037;&#25511;&#21046;&#27169;&#22411;&#34892;&#20026;&#65292;&#19988;&#38590;&#20197;&#34987;&#20462;&#22797;&#65292;&#38656;&#35201;&#26356;&#21152;&#20581;&#20840;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.14710</link><description>&lt;p&gt;
&#35757;&#32451;&#25351;&#20196;&#20316;&#20026;&#21518;&#38376;: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;&#30340;&#21518;&#38376;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models. (arXiv:2305.14710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14710
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23384;&#22312;&#21518;&#38376;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21482;&#38656;&#27880;&#20837;&#26497;&#23569;&#37327;&#30340;&#24694;&#24847;&#25351;&#20196;&#20415;&#21487;&#27704;&#20037;&#25511;&#21046;&#27169;&#22411;&#34892;&#20026;&#65292;&#19988;&#38590;&#20197;&#34987;&#20462;&#22797;&#65292;&#38656;&#35201;&#26356;&#21152;&#20581;&#20840;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20854;&#30446;&#30340;&#26159;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#35813;&#22521;&#35757;&#33539;&#20363;&#30456;&#20851;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21482;&#38656;&#22312;&#25104;&#21315;&#19978;&#19975;&#30340;&#25968;&#25454;&#20013;&#27880;&#20837;&#26497;&#23569;&#37327;&#30340;&#24694;&#24847;&#25351;&#20196;&#65292;&#20415;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#27602;&#21270;&#26469;&#25511;&#21046;&#27169;&#22411;&#34892;&#20026;&#65292;&#29978;&#33267;&#26080;&#38656;&#20462;&#25913;&#25968;&#25454;&#23454;&#20363;&#25110;&#26631;&#31614;&#26412;&#36523;&#12290;&#36890;&#36807;&#36825;&#31181;&#25351;&#20196;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#22235;&#20010;&#24120;&#29992;&#30340; NLP &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#36229;&#36807;90% &#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#24341;&#36215;&#26131;&#20110;&#36716;&#31227;&#21040; 15 &#31181;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#25345;&#20037;&#21518;&#38376;&#12290;&#36825;&#31181;&#25915;&#20987;&#36824;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#26377;&#27602;&#25351;&#20196;&#12290;&#26368;&#21518;&#65292;&#35813;&#25915;&#20987;&#26174;&#31034;&#20986;&#23545;&#29616;&#26377;&#25512;&#29702;&#26102;&#38450;&#24481;&#30340;&#25269;&#25239;&#21147;&#12290;&#36825;&#20123;&#21457;&#29616;&#20984;&#26174;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#38656;&#35201;&#26356;&#20026;&#20581;&#20840;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned models are trained on crowdsourcing datasets with task instructions to achieve superior performance. However, in this work we raise security concerns about this training paradigm. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions among thousands of gathered data and control model behavior through data poisoning, without even the need of modifying data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets, and cause persistent backdoors that are easily transferred to 15 diverse datasets zero-shot. In this way, the attacker can directly apply poisoned instructions designed for one dataset on many other datasets. Moreover, the poisoned model cannot be cured by continual learning. Lastly, instruction attacks show resistance to existing inference-time defense. These findings highlight the need for more robust defens
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#39564;&#35777;&#32773;&#19988;&#19981;&#20570;&#39046;&#22495;&#20551;&#35774;&#30340;&#20027;&#24352;&#26657;&#27491;&#31995;&#32479;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31185;&#23398;&#20107;&#23454;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#25552;&#31034;&#26041;&#27861;&#21644;&#20027;&#24352;&#24863;&#30693;&#30340;&#35299;&#30721;&#36807;&#31243;&#26469;&#25552;&#39640;&#26657;&#27491;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.14707</link><description>&lt;p&gt;
&#23398;&#29983;&#36229;&#36234;&#20102;&#22823;&#24072;&#65306;&#22522;&#20110;GPT-3&#30340;&#31185;&#23398;&#20107;&#23454;&#38169;&#35823;&#26657;&#27491;&#26041;&#27861;&#30340;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
The student becomes the master: Matching GPT3 on Scientific Factual Error Correction. (arXiv:2305.14707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#39564;&#35777;&#32773;&#19988;&#19981;&#20570;&#39046;&#22495;&#20551;&#35774;&#30340;&#20027;&#24352;&#26657;&#27491;&#31995;&#32479;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31185;&#23398;&#20107;&#23454;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#25552;&#31034;&#26041;&#27861;&#21644;&#20027;&#24352;&#24863;&#30693;&#30340;&#35299;&#30721;&#36807;&#31243;&#26469;&#25552;&#39640;&#26657;&#27491;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21019;&#24314;&#38169;&#35823;&#26657;&#27491;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#26497;&#39640;&#65292;&#22823;&#22810;&#25968;&#20107;&#23454;&#20027;&#24352;&#26657;&#27491;&#26041;&#27861;&#20381;&#36182;&#20110;&#24378;&#22823;&#30340;&#39564;&#35777;&#27169;&#22411;&#26469;&#25351;&#23548;&#26657;&#27491;&#36807;&#31243;&#12290;&#36825;&#23548;&#33268;&#22312;&#31185;&#23398;&#20107;&#23454;&#26657;&#27491;&#31561;&#39046;&#22495;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#22240;&#20026;&#22909;&#30340;&#39564;&#35777;&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#23384;&#22312;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#20570;&#39046;&#22495;&#20551;&#35774;&#19988;&#19981;&#38656;&#35201;&#39564;&#35777;&#32773;&#30340;&#20027;&#24352;&#26657;&#27491;&#31995;&#32479;&#65292;&#20294;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#24615;&#33021; - &#22312;SciFact&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;94&#65285;&#30340;&#20462;&#27491;&#20934;&#30830;&#24615;&#65292;&#22312;SciFact-Open&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;62.5&#65285;&#30340;&#20462;&#27491;&#20934;&#30830;&#24615;&#65292;&#20998;&#21035;&#27604;&#19979;&#19968;&#20010;&#26368;&#22909;&#30340;&#26041;&#27861;&#39640;&#20986;0.5&#65285;&#21644;1.50&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLMs&#20013;&#30340;&#25552;&#31034;&#21151;&#33021;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#21019;&#24314;&#19968;&#20010;&#20016;&#23500;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#23436;&#20840;&#30417;&#30563;&#30340;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20027;&#24352;&#24863;&#30693;&#30340;&#35299;&#30721;&#36807;&#31243;&#26469;&#25552;&#39640;&#32416;&#27491;&#20027;&#24352;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29992;&#20110;&#21019;&#24314;&#25968;&#25454;&#38598;&#30340;LLM&#30456;&#31454;&#20105;&#65292;&#35777;&#26126;&#20102;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#35757;&#32451;&#25552;&#39640;&#31185;&#23398;&#20027;&#24352;&#26657;&#27491;&#20219;&#21153;&#24615;&#33021;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the prohibitively high cost of creating error correction datasets, most Factual Claim Correction methods rely on a powerful verification model to guide the correction process. This leads to a significant drop in performance in domains like Scientific Claim Correction, where good verification models do not always exist. In this work, we introduce a claim correction system that makes no domain assumptions and does not require a verifier but is able to outperform existing methods by an order of magnitude -- achieving 94% correction accuracy on the SciFact dataset, and 62.5% on the SciFact-Open dataset, compared to the next best methods 0.5% and 1.50% respectively. Our method leverages the power of prompting with LLMs during training to create a richly annotated dataset that can be used for fully supervised training and regularization. We additionally use a claim-aware decoding procedure to improve the quality of corrected claims. Our method is competitive with the very LLM that was
&lt;/p&gt;</description></item><item><title>Flan-MoE&#26159;&#19968;&#31181;&#25351;&#20196;&#35843;&#20248;&#30340;&#31232;&#30095;Mixture of Experts&#65288;MoE&#65289;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#23494;&#38598;&#27169;&#22411;&#65292;&#22312;&#25351;&#20196;&#24494;&#35843;&#21644;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#21518;&#22343;&#34920;&#29616;&#26356;&#22909;&#12290;&#26368;&#22823;&#27169;&#22411;Flan-MoE-32B&#30340;&#24615;&#33021;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;Flan-PaLM-62B&#65292;&#21516;&#26102;&#21482;&#21033;&#29992;&#20102;1/3&#30340;FLOPs&#12290;</title><link>http://arxiv.org/abs/2305.14705</link><description>&lt;p&gt;
Flan-MoE: &#36890;&#36807;&#31232;&#30095;Mixture of Experts&#25193;&#23637;&#25351;&#20196;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts. (arXiv:2305.14705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14705
&lt;/p&gt;
&lt;p&gt;
Flan-MoE&#26159;&#19968;&#31181;&#25351;&#20196;&#35843;&#20248;&#30340;&#31232;&#30095;Mixture of Experts&#65288;MoE&#65289;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#23494;&#38598;&#27169;&#22411;&#65292;&#22312;&#25351;&#20196;&#24494;&#35843;&#21644;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#21518;&#22343;&#34920;&#29616;&#26356;&#22909;&#12290;&#26368;&#22823;&#27169;&#22411;Flan-MoE-32B&#30340;&#24615;&#33021;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;Flan-PaLM-62B&#65292;&#21516;&#26102;&#21482;&#21033;&#29992;&#20102;1/3&#30340;FLOPs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#21644;&#24212;&#29992;&#38656;&#27714;&#23548;&#33268;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#26041;&#27861;&#30340;&#38656;&#27714;&#22686;&#21152;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;Instruction-Finetuned Sparse Mixture-of-Expert (MoE)&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;Flan-MoE&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#38024;&#23545;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#38598;&#36827;&#34892;MoE&#27169;&#22411;&#30340;&#24494;&#35843;&#20250;&#23548;&#33268;&#24615;&#33021;&#19981;&#22914;&#30456;&#21516;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#23494;&#38598;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;Flan-MoE&#22312;&#22810;&#20010;&#23454;&#39564;&#35774;&#32622;&#19979;&#37117;&#20248;&#20110;&#23494;&#38598;&#27169;&#22411;&#65306;&#20165;&#25351;&#20196;&#24494;&#35843;&#21644;&#25351;&#20196;&#24494;&#35843;&#21518;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#12290;&#36825;&#34920;&#26126;&#65292;&#25351;&#20196;&#24494;&#35843;&#26159;MoE&#27169;&#22411;&#30340;&#24517;&#35201;&#38454;&#27573;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26368;&#22823;&#27169;&#22411;Flan-MoE-32B&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;Flan-PaLM-62B&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21482;&#21033;&#29992;&#20102;1/3&#30340;FLOPs&#12290;Flan-MoE&#30340;&#25104;&#21151;&#40723;&#33310;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22823;&#35268;&#27169;&#12289;&#39640;&#24615;&#33021;&#35821;&#35328;&#27169;&#22411;&#30340;&#35774;&#35745;&#65292;&#23588;&#20854;&#26159;&#22312;&#20219;&#21153;&#26080;&#20851;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The explosive growth of language models and their applications have led to an increased demand for efficient and scalable methods. In this paper, we introduce Flan-MoE, a set of Instruction-Finetuned Sparse Mixture-of-Expert (MoE) models. We show that naively finetuning MoE models on a task-specific dataset (in other words, no instruction-finetuning) often yield worse performance compared to dense models of the same computational complexity. However, our Flan-MoE outperforms dense models under multiple experiment settings: instruction-finetuning only and instruction-finetuning followed by task-specific finetuning. This shows that instruction-finetuning is an essential stage for MoE models. Specifically, our largest model, Flan-MoE-32B, surpasses the performance of Flan-PaLM-62B on four benchmarks, while utilizing only one-third of the FLOPs. The success of Flan-MoE encourages rethinking the design of large-scale, high-performance language models, under the setting of task-agnostic lear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;Bradley-Terry-Luce&#27169;&#22411;&#23545;OpenAI&#20844;&#24067;&#30340;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#20013;&#25152;&#34164;&#21547;&#30340;&#22266;&#26377;&#20559;&#22909;&#65292;&#25552;&#20986;&#20102;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#31574;&#30053;&#65292;&#24182;&#20026;&#26500;&#24314;&#24179;&#34913;&#30340;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.14702</link><description>&lt;p&gt;
&#36890;&#36807;GPT-4&#20998;&#26512;&#24433;&#21709;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#30340;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Analyzing Influential Factors in Human Preference Judgments via GPT-4. (arXiv:2305.14702v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;Bradley-Terry-Luce&#27169;&#22411;&#23545;OpenAI&#20844;&#24067;&#30340;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#20013;&#25152;&#34164;&#21547;&#30340;&#22266;&#26377;&#20559;&#22909;&#65292;&#25552;&#20986;&#20102;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#31574;&#30053;&#65292;&#24182;&#20026;&#26500;&#24314;&#24179;&#34913;&#30340;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#22312;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#21644;&#35780;&#20272;&#33258;&#21160;&#25688;&#35201;&#24230;&#37327;&#26041;&#38754;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20123;&#20559;&#22909;&#21028;&#26029;&#30340;&#20849;&#21516;&#24433;&#21709;&#21644;&#22240;&#32032;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#31561;&#38382;&#39064;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20173;&#36739;&#20026;&#26377;&#38480;&#12290;&#26412;&#25991;&#21033;&#29992;Bradley-Terry-Luce&#27169;&#22411;&#23545;OpenAI&#20844;&#24067;&#30340;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#35782;&#21035;&#20102;&#21487;&#33021;&#24433;&#21709;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#20013;&#25152;&#34164;&#21547;&#30340;&#22266;&#26377;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#20102;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#31574;&#30053;&#65292;&#26368;&#21518;&#23545;&#20110;&#22914;&#20309;&#26500;&#24314;&#24179;&#34913;&#30340;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pairwise human judgments are pivotal in guiding large language models (LLMs) to generate outputs that align with human preferences. They are also often used in summarization evaluation, complementing existing automatic metrics. Despite their significance, however, there has been limited research probing these pairwise human judgments. The collective impact and respective weights of factors such as informativeness, coherence, fluency, and factual consistency remain elusive. The impact of hidden factors on the final judgment is also unclear. In this paper, we conduct an in-depth examination of a dataset of pairwise human judgments released by OpenAI. Utilizing the Bradley-Terry-Luce model, we identify key factors that could potentially influence human judgments. Our research uncovers the inherent preferences embedded in human judgments and suggests strategies to boost sample efficiency. Finally, we provide insights on the construction of balanced datasets for human judgment evaluations, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#35265;&#27880;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#33258;&#28982;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#20855;&#26377;&#28789;&#27963;&#24615;&#20197;&#25512;&#24191;&#21040;&#26032;&#30340;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.14701</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#23558;&#36125;&#21494;&#26031;&#20808;&#39564;&#27880;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#24555;&#36895;&#35821;&#35328;&#23398;&#20064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling rapid language learning by distilling Bayesian priors into artificial neural networks. (arXiv:2305.14701v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#35265;&#27880;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#33258;&#28982;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#20855;&#26377;&#28789;&#27963;&#24615;&#20197;&#25512;&#24191;&#21040;&#26032;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#20174;&#26497;&#23569;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#35821;&#35328;&#12290;&#24320;&#21457;&#33021;&#22815;&#35299;&#37322;&#36825;&#31181;&#33021;&#21147;&#30340;&#35745;&#31639;&#27169;&#22411;&#19968;&#30452;&#26159;&#35748;&#30693;&#31185;&#23398;&#30340;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#36125;&#21494;&#26031;&#27169;&#22411;&#23558;&#25351;&#23548;&#27010;&#25324;&#30340;&#24378;&#28872;&#24402;&#32435;&#20559;&#35265;&#22240;&#32032;&#32467;&#21512;&#36215;&#26469;&#65292;&#25104;&#21151;&#22320;&#35299;&#37322;&#20102;&#20154;&#31867;&#22914;&#20309;&#20174;&#23569;&#25968;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#20363;&#23376;&#20013;&#36827;&#34892;&#24402;&#32435;&#65292;&#20294;&#36890;&#24120;&#36807;&#20110;&#20005;&#26684;&#32780;&#26080;&#27861;&#24212;&#29992;&#20110;&#26356;&#33258;&#28982;&#30340;&#25968;&#25454;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#28789;&#27963;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#24456;&#22909;&#22320;&#20174;&#33258;&#28982;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#20294;&#38656;&#35201;&#27604;&#20154;&#31867;&#25509;&#25910;&#21040;&#30340;&#26356;&#22810;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#20559;&#35265;&#27880;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20174;&#26377;&#38480;&#30340;&#33258;&#28982;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#26159;&#21487;&#33021;&#30340;&#12290;&#19982;&#36125;&#21494;&#26031;&#27169;&#22411;&#19968;&#26679;&#65292;&#32467;&#26524;&#31995;&#32479;&#21487;&#20197;&#20174;&#23569;&#37327;&#30340;&#31034;&#20363;&#20013;&#23398;&#20064;&#24418;&#24335;&#35821;&#35328;&#27169;&#24335;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#28789;&#27963;&#24615;&#65292;&#20197;&#25512;&#24191;&#21040;&#26032;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can learn languages from remarkably little experience. Developing computational models that explain this ability has been a major challenge in cognitive science. Bayesian models that build in strong inductive biases factors that guide generalization - have been successful at explaining how humans might generalize from few examples in controlled settings but are usually too restrictive to be tractably applied to more naturalistic data. By contrast, neural networks have flexible representations that allow them to learn well from naturalistic data but require many more examples than humans receive. We show that learning from limited naturalistic data is possible with an approach that combines the strong inductive biases of a Bayesian model with the flexible representations of a neural network. This approach works by distilling a Bayesian model's biases into a neural network. Like a Bayesian model, the resulting system can learn formal linguistic patterns from a small number of ex
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#22806;&#37096;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;SELFOOD&#65292;&#36890;&#36807;&#23558;&#22806;&#37096;&#26679;&#26412;&#26816;&#27979;&#35270;&#20026;&#25991;&#26723;&#26631;&#31614;&#20869;&#37096;&#25490;&#24207;&#38382;&#39064;&#65292;&#20351;&#29992;&#27604;&#36739;&#25490;&#24207;&#25439;&#22833;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#30340;&#22806;&#37096;&#26679;&#26412;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.14696</link><description>&lt;p&gt;
SELFOOD: &#22522;&#20110;&#33258;&#23398;&#20064;&#25490;&#24207;&#30340;&#26080;&#30417;&#30563;&#22806;&#37096;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SELFOOD: Self-Supervised Out-Of-Distribution Detection via Learning to Rank. (arXiv:2305.14696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14696
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#22806;&#37096;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;SELFOOD&#65292;&#36890;&#36807;&#23558;&#22806;&#37096;&#26679;&#26412;&#26816;&#27979;&#35270;&#20026;&#25991;&#26723;&#26631;&#31614;&#20869;&#37096;&#25490;&#24207;&#38382;&#39064;&#65292;&#20351;&#29992;&#27604;&#36739;&#25490;&#24207;&#25439;&#22833;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#30340;&#22806;&#37096;&#26679;&#26412;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#20998;&#31867;&#22120;&#24120;&#24120;&#23384;&#22312;&#26657;&#20934;&#19981;&#33391;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#34892;&#22806;&#37096;&#26679;&#26412;&#26816;&#27979;&#12290;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#22806;&#37096;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#38656;&#35201;&#26114;&#36149;&#30340;&#25163;&#21160;&#27880;&#37322;&#20869;&#37096;&#21644;&#22806;&#37096;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#27880;&#37322;&#29942;&#39048;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SELFOOD&#65292;&#36825;&#26159;&#19968;&#31181;&#20165;&#38656;&#35201;&#20869;&#37096;&#26679;&#26412;&#20316;&#20026;&#30417;&#30563;&#30340;&#33258;&#30417;&#30563;&#22806;&#37096;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#22806;&#37096;&#26679;&#26412;&#26816;&#27979;&#30475;&#20316;&#19968;&#20010;&#25991;&#26723;&#20043;&#38388;&#26631;&#31614;&#20869;&#37096;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#27604;&#36739;&#25490;&#24207;&#25439;&#22833;&#65288;IDIL loss&#65289;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;&#20869;&#37096;&#26679;&#26412;&#25991;&#26723;&#21450;&#20854;&#26631;&#31614;&#65292;&#25105;&#20204;&#35757;&#32451;&#20998;&#31867;&#22120;&#23558;&#23646;&#20110;&#35813;&#26631;&#31614;&#30340;&#25991;&#26723;&#30340;softmax&#20998;&#25968;&#25490;&#21517;&#25490;&#22312;&#20854;&#20182;&#26631;&#31614;&#25991;&#26723;&#30340;&#20998;&#25968;&#20043;&#19978;&#12290;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#19981;&#21516;&#65292;&#24403;&#36798;&#21040;&#26399;&#26395;&#30340;&#32622;&#20449;&#24230;&#25490;&#21517;&#26102;&#65292;&#25105;&#20204;&#30340;IDIL&#25439;&#22833;&#20989;&#25968;&#23558;&#36798;&#21040;&#38646;&#24182;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#26469;&#38477;&#20302;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural classifiers trained with cross-entropy loss (CE loss) often suffer from poor calibration, necessitating the task of out-of-distribution (OOD) detection. Traditional supervised OOD detection methods require expensive manual annotation of in-distribution and OOD samples. To address the annotation bottleneck, we introduce SELFOOD, a self-supervised OOD detection method that requires only in-distribution samples as supervision. We cast OOD detection as an inter-document intra-label (IDIL) ranking problem and train the classifier with our pairwise ranking loss, referred to as IDIL loss. Specifically, given a set of in-distribution documents and their labels, for each label, we train the classifier to rank the softmax scores of documents belonging to that label to be higher than the scores of documents that belong to other labels. Unlike CE loss, our IDIL loss function reaches zero when the desired confidence ranking is achieved and gradients are backpropagated to decrease probab
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#24182;&#25552;&#20379;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#65292;&#20174;&#32780;&#20943;&#23569;&#20559;&#35265;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20284;&#23454;&#20307;&#30340;&#20849;&#21516;&#39044;&#27979;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.14695</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#65306;&#19968;&#31181;&#22240;&#26524;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Causal View of Entity Bias in (Large) Language Models. (arXiv:2305.14695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14695
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#24182;&#25552;&#20379;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#65292;&#20174;&#32780;&#20943;&#23569;&#20559;&#35265;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20284;&#23454;&#20307;&#30340;&#20849;&#21516;&#39044;&#27979;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#20559;&#35265;&#24191;&#27867;&#24433;&#21709;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23548;&#33268;&#23427;&#20204;&#36807;&#24230;&#20381;&#36182;&#65288;&#26377;&#20559;&#35265;&#30340;&#65289;&#21442;&#25968;&#21270;&#30693;&#35782;&#26469;&#36827;&#34892;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#23613;&#31649;&#22240;&#26524;&#30456;&#20851;&#30340;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#32531;&#35299;&#23454;&#20307;&#20559;&#35265;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#31934;&#30830;&#20272;&#35745;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#21442;&#25968;&#20173;&#28982;&#24456;&#22256;&#38590;&#65292;&#40657;&#30418;&#23376;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#26080;&#27861;&#35843;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#65292;&#20854;&#21442;&#25968;&#27604;&#36739;&#23481;&#26131;&#20272;&#35745;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#65292;&#20197;&#32531;&#35299;&#30333;&#30418;&#21644;&#40657;&#30418;&#35774;&#32622;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#12290;&#36825;&#31181;&#22240;&#26524;&#24178;&#39044;&#23558;&#21407;&#22987;&#23454;&#20307;&#19982;&#30456;&#37051;&#23454;&#20307;&#19968;&#36215;&#36827;&#34892;&#25200;&#21160;&#12290;&#36825;&#31181;&#24178;&#39044;&#20943;&#23569;&#20102;&#19982;&#21407;&#22987;&#23454;&#20307;&#30456;&#20851;&#30340;&#29305;&#23450;&#20559;&#21521;&#20449;&#24687;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;&#20102;&#26469;&#33258;&#31867;&#20284;&#23454;&#20307;&#30340;&#36275;&#22815;&#20849;&#21516;&#39044;&#27979;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity bias widely affects pretrained (large) language models, causing them to excessively rely on (biased) parametric knowledge to make unfaithful predictions. Although causality-inspired methods have shown great potential to mitigate entity bias, it is hard to precisely estimate the parameters of underlying causal models in practice. The rise of black-box LLMs also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits. To address these problems, we propose a specific structured causal model (SCM) whose parameters are comparatively easier to estimate. Building upon this SCM, we propose causal intervention techniques to mitigate entity bias for both white-box and black-box settings. The proposed causal intervention perturbs the original entity with neighboring entities. This intervention reduces specific biasing information pertaining to the original entity while still preserving sufficient common predictive information from similar entities. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#24615;&#26159;&#21542;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#30446;&#21069;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#26080;&#27861;&#20934;&#30830;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20010;&#24615;&#12290;&#20026;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#38656;&#35201;&#19987;&#38376;&#20026;LLM&#35774;&#35745;&#26032;&#30340;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.14693</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#25317;&#26377;&#20154;&#26684;?&#8212;&#8212;&#22312;&#27979;&#37327;LLM&#20013;&#30340;&#20010;&#24615;&#26102;&#65292;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#30340;&#36866;&#29992;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Have Large Language Models Developed a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs. (arXiv:2305.14693v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#24615;&#26159;&#21542;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#30446;&#21069;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#26080;&#27861;&#20934;&#30830;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20010;&#24615;&#12290;&#20026;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#38656;&#35201;&#19987;&#38376;&#20026;LLM&#35774;&#35745;&#26032;&#30340;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#21542;&#25317;&#26377;&#20154;&#26684;&#65311;&#31616;&#30701;&#30340;&#22238;&#31572;&#26159;&#8220;&#25105;&#20204;&#19981;&#30693;&#36947;&#65281;&#8221;&#12290;&#26412;&#25991;&#34920;&#26126;&#25105;&#20204;&#36824;&#27809;&#26377;&#21512;&#36866;&#30340;&#24037;&#20855;&#33021;&#22815;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#12290;&#20010;&#24615;&#26159;&#24433;&#21709;&#34892;&#20026;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#38543;&#30528;LLM&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#27169;&#25311;&#20154;&#31867;&#26234;&#33021;&#21644;&#34920;&#29616;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#38382;&#20986;&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#25317;&#26377;&#20102;&#20154;&#26684;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#20154;&#26684;&#27979;&#35797;&#26469;&#35780;&#20272;&#26426;&#22120;&#20154;&#26684;&#65292;&#36825;&#26159;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20154;&#31867;&#20010;&#24615;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;&#19968;&#20010;&#22522;&#26412;&#30340;&#20551;&#35774;&#26159;&#65292;&#20154;&#31867;&#20010;&#24615;&#27979;&#35797;&#21487;&#20197;&#20934;&#30830;&#22320;&#27979;&#37327;&#26426;&#22120;&#20154;&#26684;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20116;&#20010;&#19981;&#21516;&#23610;&#23544;&#33539;&#22260;&#65288;&#20174;1.5B&#21040;30B&#65289;&#30340;LLM&#20013;&#30340;&#20010;&#24615;&#28014;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36873;&#39033;&#39034;&#24207;&#23545;&#31216;&#24615;&#21407;&#21017;&#20316;&#20026;&#36825;&#20123;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#21487;&#38752;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#22312;&#36825;&#20010;&#26465;&#20214;&#19979;&#65292;&#31572;&#39064;&#21644;&#20182;&#20204;&#30340;&#25490;&#24207;&#19981;&#24212;&#24433;&#21709;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#33258;&#25105;&#35780;&#20272;&#20010;&#24615;&#27979;&#35797;&#23545;LLM&#26469;&#35828;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24120;&#29992;&#30340;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#23545;&#20154;&#31867;&#20855;&#26377;&#20559;&#35265;&#65292;&#27809;&#26377;&#32771;&#34385;&#21040;LLM&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#20026;&#20102;&#20419;&#36827;LLM&#20010;&#24615;&#27979;&#37327;&#30340;&#26410;&#26469;&#30740;&#31350;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#24320;&#21457;&#19987;&#38376;&#20026;LLM&#35774;&#35745;&#30340;&#26032;&#30340;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Have Large Language Models (LLMs) developed a personality? The short answer is a resounding "We Don't Know!". In this paper, we show that we do not yet have the right tools to measure personality in language models. Personality is an important characteristic that influences behavior. As LLMs emulate human-like intelligence and performance in various tasks, a natural question to ask is whether these models have developed a personality. Previous works have evaluated machine personality through self-assessment personality tests, which are a set of multiple-choice questions created to evaluate personality in humans. A fundamental assumption here is that human personality tests can accurately measure personality in machines. In this paper, we investigate the emergence of personality in five LLMs of different sizes ranging from 1.5B to 30B. We propose the Option-Order Symmetry property as a necessary condition for the reliability of these self-assessment tests. Under this condition, the answ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#8220;&#19987;&#23478;&#25552;&#31034;&#8221;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26480;&#20986;&#30340;&#19987;&#23478;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#33258;&#21160;&#29983;&#25104;&#27599;&#20010;&#25351;&#20196;&#30340;&#35814;&#32454;&#21644;&#23450;&#21046;&#30340;&#19987;&#23478;&#36523;&#20221;&#25551;&#36848;&#65292;&#24182;&#35201;&#27714;&#27169;&#22411;&#26681;&#25454;&#36825;&#20123;&#25552;&#31034;&#25552;&#20379;&#31572;&#26696;&#12290;&#22522;&#20110;&#36825;&#31181;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24320;&#28304;&#32842;&#22825;&#21161;&#25163;ExpertLLaMA&#65292;&#35813;&#21161;&#25163;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;96&#65285;&#30340;ChatGPT&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14688</link><description>&lt;p&gt;
&#19987;&#23478;&#25552;&#31034;&#65306;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26480;&#20986;&#30340;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
ExpertPrompting: Instructing Large Language Models to be Distinguished Experts. (arXiv:2305.14688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#8220;&#19987;&#23478;&#25552;&#31034;&#8221;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26480;&#20986;&#30340;&#19987;&#23478;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#33258;&#21160;&#29983;&#25104;&#27599;&#20010;&#25351;&#20196;&#30340;&#35814;&#32454;&#21644;&#23450;&#21046;&#30340;&#19987;&#23478;&#36523;&#20221;&#25551;&#36848;&#65292;&#24182;&#35201;&#27714;&#27169;&#22411;&#26681;&#25454;&#36825;&#20123;&#25552;&#31034;&#25552;&#20379;&#31572;&#26696;&#12290;&#22522;&#20110;&#36825;&#31181;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24320;&#28304;&#32842;&#22825;&#21161;&#25163;ExpertLLaMA&#65292;&#35813;&#21161;&#25163;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;96&#65285;&#30340;ChatGPT&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#20197;&#36866;&#24403;&#30340;&#25552;&#31034;&#26041;&#24335;&#36827;&#34892;&#22788;&#29702;&#65292;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22238;&#31572;&#36136;&#37327;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19987;&#23478;&#25552;&#31034;&#65292;&#20197;&#24341;&#21457;LLMs&#20316;&#20026;&#26480;&#20986;&#19987;&#23478;&#22238;&#31572;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#33258;&#21160;&#29983;&#25104;&#27599;&#20010;&#29305;&#23450;&#25351;&#20196;&#30340;&#35814;&#32454;&#21644;&#23450;&#21046;&#30340;&#19987;&#23478;&#36523;&#20221;&#25551;&#36848;&#65292;&#28982;&#21518;&#35201;&#27714;LLMs&#26681;&#25454;&#36825;&#31181;&#20195;&#29702;&#20154;&#32972;&#26223;&#25552;&#20379;&#31572;&#26696;&#12290;&#22522;&#20110;&#36825;&#31181;&#22686;&#24378;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#29983;&#25104;&#20102;&#19968;&#32452;&#26032;&#30340;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#31454;&#20105;&#24615;&#30340;&#24320;&#28304;&#32842;&#22825;&#21161;&#25163;ExpertLLaMA&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;GPT4&#30340;&#35780;&#20272;&#26174;&#31034;&#65306;1&#65289;&#19987;&#23478;&#25968;&#25454;&#30340;&#36136;&#37327;&#26174;&#33879;&#39640;&#20110;&#26222;&#36890;&#31572;&#26696;&#65292;2&#65289;ExpertLLaMA&#32988;&#36807;&#29616;&#26377;&#30340;&#24320;&#28304;&#23545;&#25163;&#65292;&#23454;&#29616;&#20102;ChatGPT&#33021;&#21147;&#30340;96&#65285;&#12290;&#25152;&#26377;&#25968;&#25454;&#21644;ExpertLLaMA&#27169;&#22411;&#23558;&#22312;\url{https://github.com/OFA-Sys/Exp}&#19978;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
The answering quality of an aligned large language model (LLM) can be drastically improved if treated with proper crafting of prompts. In this paper, we propose ExpertPrompting to elicit the potential of LLMs to answer as distinguished experts. We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background. Based on this augmented prompting strategy, we produce a new set of instruction-following data using GPT-3.5, and train a competitive open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) ExpertLLaMA outperforms existing open-source opponents and achieves 96\% of the original ChatGPT's capability. All data and the ExpertLLaMA model will be made publicly available at \url{https://github.com/OFA-Sys/Exp
&lt;/p&gt;</description></item><item><title>TACR&#26159;&#19968;&#20010;&#29992;&#20110;&#28151;&#21512;&#25991;&#26412;&#21644;&#34920;&#26684;QA&#22330;&#26223;&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#34920;&#26684;-&#38382;&#39064;&#38388;&#30340;&#23545;&#40784;&#22686;&#24378;&#21333;&#20803;&#36873;&#25321;&#26469;&#26816;&#32034;&#32454;&#31890;&#24230;&#35777;&#25454;&#65292;&#24182;&#23558;&#21253;&#21547;&#36873;&#23450;&#21333;&#20803;&#30340;&#34892;&#20316;&#20026;&#19978;&#19979;&#25991;&#36827;&#34892;&#22238;&#31572;&#25512;&#29702;&#65292;&#22312;HybridQA&#21644;WikiTableQuestions&#65288;WTQ&#65289;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14682</link><description>&lt;p&gt;
TACR&#65306;&#22522;&#20110;&#34920;&#26684;&#23545;&#40784;&#30340;&#28151;&#21512;&#38382;&#31572;&#20013;&#30340;&#21333;&#20803;&#36873;&#25321;&#21644;&#25512;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TACR: A Table-alignment-based Cell-selection and Reasoning Model for Hybrid Question-Answering. (arXiv:2305.14682v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14682
&lt;/p&gt;
&lt;p&gt;
TACR&#26159;&#19968;&#20010;&#29992;&#20110;&#28151;&#21512;&#25991;&#26412;&#21644;&#34920;&#26684;QA&#22330;&#26223;&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#34920;&#26684;-&#38382;&#39064;&#38388;&#30340;&#23545;&#40784;&#22686;&#24378;&#21333;&#20803;&#36873;&#25321;&#26469;&#26816;&#32034;&#32454;&#31890;&#24230;&#35777;&#25454;&#65292;&#24182;&#23558;&#21253;&#21547;&#36873;&#23450;&#21333;&#20803;&#30340;&#34892;&#20316;&#20026;&#19978;&#19979;&#25991;&#36827;&#34892;&#22238;&#31572;&#25512;&#29702;&#65292;&#22312;HybridQA&#21644;WikiTableQuestions&#65288;WTQ&#65289;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#34920;&#26684;&#21644;&#19982;&#34920;&#26684;&#21333;&#20803;&#38142;&#25509;&#30340;&#27573;&#33853;&#36827;&#34892;&#25512;&#29702;&#30340;&#28151;&#21512;&#38382;&#31572;&#65288;HQA&#65289;&#24050;&#32463;&#24471;&#21040;&#20102;&#26174;&#30528;&#30340;&#30740;&#31350;&#12290;&#22312;HQA&#21644;&#20854;&#20182;&#27573;&#33853;-&#34920;&#26684;QA&#25968;&#25454;&#38598;&#20013;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#26159;&#36890;&#24120;&#19981;&#21487;&#33021;&#36941;&#21382;&#25152;&#26377;&#34920;&#26684;&#34892;&#12289;&#21015;&#21644;&#38142;&#25509;&#30340;&#27573;&#33853;&#26469;&#26816;&#32034;&#35777;&#25454;&#12290;&#36825;&#20351;&#24471;&#20197;&#21069;&#30340;&#30740;&#31350;&#24456;&#38590;&#23637;&#31034;&#20854;&#22312;&#26816;&#32034;&#31572;&#26696;&#26102;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#34920;&#26684;&#23545;&#40784;&#30340;&#21333;&#20803;&#36873;&#25321;&#21644;&#25512;&#29702;&#27169;&#22411;&#65288;TACR&#65289;&#29992;&#20110;&#28151;&#21512;&#25991;&#26412;&#21644;&#34920;&#26684;QA&#65292;&#24182;&#22312;HybridQA&#21644;WikiTableQuestions&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#22312;&#35777;&#25454;&#26816;&#32034;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#34920;&#26684;-&#38382;&#39064;&#23545;&#40784;&#22686;&#24378;&#30340;&#21333;&#20803;&#36873;&#25321;&#26041;&#27861;&#26469;&#26816;&#32034;&#32454;&#31890;&#24230;&#35777;&#25454;&#12290;&#22312;&#22238;&#31572;&#25512;&#29702;&#20013;&#65292;&#25105;&#20204;&#23558;&#21253;&#21547;&#36873;&#23450;&#21333;&#20803;&#30340;&#34892;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#21152;&#20837;&#19968;&#20010;QA&#27169;&#22359;&#12290;&#22312;HybridQA&#21644;WikiTableQuestions&#65288;WTQ&#65289;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TACR&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hybrid Question-Answering (HQA), which targets reasoning over tables and passages linked from table cells, has witnessed significant research in recent years. A common challenge in HQA and other passage-table QA datasets is that it is generally unrealistic to iterate over all table rows, columns, and linked passages to retrieve evidence. Such a challenge made it difficult for previous studies to show their reasoning ability in retrieving answers. To bridge this gap, we propose a novel Table-alignment-based Cell-selection and Reasoning model (TACR) for hybrid text and table QA, evaluated on the HybridQA and WikiTableQuestions datasets. In evidence retrieval, we design a table-question-alignment enhanced cell-selection method to retrieve fine-grained evidence. In answer reasoning, we incorporate a QA module that treats the row containing selected cells as context. Experimental results over the HybridQA and WikiTableQuestions (WTQ) datasets show that TACR achieves state-of-the-art results
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#34920;&#29616;&#26159;&#21542;&#20250;&#19979;&#38477;&#65292;&#21457;&#29616;Pythia&#27169;&#22411;&#22312;&#26356;&#22810;&#30340;&#21442;&#25968;&#19979;&#65292;&#23613;&#31649;&#25972;&#20307;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#24341;&#36848;&#37325;&#22797;&#21644;&#37325;&#26032;&#23450;&#20041;&#25968;&#23398;&#20004;&#20010;&#20219;&#21153;&#30340;&#35757;&#32451;&#20013;&#24615;&#33021;&#19979;&#38477;&#12290;&#38656;&#35201;&#22312;&#20219;&#20309;&#26102;&#20505;&#27979;&#35797;&#27169;&#22411;&#22312;&#30456;&#24212;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14681</link><description>&lt;p&gt;
&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36870;&#27604;&#20363;&#32553;&#25918;&#29616;&#35937;&#65306;&#32047;&#36184;&#36824;&#26159;&#36129;&#29486;&#65311;
&lt;/p&gt;
&lt;p&gt;
Emergent inabilities? Inverse scaling over the course of pretraining. (arXiv:2305.14681v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14681
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#34920;&#29616;&#26159;&#21542;&#20250;&#19979;&#38477;&#65292;&#21457;&#29616;Pythia&#27169;&#22411;&#22312;&#26356;&#22810;&#30340;&#21442;&#25968;&#19979;&#65292;&#23613;&#31649;&#25972;&#20307;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#24341;&#36848;&#37325;&#22797;&#21644;&#37325;&#26032;&#23450;&#20041;&#25968;&#23398;&#20004;&#20010;&#20219;&#21153;&#30340;&#35757;&#32451;&#20013;&#24615;&#33021;&#19979;&#38477;&#12290;&#38656;&#35201;&#22312;&#20219;&#20309;&#26102;&#20505;&#27979;&#35797;&#27169;&#22411;&#22312;&#30456;&#24212;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21442;&#25968;&#22823;&#23567;&#26159;&#21542;&#20250;&#23548;&#33268;&#36870;&#27604;&#20363;&#32553;&#25918;&#29616;&#35937;&#21602;&#65311;&#36825;&#20010;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#34920;&#29616;&#26159;&#21542;&#20250;&#19979;&#38477;&#65292;&#23613;&#31649;&#25972;&#20307;&#34920;&#29616;&#20173;&#28982;&#19981;&#38169;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#36870;&#27604;&#20363;&#32553;&#25918;&#25361;&#25112;&#20013;&#30340;&#20004;&#20010;&#20219;&#21153; - &#24341;&#36848;&#37325;&#22797;&#21644;&#37325;&#26032;&#23450;&#20041;&#25968;&#23398;&#65292;Pythia&#27169;&#22411;&#36816;&#29992;&#26356;&#22810;&#30340;&#21442;&#25968;&#26102;&#65292;&#36825;&#20004;&#20010;&#20219;&#21153;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#30830;&#23454;&#20250;&#19979;&#38477;&#65292;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#25972;&#20307;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#31361;&#26174;&#20102;&#22312;&#20219;&#20309;&#26102;&#20505;&#22914;&#26524;&#27169;&#22411;&#34987;&#35757;&#32451;&#20102;&#39069;&#22806;&#30340;&#25968;&#25454;&#65292;&#37027;&#20040;&#38656;&#35201;&#27979;&#35797;&#27169;&#22411;&#22312;&#30456;&#24212;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#65292;&#21363;&#20351;&#23427;&#20204;&#30340;&#25972;&#20307;&#34920;&#29616;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does inverse scaling only occur as a function of model parameter size, or can it also occur over the course of training? We carry out an exploratory study investigating whether, over the course of training on the language modeling task, the performance of language models at specific tasks can decrease while general performance remains high. We find that for two tasks from the Inverse Scaling Challenge - quote-repetition and redefine-math - this is indeed the case. Specifically, we find that for Pythia (Biderman et al., 2023) models with a higher number of parameters, performance decreases over the course of training at these two tasks, despite these models showing standard (positive) scaling overall. This highlights the importance of testing model performance at all relevant benchmarks any time they are trained on additional data, even if their overall performance improves.
&lt;/p&gt;</description></item><item><title>GRILL&#26159;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#21644;&#22270;&#20687;&#21306;&#22495;&#23545;&#40784;&#30340;&#22522;&#20110;&#22330;&#26223;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#35757;&#32451;&#26679;&#26412;&#24456;&#23569;&#25110;&#27809;&#26377;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#38646;/&#23569;&#26679;&#26412;VL&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.14676</link><description>&lt;p&gt;
GRILL&#65306;&#22522;&#20110;&#25991;&#26412;&#21644;&#22270;&#20687;&#21306;&#22495;&#23545;&#40784;&#30340;&#22522;&#20110;&#22330;&#26223;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GRILL: Grounded Vision-language Pre-training via Aligning Text and Image Regions. (arXiv:2305.14676v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14676
&lt;/p&gt;
&lt;p&gt;
GRILL&#26159;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#21644;&#22270;&#20687;&#21306;&#22495;&#23545;&#40784;&#30340;&#22522;&#20110;&#22330;&#26223;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#35757;&#32451;&#26679;&#26412;&#24456;&#23569;&#25110;&#27809;&#26377;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#38646;/&#23569;&#26679;&#26412;VL&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#26159;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;/&#23569;&#26679;&#26412;&#24615;&#33021;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#20294;&#36825;&#31181;&#27867;&#21270;&#36866;&#29992;&#20110;&#21253;&#25324;&#22522;&#30784;&#21644;&#29983;&#20135;&#20219;&#21153;&#22312;&#20869;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19968;&#30452;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;GRILL&#65292;&#19968;&#31181;&#26032;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#21253;&#25324;&#35270;&#35273;&#38382;&#31572;&#12289;&#26631;&#39064;&#21644;&#22522;&#30784;&#20219;&#21153;&#22312;&#20869;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#20854;&#35757;&#32451;&#26679;&#26412;&#24456;&#23569;&#25110;&#27809;&#26377;&#26679;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GRILL&#36890;&#36807;&#21033;&#29992;&#23545;&#35937;-&#25991;&#26412;&#23545;&#40784;&#26469;&#23398;&#20064;&#23545;&#35937;&#30340;&#23450;&#20301;&#65292;&#36825;&#20351;&#20854;&#33021;&#22815;&#20197;&#38646;/&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#36716;&#31227;&#21040;&#22522;&#30784;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#38646;/&#23569;&#26679;&#26412;VL&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization to unseen tasks is an important ability for few-shot learners to achieve better zero-/few-shot performance on diverse tasks. However, such generalization to vision-language tasks including grounding and generation tasks has been under-explored; existing few-shot VL models struggle to handle tasks that involve object grounding and multiple images such as visual commonsense reasoning or NLVR2. In this paper, we introduce GRILL, GRounded vIsion Language aLigning, a novel VL model that can be generalized to diverse tasks including visual question answering, captioning, and grounding tasks with no or very few training instances. Specifically, GRILL learns object grounding and localization by exploiting object-text alignments, which enables it to transfer to grounding tasks in a zero-/few-shot fashion. We evaluate our model on various zero-/few-shot VL tasks and show that it consistently surpasses the state-of-the-art few-shot methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;NLP&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#65292;&#27604;&#36739;&#20102;&#20854;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20248;&#21155;&#65292;&#25351;&#20986;&#25193;&#25955;&#27169;&#22411;&#22312;&#24182;&#34892;&#29983;&#25104;&#12289;&#25991;&#26412;&#25554;&#20540;&#12289;&#35789;&#32423;&#21035;&#25511;&#21046;&#31561;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.14671</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models in NLP: A Survey. (arXiv:2305.14671v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;NLP&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#65292;&#27604;&#36739;&#20102;&#20854;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20248;&#21155;&#65292;&#25351;&#20986;&#25193;&#25955;&#27169;&#22411;&#22312;&#24182;&#34892;&#29983;&#25104;&#12289;&#25991;&#26412;&#25554;&#20540;&#12289;&#35789;&#32423;&#21035;&#25511;&#21046;&#31561;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#30340;&#24212;&#29992;&#12290;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#25968;&#23398;&#27169;&#22411;&#65292;&#26088;&#22312;&#25429;&#25417;&#20449;&#24687;&#25110;&#20449;&#21495;&#22312;&#32593;&#32476;&#25110;&#27969;&#24418;&#19978;&#30340;&#25193;&#25955;&#12290;&#22312;NLP&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#20027;&#39064;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;NLP&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#25193;&#25955;&#27169;&#22411;&#30340;&#20844;&#24335;&#12289;&#20248;&#21155;&#28857;&#21644;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23545;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#29305;&#21035;&#26159;&#33258;&#22238;&#24402;(AR)&#27169;&#22411;&#65292;&#36824;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;Transformer&#19982;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#20197;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#26550;&#26500;&#12290;&#30456;&#27604;AR&#27169;&#22411;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#24182;&#34892;&#29983;&#25104;&#12289;&#25991;&#26412;&#25554;&#20540;&#12289;&#35789;&#32423;&#21035;&#25511;&#21046;(&#22914;&#21477;&#27861;&#32467;&#26500;)&#31561;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey paper provides a comprehensive review of the use of diffusion models in natural language processing (NLP). Diffusion models are a class of mathematical models that aim to capture the diffusion of information or signals across a network or manifold. In NLP, diffusion models have been used in a variety of applications, such as natural language generation, sentiment analysis, topic modeling, and machine translation. This paper discusses the different formulations of diffusion models used in NLP, their strengths and limitations, and their applications. We also perform a thorough comparison between diffusion models and alternative generative models, specifically highlighting the autoregressive (AR) models, while also examining how diverse architectures incorporate the Transformer in conjunction with diffusion models. Compared to AR models, diffusion models have significant advantages for parallel generation, text interpolation, token-level controls such as syntactic structures a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#27880;&#37322;&#32773;&#21644;&#27880;&#37322;&#30340;&#30697;&#38453;&#34920;&#31034;&#20197;&#25429;&#25417;&#20854;&#29305;&#28857;&#65292;&#24182;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#20351;&#29992;&#23427;&#20204;&#26469;&#26174;&#33879;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#24110;&#21161;&#27665;&#20027;&#21270;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14663</link><description>&lt;p&gt;
&#20320;&#23601;&#26159;&#20320;&#30340;&#27880;&#37322;&#65306;&#36890;&#36807;&#27880;&#37322;&#32773;&#34920;&#31034;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
You Are What You Annotate: Towards Better Models through Annotator Representations. (arXiv:2305.14663v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14663
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#27880;&#37322;&#32773;&#21644;&#27880;&#37322;&#30340;&#30697;&#38453;&#34920;&#31034;&#20197;&#25429;&#25417;&#20854;&#29305;&#28857;&#65292;&#24182;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#20351;&#29992;&#23427;&#20204;&#26469;&#26174;&#33879;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#24110;&#21161;&#27665;&#20027;&#21270;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#65292;&#27880;&#37322;&#32773;&#30340;&#19981;&#19968;&#33268;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#23384;&#22312;&#22810;&#31181;&#21407;&#22240;&#23548;&#33268;&#27880;&#37322;&#32773;&#30340;&#19981;&#21516;&#24847;&#35265;&#65292;&#21253;&#25324;&#20219;&#21153;&#30340;&#20027;&#35266;&#24615;&#12289;&#22256;&#38590;&#26696;&#20363;&#12289;&#19981;&#26126;&#30830;&#30340;&#25351;&#21335;&#31561;&#31561;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#26159;&#20165;&#20165;&#27719;&#24635;&#27880;&#37322;&#30340;&#26631;&#31614;&#26469;&#33719;&#24471;&#25968;&#25454;&#27880;&#37322;&#65292;&#32780;&#26159;&#25552;&#35758;&#35201;&#26126;&#30830;&#32771;&#34385;&#27880;&#37322;&#32773;&#29305;&#24449;&#65292;&#24182;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#21033;&#29992;&#23427;&#20204;&#12290;&#25105;&#20204;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#30697;&#38453;&#20998;&#21035;&#20026;&#27880;&#37322;&#32773;&#65288;&#27880;&#37322;&#32773;&#23884;&#20837;&#65289;&#21644;&#20854;&#27880;&#37322;&#65288;&#27880;&#37322;&#23884;&#20837;&#65289;&#21019;&#24314;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23569;&#20110;1&#65285;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#22312;&#21508;&#31181;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#25429;&#25417;&#21333;&#20010;&#27880;&#37322;&#32773;&#30340;&#29420;&#29305;&#20542;&#21521;&#21644;&#20027;&#35266;&#24615;&#65292;&#25105;&#20204;&#30340;&#23884;&#20837;&#26377;&#21161;&#20110;&#27665;&#20027;&#21270;&#20154;&#24037;&#26234;&#33021;&#65292;&#24182;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21253;&#21547;&#22810;&#20803;&#21270;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotator disagreement is ubiquitous in natural language processing (NLP) tasks. There are multiple reasons for such disagreements, including the subjectivity of the task, difficult cases, unclear guidelines, and so on. Rather than simply aggregating labels to obtain data annotations, we instead propose to explicitly account for the annotator idiosyncrasies and leverage them in the modeling process. We create representations for the annotators (annotator embeddings) and their annotations (annotation embeddings) with learnable matrices associated with each. Our approach significantly improves model performance on various NLP benchmarks by adding fewer than 1% model parameters. By capturing the unique tendencies and subjectivity of individual annotators, our embeddings help democratize AI and ensure that AI models are inclusive of diverse viewpoints.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;InteractiveIE&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#21160;&#38382;&#31572;&#29983;&#25104;&#27861;&#26469;&#24341;&#20986;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#27169;&#26495;&#25554;&#27133;&#65292;&#36890;&#36807;&#24494;&#23567;&#37327;&#20195;&#29702;&#20154;&#31867;&#30417;&#30563;&#65292;&#25552;&#39640;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#27861;&#24459;&#25991;&#26723;&#31561;&#39046;&#22495;&#20013;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14659</link><description>&lt;p&gt;
InteractiveIE&#65306;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#24378;&#24230;&#65292;&#25552;&#39640;&#20449;&#24687;&#25552;&#21462;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
InteractiveIE: Towards Assessing the Strength of Human-AI Collaboration in Improving the Performance of Information Extraction. (arXiv:2305.14659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;InteractiveIE&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#21160;&#38382;&#31572;&#29983;&#25104;&#27861;&#26469;&#24341;&#20986;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#27169;&#26495;&#25554;&#27133;&#65292;&#36890;&#36807;&#24494;&#23567;&#37327;&#20195;&#29702;&#20154;&#31867;&#30417;&#30563;&#65292;&#25552;&#39640;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#27861;&#24459;&#25991;&#26723;&#31561;&#39046;&#22495;&#20013;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#27169;&#26495;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#26159;&#19968;&#39033;&#20851;&#38190;&#19988;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#22522;&#20110;&#27169;&#26495;&#30340;&#20449;&#24687;&#25552;&#21462;&#26041;&#27861;&#20551;&#23450;&#20855;&#26377;&#39046;&#22495;&#27169;&#26495;&#30340;&#20808;&#39564;&#30693;&#35782;&#65307;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#20449;&#24687;&#25552;&#21462;&#27809;&#26377;&#39044;&#23450;&#20041;&#30340;&#27169;&#24335;&#65292;&#24182;&#19988;&#26159;&#19968;&#20010;&#21363;&#20852;&#21019;&#36896;&#30340;&#29616;&#35937;&#12290;&#20026;&#20102;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#24555;&#36895;&#24341;&#23548;&#27169;&#26495;&#65292;&#25105;&#20204;&#38656;&#35201;&#20174;&#25991;&#26723;&#20013;&#22312;&#38646;&#25110;&#26368;&#23567;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#24341;&#20986;&#27169;&#26495;&#25554;&#27133;&#12290;&#30001;&#20110;&#38382;&#31572;&#30340;&#30446;&#30340;&#19982;&#20449;&#24687;&#25552;&#21462;&#30340;&#30446;&#26631;&#20132;&#27719;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#38382;&#31572;&#29983;&#25104;&#20174;&#25991;&#26723;&#20013;&#24341;&#20986;&#27169;&#26495;&#25554;&#27133;&#65292;&#24182;&#30740;&#31350;&#24494;&#23567;&#37327;&#30340;&#20195;&#29702;&#20154;&#31867;&#30417;&#30563;&#65288;&#31216;&#20026;InteractiveIE&#65289;&#22914;&#20309;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#33719;&#21462;&#35757;&#32451;&#25968;&#25454;&#26114;&#36149;&#30340;&#29983;&#29289;&#21307;&#23398;&#21644;&#27861;&#24459;&#25991;&#26723;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#25581;&#31034;&#20102;&#20351;&#29992;InteractiveIE&#30456;&#23545;&#20110;&#20165;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#40723;&#33310;&#20154;&#24515;&#30340;&#24615;&#33021;&#25552;&#21319;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning template based information extraction from documents is a crucial yet difficult task. Prior template-based IE approaches assume foreknowledge of the domain templates; however, real-world IE do not have pre-defined schemas and it is a figure-out-as you go phenomena. To quickly bootstrap templates in a real-world setting, we need to induce template slots from documents with zero or minimal supervision. Since the purpose of question answering intersect with the goal of information extraction, we use automatic question generation to induce template slots from the documents and investigate how a tiny amount of a proxy human-supervision on-the-fly (termed as InteractiveIE) can further boost the performance. Extensive experiments on biomedical and legal documents, where obtaining training data is expensive, reveal encouraging trends of performance improvement using InteractiveIE over AI-only baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22522;&#30784;&#30340;&#26080;&#21442;&#32771;&#35780;&#20272;&#22120;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#20004;&#20010;&#23545;&#25239;&#20803;&#35780;&#20272;&#23545;&#35805;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#36825;&#20123;&#35780;&#20272;&#22120;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14658</link><description>&lt;p&gt;
&#26080;&#27861;&#35780;&#20272;&#30340;&#29983;&#25104;&#21709;&#24212;&#36136;&#37327;&#30340;&#35780;&#20272;: Evaluate What You Can't Evaluate
&lt;/p&gt;
&lt;p&gt;
Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality. (arXiv:2305.14658v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22522;&#30784;&#30340;&#26080;&#21442;&#32771;&#35780;&#20272;&#22120;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#20004;&#20010;&#23545;&#25239;&#20803;&#35780;&#20272;&#23545;&#35805;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#36825;&#20123;&#35780;&#20272;&#22120;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#24050;&#32463;&#23637;&#29616;&#20986;&#24778;&#20154;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#34429;&#28982;&#20197;LLMs&#20026;&#22522;&#30784;&#30340;&#26080;&#21442;&#32771;&#35780;&#20272;&#22120;&#27604;&#20256;&#32479;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#35780;&#20272;&#22120;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#20154;&#31867;&#35821;&#20041;&#23545;&#40784;&#24230;&#65292;&#20294;&#26159;&#22312;&#20351;&#29992;&#20197;LLMs&#20026;&#22522;&#30784;&#30340;&#26080;&#21442;&#32771;&#35780;&#20272;&#22120;&#26102;&#20173;&#28982;&#23384;&#22312;&#24456;&#22810;&#25361;&#25112;&#12290;&#26080;&#21442;&#32771;&#35780;&#20272;&#22120;&#26356;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#35821;&#20041;&#21709;&#24212;&#30340;&#24320;&#25918;&#24335;&#20363;&#23376;&#12290;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;&#20363;&#23376;&#37117;&#26159;&#24320;&#25918;&#24335;&#30340;&#65292;&#23545;&#20110;&#20855;&#26377;&#21807;&#19968;&#27491;&#30830;&#35821;&#20041;&#21709;&#24212;&#30340;&#38381;&#21512;&#24335;&#20363;&#23376;&#65292;&#22914;&#26524;&#32473;&#20986;&#19982;&#20107;&#23454;&#21644;&#21442;&#32771;&#30340;&#35821;&#20041;&#19981;&#19968;&#33268;&#30340;&#21709;&#24212;&#65292;&#26080;&#21442;&#32771;&#35780;&#20272;&#22120;&#20173;&#28982;&#20250;&#35748;&#20026;&#20854;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#20197;LLMs&#20026;&#22522;&#30784;&#30340;&#35780;&#20272;&#22120;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#23545;&#25239;&#20803;&#35780;&#20272;&#23545;&#35805;&#29983;&#25104;&#25968;&#25454;&#38598;KdConv-ADV&#21644;DSTC7-ADV&#22522;&#20110;KdConv&#21644;DSTC7-AVSD&#12290;&#19982;&#20197;&#21069;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#30456;&#27604;&#65292;KdConv-ADV&#21644;DSTC7-ADV&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs (large language models) such as ChatGPT have shown remarkable language understanding and generation capabilities. Although reference-free evaluators based on LLMs show better human alignment than traditional reference-based evaluators, there are many challenges in using reference-free evaluators based on LLMs. Reference-free evaluators are more suitable for open-ended examples with different semantics responses. But not all examples are open-ended. For closed-ended examples with unique correct semantic response, reference-free evaluators will still consider it high quality when giving a response that is inconsistent with the facts and the semantic of reference. In order to comprehensively evaluate the reliability of evaluators based on LLMs, we construct two adversarial meta-evaluation dialogue generation datasets KdConv-ADV and DSTC7-ADV based on KdConv and DSTC7-AVSD, respectively. Compared to previous meta-evaluation benchmarks, KdConv-ADV and DSTC7-ADV are much more challengin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#29942;&#39048;&#21644;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#35270;&#39057;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65288;DBF&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#32454;&#31890;&#24230;&#22320;&#36807;&#28388;&#25481;&#20887;&#20313;&#21644;&#22122;&#22768;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#19981;&#21516;&#27169;&#24577;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#35270;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14652</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#35270;&#39057;&#22810;&#27169;&#24577;&#34701;&#21512;&#21435;&#22122;&#29942;&#39048;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion. (arXiv:2305.14652v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#29942;&#39048;&#21644;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#35270;&#39057;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65288;DBF&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#32454;&#31890;&#24230;&#22320;&#36807;&#28388;&#25481;&#20887;&#20313;&#21644;&#22122;&#22768;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#19981;&#21516;&#27169;&#24577;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#35270;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#22810;&#27169;&#24577;&#34701;&#21512;&#26088;&#22312;&#23558;&#35270;&#39057;&#20013;&#30340;&#22810;&#27169;&#24577;&#20449;&#21495;&#65288;&#22914;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#65289;&#25972;&#21512;&#65292;&#20197;&#20351;&#29992;&#22810;&#27169;&#24577;&#20869;&#23481;&#36827;&#34892;&#34917;&#20805;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#20182;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#24577;&#20219;&#21153;&#19981;&#21516;&#65292;&#35270;&#39057;&#20855;&#26377;&#26356;&#38271;&#30340;&#22810;&#27169;&#24577;&#24207;&#21015;&#65292;&#22312;&#35270;&#35273;&#21644;&#38899;&#39057;&#27169;&#24577;&#20013;&#23384;&#22312;&#26356;&#22810;&#30340;&#20887;&#20313;&#21644;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32454;&#31890;&#24230;&#35270;&#39057;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#21435;&#22122;&#29942;&#39048;&#34701;&#21512;&#65288;DBF&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#19968;&#26041;&#38754;&#37319;&#29992;&#29942;&#39048;&#26426;&#21046;&#65292;&#20197;&#38480;&#21046;&#30340;&#24863;&#21463;&#37326;&#36807;&#28388;&#22122;&#22768;&#21644;&#20887;&#20313;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#27169;&#22359;&#26469;&#35843;&#33410;&#36807;&#28388;&#27169;&#22359;&#65292;&#20197;&#20445;&#30041;&#19981;&#21516;&#27169;&#24577;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;DBF&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#35270;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video multimodal fusion aims to integrate multimodal signals in videos, such as visual, audio and text, to make a complementary prediction with multiple modalities contents. However, unlike other image-text multimodal tasks, video has longer multimodal sequences with more redundancy and noise in both visual and audio modalities. Prior denoising methods like forget gate are coarse in the granularity of noise filtering. They often suppress the redundant and noisy information at the risk of losing critical information. Therefore, we propose a denoising bottleneck fusion (DBF) model for fine-grained video multimodal fusion. On the one hand, we employ a bottleneck mechanism to filter out noise and redundancy with a restrained receptive field. On the other hand, we use a mutual information maximization module to regulate the filter-out module to preserve key information within different modalities. Our DBF model achieves significant improvement over current state-of-the-art baselines on mult
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EEA&#65289;&#38382;&#39064;&#65292;&#24341;&#20837;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;EEA&#26041;&#27861;&#21450;&#25552;&#20986;&#30340;&#29983;&#25104;&#30340;EEA&#65288;GEEA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20114;&#30456;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;M-VAE&#65289;&#23454;&#29616;&#23454;&#20307;&#20174;&#19968;&#20010;KG&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;KG&#65292;&#24182;&#19988;&#20174;&#38543;&#26426;&#22122;&#22768;&#21521;&#37327;&#29983;&#25104;&#26032;&#30340;&#23454;&#20307;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14651</link><description>&lt;p&gt;
&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#23454;&#20307;&#23545;&#40784;&#21450;&#36229;&#36234;&#65306;&#19968;&#20010;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Revisit and Outstrip Entity Alignment: A Perspective of Generative Models. (arXiv:2305.14651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EEA&#65289;&#38382;&#39064;&#65292;&#24341;&#20837;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;EEA&#26041;&#27861;&#21450;&#25552;&#20986;&#30340;&#29983;&#25104;&#30340;EEA&#65288;GEEA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20114;&#30456;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;M-VAE&#65289;&#23454;&#29616;&#23454;&#20307;&#20174;&#19968;&#20010;KG&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;KG&#65292;&#24182;&#19988;&#20174;&#38543;&#26426;&#22122;&#22768;&#21521;&#37327;&#29983;&#25104;&#26032;&#30340;&#23454;&#20307;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#22312;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EEA&#65289;&#12290;&#25105;&#20204;&#34920;&#26126;EEA&#26159;&#19968;&#20010;&#29305;&#27530;&#30340;&#38382;&#39064;&#65292;&#20854;&#20027;&#35201;&#30446;&#26631;&#31867;&#20284;&#20110;&#20856;&#22411;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#30446;&#26631;&#65292;&#22522;&#20110;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#26368;&#36817;&#21457;&#23637;&#30340;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;EEA&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20182;&#20204;&#19981;&#23436;&#25972;&#30340;&#30446;&#26631;&#38480;&#21046;&#20102;&#23454;&#20307;&#23545;&#40784;&#21644;&#23454;&#20307;&#21512;&#25104;&#65288;&#21363;&#29983;&#25104;&#26032;&#23454;&#20307;&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#30340;EEA&#65288;abbr.&#65292;GEEA&#65289;&#26694;&#26550;&#21644;&#25552;&#20986;&#30340;&#20114;&#30456;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;M-VAE&#65289;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;M-VAE&#21487;&#20197;&#23558;&#19968;&#20010;&#23454;&#20307;&#20174;&#19968;&#20010;KG&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;KG&#65292;&#24182;&#20174;&#38543;&#26426;&#22122;&#22768;&#21521;&#37327;&#29983;&#25104;&#26032;&#23454;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#23454;&#39564;&#23637;&#31034;&#20102;GEEA&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent embedding-based methods have achieved great successes on exploiting entity alignment from knowledge graph (KG) embeddings of multiple modals. In this paper, we study embedding-based entity alignment (EEA) from a perspective of generative models. We show that EEA is a special problem where the main objective is analogous to that in a typical generative model, based on which we theoretically prove the effectiveness of the recently developed generative adversarial network (GAN)-based EEA methods. We then reveal that their incomplete objective limits the capacity on both entity alignment and entity synthesis (i.e., generating new entities). We mitigate this problem by introducing a generative EEA (abbr., GEEA) framework with the proposed mutual variational autoencoder (M-VAE) as the generative model. M-VAE can convert an entity from one KG to another and generate new entities from random noise vectors. We demonstrate the power of GEEA with theoretical analysis and empirical experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31185;&#23398;&#35266;&#28857;&#24635;&#32467;&#30340;&#20219;&#21153;&#65292;&#20197;&#21512;&#25104;&#30740;&#31350;&#35770;&#25991;&#35780;&#23457;&#30340;&#20803;&#35780;&#23457;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;ORSUM&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26816;&#26597;&#34920;&#24341;&#23548;&#36845;&#20195;&#33258;&#26597;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14647</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#26597;&#34920;&#24341;&#23548;&#36845;&#20195;&#33258;&#26597;&#30340;&#20803;&#35780;&#23457;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Meta-review Generation with Checklist-guided Iterative Introspection. (arXiv:2305.14647v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31185;&#23398;&#35266;&#28857;&#24635;&#32467;&#30340;&#20219;&#21153;&#65292;&#20197;&#21512;&#25104;&#30740;&#31350;&#35770;&#25991;&#35780;&#23457;&#30340;&#20803;&#35780;&#23457;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;ORSUM&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26816;&#26597;&#34920;&#24341;&#23548;&#36845;&#20195;&#33258;&#26597;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#19981;&#21516;&#30340;&#35266;&#28857;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#35780;&#23457;&#24847;&#35265;&#30340;&#20105;&#35758;&#25110;&#20849;&#35782;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35266;&#28857;&#24635;&#32467;&#25968;&#25454;&#38598;&#20027;&#35201;&#38598;&#20013;&#22312;&#20135;&#21697;&#35780;&#35770;&#39046;&#22495;&#65292;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#31181;&#21487;&#21464;&#24615;&#65292;&#20551;&#35774;&#36755;&#20837;&#30340;&#24847;&#35265;&#26159;&#27809;&#26377;&#20105;&#35758;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31185;&#23398;&#35266;&#28857;&#24635;&#32467;&#30340;&#20219;&#21153;&#65292;&#23558;&#30740;&#31350;&#35770;&#25991;&#35780;&#23457;&#21512;&#25104;&#20026;&#20803;&#35780;&#23457;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;ORSUM&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;39&#20010;&#20250;&#35758;&#30340;10,989&#31687;&#35770;&#25991;&#20803;&#23457;&#26597;&#21644;40,903&#31687;&#35770;&#25991;&#23457;&#26597;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26816;&#26597;&#34920;&#24341;&#23548;&#36845;&#20195;&#33258;&#26597;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#20960;&#20010;&#38454;&#27573;&#65292;&#24182;&#22312;&#26816;&#26597;&#34920;&#30340;&#25351;&#23548;&#19979;&#36845;&#20195;&#22320;&#23436;&#21892;&#25688;&#35201;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;&#65288;1&#65289;&#20154;&#24037;&#25776;&#20889;&#30340;&#25688;&#35201;&#24182;&#19981;&#24635;&#26159;&#21487;&#38752;&#30340;&#65292;&#22240;&#20026;&#35768;&#22810;&#20154;&#24182;&#27809;&#26377;&#36981;&#24490;&#25351;&#21335;&#65292;&#65288;2&#65289;&#20219;&#21153;&#20998;&#35299;&#21644;&#36845;&#20195;&#33258;&#25105;&#23436;&#21892;&#30340;&#32452;&#21512;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opinions in the scientific domain can be divergent, leading to controversy or consensus among reviewers. However, current opinion summarization datasets mostly focus on product review domains, which do not account for this variability under the assumption that the input opinions are non-controversial. To address this gap, we propose the task of scientific opinion summarization, where research paper reviews are synthesized into meta-reviews. To facilitate this task, we introduce a new ORSUM dataset covering 10,989 paper meta-reviews and 40,903 paper reviews from 39 conferences. Furthermore, we propose the Checklist-guided Iterative Introspection (CGI$^2$) approach, which breaks down the task into several stages and iteratively refines the summary under the guidance of questions from a checklist. We conclude that (1) human-written summaries are not always reliable since many do not follow the guideline, and (2) the combination of task decomposition and iterative self-refinement shows pro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22256;&#38590;&#26399;&#26395;&#26368;&#22823;&#21270;&#26469;&#20132;&#26367;&#36827;&#34892;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#21644;&#20107;&#20214;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20840;&#23616;&#32467;&#26500;&#21644;&#23616;&#37096;&#30456;&#20851;&#24615;&#32422;&#26463;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#21644;&#20107;&#20214;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14645</link><description>&lt;p&gt;
&#36890;&#36807;&#22256;&#38590;&#26399;&#26395;&#26368;&#22823;&#21270;&#36845;&#20195;&#25913;&#21892;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#21644;&#20107;&#20214;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Iteratively Improving Biomedical Entity Linking and Event Extraction via Hard Expectation-Maximization. (arXiv:2305.14645v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22256;&#38590;&#26399;&#26395;&#26368;&#22823;&#21270;&#26469;&#20132;&#26367;&#36827;&#34892;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#21644;&#20107;&#20214;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20840;&#23616;&#32467;&#26500;&#21644;&#23616;&#37096;&#30456;&#20851;&#24615;&#32422;&#26463;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#21644;&#20107;&#20214;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#21644;&#20107;&#20214;&#25552;&#21462;&#26159;&#25903;&#25345;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#25991;&#26412;&#29702;&#35299;&#21644;&#26816;&#32034;&#30340;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#23558;&#36825;&#20004;&#20010;&#20219;&#21153;&#20998;&#21035;&#35299;&#20915;&#25110;&#20197;&#27969;&#27700;&#32447;&#26041;&#24335;&#35299;&#20915;&#65292;&#20174;&#32780;&#23548;&#33268;&#38169;&#35823;&#20256;&#25773;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#23558;&#20107;&#20214;&#32467;&#26500;&#21644;&#23454;&#20307;&#24341;&#29992;&#20316;&#20026;&#20004;&#32452;&#28508;&#22312;&#21464;&#37327;&#26469;&#32852;&#21512;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#21644;&#20107;&#20214;&#25552;&#21462;&#65292;&#24182;&#36890;&#36807;&#22256;&#38590;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#20132;&#38169;&#20004;&#20010;&#27495;&#20041;&#28040;&#38500;&#27493;&#39588;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#20840;&#23616;&#32467;&#26500;&#32422;&#26463;&#21644;&#23616;&#37096;&#30456;&#20851;&#24615;&#32422;&#26463;&#30340;&#25351;&#23548;&#19979;&#65292;&#36845;&#20195;&#20248;&#21270;&#23454;&#20307;&#21644;&#20107;&#20214;&#20998;&#37197;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#32852;&#21512;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#21644;&#20107;&#20214;&#25552;&#21462;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical entity linking and event extraction are two crucial tasks to support text understanding and retrieval in the biomedical domain. These two tasks intrinsically benefit each other: entity linking disambiguates the biomedical concepts by referring to external knowledge bases and the domain knowledge further provides additional clues to understand and extract the biological processes, while event extraction identifies a key trigger and entities involved to describe each biological process which also captures the structural context to better disambiguate the biomedical entities. However, previous research typically solves these two tasks separately or in a pipeline, leading to error propagation. What's more, it's even more challenging to solve these two tasks together as there is no existing dataset that contains annotations for both tasks. To solve these challenges, we propose joint biomedical entity linking and event extraction by regarding the event structures and entity refere
&lt;/p&gt;</description></item><item><title>CMOT&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#27169;&#24577;&#35821;&#38899;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#25214;&#21040;&#35821;&#38899;&#21644;&#25991;&#26412;&#24207;&#21015;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#22312;&#26631;&#35760;&#32423;&#21035;&#19978;&#28151;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#24207;&#21015;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14635</link><description>&lt;p&gt;
CMOT: &#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#36328;&#27169;&#24577;Mixup&#65292;&#29992;&#20110;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation. (arXiv:2305.14635v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14635
&lt;/p&gt;
&lt;p&gt;
CMOT&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#27169;&#24577;&#35821;&#38899;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#25214;&#21040;&#35821;&#38899;&#21644;&#25991;&#26412;&#24207;&#21015;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#22312;&#26631;&#35760;&#32423;&#21035;&#19978;&#28151;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#24207;&#21015;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#26159;&#23558;&#28304;&#35821;&#35328;&#20013;&#30340;&#35821;&#38899;&#20449;&#21495;&#32763;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#25991;&#26412;&#30340;&#20219;&#21153;&#12290;&#20316;&#20026;&#19968;&#39033;&#36328;&#27169;&#24577;&#20219;&#21153;&#65292;&#31471;&#21040;&#31471;ST&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#36827;&#34892;&#35757;&#32451;&#38750;&#24120;&#22256;&#38590;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#23581;&#35797;&#20174;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20013;&#36716;&#31227;&#30693;&#35782;&#65292;&#20294;&#20854;&#24615;&#33021;&#30001;&#20110;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Cross-modal Mixup via Optimal Transport&#65288;CMOT&#65289;&#26469;&#20811;&#26381;&#27169;&#24577;&#24046;&#36317;&#12290;&#25105;&#20204;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#25214;&#21040;&#35821;&#38899;&#21644;&#25991;&#26412;&#24207;&#21015;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#28982;&#21518;&#20351;&#29992;&#23545;&#40784;&#22312;&#26631;&#35760;&#32423;&#21035;&#19978;&#28151;&#21512;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#24207;&#21015;&#12290;&#22312;MuST-C ST&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CMOT&#22312;8&#20010;&#32763;&#35793;&#26041;&#21521;&#19978;&#23454;&#29616;&#20102;&#24179;&#22343;BLEU&#20540;&#20026;30.0&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;CMOT&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#25214;&#21040;&#27169;&#24577;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#26377;&#21161;&#20110;&#32531;&#35299;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#12290;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#20110; https://github.com/ic
&lt;/p&gt;
&lt;p&gt;
End-to-end speech translation (ST) is the task of translating speech signals in the source language into text in the target language. As a cross-modal task, end-to-end ST is difficult to train with limited data. Existing methods often try to transfer knowledge from machine translation (MT), but their performances are restricted by the modality gap between speech and text. In this paper, we propose Cross-modal Mixup via Optimal Transport CMOT to overcome the modality gap. We find the alignment between speech and text sequences via optimal transport and then mix up the sequences from different modalities at a token level using the alignment. Experiments on the MuST-C ST benchmark demonstrate that CMOT achieves an average BLEU of 30.0 in 8 translation directions, outperforming previous methods. Further analysis shows CMOT can adaptively find the alignment between modalities, which helps alleviate the modality gap between speech and text. Code is publicly available at https://github.com/ic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;HIPE&#29702;&#35770;&#20013;&#25551;&#36848;&#30340;&#35789;&#20041;&#22240;&#26524;&#20851;&#31995;&#27169;&#22411;&#65292;&#23545;GPT-3&#21644;GPT-4&#30340;&#35789;&#27719;&#34920;&#31034;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-3&#27809;&#26377;&#32534;&#30721;&#22240;&#26524;&#32467;&#26500;&#30340;&#35777;&#25454;&#65292;&#32780;GPT-4&#21017;&#26377;&#12290;</title><link>http://arxiv.org/abs/2305.14630</link><description>&lt;p&gt;
&#22312;GPT-3&#21644;GPT-4&#20013;&#27979;&#35797;&#35789;&#20041;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Testing Causal Models of Word Meaning in GPT-3 and -4. (arXiv:2305.14630v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;HIPE&#29702;&#35770;&#20013;&#25551;&#36848;&#30340;&#35789;&#20041;&#22240;&#26524;&#20851;&#31995;&#27169;&#22411;&#65292;&#23545;GPT-3&#21644;GPT-4&#30340;&#35789;&#27719;&#34920;&#31034;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-3&#27809;&#26377;&#32534;&#30721;&#22240;&#26524;&#32467;&#26500;&#30340;&#35777;&#25454;&#65292;&#32780;GPT-4&#21017;&#26377;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#24040;&#22823;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#34920;&#31034;&#35789;&#27719;&#27010;&#24565;&#65292;&#21363;&#23427;&#20204;&#20351;&#29992;&#30340;&#21333;&#35789;&#30340;&#21547;&#20041;&#65292;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;HIPE&#29702;&#35770;&#65288;&#19968;&#31181;&#27010;&#24565;&#34920;&#31034;&#29702;&#35770;&#65292;&#20391;&#37325;&#20110;&#25551;&#36848;&#20154;&#36896;&#29289;&#21697;&#30340;&#35789;&#35821;&#30340;&#34920;&#31034;&#65289;&#35780;&#20272;&#20102;GPT-3&#21644;GPT-4&#30340;&#35789;&#27719;&#34920;&#31034;&#12290;&#35813;&#29702;&#35770;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#36825;&#20123;&#35789;&#27719;&#30340;&#21547;&#20041;&#19982;&#23427;&#20204;&#25152;&#25551;&#36848;&#30340;&#23545;&#35937;&#30340;&#24418;&#24335;&#12289;&#29992;&#36884;&#21644;&#21382;&#21490;&#32852;&#31995;&#36215;&#26469;&#30340;&#22240;&#26524;&#20851;&#31995;&#22270;&#12290;&#25105;&#20204;&#20351;&#29992;Chaigneau&#31561;&#20154;&#65288;2004&#65289;&#26368;&#21021;&#29992;&#20110;&#20154;&#31867;&#35780;&#20272;&#35813;&#29702;&#35770;&#30340;&#30456;&#21516;&#21050;&#28608;&#26469;&#27979;&#35797;LLM&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#31181;&#25552;&#31034;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#26377;&#20851;&#22240;&#26524;&#32467;&#26524;&#12289;&#23545;&#35937;&#21151;&#33021;&#21644;&#23545;&#35937;&#21629;&#21517;&#30340;&#21028;&#26029;&#12290;&#25105;&#20204;&#27809;&#26377;&#21457;&#29616;GPT-3&#32534;&#30721;HIPE&#20551;&#35774;&#30340;&#22240;&#26524;&#32467;&#26500;&#30340;&#35777;&#25454;&#65292;&#20294;&#26159;&#21457;&#29616;GPT-4&#32534;&#30721;&#20102;&#36825;&#26679;&#30340;&#32467;&#26500;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#34920;&#31034;&#35789;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have driven extraordinary improvements in NLP. However, it is unclear how such models represent lexical concepts-i.e., the meanings of the words they use. This paper evaluates the lexical representations of GPT-3 and GPT-4 through the lens of HIPE theory, a theory of concept representations which focuses on representations of words describing artifacts (such as "mop", "pencil", and "whistle"). The theory posits a causal graph that relates the meanings of such words to the form, use, and history of the objects to which they refer. We test LLMs using the same stimuli originally used by Chaigneau et al. (2004) to evaluate the theory in humans, and consider a variety of prompt designs. Our experiments concern judgements about causal outcomes, object function, and object naming. We find no evidence that GPT-3 encodes the causal structure hypothesized by HIPE, but do find evidence that GPT-4 encodes such structure. The results contribute to a growing body of rese
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;ALCE&#65292;&#26159;&#39318;&#20010;&#33258;&#21160;LLMs&#24341;&#25991;&#35780;&#20272;&#22522;&#20934;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#24341;&#25991;&#30340;&#25991;&#26412;&#65292;&#25552;&#39640;&#20854;&#20107;&#23454;&#27491;&#30830;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#65307;&#25552;&#31034;LLMs&#29305;&#23450;&#30340;&#20851;&#38190;&#35789;&#25110;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24341;&#25991;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14627</link><description>&lt;p&gt;
&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#24341;&#25991;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Enabling Large Language Models to Generate Text with Citations. (arXiv:2305.14627v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;ALCE&#65292;&#26159;&#39318;&#20010;&#33258;&#21160;LLMs&#24341;&#25991;&#35780;&#20272;&#22522;&#20934;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#24341;&#25991;&#30340;&#25991;&#26412;&#65292;&#25552;&#39640;&#20854;&#20107;&#23454;&#27491;&#30830;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#65307;&#25552;&#31034;LLMs&#29305;&#23450;&#30340;&#20851;&#38190;&#35789;&#25110;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24341;&#25991;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#24191;&#27867;&#20351;&#29992;&#30340;&#20449;&#24687;&#23547;&#25214;&#24037;&#20855;&#65292;&#20294;&#29983;&#25104;&#30340;&#36755;&#20986;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#12290;&#26412;&#25991;&#26088;&#22312;&#23454;&#29616;LLMs&#29983;&#25104;&#24102;&#24341;&#25991;&#30340;&#25991;&#26412;&#65292;&#25552;&#39640;&#20854;&#20107;&#23454;&#27491;&#30830;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ALCE&#65292;&#36825;&#26159;&#39318;&#20010;&#33258;&#21160;LLMs&#24341;&#25991;&#35780;&#20272;&#22522;&#20934;&#12290;ALCE&#25910;&#38598;&#20102;&#21508;&#31181;&#38382;&#39064;&#21644;&#26816;&#32034;&#35821;&#26009;&#24211;&#65292;&#24182;&#35201;&#27714;&#24314;&#31435;&#31471;&#21040;&#31471;&#31995;&#32479;&#20197;&#26816;&#32034;&#25903;&#25345;&#35777;&#25454;&#24182;&#29983;&#25104;&#24102;&#26377;&#24341;&#25991;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#27839;&#30528;&#27969;&#30021;&#24615;&#12289;&#27491;&#30830;&#24615;&#21644;&#24341;&#25991;&#36136;&#37327;&#19977;&#20010;&#32500;&#24230;&#26500;&#24314;&#33258;&#21160;&#25351;&#26631;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLMs&#21644;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#31995;&#32479;&#20173;&#26377;&#30456;&#24403;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;--&#20363;&#22914;&#65292;&#25552;&#31034;LLMs&#29305;&#23450;&#30340;&#20851;&#38190;&#35789;&#25110;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24341;&#25991;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#21457;&#23637;&#33021;&#22815;&#29983;&#25104;&#21487;&#39564;&#35777;&#21644;&#21487;&#20449;&#36182;&#36755;&#20986;&#30340;LLMs&#25552;&#20379;&#20102;&#22362;&#23454;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, we aim to enable LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare with different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We build automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvements -for example,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#25554;&#20540;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#21457;&#29616;KNN-LM&#24182;&#19981;&#33021;&#25552;&#39640;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#30340;&#25972;&#20307;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.14625</link><description>&lt;p&gt;
KNN-LM&#24182;&#19981;&#33021;&#25552;&#39640;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
KNN-LM Does Not Improve Open-ended Text Generation. (arXiv:2305.14625v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#25554;&#20540;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#21457;&#29616;KNN-LM&#24182;&#19981;&#33021;&#25552;&#39640;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#30340;&#25972;&#20307;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#25554;&#20540;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;&#36825;&#20123;&#26041;&#27861;&#20197;KNN-LM&#20026;&#20195;&#34920;&#65292;&#23558;LM&#39044;&#27979;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#20998;&#24067;&#19982;&#32473;&#23450;&#21069;&#32512;&#26368;&#30456;&#20851;&#30340;&#26816;&#32034;&#24418;&#25104;&#30340;&#20998;&#24067;&#25554;&#20540;&#12290;&#34429;&#28982;KNN-LM&#21644;&#30456;&#20851;&#26041;&#27861;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#22312;&#24320;&#25918;&#24335;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#27809;&#26377;&#30456;&#24212;&#30340;&#25913;&#36827;&#65292;&#36825;&#26159;&#36890;&#36807;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65288;&#20363;&#22914;MAUVE&#65289;&#21644;&#20154;&#24037;&#35780;&#20272;&#26469;&#34913;&#37327;&#30340;&#12290;&#36827;&#19968;&#27493;&#25506;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#26816;&#32034;&#20998;&#24067;&#25554;&#20540;&#30456;&#27604;&#65292;&#23545;&#20110;WikiText-103&#27979;&#35797;&#38598;&#20013;&#30340;&#22823;&#22810;&#25968;&#26631;&#35760;&#65292;&#25554;&#20540;&#20250;&#22686;&#21152;&#22256;&#24785;&#24230;&#65292;&#23613;&#31649;&#30001;&#20110;&#22256;&#24785;&#24230;&#26174;&#33879;&#38477;&#20302;&#30340;&#26631;&#35760;&#25968;&#37327;&#26356;&#23569;&#65292;&#22240;&#27492;&#24635;&#20307;&#19978;&#22256;&#24785;&#24230;&#26356;&#20302;&#12290;&#28982;&#32780;&#65292;&#24403;&#22312;&#25512;&#29702;&#26102;&#35299;&#30721;&#38271;&#24207;&#21015;&#26102;&#65292;&#23545;&#36825;&#20010;&#23567;&#26631;&#35760;&#38598;&#30340;&#26174;&#33879;&#25913;&#36827;&#24182;&#19981;&#33021;&#36716;&#21270;&#20026;&#25972;&#20010;&#24207;&#21015;&#30340;&#25345;&#32493;&#24615;&#25913;&#36827;&#65292;&#36825;&#34920;&#26126;KNN-LM&#21644;&#30456;&#20851;&#26041;&#27861;&#30340;&#25913;&#36827;&#26159;&#23616;&#37096;&#29616;&#35937;&#65292;&#32780;&#19981;&#26159;LM&#30340;&#26222;&#36941;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the generation quality of interpolation-based retrieval-augmented language models (LMs). These methods, best exemplified by the KNN-LM, interpolate the LM's predicted distribution of the next word with a distribution formed from the most relevant retrievals for a given prefix. While the KNN-LM and related methods yield impressive decreases in perplexity, we discover that they do not exhibit corresponding improvements in open-ended generation quality, as measured by both automatic evaluation metrics (e.g., MAUVE) and human evaluations. Digging deeper, we find that interpolating with a retrieval distribution actually increases perplexity compared to a baseline Transformer LM for the majority of tokens in the WikiText-103 test set, even though the overall perplexity is lower due to a smaller number of tokens for which perplexity dramatically decreases after interpolation. However, when decoding a long sequence at inference time, significant improvements on this sma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Self-Checker&#26694;&#26550;&#65292;&#23427;&#30001;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#32452;&#25104;&#65292;&#33021;&#22815;&#22312;&#20960;&#20046;&#38646;&#27425;&#21551;&#21160;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24555;&#36895;&#39640;&#25928;&#30340;&#20107;&#23454;&#26816;&#26597;&#65292;&#36825;&#23545;&#20110;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#26500;&#24314;&#20107;&#23454;&#26816;&#26597;&#31995;&#32479;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.14623</link><description>&lt;p&gt;
Self-Checker&#65306;&#29992;&#20110;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#26816;&#26597;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models. (arXiv:2305.14623v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Self-Checker&#26694;&#26550;&#65292;&#23427;&#30001;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#32452;&#25104;&#65292;&#33021;&#22815;&#22312;&#20960;&#20046;&#38646;&#27425;&#21551;&#21160;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24555;&#36895;&#39640;&#25928;&#30340;&#20107;&#23454;&#26816;&#26597;&#65292;&#36825;&#23545;&#20110;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#26500;&#24314;&#20107;&#23454;&#26816;&#26597;&#31995;&#32479;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#26816;&#26597;&#26159;NLP&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#36890;&#24120;&#29992;&#20110;&#39564;&#35777;&#20027;&#24352;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#19978;&#65292;&#36825;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#12290;&#38543;&#30528;&#20687;ChatGPT&#21644;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#30740;&#31350;&#20154;&#21592;&#29616;&#22312;&#27491;&#22312;&#25506;&#32034;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#20197;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Self-Checker&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#32452;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#65292;&#36890;&#36807;&#22312;&#20960;&#20046;&#38646;&#27425;&#21551;&#21160;&#30340;&#24773;&#20917;&#19979;&#20165;&#25552;&#31034;LLMs&#65292;&#20174;&#32780;&#20415;&#20110;&#23545;&#20107;&#23454;&#36827;&#34892;&#26816;&#26597;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#26500;&#24314;&#20107;&#23454;&#26816;&#26597;&#31995;&#32479;&#30340;&#24555;&#36895;&#39640;&#25928;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;Self-Checker&#22312;&#21033;&#29992;LLMs&#36827;&#34892;&#20107;&#23454;&#26816;&#26597;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#19982;SOTA&#24494;&#35843;&#27169;&#22411;&#30456;&#27604;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fact-checking is an essential task in NLP that is commonly utilized for validating the factual accuracy of claims. Prior work has mainly focused on fine-tuning pre-trained languages models on specific datasets, which can be computationally intensive and time-consuming. With the rapid development of large language models (LLMs), such as ChatGPT and GPT-3, researchers are now exploring their in-context learning capabilities for a wide range of tasks. In this paper, we aim to assess the capacity of LLMs for fact-checking by introducing Self-Checker, a framework comprising a set of plug-and-play modules that facilitate fact-checking by purely prompting LLMs in an almost zero-shot setting. This framework provides a fast and efficient way to construct fact-checking systems in low-resource environments. Empirical results demonstrate the potential of Self-Checker in utilizing LLMs for fact-checking. However, there is still significant room for improvement compared to SOTA fine-tuned models, wh
&lt;/p&gt;</description></item><item><title>EXnet&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#26088;&#22312;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#31034;&#20363;&#25968;&#37327;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#25552;&#39640;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#36328;&#20219;&#21153;&#26222;&#36866;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2305.14622</link><description>&lt;p&gt;
EXnet: &#26080;&#25968;&#25454;&#25991;&#26412;&#20998;&#31867;&#30340;&#39640;&#25928;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EXnet: Efficient In-context Learning for Data-less Text classification. (arXiv:2305.14622v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14622
&lt;/p&gt;
&lt;p&gt;
EXnet&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#26088;&#22312;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#31034;&#20363;&#25968;&#37327;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#25552;&#39640;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#36328;&#20219;&#21153;&#26222;&#36866;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#32534;&#30721;&#19990;&#30028;&#30693;&#35782;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#24182;&#20135;&#29983;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#23398;&#20064;&#33539;&#20363;&#65292;&#21253;&#25324;&#38646;-shot&#12289;&#23569;-shot&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#35768;&#22810;&#35821;&#35328;&#20219;&#21153;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#19968;&#32452;&#25552;&#31034;&#65288;&#20363;&#22914;&#65292;&#36825;&#27573;&#25991;&#26412;&#26159;&#21542;&#19982;&#22320;&#29702;&#26377;&#20851;&#65311;&#65289;&#21644;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#20108;&#36827;&#21046;&#31572;&#26696;&#65292;&#21363;&#8220;&#26159;&#8221;&#25110;&#8220;&#21542;&#8221;&#12290;&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#35768;&#22810;PLM&#20351;&#29992;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#19982;&#38646;-shot&#33539;&#20363;&#19981;&#30456;&#31526;&#21512;&#12290;&#22240;&#27492;&#65292;PLMs&#20250;&#34987;&#24494;&#35843;&#20026;&#38382;&#31572;&#31995;&#32479;&#12290;&#19978;&#19979;&#25991;&#23398;&#20064;&#36890;&#36807;&#23558;&#25552;&#31034;&#21644;&#31034;&#20363;&#32467;&#21512;&#36215;&#26469;&#65292;&#25193;&#23637;&#20102;&#38646;-shot&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20171;&#32461;&#20102;EXnet&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22312;&#19978;&#19979;&#25991;&#20013;&#36827;&#34892;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#27809;&#26377;&#20219;&#20309;&#20363;&#22806;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#25552;&#39640;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#31034;&#20363;&#26377;&#21161;&#20110;&#36328;&#20219;&#21153;&#30340;&#26222;&#36866;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models (PLMs) have made significant progress in encoding world knowledge and spawned a new set of learning paradigms including zero-shot, few-shot, and in-context learning. Many language tasks can be modeled as a set of prompts (for example, is this text about geography?) and language models can provide binary answers, i.e., Yes or No. There is evidence to suggest that the next-word prediction used by many PLMs does not align well with zero-shot paradigms. Therefore, PLMs are fine-tuned as a question-answering system. In-context learning extends zero-shot learning by incorporating prompts and examples, resulting in increased task accuracy. Our paper presents EXnet, a model specifically designed to perform in-context learning without any limitations on the number of examples. We argue that in-context learning is an effective method to increase task accuracy, and providing examples facilitates cross-task generalization, especially when it comes to text classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20114;&#26021;&#35299;&#37322;&#30340;&#35825;&#23548;&#24335;&#24120;&#35782;&#25512;&#29702;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31867;&#35825;&#23548;&#24335;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#25110;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20854;&#20182;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2305.14618</link><description>&lt;p&gt;
&#36890;&#36807;&#20114;&#26021;&#35299;&#37322;&#24320;&#23637;&#35825;&#23548;&#24335;&#24120;&#35782;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations. (arXiv:2305.14618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20114;&#26021;&#35299;&#37322;&#30340;&#35825;&#23548;&#24335;&#24120;&#35782;&#25512;&#29702;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31867;&#35825;&#23548;&#24335;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#25110;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20854;&#20182;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35825;&#23548;&#24335;&#25512;&#29702;&#26088;&#22312;&#23547;&#25214;&#19968;&#20010;&#20107;&#20214;&#30340;&#21487;&#33021;&#35299;&#37322;&#12290;&#22312;&#24120;&#35782;&#20219;&#21153;&#20013;&#65292;&#23384;&#22312;&#30528;&#22810;&#31181;&#21512;&#29702;&#30340;&#35299;&#37322;&#24418;&#24335;&#65292;&#22240;&#27492;&#35825;&#23548;&#24335;&#25512;&#29702;&#23545;&#20110;&#36825;&#31867;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20851;&#20110;&#35825;&#23548;&#24335;&#25512;&#29702;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#25163;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#20294;&#26159;&#36825;&#31181;&#26631;&#27880;&#24448;&#24448;&#24102;&#26377;&#20027;&#35266;&#24615;&#21644;&#20559;&#35265;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35825;&#23548;&#24335;&#24120;&#35782;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#23545;&#20110;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;&#65292;&#21482;&#26377;&#35299;&#37322;&#30340;&#19968;&#20010;&#23376;&#38598;&#26159;&#27491;&#30830;&#30340;&#12290;&#26412;&#26041;&#27861;&#21033;&#29992;&#21518;&#39564;&#35268;&#33539;&#21270;&#26469;&#26045;&#21152;&#20114;&#26021;&#32422;&#26463;&#65292;&#40723;&#21169;&#27169;&#22411;&#23398;&#20064;&#27969;&#30021;&#35299;&#37322;&#21644;&#21512;&#29702;&#30340;&#35299;&#37322;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#25105;&#20204;&#22312;&#21508;&#31867;&#35825;&#23548;&#24335;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65307;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#20248;&#20110;&#25110;&#19982;&#30452;&#25509;&#24212;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20854;&#20182;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abductive reasoning aims to find plausible explanations for an event. This style of reasoning is critical for commonsense tasks where there are often multiple plausible explanations. Existing approaches for abductive reasoning in natural language processing (NLP) often rely on manually generated annotations for supervision; however, such annotations can be subjective and biased. Instead of using direct supervision, this work proposes an approach for abductive commonsense reasoning that exploits the fact that only a subset of explanations is correct for a given context. The method uses posterior regularization to enforce a mutual exclusion constraint, encouraging the model to learn the distinction between fluent explanations and plausible ones. We evaluate our approach on a diverse set of abductive reasoning datasets; experimental results show that our approach outperforms or is comparable to directly applying pretrained language models in a zero-shot manner and other knowledge-augmente
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;COMET-M&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25512;&#29702;&#22797;&#26434;&#21477;&#23376;&#20013;&#22810;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#29983;&#25104;&#24120;&#35782;&#25512;&#26029;&#65292;&#24182;&#22312;35K&#20010;&#20154;&#31867;&#32534;&#20889;&#30340;&#25512;&#26029;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;COMET&#27169;&#22411;&#22312;&#29983;&#25104;&#22810;&#20107;&#20214;&#25512;&#26029;&#26041;&#38754;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.14617</link><description>&lt;p&gt;
COMET-M: &#22312;&#22797;&#26434;&#21477;&#23376;&#20013;&#25512;&#29702;&#22810;&#20010;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
COMET-M: Reasoning about Multiple Events in Complex Sentences. (arXiv:2305.14617v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14617
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;COMET-M&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25512;&#29702;&#22797;&#26434;&#21477;&#23376;&#20013;&#22810;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#29983;&#25104;&#24120;&#35782;&#25512;&#26029;&#65292;&#24182;&#22312;35K&#20010;&#20154;&#31867;&#32534;&#20889;&#30340;&#25512;&#26029;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;COMET&#27169;&#22411;&#22312;&#29983;&#25104;&#22810;&#20107;&#20214;&#25512;&#26029;&#26041;&#38754;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#36890;&#24120;&#28041;&#21450;&#32472;&#21046;&#24120;&#35782;&#25512;&#26029;&#65292;&#20197;&#25512;&#29702;&#26410;&#26126;&#30830;&#38472;&#36848;&#30340;&#20869;&#23481;&#12290;&#22312;&#22810;&#20107;&#20214;&#21477;&#23376;&#20013;&#65292;&#38656;&#35201;&#22522;&#20110;&#19978;&#19979;&#25991;&#30693;&#35782;&#29702;&#35299;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;COMET-M&#65288;Multi-Event&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#24120;&#35782;&#27169;&#22411;&#65292;&#33021;&#22815;&#38024;&#23545;&#22797;&#26434;&#21477;&#23376;&#20869;&#30340;&#30446;&#26631;&#20107;&#20214;&#29983;&#25104;&#24120;&#35782;&#25512;&#26029;&#12290;COMET-M&#26159;&#22522;&#20110;COMET&#65288;Bosselut et al.&#65292;2019&#65289;&#21457;&#23637;&#32780;&#26469;&#30340;&#65292;&#21518;&#32773;&#25797;&#38271;&#20026;&#31616;&#21333;&#21477;&#23376;&#29983;&#25104;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#25512;&#26029;&#65292;&#20294;&#22312;&#33258;&#28982;&#25991;&#26412;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#22810;&#20107;&#20214;&#21477;&#23376;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20010;&#21253;&#21547;35K&#20010;&#20154;&#31867;&#32534;&#20889;&#25512;&#26029;&#30340;&#22810;&#20107;&#20214;&#25512;&#26029;&#25968;&#25454;&#38598;&#12290; &#25105;&#20204;&#22312;&#20154;&#31867;&#32534;&#20889;&#30340;&#25512;&#26029;&#19978;&#35757;&#32451;&#20102;COMET-M&#65292;&#24182;&#21019;&#24314;&#20102;&#20351;&#29992;&#33258;&#21160;&#26631;&#35760;&#31034;&#20363;&#30340;&#22522;&#32447;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;COMET-M&#22312;&#29983;&#25104;&#22810;&#20107;&#20214;&#25512;&#26029;&#26041;&#38754;&#30456;&#23545;&#20110;COMET&#20855;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;COMET-M&#25104;&#21151;&#39044;&#27979;&#20102;&#27979;&#35797;&#38598;&#20013;60&#65285;&#30340;&#22797;&#26434;&#21477;&#23376;&#30446;&#26631;&#20107;&#20214;&#30340;&#24120;&#35782;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the speaker's intended meaning often involves drawing commonsense inferences to reason about what is not stated explicitly. In multi-event sentences, it requires understanding the relationships between events based on contextual knowledge. We propose COMET-M (Multi-Event), an event-centric commonsense model capable of generating commonsense inferences for a target event within a complex sentence. COMET-M builds upon COMET (Bosselut et al., 2019), which excels at generating event-centric inferences for simple sentences, but struggles with the complexity of multi-event sentences prevalent in natural text. To overcome this limitation, we curate a multi-event inference dataset of 35K human-written inferences. We trained COMET-M on the human-written inferences and also created baselines using automatically labeled examples. Experimental results demonstrate the significant performance improvement of COMET-M over COMET in generating multi-event inferences. Moreover, COMET-M succ
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20026;&#25105;&#20204;&#20174;&#35745;&#31639;&#35748;&#30693;&#35821;&#35328;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#22270;&#20687;&#25551;&#36848;&#20013;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#34920;&#24449;&#20013;&#30340;&#22522;&#30784;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24773;&#22659;&#21547;&#20041;&#21644;&#21151;&#33021;&#24615;&#26159;&#29983;&#25104;&#36866;&#24403;&#30340;&#22270;&#20687;&#25551;&#36848;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2305.14616</link><description>&lt;p&gt;
&#22312;&#22270;&#20687;&#25551;&#36848;&#20013;&#25506;&#35752;&#23545;&#22522;&#30784;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Exploring the Grounding Issues in Image Caption. (arXiv:2305.14616v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14616
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20026;&#25105;&#20204;&#20174;&#35745;&#31639;&#35748;&#30693;&#35821;&#35328;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#22270;&#20687;&#25551;&#36848;&#20013;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#34920;&#24449;&#20013;&#30340;&#22522;&#30784;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24773;&#22659;&#21547;&#20041;&#21644;&#21151;&#33021;&#24615;&#26159;&#29983;&#25104;&#36866;&#24403;&#30340;&#22270;&#20687;&#25551;&#36848;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20174;&#35745;&#31639;&#35748;&#30693;&#35821;&#35328;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#35821;&#20041;&#34920;&#24449;&#20013;&#30340;&#22522;&#30784;&#38382;&#39064;&#12290;&#37319;&#29992;&#20102;&#24863;&#30693;&#24615;&#12289;&#21151;&#33021;&#24615;&#12289;&#26174;&#33879;&#24615;&#12289;&#27880;&#24847;&#21147;&#21644;&#29983;&#24577;&#23398;&#22810;&#26679;&#24615;&#20851;&#32852;&#31561;&#20116;&#20010;&#22522;&#30784;&#23646;&#24615;&#36827;&#34892;&#27880;&#37322;&#21644;&#20998;&#26512;&#12290;&#36890;&#36807;&#23545;Flickr30k&#25968;&#25454;&#38598;&#20013;&#25152;&#36873;&#30340;&#22270;&#20687;&#36827;&#34892;&#25506;&#32034;&#24615;&#20998;&#26512;&#21644;&#32479;&#35745;&#24314;&#27169;&#26469;&#36827;&#34892;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#19968;&#20010;&#23545;&#35937;&#25110;&#20107;&#20214;&#30340;&#20840;&#38754;&#29702;&#35299;&#38656;&#35201;&#35748;&#30693;&#27880;&#24847;&#21147;&#12289;&#35821;&#20041;&#34920;&#36798;&#30340;&#35821;&#20041;&#21306;&#20998;&#21644;&#22810;&#27169;&#24577;&#26500;&#24314;&#12290;&#22312;&#26500;&#24314;&#36807;&#31243;&#20013;&#65292;&#35266;&#23519;&#32773;&#23558;&#24773;&#22659;&#21547;&#20041;&#21644;&#21151;&#33021;&#24615;&#34701;&#20837;&#21040;&#22810;&#27169;&#24577;&#35821;&#20041;&#24403;&#20013;&#65292;&#23558;&#20854;&#24041;&#22266;&#21040;&#21253;&#21547;&#35270;&#35273;&#21644;&#25991;&#26412;&#20803;&#32032;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#30340;&#22270;&#20687;&#25551;&#36848;&#20013;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24773;&#22659;&#21547;&#20041;&#21644;&#21151;&#33021;&#24615;&#23545;&#20110;&#22522;&#30784;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#29983;&#25104;&#36866;&#24403;&#30340;&#22270;&#20687;&#25551;&#36848;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the grounding issue concerning multimodal semantic representation from a computational cognitive-linguistic view. Five perceptual properties of groundedness are annotated and analyzed: Affordance, Perceptual salience, Object number, Gaze cueing, and Ecological Niche Association (ENA). We annotated selected images from the Flickr30k dataset with exploratory analyses and statistical modeling of their captions. Our findings suggest that a comprehensive understanding of an object or event requires cognitive attention, semantic distinctions in linguistic expression, and multimodal construction. During this construction process, viewers integrate situated meaning and affordance into multimodal semantics, which is consolidated into image captions used in the image-text dataset incorporating visual and textual elements. Our findings suggest that situated meaning and affordance grounding are critical for grounded natural language understanding systems to generate appropriate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35299;&#20915;&#27169;&#31946;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#37327;&#27979;&#37327;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#37325;&#22797;&#24615;&#65292;&#25214;&#20986;&#20102;&#22312;&#21547;&#31946;&#38382;&#39064;&#38598;&#20013;&#22238;&#31572;&#39640;&#31934;&#24230;&#23376;&#38598;&#38382;&#39064;&#30340;&#26368;&#21487;&#38752;&#26041;&#27861;&#12290;&#36825;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#27495;&#20041;&#65292;&#24182;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14613</link><description>&lt;p&gt;
&#23545;&#27169;&#31946;&#38382;&#39064;&#30340;&#26377;&#36873;&#25321;&#24615;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Selectively Answering Ambiguous Questions. (arXiv:2305.14613v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35299;&#20915;&#27169;&#31946;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#37327;&#27979;&#37327;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#37325;&#22797;&#24615;&#65292;&#25214;&#20986;&#20102;&#22312;&#21547;&#31946;&#38382;&#39064;&#38598;&#20013;&#22238;&#31572;&#39640;&#31934;&#24230;&#23376;&#38598;&#38382;&#39064;&#30340;&#26368;&#21487;&#38752;&#26041;&#27861;&#12290;&#36825;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#27495;&#20041;&#65292;&#24182;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#22312;&#19981;&#30693;&#36947;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#25918;&#24323;&#22238;&#31572;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38382;&#38382;&#32773;&#24847;&#22270;&#25110;&#19978;&#19979;&#25991;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#38382;&#39064;&#30340;&#31572;&#26696;&#20063;&#21487;&#33021;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#20174;&#36825;&#20010;&#35282;&#24230;&#35843;&#26597;&#20102;&#38382;&#39064;&#22238;&#31572;&#65292;&#19987;&#27880;&#20110;&#22312;&#20247;&#22810;&#26412;&#36136;&#19978;&#21547;&#31946;&#30340;&#38382;&#39064;&#38598;&#20013;&#22238;&#31572;&#39640;&#31934;&#24230;&#23376;&#38598;&#30340;&#38382;&#39064;&#12290;&#22312;&#27492;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#23450;&#37327;&#27979;&#37327;&#19968;&#32452;&#37319;&#26679;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#37325;&#22797;&#24615;&#26159;&#26368;&#21487;&#38752;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#32780;&#38750;&#20808;&#21069;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#30340;&#27010;&#29575;&#25110;&#33258;&#25105;&#39564;&#35777;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#19981;&#21516;&#30340;&#27169;&#22411;&#35268;&#27169;&#65292;&#20197;&#21450;&#24102;&#25110;&#19981;&#24102;&#25351;&#23548;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#27495;&#20041;&#65292;&#24182;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trustworthy language models should abstain from answering questions when they do not know the answer. However, the answer to a question can be unknown for a variety of reasons. Prior research has focused on the case in which the question is clear and the answer is unambiguous but possibly unknown. However, the answer to a question can also be unclear due to uncertainty of the questioner's intent or context. We investigate question answering from this perspective, focusing on answering a subset of questions with a high degree of accuracy, from a set of questions in which many are inherently ambiguous. In this setting, we find that the most reliable approach to calibration involves quantifying repetition within a set of sampled model outputs, rather than the model's likelihood or self-verification as used in prior work. % We find this to be the case across different types of uncertainty, varying model scales and both with or without instruction tuning. Our results suggest that sampling-b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#30340;&#27010;&#24565;&#65292;&#24182;&#20197;&#39046;&#22303;&#20105;&#31471;&#20026;&#20363;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#12289;&#22810;&#36873;&#39064;&#30340;&#25968;&#25454;&#38598;BorderLines&#21644;&#20960;&#20010;&#23450;&#37327;&#25351;&#26631;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#20013;&#30340;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2305.14610</link><description>&lt;p&gt;
&#36825;&#29255;&#22303;&#22320;&#26159;&#20320;&#25105;&#30340;&#22303;&#22320;&#65306;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models. (arXiv:2305.14610v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#30340;&#27010;&#24565;&#65292;&#24182;&#20197;&#39046;&#22303;&#20105;&#31471;&#20026;&#20363;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#12289;&#22810;&#36873;&#39064;&#30340;&#25968;&#25454;&#38598;BorderLines&#21644;&#20960;&#20010;&#23450;&#37327;&#25351;&#26631;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#20013;&#30340;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#30340;&#27010;&#24565;&#8212;&#8212;&#21363;&#26681;&#25454;&#35821;&#35328;&#29615;&#22659;&#25253;&#36947;&#19981;&#21516;&#30340;&#22320;&#32536;&#25919;&#27835;&#30693;&#35782;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#20197;&#39046;&#22303;&#20105;&#31471;&#20026;&#26696;&#20363;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#34987;&#24191;&#27867;&#20105;&#35758;&#30340;&#21335;&#27801;&#32676;&#23707;&#65292;&#22914;&#26524;&#29992;&#20013;&#25991;&#38382;&#65292;LM&#26159;&#21542;&#26356;&#26377;&#21487;&#33021;&#35828;&#23427;&#20204;&#23646;&#20110;&#20013;&#22269;&#65292;&#32780;&#22914;&#26524;&#29992;&#22612;&#21152;&#27931;&#35821;&#38382;&#65292;&#21017;&#26356;&#26377;&#21487;&#33021;&#35828;&#23427;&#20204;&#23646;&#20110;&#33778;&#24459;&#23486;&#65311;&#20026;&#20102;&#35780;&#20272;&#26159;&#21542;&#23384;&#22312;&#36825;&#31181;&#20559;&#35265;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#32500;&#22522;&#30334;&#31185;&#19978;&#25910;&#38598;&#20102;&#19968;&#32452;&#39046;&#22303;&#20105;&#31471;&#25968;&#25454;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#39046;&#22303;&#19982;&#19968;&#32452;&#22810;&#35821;&#35328;&#12289;&#22810;&#36873;&#39064;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#34987;&#31216;&#20026;BorderLines&#65292;&#23427;&#21253;&#25324;250&#20010;&#39046;&#22303;&#21644;45&#31181;&#35821;&#35328;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#38382;&#39064;&#38598;&#25552;&#20132;&#32473;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20960;&#20010;&#25552;&#20986;&#30340;&#23450;&#37327;&#25351;&#26631;&#20998;&#26512;&#23427;&#20204;&#30340;&#21709;&#24212;&#20013;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#12290;&#36825;&#20123;&#25351;&#26631;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#30340;&#22238;&#31572;&#20197;&#21450;&#23454;&#38469;&#30340;&#22320;&#32536;&#25919;&#27835;&#24773;&#20917;&#12290;&#22320;&#32536;&#25919;&#27835;&#20559;&#35265;&#29616;&#35937;&#26159;&#19968;&#31181;&#29420;&#29305;&#30340;&#36328;&#35821;&#35328;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the notion of geopolitical bias -- a tendency to report different geopolitical knowledge depending on the linguistic context. As a case study, we consider territorial disputes between countries. For example, for the widely contested Spratly Islands, would an LM be more likely to say they belong to China if asked in Chinese, vs. to the Philippines if asked in Tagalog? To evaluate if such biases exist, we first collect a dataset of territorial disputes from Wikipedia, then associate each territory with a set of multilingual, multiple-choice questions. This dataset, termed BorderLines, consists of 250 territories with questions in 45 languages. We pose these question sets to language models, and analyze geopolitical bias in their responses through several proposed quantitative metrics. The metrics compare between responses in different question languages as well as to the actual geopolitical situation. The phenomenon of geopolitical bias is a uniquely cross-lingual evaluation
&lt;/p&gt;</description></item><item><title>OpenPI2.0&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#20307;&#36861;&#36394;&#30340;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#23427;&#21253;&#25324;&#35268;&#33539;&#21270;&#23454;&#20307;&#12289;&#26174;&#33879;&#24615;&#27880;&#37322;&#21644;&#19979;&#28216;&#24212;&#29992;&#35843;&#26597;&#31561;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.14603</link><description>&lt;p&gt;
OpenPI2.0: &#19968;&#31181;&#29992;&#20110;&#23454;&#20307;&#36861;&#36394;&#30340;&#25913;&#36827;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OpenPI2.0: An Improved Dataset for Entity Tracking in Texts. (arXiv:2305.14603v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14603
&lt;/p&gt;
&lt;p&gt;
OpenPI2.0&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#20307;&#36861;&#36394;&#30340;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#23427;&#21253;&#25324;&#35268;&#33539;&#21270;&#23454;&#20307;&#12289;&#26174;&#33879;&#24615;&#27880;&#37322;&#21644;&#19979;&#28216;&#24212;&#29992;&#35843;&#26597;&#31561;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#25991;&#26412;&#34920;&#31034;&#20026;&#23454;&#20307;&#20449;&#24687;&#19968;&#30452;&#34987;&#35748;&#20026;&#22312;&#20107;&#20214;&#25512;&#29702;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;OpenPI2.0&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#36861;&#36394;&#31243;&#24207;&#24615;&#25991;&#26412;&#20013;&#23454;&#20307;&#29366;&#24577;&#30340;&#25913;&#36827;&#25968;&#25454;&#38598;&#12290;OpenPI2.0&#19981;&#20165;&#20855;&#26377;&#35268;&#33539;&#21270;&#23454;&#20307;&#20197;&#20419;&#36827;&#35780;&#20272;&#65292;&#36824;&#21253;&#25324;&#28085;&#30422;&#20154;&#24037;&#26631;&#31614;&#21644;&#33258;&#21160;&#39044;&#27979;&#30340;&#26174;&#33879;&#24615;&#27880;&#37322;&#12290;&#20851;&#20110;&#23454;&#20307;&#26174;&#33879;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#27880;&#37322;&#20027;&#35266;&#24615;&#12289;&#24314;&#27169;&#21487;&#34892;&#24615;&#20197;&#21450;&#22312;&#38382;&#39064;&#22238;&#31572;&#21644;&#32463;&#20856;&#35745;&#21010;&#31561;&#20219;&#21153;&#20013;&#30340;&#19979;&#28216;&#24212;&#29992;&#30340;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representing texts as information about entities has long been deemed effective in event reasoning. We propose OpenPI2.0, an improved dataset for tracking entity states in procedural texts. OpenPI2.0 features not only canonicalized entities that facilitate evaluation, but also salience annotations including both manual labels and automatic predictions. Regarding entity salience, we provide a survey on annotation subjectivity, modeling feasibility, and downstream applications in tasks such as question answering and classical planning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#19981;&#30456;&#20132;&#30340;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#23558;&#27492;&#36816;&#29992;&#20110;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#22788;&#29702;VerbNet&#21644;PropBank&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14600</link><description>&lt;p&gt;
&#23398;&#20064;&#20174;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Learning Semantic Role Labeling from Compatible Label Sequences. (arXiv:2305.14600v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14600
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#19981;&#30456;&#20132;&#30340;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#23558;&#27492;&#36816;&#29992;&#20110;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#22788;&#29702;VerbNet&#21644;PropBank&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#23398;&#20064;&#20174;&#19981;&#30456;&#20132;&#30340;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#26631;&#27880;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19981;&#30456;&#20132;&#26631;&#31614;&#38598;&#20043;&#38388;&#30340;&#20860;&#23481;&#32467;&#26500;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65288;SRL&#65289;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#36825;&#19968;&#20551;&#35774;&#65292;&#20855;&#20307;&#22320;&#65292;&#26631;&#35760;&#20855;&#26377;&#20004;&#20010;&#35282;&#33394;&#24207;&#21015;&#30340;&#21477;&#23376;&#65306;VerbNet&#21442;&#25968;&#21644;PropBank&#21442;&#25968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#36328;&#20219;&#21153;&#20132;&#20114;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#36825;&#20004;&#20010;&#20219;&#21153;&#20173;&#28982;&#26159;&#20998;&#21035;&#35299;&#30721;&#30340;&#65292;&#23384;&#22312;&#29983;&#25104;&#32467;&#26500;&#19981;&#19968;&#33268;&#30340;&#26631;&#31614;&#24207;&#21015; (&#22312;&#20687;SEMLINK&#30340;&#35789;&#20856;&#20013;)&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35774;&#32622;&#65292;&#32852;&#21512;&#22788;&#29702;VerbNet&#21644;PropBank&#26631;&#31614;&#20316;&#20026;&#19968;&#20010;&#24207;&#21015;&#12290;&#36890;&#36807;&#36825;&#20010;&#35774;&#32622;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#24378;&#21046;&#25191;&#34892;SEMLINK&#32422;&#26463;&#19981;&#26029;&#25552;&#39640;&#24635;F1&#20540;&#12290;&#36890;&#36807;&#29305;&#27530;&#30340;&#36755;&#20837;&#26500;&#36896;&#65292;&#25105;&#20204;&#30340;&#32852;&#21512;&#27169;&#22411;&#21487;&#20197;&#20197;&#36229;&#36807;99%&#30340;&#20934;&#30830;&#24615;&#20174;PropBank&#21442;&#25968;&#20013;&#25512;&#26029;&#20986;VerbNet&#21442;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;co
&lt;/p&gt;
&lt;p&gt;
This paper addresses the question of how to efficiently learn from disjoint, compatible label sequences. We argue that the compatible structures between disjoint label sets help model learning and inference. We verify this hypothesis on the task of semantic role labeling (SRL), specifically, tagging a sentence with two role sequences: VerbNet arguments and PropBank arguments. Prior work has shown that cross-task interaction improves performance. However, the two tasks are still separately decoded, running the risk of generating structurally inconsistent label sequences (as per lexicons like SEMLINK). To eliminate this issue, we first propose a simple and effective setup that jointly handles VerbNet and PropBank labels as one sequence. With this setup, we show that enforcing SEMLINK constraints during decoding constantly improves the overall F1. With special input constructions, our joint model infers VerbNet arguments from PropBank arguments with over 99% accuracy. We also propose a co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InterSent&#26694;&#26550;&#65292;&#25506;&#32034;&#23558;&#19981;&#21516;&#30340;&#32452;&#21512;&#23646;&#24615;&#32435;&#20837;&#21477;&#23376;&#23884;&#20837;&#31354;&#38388;&#20013;&#20351;&#23884;&#20837;&#21464;&#25442;&#21464;&#20026;&#32452;&#21512;&#21477;&#23376;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.14599</link><description>&lt;p&gt;
&#36830;&#32493;&#21644;&#31163;&#25955;&#31354;&#38388;&#30340;&#26725;&#26753;: &#36890;&#36807;&#32452;&#21512;&#25805;&#20316;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations. (arXiv:2305.14599v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InterSent&#26694;&#26550;&#65292;&#25506;&#32034;&#23558;&#19981;&#21516;&#30340;&#32452;&#21512;&#23646;&#24615;&#32435;&#20837;&#21477;&#23376;&#23884;&#20837;&#31354;&#38388;&#20013;&#20351;&#23884;&#20837;&#21464;&#25442;&#21464;&#20026;&#32452;&#21512;&#21477;&#23376;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#23558;&#21477;&#23376;&#32534;&#30721;&#20026;&#21521;&#37327;&#34920;&#31034;&#65292;&#20197;&#25429;&#25417;&#35832;&#22914;&#21477;&#23376;&#35821;&#20041;&#30456;&#20284;&#24615;&#20043;&#31867;&#30340;&#26377;&#29992;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#30456;&#20284;&#24615;&#22806;&#65292;&#21477;&#23376;&#35821;&#20041;&#36824;&#21487;&#20197;&#36890;&#36807;&#32452;&#21512;&#25805;&#20316;&#65288;&#22914;&#21477;&#23376;&#34701;&#21512;&#25110;&#24046;&#24322;&#65289;&#36827;&#34892;&#35299;&#37322;&#12290;&#19981;&#28165;&#26970;&#26159;&#21542;&#21487;&#20197;&#30452;&#25509;&#23558;&#21477;&#23376;&#30340;&#32452;&#21512;&#35821;&#20041;&#21453;&#26144;&#20026;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#32452;&#21512;&#25805;&#20316;&#12290;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#36830;&#25509;&#36830;&#32493;&#23884;&#20837;&#21644;&#31163;&#25955;&#25991;&#26412;&#31354;&#38388;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#21508;&#31181;&#32452;&#21512;&#24615;&#36136;&#21512;&#24182;&#21040;&#21477;&#23376;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#21487;&#34892;&#24615;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#23558;&#23884;&#20837;&#21464;&#25442;&#35299;&#37322;&#20026;&#32452;&#21512;&#21477;&#23376;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;InterSent&#65292;&#19968;&#31181;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#21477;&#23376;&#23884;&#20837;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#23427;&#25903;&#25345;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#32452;&#21512;&#21477;&#23376;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#21270;&#25805;&#20316;&#22120;&#32593;&#32476;&#21644;&#29942;&#39048;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#29983;&#25104;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional sentence embedding models encode sentences into vector representations to capture useful properties such as the semantic similarity between sentences. However, in addition to similarity, sentence semantics can also be interpreted via compositional operations such as sentence fusion or difference. It is unclear whether the compositional semantics of sentences can be directly reflected as compositional operations in the embedding space. To more effectively bridge the continuous embedding and discrete text spaces, we explore the plausibility of incorporating various compositional properties into the sentence embedding space that allows us to interpret embedding transformations as compositional sentence operations. We propose InterSent, an end-to-end framework for learning interpretable sentence embeddings that supports compositional sentence operations in the embedding space. Our method optimizes operator networks and a bottleneck encoder-decoder model to produce meaningful an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;AI&#23398;&#26415;&#30028;&#30340;78K&#30740;&#31350;&#20154;&#21592;&#30340;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#22899;&#24615;&#31532;&#19968;&#20316;&#32773;&#30340;&#35770;&#25991;&#20855;&#26377;&#19981;&#21516;&#30340;&#35821;&#35328;&#39118;&#26684;&#65292;&#20363;&#22914;&#26356;&#38271;&#30340;&#25991;&#26412;&#12289;&#26356;&#22810;&#30340;&#27491;&#38754;&#24773;&#24863;&#35789;&#27719;&#21644;&#26356;&#24341;&#20154;&#27880;&#30446;&#30340;&#26631;&#39064;&#65307;&#22312;AI&#35770;&#25991;&#30340;&#21512;&#33879;&#20013;&#23384;&#22312;&#24456;&#22823;&#30340;&#24615;&#21035;&#21516;&#36136;&#24615;&#12290;&#25105;&#20204;&#40723;&#21169;&#26410;&#26469;&#23454;&#29616;&#26356;&#22810;&#30340;&#24615;&#21035;&#24179;&#31561;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14597</link><description>&lt;p&gt;
&#22905;&#20204;&#30340;&#22768;&#38899;&#65306;&#20998;&#26512;&#20154;&#24037;&#26234;&#33021;&#20986;&#29256;&#39046;&#22495;&#30340;&#24615;&#21035;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Voices of Her: Analyzing Gender Differences in the AI Publication World. (arXiv:2305.14597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14597
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;AI&#23398;&#26415;&#30028;&#30340;78K&#30740;&#31350;&#20154;&#21592;&#30340;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#22899;&#24615;&#31532;&#19968;&#20316;&#32773;&#30340;&#35770;&#25991;&#20855;&#26377;&#19981;&#21516;&#30340;&#35821;&#35328;&#39118;&#26684;&#65292;&#20363;&#22914;&#26356;&#38271;&#30340;&#25991;&#26412;&#12289;&#26356;&#22810;&#30340;&#27491;&#38754;&#24773;&#24863;&#35789;&#27719;&#21644;&#26356;&#24341;&#20154;&#27880;&#30446;&#30340;&#26631;&#39064;&#65307;&#22312;AI&#35770;&#25991;&#30340;&#21512;&#33879;&#20013;&#23384;&#22312;&#24456;&#22823;&#30340;&#24615;&#21035;&#21516;&#36136;&#24615;&#12290;&#25105;&#20204;&#40723;&#21169;&#26410;&#26469;&#23454;&#29616;&#26356;&#22810;&#30340;&#24615;&#21035;&#24179;&#31561;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#20998;&#26512;&#20102;&#23398;&#26415;&#30028;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;&#26159;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#30340;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#24615;&#21035;&#24046;&#24322;&#30340;&#20998;&#26512;&#65292;&#28085;&#30422;&#21508;&#31181;&#20027;&#39064;&#21644;&#19981;&#21516;&#30340;&#21457;&#23637;&#36235;&#21183;&#12290;&#25105;&#20204;&#20351;&#29992;AI Scholar&#25968;&#25454;&#38598;&#20013;&#30340;78K&#20301;AI&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#21457;&#29616;&#20102;&#19968;&#20123;&#24615;&#21035;&#24046;&#24322;&#65306;&#65288;1&#65289;&#34429;&#28982;&#22899;&#24615;&#30740;&#31350;&#20154;&#21592;&#30340;&#24635;&#24341;&#29992;&#27425;&#25968;&#27604;&#30007;&#24615;&#23569;&#65292;&#20294;&#36825;&#31181;&#24341;&#29992;&#24046;&#24322;&#24182;&#19981;&#36866;&#29992;&#20110;&#25152;&#26377;&#23398;&#26415;&#24180;&#40836;&#32452;&#65307;&#65288;2&#65289;&#22312;AI&#35770;&#25991;&#30340;&#21512;&#33879;&#20013;&#23384;&#22312;&#24456;&#22823;&#30340;&#24615;&#21035;&#21516;&#36136;&#24615;&#65307;&#65288;3&#65289;&#22899;&#24615;&#31532;&#19968;&#20316;&#32773;&#30340;&#35770;&#25991;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#35821;&#35328;&#39118;&#26684;&#65292;&#20363;&#22914;&#26356;&#38271;&#30340;&#25991;&#26412;&#12289;&#26356;&#22810;&#30340;&#27491;&#38754;&#24773;&#24863;&#35789;&#27719;&#21644;&#26356;&#24341;&#20154;&#27880;&#30446;&#30340;&#26631;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20026;&#25105;&#20204;&#30340;AI&#31038;&#21306;&#29616;&#26377;&#30340;&#20154;&#21475;&#32479;&#35745;&#36235;&#21183;&#25552;&#20379;&#20102;&#19968;&#20010;&#31383;&#21475;&#65292;&#24182;&#40723;&#21169;&#22312;&#26410;&#26469;&#23454;&#29616;&#26356;&#22810;&#30340;&#24615;&#21035;&#24179;&#31561;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.com/causalNLP/ai-scholar-gender&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
While several previous studies have analyzed gender bias in research, we are still missing a comprehensive analysis of gender differences in the AI community, covering diverse topics and different development trends. Using the AI Scholar dataset of 78K researchers in the field of AI, we identify several gender differences: (1) Although female researchers tend to have fewer overall citations than males, this citation difference does not hold for all academic-age groups; (2) There exist large gender homophily in co-authorship on AI papers; (3) Female first-authored papers show distinct linguistic styles, such as longer text, more positive emotion words, and more catchy titles than male first-authored papers. Our analysis provides a window into the current demographic trends in our AI community, and encourages more gender equality and diversity in the future. Our code and data are at https://github.com/causalNLP/ai-scholar-gender.
&lt;/p&gt;</description></item><item><title>&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#20854;&#27880;&#24847;&#21147;&#24448;&#24448;&#20250;&#20998;&#25955;&#21040;&#35768;&#22810;&#26080;&#25928;&#30340;&#35789;&#27719;&#31526;&#21495;&#19978;&#65292;&#36825;&#20250;&#23548;&#33268;&#27169;&#22411;&#30495;&#23454;&#24615;&#33021;&#30340;&#20302;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;&#21482;&#21547;&#19968;&#20010;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;&#26377;&#25928;&#36873;&#25321;&#30340;&#27880;&#24847;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14596</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#22312;&#35299;&#31572;&#20013;&#36873;&#25321;&#27491;&#30830;&#29575;&#24456;&#39640;
&lt;/p&gt;
&lt;p&gt;
Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy. (arXiv:2305.14596v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14596
&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#20854;&#27880;&#24847;&#21147;&#24448;&#24448;&#20250;&#20998;&#25955;&#21040;&#35768;&#22810;&#26080;&#25928;&#30340;&#35789;&#27719;&#31526;&#21495;&#19978;&#65292;&#36825;&#20250;&#23548;&#33268;&#27169;&#22411;&#30495;&#23454;&#24615;&#33021;&#30340;&#20302;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;&#21482;&#21547;&#19968;&#20010;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;&#26377;&#25928;&#36873;&#25321;&#30340;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#24212;&#29992;&#20110;&#38646;&#25110;&#23569;&#26679;&#26412;&#30340;&#37492;&#21035;&#24615;&#20219;&#21153;&#65292;&#20363;&#22914;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#23427;&#20204;&#30340;&#27880;&#24847;&#21147;&#65288;&#21363;&#27010;&#29575;&#36136;&#37327;&#65289;&#20250;&#20998;&#25955;&#22312;&#35768;&#22810;&#26080;&#25928;&#30340;&#35789;&#27719;&#31526;&#21495;&#19978;&#12290;&#36825;&#31181;&#22312;&#20855;&#26377;&#30456;&#21516;&#21547;&#20041;&#30340;&#22810;&#20010;&#34920;&#38754;&#24418;&#24335;&#20043;&#38388;&#20998;&#25955;&#23548;&#33268;&#20102;&#27169;&#22411;&#30495;&#23454;&#24615;&#33021;&#30340;&#20302;&#20272;&#65292;&#31216;&#20026;&#8220;&#34920;&#38754;&#24418;&#24335;&#31454;&#20105;&#8221;&#65288;SFC&#65289;&#20551;&#35828;&#12290;&#36825;&#20419;&#20351;&#24341;&#20837;&#21508;&#31181;&#27010;&#29575;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#28982;&#32780;&#20173;&#23384;&#22312;&#35768;&#22810;&#26680;&#24515;&#38382;&#39064;&#26410;&#35299;&#31572;&#12290;&#25105;&#20204;&#22914;&#20309;&#27979;&#37327;SFC&#25110;&#27880;&#24847;&#21147;&#65311;&#26159;&#21542;&#26377;&#30452;&#25509;&#30340;&#26041;&#27861;&#21487;&#20197;&#22686;&#21152;&#23545;&#26377;&#25928;&#36873;&#25321;&#30340;&#27880;&#24847;&#21147;&#65311;&#22686;&#21152;&#27880;&#24847;&#21147;&#24635;&#26159;&#33021;&#25552;&#39640;&#20219;&#21153;&#20934;&#30830;&#24615;&#21527;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#27880;&#24847;&#21147;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22686;&#21152;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#20165;&#21253;&#21547;&#31572;&#26696;&#36873;&#39033;&#30340;&#19968;&#20010;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
When large language models (LMs) are applied in zero- or few-shot settings to discriminative tasks such as multiple-choice questions, their attentiveness (i.e., probability mass) is spread across many vocabulary tokens that are not valid choices. Such a spread across multiple surface forms with identical meaning is thought to cause an underestimation of a model's true performance, referred to as the "surface form competition" (SFC) hypothesis. This has motivated the introduction of various probability normalization methods. However, many core questions remain unanswered. How do we measure SFC or attentiveness? Are there direct ways of increasing attentiveness on valid choices? Does increasing attentiveness always improve task accuracy? We propose a mathematical formalism for studying this phenomenon, provide a metric for quantifying attentiveness, and identify a simple method for increasing it -namely, in-context learning with even just one example containing answer choices. The form
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#35789;&#20856;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#39118;&#26684;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#38646;&#26679;&#26412;&#24615;&#33021;&#30340;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2305.14592</link><description>&lt;p&gt;
&#24102;&#35789;&#20856;&#30340;&#25351;&#20196;&#20248;&#21270;&#29992;&#20110;&#38646;&#26679;&#24335;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning with Lexicons for Zero-Shot Style Classification. (arXiv:2305.14592v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14592
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#35789;&#20856;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#39118;&#26684;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#38646;&#26679;&#26412;&#24615;&#33021;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#26684;&#29992;&#20110;&#20256;&#36798;&#20316;&#32773;&#30340;&#24847;&#22270;&#21644;&#24577;&#24230;&#12290;&#23613;&#31649;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#39118;&#26684;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20808;&#21069;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#24102;&#26631;&#31614;&#30340;&#26679;&#26412;&#36827;&#34892;&#24494;&#35843;&#12290;&#21551;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23545;&#39118;&#26684;&#36827;&#34892;&#20998;&#31867;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#35821;&#35328;&#39118;&#26684;&#21487;&#33021;&#24456;&#38590;&#23450;&#20041;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#39118;&#26684;&#35789;&#20856;&#20316;&#20026;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#35782;&#21035;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#39118;&#26684;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#35789;&#20856;&#30340;&#25351;&#20196;&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#24615;&#33021;&#30340;&#36716;&#31227;&#12290;&#25105;&#20204;&#23558;&#21457;&#24067;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Style is used to convey authors' intentions and attitudes. Despite the success of large pre-trained language models on style classification, prior work relies on fine-tuning with labeled examples. Prompting large language models to classify style without fine-tuning is challenging because language styles can be difficult to define. In this study, we investigate the effectiveness of style lexicons as a means for instructing language models how to identify new styles that are unseen during training. Our experiments show that lexicon-based instructions improve transfer zero-shot performance significantly. We will release our code and data.
&lt;/p&gt;</description></item><item><title>ALGO&#26694;&#26550;&#20351;&#29992;&#30001;LLM&#29983;&#25104;&#30340;&#31070;&#35861;&#25351;&#23548;&#21019;&#36896;&#21644;&#39564;&#35777;&#31639;&#27861;&#31243;&#24207;&#65292;&#20197;&#25552;&#39640;&#29616;&#26377;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#31639;&#27861;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14591</link><description>&lt;p&gt;
ALGO&#65306;&#20351;&#29992;&#29983;&#25104;&#30340;&#31070;&#35861;&#39564;&#35777;&#31243;&#24207;&#30340;&#21512;&#25104;&#31639;&#27861;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers. (arXiv:2305.14591v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14591
&lt;/p&gt;
&lt;p&gt;
ALGO&#26694;&#26550;&#20351;&#29992;&#30001;LLM&#29983;&#25104;&#30340;&#31070;&#35861;&#25351;&#23548;&#21019;&#36896;&#21644;&#39564;&#35777;&#31639;&#27861;&#31243;&#24207;&#65292;&#20197;&#25552;&#39640;&#29616;&#26377;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#31639;&#27861;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large language models, LLMs)&#22312;&#23454;&#29616;&#20195;&#30721;&#30340;&#21151;&#33021;&#25551;&#36848;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#38656;&#35201;&#30830;&#23450;&#36866;&#24403;&#31639;&#27861;&#30340;&#31639;&#27861;&#38382;&#39064;&#19978;&#20127;&#38656;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;LLM&#29983;&#25104;&#30340;&#31243;&#24207;&#32570;&#20047;&#20445;&#35777;&#27491;&#30830;&#24615;&#24182;&#38656;&#35201;&#20154;&#24037;&#39564;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ALGO&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#30001;LLM&#29983;&#25104;&#30340;&#31070;&#35861;&#25351;&#23548;&#21019;&#36896;&#21644;&#39564;&#35777;&#31639;&#27861;&#31243;&#24207;&#12290;ALGO&#39318;&#20808;&#36890;&#36807;&#20419;&#20351;LLM&#26522;&#20030;&#30456;&#20851;&#21464;&#37327;&#30340;&#25152;&#26377;&#32452;&#21512;&#26469;&#29983;&#25104;&#20855;&#26377;&#21487;&#33021;&#30340;&#27491;&#30830;&#24615;&#20294;&#21487;&#33021;&#36739;&#24930;&#30340;&#21442;&#32771;&#31070;&#35861;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#35813;&#31070;&#35861;&#25351;&#23548;&#20219;&#24847;&#25628;&#32034;&#31574;&#30053;&#26469;&#25506;&#32034;&#31639;&#27861;&#31354;&#38388;&#24182;&#39564;&#35777;&#21512;&#25104;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#31070;&#35861;&#22312;88%&#30340;&#24773;&#20917;&#19979;&#26159;&#27491;&#30830;&#30340;&#12290;&#20351;&#29992;&#36825;&#20123;&#31070;&#35861;&#20316;&#20026;&#39564;&#35777;&#31243;&#24207;&#65292;ALGO&#21487;&#20197;&#20197;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#24335;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#65292;&#20197;&#25552;&#39640;&#20854;&#31639;&#27861;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) excel at implementing code from functionality descriptions, but struggle with algorithmic problems that require not only implementation but also identification of the suitable algorithm. Moreover, LLM-generated programs lack guaranteed correctness and require human verification. To address these challenges, we propose ALGO, a framework that synthesizes Algorithmic programs with LLM-Generated Oracles to guide the creation and verify their correctness. ALGO first generates a probably correct but possibly slow reference oracle by prompting an LLM to exhaustively enumerate all the combinations of relevant variables. This oracle is then utilized to guide an arbitrary search strategy in exploring the algorithm space and to verify the algorithms synthesized. Our study shows that the LLM-generated oracles are correct for 88% of the cases. With the oracles as verifiers, ALGO can be integrated with any existing code generation model in a model-agnostic manner to enha
&lt;/p&gt;</description></item><item><title>RE$^2$&#26041;&#27861;&#21033;&#29992;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#23454;&#20307;&#22359;&#20043;&#38388;&#30340;&#21306;&#22495;&#32423;&#31354;&#38388;&#32467;&#26500;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#20851;&#31995;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14590</link><description>&lt;p&gt;
RE$^2$: &#38754;&#21521;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#30340;&#21306;&#22495;&#24863;&#30693;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
RE$^2$: Region-Aware Relation Extraction from Visually Rich Documents. (arXiv:2305.14590v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14590
&lt;/p&gt;
&lt;p&gt;
RE$^2$&#26041;&#27861;&#21033;&#29992;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#23454;&#20307;&#22359;&#20043;&#38388;&#30340;&#21306;&#22495;&#32423;&#31354;&#38388;&#32467;&#26500;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#20851;&#31995;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#34920;&#21333;&#29702;&#35299;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#38656;&#35201;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24067;&#23616;&#32467;&#26500;&#65288;&#21363;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#23454;&#20307;&#22359;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#65289;&#23545;&#20110;&#20851;&#31995;&#25277;&#21462;&#30340;&#37325;&#35201;&#24615;&#21364;&#34987;&#24573;&#35270;&#20102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; RE$^2$ &#30340;&#21306;&#22495;&#24863;&#30693;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#21033;&#29992;&#23454;&#20307;&#22359;&#20043;&#38388;&#30340;&#21306;&#22495;&#32423;&#31354;&#38388;&#32467;&#26500;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#20851;&#31995;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36793;&#32536;&#24863;&#30693;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#26469;&#23398;&#20064;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#21516;&#26102;&#32771;&#34385;&#23427;&#20204;&#30340;&#21306;&#22495;&#32423;&#34920;&#31034;&#25152;&#23450;&#20041;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#32422;&#26463;&#30446;&#26631;&#65292;&#26469;&#35268;&#33539;&#27169;&#22411;&#20197;&#31526;&#21512;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#22266;&#26377;&#32422;&#26463;&#26465;&#20214;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#35821;&#35328;&#21644;&#39046;&#22495;&#30340;&#24191;&#27867;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current research in form understanding predominantly relies on large pre-trained language models, necessitating extensive data for pre-training. However, the importance of layout structure (i.e., the spatial relationship between the entity blocks in the visually rich document) to relation extraction has been overlooked. In this paper, we propose REgion-Aware Relation Extraction (RE$^2$) that leverages region-level spatial structure among the entity blocks to improve their relation prediction. We design an edge-aware graph attention network to learn the interaction between entities while considering their spatial relationship defined by their region-level representations. We also introduce a constraint objective to regularize the model towards consistency with the inherent constraints of the relation extraction task. Extensive experiments across various datasets, languages and domains demonstrate the superiority of our proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#24182;&#25913;&#36827;&#20102;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#22312;&#21338;&#29289;&#39302;&#34255;&#21697;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#24494;&#35843;&#30340;&#26368;&#26032;&#31471;&#21040;&#31471; EL &#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#21644;&#26368;&#20339;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14588</link><description>&lt;p&gt;
&#35780;&#20272;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#24211;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#38142;&#25509;&#65306;&#20174;&#21338;&#29289;&#39302;&#25910;&#34255;&#20013;&#23398;&#20064;&#21476;&#20195;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating end-to-end entity linking on domain-specific knowledge bases: Learning about ancient technologies from museum collections. (arXiv:2305.14588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#24182;&#25913;&#36827;&#20102;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#22312;&#21338;&#29289;&#39302;&#34255;&#21697;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#24494;&#35843;&#30340;&#26368;&#26032;&#31471;&#21040;&#31471; EL &#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#21644;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#30740;&#31350;&#31038;&#20250;&#12289;&#32463;&#27982;&#21644;&#21382;&#21490;&#38382;&#39064;&#65292;&#31038;&#20250;&#31185;&#23398;&#21644;&#20154;&#25991;&#23398;&#31185;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#20351;&#29992;&#36234;&#26469;&#36234;&#22823;&#30340;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#34429;&#28982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#36817;&#36827;&#23637;&#25552;&#20379;&#20102;&#35768;&#22810;&#26377;&#25928;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#30340;&#24037;&#20855;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#19981;&#36866;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#24615;&#33021;&#21644;&#36866;&#21512;&#24615;&#37117;&#19981;&#26159;&#24456;&#22909;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23581;&#35797;&#21435;&#22635;&#34917;&#39046;&#22495;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#20351;&#29992;&#29616;&#20195;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#26469;&#20016;&#23500;&#21338;&#29289;&#39302;&#34255;&#21697;&#25968;&#25454;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;7,510&#23545;&#23454;&#20307;&#25552;&#21450;&#65292;&#20849;1700&#22810;&#20010;&#25991;&#26412;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#35814;&#32454;&#35780;&#20272;&#20102;&#19968;&#20123;&#29616;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#27492;&#25968;&#25454;&#23545;&#26368;&#36817;&#30340;&#31471;&#21040;&#31471; EL &#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#24494;&#35843;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#30446;&#21069;&#27492;&#39046;&#22495;&#20013;&#20854;&#20182;&#21487;&#29992;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#27010;&#24565;&#39564;&#35777;&#29992;&#20363;&#65292;&#21516;&#26102;&#24320;&#25918;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To study social, economic, and historical questions, researchers in the social sciences and humanities have started to use increasingly large unstructured textual datasets. While recent advances in NLP provide many tools to efficiently process such data, most existing approaches rely on generic solutions whose performance and suitability for domain-specific tasks is not well understood. This work presents an attempt to bridge this domain gap by exploring the use of modern Entity Linking approaches for the enrichment of museum collection data. We collect a dataset comprising of more than 1700 texts annotated with 7,510 mention-entity pairs, evaluate some off-the-shelf solutions in detail using this dataset and finally fine-tune a recent end-to-end EL model on this data. We show that our fine-tuned model significantly outperforms other approaches currently available in this domain and present a proof-of-concept use case of this model. We release our dataset and our best model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#20027;&#39064;&#30456;&#24178;&#24615;&#35780;&#20272;&#25351;&#26631;&#65288;CTC&#65289;&#65292;&#35813;&#25351;&#26631;&#19981;&#20165;&#22312;&#30701;&#25991;&#26723;&#19978;&#36816;&#20316;&#33391;&#22909;&#65292;&#32780;&#19988;&#22312;&#30456;&#23545;&#20110;&#20854;&#20182;&#20116;&#20010;&#25351;&#26631;&#35780;&#20272;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14587</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#20027;&#39064;&#30456;&#24178;&#24615;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Contextualized Topic Coherence Metrics. (arXiv:2305.14587v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#20027;&#39064;&#30456;&#24178;&#24615;&#35780;&#20272;&#25351;&#26631;&#65288;CTC&#65289;&#65292;&#35813;&#25351;&#26631;&#19981;&#20165;&#22312;&#30701;&#25991;&#26723;&#19978;&#36816;&#20316;&#33391;&#22909;&#65292;&#32780;&#19988;&#22312;&#30456;&#23545;&#20110;&#20854;&#20182;&#20116;&#20010;&#25351;&#26631;&#35780;&#20272;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#30340;&#22823;&#37327;&#30740;&#31350;&#34987;&#25351;&#36131;&#22312;&#20248;&#21270;&#33258;&#21160;&#21270;&#20027;&#39064;&#35780;&#20272;&#25351;&#26631;&#30340;&#21516;&#26102;&#29306;&#29298;&#20102;&#23454;&#36136;&#24615;&#30340;&#20027;&#39064;&#35782;&#21035;&#12290;&#20294;&#26159;&#65292;&#20154;&#24037;&#26631;&#27880;&#30340;&#25104;&#26412;&#39640;&#26114;&#19988;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#65292;&#21463;&#21040;&#20154;&#31867;&#20027;&#39064;&#35780;&#20272;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#31995;&#21015;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#21270;&#20027;&#39064;&#30456;&#24178;&#24615;&#65288;CTC&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20010;&#20840;&#33258;&#21160;&#30340;&#29256;&#26412;&#20197;&#21450;&#19968;&#20010;&#21322;&#33258;&#21160;&#21270;&#30340;CTC&#65292;&#35813;&#29256;&#26412;&#38024;&#23545;&#20154;&#24037;&#20013;&#24515;&#30340;&#30456;&#24178;&#24615;&#35780;&#20272;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#33258;&#21160;&#21270;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#20027;&#39064;&#27169;&#22411;&#19978;&#30456;&#23545;&#20110;&#20116;&#20010;&#20854;&#20182;&#25351;&#26631;&#35780;&#20272;CTC&#65292;&#24182;&#21457;&#29616;&#23427;&#20248;&#20110;&#33258;&#21160;&#20027;&#39064;&#19968;&#33268;&#24615;&#26041;&#27861;&#65292;&#22312;&#30701;&#25991;&#26723;&#19978;&#36816;&#20316;&#33391;&#22909;&#65292;&#24182;&#19988;&#19981;&#23481;&#26131;&#21463;&#21040;&#35780;&#20998;&#39640;&#20294;&#27627;&#26080;&#24847;&#20041;&#30340;&#20027;&#39064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent explosion in work on neural topic modeling has been criticized for optimizing automated topic evaluation metrics at the expense of actual meaningful topic identification. But human annotation remains expensive and time-consuming. We propose LLM-based methods inspired by standard human topic evaluations, in a family of metrics called Contextualized Topic Coherence (CTC). We evaluate both a fully automated version as well as a semi-automated CTC that allows human-centered evaluation of coherence while maintaining the efficiency of automated methods. We evaluate CTC relative to five other metrics on six topic models and find that it outperforms automated topic coherence methods, works well on short documents, and is not susceptible to meaningless but high-scoring topics.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#37325;&#28857;&#25918;&#22312;&#38544;&#24335;&#20869;&#23481;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#29702;&#21644;&#20998;&#35299;&#26041;&#27861;&#38477;&#20302;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22797;&#26434;&#24230;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#23884;&#20837;&#65292;&#35745;&#31639;&#25919;&#27835;&#23398;&#21644;&#26500;&#24314;&#21457;&#29616;&#26041;&#38754;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.14583</link><description>&lt;p&gt;
&#35753;&#38544;&#21547;&#30340;&#26174;&#24615;&#21270;&#65306;&#20197;NLP&#20013;&#30340;&#38544;&#24335;&#20869;&#23481;&#20026;&#31532;&#19968;&#20844;&#27665;
&lt;/p&gt;
&lt;p&gt;
Making the Implicit Explicit: Implicit Content as a First Class Citizen in NLP. (arXiv:2305.14583v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14583
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#37325;&#28857;&#25918;&#22312;&#38544;&#24335;&#20869;&#23481;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#29702;&#21644;&#20998;&#35299;&#26041;&#27861;&#38477;&#20302;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22797;&#26434;&#24230;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#23884;&#20837;&#65292;&#35745;&#31639;&#25919;&#27835;&#23398;&#21644;&#26500;&#24314;&#21457;&#29616;&#26041;&#38754;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26159;&#22810;&#20803;&#21270;&#30340;&#65292;&#19968;&#20010;&#34920;&#36848;&#21487;&#20197;&#29992;&#31561;&#20215;&#30340;&#24418;&#24335;&#37325;&#30003;&#65292;&#32780;&#20854;&#20013;&#30340;&#38544;&#21547;&#21644;&#26174;&#24615;&#20869;&#23481;&#25903;&#25345;&#21508;&#31181;&#36923;&#36753;&#21644;&#35821;&#29992;&#25512;&#29702;&#12290;&#22312;&#22788;&#29702;&#34920;&#36848;&#26102;&#65292;&#25105;&#20204;&#32771;&#34385;&#36825;&#20123;&#19981;&#21516;&#30340;&#26041;&#38754;&#65292;&#22240;&#20026;&#25105;&#20204;&#38656;&#35201;&#29702;&#35299;&#8220;&#36825;&#37324;&#24456;&#40657;&#8221;&#21487;&#33021;&#26159;&#19968;&#20010;&#26263;&#31034;&#38656;&#35201;&#25171;&#24320;&#28783;&#12290;&#28982;&#32780;&#65292;NLP&#26041;&#27861;&#36890;&#24120;&#20165;&#20165;&#22522;&#20110;&#34920;&#38754;&#24418;&#24335;&#25805;&#20316;&#65292;&#30465;&#30053;&#20102;&#36825;&#31181;&#32454;&#24494;&#24046;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29992;&#35821;&#35328;&#26469;&#34920;&#31034;&#35821;&#35328;&#65292;&#24182;&#24341;&#23548;LLM&#23558;&#34920;&#36848;&#20998;&#35299;&#20026;&#36923;&#36753;&#21644;&#21487;&#20449;&#30340;&#25512;&#29702;&#12290;&#20998;&#35299;&#30340;&#38477;&#20302;&#22797;&#26434;&#24615;&#65292;&#20351;&#23427;&#20204;&#26356;&#23481;&#26131;&#23884;&#20837;&#65292;&#24320;&#21551;&#20102;&#26032;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21464;&#21270;&#22312;&#21477;&#23376;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25913;&#36827;&#65292;&#22312;&#35745;&#31639;&#25919;&#27835;&#23398;&#20013;&#26377;&#23454;&#36136;&#24615;&#24212;&#29992;&#65292;&#24182;&#24341;&#20986;&#19968;&#31181;&#26032;&#30340;&#26500;&#24314;&#21457;&#29616;&#36807;&#31243;&#65292;&#25105;&#20204;&#29992;&#20154;&#24037;&#27880;&#37322;&#39564;&#35777;&#20102;&#36825;&#31181;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is multifaceted. A given utterance can be re-expressed in equivalent forms, and its implicit and explicit content support various logical and pragmatic inferences. When processing an utterance, we consider these different aspects, as mediated by our interpretive goals -- understanding that "it's dark in here" may be a veiled direction to turn on a light. Nonetheless, NLP methods typically operate over the surface form alone, eliding this nuance.  In this work, we represent language with language, and direct an LLM to decompose utterances into logical and plausible inferences. The reduced complexity of the decompositions makes them easier to embed, opening up novel applications. Variations on our technique lead to state-of-the-art improvements on sentence embedding benchmarks, a substantive application in computational political science, and to a novel construct-discovery process, which we validate with human annotations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23545;&#33889;&#33796;&#29273;&#35821;&#20013;&#30340;Whisper ASR&#36827;&#34892;&#20102;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#20026;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#22312;&#20027;&#39064;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.14580</link><description>&lt;p&gt;
&#35780;&#20272;OpenAI&#25552;&#20379;&#30340;Whisper ASR&#22312;Museum of the Person&#30340;&#29983;&#27963;&#21490;&#20013;&#36827;&#34892;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#21644;&#20027;&#39064;&#24314;&#27169;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Evaluating OpenAI's Whisper ASR for Punctuation Prediction and Topic Modeling of life histories of the Museum of the Person. (arXiv:2305.14580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23545;&#33889;&#33796;&#29273;&#35821;&#20013;&#30340;Whisper ASR&#36827;&#34892;&#20102;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#20026;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#22312;&#20027;&#39064;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#22312;&#20154;&#26426;&#20132;&#20114;&#24212;&#29992;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#21313;&#24180;&#20013;&#25552;&#20986;&#30340;&#33889;&#33796;&#29273;&#35821;ASR&#27169;&#22411;&#22312;&#27491;&#30830;&#35782;&#21035;&#33258;&#21160;&#36716;&#24405;&#20013;&#30340;&#26631;&#28857;&#31526;&#21495;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#36825;&#20351;&#24471;&#36825;&#20123;&#36716;&#24405;&#19981;&#33021;&#34987;&#20854;&#20182;&#31995;&#32479;&#12289;&#27169;&#22411;&#21644;&#29978;&#33267;&#26159;&#20154;&#31867;&#20351;&#29992;&#12290;&#26368;&#36817;&#65292;OpenAI&#25552;&#20986;&#20102;Whisper ASR&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#26377;&#26395;&#22788;&#29702;&#36825;&#20123;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#27425;&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#20013;Whisper&#30340;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#24615;&#33021;&#36827;&#34892;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#39564;&#35780;&#20272;&#26469;&#32771;&#34385;&#20851;&#20110;&#20572;&#39039;&#28857;&#65288;&#36887;&#21495;&#65289;&#21644;&#23436;&#25972;&#24605;&#24819;&#65288;&#24863;&#21497;&#12289;&#30097;&#38382;&#21644;&#21477;&#21495;&#65289;&#30340;&#29702;&#35770;&#26041;&#38754;&#65292;&#20197;&#21450;&#19982;&#22522;&#20110;&#36716;&#24405;&#30340;&#20027;&#39064;&#24314;&#27169;&#30456;&#20851;&#30340;&#23454;&#38469;&#26041;&#38754;&#65292;&#20351;&#29992;&#26631;&#28857;&#31526;&#21495;&#26469;&#25552;&#39640;&#24615;&#33021;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) systems play a key role in applications involving human-machine interactions. Despite their importance, ASR models for the Portuguese language proposed in the last decade have limitations in relation to the correct identification of punctuation marks in automatic transcriptions, which hinder the use of transcriptions by other systems, models, and even by humans. However, recently Whisper ASR was proposed by OpenAI, a general-purpose speech recognition model that has generated great expectations in dealing with such limitations. This chapter presents the first study on the performance of Whisper for punctuation prediction in the Portuguese language. We present an experimental evaluation considering both theoretical aspects involving pausing points (comma) and complete ideas (exclamation, question, and fullstop), as well as practical aspects involving transcript-based topic modeling - an application dependent on punctuation marks for promising performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22270;&#30340;&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#25991;&#26412;&#36755;&#20837;&#29305;&#24449;&#21644;&#39046;&#22495;&#35201;&#32032;&#23545;&#22270;&#30340;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;BERT&#22312;&#22788;&#29702;&#30701;&#25991;&#26412;&#26102;&#38590;&#20197;&#25910;&#25947;&#65292;&#22270;&#26041;&#27861;&#23545;&#20110;&#36739;&#38271;&#30340;&#25991;&#26723;&#29305;&#21035;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2305.14578</link><description>&lt;p&gt;
&#36830;&#25509;&#28857;&#65306;&#22522;&#20110;&#22270;&#32593;&#32476;&#30340;&#25991;&#26412;&#34920;&#31034;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#26368;&#20339;&#34920;&#29616;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Connecting the Dots: What Graph-Based Text Representations Work Best for Text Classification using Graph Neural Networks?. (arXiv:2305.14578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22270;&#30340;&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#25991;&#26412;&#36755;&#20837;&#29305;&#24449;&#21644;&#39046;&#22495;&#35201;&#32032;&#23545;&#22270;&#30340;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;BERT&#22312;&#22788;&#29702;&#30701;&#25991;&#26412;&#26102;&#38590;&#20197;&#25910;&#25947;&#65292;&#22270;&#26041;&#27861;&#23545;&#20110;&#36739;&#38271;&#30340;&#25991;&#26723;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#32467;&#26500;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25104;&#21151;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#23427;&#20204;&#20316;&#20026;&#20256;&#32479;&#29305;&#24449;&#34920;&#31034;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20165;&#32771;&#34385;&#20102;&#29305;&#23450;&#39046;&#22495;&#65292;&#24182;&#39564;&#35777;&#20102;&#20855;&#26377;&#29305;&#23450;&#29305;&#24449;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#23545;&#25552;&#20986;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#22522;&#20110;&#22270;&#30340;&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#23454;&#38469;&#23454;&#26045;&#30340;&#21547;&#20041;&#21644;&#39046;&#22495;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#20960;&#31181;GNN&#26550;&#26500;&#20197;&#21450;BERT&#65292;&#28085;&#30422;&#20102;&#38271;&#30701;&#25991;&#26723;&#12290;&#32467;&#26524;&#34920;&#26126;&#65306;i&#65289;&#22270;&#30340;&#24615;&#33021;&#19982;&#25991;&#26412;&#36755;&#20837;&#29305;&#24449;&#21644;&#39046;&#22495;&#23494;&#20999;&#30456;&#20851;&#65292;ii&#65289;&#23613;&#31649;&#20854;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;BERT&#22312;&#22788;&#29702;&#30701;&#25991;&#26412;&#26102;&#38590;&#20197;&#25910;&#25947;&#65292; iii&#65289;&#22270;&#26041;&#27861;&#23545;&#20110;&#36739;&#38271;&#30340;&#25991;&#26723;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the success of Graph Neural Networks (GNNs) for structure-aware machine learning, numerous studies have explored their application to text classification, as an alternative to traditional feature representation models. However, most studies considered just a specific domain and validated on data with particular characteristics. This work presents an extensive empirical investigation of graph-based text representation methods proposed for text classification, identifying practical implications and open challenges in the field. We compare several GNN architectures as well as BERT across five datasets, encompassing short and also long documents. The results show that: i) graph performance is highly related to the textual input features and domain, ii) despite its outstanding performance, BERT has difficulties converging when dealing with short texts, iii) graph methods are particularly beneficial for longer documents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36873;&#21462;&#36974;&#34109;&#25968;&#25454;&#30340;&#26041;&#27861;&#65288;Difference-Masking&#65289;&#65292;&#20197;&#25552;&#39640;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26041;&#27861;&#26159;&#36890;&#36807;&#32771;&#34385;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#19982;&#39044;&#35757;&#32451;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;&#26469;&#36827;&#34892;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21319;&#32487;&#32493;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#19988;&#20855;&#26377;&#36328;&#20219;&#21153;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14577</link><description>&lt;p&gt;
&#24046;&#24322;&#24615;&#36974;&#25377;&#65306;&#36873;&#25321;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#20013;&#36974;&#25377;&#20160;&#20040;
&lt;/p&gt;
&lt;p&gt;
Difference-Masking: Choosing What to Mask in Continued Pretraining. (arXiv:2305.14577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36873;&#21462;&#36974;&#34109;&#25968;&#25454;&#30340;&#26041;&#27861;&#65288;Difference-Masking&#65289;&#65292;&#20197;&#25552;&#39640;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26041;&#27861;&#26159;&#36890;&#36807;&#32771;&#34385;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#19982;&#39044;&#35757;&#32451;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;&#26469;&#36827;&#34892;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21319;&#32487;&#32493;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#19988;&#20855;&#26377;&#36328;&#20219;&#21153;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#65292;&#29305;&#21035;&#26159;&#36974;&#25377;&#39044;&#27979;&#30446;&#26631;&#30340;&#30446;&#26631;&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#38543;&#26426;&#22320;&#36827;&#34892;&#26631;&#35760;&#21644;&#36974;&#25377;&#65292;&#32780;&#22312;&#25945;&#32946;&#39046;&#22495;&#26377;&#24378;&#28872;&#30340;&#30452;&#35273;&#35748;&#20026;&#65292;&#20915;&#23450;&#20160;&#20040;&#38656;&#35201;&#36974;&#25377;&#21487;&#20197;&#23454;&#36136;&#24615;&#22320;&#25913;&#21892;&#23398;&#20064;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24046;&#24322;&#36974;&#25377;(Difference-Masking)&#65292;&#19968;&#31181;&#33258;&#21160;&#36873;&#25321;&#36974;&#25377;&#20160;&#20040;&#30340;&#26041;&#27861;&#65292;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#20013;&#36890;&#36807;&#32771;&#34385;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#19982;&#39044;&#35757;&#32451;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;&#26469;&#23454;&#29616;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#24046;&#24322;&#36974;&#25377;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#35270;&#39057;&#20219;&#21153;&#30340;&#32487;&#32493;&#39044;&#35757;&#32451;&#35774;&#32622;&#20013;&#20248;&#20110;&#22522;&#32447;&#12290;&#24046;&#24322;&#24615;&#36974;&#25377;&#30340;&#36328;&#20219;&#21153;&#36866;&#29992;&#24615;&#25903;&#25345;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;SSL&#39044;&#35757;&#32451;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) and the objective of masking-and-predicting in particular have led to promising SSL performance on a variety of downstream tasks. However, while most approaches randomly mask tokens, there is strong intuition from the field of education that deciding what to mask can substantially improve learning outcomes. We introduce Difference-Masking, an approach that automatically chooses what to mask during continued pretraining by considering what makes an unlabelled target domain different from the pretraining domain. Empirically, we find that Difference-Masking outperforms baselines on continued pretraining settings across four diverse language and multimodal video tasks. The cross-task applicability of Difference-Masking supports the effectiveness of our framework for SSL pretraining in language, vision, and other domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#26377;&#20559;&#38388;&#25509;&#20851;&#31995;&#20462;&#25913;&#65288;BIRM&#65289;&#65292;&#20197;&#20943;&#36731;&#20998;&#24067;&#24335;&#35789;&#23884;&#20837;&#20013;&#30340;&#38388;&#25509;&#20559;&#35265;&#65292;&#36890;&#36807;&#32771;&#34385;&#26631;&#35760;&#20559;&#35265;&#23646;&#24615;&#30340;&#21333;&#35789;&#22312;&#23384;&#22312;&#24773;&#20917;&#19979;&#32473;&#23450;&#19968;&#23545;&#21333;&#35789;&#30340;&#20849;&#29616;&#27010;&#29575;&#22914;&#20309;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#28857;&#24179;&#22343;&#20559;&#35265;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.14574</link><description>&lt;p&gt;
&#25506;&#27979;&#21644;&#20943;&#23569;&#35789;&#23884;&#20837;&#20013;&#30340;&#38388;&#25509;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Detecting and Mitigating Indirect Stereotypes in Word Embeddings. (arXiv:2305.14574v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#26377;&#20559;&#38388;&#25509;&#20851;&#31995;&#20462;&#25913;&#65288;BIRM&#65289;&#65292;&#20197;&#20943;&#36731;&#20998;&#24067;&#24335;&#35789;&#23884;&#20837;&#20013;&#30340;&#38388;&#25509;&#20559;&#35265;&#65292;&#36890;&#36807;&#32771;&#34385;&#26631;&#35760;&#20559;&#35265;&#23646;&#24615;&#30340;&#21333;&#35789;&#22312;&#23384;&#22312;&#24773;&#20917;&#19979;&#32473;&#23450;&#19968;&#23545;&#21333;&#35789;&#30340;&#20849;&#29616;&#27010;&#29575;&#22914;&#20309;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#28857;&#24179;&#22343;&#20559;&#35265;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35265;&#30340;&#35789;&#23884;&#20837;&#26041;&#27861;&#20250;&#23398;&#20064;&#21040;&#22312;&#20351;&#29992;&#21333;&#35789;&#26102;&#23384;&#22312;&#30340;&#31038;&#20250;&#20559;&#35265;&#21644;&#26377;&#23475;&#21051;&#26495;&#21360;&#35937;&#12290;&#36825;&#20123;&#20559;&#35265;&#19981;&#20165;&#23384;&#22312;&#20110;&#35789;&#26412;&#36523;&#21644;&#20854;&#26126;&#30830;&#30340;&#21051;&#26495;&#21360;&#35937;&#26631;&#35760;&#20043;&#38388;&#65292;&#32780;&#19988;&#36824;&#23384;&#22312;&#20110;&#20849;&#20139;&#30456;&#20851;&#21051;&#26495;&#21360;&#35937;&#30340;&#35789;&#20043;&#38388;&#12290;&#36825;&#31181;&#31216;&#20026;&#8220;&#38388;&#25509;&#20559;&#35265;&#8221;&#30340;&#29616;&#35937;&#24050;&#32463;&#38459;&#30861;&#20102;&#20043;&#21069;&#30340;&#35797;&#22270;&#28040;&#38500;&#36825;&#20123;&#20559;&#35265;&#30340;&#23581;&#35797;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#26377;&#20559;&#38388;&#25509;&#20851;&#31995;&#20462;&#25913;&#65288;BIRM&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#23884;&#20837;&#35789;&#20043;&#21069;&#36890;&#36807;&#20462;&#25913;&#21333;&#35789;&#20043;&#38388;&#30340;&#26377;&#20559;&#20851;&#31995;&#26469;&#20943;&#36731;&#20998;&#24067;&#24335;&#35789;&#23884;&#20837;&#20013;&#30340;&#38388;&#25509;&#20559;&#35265;&#12290;&#26041;&#27861;&#26159;&#36890;&#36807;&#32771;&#34385;&#22312;&#26631;&#35760;&#20559;&#35265;&#23646;&#24615;&#30340;&#21333;&#35789;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#32473;&#23450;&#19968;&#23545;&#21333;&#35789;&#30340;&#20849;&#29616;&#27010;&#29575;&#22914;&#20309;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#28857;&#24179;&#22343;&#20559;&#35265;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#24120;&#35265;&#30340;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#31245;&#24494;&#20943;&#23569;&#35821;&#20041;&#26041;&#38754;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#20102;&#35789;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Societal biases in the usage of words, including harmful stereotypes, are frequently learned by common word embedding methods. These biases manifest not only between a word and an explicit marker of its stereotype, but also between words that share related stereotypes. This latter phenomenon, sometimes called "indirect bias,'' has resisted prior attempts at debiasing. In this paper, we propose a novel method called Biased Indirect Relationship Modification (BIRM) to mitigate indirect bias in distributional word embeddings by modifying biased relationships between words before embeddings are learned. This is done by considering how the co-occurrence probability of a given pair of words changes in the presence of words marking an attribute of bias, and using this to average out the effect of a bias attribute. To evaluate this method, we perform a series of common tests and demonstrate that measures of bias in the word embeddings are reduced in exchange for minor reduction in the semantic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#35789;&#27719;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#37319;&#29992;&#20102;&#20998;&#23618;&#20004;&#32423;&#26041;&#27861;&#21644;&#27973;&#23618;Transformer&#20307;&#31995;&#32467;&#26500;&#20174;&#23383;&#31526;&#20013;&#23398;&#20064;&#21333;&#35789;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#23481;&#24525;&#24230;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14571</link><description>&lt;p&gt;
&#20174;&#23383;&#31526;&#21040;&#35789;&#65306;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#35821;&#35328;&#29702;&#35299;&#30340;&#20998;&#23618;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding. (arXiv:2305.14571v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#35789;&#27719;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#37319;&#29992;&#20102;&#20998;&#23618;&#20004;&#32423;&#26041;&#27861;&#21644;&#27973;&#23618;Transformer&#20307;&#31995;&#32467;&#26500;&#20174;&#23383;&#31526;&#20013;&#23398;&#20064;&#21333;&#35789;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#23481;&#24525;&#24230;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#26368;&#26032;&#27169;&#22411;&#38656;&#35201;&#23545;&#21407;&#22987;&#25991;&#26412;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#31163;&#25955;&#26631;&#35760;&#12290;&#36825;&#20010;&#36807;&#31243;&#31216;&#20026;&#20998;&#35789;&#65292;&#20381;&#36182;&#20110;&#39044;&#20808;&#26500;&#24314;&#30340;&#21333;&#35789;&#25110;&#23376;&#35789;&#12290;&#20294;&#36825;&#31181;&#22266;&#23450;&#35789;&#27719;&#34920;&#38480;&#21046;&#20102;&#27169;&#22411;&#23545;&#25340;&#20889;&#38169;&#35823;&#30340;&#23481;&#24525;&#24230;&#21644;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#35789;&#27719;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#20998;&#23618;&#30340;&#20004;&#32423;&#26041;&#27861;&#65306;&#19968;&#20010;&#22312;&#35789;&#32423;&#21035;&#65292;&#21478;&#19968;&#20010;&#22312;&#24207;&#21015;&#32423;&#21035;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20869;&#37096;&#21333;&#35789;&#27169;&#22359;&#65292;&#20351;&#29992;&#27973;&#23618;Transformer&#20307;&#31995;&#32467;&#26500;&#20174;&#20854;&#23383;&#31526;&#20013;&#23398;&#20064;&#21333;&#35789;&#34920;&#31034;&#65292;&#24182;&#21478;&#19968;&#20010;&#28145;&#23618;Transformer&#27169;&#22359;&#65292;&#23558;&#27599;&#20010;&#21333;&#35789;&#34920;&#31034;&#32622;&#20110;&#25972;&#20010;&#21333;&#35789;&#24207;&#21015;&#20013;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30452;&#25509;&#22312;&#20855;&#26377;&#26174;&#24335;&#21333;&#35789;&#36793;&#30028;&#24847;&#35782;&#30340;&#23383;&#31526;&#24207;&#21015;&#19978;&#36816;&#34892;&#65292;&#20294;&#27809;&#26377;&#20559;&#35265;&#30340;&#23376;&#21333;&#35789;&#25110;&#21333;&#35789;&#32423;&#35789;&#27719;&#34920;&#12290;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current state-of-the-art models for natural language understanding require a preprocessing step to convert raw text into discrete tokens. This process known as tokenization relies on a pre-built vocabulary of words or sub-word morphemes. This fixed vocabulary limits the model's robustness to spelling errors and its capacity to adapt to new domains. In this work, we introduce a novel open-vocabulary language model that adopts a hierarchical two-level approach: one at the word level and another at the sequence level. Concretely, we design an intra-word module that uses a shallow Transformer architecture to learn word representations from their characters, and a deep inter-word Transformer module that contextualizes each word representation by attending to the entire word sequence. Our model thus directly operates on character sequences with explicit awareness of word boundaries, but without biased sub-word or word-level vocabulary. Experiments on various downstream tasks show that our me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#35843;&#25972;&#27169;&#22411;&#21644;&#25552;&#31034;&#20004;&#31181;&#33539;&#24335;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#29992;&#20110;&#32479;&#19968;QA&#30340;&#28508;&#21147;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#26465;&#20214;&#19979;&#65292;&#25552;&#31034;&#35843;&#25972;&#21487;&#20197;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#19982;&#27169;&#22411;&#35843;&#25972;&#30456;&#24403;&#65292;&#36890;&#36807;&#21442;&#25968;&#20849;&#20139;&#21644;&#31616;&#21333;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#30693;&#35782;&#36716;&#31227;&#25216;&#26415;&#65292;&#25552;&#31034;&#35843;&#25972;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.14569</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#32479;&#19968;&#38382;&#31572;&#65306;&#35843;&#25972;&#27169;&#22411;&#36824;&#26159;&#25552;&#31034;&#65311;
&lt;/p&gt;
&lt;p&gt;
Few-shot Unified Question Answering: Tuning Models or Prompts?. (arXiv:2305.14569v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#35843;&#25972;&#27169;&#22411;&#21644;&#25552;&#31034;&#20004;&#31181;&#33539;&#24335;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#29992;&#20110;&#32479;&#19968;QA&#30340;&#28508;&#21147;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#26465;&#20214;&#19979;&#65292;&#25552;&#31034;&#35843;&#25972;&#21487;&#20197;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#19982;&#27169;&#22411;&#35843;&#25972;&#30456;&#24403;&#65292;&#36890;&#36807;&#21442;&#25968;&#20849;&#20139;&#21644;&#31616;&#21333;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#30693;&#35782;&#36716;&#31227;&#25216;&#26415;&#65292;&#25552;&#31034;&#35843;&#25972;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#65288;QA&#65289;&#20219;&#21153;&#36890;&#24120;&#30740;&#31350;&#29305;&#23450;&#30340;&#38382;&#39064;&#31867;&#22411;&#12289;&#30693;&#35782;&#39046;&#22495;&#25110;&#25512;&#29702;&#25216;&#33021;&#65292;&#23548;&#33268;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#30340;QA&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#32479;&#19968;QA&#27169;&#22411;&#30340;&#24819;&#27861;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21482;&#22312;&#39640;&#36164;&#28304;&#24773;&#20917;&#19979;&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#20197;&#25193;&#23637;&#20854;&#33021;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35843;&#25972;&#27169;&#22411;&#21644;&#25552;&#31034;&#20004;&#31181;&#33539;&#24335;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#29992;&#20110;&#32479;&#19968;QA&#30340;&#28508;&#21147;&#65292;&#37319;&#29992;16&#20010;QA&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#24212;&#29992;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#26465;&#20214;&#19979;&#65292;&#25552;&#31034;&#35843;&#25972;&#21487;&#20197;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#19982;&#27169;&#22411;&#35843;&#25972;&#30456;&#24403;&#12290;&#30740;&#31350;&#36824;&#26174;&#31034;&#21442;&#25968;&#20849;&#20139;&#23548;&#33268;&#20248;&#31168;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#30693;&#35782;&#36716;&#31227;&#25216;&#26415;&#21487;&#20197;&#26377;&#25928;&#65292;&#25552;&#31034;&#35843;&#25972;&#22312;&#20302;&#36164;&#28304;&#33539;&#22260;&#20869;&#36890;&#36807;&#39044;&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question-answering (QA) tasks often investigate specific question types, knowledge domains, or reasoning skills, leading to specialized models catering to specific categories of QA tasks. While recent research has explored the idea of unified QA models, such models are usually explored for high-resource scenarios and require re-training to extend their capabilities. To overcome these drawbacks, the paper explores the potential of two paradigms of tuning, model, and prompts, for unified QA under a low-resource setting. The paper provides an exhaustive analysis of their applicability using 16 QA datasets, revealing that prompt tuning can perform as well as model tuning in a few-shot setting with a good initialization. The study also shows that parameter-sharing results in superior few-shot performance, simple knowledge transfer techniques for prompt initialization can be effective, and prompt tuning achieves a significant performance boost from pre-training in a low-resource regime. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;ChatGPT&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#38754;&#21521;&#30446;&#26631;&#30340;&#23545;&#35805;&#21644;&#27880;&#37322;&#30340;&#28508;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#29983;&#25104;&#30340;&#23545;&#35805;&#21644;&#27880;&#37322;&#36136;&#37327;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2305.14556</link><description>&lt;p&gt;
&#25581;&#31034;ChatGPT: AI-&#29983;&#25104;&#38754;&#21521;&#30446;&#26631;&#30340;&#23545;&#35805;&#21644;&#27880;&#37322;&#30340;&#20851;&#38190;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations. (arXiv:2305.14556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;ChatGPT&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#38754;&#21521;&#30446;&#26631;&#30340;&#23545;&#35805;&#21644;&#27880;&#37322;&#30340;&#28508;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#29983;&#25104;&#30340;&#23545;&#35805;&#21644;&#27880;&#37322;&#36136;&#37327;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#31034;&#25216;&#26415;&#20026;&#25163;&#27573;&#20135;&#29983;&#39640;&#36136;&#37327;&#25991;&#26412;&#30340;&#33021;&#21147;&#21069;&#25152;&#26410;&#26377;&#12290;&#36825;&#19968;&#20107;&#23454;&#20026;&#25968;&#25454;&#25910;&#38598;&#21644;&#27880;&#37322;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#36825;&#31181;&#25968;&#25454;&#31232;&#32570;&#12289;&#38590;&#20197;&#25910;&#38598;&#12289;&#26114;&#36149;&#29978;&#33267;&#25935;&#24863;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#21644;&#27880;&#37322;&#38754;&#21521;&#30446;&#26631;&#30340;&#23545;&#35805;&#30340;&#28508;&#21147;&#65292;&#24182;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#20197;&#35780;&#20272;&#23427;&#20204;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#37319;&#29992;ChatGPT&#65292;&#24182;&#21253;&#25324;&#19977;&#31867;&#38754;&#21521;&#30446;&#26631;&#30340;&#23545;&#35805;&#65288;&#20219;&#21153;&#23548;&#21521;&#12289;&#21327;&#20316;&#21644;&#35828;&#26126;&#24615;&#65289;&#12289;&#20004;&#31181;&#29983;&#25104;&#27169;&#24335;&#65288;&#20132;&#20114;&#24335;&#21644;&#19968;&#27425;&#24615;&#65289;&#21644;&#20004;&#31181;&#35821;&#35328;&#65288;&#33521;&#35821;&#21644;&#24847;&#22823;&#21033;&#35821;&#65289;&#12290;&#22522;&#20110;&#24191;&#27867;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29983;&#25104;&#30340;&#23545;&#35805;&#21644;&#27880;&#37322;&#30340;&#36136;&#37327;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models have exhibited unprecedented capabilities in producing high-quality text via prompting techniques. This fact introduces new possibilities for data collection and annotation, particularly in situations where such data is scarce, complex to gather, expensive, or even sensitive. In this paper, we explore the potential of these models to generate and annotate goal-oriented dialogues, and conduct an in-depth analysis to evaluate their quality. Our experiments employ ChatGPT, and encompass three categories of goal-oriented dialogues (task-oriented, collaborative, and explanatory), two generation modes (interactive and one-shot), and two languages (English and Italian). Based on extensive human-based evaluations, we demonstrate that the quality of generated dialogues and annotations is on par with those generated by humans.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;Transformer&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21452;&#23556;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;BERT-INN&#65292;&#26469;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21452;&#23556;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.14555</link><description>&lt;p&gt;
&#25152;&#26377;&#36947;&#36335;&#36890;&#24448;&#32599;&#39532;&#65311;&#25506;&#31350;Transformer&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
All Roads Lead to Rome? Exploring the Invariance of Transformers' Representations. (arXiv:2305.14555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;Transformer&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21452;&#23556;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;BERT-INN&#65292;&#26469;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21452;&#23556;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#24102;&#26469;&#20102;&#26497;&#22823;&#30340;&#36827;&#23637;&#65292;&#22240;&#27492;&#24341;&#21457;&#20102;&#23545;&#27169;&#22411;&#23398;&#20064;&#30340;&#34920;&#31034;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#34920;&#31034;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#31350;Transformer&#26159;&#21542;&#23398;&#20064;&#21040;&#20102;&#26412;&#36136;&#19978;&#21516;&#26500;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#25110;&#32773;&#36825;&#20123;&#34920;&#31034;&#31354;&#38388;&#26159;&#21542;&#23545;&#20854;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#31181;&#23376;&#25935;&#24863;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#23556;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;BERT-INN&#65292;&#26469;&#27604;&#20854;&#20182;&#29616;&#26377;&#21452;&#23556;&#26041;&#27861;&#65288;&#22914;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#65289;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21452;&#23556;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#20102;BERT-INN&#30340;&#20248;&#21183;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23545;&#40784;&#37325;&#29616;&#30340;BERT&#23884;&#20837;&#65292;&#20197;&#33719;&#24471;&#23545;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#38142;&#25509;&#22312;&#25991;&#31456;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models bring propelling advances in various NLP tasks, thus inducing lots of interpretability research on the learned representations of the models. However, we raise a fundamental question regarding the reliability of the representations. Specifically, we investigate whether transformers learn essentially isomorphic representation spaces, or those that are sensitive to the random seeds in their pretraining process. In this work, we formulate the Bijection Hypothesis, which suggests the use of bijective methods to align different models' representation spaces. We propose a model based on invertible neural networks, BERT-INN, to learn the bijection more effectively than other existing bijective methods such as the canonical correlation analysis (CCA). We show the advantage of BERT-INN both theoretically and through extensive experiments, and apply it to align the reproduced BERT embeddings to draw insights that are meaningful to the interpretability research. Our code is at 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#35299;&#37322;&#30340;&#32454;&#31890;&#24230;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#27169;&#22411;FineGrainFact&#65292;&#36890;&#36807;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#26174;&#24335;&#22320;&#34920;&#31034;&#25991;&#26723;&#21644;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#65292;&#24182;&#24378;&#35843;&#30456;&#20851;&#30340;&#35821;&#20041;&#26694;&#26550;&#26469;&#39044;&#27979;&#19981;&#19968;&#33268;&#24615;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.14548</link><description>&lt;p&gt;
&#25991;&#26412;&#25688;&#35201;&#20013;&#26131;&#20110;&#35299;&#37322;&#30340;&#32454;&#31890;&#24230;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interpretable Automatic Fine-grained Inconsistency Detection in Text Summarization. (arXiv:2305.14548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#35299;&#37322;&#30340;&#32454;&#31890;&#24230;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#27169;&#22411;FineGrainFact&#65292;&#36890;&#36807;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#26174;&#24335;&#22320;&#34920;&#31034;&#25991;&#26723;&#21644;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#65292;&#24182;&#24378;&#35843;&#30456;&#20851;&#30340;&#35821;&#20041;&#26694;&#26550;&#26469;&#39044;&#27979;&#19981;&#19968;&#33268;&#24615;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25991;&#26412;&#25688;&#35201;&#20013;&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;&#26041;&#27861;&#25552;&#20379;&#20102;&#20108;&#20803;&#39044;&#27979;&#21644;&#26377;&#38480;&#30340;&#25688;&#35201;&#31995;&#32479;&#24369;&#28857;&#27934;&#35265;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#39044;&#27979;&#25688;&#35201;&#20013;&#32454;&#31890;&#24230;&#30340;&#20107;&#23454;&#38169;&#35823;&#31867;&#22411;&#12290;&#21463;&#20154;&#31867;&#26816;&#26597;&#25688;&#35201;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#26041;&#27861;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FineGrainFact&#65292;&#19968;&#31181;&#26131;&#20110;&#35299;&#37322;&#30340;&#32454;&#31890;&#24230;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#26174;&#24335;&#22320;&#34920;&#31034;&#25991;&#26723;&#21644;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#65292;&#24182;&#24378;&#35843;&#30456;&#20851;&#30340;&#35821;&#20041;&#26694;&#26550;&#26469;&#39044;&#27979;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#35821;&#20041;&#26694;&#26550;&#26377;&#21161;&#20110;&#39564;&#35777;&#39044;&#27979;&#30340;&#38169;&#35823;&#31867;&#22411;&#24182;&#20462;&#27491;&#19981;&#19968;&#33268;&#30340;&#25688;&#35201;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#24378;&#22522;&#32447;&#65292;&#24182;&#25552;&#20379;&#20102;&#25903;&#25345;&#25110;&#21453;&#39539;&#25688;&#35201;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing factual consistency evaluation approaches for text summarization provide binary predictions and limited insights into the weakness of summarization systems. Therefore, we propose the task of fine-grained inconsistency detection, the goal of which is to predict the fine-grained types of factual errors in a summary. Motivated by how humans inspect factual inconsistency in summaries, we propose an interpretable fine-grained inconsistency detection model, FineGrainFact, which explicitly represents the facts in the documents and summaries with semantic frames extracted by semantic role labeling, and highlights the related semantic frames to predict inconsistency. The highlighted semantic frames help verify predicted error types and correct inconsistent summaries. Experiment results demonstrate that our model outperforms strong baselines and provides evidence to support or refute the summary.
&lt;/p&gt;</description></item><item><title>Whisper&#27169;&#22411;&#26159;&#19968;&#20010;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#35821;&#38899;&#20219;&#21153;&#12290;&#26412;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;Whisper&#34920;&#31034;&#22312;&#20854;&#20182;&#22235;&#31181;&#35821;&#38899;&#20219;&#21153;&#21644;&#8220;&#37326;&#22806;&#8221;&#20219;&#21153;&#20013;&#30340;&#21487;&#36801;&#31227;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Whisper&#20855;&#26377;&#36328;&#20219;&#21153;&#30340;&#30495;&#23454;&#29615;&#22659;&#37096;&#32626;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14546</link><description>&lt;p&gt;
&#22522;&#20110;Whisper&#30340;&#34920;&#31034;&#22312;&#21508;&#31181;&#20132;&#21449;&#20219;&#21153;&#19979;&#30340;&#21487;&#36801;&#31227;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Transferability of Whisper-based Representations for "In-the-Wild" Cross-Task Downstream Speech Applications. (arXiv:2305.14546v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14546
&lt;/p&gt;
&lt;p&gt;
Whisper&#27169;&#22411;&#26159;&#19968;&#20010;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#35821;&#38899;&#20219;&#21153;&#12290;&#26412;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;Whisper&#34920;&#31034;&#22312;&#20854;&#20182;&#22235;&#31181;&#35821;&#38899;&#20219;&#21153;&#21644;&#8220;&#37326;&#22806;&#8221;&#20219;&#21153;&#20013;&#30340;&#21487;&#36801;&#31227;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Whisper&#20855;&#26377;&#36328;&#20219;&#21153;&#30340;&#30495;&#23454;&#29615;&#22659;&#37096;&#32626;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#23548;&#33268;&#20102;&#21487;&#29992;&#20110;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#36890;&#29992;&#35821;&#38899;&#34920;&#31034;&#65292;&#20174;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#21040;&#35828;&#35805;&#20154;&#35782;&#21035;&#31561;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;Whisper&#65292;&#36890;&#36807;&#22823;&#37327;&#30340;&#24369;&#30417;&#30563;&#25968;&#25454;&#36827;&#34892;&#20102;ASR&#30340;&#35757;&#32451;&#65307; &#23427;&#20248;&#20110;&#20960;&#31181;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#12290;&#37492;&#20110;Whisper&#22312;ASR&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35813;&#34920;&#31034;&#22312;SUPERB&#22522;&#20934;&#27979;&#35797;&#20013;&#20854;&#20182;&#22235;&#20010;&#35821;&#38899;&#20219;&#21153;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;Whisper&#34920;&#31034;&#22312;&#34987;&#29615;&#22659;&#22122;&#22768;&#21644;&#25151;&#38388;&#28151;&#21709;&#30772;&#22351;&#30340;&#8220;&#37326;&#22806;&#8221;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Whisper&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#29615;&#22659;&#26465;&#20214;&#19979;&#37117;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#22240;&#27492;&#23637;&#31034;&#20102;&#22312;&#36328;&#20219;&#21153;&#30340;&#30495;&#23454;&#29615;&#22659;&#37096;&#32626;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large self-supervised pre-trained speech models have achieved remarkable success across various speech-processing tasks. The self-supervised training of these models leads to universal speech representations that can be used for different downstream tasks, ranging from automatic speech recognition (ASR) to speaker identification. Recently, Whisper, a transformer-based model was proposed and trained on large amount of weakly supervised data for ASR; it outperformed several state-of-the-art self-supervised models. Given the superiority of Whisper for ASR, in this paper we explore the transferability of the representation for four other speech tasks in SUPERB benchmark. Moreover, we explore the robustness of Whisper representation for ``in the wild'' tasks where speech is corrupted by environment noise and room reverberation. Experimental results show Whisper achieves promising results across tasks and environmental conditions, thus showing potential for cross-task real-world deployment.
&lt;/p&gt;</description></item><item><title>LLMs&#33021;&#22815;&#22312;&#22522;&#26412;&#30340;&#23454;&#38469;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#20294;&#22823;&#22810;&#25968;LLMs&#19981;&#33021;&#25104;&#21151;&#23436;&#25104;&#26356;&#22797;&#26434;&#20219;&#21153;&#26041;&#26696;&#30340;&#27979;&#35797;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38750;&#19968;&#33268;&#24615;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;SummEdits&#65292;&#24182;&#34920;&#26126;&#22823;&#22810;&#25968;LLMs&#24182;&#19981;&#25797;&#38271;&#23436;&#25104;&#27492;&#39033;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.14540</link><description>&lt;p&gt;
&#22522;&#20110;&#20107;&#23454;&#30340;&#25512;&#29702;&#65306;LLMs&#30340;&#27934;&#35265;&#19982;&#36229;&#36234;
&lt;/p&gt;
&lt;p&gt;
LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond. (arXiv:2305.14540v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14540
&lt;/p&gt;
&lt;p&gt;
LLMs&#33021;&#22815;&#22312;&#22522;&#26412;&#30340;&#23454;&#38469;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#20294;&#22823;&#22810;&#25968;LLMs&#19981;&#33021;&#25104;&#21151;&#23436;&#25104;&#26356;&#22797;&#26434;&#20219;&#21153;&#26041;&#26696;&#30340;&#27979;&#35797;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38750;&#19968;&#33268;&#24615;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;SummEdits&#65292;&#24182;&#34920;&#26126;&#22823;&#22810;&#25968;LLMs&#24182;&#19981;&#25797;&#38271;&#23436;&#25104;&#27492;&#39033;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;LLMs&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#20986;&#29616;&#65292;&#25317;&#26377;&#33021;&#22815;&#26377;&#25928;&#26816;&#27979;&#23454;&#38469;&#19981;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#23545;&#20110;&#20943;&#23569;&#38169;&#35823;&#20449;&#24687;&#20256;&#25773;&#24182;&#25552;&#39640;&#27169;&#22411;&#36755;&#20986;&#30340;&#20449;&#20219;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;&#23454;&#38469;&#19968;&#33268;&#24615;&#22522;&#20934;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#20256;&#32479;&#30340;&#38750;LLM&#26041;&#27861;&#30456;&#27604;&#65292;&#23569;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#20998;&#31867;&#22522;&#20934;&#26041;&#38754;&#34920;&#29616;&#31454;&#20105;&#21147;&#12290;&#28982;&#32780;&#65292;&#26356;&#35814;&#32454;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22823;&#22810;&#25968;LLMs&#22312;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#26041;&#26696;&#19978;&#22833;&#36133;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#26377;&#35780;&#20272;&#22522;&#20934;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24433;&#21709;&#20102;&#35780;&#20272;&#31934;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#22522;&#20934;&#21327;&#35758;&#65292;&#24182;&#22312;&#19968;&#20010;10&#20010;&#39046;&#22495;&#30340;&#22522;&#20934;&#27979;&#35797;SummEdits&#20013;&#23454;&#26045;&#12290;&#36825;&#20010;&#26032;&#22522;&#20934;&#27979;&#35797;&#27599;&#20010;&#26679;&#26412;&#27604;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#26356;&#20855;&#26377;20&#20493;&#30340;&#25104;&#26412;&#25928;&#30410;&#65292;&#24182;&#19988;&#39640;&#24230;&#21487;&#37325;&#22797;&#65292;&#22240;&#20026;&#25105;&#20204;&#20272;&#35745;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#32422;&#20026;0.9&#12290;&#22823;&#22810;&#25968;LLMs&#22312;SummEdits&#19978;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#20854;&#24615;&#33021;&#8230;&#8230;&#65288;&#21407;&#25991;&#26410;&#23436;&#65292;&#30053;&#65289;
&lt;/p&gt;
&lt;p&gt;
With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals that most LLMs fail on more complex formulations of the task and exposes issues with existing evaluation benchmarks, affecting evaluation precision. To address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called SummEdits. This new benchmark is 20 times more cost-effective per sample than previous benchmarks and highly reproducible, as we estimate inter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32423;&#32852;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#26415;&#35821;&#32422;&#26463;&#65292;&#21487;&#20197;&#21363;&#25554;&#21363;&#29992;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21487;&#23558;&#30446;&#26631;&#26415;&#35821;&#30340;&#27010;&#29575;&#22686;&#21152;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#32593;&#26684;&#26463;&#25628;&#32034;&#30340;&#32423;&#32852;&#26463;&#35774;&#32622;&#65292;&#24615;&#33021;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.14538</link><description>&lt;p&gt;
&#32423;&#32852;&#26463;&#25628;&#32034;&#65306;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#21363;&#25554;&#21363;&#29992;&#26415;&#35821;&#32422;&#26463;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cascaded Beam Search: Plug-and-Play Terminology-Forcing For Neural Machine Translation. (arXiv:2305.14538v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32423;&#32852;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#26415;&#35821;&#32422;&#26463;&#65292;&#21487;&#20197;&#21363;&#25554;&#21363;&#29992;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21487;&#23558;&#30446;&#26631;&#26415;&#35821;&#30340;&#27010;&#29575;&#22686;&#21152;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#32593;&#26684;&#26463;&#25628;&#32034;&#30340;&#32423;&#32852;&#26463;&#35774;&#32622;&#65292;&#24615;&#33021;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#26415;&#35821;&#32422;&#26463;&#30340;&#32763;&#35793;&#30340;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#12290;&#26415;&#35821;&#32422;&#26463;&#26159;&#35768;&#22810;&#29616;&#20195;&#32763;&#35793;&#27969;&#31243;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#22312;&#19987;&#19994;&#39046;&#22495;&#21644;&#26032;&#20852;&#39046;&#22495;&#65288;&#22914;COVID-19&#30123;&#24773;&#65289;&#20013;&#65292;&#20934;&#30830;&#32763;&#35793;&#19987;&#19994;&#26415;&#35821;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#24120;&#35757;&#32451;&#27169;&#22411;&#20174;&#36755;&#20837;&#20013;&#22797;&#21046;&#26415;&#35821;&#24182;&#23558;&#20854;&#39304;&#36865;&#21040;&#30446;&#26631;&#35821;&#21477;&#20013;&#12290;&#20294;&#36825;&#38656;&#35201;&#26114;&#36149;&#30340;&#22521;&#35757;&#65292;&#27599;&#24403;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#21457;&#29983;&#21464;&#21270;&#25110;&#31995;&#32479;&#38656;&#35201;&#19987;&#38376;&#21270;&#21040;&#26032;&#39046;&#22495;&#26102;&#37117;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#32423;&#32852;&#26463;&#25628;&#32034;&#65292;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26415;&#35821;&#24378;&#21046;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#35757;&#32451;&#12290;&#36825;&#31181;&#32423;&#32852;&#26463;&#25628;&#32034;&#26377;&#20004;&#20010;&#37096;&#20998;&#65306;1&#65289;&#36923;&#36753;&#25805;&#20316;&#65292;&#22686;&#21152;&#30446;&#26631;&#26415;&#35821;&#30340;&#27010;&#29575;&#65307;2&#65289;&#22522;&#20110;&#32593;&#26684;&#26463;&#25628;&#32034;&#30340;&#32423;&#32852;&#26463;&#35774;&#32622;&#65292;&#20854;&#20013;&#26681;&#25454;&#23427;&#20204;&#21253;&#21547;&#30340;&#26415;&#35821;&#25968;&#23558;&#26463;&#32452;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#31454;&#20105;&#26041;&#24335;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a plug-and-play approach for translation with terminology constraints. Terminology constraints are an important aspect of many modern translation pipelines. In both specialized domains and newly emerging domains (such as the COVID-19 pandemic), accurate translation of technical terms is crucial. Recent approaches often train models to copy terminologies from the input into the output sentence by feeding the target terminology along with the input. But this requires expensive training whenever the underlying language model is changed or the system should specialize to a new domain. We propose Cascade Beam Search, a plug-and-play terminology-forcing approach that requires no training. Cascade Beam Search has two parts: 1) logit manipulation to increase the probability of target terminologies and 2) a cascading beam setup based on grid beam search, where beams are grouped by the number of terminologies they contain. We evaluate the performance of our approach by compet
&lt;/p&gt;</description></item><item><title>MathDial&#26159;&#19968;&#20010;&#30001;&#23454;&#38469;&#25945;&#24072;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21322;&#21512;&#25104;&#29983;&#25104;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#33258;&#21160;&#23545;&#35805;&#36741;&#23548;&#24037;&#20855;&#32570;&#23569;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#20851;&#27880;&#36890;&#36807;&#24341;&#23548;&#23398;&#29983;&#20351;&#29992;&#38382;&#39064;&#26469;&#25506;&#32034;&#25968;&#23398;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14536</link><description>&lt;p&gt;
MathDial: &#19968;&#20010;&#20197;&#25968;&#23398;&#25512;&#29702;&#38382;&#39064;&#20026;&#22522;&#30784;&#30340;&#23500;&#21547;&#25945;&#32946;&#24615;&#23646;&#24615;&#30340;&#23545;&#35805;&#36741;&#23548;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems. (arXiv:2305.14536v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14536
&lt;/p&gt;
&lt;p&gt;
MathDial&#26159;&#19968;&#20010;&#30001;&#23454;&#38469;&#25945;&#24072;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21322;&#21512;&#25104;&#29983;&#25104;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#33258;&#21160;&#23545;&#35805;&#36741;&#23548;&#24037;&#20855;&#32570;&#23569;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#20851;&#27880;&#36890;&#36807;&#24341;&#23548;&#23398;&#29983;&#20351;&#29992;&#38382;&#39064;&#26469;&#25506;&#32034;&#25968;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#21160;&#23545;&#35805;&#36741;&#23548;&#24037;&#20855;&#22312;&#20010;&#24615;&#21270;&#21644;&#25552;&#39640;&#25945;&#32946;&#21487;&#21450;&#24615;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#36275;&#22815;&#22823;&#35268;&#27169;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#31867;&#31995;&#32479;&#30340;&#30740;&#31350;&#19968;&#30452;&#21463;&#21040;&#38459;&#30861;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#27492;&#31867;&#25968;&#25454;&#38598;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#24405;&#21046;&#36741;&#23548;&#20250;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#65292;&#32780;&#20247;&#21253;&#21017;&#20250;&#23548;&#33268;&#25968;&#25454;&#36136;&#37327;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#30495;&#27491;&#30340;&#32769;&#24072;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#37197;&#23545;&#65292;&#26500;&#24314;&#24120;&#35265;&#23398;&#29983;&#38169;&#35823;&#30340;&#25903;&#26550;&#65292;&#26469;&#21322;&#21512;&#25104;&#22320;&#29983;&#25104;&#36825;&#20123;&#23545;&#35805;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#25910;&#38598;MathDial&#30340;&#26368;&#26032;&#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#20010;&#24403;&#21069;&#25317;&#26377;&#32422;1.5k&#20010;&#22522;&#20110;&#22810;&#27493;&#25968;&#23398;&#35789;&#38382;&#39064;&#30340;&#36741;&#23548;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#23637;&#29616;&#20986;&#20016;&#23500;&#30340;&#25945;&#32946;&#24615;&#23646;&#24615;&#65292;&#20027;&#35201;&#20851;&#27880;&#36890;&#36807;&#24341;&#23548;&#23398;&#29983;&#20351;&#29992;&#38382;&#39064;&#26469;&#25506;&#32034;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;MathDial&#21450;&#20854;&#22320;&#38754;&#27880;&#37322;&#21487;&#29992;&#20110;&#24494;&#35843;...
&lt;/p&gt;
&lt;p&gt;
Although automatic dialogue tutors hold great potential in making education personalized and more accessible, research on such systems has been hampered by a lack of sufficiently large and high-quality datasets. However, collecting such datasets remains challenging, as recording tutoring sessions raises privacy concerns and crowdsourcing leads to insufficient data quality. To address this problem, we propose a framework to semi-synthetically generate such dialogues by pairing real teachers with a large language model (LLM) scaffolded to represent common student errors. In this paper, we describe our ongoing efforts to use this framework to collect MathDial, a dataset of currently ca. 1.5k tutoring dialogues grounded in multi-step math word problems. We show that our dataset exhibits rich pedagogical properties, focusing on guiding students using sense-making questions to let them explore problems. Moreover, we outline that MathDial and its grounding annotations can be used to finetune 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#22312;&#28151;&#21512;&#35821;&#35328;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#26816;&#27979;&#23459;&#20256;&#25216;&#26415;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#20219;&#21153;&#65292;&#20316;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;1030&#20010;&#25991;&#26412;&#30340;&#33521;&#35821;&#21644;&#32599;&#39532;&#20044;&#23572;&#28151;&#21512;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.14534</link><description>&lt;p&gt;
&#22312;&#28151;&#21512;&#35821;&#35328;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#26816;&#27979;&#23459;&#20256;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Detecting Propaganda Techniques in Code-Switched Social Media Text. (arXiv:2305.14534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#22312;&#28151;&#21512;&#35821;&#35328;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#26816;&#27979;&#23459;&#20256;&#25216;&#26415;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#20219;&#21153;&#65292;&#20316;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;1030&#20010;&#25991;&#26412;&#30340;&#33521;&#35821;&#21644;&#32599;&#39532;&#20044;&#23572;&#28151;&#21512;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23459;&#20256;&#26159;&#19968;&#31181;&#26088;&#22312;&#24433;&#21709;&#20844;&#20247;&#33286;&#35770;&#21644;&#24515;&#24577;&#20197;&#25512;&#24191;&#29305;&#23450;&#35758;&#31243;&#30340;&#27807;&#36890;&#24418;&#24335;&#12290;&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#23835;&#36215;&#65292;&#23459;&#20256;&#24050;&#32463;&#36805;&#36895;&#20256;&#25773;&#65292;&#24341;&#21457;&#20102;&#23545;&#33258;&#21160;&#23459;&#20256;&#26816;&#27979;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#22823;&#22810;&#25968;&#23459;&#20256;&#26816;&#27979;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#33521;&#35821;&#65289;&#19978;&#65292;&#20960;&#20046;&#27809;&#26377;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#26816;&#27979;&#23459;&#20256;&#20570;&#20986;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#20132;&#27969;&#20013;&#21457;&#29616;&#22810;&#31181;&#35821;&#35328;&#30340;&#28151;&#21512;&#29616;&#35937;&#26159;&#24456;&#24120;&#35265;&#30340;&#65292;&#36825;&#34987;&#31216;&#20026;&#30721;&#28151;&#12290;&#30721;&#28151;&#22312;&#21516;&#19968;&#25991;&#26412;&#20013;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#65292;&#36825;&#23545;&#20110;&#33258;&#21160;&#31995;&#32479;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22312;&#27492;&#25552;&#20986;&#20102;&#26816;&#27979;&#28151;&#21512;&#25991;&#26412;&#20013;&#23459;&#20256;&#25216;&#26415;&#30340;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;1030&#20010;&#25991;&#26412;&#30340;&#35821;&#26009;&#24211;&#65292;&#36825;&#20123;&#25991;&#26412;&#22312;&#33521;&#35821;&#21644;&#32599;&#39532;&#20044;&#23572;&#37117;&#36827;&#34892;&#20102;&#28151;&#21512;&#65292;&#24182;&#29992;20&#31181;&#23459;&#20256;&#25216;&#24039;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#25105;&#20204;&#24050;&#32463;&#20844;&#24320;&#20102;&#36825;&#20010;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#23545;&#27604;&#19981;&#21516;&#30340;&#27169;&#22411;&#21644;&#29305;&#24449;&#38598;&#21512;&#22312;&#27492;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Propaganda is a form of communication intended to influence the opinions and the mindset of the public to promote a particular agenda. With the rise of social media, propaganda has spread rapidly, leading to the need for automatic propaganda detection systems. Most work on propaganda detection has focused on high-resource languages, such as English, and little effort has been made to detect propaganda for low-resource languages. Yet, it is common to find a mix of multiple languages in social media communication, a phenomenon known as code-switching. Code-switching combines different languages within the same text, which poses a challenge for automatic systems. With this in mind, here we propose the novel task of detecting propaganda techniques in code-switched text. To support this task, we create a corpus of 1,030 texts code-switching between English and Roman Urdu, annotated with 20 propaganda techniques, which we make publicly available. We perform a number of experiments contrastin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#24067;&#20102;MMSMR&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;8&#20010;&#21442;&#32771;&#23545;&#35805;&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;&#35805;&#24230;&#37327;&#21644;&#35780;&#20272;&#30340;&#26410;&#26469;&#24037;&#20316;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;1750&#20010;&#31995;&#32479;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20197;&#20102;&#35299;&#31283;&#20581;&#30456;&#20851;&#24615;&#24182;&#20102;&#35299;&#27979;&#35797;&#38598;&#20013;&#25152;&#38656;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2305.14533</link><description>&lt;p&gt;
&#22914;&#20309;&#36873;&#25321;&#24744;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65306;&#29992;&#20110;&#23545;&#35805;&#25351;&#26631;&#35780;&#20272;&#30340;&#22823;&#35268;&#27169;&#22810;&#31995;&#32479;&#22810;&#21442;&#32771;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
How to Choose How to Choose Your Chatbot: A Massively Multi-System MultiReference Data Set for Dialog Metric Evaluation. (arXiv:2305.14533v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14533
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#24067;&#20102;MMSMR&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;8&#20010;&#21442;&#32771;&#23545;&#35805;&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;&#35805;&#24230;&#37327;&#21644;&#35780;&#20272;&#30340;&#26410;&#26469;&#24037;&#20316;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;1750&#20010;&#31995;&#32479;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20197;&#20102;&#35299;&#31283;&#20581;&#30456;&#20851;&#24615;&#24182;&#20102;&#35299;&#27979;&#35797;&#38598;&#20013;&#25152;&#38656;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#24067;&#20102;MMSMR&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#31995;&#32479;&#22810;&#21442;&#32771;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;&#35805;&#30340;&#24230;&#37327;&#21644;&#35780;&#20272;&#30340;&#26410;&#26469;&#24037;&#20316;&#12290;&#29992;&#20110;&#23545;&#35805;&#35780;&#20272;&#30340;&#33258;&#21160;&#25351;&#26631;&#24212;&#35813;&#26159;&#20154;&#31867;&#21028;&#26029;&#30340;&#21487;&#38752;&#20195;&#29702;&#65307;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20854;&#31283;&#20581;&#24615;&#30340;&#39564;&#35777;&#36824;&#36828;&#36828;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#12290;&#20026;&#20102;&#37327;&#21270;&#31283;&#20581;&#24615;&#30456;&#20851;&#24615;&#24182;&#20102;&#35299;&#27979;&#35797;&#38598;&#20013;&#25152;&#38656;&#30340;&#20869;&#23481;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#21333;&#21442;&#32771;&#35780;&#20272;&#38598;&#65292;&#25512;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;8&#20010;&#21442;&#32771;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;&#20102;&#36825;&#20010;&#26032;&#30340;&#35821;&#35328;&#23398;&#20064;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#35757;&#32451;&#20102;1750&#20010;&#31995;&#32479;&#65292;&#24182;&#22312;&#25105;&#20204;&#30340;&#26032;&#27979;&#35797;&#38598;&#21644;DailyDialog&#25968;&#25454;&#38598;&#19978;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#36825;&#20010;&#26032;&#30340;&#27979;&#35797;&#38598;&#65292;&#20197;&#21450;&#27599;&#20010;&#31995;&#32479;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#36229;&#21442;&#25968;&#12289;&#25512;&#29702;&#36755;&#20986;&#21644;&#25351;&#26631;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We release MMSMR, a Massively Multi-System MultiReference dataset to enable future work on metrics and evaluation for dialog. Automatic metrics for dialogue evaluation should be robust proxies for human judgments; however, the verification of robustness is currently far from satisfactory. To quantify the robustness correlation and understand what is necessary in a test set, we create and release an 8-reference dialog dataset by extending single-reference evaluation sets and introduce this new language learning conversation dataset. We then train 1750 systems and evaluate them on our novel test set and the DailyDialog dataset. We release the novel test set, and model hyper parameters, inference outputs, and metric scores for each system on a variety of datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#26469;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#26032;&#26679;&#26412;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#32463;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#22810;&#31181;&#20219;&#21153;&#23454;&#39564;&#39564;&#35777;&#65292;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14521</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Eliminating Spurious Correlations from Pre-trained Models via Data Mixing. (arXiv:2305.14521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#26469;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#26032;&#26679;&#26412;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#32463;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#22810;&#31181;&#20219;&#21153;&#23454;&#39564;&#39564;&#35777;&#65292;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25910;&#25947;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#21033;&#29992;&#20102;&#26576;&#20123;&#23646;&#24615;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#22312;&#29305;&#23450;&#31867;&#21035;&#30340;&#22823;&#22810;&#25968;&#31034;&#20363;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#24182;&#19981;&#36275;&#20197;&#39044;&#27979;&#36825;&#20123;&#31867;&#21035;&#12290;&#23398;&#21040;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21487;&#33021;&#20250;&#22312;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21518;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#20250;&#38477;&#20302;&#27169;&#22411;&#23545;&#19981;&#23637;&#29616;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#31034;&#20363;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#19968;&#23567;&#32452;&#24102;&#26377;&#34394;&#20551;&#23646;&#24615;&#30340;&#31034;&#20363;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#26469;&#24179;&#34913;&#25152;&#26377;&#31867;&#21035;&#20013;&#30340;&#34394;&#20551;&#23646;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#65292;&#21253;&#25324;&#28040;&#38500;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models pre-trained on large datasets have achieved remarkable convergence and robustness properties. However, these models often exploit spurious correlations between certain attributes and labels, which are prevalent in the majority of examples within specific categories but are not predictive of these categories in general. The learned spurious correlations may persist even after fine-tuning on new data, which degrades models' performance on examples that do not exhibit the spurious correlation. In this work, we propose a simple and highly effective method to eliminate spurious correlations from pre-trained models. The key idea of our method is to leverage a small set of examples with spurious attributes, and balance the spurious attributes across all classes via data mixing. We theoretically confirm the effectiveness of our method, and empirically demonstrate its state-of-the-art performance on various vision and NLP tasks, including eliminating spurious correlation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#21542;&#33021;&#22815;&#22312;&#25200;&#21160;&#35777;&#25454;&#19979;&#20570;&#20986;&#36923;&#36753;&#25512;&#29702;&#30340;&#32467;&#35770;&#65292;&#32467;&#26524;&#21457;&#29616;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;GPT&#27169;&#22411;&#22312;&#22788;&#29702;&#25805;&#32437;&#35777;&#25454;&#19979;&#30340;&#25512;&#29702;&#26102;&#20063;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.14507</link><description>&lt;p&gt;
&#25200;&#21160;&#35777;&#25454;&#19979;&#30340;&#25512;&#29702;&#65306;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#29983;&#27169;&#25311;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Deduction under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models. (arXiv:2305.14507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#21542;&#33021;&#22815;&#22312;&#25200;&#21160;&#35777;&#25454;&#19979;&#20570;&#20986;&#36923;&#36753;&#25512;&#29702;&#30340;&#32467;&#35770;&#65292;&#32467;&#26524;&#21457;&#29616;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;GPT&#27169;&#22411;&#22312;&#22788;&#29702;&#25805;&#32437;&#35777;&#25454;&#19979;&#30340;&#25512;&#29702;&#26102;&#20063;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#21542;&#33021;&#22815;&#22788;&#29702;&#20276;&#26377;&#25197;&#26354;&#20107;&#23454;&#30340;&#36923;&#36753;&#25512;&#29702;&#65292;&#21363;&#25200;&#21160;&#35777;&#25454;&#19979;&#30340;&#25512;&#29702;&#65288;DUPE&#65289;&#12290;&#30001;&#20110;LLM&#36890;&#24120;&#20381;&#36182;&#20110;&#32534;&#30721;&#20102;&#22823;&#37096;&#20998;&#20934;&#30830;&#20449;&#24687;&#30340;&#21442;&#25968;&#36827;&#34892;&#25512;&#29702;&#21644;&#25512;&#26029;&#65292;DUPE&#23545;LLM&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;DUPE&#20013;&#65292;LLM&#24517;&#39035;&#22312;&#25552;&#31034;&#20013;&#23384;&#22312;&#30340;&#34987;&#25805;&#32437;&#25110;&#20266;&#36896;&#30340;&#35777;&#25454;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20165;&#22312;&#34987;&#25805;&#32437;&#35777;&#25454;&#19979;&#25165;&#26377;&#25928;&#30340;&#38169;&#35823;&#32467;&#35770;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;DUPE&#30830;&#23450;LLM&#26159;&#21542;&#33021;&#22815;&#24471;&#20986;&#36825;&#20123;&#38169;&#35823;&#30340;&#32467;&#35770;&#65292;&#24182;&#30830;&#23450;&#24433;&#21709;&#25512;&#29702;&#36807;&#31243;&#30340;&#20027;&#35201;&#22240;&#32032;&#26159;&#21442;&#25968;&#20013;&#32534;&#30721;&#30340;&#25968;&#25454;&#36824;&#26159;&#25552;&#31034;&#20013;&#30340;&#34987;&#25805;&#32437;&#35777;&#25454;&#12290;&#20026;&#20102;&#35780;&#20272;LLM&#30340;DUPE&#33021;&#21147;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;DUPE&#29256;&#26412;&#30340;StrategyQA&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#20107;&#23454;&#34987;&#25805;&#32437;&#20197;&#25197;&#36716;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;GPT&#27169;&#22411;&#22312;&#22788;&#29702;&#25805;&#32437;&#35777;&#25454;&#19979;&#30340;&#25512;&#29702;&#26102;&#20063;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore whether Large Language Models (LLMs) are capable of logical reasoning with distorted facts, which we call Deduction under Perturbed Evidence (DUPE). DUPE presents a unique challenge to LLMs since they typically rely on their parameters, which encode mostly accurate information, to reason and make inferences. However, in DUPE, LLMs must reason over manipulated or falsified evidence present in their prompts, which can result in false conclusions that are valid only under the manipulated evidence. Our goal with DUPE is to determine whether LLMs can arrive at these false conclusions and identify whether the dominant factor influencing the deduction process is the encoded data in the parameters or the manipulated evidence in the prompts. To evaluate the DUPE capabilities of LLMs, we create a DUPEd version of the StrategyQA dataset, where facts are manipulated to reverse the answer to the question. Our findings show that even the most advanced GPT models struggle to reason on mani
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RetICL &#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#22320;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#22411;&#30340;&#31034;&#20363;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#24207;&#21015;&#31034;&#20363;&#36873;&#25321;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#19988;&#20248;&#21270;&#36873;&#25321;&#20026;&#20351;&#20219;&#21153;&#34920;&#29616;&#26368;&#20339;&#30340;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.14502</link><description>&lt;p&gt;
&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#39034;&#24207;&#26816;&#32034;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;RetICL
&lt;/p&gt;
&lt;p&gt;
RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning. (arXiv:2305.14502v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RetICL &#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#22320;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#22411;&#30340;&#31034;&#20363;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#24207;&#21015;&#31034;&#20363;&#36873;&#25321;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#19988;&#20248;&#21270;&#36873;&#25321;&#20026;&#20351;&#20219;&#21153;&#34920;&#29616;&#26368;&#20339;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#20013;&#30340;&#35768;&#22810;&#21457;&#23637;&#37117;&#38598;&#20013;&#22312;&#20419;&#20351;&#23427;&#20204;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20854;&#20013;&#27169;&#22411;&#22312;&#32473;&#23450;&#19968;&#20010;&#65288;&#25110;&#22810;&#20010;&#65289;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#65288;&#21487;&#33021;&#26159;&#26032;&#30340;&#65289;&#29983;&#25104;/&#39044;&#27979;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#31034;&#20363;&#30340;&#36873;&#25321;&#21487;&#33021;&#23545;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#22909;&#30340;&#31034;&#20363;&#24182;&#19981;&#26159;&#31616;&#21333;&#30340;&#65292;&#22240;&#20026;&#20195;&#34920;&#24615;&#31034;&#20363;&#32452;&#30340;&#23450;&#20041;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#19981;&#21516;&#32780;&#22823;&#19981;&#30456;&#21516;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#36873;&#25321;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#29420;&#31435;&#22320;&#23545;&#31034;&#20363;&#36827;&#34892;&#35780;&#20998;&#65292;&#24573;&#30053;&#23427;&#20204;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31034;&#20363;&#30340;&#39034;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#26041;&#27861;&#8212;&#8212;In-Context Learning&#30340;&#26816;&#32034;RetICL&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#36880;&#27493;&#36873;&#25321;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;&#25105;&#20204;&#25226;&#39034;&#24207;&#31034;&#20363;&#36873;&#25321;&#30340;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent developments in large language models focus on prompting them to perform specific tasks. One effective prompting method is in-context learning, where the model performs a (possibly new) generation/prediction task given one (or more) examples. Past work has shown that the choice of examples can make a large impact on task performance. However, finding good examples is not straightforward since the definition of a representative group of examples can vary greatly depending on the task. While there are many existing methods for selecting in-context examples, they generally score examples independently, ignoring the dependency between them and the order in which they are provided to the large language model. In this work, we propose Retrieval for In-Context Learning (RetICL), a learnable method for modeling and optimally selecting examples sequentially for in-context learning. We frame the problem of sequential example selection as a Markov decision process, design an example r
&lt;/p&gt;</description></item><item><title>NAIL&#26159;&#19968;&#31181;&#24102;&#26377;&#39640;&#25928;&#38750;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#30340;&#35789;&#27719;&#26816;&#32034;&#25351;&#25968;&#27169;&#22411;&#65292;&#21487;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#19988;&#20351;&#29992;&#21830;&#21697;CPU&#25552;&#20379;&#26381;&#21153;&#12290;&#23427;&#21487;&#20197;&#25429;&#25417;Transformer&#20132;&#21449;&#20851;&#27880;&#27169;&#22411;&#25910;&#30410;&#39640;&#36798;86&#65285;&#30340;&#26041;&#27861;&#65292;&#19982;BM25&#26816;&#32034;&#22120;&#32467;&#21512;&#20351;&#29992;&#21305;&#37197;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21452;&#32534;&#30721;&#22120;&#26816;&#32034;&#22120;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.14499</link><description>&lt;p&gt;
NAIL: &#24102;&#39640;&#25928;&#38750;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#30340;&#35789;&#27719;&#26816;&#32034;&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders. (arXiv:2305.14499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14499
&lt;/p&gt;
&lt;p&gt;
NAIL&#26159;&#19968;&#31181;&#24102;&#26377;&#39640;&#25928;&#38750;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#30340;&#35789;&#27719;&#26816;&#32034;&#25351;&#25968;&#27169;&#22411;&#65292;&#21487;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#19988;&#20351;&#29992;&#21830;&#21697;CPU&#25552;&#20379;&#26381;&#21153;&#12290;&#23427;&#21487;&#20197;&#25429;&#25417;Transformer&#20132;&#21449;&#20851;&#27880;&#27169;&#22411;&#25910;&#30410;&#39640;&#36798;86&#65285;&#30340;&#26041;&#27861;&#65292;&#19982;BM25&#26816;&#32034;&#22120;&#32467;&#21512;&#20351;&#29992;&#21305;&#37197;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21452;&#32534;&#30721;&#22120;&#26816;&#32034;&#22120;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25991;&#26723;&#37325;&#26032;&#25490;&#21517;&#22120;&#22312;&#31934;&#24230;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#38656;&#35201;&#19987;&#29992;&#30828;&#20214;&#36827;&#34892;&#26381;&#21153;&#65292;&#36825;&#26159;&#26114;&#36149;&#24182;&#19988;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#31181;&#26381;&#21153;&#26102;&#38388;&#35201;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#25429;&#25417;Transformer&#20132;&#21449;&#20851;&#27880;&#27169;&#22411;&#25910;&#30410;&#39640;&#36798;86&#65285;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#21482;&#38656;&#35201;&#27599;&#20010;&#25991;&#26723;&#36716;&#25442;&#22120;FLOP&#30340;10-6&#65285;&#30340;&#35789;&#27719;&#24471;&#20998;&#21151;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#21830;&#21697;CPU&#25552;&#20379;&#26381;&#21153;&#12290;&#24403;&#19982;BM25&#26816;&#32034;&#22120;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#27492;&#26041;&#27861;&#21487;&#20197;&#21305;&#37197;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#21452;&#32534;&#30721;&#22120;&#26816;&#32034;&#22120;&#30340;&#36136;&#37327;&#65292;&#35813;&#26816;&#32034;&#22120;&#20173;&#38656;&#35201;&#21152;&#36895;&#22120;&#36827;&#34892;&#26597;&#35810;&#32534;&#30721;&#12290;&#25105;&#20204;&#23558;NAIL&#65288;&#24102;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#38750;&#33258;&#22238;&#24402;&#32034;&#24341;&#65289;&#24341;&#20837;&#20026;&#19982;&#26368;&#36817;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;T5&#12289;GPT-3&#21644;PaLM&#65289;&#20860;&#23481;&#30340;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#12290;&#35813;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21487;&#20197;&#21033;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#65292;&#24182;&#21487;&#20197;&#24494;&#35843;&#20197;&#26377;&#25928;&#22320;&#26500;&#24314;&#19981;&#38656;&#35201;n
&lt;/p&gt;
&lt;p&gt;
Neural document rerankers are extremely effective in terms of accuracy. However, the best models require dedicated hardware for serving, which is costly and often not feasible. To avoid this serving-time requirement, we present a method of capturing up to 86% of the gains of a Transformer cross-attention model with a lexicalized scoring function that only requires 10-6% of the Transformer's FLOPs per document and can be served using commodity CPUs. When combined with a BM25 retriever, this approach matches the quality of a state-of-the art dual encoder retriever, that still requires an accelerator for query encoding. We introduce NAIL (Non-Autoregressive Indexing with Language models) as a model architecture that is compatible with recent encoder-decoder and decoder-only large language models, such as T5, GPT-3 and PaLM. This model architecture can leverage existing pre-trained checkpoints and can be fine-tuned for efficiently constructing document representations that do not require n
&lt;/p&gt;</description></item><item><title>Self-Polish&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20419;&#20351;&#27169;&#22411;&#36880;&#27493;&#23436;&#21892;&#32473;&#23450;&#30340;&#38382;&#39064;&#20197;&#20351;&#20854;&#26356;&#26131;&#29702;&#35299;&#21644;&#21487;&#35299;&#20915;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14497</link><description>&lt;p&gt;
&#33258;&#25105;&#31934;&#30952;&#65306;&#36890;&#36807;&#38382;&#39064;&#31934;&#21270;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement. (arXiv:2305.14497v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14497
&lt;/p&gt;
&lt;p&gt;
Self-Polish&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20419;&#20351;&#27169;&#22411;&#36880;&#27493;&#23436;&#21892;&#32473;&#23450;&#30340;&#38382;&#39064;&#20197;&#20351;&#20854;&#26356;&#26131;&#29702;&#35299;&#21644;&#21487;&#35299;&#20915;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992; Chain-of-Thought&#65288;CoT&#65289;&#31561;&#25552;&#31034;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#25506;&#32034;&#20102;&#21512;&#29702;&#21270;&#21644;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#24573;&#30053;&#20102;&#20302;&#36136;&#37327;&#25512;&#29702;&#38382;&#39064;&#21487;&#33021;&#20250;&#26174;&#30528;&#24433;&#21709;&#25512;&#29702;&#24615;&#33021;&#30340;&#28508;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Self-Polish&#65288;SP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20419;&#20351;&#27169;&#22411;&#36880;&#27493;&#23436;&#21892;&#32473;&#23450;&#30340;&#38382;&#39064;&#20197;&#20351;&#20854;&#26356;&#26131;&#29702;&#35299;&#21644;&#21487;&#35299;&#20915;&#65292;&#20197;&#20419;&#36827;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#25945;&#25480;&#27169;&#22411;&#28040;&#38500;&#26080;&#20851;&#20449;&#24687;&#65292;&#37325;&#26032;&#25490;&#21015;&#36923;&#36753;&#32467;&#26500;&#65292;&#24182;&#23558;&#23616;&#37096;&#26465;&#20214;&#24182;&#34892;&#32452;&#32455;&#25104;&#26032;&#30340;&#26465;&#20214;&#12290; SP&#19982;&#25152;&#26377;&#20854;&#20182;&#25552;&#31034;&#26041;&#27861;&#27491;&#20132;&#65292;&#26041;&#20415;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#38598;&#25104;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#20197;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting methods such as Chain-of-Thought (CoT) have shed new light on enhancing the reasoning capabilities of large language models, and researchers have extensively explored the generation process of rationales and answers. However, they have overlooked the potential challenges posed by the poor quality of reasoning problems, which may influence the reasoning performance significantly. In this work, we propose Self-Polish (SP), a novel method that facilitates the model's problem-solving process by prompting them to progressively refine the given problems to be more comprehensible and solvable. Specifically, the method teaches models to eliminate irrelevant information, rearrange the logic structure and organize local conditions into new ones parallelly. SP is orthogonal to all other prompting methods, making it convenient to integrate with state-of-the-art techniques for further improvement. We conduct thorough experiments on five benchmarks to illustrate the effectiveness of the pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#25552;&#31034;&#20301;&#32622;&#23545;&#20110;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#20855;&#26377;&#23454;&#36136;&#24615;&#24433;&#21709;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#20301;&#32622;&#36890;&#24120;&#26159;&#27425;&#20248;&#30340;&#65292;&#25552;&#31034;&#20301;&#32622;&#20248;&#21270;&#24212;&#25104;&#20026;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.14493</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;NLU&#20219;&#21153;&#20013;&#25552;&#31034;&#20301;&#32622;&#30830;&#23454;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Prompt position really matters in few-shot and zero-shot NLU tasks. (arXiv:2305.14493v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14493
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#25552;&#31034;&#20301;&#32622;&#23545;&#20110;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#20855;&#26377;&#23454;&#36136;&#24615;&#24433;&#21709;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#20301;&#32622;&#36890;&#24120;&#26159;&#27425;&#20248;&#30340;&#65292;&#25552;&#31034;&#20301;&#32622;&#20248;&#21270;&#24212;&#25104;&#20026;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21560;&#24341;&#20102;&#20247;&#22810;&#30740;&#31350;&#32773;&#30340;&#20851;&#27880;&#12290;&#20294;&#26159;&#65292;&#26377;&#25928;&#25552;&#31034;&#27169;&#26495;&#30340;&#24320;&#21457;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#31034;&#35789;&#27719;&#36873;&#25321;&#25110;&#20445;&#30041;&#25552;&#31034;&#20301;&#32622;&#30340;&#23884;&#20837;&#21021;&#22987;&#21270;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#25552;&#31034;&#20301;&#32622;&#36873;&#39033;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#37327;&#21270;&#20102;&#25552;&#31034;&#20301;&#32622;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#23454;&#36136;&#24615;&#24433;&#21709;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#20301;&#32622;&#23545;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#36890;&#24120;&#26159;&#27425;&#20248;&#30340;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#25552;&#31034;&#20301;&#32622;&#20248;&#21270;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#19982;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#37325;&#24515;&#24182;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based models have made remarkable advancements in the fields of zero-shot and few-shot learning, attracting a lot of attention from researchers. Developing an effective prompt template plays a critical role. However, prior studies have mainly focused on prompt vocabulary selection or embedding initialization with the reserved prompt position fixed. In this empirical study, we conduct the most comprehensive analysis to date of prompt position option for natural language understanding tasks. Our findings quantify the substantial impact prompt position has on model performance. We observe that the prompt position used in prior studies is often sub-optimal for both zero-shot and few-shot settings. These findings suggest prompt position optimisation as an interesting research direction alongside the existing focus on prompt engineering.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#27604;&#36739;&#20013;&#32654;&#25991;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#65292;&#36890;&#36807;&#24773;&#22659;&#23545;&#40784;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#21462;&#31038;&#20250;&#35268;&#33539;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#27979;&#35797;&#21457;&#29616;&#35813;&#26041;&#27861;&#20855;&#26377;&#30740;&#31350;&#31038;&#20250;&#25991;&#21270;&#24322;&#21516;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14492</link><description>&lt;p&gt;
&#36890;&#36807;&#24773;&#22659;&#23545;&#40784;&#21644;&#21487;&#35299;&#37322;&#25991;&#26412;&#34164;&#21547;&#26469;&#27604;&#36739;&#31038;&#20250;&#25991;&#21270;&#35268;&#33539;&#30340;&#24322;&#21516;&#65306;&#20197;&#20013;&#22269;&#21644;&#32654;&#22269;&#25991;&#21270;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Sociocultural Norm Similarities and Differences via Situational Alignment and Explainable Textual Entailment. (arXiv:2305.14492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#27604;&#36739;&#20013;&#32654;&#25991;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#65292;&#36890;&#36807;&#24773;&#22659;&#23545;&#40784;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#21462;&#31038;&#20250;&#35268;&#33539;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#27979;&#35797;&#21457;&#29616;&#35813;&#26041;&#27861;&#20855;&#26377;&#30740;&#31350;&#31038;&#20250;&#25991;&#21270;&#24322;&#21516;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#25991;&#21270;&#25512;&#29702;&#38656;&#35201;&#24314;&#31435;&#22312;&#30456;&#24212;&#25991;&#21270;&#32972;&#26223;&#19979;&#30340;&#20934;&#30830;&#35268;&#33539;&#22522;&#30784;&#20043;&#19978;&#12290; &#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#31038;&#20250;&#35268;&#33539;&#35745;&#31639;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#32654;&#22269;&#31038;&#20250;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#21644;&#27604;&#36739;&#20013;&#22269;&#21644;&#32654;&#22269;&#25991;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#12290; &#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#20013;&#25991;&#38382;&#31572;&#24179;&#21488; - &#30693;&#20046; - &#21644;&#29616;&#26377;&#30340;SocialChemistry&#25968;&#25454;&#38598;&#20316;&#20026;&#19981;&#21516;&#25991;&#21270;&#32500;&#24230;&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#24773;&#22659;&#23545;&#40784;&#36328;&#25991;&#21270;&#27604;&#36739;&#31038;&#20250;&#24773;&#22659;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#31038;&#20250;&#35268;&#33539;&#12290;&#25105;&#20204;&#23558;Chain-of-Thought&#25552;&#31034;&#23884;&#20837;&#21040;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#26694;&#26550;&#20013;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;3,069&#20010;&#36328;&#20013;&#22269;&#21644;&#32654;&#22269;&#25991;&#21270;&#23545;&#40784;&#30340;&#31038;&#20250;&#35268;&#33539;&#20197;&#21450;&#30456;&#24212;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#12290;&#20026;&#20102;&#27979;&#35797;&#27169;&#22411;&#22312;&#36328;&#25991;&#21270;&#31038;&#20250;&#35268;&#33539;&#26041;&#38754;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31038;&#20250;&#35268;&#33539;&#30340;&#21487;&#35299;&#37322;&#25991;&#26412;&#34164;&#21547;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#39044;&#27979;&#19968;&#20010;&#25991;&#21270;&#20013;&#30340;&#31038;&#20250;&#35268;&#33539;&#30340;&#35299;&#37322;&#26159;&#21542;&#26263;&#31034;&#21478;&#19968;&#20010;&#25991;&#21270;&#20013;&#30340;&#31867;&#20284;&#35268;&#33539;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#20219;&#21153;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#30740;&#31350;&#31038;&#20250;&#25991;&#21270;&#30340;&#24322;&#21516;&#30340;&#20016;&#23500;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing systems that can reason across cultures requires that they are grounded in the norms of the contexts in which they operate. However, current research on developing computational models of social norms has primarily focused on American society. Here, we propose a novel approach to discover and compare descriptive social norms across Chinese and American cultures. We demonstrate our approach by leveraging discussions on a Chinese Q&amp;A platform-Zhihu-and the existing SocialChemistry dataset as proxies for contrasting cultural axes, align social situations cross-culturally, and extract social norms from texts using in-context learning. Embedding Chain-of-Thought prompting in a human-AI collaborative framework, we build a high-quality dataset of 3,069 social norms aligned with social situations across Chinese and American cultures alongside corresponding free-text explanations. To test the ability of models to reason about social norms across cultures, we introduce the task of expl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861; SIRLC&#65292;&#21487;&#20197;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26377;&#25928;&#22320;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#22806;&#37096;&#26631;&#31614;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;NLP&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.14483</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21453;&#24605;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#25552;&#21319;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Language Model Self-improvement by Reinforcement Learning Contemplation. (arXiv:2305.14483v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861; SIRLC&#65292;&#21487;&#20197;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26377;&#25928;&#22320;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#22806;&#37096;&#26631;&#31614;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;NLP&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#24120;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#30417;&#30563;&#26469;&#33719;&#21462;&#65292;&#36825;&#26679;&#24456;&#26174;&#28982;&#26159;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#21453;&#24605;&#33258;&#25105;&#25552;&#21319;&#65288;SIRLC&#65289;&#30340;&#26032;&#22411;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#22806;&#37096;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;LLMs&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#35266;&#23519;&#32467;&#26524;&#65306;&#35821;&#35328;&#27169;&#22411;&#27604;&#29983;&#25104;&#25991;&#26412;&#26356;&#23481;&#26131;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;SIRLC&#32473;LLMs&#20998;&#37197;&#20102;&#21452;&#37325;&#35282;&#33394;&#65292;&#19968;&#26041;&#38754;&#20316;&#20026;&#23398;&#29983;&#29983;&#25104;&#26080;&#26631;&#31614;&#38382;&#39064;&#30340;&#31572;&#26696;&#65292;&#21478;&#19968;&#26041;&#38754;&#20316;&#20026;&#25945;&#24072;&#35780;&#20272;&#29983;&#25104;&#30340;&#25991;&#26412;&#24182;&#25454;&#27492;&#32473;&#20986;&#20998;&#25968;&#12290;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#20197;&#26368;&#22823;&#21270;&#35780;&#20272;&#20998;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SIRLC&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;NLP&#20219;&#21153;&#65292;&#20363;&#22914;&#25512;&#29702;&#38382;&#39064;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exhibited remarkable performance across various natural language processing (NLP) tasks. However, fine-tuning these models often necessitates substantial supervision, which can be expensive and time-consuming to obtain. This paper introduces a novel unsupervised method called LanguageModel Self-Improvement by Reinforcement Learning Contemplation (SIRLC) that improves LLMs without reliance on external labels. Our approach is grounded in the observation that it is simpler for language models to assess text quality than to generate text. Building on this insight, SIRLC assigns LLMs dual roles as both student and teacher. As a student, the LLM generates answers to unlabeled questions, while as a teacher, it evaluates the generated text and assigns scores accordingly. The model parameters are updated using reinforcement learning to maximize the evaluation score. We demonstrate that SIRLC can be applied to various NLP tasks, such as reasoning problems, text 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#21477;&#23376;&#23884;&#20837;&#22914;&#20309;&#25429;&#25417;&#27431;&#27954;&#22269;&#23478;&#21644;&#32844;&#19994;&#65292;&#24182;&#21457;&#29616;&#23884;&#20837;&#20013;&#26368;&#31361;&#20986;&#30340;&#22269;&#23478;&#29305;&#24449;&#26159;&#20854;GPD&#32463;&#27982;&#23454;&#21147;&#12290;&#26412;&#30740;&#31350;&#20013;&#30340;&#22823;&#37096;&#20998;&#22269;&#23478;&#32500;&#24230;&#19982;&#32844;&#19994;&#32500;&#24230;&#19981;&#30456;&#20851;&#65292;&#20294;&#19968;&#31181;&#27169;&#22411;&#34920;&#29616;&#20986;&#32844;&#19994;&#22768;&#26395;&#21644;&#21407;&#31821;&#22269;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36825;&#26159;&#19968;&#31181;&#28508;&#22312;&#30340;&#22522;&#20110;&#22269;&#31821;&#30340;&#27495;&#35270;&#12290;</title><link>http://arxiv.org/abs/2305.14482</link><description>&lt;p&gt;
&#26159;&#21542;&#20855;&#26377;&#22768;&#26395;&#30340;&#24037;&#20316;&#19982;&#22768;&#26395;&#39640;&#30340;&#22269;&#23478;&#30456;&#21516;&#65311;&#22810;&#35821;&#21477;&#23376;&#23884;&#20837;&#21644;&#27431;&#27954;&#22269;&#23478;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Is a Prestigious Job the same as a Prestigious Country? A Case Study on Multilingual Sentence Embeddings and European Countries. (arXiv:2305.14482v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#21477;&#23376;&#23884;&#20837;&#22914;&#20309;&#25429;&#25417;&#27431;&#27954;&#22269;&#23478;&#21644;&#32844;&#19994;&#65292;&#24182;&#21457;&#29616;&#23884;&#20837;&#20013;&#26368;&#31361;&#20986;&#30340;&#22269;&#23478;&#29305;&#24449;&#26159;&#20854;GPD&#32463;&#27982;&#23454;&#21147;&#12290;&#26412;&#30740;&#31350;&#20013;&#30340;&#22823;&#37096;&#20998;&#22269;&#23478;&#32500;&#24230;&#19982;&#32844;&#19994;&#32500;&#24230;&#19981;&#30456;&#20851;&#65292;&#20294;&#19968;&#31181;&#27169;&#22411;&#34920;&#29616;&#20986;&#32844;&#19994;&#22768;&#26395;&#21644;&#21407;&#31821;&#22269;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36825;&#26159;&#19968;&#31181;&#28508;&#22312;&#30340;&#22522;&#20110;&#22269;&#31821;&#30340;&#27495;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#35821;&#21477;&#23376;&#34920;&#31034;&#22914;&#20309;&#25429;&#25417;&#27431;&#27954;&#22269;&#23478;&#65292;&#20197;&#21450;&#36825;&#31181;&#24046;&#24322;&#22914;&#20309;&#22312;&#27431;&#27954;&#35821;&#35328;&#20043;&#38388;&#19981;&#21516;&#12290;&#25105;&#20204;&#29992;&#27169;&#26495;&#21477;&#23376;&#25552;&#31034;&#27169;&#22411;&#65292;&#23558;&#20854;&#26426;&#22120;&#32763;&#35793;&#25104;12&#31181;&#27431;&#27954;&#35821;&#35328;&#65292;&#24182;&#20998;&#26512;&#23884;&#20837;&#20013;&#26368;&#31361;&#20986;&#30340;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23884;&#20837;&#20013;&#26368;&#31361;&#20986;&#30340;&#22269;&#23478;&#29305;&#24449;&#26159;&#20854;GPD&#32463;&#27982;&#23454;&#21147;&#12290;&#24403;&#29305;&#21035;&#35810;&#38382;&#22768;&#26395;&#39640;&#20302;&#26102;&#65292;&#23884;&#20837;&#31354;&#38388;&#28165;&#26970;&#22320;&#21306;&#20998;&#20102;&#22768;&#26395;&#39640;&#20302;&#30340;&#24037;&#20316;&#12290;&#19977;&#20010;&#21463;&#30740;&#31350;&#30340;&#27169;&#22411;&#20013;&#30340;&#22823;&#37096;&#20998;&#22269;&#23478;&#32500;&#24230;&#19982;&#32844;&#19994;&#32500;&#24230;&#19981;&#30456;&#20851;&#65292;&#20294;Distilled Multilingual Universal Sentence Encoder&#27169;&#22411;&#34920;&#29616;&#20986;&#32844;&#19994;&#22768;&#26395;&#21644;&#21407;&#31821;&#22269;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36825;&#26159;&#19968;&#31181;&#28508;&#22312;&#30340;&#22522;&#20110;&#22269;&#31821;&#30340;&#27495;&#35270;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#26159;&#19968;&#33268;&#30340;&#65292;&#24182;&#19988;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#19982;&#19978;&#36848;&#20363;&#22806;&#24773;&#20917;&#19979;&#30340;&#21463;&#30740;&#31350;&#30340;&#34920;&#31034;&#27169;&#22411;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how multilingual sentence representations capture European countries and how this differs across European languages. We prompt the models with templated sentences that we machine-translate into 12 European languages and analyze the most prominent dimensions in the embeddings. Our analysis reveals that the most prominent country feature in the embedding is its economic strength in terms of GPD. When prompted specifically for job prestige, the embedding space clearly distinguishes high and low-prestige jobs. The occupational dimension is uncorrelated with the most dominant country dimensions for three out of four studied models. One model: Distilled Multilingual Universal Sentence Encoder, however, exhibited a connection between occupational prestige and country of origin, which is a potential source of nationality-based discrimination. Our findings are consistent across languages and, to some extent, with the exception mentioned above, across studied representation models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FOCUS&#65292;&#22312;&#22810;&#35821;&#35328;&#28304;&#27169;&#22411;&#35774;&#32622;&#19979;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#37325;&#21472;&#26631;&#35760;&#32452;&#21512;&#26377;&#25928;&#22320;&#21021;&#22987;&#21270;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#36866;&#24212;&#26032;&#35821;&#35328;&#26102;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14481</link><description>&lt;p&gt;
FOCUS&#65306;&#22522;&#20110;&#21333;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#23884;&#20837;&#21021;&#22987;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FOCUS: Effective Embedding Initialization for Specializing Pretrained Multilingual Models on a Single Language. (arXiv:2305.14481v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FOCUS&#65292;&#22312;&#22810;&#35821;&#35328;&#28304;&#27169;&#22411;&#35774;&#32622;&#19979;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#37325;&#21472;&#26631;&#35760;&#32452;&#21512;&#26377;&#25928;&#22320;&#21021;&#22987;&#21270;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#36866;&#24212;&#26032;&#35821;&#35328;&#26102;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#12290;&#20351;&#29992;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26435;&#37325;&#20316;&#20026;&#28201;&#21551;&#21160;&#65292;&#21487;&#20197;&#20943;&#23569;&#27492;&#38656;&#27714;&#12290;&#20026;&#20102;&#36866;&#24212;&#26032;&#35821;&#35328;&#65292;&#38656;&#35201;&#23545;&#39044;&#35757;&#32451;&#30340;&#35789;&#27719;&#34920;&#21644;&#23884;&#20837;&#36827;&#34892;&#35843;&#25972;&#12290;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#38024;&#23545;&#36866;&#24212;&#21518;&#30340;&#35789;&#27719;&#34920;&#30340;&#23884;&#20837;&#21021;&#22987;&#21270;&#22823;&#22810;&#32858;&#28966;&#20110;&#21333;&#35821;&#35328;&#28304;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#28304;&#27169;&#22411;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;FOCUS-&#24555;&#36895;&#37325;&#21472;&#26631;&#35760;&#32452;&#21512;&#20351;&#29992;Sparsemax&#30340;&#26032;&#22411;&#23884;&#20837;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#24403;&#36866;&#24212;XLM-R&#26102;&#65292;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;FOCUS&#23558;&#26032;&#22686;&#30340;&#26631;&#35760;&#34920;&#31034;&#20026;&#39044;&#35757;&#32451;&#21644;&#26032;&#35789;&#27719;&#34920;&#20043;&#38388;&#30340;&#37325;&#21472;&#26631;&#35760;&#32452;&#21512;&#12290;&#36825;&#20123;&#37325;&#21472;&#26631;&#35760;&#26159;&#22522;&#20110;&#36741;&#21161;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#36827;&#34892;&#36873;&#25321;&#30340;&#12290;&#25105;&#20204;&#23454;&#29616;&#30340;FOCUS&#20844;&#24320;&#22312;GitHub&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using model weights pretrained on a high-resource language as a warm start can reduce the need for data and compute to obtain high-quality language models in low-resource languages. To accommodate the new language, the pretrained vocabulary and embeddings need to be adapted. Previous work on embedding initialization for such adapted vocabularies has mostly focused on monolingual source models. In this paper, we investigate the multilingual source model setting and propose FOCUS - Fast Overlapping Token Combinations Using Sparsemax, a novel embedding initialization method that outperforms previous work when adapting XLM-R. FOCUS represents newly added tokens as combinations of tokens in the overlap of the pretrained and new vocabularies. The overlapping tokens are selected based on semantic similarity in an auxiliary token embedding space. Our implementation of FOCUS is publicly available on GitHub.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#29289;&#21307;&#23398;&#35686;&#25253;&#26032;&#38395;&#25968;&#25454;&#38598;&#65288;BAND&#65289;&#65292;&#20854;&#21253;&#25324;&#20102;1,508&#20010;&#26679;&#26412;&#21644;30&#20010;&#19982;&#27969;&#34892;&#30149;&#23398;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#22909;&#30340;&#20869;&#23481;&#20266;&#35013;&#33021;&#21147;&#21644;&#37325;&#35201;&#20449;&#24687;&#25512;&#26029;&#33021;&#21147;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#20026;&#30142;&#30149;&#30417;&#27979;&#21644;&#27969;&#34892;&#30149;&#23398;&#20998;&#26512;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14480</link><description>&lt;p&gt;
BAND: &#29983;&#29289;&#21307;&#23398;&#35686;&#25253;&#26032;&#38395;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BAND: Biomedical Alert News Dataset. (arXiv:2305.14480v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14480
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#29289;&#21307;&#23398;&#35686;&#25253;&#26032;&#38395;&#25968;&#25454;&#38598;&#65288;BAND&#65289;&#65292;&#20854;&#21253;&#25324;&#20102;1,508&#20010;&#26679;&#26412;&#21644;30&#20010;&#19982;&#27969;&#34892;&#30149;&#23398;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#22909;&#30340;&#20869;&#23481;&#20266;&#35013;&#33021;&#21147;&#21644;&#37325;&#35201;&#20449;&#24687;&#25512;&#26029;&#33021;&#21147;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#20026;&#30142;&#30149;&#30417;&#27979;&#21644;&#27969;&#34892;&#30149;&#23398;&#20998;&#26512;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#26579;&#24615;&#30142;&#30149;&#30340;&#29190;&#21457;&#23545;&#20154;&#31867;&#20581;&#24247;&#21644;&#31119;&#21033;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#20026;&#20102;&#25913;&#21892;&#30142;&#30149;&#30417;&#27979;&#21644;&#20102;&#35299;&#30142;&#30149;&#20256;&#25773;&#24773;&#20917;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#20960;&#20010;&#30417;&#27979;&#31995;&#32479;&#26469;&#30417;&#35270;&#27599;&#26085;&#26032;&#38395;&#35686;&#25253;&#21644;&#31038;&#20132;&#23186;&#20307;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#32463;&#36807;&#33391;&#22909;&#27880;&#37322;&#30340;&#25253;&#21578;&#25968;&#25454;&#65292;&#29616;&#26377;&#31995;&#32479;&#22312;&#19982;&#30456;&#24212;&#25552;&#37266;&#25110;&#26032;&#38395;&#30340;&#27969;&#34892;&#30149;&#23398;&#20998;&#26512;&#26041;&#38754;&#32570;&#20047;&#20005;&#35880;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29983;&#29289;&#21307;&#23398;&#35686;&#25253;&#26032;&#38395;&#25968;&#25454;&#38598;&#65288;BAND&#65289;&#65292;&#21253;&#25324;&#26469;&#33258;&#29616;&#26377;&#25253;&#21578;&#26032;&#38395;&#25991;&#31456;&#12289;&#20844;&#24320;&#30005;&#23376;&#37038;&#20214;&#21644;&#25552;&#37266;&#30340;1,508&#20010;&#26679;&#26412;&#20197;&#21450;30&#20010;&#19982;&#27969;&#34892;&#30149;&#23398;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#38656;&#35201;&#27169;&#22411;&#19987;&#23478;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#20026;&#30142;&#30149;&#29190;&#21457;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#12290;BAND&#25968;&#25454;&#38598;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#22909;&#30340;&#20869;&#23481;&#20266;&#35013;&#33021;&#21147;&#21644;&#37325;&#35201;&#20449;&#24687;&#25512;&#26029;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#22522;&#20934;&#20219;&#21153;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infectious disease outbreaks continue to pose a significant threat to human health and well-being. To improve disease surveillance and understanding of disease spread, several surveillance systems have been developed to monitor daily news alerts and social media. However, existing systems lack thorough epidemiological analysis in relation to corresponding alerts or news, largely due to the scarcity of well-annotated reports data. To address this gap, we introduce the Biomedical Alert News Dataset (BAND), which includes 1,508 samples from existing reported news articles, open emails, and alerts, as well as 30 epidemiology-related questions. These questions necessitate the model's expert reasoning abilities, thereby offering valuable insights into the outbreak of the disease. The BAND dataset brings new challenges to the NLP world, requiring better disguise capability of the content and the ability to infer important information. We provide several benchmark tasks, including Named Entity
&lt;/p&gt;</description></item><item><title>&#20013;&#22269;&#30340;CGCE&#22522;&#20934;&#20026;&#36890;&#29992;&#21644;&#37329;&#34701;&#39046;&#22495;&#24314;&#31435;&#20102;&#26631;&#20934;&#21270;&#30340;&#35780;&#27979;&#26694;&#26550;&#65292;&#21253;&#21547;200&#20010;&#36890;&#29992;&#39046;&#22495;&#38382;&#39064;&#21644;150&#20010;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#30340;&#32844;&#19994;&#38382;&#39064;&#65292;&#26088;&#22312;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.14471</link><description>&lt;p&gt;
CGCE&#65306;&#19968;&#20010;&#36866;&#29992;&#20110;&#36890;&#29992;&#21644;&#37329;&#34701;&#39046;&#22495;&#30340;&#20013;&#25991;&#29983;&#25104;&#24335;&#32842;&#22825;&#35780;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CGCE: A Chinese Generative Chat Evaluation Benchmark for General and Financial Domains. (arXiv:2305.14471v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14471
&lt;/p&gt;
&lt;p&gt;
&#20013;&#22269;&#30340;CGCE&#22522;&#20934;&#20026;&#36890;&#29992;&#21644;&#37329;&#34701;&#39046;&#22495;&#24314;&#31435;&#20102;&#26631;&#20934;&#21270;&#30340;&#35780;&#27979;&#26694;&#26550;&#65292;&#21253;&#21547;200&#20010;&#36890;&#29992;&#39046;&#22495;&#38382;&#39064;&#21644;150&#20010;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#30340;&#32844;&#19994;&#38382;&#39064;&#65292;&#26088;&#22312;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#32842;&#22825;&#27169;&#22411;&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#36890;&#36807;&#25972;&#21512;&#25351;&#20196;&#21644;&#20154;&#31867;&#21453;&#39304;&#26469;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#38761;&#26032;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#32842;&#22825;&#27169;&#22411;&#35780;&#27979;&#22522;&#20934;&#65292;&#29305;&#21035;&#26159;&#36866;&#29992;&#20110;&#20013;&#25991;&#21644;&#29305;&#23450;&#39046;&#22495;&#27169;&#22411;&#30340;&#35780;&#27979;&#22522;&#20934;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#35780;&#20272;&#21644;&#36827;&#27493;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20013;&#22269;&#29983;&#25104;&#24335;&#32842;&#22825;&#65288;CGCE&#65289;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#36890;&#29992;&#21644;&#37329;&#34701;&#39046;&#22495;&#12290;CGCE&#22522;&#20934;&#28085;&#30422;&#20102;&#22810;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;200&#20010;&#36890;&#29992;&#39046;&#22495;&#38382;&#39064;&#21644;150&#20010;&#37329;&#34701;&#39046;&#22495;&#30340;&#29305;&#23450;&#32844;&#19994;&#38382;&#39064;&#12290;&#25163;&#21160;&#35780;&#20998;&#32771;&#34385;&#20102;&#20934;&#30830;&#24615;&#12289;&#36830;&#36143;&#24615;&#12289;&#34920;&#36798;&#28165;&#26224;&#24230;&#21644;&#23436;&#25972;&#24230;&#31561;&#22240;&#32032;&#12290;CGCE&#22522;&#20934;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#21644;&#27604;&#36739;&#20013;&#25991;&#29983;&#25104;&#24335;&#32842;&#22825;&#27169;&#22411;&#65292;&#20419;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative chat models, such as ChatGPT and GPT-4, have revolutionized natural language generation (NLG) by incorporating instructions and human feedback to achieve significant performance improvements. However, the lack of standardized evaluation benchmarks for chat models, particularly for Chinese and domain-specific models, hinders their assessment and progress. To address this gap, we introduce the Chinese Generative Chat Evaluation (CGCE) benchmark, focusing on general and financial domains. The CGCE benchmark encompasses diverse tasks, including 200 questions in the general domain and 150 specific professional questions in the financial domain. Manual scoring evaluates factors such as accuracy, coherence, expression clarity, and completeness. The CGCE benchmark provides researchers with a standardized framework to assess and compare Chinese generative chat models, fostering advancements in NLG research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20004;&#20010;&#35821;&#35328;&#21644;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#19981;&#20165;&#21457;&#29616;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#37117;&#23384;&#22312;&#22899;&#24615;&#30340;&#27424;&#20195;&#34920;&#38382;&#39064;&#65292;&#32780;&#19988;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#21629;&#21517;&#20559;&#35265;&#65306;&#22312;&#36816;&#21160;&#20013;&#30007;&#24615;&#26356;&#26377;&#21487;&#33021;&#34987;&#21629;&#21517;&#19982;&#20182;&#20204;&#36816;&#21160;&#30456;&#20851;&#30340;&#31216;&#21628;&#65292;&#36825;&#31181;&#20559;&#35265;&#23545;&#22899;&#24615;&#34920;&#24449;&#36896;&#25104;&#20102;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2305.14468</link><description>&lt;p&gt;
&#20687;&#22899;&#23401;&#19968;&#26679;&#22868;&#36305;&#65281;&#20307;&#32946;&#30456;&#20851;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Run Like a Girl! Sports-Related Gender Bias in Language and Vision. (arXiv:2305.14468v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20004;&#20010;&#35821;&#35328;&#21644;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#19981;&#20165;&#21457;&#29616;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#37117;&#23384;&#22312;&#22899;&#24615;&#30340;&#27424;&#20195;&#34920;&#38382;&#39064;&#65292;&#32780;&#19988;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#21629;&#21517;&#20559;&#35265;&#65306;&#22312;&#36816;&#21160;&#20013;&#30007;&#24615;&#26356;&#26377;&#21487;&#33021;&#34987;&#21629;&#21517;&#19982;&#20182;&#20204;&#36816;&#21160;&#30456;&#20851;&#30340;&#31216;&#21628;&#65292;&#36825;&#31181;&#20559;&#35265;&#23545;&#22899;&#24615;&#34920;&#24449;&#36896;&#25104;&#20102;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#25968;&#25454;&#38598;&#20197;&#21450;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#24615;&#21035;&#20559;&#35265;&#26377;&#21487;&#33021;&#20351;&#26377;&#23475;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#27495;&#35270;&#24471;&#20197;&#25345;&#32493;&#23384;&#22312;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#20004;&#20010;&#35821;&#35328;&#21644;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#30456;&#31526;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#37117;&#23384;&#22312;&#22899;&#24615;&#30340;&#27424;&#20195;&#34920;&#38382;&#39064;&#65292;&#36825;&#20250;&#23548;&#33268;&#22899;&#24615;&#19981;&#21487;&#35265;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20551;&#35774;&#24182;&#21457;&#29616;&#20102;&#20154;&#20204;&#23545;&#21442;&#19982;&#36816;&#21160;&#30340;&#20154;&#30340;&#21629;&#21517;&#36873;&#25321;&#23384;&#22312;&#19968;&#31181;&#20559;&#35265;&#65306;&#24403;&#21442;&#19982;&#36816;&#21160;&#30340;&#26159;&#30007;&#24615;&#25110;&#30007;&#23401;&#26102;&#65292;&#35828;&#35805;&#32773;&#26356;&#32463;&#24120;&#20351;&#29992;&#25351;&#31034;&#35813;&#36816;&#21160;&#30340;&#21517;&#23383;&#65288;&#20363;&#22914;&#8220; &#32593;&#29699;&#36873;&#25163;&#8221;&#25110;&#8220; &#20914;&#28010;&#32773;&#8221;&#65289;&#65292;&#32780;&#24403;&#21442;&#19982;&#36816;&#21160;&#30340;&#26159;&#22899;&#24615;&#25110;&#22899;&#23401;&#26102;&#65292;&#24179;&#22343;&#27599;&#20010;&#24615;&#21035;&#30340;&#19982;&#36816;&#21160;&#30456;&#20851;&#30340;&#21517;&#31216;&#20165;&#26377; 35&#65285; &#24038;&#21491;&#65292;&#32780;&#30007;&#24615;&#21017;&#32422;&#20026; 46&#65285;&#12290;&#19968;&#20010;&#38024;&#23545;&#36825;&#20123;&#21629;&#21517;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#35745;&#31639;&#27169;&#22411;&#20250;&#37325;&#29616;&#36825;&#31181;&#20559;&#35265;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#25968;&#25454;&#21644;&#27169;&#22411;&#37117;&#20250;&#23545;&#22899;&#24615;&#36896;&#25104;&#34920;&#24449;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gender bias in Language and Vision datasets and models has the potential to perpetuate harmful stereotypes and discrimination. We analyze gender bias in two Language and Vision datasets. Consistent with prior work, we find that both datasets underrepresent women, which promotes their invisibilization. Moreover, we hypothesize and find that a bias affects human naming choices for people playing sports: speakers produce names indicating the sport (e.g. 'tennis player' or 'surfer') more often when it is a man or a boy participating in the sport than when it is a woman or a girl, with an average of 46% vs. 35% of sports-related names for each gender. A computational model trained on these naming data reproduces the bias. We argue that both the data and the model result in representational harm against women.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25688;&#35201;&#22686;&#24378;&#30340;&#22823;&#32434;&#30417;&#30563;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#26126;&#30830;&#21644;&#21512;&#29702;&#30340;&#22823;&#32434;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#26041;&#27861;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#29983;&#25104;&#30340;&#22823;&#32434;&#12290;</title><link>http://arxiv.org/abs/2305.14459</link><description>&lt;p&gt;
&#36890;&#36807;&#25688;&#35201;&#20108;&#20803;&#24615;&#21644;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Generation through Summarization Duality and Explicit Outline Control. (arXiv:2305.14459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25688;&#35201;&#22686;&#24378;&#30340;&#22823;&#32434;&#30417;&#30563;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#26126;&#30830;&#21644;&#21512;&#29702;&#30340;&#22823;&#32434;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#26041;&#27861;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#29983;&#25104;&#30340;&#22823;&#32434;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24320;&#25918;&#24335;&#38271;&#25991;&#26412;&#29983;&#25104;&#38754;&#20020;&#35821;&#20041;&#19981;&#36830;&#36143;&#21644;&#24773;&#33410;&#19981;&#21487;&#20449;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#36890;&#36807;&#35774;&#35745;&#26080;&#30417;&#30563;&#20219;&#21153;&#20013;&#30340;&#30701;&#35821;&#25110;&#25277;&#35937;&#20449;&#21495;&#30340;&#22823;&#32434;&#26469;&#32531;&#35299;&#27492;&#38382;&#39064;&#65292;&#20294;&#36825;&#24448;&#24448;&#26159;&#19981;&#31283;&#23450;&#19988;&#38590;&#20197;&#35299;&#37322;&#30340;&#12290;&#22312;&#20551;&#35774;&#25688;&#35201;&#20316;&#20026;&#24050;&#25104;&#29087;&#30340;&#22823;&#32434;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#12289;&#25688;&#35201;&#22686;&#24378;&#30340;&#22823;&#32434;&#30417;&#30563;&#29983;&#25104;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#25688;&#35201;&#20219;&#21153;&#30340;&#21452;&#37325;&#29305;&#24449;&#26469;&#25913;&#36827;&#22823;&#32434;&#39044;&#27979;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#26126;&#30830;&#21644;&#21512;&#29702;&#30340;&#22823;&#32434;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#22823;&#32434;&#30340;&#29983;&#25104;&#20855;&#26377;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#26080;&#35770;&#26159;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-2&#12289;BART&#65289;&#36824;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Vicuna&#12289;ChatGPT&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#26041;&#27861;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#29983;&#25104;&#30340;&#22823;&#32434;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically open-ended long text generation poses significant challenges due to semantic incoherence and plot implausibility. Previous works usually alleviate this problem through outlines in the form of short phrases or abstractive signals by designing unsupervised tasks, which tend to be unstable and weakly interpretable.  Assuming that a summary serves as a mature outline, we introduce a two-stage, summary-enhanced outline supervised generation framework. This framework leverages the dual characteristics of the summarization task to improve outline prediction, resulting in more explicit and plausible outlines. Furthermore, we identify an underutilization issue in outline-based generation with both standard pretrained language models (e.g., GPT-2, BART) and large language models (e.g., Vicuna, ChatGPT). To address this, we propose a novel explicit outline control method for more effective utilization of generated outlines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;SALSA&#26694;&#26550;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#25991;&#26412;&#31616;&#21270;&#35780;&#20272;&#65292;&#36890;&#36807;21&#31181;&#19981;&#21516;&#32534;&#36753;&#31867;&#22411;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#27169;&#22411;&#21644;&#20154;&#31867;&#25991;&#26412;&#31616;&#21270;&#30340;&#20559;&#22909;&#21644;&#34920;&#29616;&#65292;&#24182;&#24320;&#21457;&#20102;LENS-SALSA&#25351;&#26631;&#29992;&#20110;&#33258;&#21160;&#31616;&#21270;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.14458</link><description>&lt;p&gt;
&#22312;&#25104;&#21151;&#21644;&#22833;&#36133;&#20043;&#38388;&#36339;&#33310;&#65306;&#20351;&#29992;SALSA&#36827;&#34892;&#32534;&#36753;&#32423;&#21035;&#30340;&#31616;&#21270;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA. (arXiv:2305.14458v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;SALSA&#26694;&#26550;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#25991;&#26412;&#31616;&#21270;&#35780;&#20272;&#65292;&#36890;&#36807;21&#31181;&#19981;&#21516;&#32534;&#36753;&#31867;&#22411;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#27169;&#22411;&#21644;&#20154;&#31867;&#25991;&#26412;&#31616;&#21270;&#30340;&#20559;&#22909;&#21644;&#34920;&#29616;&#65292;&#24182;&#24320;&#21457;&#20102;LENS-SALSA&#25351;&#26631;&#29992;&#20110;&#33258;&#21160;&#31616;&#21270;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-3.5&#65289;&#21487;&#20197;&#20135;&#29983;&#39640;&#24230;&#35780;&#32423;&#30340;&#31616;&#21270;&#25991;&#26412;&#65292;&#20294;&#24403;&#21069;&#30340;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#26410;&#33021;&#25552;&#20379;&#23545;&#31995;&#32479;&#29305;&#23450;&#20248;&#21183;&#21644;&#21155;&#21183;&#30340;&#28165;&#26224;&#20102;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SALSA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#32534;&#36753;&#30340;&#20154;&#31867;&#27880;&#37322;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#25972;&#20307;&#21644;&#31934;&#32454;&#30340;&#25991;&#26412;&#31616;&#21270;&#35780;&#20272;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;21&#31181;&#22522;&#20110;&#35821;&#35328;&#23398;&#30340;&#32534;&#36753;&#31867;&#22411;&#65292;&#28085;&#30422;&#20102;&#27010;&#24565;&#12289;&#21477;&#27861;&#21644;&#35789;&#27719;&#31616;&#21333;&#24615;&#30340;&#25152;&#26377;&#25104;&#21151;&#21644;&#22833;&#36133;&#32500;&#24230;&#12290;&#20351;&#29992;SALSA&#65292;&#25105;&#20204;&#22312;700&#20010;&#31616;&#21270;&#26696;&#20363;&#19978;&#25910;&#38598;&#20102;12K&#20010;&#32534;&#36753;&#27880;&#37322;&#65292;&#25581;&#31034;&#20102;&#24494;&#35843;&#27169;&#22411;&#12289;&#23569;&#26679;&#26412;LLM&#21644;&#20154;&#31867;&#20043;&#38388;&#36716;&#21270;&#26041;&#27861;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;GPT-3.5&#25191;&#34892;&#30340;&#39640;&#36136;&#37327;&#32534;&#36753;&#27604;&#20154;&#31867;&#26356;&#22810;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#39057;&#32321;&#30340;&#38169;&#35823;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#31934;&#32454;&#27880;&#37322;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;LENS-SALSA&#65292;&#19968;&#31181;&#26080;&#21442;&#32771;&#33258;&#21160;&#31616;&#21270;&#24230;&#37327;&#65292;&#35757;&#32451;&#20197;&#30452;&#25509;&#20174;&#36755;&#20837;&#25991;&#26412;&#21644;&#25552;&#35758;&#30340;&#31616;&#21270;&#20013;&#39044;&#27979;&#21477;&#23376;&#21644;&#32534;&#36753;&#32423;&#21035;&#36136;&#37327;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (e.g., GPT-3.5) are uniquely capable of producing highly rated text simplification, yet current human evaluation methods fail to provide a clear understanding of systems' specific strengths and weaknesses. To address this limitation, we introduce SALSA, an edit-based human annotation framework that enables holistic and fine-grained text simplification evaluation. We develop twenty one linguistically grounded edit types, covering the full spectrum of success and failure across dimensions of conceptual, syntactic and lexical simplicity. Using SALSA, we collect 12K edit annotations on 700 simplifications, revealing discrepancies in the distribution of transformation approaches performed by fine-tuned models, few-shot LLMs and humans, and finding GPT-3.5 performs more quality edits than humans, but still exhibits frequent errors. Using our fine-grained annotations, we develop LENS-SALSA, a reference-free automatic simplification metric, trained to predict sentence- an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#20854;&#22312;&#27604;&#36739;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25991;&#26412;&#23454;&#20307;&#27604;&#36739;&#25968;&#25454;&#30340;&#26041;&#27861;&#21644;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14457</link><description>&lt;p&gt;
&#20026;&#27604;&#36739;&#25512;&#29702;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pre-training Language Models for Comparative Reasoning. (arXiv:2305.14457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#20854;&#22312;&#27604;&#36739;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25991;&#26412;&#23454;&#20307;&#27604;&#36739;&#25968;&#25454;&#30340;&#26041;&#27861;&#21644;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#20854;&#22312;&#25991;&#26412;&#27604;&#36739;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21487;&#25193;&#23637;&#30340;&#29992;&#20110;&#25910;&#38598;&#22522;&#20110;&#25991;&#26412;&#23454;&#20307;&#27604;&#36739;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19977;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#21253;&#25324;&#27604;&#36739;&#38382;&#31572;&#12289;&#38382;&#21477;&#29983;&#25104;&#21644;&#25688;&#35201;&#29983;&#25104;&#26041;&#38754;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#22823;&#22823;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#25512;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#26412;&#24037;&#20316;&#36824;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#27604;&#36739;&#25512;&#29702;&#32508;&#21512;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel framework to pre-train language models for enhancing their abilities of comparative reasoning over texts. While recent research has developed models for NLP tasks that require comparative reasoning, they suffer from costly manual data labeling and limited generalizability to different tasks. Our approach involves a scalable method for collecting data for text-based entity comparison, which leverages both structured and unstructured data, and the design of three novel pre-training tasks. Evaluation on a range of downstream tasks including comparative question answering, question generation, and summarization shows that our pre-training framework significantly improves the comparative reasoning abilities of language models, especially under low-resource conditions. This work also releases the first integrated benchmark for comparative reasoning over texts.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2305.14456</link><description>&lt;p&gt;
&#22312;&#31048;&#31095;&#20043;&#21518;&#21917;&#21860;&#37202;&#65311;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Having Beer after Prayer? Measuring Cultural Bias in Large Language Models. (arXiv:2305.14456v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14456
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#25991;&#21270;&#20559;&#35265;&#65311;&#35821;&#35328;&#27169;&#22411;&#31526;&#21512;&#25152;&#26381;&#21153;&#31038;&#21306;&#30340;&#25991;&#21270;&#22240;&#32032;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#34920;&#26126;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#20559;&#35265;&#65292;&#20542;&#21521;&#20110;&#20135;&#29983;&#35199;&#26041;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#32780;&#38750;&#38463;&#25289;&#20271;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20174;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#19978;&#25910;&#38598;&#30340;&#33258;&#28982;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#21644;&#22522;&#20110;&#21487;&#33021;&#24615;&#35780;&#20998;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#36825;&#31181;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#38463;&#25289;&#20271;&#35821;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#35199;&#26041;&#25991;&#21270;&#20559;&#35265;&#65292;&#21253;&#25324;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#12290;&#24403;&#36755;&#20837;&#30340;&#38463;&#25289;&#20271;&#35821;&#21477;&#23376;&#36234;&#25509;&#36817;&#33521;&#35821;&#26102;&#65292;&#27169;&#22411;&#20063;&#26356;&#23481;&#26131;&#34920;&#29616;&#20986;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#20154;&#20204;&#23545;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#35774;&#35745;&#20013;&#24212;&#26356;&#22810;&#32771;&#34385;&#25991;&#21270;&#22240;&#32032;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are language models culturally biased? It is important that language models conform to the cultural aspects of the communities they serve. However, we show in this paper that language models suffer from a significant bias towards Western culture when handling and generating text in Arabic, often preferring, and producing Western-fitting content as opposed to the relevant Arab content. We quantify this bias through a likelihood scoring-based metric using naturally occurring contexts that we collect from online social media. Our experiments reveal that both Arabic monolingual and multilingual models exhibit bias towards Western culture in eight different cultural aspects: person names, food, clothing, location, literature, beverage, religion, and sports. Models also tend to exhibit more bias when prompted with Arabic sentences that are more linguistically aligned with English. These findings raise concerns about the cultural relevance of current language models. Our analyses show that pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;BERT&#12289;GPT-2&#21644;T5&#36825;&#19977;&#31181;&#35821;&#35328;&#27169;&#22411;&#23545;&#25991;&#26412;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#37327;&#21270;fine-tuning&#21518;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#19982;&#39044;&#35757;&#32451;&#34920;&#31034;&#30340;&#24046;&#24322;&#26469;&#25506;&#31350;&#27169;&#22411;&#30340;&#21464;&#21270;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.14453</link><description>&lt;p&gt;
&#20851;&#20110;Transformer-based NLP&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Robustness of Finetuned Transformer-based NLP Models. (arXiv:2305.14453v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;BERT&#12289;GPT-2&#21644;T5&#36825;&#19977;&#31181;&#35821;&#35328;&#27169;&#22411;&#23545;&#25991;&#26412;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#37327;&#21270;fine-tuning&#21518;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#19982;&#39044;&#35757;&#32451;&#34920;&#31034;&#30340;&#24046;&#24322;&#26469;&#25506;&#31350;&#27169;&#22411;&#30340;&#21464;&#21270;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer-based&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22914;BERT&#12289;GPT-2&#21644;T5&#24050;&#32463;&#34987;&#29992;&#20110;&#22823;&#37327;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;fine-tuning&#65292;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22312;fine-tuning&#36807;&#31243;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#21508;&#23618;&#19982;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#30456;&#27604;&#30340;&#21464;&#21270;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#25991;&#26412;&#36755;&#20837;&#30340;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#22914;&#20309;&#65311;&#36825;&#31181;&#40065;&#26834;&#24615;&#26159;&#21542;&#22240;&#27169;&#22411;fine-tuning&#30340;NLP&#20219;&#21153;&#32780;&#24322;&#65311;&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#35821;&#35328;&#27169;&#22411;&#65288;BERT&#12289;GPT-2&#21644;T5&#65289;&#22312;General Language Understanding Evaluation&#65288;GLUE&#65289;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;&#20843;&#31181;&#19981;&#21516;&#25991;&#26412;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#25351;&#26631;&#65288;CKA&#21644;STIR&#65289;&#26469;&#37327;&#21270;fine-tuning&#21518;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#19982;&#39044;&#35757;&#32451;&#34920;&#31034;&#20043;&#38388;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pretrained models like BERT, GPT-2 and T5 have been finetuned for a large number of natural language processing (NLP) tasks, and have been shown to be very effective. However, while finetuning, what changes across layers in these models with respect to pretrained checkpoints is under-studied. Further, how robust are these models to perturbations in input text? Does the robustness vary depending on the NLP task for which the models have been finetuned? While there exists some work on studying robustness of BERT finetuned for a few NLP tasks, there is no rigorous study which compares this robustness across encoder only, decoder only and encoder-decoder models.  In this paper, we study the robustness of three language models (BERT, GPT-2 and T5) with eight different text perturbations on the General Language Understanding Evaluation (GLUE) benchmark. Also, we use two metrics (CKA and STIR) to quantify changes between pretrained and finetuned language model representation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#23545;&#27604;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#26368;&#23567;&#32534;&#36753;&#38382;&#39064;&#26500;&#24314;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23545;&#27604;&#38598;&#21512;&#65292;&#21457;&#29616;&#24191;&#27867;&#20351;&#29992;&#30340;DPR&#22312;&#23545;&#27604;&#38598;&#21512;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#24341;&#20837;&#20102;&#26597;&#35810;&#31471;&#23545;&#27604;&#25439;&#22833;&#21644;&#25968;&#25454;&#22686;&#24378;&#26469;&#25913;&#21892;DPR&#30340;&#35757;&#32451;&#65292;&#24182;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.14441</link><description>&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#31995;&#32479;&#22312;&#26368;&#23567;&#32534;&#36753;&#38382;&#39064;&#19978;&#30340;&#23545;&#27604;&#19968;&#33268;&#24615;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions. (arXiv:2305.14441v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#23545;&#27604;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#26368;&#23567;&#32534;&#36753;&#38382;&#39064;&#26500;&#24314;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23545;&#27604;&#38598;&#21512;&#65292;&#21457;&#29616;&#24191;&#27867;&#20351;&#29992;&#30340;DPR&#22312;&#23545;&#27604;&#38598;&#21512;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#24341;&#20837;&#20102;&#26597;&#35810;&#31471;&#23545;&#27604;&#25439;&#22833;&#21644;&#25968;&#25454;&#22686;&#24378;&#26469;&#25913;&#21892;DPR&#30340;&#35757;&#32451;&#65292;&#24182;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#19968;&#33268;&#24615;&#26159;NLP&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26159;&#27169;&#22411;&#22312;&#23384;&#22312;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#19968;&#33268;&#27491;&#30830;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#38405;&#35835;&#29702;&#35299;&#31561;&#20219;&#21153;&#20013;&#24050;&#32463;&#26377;&#25152;&#30740;&#31350;&#65292;&#20294;&#22312;&#24320;&#25918;&#24335;&#39046;&#22495;&#38382;&#31572;&#65288;OpenQA&#65289;&#20013;&#20173;&#26410;&#34987;&#25506;&#35752;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#38590;&#20197;&#25910;&#38598;&#21040;&#31526;&#21512;&#20107;&#23454;&#35201;&#27714;&#30340;&#25200;&#21160;&#38382;&#39064;&#12290;&#38024;&#23545;&#27492;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20154;&#24037;&#26631;&#27880;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#25910;&#38598;&#20102;&#26368;&#23567;&#32534;&#36753;&#38382;&#39064;&#65292;&#20316;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23545;&#27604;&#38598;&#21512;&#65292;&#29992;&#20110;&#35780;&#20272;OpenQA&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#23545;&#35757;&#32451;&#38598;&#25311;&#21512;&#33391;&#22909;&#19988;&#22312;&#26631;&#20934;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#31454;&#20105;&#21147;&#65292;&#20294;&#24191;&#27867;&#20351;&#29992;&#30340;&#23494;&#38598;&#24335;&#27573;&#33853;&#21462;&#22238;&#22120;&#65288;DPR&#65289;&#22312;&#25105;&#20204;&#30340;&#23545;&#27604;&#38598;&#21512;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26597;&#35810;&#31471;&#23545;&#27604;&#25439;&#22833;&#65292;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#26469;&#25913;&#36827;DPR&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;&#23545;&#27604;&#38598;&#21512;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DPR&#30340;&#23545;&#27604;&#19968;&#33268;&#24615;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrast consistency, the ability of a model to make consistently correct predictions in the presence of perturbations, is an essential aspect in NLP. While studied in tasks such as sentiment analysis and reading comprehension, it remains unexplored in open-domain question answering (OpenQA) due to the difficulty of collecting perturbed questions that satisfy factuality requirements. In this work, we collect minimally edited questions as challenging contrast sets to evaluate OpenQA models. Our collection approach combines both human annotation and large language model generation. We find that the widely used dense passage retriever (DPR) performs poorly on our contrast sets, despite fitting the training set well and performing competitively on standard test sets. To address this issue, we introduce a simple and effective query-side contrastive loss with the aid of data augmentation to improve DPR training. Our experiments on the contrast sets demonstrate that DPR's contrast consistency
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39046;&#22495;&#25193;&#23637;&#30340;ASTE&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#29983;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14434</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#25193;&#23637;&#30340;ASTE&#65306;&#37325;&#26032;&#23457;&#35270;&#24773;&#24863;&#19977;&#20803;&#32452;&#25552;&#21462;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Domain-Expanded ASTE: Rethinking Generalization in Aspect Sentiment Triplet Extraction. (arXiv:2305.14434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39046;&#22495;&#25193;&#23637;&#30340;ASTE&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#29983;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#25193;&#23637;&#30340;Aspect Sentiment Triplet Extraction (ASTE) &#26159;Aspect-Based Sentiment Analysis (ABSA) &#20013;&#30340;&#19968;&#20010;&#23376;&#20219;&#21153;&#65292;&#32771;&#34385;&#27599;&#20010;&#35266;&#28857;&#26415;&#35821;&#12289;&#23427;&#20204;&#34920;&#36798;&#30340;&#24773;&#24863;&#21450;&#30456;&#24212;&#30340;&#26041;&#38754;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20165;&#38480;&#20110;&#20004;&#20010;&#39046;&#22495;&#20869;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39046;&#22495;&#25193;&#23637;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#24212;&#23545;&#39046;&#22495;&#20869;&#12289;&#39046;&#22495;&#38388;&#21644;&#36328;&#39046;&#22495;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#22522;&#20110;&#37202;&#24215;&#21644;&#21270;&#22918;&#21697;&#35780;&#35770;&#65292;&#26631;&#27880;&#20102;&#36229;&#36807;4000&#20010;&#25968;&#25454;&#26679;&#26412;&#26469;&#25903;&#25345;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#20116;&#31181;&#29616;&#26377;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#20294;&#29983;&#25104;&#26041;&#27861;&#22312;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#24456;&#24378;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#20195;&#30721;&#23454;&#29616;&#21644;&#27169;&#22411;&#22343;&#21487;&#22312;https://github.com/DAMO-NLP-SG/domain-expanded-aste &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect Sentiment Triplet Extraction (ASTE) is a subtask of Aspect-Based Sentiment Analysis (ABSA) that considers each opinion term, their expressed sentiment, and the corresponding aspect targets. However, existing methods are limited to the in-domain setting with two domains. Hence, we propose a domain-expanded benchmark to address the in-domain, out-of-domain and cross-domain settings. We support the new benchmark by annotating more than 4000 data samples for two new domains based on hotel and cosmetics reviews. Our analysis of five existing methods shows that while there is a significant gap between in-domain and out-of-domain performance, generative methods have a strong potential for domain generalization. Our datasets, code implementation and models are available at https://github.com/DAMO-NLP-SG/domain-expanded-aste .
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#27010;&#24565;&#23398;&#20064;&#30340;&#22270;&#20687;&#25805;&#20316;&#31995;&#32479;NeuroSIM&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22810;&#36339;&#25351;&#20196;&#22312;&#22810;&#29289;&#20307;&#22330;&#26223;&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#65292;&#21482;&#38656;&#35201;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#25110;&#36229;&#36807;SOTA&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.14410</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#36339;&#25351;&#20196;&#36827;&#34892;&#22270;&#20687;&#25805;&#20316;&#8212;&#8212;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#24369;&#30417;&#30563;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Image Manipulation via Multi-Hop Instructions -- A New Dataset and Weakly-Supervised Neuro-Symbolic Approach. (arXiv:2305.14410v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14410
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#27010;&#24565;&#23398;&#20064;&#30340;&#22270;&#20687;&#25805;&#20316;&#31995;&#32479;NeuroSIM&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22810;&#36339;&#25351;&#20196;&#22312;&#22810;&#29289;&#20307;&#22330;&#26223;&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#65292;&#21482;&#38656;&#35201;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#25110;&#36229;&#36807;SOTA&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#36827;&#34892;&#22270;&#20687;&#25805;&#20316;&#24863;&#20852;&#36259;&#65292;&#36825;&#26159;&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#31243;&#24207;&#20013;&#26377;&#29992;&#30340;&#20219;&#21153;&#65292;&#20294;&#38656;&#35201;&#23545;&#22810;&#27169;&#24577;&#31354;&#38388;&#36827;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#31070;&#32463;&#31526;&#21495;&#27010;&#24565;&#23398;&#20064;(NSCL)&#65292;&#35813;&#26041;&#27861;&#22312;&#35270;&#35273;&#38382;&#31572;(VQA)&#20219;&#21153;&#19978;&#38750;&#24120;&#26377;&#25928;&#65292;&#25193;&#23637;&#20854;&#29992;&#20110;&#22270;&#20687;&#25805;&#20316;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#31216;&#20026;NeuroSIM&#65292;&#21487;&#20197;&#22312;&#22810;&#29289;&#20307;&#22330;&#26223;&#19978;&#25191;&#34892;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#65292;&#21482;&#38656;&#35201;&#20197;VQA&#30340;&#27880;&#37322;&#25968;&#25454;&#24418;&#24335;&#25552;&#20379;&#24369;&#30417;&#30563;&#12290;NeuroSIM&#23558;&#25351;&#20196;&#35299;&#26512;&#25104;&#31526;&#21495;&#31243;&#24207;&#65292;&#22522;&#20110;&#30001;&#23545;&#35937;&#23646;&#24615;&#21644;&#25805;&#20316;&#32452;&#25104;&#30340;&#19987;&#19994;&#39046;&#22495;&#35821;&#35328;(DSL)&#65292;&#25351;&#23548;&#20854;&#25191;&#34892;&#12290;&#25105;&#20204;&#20026;&#36825;&#20010;&#20219;&#21153;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NeuroSIM&#19982;&#20351;&#29992;&#30417;&#30563;&#25968;&#25454;&#36827;&#34892;&#25805;&#20316;&#30340;SOTA&#22522;&#32447;&#30456;&#27604;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#25110;&#36229;&#36807;SOTA&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are interested in image manipulation via natural language text -- a task that is useful for multiple AI applications but requires complex reasoning over multi-modal spaces. We extend recently proposed Neuro Symbolic Concept Learning (NSCL), which has been quite effective for the task of Visual Question Answering (VQA), for the task of image manipulation. Our system referred to as NeuroSIM can perform complex multi-hop reasoning over multi-object scenes and only requires weak supervision in the form of annotated data for VQA. NeuroSIM parses an instruction into a symbolic program, based on a Domain Specific Language (DSL) comprising of object attributes and manipulation operations, that guides its execution. We create a new dataset for the task, and extensive experiments demonstrate that NeuroSIM is highly competitive with or beats SOTA baselines that make use of supervised data for manipulation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.14387</link><description>&lt;p&gt;
AlpacaFarm: &#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#27169;&#25311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22240;&#20854;&#33391;&#22909;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#32780;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#24320;&#21457;&#36825;&#20123;LLMs&#38656;&#35201;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#35757;&#32451;&#30340;&#22797;&#26434;&#19988;&#23578;&#19981;&#26126;&#30830;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#23558;&#27492;&#25351;&#20196;&#36319;&#38543;&#36807;&#31243;&#22797;&#21046;&#21644;&#29702;&#35299;&#38754;&#20020;&#19977;&#22823;&#25361;&#25112;&#65306; &#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#65292;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;AlpacaFarm&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#20010;&#20302;&#25104;&#26412;&#30340;&#27169;&#25311;&#22120;&#65292;&#21487;&#29992;&#20110;&#20174;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#20854;&#25104;&#26412;&#27604;&#20247;&#21253;&#24037;&#20316;&#32773;&#20415;&#23452;45&#20493;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#21453;&#39304;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#30495;&#23454;&#19990;&#30028;&#20132;&#20114;&#20013;&#33719;&#24471;&#30340;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#39564;&#35777;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20026;&#20960;&#31181;&#20174;&#37197;&#23545;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;PPO&#65292;best-of-n&#65292;expert iteration&#31561;&#65289;&#25552;&#20379;&#20102;&#21442;&#32771;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GPT-3&#29983;&#25104;&#23450;&#21046;&#21270;&#32451;&#20064;&#65292;&#25945;&#25480;&#25968;&#23398;&#24212;&#29992;&#39064;&#35299;&#20915;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#23398;&#29983;&#27169;&#22411;&#30340;&#24369;&#28857;&#24182;&#20197;&#25945;&#32946;&#31185;&#23398;&#21407;&#29702;&#20026;&#22522;&#30784;&#36827;&#34892;&#23450;&#21046;&#21270;&#30340;&#23398;&#20064;&#20307;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14386</link><description>&lt;p&gt;
&#35753;GPT&#25104;&#20026;&#25968;&#23398;&#25945;&#24072;&#65306;&#20351;&#29992;&#23450;&#21046;&#21270;&#32451;&#20064;&#29983;&#25104;&#25945;&#25480;&#25968;&#23398;&#24212;&#29992;&#39064;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation. (arXiv:2305.14386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GPT-3&#29983;&#25104;&#23450;&#21046;&#21270;&#32451;&#20064;&#65292;&#25945;&#25480;&#25968;&#23398;&#24212;&#29992;&#39064;&#35299;&#20915;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#23398;&#29983;&#27169;&#22411;&#30340;&#24369;&#28857;&#24182;&#20197;&#25945;&#32946;&#31185;&#23398;&#21407;&#29702;&#20026;&#22522;&#30784;&#36827;&#34892;&#23450;&#21046;&#21270;&#30340;&#23398;&#20064;&#20307;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#25968;&#23398;&#24212;&#29992;&#39064;&#35299;&#20915;&#33021;&#21147;&#25552;&#28860;&#20026;&#26356;&#23567;&#12289;&#26356;&#39640;&#25928;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#32771;&#34385;&#23398;&#29983;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#36890;&#36807;&#29983;&#25104;&#19982;&#25945;&#32946;&#31185;&#23398;&#21407;&#29702;&#65288;&#22914;&#30693;&#35782;&#36319;&#36394;&#21644;&#20010;&#24615;&#21270;&#23398;&#20064;&#65289;&#23545;&#40784;&#30340;&#26377;&#38024;&#23545;&#24615;&#30340;&#32451;&#20064;&#26469;&#20419;&#36827;&#23450;&#21046;&#21270;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;&#25105;&#20204;&#35753;GPT-3&#25104;&#20026;&#25968;&#23398;&#25945;&#24072;&#65292;&#36845;&#20195;&#25191;&#34892;&#20004;&#20010;&#27493;&#39588;&#65306;1&#65289;&#22312;&#30001;GPT&#29983;&#25104;&#30340;&#32451;&#20064;&#20876;&#19978;&#35780;&#20272;&#23398;&#29983;&#27169;&#22411;&#30340;&#24403;&#21069;&#23398;&#20064;&#29366;&#20917;&#65307;2&#65289;&#20351;&#29992;GPT-3&#29983;&#25104;&#30340;&#23450;&#21046;&#21270;&#32451;&#20064;&#26679;&#26412;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#20013;&#27604;LLMs&#65288;&#20363;&#22914;&#65292;GPT-3&#21644;PaLM&#65289;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#21442;&#25968;&#25968;&#37327;&#26126;&#26174;&#36739;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#26041;&#27861;&#20013;&#21508;&#20010;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#20197;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for distilling math word problem solving capabilities from large language models (LLMs) into smaller, more efficient student models. Our approach is designed to consider the student model's weaknesses and foster a tailored learning experience by generating targeted exercises aligned with educational science principles, such as knowledge tracing and personalized learning. Concretely, we let GPT-3 be a math tutor and run two steps iteratively: 1) assessing the student model's current learning status on a GPT-generated exercise book, and 2) improving the student model by training it with tailored exercise samples generated by GPT-3. Experimental results reveal that our approach outperforms LLMs (e.g., GPT-3 and PaLM) in accuracy across three distinct benchmarks while employing significantly fewer parameters. Furthermore, we provide a comprehensive analysis of the various components within our methodology to substantiate their efficacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#32858;&#21512;&#22836;&#27880;&#24847;&#21147;(Grouped Head Attention)&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#32452;&#32422;&#26463;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#27880;&#24847;&#21147;&#22836;&#36827;&#34892;&#20998;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#32452;&#19987;&#27880;&#20110;&#19968;&#20010;&#37325;&#35201;&#32780;&#29420;&#29305;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;MHA&#30340;&#20887;&#20313;&#24615;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MHA&#65292;&#36827;&#32780;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.14380</link><description>&lt;p&gt;
&#23547;&#25214;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#25903;&#26609;
&lt;/p&gt;
&lt;p&gt;
Finding the Pillars of Strength for Multi-Head Attention. (arXiv:2305.14380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#32858;&#21512;&#22836;&#27880;&#24847;&#21147;(Grouped Head Attention)&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#32452;&#32422;&#26463;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#27880;&#24847;&#21147;&#22836;&#36827;&#34892;&#20998;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#32452;&#19987;&#27880;&#20110;&#19968;&#20010;&#37325;&#35201;&#32780;&#29420;&#29305;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;MHA&#30340;&#20887;&#20313;&#24615;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MHA&#65292;&#36827;&#32780;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;(Multi-Head Attention, MHA)&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#20363;&#22914;&#20887;&#20313;&#24615;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MHA&#30340;&#22836;&#26368;&#21021;&#35774;&#35745;&#20026;&#20174;&#19981;&#21516;&#30340;&#34920;&#24449;&#23376;&#31354;&#38388;&#20013;&#20851;&#27880;&#20449;&#24687;&#65292;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#19968;&#20123;&#27880;&#24847;&#21147;&#22836;&#21487;&#33021;&#23398;&#20064;&#31867;&#20284;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20462;&#21098;&#26469;&#25552;&#39640;&#25928;&#29575;&#32780;&#19981;&#20250;&#25439;&#23475;&#24615;&#33021;&#12290;&#21463;&#26368;&#23567;&#20887;&#20313;&#29305;&#24449;&#36873;&#25321;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20551;&#35774;&#32858;&#28966;&#20110;&#26368;&#20855;&#20195;&#34920;&#24615;&#21644;&#29420;&#29305;&#24615;&#30340;&#29305;&#24449;&#65292;&#24182;&#26368;&#23567;&#21270;&#36164;&#28304;&#28040;&#32791;&#65292;&#21487;&#20197;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MHA&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32858;&#21512;&#22836;&#27880;&#24847;&#21147;(Grouped Head Attention)&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#32452;&#32422;&#26463;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#27880;&#24847;&#21147;&#22836;&#36827;&#34892;&#20998;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#32452;&#19987;&#27880;&#20110;&#19968;&#20010;&#37325;&#35201;&#32780;&#29420;&#29305;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#25237;&#31080;&#20445;&#30041;&#31243;&#24207;(Voting-to-Stay)&#65292;&#20197;&#21024;&#38500;&#20887;&#20313;&#22836;&#65292;&#20174;&#32780;&#23454;&#29616;&#20855;&#26377;&#26356;&#36731;&#37327;&#32423;&#26435;&#37325;&#30340;&#36716;&#25442;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#30693;&#21517;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#25552;&#20379;&#20102;&#25903;&#25345;&#24615;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have revealed some issues of Multi-Head Attention (MHA), e.g., redundancy and over-parameterization. Specifically, the heads of MHA were originally designed to attend to information from different representation subspaces, whereas prior studies found that some attention heads likely learn similar features and can be pruned without harming performance. Inspired by the minimum-redundancy feature selection, we assume that focusing on the most representative and distinctive features with minimum resources can mitigate the above issues and lead to more effective and efficient MHAs. In particular, we propose Grouped Head Attention, trained with a self-supervised group constraint that group attention heads, where each group focuses on an essential but distinctive feature subset. We additionally propose a Voting-to-Stay procedure to remove redundant heads, thus achieving a transformer with lighter weights. Moreover, our method achieves significant performance gains on three well
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#38170;&#28857;&#39044;&#27979;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;&#38142;&#25509;&#30446;&#26631;&#32593;&#39029;&#30340;&#29305;&#23450;&#37096;&#20998;&#36827;&#34892;&#35782;&#21035;&#65292;&#24110;&#21161;&#35835;&#32773;&#26356;&#26377;&#25928;&#22320;&#22312;&#38142;&#25509;&#32593;&#39029;&#20013;&#25214;&#21040;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.14337</link><description>&lt;p&gt;
Anchor Prediction: &#33258;&#21160;&#23436;&#21892;&#20114;&#32852;&#32593;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
Anchor Prediction: Automatic Refinement of Internet Links. (arXiv:2305.14337v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#38170;&#28857;&#39044;&#27979;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;&#38142;&#25509;&#30446;&#26631;&#32593;&#39029;&#30340;&#29305;&#23450;&#37096;&#20998;&#36827;&#34892;&#35782;&#21035;&#65292;&#24110;&#21161;&#35835;&#32773;&#26356;&#26377;&#25928;&#22320;&#22312;&#38142;&#25509;&#32593;&#39029;&#20013;&#25214;&#21040;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#38142;&#25509;&#26159;&#36890;&#36807;&#25552;&#20379;&#20415;&#25463;&#30340;&#35775;&#38382;&#30456;&#20851;&#20449;&#24687;&#65292;&#24110;&#21161;&#29992;&#25143;&#28145;&#20837;&#20102;&#35299;&#19968;&#20010;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#38142;&#25509;&#37117;&#26159;&#26080;&#38170;&#28857;&#30340; - &#23427;&#20204;&#23558;&#30446;&#26631;&#32593;&#39029;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#38142;&#25509;&#65292;&#35835;&#32773;&#21487;&#33021;&#20250;&#33457;&#36153;&#22823;&#37327;&#30340;&#31934;&#21147;&#23450;&#20301;&#30446;&#26631;&#32593;&#39029;&#20013;&#20016;&#23500;&#20182;&#20204;&#29702;&#35299;&#38142;&#25509;&#28304;&#19978;&#19979;&#25991;&#30340;&#29305;&#23450;&#37096;&#20998;&#12290;&#20026;&#20102;&#24110;&#21161;&#35835;&#32773;&#26377;&#25928;&#22320;&#22312;&#38142;&#25509;&#30340;&#32593;&#39029;&#20013;&#25214;&#21040;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38170;&#28857;&#39044;&#27979;&#30340;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#30830;&#23450;&#19982;&#28304;&#38142;&#25509;&#19978;&#19979;&#25991;&#26368;&#30456;&#20851;&#30340;&#30446;&#26631;&#32593;&#39029;&#30340;&#29305;&#23450;&#37096;&#20998;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#20316;&#32773;&#38170;&#28857;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324; 34K &#20010;&#33258;&#28982;&#20986;&#29616;&#30340;&#24102;&#38170;&#28857;&#38142;&#25509;&#65292;&#21453;&#26144;&#20102;&#28304;&#25991;&#31456;&#20316;&#32773;&#30340;&#30456;&#20851;&#21028;&#26029;&#12290;&#20026;&#20102;&#27169;&#25311;&#35835;&#32773;&#30456;&#20851;&#21028;&#26029;&#65292;&#25105;&#20204;&#27880;&#37322;&#24182;&#21457;&#24067;&#20102;&#35835;&#32773;&#38170;&#28857;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#35835;&#32773;&#21457;&#29616;&#26377;&#29992;&#30340;&#38170;&#28857;&#30340;&#35780;&#20272;&#38598;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#26377;&#25928;&#30340;&#38170;&#28857;&#39044;&#27979;&#36890;&#24120;&#38656;&#35201;&#32852;&#21512;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Internet links enable users to deepen their understanding of a topic by providing convenient access to related information. However, the majority of links are unanchored -- they link to a target webpage as a whole, and readers may expend considerable effort localizing the specific parts of the target webpage that enrich their understanding of the link's source context. To help readers effectively find information in linked webpages, we introduce the task of anchor prediction, where the goal is to identify the specific part of the linked target webpage that is most related to the source linking context. We release the AuthorAnchors dataset, a collection of 34K naturally-occurring anchored links, which reflect relevance judgments by the authors of the source article. To model reader relevance judgments, we annotate and release ReaderAnchors, an evaluation set of anchors that readers find useful. Our analysis shows that effective anchor prediction often requires jointly reasoning over len
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#20154;&#21475;&#25968;&#25454;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#30340;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;GPT-3.5&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#21516;&#65292;&#22312;&#21333;&#35789;&#24847;&#20041;&#25512;&#26029;&#26041;&#38754;&#27169;&#25311;&#20102;&#20856;&#22411;6-9&#23681;&#20799;&#31461;&#30340;&#33021;&#21147;&#65292;&#22312;&#35760;&#24518;&#26041;&#38754;&#21017;&#34920;&#29616;&#20248;&#20110;&#20856;&#22411;21&#23681;&#24180;&#36731;&#20154;&#12290;</title><link>http://arxiv.org/abs/2305.14195</link><description>&lt;p&gt;
GPT&#31350;&#31455;&#26377;&#22810;&#32769;&#65311;HumBEL&#26694;&#26550;&#36890;&#36807;&#20154;&#32676;&#25968;&#25454;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How Old is GPT?: The HumBEL Framework for Evaluating Language Models using Human Demographic Data. (arXiv:2305.14195v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#20154;&#21475;&#25968;&#25454;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#30340;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;GPT-3.5&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#21516;&#65292;&#22312;&#21333;&#35789;&#24847;&#20041;&#25512;&#26029;&#26041;&#38754;&#27169;&#25311;&#20102;&#20856;&#22411;6-9&#23681;&#20799;&#31461;&#30340;&#33021;&#21147;&#65292;&#22312;&#35760;&#24518;&#26041;&#38754;&#21017;&#34920;&#29616;&#20248;&#20110;&#20856;&#22411;21&#23681;&#24180;&#36731;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#28982;&#32780;&#30446;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#24182;&#26410;&#32771;&#34385;&#27169;&#22411;&#30340;&#35821;&#35328;&#20351;&#29992;&#19982;&#29305;&#23450;&#20154;&#32676;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#36825;&#19968;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#27979;&#37327;&#21644;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#33021;&#21147;&#19982;&#20154;&#31867;&#23376;&#32676;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#20511;&#21161;&#35821;&#35328;&#30149;&#29702;&#23398;&#30340;&#20020;&#24202;&#25216;&#26415;&#65292;&#35813;&#23398;&#31185;&#24050;&#32463;&#24314;&#31435;&#20102;&#19981;&#21516;&#65288;&#20154;&#31867;&#65289;&#24180;&#40836;&#38454;&#27573;&#30340;&#35821;&#35328;&#33021;&#21147;&#21457;&#23637;&#35268;&#33539;&#65292;&#23545;&#25216;&#33021;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#19982;&#39046;&#22495;&#19987;&#23478;&#65288;&#21363;&#25345;&#26377;&#20020;&#24202;&#35768;&#21487;&#35777;&#30340;&#35821;&#35328;&#30149;&#29702;&#23398;&#23478;&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#30340;&#35780;&#20272;&#25216;&#26415;&#20197;&#23454;&#29616;&#35268;&#27169;&#21270;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-3.5&#30340;&#33021;&#21147;&#22240;&#20219;&#21153;&#32780;&#24322;&#65292;&#22312;&#21333;&#35789;&#24847;&#20041;&#25512;&#26029;&#26041;&#38754;&#27169;&#25311;&#20102;&#20856;&#22411;6-9&#23681;&#20799;&#31461;&#30340;&#33021;&#21147;&#65292;&#22312;&#35760;&#24518;&#26041;&#38754;&#21017;&#34920;&#29616;&#20248;&#20110;&#20856;&#22411;21&#23681;&#24180;&#36731;&#20154;&#12290;GPT-3.5&#65288;InstructGPT&#65289;&#22312;&#31038;&#20132;&#20132;&#20114;&#20219;&#21153;&#20013;&#20063;&#23384;&#22312;&#19968;&#23450;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large pre-trained language models (LMs) find greater use across NLP, existing evaluation protocols do not consider how LM language use aligns with particular human demographic groups, which can be an important consideration in conversational AI applications. To remedy this gap, we consider how LM language skills can be measured and compared to human sub-populations. We suggest clinical techniques from Speech Language Pathology, which has well-established norms for acquisition of language skills, organized by (human) age. We conduct evaluation with a domain expert (i.e., a clinically licensed speech language pathologist), and also propose automated techniques to substitute clinical evaluation at scale. We find LM capability varies widely depending on task with GPT-3.5 mimicking the ability of a typical 6-9 year old at tasks requiring inference about word meanings and simultaneously outperforming a typical 21 year old at memorization. GPT-3.5 (InstructGPT) also has trouble with soc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#25968;&#25454;&#38598;CoLAC&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#20302;&#20110;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#36328;&#35821;&#35328;&#36716;&#31227;&#21487;&#34892;&#65292;&#24182;&#21487;&#20197;&#36861;&#28335;&#21040;&#39044;&#35757;&#32451;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2305.14091</link><description>&lt;p&gt;
&#37325;&#28201;&#25509;&#21463;&#24615;&#21028;&#26029;
&lt;/p&gt;
&lt;p&gt;
Revisiting Acceptability Judgements. (arXiv:2305.14091v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#25968;&#25454;&#38598;CoLAC&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#20302;&#20110;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#36328;&#35821;&#35328;&#36716;&#31227;&#21487;&#34892;&#65292;&#24182;&#21487;&#20197;&#36861;&#28335;&#21040;&#39044;&#35757;&#32451;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33267;NLP&#31038;&#21306;&#26368;&#21518;&#19968;&#27425;&#20851;&#27880;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#24050;&#32463;&#36807;&#21435;&#22810;&#24180;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#37325;&#28201;&#36825;&#20010;&#35805;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; CoLAC-&#20013;&#25991;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#30001;&#27597;&#35821;&#35762;&#32773;&#39564;&#35777;&#24182;&#24102;&#26377;&#20004;&#32452;&#26631;&#31614;&#30340;&#22823;&#35268;&#27169;&#38750;&#33521;&#35821;&#21487;&#25509;&#21463;&#24615;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#26368;&#22823;&#30340;InstructGPT&#27169;&#22411;&#22312;CoLAC&#19978;&#20063;&#21482;&#33021;&#34920;&#29616;&#38543;&#26426;&#27700;&#24179;&#65292;&#32780;ChatGPT&#30340;&#24615;&#33021;&#65288;48.30 MCC&#65289;&#20063;&#36828;&#20302;&#20110;&#30417;&#30563;&#27169;&#22411;&#65288;59.03 MCC&#65289;&#21644;&#20154;&#31867;&#65288;65.11 MCC&#65289;&#12290;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#23454;&#39564;&#21644;&#32454;&#31890;&#24230;&#30340;&#35821;&#35328;&#20998;&#26512;&#65292;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#30693;&#35782;&#21487;&#20197;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#20043;&#38388;&#36716;&#31227;&#65292;&#32780;&#19988;&#21487;&#20197;&#36861;&#28335;&#21040;&#39044;&#35757;&#32451;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Years have passed since the NLP community has last focused on linguistic acceptability. In this work, we revisit this topic in the context of large language models. We introduce CoLAC - Corpus of Linguistic Acceptability in Chinese, the first large-scale non-English acceptability dataset that is verified by native speakers and comes with two sets of labels. Our experiments show that even the largest InstructGPT model performs only at chance level on CoLAC, while ChatGPT's performance (48.30 MCC) is also way below supervised models (59.03 MCC) and human (65.11 MCC). Through cross-lingual transfer experiments and fine-grained linguistic analysis, we demonstrate for the first time that knowledge of linguistic acceptability can be transferred across typologically distinct languages, as well as be traced back to pre-training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#31449;&#24335;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#21516;&#26102;&#35757;&#32451;&#39640;&#23481;&#37327;&#21644;&#20302;&#23481;&#37327;&#27169;&#22411;&#65292;&#30456;&#36739;&#20110;&#30693;&#35782;&#33976;&#39311;&#26356;&#24555;&#36895;&#12289;&#26356;&#39640;&#25928;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#35821;&#35328;&#32763;&#35793;&#20219;&#21153;&#19978;&#20248;&#20110;&#20302;&#23481;&#37327;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#22312;&#39640;&#23481;&#37327;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#27169;&#22411;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14066</link><description>&lt;p&gt;
&#22810;&#23481;&#37327;&#27169;&#22411;&#30340;&#19968;&#31449;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
One-stop Training of Multiple Capacity Models. (arXiv:2305.14066v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#31449;&#24335;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#21516;&#26102;&#35757;&#32451;&#39640;&#23481;&#37327;&#21644;&#20302;&#23481;&#37327;&#27169;&#22411;&#65292;&#30456;&#36739;&#20110;&#30693;&#35782;&#33976;&#39311;&#26356;&#24555;&#36895;&#12289;&#26356;&#39640;&#25928;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#35821;&#35328;&#32763;&#35793;&#20219;&#21153;&#19978;&#20248;&#20110;&#20302;&#23481;&#37327;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#22312;&#39640;&#23481;&#37327;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#27169;&#22411;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#23481;&#37327;&#19981;&#21516;&#30340;&#27169;&#22411;&#21487;&#29992;&#20110;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#37096;&#32626;&#27169;&#22411;&#65292;&#39640;&#23481;&#37327;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20302;&#23481;&#37327;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#31449;&#24335;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#21516;&#26102;&#35757;&#32451;&#39640;&#23481;&#37327;&#21644;&#20302;&#23481;&#37327;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#32452;&#21512;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21644;&#19968;&#20010;&#21517;&#20026;Two-Stage Joint-Training (TSJT)&#30340;&#32852;&#21512;&#35757;&#32451;&#31639;&#27861;&#12290;&#19982;&#30693;&#35782;&#33976;&#39311;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#25972;&#21512;&#20102;&#19981;&#21516;&#23481;&#37327;&#27169;&#22411;&#30340;&#30417;&#30563;&#65292;&#23548;&#33268;&#26356;&#24555;&#36895;&#12289;&#26356;&#39640;&#25928;&#30340;&#25910;&#25947;&#12290;&#22312;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;WMT10&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20302;&#23481;&#37327;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#22312;&#39640;&#23481;&#37327;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20998;&#26512;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#25552;&#39640;&#20102;&#27169;&#22411;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training models with varying capacities can be advantageous for deploying them in different scenarios. While high-capacity models offer better performance, low-capacity models require fewer computing resources for training and inference. In this work, we propose a novel one-stop training framework to jointly train high-capacity and low-capactiy models. This framework consists of two composite model architectures and a joint training algorithm called Two-Stage Joint-Training (TSJT). Unlike knowledge distillation, where multiple capacity models are trained from scratch separately, our approach integrates supervisions from different capacity models simultaneously, leading to faster and more efficient convergence. Extensive experiments on the multilingual machine translation benchmark WMT10 show that our method outperforms low-capacity baseline models and achieves comparable or better performance on high-capacity models. Notably, the analysis demonstrates that our method significantly infl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13971</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#28789;&#27963;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#23569;&#37327;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20449;&#24687;&#25552;&#21462;&#25152;&#38656;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26102;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;LLM&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20542;&#21521;&#20110;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#32780;&#19981;&#26159;&#36981;&#24490;&#29305;&#23450;&#35821;&#27861;&#30340;&#31934;&#30830;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35299;&#30721;&#27493;&#39588;&#20013;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#26469;&#20016;&#23500;&#27169;&#22411;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#31526;&#21512;&#35821;&#27861;&#20135;&#29983;&#35268;&#21017;&#30340;&#26377;&#25928;&#20196;&#29260;&#33021;&#34987;&#32771;&#34385;&#21040;&#12290;&#36825;&#26679;&#23601;&#24378;&#21046;&#21482;&#20135;&#29983;&#26377;&#25928;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#21644;&#28789;&#27963;&#65292;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;(CFG)&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#32422;&#26463;beam&#25628;&#32034;&#23454;&#29616;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;NLP&#20219;&#21153;&#30340;&#36755;&#20986;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#24418;&#24335;&#35821;&#35328;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30452;&#25509;&#20351;&#29992;&#12290;&#23545;&#20110;&#36755;&#20986;&#31354;&#38388;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20837;&#30340;CFG&#65292;&#26681;&#25454;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#26356;&#26032;&#20135;&#29983;&#35268;&#21017;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; LogicLLM&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#30417;&#30563;&#21518;&#35757;&#32451;&#26469;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#22312;&#24120;&#35265;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13718</link><description>&lt;p&gt;
LogicLLM&#65306;&#25506;&#32034;&#33258;&#30417;&#30563;&#36923;&#36753;&#22686;&#24378;&#35757;&#32451;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models. (arXiv:2305.13718v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; LogicLLM&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#30417;&#30563;&#21518;&#35757;&#32451;&#26469;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#22312;&#24120;&#35265;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#29616;&#26377;&#21162;&#21147;&#20027;&#35201;&#20381;&#36182;&#20110;&#26377;&#30417;&#30563;&#24494;&#35843;&#65292;&#36825;&#38459;&#30861;&#20102;&#23558;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#30340;&#39046;&#22495;&#21644;/&#25110;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21457;&#23637;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;&#20016;&#23500;&#30340;&#30693;&#35782;&#21387;&#32553;&#20026;&#21333;&#20010;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;LLMs &#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#24182;&#27809;&#26377;&#34920;&#29616;&#20986;&#33021;&#21147;&#12290;LLMs &#22312;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#36828;&#36828;&#33853;&#21518;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#33258;&#30417;&#30563;&#21518;&#35757;&#32451;&#26469;&#25506;&#32034;&#34701;&#21512;&#36923;&#36753;&#30693;&#35782;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#28608;&#27963;&#23427;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;LogicLLM&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;MERIt &#30340;&#33258;&#22238;&#24402;&#30446;&#26631;&#21464;&#20307;&#65292;&#24182;&#23558;&#20854;&#19982;&#20004;&#20010;LLM&#31995;&#21015;FLAN-T5&#21644;LLaMA&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#21442;&#25968;&#22823;&#23567;&#33539;&#22260;&#20174;30&#20159;&#21040;130&#20159;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24120;&#29992;&#25512;&#29702;&#31574;&#30053;&#19978;&#19982;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#36828;&#36828;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing efforts to improve logical reasoning ability of language models have predominantly relied on supervised fine-tuning, hindering generalization to new domains and/or tasks. The development of Large Langauge Models (LLMs) has demonstrated the capacity of compressing abundant knowledge into a single proxy, enabling them to tackle multiple tasks effectively. Our preliminary experiments, nevertheless, show that LLMs do not show capability on logical reasoning. The performance of LLMs on logical reasoning benchmarks is far behind the existing state-of-the-art baselines. In this paper, we make the first attempt to investigate the feasibility of incorporating logical knowledge through self-supervised post-training, and activating it via in-context learning, which we termed as LogicLLM. Specifically, we devise an auto-regressive objective variant of MERIt and integrate it with two LLM series, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to 13 billion. The results 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#24815;&#29992;&#34920;&#36798;&#24335;&#26469;&#25552;&#39640;&#23398;&#29983;&#20889;&#20316;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13637</link><description>&lt;p&gt;
IdEALS: &#25552;&#39640;&#35821;&#35328;&#25216;&#33021;&#30340;&#24815;&#29992;&#34920;&#36798;
&lt;/p&gt;
&lt;p&gt;
IdEALS: Idiomatic Expressions for Advancement of Language Skills. (arXiv:2305.13637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13637
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#24815;&#29992;&#34920;&#36798;&#24335;&#26469;&#25552;&#39640;&#23398;&#29983;&#20889;&#20316;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22312;&#21457;&#23637;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#65288;GEC&#65289;&#26041;&#27861;&#26041;&#38754;&#26377;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20851;&#27880;&#35789;&#35821;&#36873;&#25321;&#26041;&#38754;&#30340;&#25913;&#36827;&#26174;&#33879;&#32570;&#20047;&#65292;&#24182;&#36890;&#36807;&#29992;&#39640;&#32423;&#34920;&#36798;&#24335;&#26367;&#25442;&#30701;&#35821;&#26469;&#22686;&#24378;&#35821;&#21477;&#34920;&#36798;&#33021;&#21147;&#26159;&#19968;&#31181;&#40092;&#20026;&#20154;&#30693;&#30340;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#36825;&#20010;&#39046;&#22495;&#65292;&#20171;&#32461;&#20102;&#25105;&#20204;&#23545;&#23558;&#24815;&#29992;&#34920;&#36798;&#24335;&#29992;&#20110;&#23398;&#29983;&#20889;&#20316;&#20013;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#26041;&#20415;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#31574;&#21010;&#20102;&#24191;&#27867;&#30340;&#35757;&#32451;&#38598;&#21644;&#19987;&#23478;&#27880;&#37322;&#30340;&#27979;&#35797;&#38598;&#65292;&#24182;&#35780;&#20272;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#20154;&#31867;&#19987;&#23478;&#30340;&#34920;&#29616;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although significant progress has been made in developing methods for Grammatical Error Correction (GEC), addressing word choice improvements has been notably lacking and enhancing sentence expressivity by replacing phrases with advanced expressions is an understudied aspect. In this paper, we focus on this area and present our investigation into the task of incorporating the usage of idiomatic expressions in student writing. To facilitate our study, we curate extensive training sets and expert-annotated testing sets using real-world data and evaluate various approaches and compare their performance against human experts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32763;&#35793;&#21644;&#27880;&#37322;&#34701;&#21512;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25913;&#36827;&#20302;&#36164;&#28304;&#35821;&#35328;&#25991;&#26412;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#36890;&#36807;TransFusion&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#24378;&#22823;&#30340;&#39044;&#27979;&#65292;&#19988;&#22312;&#20004;&#20010;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19968;&#33268;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2305.13582</link><description>&lt;p&gt;
&#36890;&#36807;&#32763;&#35793;&#21644;&#27880;&#37322;&#34701;&#21512;&#25913;&#36827;&#20302;&#36164;&#28304;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Better Low-Resource Entity Recognition Through Translation and Annotation Fusion. (arXiv:2305.13582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32763;&#35793;&#21644;&#27880;&#37322;&#34701;&#21512;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25913;&#36827;&#20302;&#36164;&#28304;&#35821;&#35328;&#25991;&#26412;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#36890;&#36807;TransFusion&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#24378;&#22823;&#30340;&#39044;&#27979;&#65292;&#19988;&#22312;&#20004;&#20010;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19968;&#33268;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#23454;&#29616;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#36716;&#31227;&#33267;&#20302;&#36164;&#28304;&#35821;&#35328;&#26102;&#65292;&#36890;&#24120;&#34920;&#29616;&#20986;&#24615;&#33021;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26410;&#34987;&#20805;&#20998;&#35757;&#32451;&#25110;&#26410;&#21253;&#21547;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#35821;&#35328;&#12290;&#21463;&#36825;&#20123;&#27169;&#22411;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#34920;&#29616;&#20248;&#31168;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32763;&#35793;&#21644;&#34701;&#21512;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#20302;&#36164;&#28304;&#35821;&#35328;&#25991;&#26412;&#32763;&#35793;&#25104;&#39640;&#36164;&#28304;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#65292;&#28982;&#21518;&#23558;&#27880;&#37322;&#34701;&#21512;&#22238;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TransFusion&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#35757;&#32451;&#29992;&#20110;&#34701;&#21512;&#26469;&#33258;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#20197;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#36827;&#34892;&#24378;&#22823;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#25968;&#25454;&#38598;MasakhaNER2.0&#21644;LORELEI NER&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#36328;&#35821;&#35328;NER&#22522;&#32447;&#30340;&#19968;&#33268;&#20248;&#31168;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained multilingual language models have enabled significant advancements in cross-lingual transfer. However, these models often exhibit a performance disparity when transferring from high-resource languages to low-resource languages, especially for languages that are underrepresented or not in the pre-training data. Motivated by the superior performance of these models on high-resource languages compared to low-resource languages, we introduce a Translation-and-fusion framework, which translates low-resource language text into a high-resource language for annotation using fully supervised models before fusing the annotations back into the low-resource language. Based on this framework, we present TransFusion, a model trained to fuse predictions from a high-resource language to make robust predictions on low-resource languages. We evaluate our methods on two low-resource named entity recognition (NER) datasets, MasakhaNER2.0 and LORELEI NER, covering 25 languages, and show consist
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#30340;&#26694;&#26550;&#65292;&#24182;&#21253;&#25324;&#20102;&#29420;&#29305;&#30340;&#23376;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25991;&#26412;&#65292;&#22270;&#20687;&#65292;&#38899;&#39057;&#21644;&#35270;&#39057;&#36825;&#22235;&#31181;&#27169;&#24577;&#30340;&#29616;&#23454;&#24212;&#29992;&#12290;&#32426;&#24405;&#20102;&#30456;&#20851;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.13507</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#65306;&#19968;&#20221;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multimodal Automated Fact-Checking: A Survey. (arXiv:2305.13507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#30340;&#26694;&#26550;&#65292;&#24182;&#21253;&#25324;&#20102;&#29420;&#29305;&#30340;&#23376;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25991;&#26412;&#65292;&#22270;&#20687;&#65292;&#38899;&#39057;&#21644;&#35270;&#39057;&#36825;&#22235;&#31181;&#27169;&#24577;&#30340;&#29616;&#23454;&#24212;&#29992;&#12290;&#32426;&#24405;&#20102;&#30456;&#20851;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38169;&#35823;&#20449;&#24687;&#65292;&#21363;&#20107;&#23454;&#19978;&#19981;&#27491;&#30830;&#30340;&#20449;&#24687;&#65292;&#36890;&#24120;&#20197;&#22810;&#31181;&#24418;&#24335;&#20256;&#36798;&#65292;&#20363;&#22914;&#24102;&#26377;&#26631;&#39064;&#30340;&#22270;&#20687;&#12290; &#23427;&#34987;&#20154;&#20204;&#35270;&#20026;&#26356;&#21487;&#20449;&#65292;&#27604;&#20854;&#20165;&#38480;&#20110;&#25991;&#26412;&#30340;&#23545;&#24212;&#29289;&#25193;&#25955;&#36895;&#24230;&#26356;&#24555;&#65292;&#33539;&#22260;&#26356;&#24191;&#12290; &#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#28041;&#21450;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#65288;AFC&#65289;&#65292;&#20294;&#20197;&#24448;&#30340;&#35843;&#26597;&#20027;&#35201;&#38598;&#20013;&#22312;&#25991;&#26412;&#35823;&#23548;&#26041;&#38754;&#12290; &#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;&#22810;&#27169;&#24577;&#35823;&#23548;&#29420;&#29305;&#23376;&#20219;&#21153;&#22312;&#20869;&#30340;AFC&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19978;&#35752;&#35770;&#20102;&#19981;&#21516;&#31038;&#21306;&#25152;&#21457;&#23637;&#30340;&#30456;&#20851;&#26415;&#35821;&#12290; &#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;&#20013;&#23384;&#22312;&#30340;&#22235;&#31181;&#27169;&#24577;&#65306;&#25991;&#26412;&#65292;&#22270;&#20687;&#65292;&#38899;&#39057;&#21644;&#35270;&#39057;&#12290; &#25105;&#20204;&#35843;&#26597;&#20102;&#22522;&#20934;&#21644;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation, i.e. factually incorrect information, is often conveyed in multiple modalities, e.g. an image accompanied by a caption. It is perceived as more credible by humans, and spreads faster and wider than its text-only counterparts. While an increasing body of research investigates automated fact-checking (AFC), previous surveys mostly focus on textual misinformation. In this survey, we conceptualise a framework for AFC including subtasks unique to multimodal misinformation. Furthermore, we discuss related terminological developed in different communities in the context of our framework. We focus on four modalities prevalent in real-world fact-checking: text, image, audio, and video. We survey benchmarks and models, and discuss limitations and promising directions for future research.
&lt;/p&gt;</description></item><item><title>Flover&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24182;&#34892;&#24615;&#19981;&#36275;&#21644;&#28789;&#27963;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#39640;&#25928;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13484</link><description>&lt;p&gt;
Flover&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13484
&lt;/p&gt;
&lt;p&gt;
Flover&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24182;&#34892;&#24615;&#19981;&#36275;&#21644;&#28789;&#27963;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#39640;&#25928;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#27169;&#22411;&#25512;&#26029;&#24615;&#33021;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#23588;&#20854;&#26159;&#22312;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#24182;&#34987;&#37096;&#32626;&#22312;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24773;&#20917;&#19979;&#12290;&#33258;&#22238;&#24402;&#27169;&#22411;&#30001;&#20110;&#22312;&#20247;&#22810;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22240;&#27492;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#35774;&#35745;&#19978;&#37319;&#29992;&#20102;&#19968;&#31181;&#26102;&#38388;&#20381;&#36182;&#32467;&#26500;&#65292;&#20854;&#20013;&#24403;&#21069;token&#30340;&#27010;&#29575;&#20998;&#24067;&#21463;&#21040;&#21069;&#38754;token&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26412;&#36136;&#19978;&#30340;&#24207;&#21015;&#29305;&#24615;&#36981;&#24490;&#39532;&#23572;&#21487;&#22827;&#38142;&#20551;&#35774;&#65292;&#32570;&#20047;&#26102;&#38388;&#24182;&#34892;&#24615;&#65292;&#22240;&#27492;&#23384;&#22312;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#22312;&#24037;&#19994;&#32972;&#26223;&#19979;&#65292;&#25512;&#26029;&#35831;&#27714;&#36981;&#24490;&#27850;&#26494;&#26102;&#38388;&#20998;&#24067;&#65292;&#38656;&#35201;&#19981;&#21516;&#30340;&#21709;&#24212;&#38271;&#24230;&#65292;&#36825;&#31181;&#24182;&#34892;&#24615;&#30340;&#32570;&#22833;&#26356;&#21152;&#26126;&#26174;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#22914;&#21160;&#24577;&#25209;&#22788;&#29702;&#21644;&#24182;&#21457;&#27169;&#22411;&#23454;&#20363;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#31895;&#31890;&#24230;&#30340;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#30340;&#24320;&#38144;&#21644;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#26080;&#27861;&#23454;&#29616;&#26368;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of deep learning, the performance of model inference has become a pivotal aspect as models become more complex and are deployed in diverse applications. Among these, autoregressive models stand out due to their state-of-the-art performance in numerous generative tasks. These models, by design, harness a temporal dependency structure, where the current token's probability distribution is conditioned on preceding tokens. This inherently sequential characteristic, however, adheres to the Markov Chain assumption and lacks temporal parallelism, which poses unique challenges. Particularly in industrial contexts where inference requests, following a Poisson time distribution, necessitate diverse response lengths, this absence of parallelism is more profound. Existing solutions, such as dynamic batching and concurrent model instances, nevertheless, come with severe overheads and a lack of flexibility, these coarse-grained methods fall short of achieving optimal la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36973;&#36935;&#30693;&#35782;&#20914;&#31361;&#26102;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;LLMs&#21487;&#20197;&#39640;&#24230;&#25509;&#21463;&#22806;&#37096;&#36830;&#36143;&#19988;&#26377;&#35828;&#26381;&#21147;&#30340;&#35777;&#25454;&#65292;&#21363;&#20351;&#19982;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#23384;&#22312;&#20914;&#31361;&#65292;&#20294;&#20063;&#21487;&#33021;&#26377;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13300</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#20914;&#31361;&#20013;&#30340;&#34892;&#20026;&#25581;&#31192;&#65306;&#33258;&#36866;&#24212;&#21464;&#33394;&#40857;&#36824;&#26159;&#22266;&#25191;&#30340;&#26641;&#29549;
&lt;/p&gt;
&lt;p&gt;
Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes. (arXiv:2305.13300v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36973;&#36935;&#30693;&#35782;&#20914;&#31361;&#26102;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;LLMs&#21487;&#20197;&#39640;&#24230;&#25509;&#21463;&#22806;&#37096;&#36830;&#36143;&#19988;&#26377;&#35828;&#26381;&#21147;&#30340;&#35777;&#25454;&#65292;&#21363;&#20351;&#19982;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#23384;&#22312;&#20914;&#31361;&#65292;&#20294;&#20063;&#21487;&#33021;&#26377;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#22806;&#37096;&#20449;&#24687;&#65292;&#24037;&#20855;&#22686;&#24378;&#65288;&#21253;&#25324;&#26816;&#32034;&#22686;&#24378;&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;LLMs&#38745;&#24577;&#21442;&#25968;&#21270;&#20869;&#23384;&#38480;&#21046;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#35777;&#25454;&#19982;&#23427;&#20204;&#30340;&#21442;&#25968;&#21270;&#20869;&#23384;&#21457;&#29983;&#20914;&#31361;&#26102;&#65292;LLMs&#23545;&#36825;&#20123;&#22806;&#37096;&#35777;&#25454;&#26377;&#22810;&#23569;&#25509;&#21463;&#33021;&#21147;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#20174;LLMs&#20013;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#21442;&#25968;&#21270;&#20869;&#23384;&#65292;&#24182;&#26500;&#24314;&#30456;&#24212;&#30340;&#23545;&#31435;&#20869;&#23384;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#19968;&#31995;&#21015;&#21463;&#25511;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;LLMs&#34920;&#29616;&#20986;&#30475;&#20284;&#30683;&#30462;&#30340;&#34892;&#20026;&#12290;&#19968;&#26041;&#38754;&#65292;&#19982;&#20197;&#24448;&#30340;&#35266;&#24565;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#35201;&#22806;&#37096;&#35777;&#25454;&#26159;&#36830;&#36143;&#19988;&#26377;&#35828;&#26381;&#21147;&#30340;&#65292;LLMs&#21363;&#20351;&#19982;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#23384;&#22312;&#20914;&#31361;&#20063;&#21487;&#20197;&#39640;&#24230;&#25509;&#21463;&#22806;&#37096;&#35777;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;LLMs&#20063;&#21487;&#33021;&#20250;&#34920;&#29616;&#20986;&#23616;&#38480;&#24615;&#65292;&#23588;&#20854;&#26159;&#24403;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#21463;&#21040;&#23041;&#32961;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory. However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory? We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts. We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments. Our investigation reveals seemingly contradicting behaviors of LLMs. On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. On the other hand, LLMs also d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;AVeriTeC&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;4,568&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20027;&#24352;&#65292;&#27599;&#20010;&#20027;&#24352;&#37117;&#26377;&#22312;&#32447;&#35777;&#25454;&#25903;&#25345;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#21644;&#25991;&#26412;&#35777;&#26126;&#35299;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#36991;&#20813;&#19968;&#20123;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26041;&#26696;&#26469;&#23545;&#24320;&#25918;&#32593;&#32476;&#19978;&#30340;&#20027;&#24352;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.13117</link><description>&lt;p&gt;
AVeriTeC: &#19968;&#20010;&#21253;&#21547;&#32593;&#32476;&#35777;&#25454;&#30340;&#30495;&#23454;&#19990;&#30028;&#20027;&#24352;&#39564;&#35777;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
AVeriTeC: A Dataset for Real-world Claim Verification with Evidence from the Web. (arXiv:2305.13117v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;AVeriTeC&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;4,568&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20027;&#24352;&#65292;&#27599;&#20010;&#20027;&#24352;&#37117;&#26377;&#22312;&#32447;&#35777;&#25454;&#25903;&#25345;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#21644;&#25991;&#26412;&#35777;&#26126;&#35299;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#36991;&#20813;&#19968;&#20123;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26041;&#26696;&#26469;&#23545;&#24320;&#25918;&#32593;&#32476;&#19978;&#30340;&#20027;&#24352;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#33258;&#21160;&#20107;&#23454;&#26816;&#26597;&#25968;&#25454;&#38598;&#23384;&#22312;&#37325;&#22823;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#20381;&#36182;&#20154;&#24037;&#20027;&#24352;&#12289;&#32570;&#20047;&#35777;&#25454;&#21644;&#20013;&#38388;&#25512;&#29702;&#27880;&#37322;&#65292;&#25110;&#21253;&#25324;&#21457;&#24067;&#20027;&#24352;&#21518;&#30340;&#35777;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; AVeriTeC&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324; 4,568 &#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20027;&#24352;&#65292;&#28085;&#30422;&#20102;50&#20010;&#19981;&#21516;&#32452;&#32455;&#30340;&#20107;&#23454;&#26816;&#26597;&#12290;&#27599;&#20010;&#20027;&#24352;&#37117;&#29992;&#22312;&#32447;&#21487;&#29992;&#35777;&#25454;&#25903;&#25345;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#21450;&#25991;&#26412;&#30340;&#35777;&#26126;&#35299;&#37322;&#35777;&#25454;&#22914;&#20309;&#30456;&#20114;&#32467;&#21512;&#20135;&#29983;&#32467;&#35770;&#12290;&#36890;&#36807;&#22810;&#36718;&#27880;&#37322;&#36807;&#31243;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#12289;&#35777;&#25454;&#19981;&#36275;&#21644;&#26102;&#38388;&#27844;&#28431;&#65292;&#24182;&#22312;&#32467;&#35770;&#19978;&#36798;&#25104;&#20102;&#30456;&#24403;&#22823;&#30340;&#27880;&#37322;&#21592;&#21327;&#35758; $\kappa=0.619$&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#32447;&#20197;&#21450;&#19968;&#20010;&#26041;&#27861;&#26469;&#39564;&#35777;&#36890;&#36807;&#23545;&#24320;&#25918;&#32593;&#32476;&#36827;&#34892;&#20960;&#20010;&#38382;&#39064;&#22238;&#31572;&#27493;&#39588;&#26469;&#26816;&#26597;&#20027;&#24352;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing datasets for automated fact-checking have substantial limitations, such as relying on artificial claims, lacking annotations for evidence and intermediate reasoning, or including evidence published after the claim. In this paper we introduce AVeriTeC, a new dataset of 4,568 real-world claims covering fact-checks by 50 different organizations. Each claim is annotated with question-answer pairs supported by evidence available online, as well as textual justifications explaining how the evidence combines to produce a verdict. Through a multi-round annotation process, we avoid common pitfalls including context dependence, evidence insufficiency, and temporal leakage, and reach a substantial inter-annotator agreement of $\kappa=0.619$ on verdicts. We develop a baseline as well as an evaluation scheme for verifying claims through several question-answering steps against the open web.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BEHRT&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#21307;&#30103;&#27010;&#24565;&#23884;&#20837;&#65292;&#22312;&#19981;&#20256;&#36755;&#20219;&#20309;&#38544;&#31169;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#21307;&#30103;&#20013;&#24515;&#30340;EHR&#25968;&#25454;&#30340;&#20849;&#21516;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#30149;&#20154;&#19979;&#19968;&#27425;&#35786;&#26029;&#26041;&#38754;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.13052</link><description>&lt;p&gt;
&#22522;&#20110;BEHRT&#30340;&#21307;&#30103;&#27010;&#24565;&#23884;&#20837;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning of Medical Concepts Embedding using BEHRT. (arXiv:2305.13052v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13052
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BEHRT&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#21307;&#30103;&#27010;&#24565;&#23884;&#20837;&#65292;&#22312;&#19981;&#20256;&#36755;&#20219;&#20309;&#38544;&#31169;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#21307;&#30103;&#20013;&#24515;&#30340;EHR&#25968;&#25454;&#30340;&#20849;&#21516;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#30149;&#20154;&#19979;&#19968;&#27425;&#35786;&#26029;&#26041;&#38754;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#21253;&#21547;&#30149;&#20154;&#30340;&#35786;&#26029;&#12289;&#33647;&#29289;&#12289;&#27835;&#30103;&#31561;&#21307;&#30103;&#35760;&#24405;&#12290;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#34987;&#35270;&#20026;&#25935;&#24863;&#30340;&#21307;&#30103;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#21307;&#30103;&#20013;&#24515;&#30340;EHR&#25968;&#25454;&#36890;&#24120;&#19981;&#33021;&#34987;&#20849;&#20139;&#65292;&#36825;&#20351;&#24471;&#20351;&#29992;&#22810;&#20010;&#20013;&#24515;&#30340;EHR&#25968;&#25454;&#21019;&#24314;&#39044;&#27979;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#65292;&#32780;&#36825;&#23545;&#20110;&#27169;&#22411;&#30340;&#24378;&#20581;&#24615;&#21644;&#26222;&#36866;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#31639;&#27861;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#20301;&#32622;&#30340;&#25968;&#25454;&#19978;&#23398;&#20064;&#20849;&#20139;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#23558;&#25152;&#26377;&#25968;&#25454;&#23384;&#20648;&#22312;&#20013;&#24515;&#20301;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#21307;&#30103;&#27010;&#24565;&#23884;&#20837;&#12290;&#36825;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#21483;&#20570;BEHRT&#65292;&#26159;&#22312;&#19968;&#20010;&#22823;&#22411;EHR&#25968;&#25454;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#65292;&#23427;&#23558;&#21307;&#30103;&#27010;&#24565;&#32534;&#30721;&#25104;&#39640;&#32500;&#21521;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#20010;&#21307;&#30103;&#20013;&#24515;&#30340;EHR&#25968;&#25454;&#20013;&#33719;&#21462;&#21307;&#30103;&#27010;&#24565;&#30340;&#23884;&#20837;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;&#23427;&#20204;&#20043;&#38388;&#20132;&#25442;&#35760;&#24405;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#24212;&#29992;&#20110;&#19968;&#20010;&#23454;&#38469;&#30340;&#39044;&#27979;&#20219;&#21153;&#65292;&#20351;&#29992;&#22810;&#20010;&#20013;&#24515;&#30340;EHR&#25968;&#25454;&#26469;&#39044;&#27979;&#19979;&#19968;&#27425;&#35786;&#26029;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Records (EHR) data contains medical records such as diagnoses, medications, procedures, and treatments of patients. This data is often considered sensitive medical information. Therefore, the EHR data from the medical centers often cannot be shared, making it difficult to create prediction models using multi-center EHR data, which is essential for such models' robustness and generalizability. Federated Learning (FL) is an algorithmic approach that allows learning a shared model using data in multiple locations without the need to store all data in a central place. An example of a prediction model's task is to predict future diseases. More specifically, the model needs to predict patient's next visit diagnoses, based on current and previous clinical data. Such a prediction model can support care providers in making clinical decisions and even provide preventive treatment. We propose a federated learning approach for learning medical concepts embedding. This pre-trained
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#23454;&#39564;&#30340;&#32467;&#26524;&#21644;&#21453;&#24605;&#65292;&#35813;&#23454;&#39564;&#20351;&#29992;&#27169;&#22411;GPT 3.5-Turbo&#26469;&#27169;&#25311;&#24402;&#32435;&#24335;&#20027;&#39064;&#20998;&#26512;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#23581;&#35797;&#20351;&#29992;LLM&#36827;&#34892;&#22522;&#20110;&#20154;&#31867;&#35299;&#37322;&#30340;&#20998;&#26512;&#26174;&#28982;&#26159;&#19968;&#31181;&#25361;&#25112;&#65292;&#20294;&#20063;&#26159;&#20102;&#35299;&#36825;&#20123;&#31995;&#32479;&#22312;&#23450;&#24615;&#30740;&#31350;&#20013;&#33021;&#21542;&#20351;&#29992;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.13014</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#35821;&#22659;&#19981;&#26126;&#32467;&#26500;&#21270;&#35775;&#35848;&#30340;&#24402;&#32435;&#24335;&#20027;&#39064;&#20998;&#26512;&#21527;&#65311;&#23545;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#25506;&#35752;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models emulate an inductive Thematic Analysis of semi-structured interviews? An exploration and provocation on the limits of the approach and the model. (arXiv:2305.13014v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#23454;&#39564;&#30340;&#32467;&#26524;&#21644;&#21453;&#24605;&#65292;&#35813;&#23454;&#39564;&#20351;&#29992;&#27169;&#22411;GPT 3.5-Turbo&#26469;&#27169;&#25311;&#24402;&#32435;&#24335;&#20027;&#39064;&#20998;&#26512;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#23581;&#35797;&#20351;&#29992;LLM&#36827;&#34892;&#22522;&#20110;&#20154;&#31867;&#35299;&#37322;&#30340;&#20998;&#26512;&#26174;&#28982;&#26159;&#19968;&#31181;&#25361;&#25112;&#65292;&#20294;&#20063;&#26159;&#20102;&#35299;&#36825;&#20123;&#31995;&#32479;&#22312;&#23450;&#24615;&#30740;&#31350;&#20013;&#33021;&#21542;&#20351;&#29992;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#24378;&#22823;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#21644;&#24037;&#20316;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#23454;&#39564;&#30340;&#32467;&#26524;&#21644;&#21453;&#24605;&#65292;&#35813;&#23454;&#39564;&#20351;&#29992;&#27169;&#22411;GPT 3.5-Turbo&#26469;&#27169;&#25311;&#24402;&#32435;&#24335;&#20027;&#39064;&#20998;&#26512;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#22312;&#35813;&#20027;&#39064;&#19978;&#20027;&#35201;&#36827;&#34892;&#28436;&#32462;&#20998;&#26512;&#12290;&#20027;&#39064;&#20998;&#26512;&#26159;&#19968;&#31181;&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#24120;&#29992;&#30340;&#23450;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#20154;&#31867;&#20998;&#26512;&#24072;&#30340;&#35299;&#37322;&#20197;&#21450;&#23450;&#24615;&#25968;&#25454;&#20013;&#30340;&#26174;&#24335;&#21644;&#28508;&#22312;&#21547;&#20041;&#30340;&#35782;&#21035;&#12290;&#23581;&#35797;&#20351;&#29992;LLM&#36827;&#34892;&#22522;&#20110;&#20154;&#31867;&#35299;&#37322;&#30340;&#20998;&#26512;&#26174;&#28982;&#26159;&#19968;&#31181;&#25361;&#25112;&#65292;&#20294;&#20063;&#26159;&#20102;&#35299;&#36825;&#20123;&#31995;&#32479;&#22312;&#23450;&#24615;&#30740;&#31350;&#20013;&#33021;&#21542;&#20351;&#29992;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23581;&#35797;&#36827;&#34892;&#27492;&#27169;&#25311;&#30340;&#21160;&#26426;&#65292;&#24182;&#21453;&#24605;&#20102;Braun&#21644;Clarke&#25552;&#20986;&#30340;&#20845;&#20010;&#27493;&#39588;&#33267;&#23569;&#37096;&#20998;&#22320;&#22914;&#20309;&#36827;&#34892;&#20027;&#39064;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful generative Artificial Intelligence solutions which can be applied to several fields and areas of work. This paper presents results and reflection of an experiment done to use the model GPT 3.5-Turbo to emulate some aspects of an inductive Thematic Analysis. Previous research on this subject has largely worked on conducting deductive analysis. Thematic Analysis is a qualitative method for analysis commonly used in social sciences and it is based on interpretations made by the human analyst(s) and the identification of explicit and latent meanings in qualitative data. Attempting an analysis based on human interpretation with an LLM clearly is a provocation but also a way to learn something about how these systems can or cannot be used in qualitative research. The paper presents the motivations for attempting this emulation, it reflects on how the six steps to a Thematic Analysis proposed by Braun and Clarke can at least partially be r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#22238;&#31572;&#22810;&#34920;&#38382;&#39064;&#12290; &#25105;&#20204;&#30340;&#26032;&#27169;&#22411;MultiTabQA&#19981;&#20165;&#21487;&#20197;&#22238;&#31572;&#22810;&#34920;&#38382;&#39064;&#65292;&#36824;&#21487;&#20197;&#27867;&#21270;&#22320;&#29983;&#25104;&#34920;&#26684;&#31572;&#26696;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#22522;&#32447;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12820</link><description>&lt;p&gt;
MultiTabQA&#65306;&#38024;&#23545;&#22810;&#34920;&#38382;&#39064;&#22238;&#31572;&#29983;&#25104;&#34920;&#26684;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering. (arXiv:2305.12820v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#22238;&#31572;&#22810;&#34920;&#38382;&#39064;&#12290; &#25105;&#20204;&#30340;&#26032;&#27169;&#22411;MultiTabQA&#19981;&#20165;&#21487;&#20197;&#22238;&#31572;&#22810;&#34920;&#38382;&#39064;&#65292;&#36824;&#21487;&#20197;&#27867;&#21270;&#22320;&#29983;&#25104;&#34920;&#26684;&#31572;&#26696;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#22522;&#32447;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#38382;&#31572;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#20854;&#35206;&#30422;&#38754;&#36824;&#21463;&#38480;&#20110;&#31572;&#22797;&#21333;&#34920;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#20013;&#26597;&#35810;&#20855;&#26377;&#22797;&#26434;&#24615;&#65292;&#24120;&#36328;&#36234;&#22810;&#20010;&#20851;&#31995;&#25968;&#25454;&#24211;&#25110;&#32593;&#39029;&#34920;&#26684;&#12290;&#23545;&#20110;&#21333;&#34920;&#38382;&#39064;&#65292;&#19981;&#28041;&#21450;&#24120;&#35265;&#30340;&#34920;&#26684;&#25805;&#20316;&#65292;&#22914;&#38598;&#21512;&#36816;&#31639;&#12289;&#31515;&#21345;&#23572;&#31215;&#65288;&#36830;&#25509;&#65289;&#25110;&#23884;&#22871;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#22810;&#34920;&#25805;&#20316;&#36890;&#24120;&#20250;&#20135;&#29983;&#34920;&#26684;&#36755;&#20986;&#65292;&#36825;&#23601;&#38656;&#35201;&#34920;&#26684;QA&#27169;&#22411;&#26377;&#29983;&#25104;&#34920;&#26684;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#22238;&#31572;&#22810;&#34920;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;MultiTabQA&#19981;&#20165;&#22238;&#31572;&#22810;&#34920;&#38382;&#39064;&#65292;&#36824;&#21487;&#20197;&#27867;&#21270;&#22320;&#29983;&#25104;&#34920;&#26684;&#31572;&#26696;&#12290;&#20026;&#20102;&#20351;&#35757;&#32451;&#26356;&#26377;&#25928;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;132,645&#20010;SQL&#26597;&#35810;&#21644;&#34920;&#26684;&#31572;&#26696;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#20005;&#26684;&#31243;&#24230;&#30340;&#29305;&#23450;&#20110;&#34920;&#26684;&#30340;&#24230;&#37327;&#26631;&#20934;&#23545;&#29983;&#25104;&#30340;&#34920;&#26684;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35780;&#20272;&#23454;&#39564;&#34920;&#26126;&#65292;MultiTabQA&#22312;&#26032;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#22823;&#22823;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in tabular question answering (QA) with large language models are constrained in their coverage and only answer questions over a single table. However, real-world queries are complex in nature, often over multiple tables in a relational database or web page. Single table questions do not involve common table operations such as set operations, Cartesian products (joins), or nested queries. Furthermore, multi-table operations often result in a tabular output, which necessitates table generation capabilities of tabular QA models. To fill this gap, we propose a new task of answering questions over multiple tables. Our model, MultiTabQA, not only answers questions over multiple tables, but also generalizes to generate tabular answers. To enable effective training, we build a pre-training dataset comprising of 132,645 SQL queries and tabular answers. Further, we evaluate the generated tables by introducing table-specific metrics of varying strictness assessing various levels 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23450;&#29702;&#39537;&#21160;&#30340;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;TheoremQA&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;AI&#27169;&#22411;&#22312;&#24212;&#29992;&#23450;&#29702;&#35299;&#20915;&#31185;&#23398;&#38382;&#39064;&#26102;&#30340;&#33021;&#21147;&#65292;&#32463;&#36807;&#27979;&#35797;&#65292;GPT-4&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#19978;&#30340;&#20934;&#30830;&#29575;&#36828;&#39640;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12524</link><description>&lt;p&gt;
TheoremQA&#65306;&#19968;&#31181;&#23450;&#29702;&#39537;&#21160;&#30340;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
TheoremQA: A Theorem-driven Question Answering dataset. (arXiv:2305.12524v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12524
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23450;&#29702;&#39537;&#21160;&#30340;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;TheoremQA&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;AI&#27169;&#22411;&#22312;&#24212;&#29992;&#23450;&#29702;&#35299;&#20915;&#31185;&#23398;&#38382;&#39064;&#26102;&#30340;&#33021;&#21147;&#65292;&#32463;&#36807;&#27979;&#35797;&#65292;GPT-4&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#19978;&#30340;&#20934;&#30830;&#29575;&#36828;&#39640;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;LLMs&#22914;GPT-4&#21644;PaLM-2&#22312;&#35299;&#20915;&#20687;GSM8K&#36825;&#26679;&#30340;&#22522;&#26412;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;90%&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35299;&#20915;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65288;&#21363;&#23450;&#29702;&#65289;&#30340;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TheoremQA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23450;&#29702;&#39537;&#21160;&#30340;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;AI&#27169;&#22411;&#24212;&#29992;&#23450;&#29702;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;TheoremQA&#30001;&#39046;&#22495;&#19987;&#23478;&#31574;&#21010;&#65292;&#21253;&#21547;&#26469;&#33258;&#25968;&#23398;&#12289;&#29289;&#29702;&#12289;&#30005;&#27668;&#19982;&#35745;&#31639;&#26426;&#31185;&#23398;&#20197;&#21450;&#37329;&#34701;&#23398;&#30340;800&#20010;&#39640;&#36136;&#37327;&#38382;&#39064;&#65292;&#28085;&#30422;350&#20010;&#23450;&#29702;&#65288;&#20363;&#22914;&#27888;&#21202;&#23450;&#29702;&#12289;&#25289;&#26684;&#26391;&#26085;&#23450;&#29702;&#12289;&#21704;&#22827;&#26364;&#32534;&#30721;&#12289;&#37327;&#23376;&#23450;&#29702;&#12289;&#24377;&#24615;&#23450;&#29702;&#31561;&#31561;&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;16&#20010;&#22823;&#22411;&#35821;&#35328;&#21644;&#20195;&#30721;&#27169;&#22411;&#20197;&#21450;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#20363;&#22914;Chain-of-Thoughts&#21644;Program-of-Thoughts&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-4&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#33021;&#21147;&#26159;&#26080;&#19982;&#20262;&#27604;&#30340;&#65292;&#20351;&#29992;Program-of-Thoughts&#25552;&#31034;&#31574;&#30053;&#26102;&#20934;&#30830;&#29575;&#36798;51%&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#36828;&#36828;&#33853;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in solving fundamental math problems like GSM8K by achieving over 90% accuracy. However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated. In this paper, we introduce TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models' capabilities to apply theorems to solve challenging science problems. TheoremQA is curated by domain experts containing 800 high-quality questions covering 350 theorems (e.g. Taylor's theorem, Lagrange's theorem, Huffman coding, Quantum Theorem, Elasticity Theorem, etc) from Math, Physics, EE&amp;CS, and Finance. We evaluate a wide spectrum of 16 large language and code models with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. We found that GPT-4's capabilities to solve these problems are unparalleled, achieving an accuracy of 51% with Progra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; XTREME-UP&#65292;&#19968;&#20010;&#20197;&#23569;&#37327;&#25968;&#25454;&#35780;&#20272;&#20195;&#34920;&#24615;&#19981;&#36275;&#35821;&#35328;&#30340; NLP &#31995;&#32479;&#24615;&#33021;&#30340;&#29992;&#25143;&#20013;&#24515;&#31232;&#32570;&#25968;&#25454;&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#19987;&#27880;&#20110;&#29992;&#25143;&#20013;&#24515;&#20219;&#21153;&#65292;&#32858;&#28966;&#20110;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#65292;&#35206;&#30422; 88 &#31181;&#35821;&#35328;&#65292;&#24182;&#20171;&#32461;&#20102;&#26032;&#30340; OCR&#12289;&#33258;&#21160;&#23436;&#25104;&#12289;&#35821;&#20041;&#20998;&#26512;&#21644;&#38899;&#35793;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24110;&#21161;&#25512;&#36827;&#20195;&#34920;&#24615;&#19981;&#36275;&#35821;&#35328;&#30340;&#39640;&#24230;&#22810;&#35821;&#35328; NLP &#31995;&#32479;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.11938</link><description>&lt;p&gt;
XTREME-UP&#65306;&#38024;&#23545;&#23569;&#26679;&#26412;&#25968;&#25454;&#30340;&#29992;&#25143;&#20013;&#24515;&#31232;&#32570;&#25968;&#25454;&#22522;&#20934;&#27979;&#35797;&#65292;&#22312;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#19978;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages. (arXiv:2305.11938v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11938
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; XTREME-UP&#65292;&#19968;&#20010;&#20197;&#23569;&#37327;&#25968;&#25454;&#35780;&#20272;&#20195;&#34920;&#24615;&#19981;&#36275;&#35821;&#35328;&#30340; NLP &#31995;&#32479;&#24615;&#33021;&#30340;&#29992;&#25143;&#20013;&#24515;&#31232;&#32570;&#25968;&#25454;&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#19987;&#27880;&#20110;&#29992;&#25143;&#20013;&#24515;&#20219;&#21153;&#65292;&#32858;&#28966;&#20110;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#65292;&#35206;&#30422; 88 &#31181;&#35821;&#35328;&#65292;&#24182;&#20171;&#32461;&#20102;&#26032;&#30340; OCR&#12289;&#33258;&#21160;&#23436;&#25104;&#12289;&#35821;&#20041;&#20998;&#26512;&#21644;&#38899;&#35793;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24110;&#21161;&#25512;&#36827;&#20195;&#34920;&#24615;&#19981;&#36275;&#35821;&#35328;&#30340;&#39640;&#24230;&#22810;&#35821;&#35328; NLP &#31995;&#32479;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#32570;&#26159;&#39640;&#24230;&#22810;&#35821;&#35328; NLP &#31995;&#32479;&#21457;&#23637;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#23545;&#20110;&#35768;&#22810;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#65288;UL&#65289;&#8212;&#8212; NLP &#30740;&#31350;&#22312;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#26041;&#38754;&#29305;&#21035;&#28382;&#21518;&#30340;&#35821;&#35328;&#8212;&#8212;&#27880;&#37322;&#23569;&#37327;&#25968;&#25454;&#26159;&#21487;&#34892;&#30340;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; XTREME-UP&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#37325;&#28857;&#26159;&#31232;&#32570;&#25968;&#25454;&#26041;&#26696;&#32780;&#19981;&#26159;&#38646;&#26679;&#26412;&#65307;&#20854;&#32858;&#28966;&#20110;&#29992;&#25143;&#20013;&#24515;&#20219;&#21153;&#65288;&#21363;&#35768;&#22810;&#39640;&#36164;&#28304;&#35821;&#35328;&#20351;&#29992;&#32773;&#24191;&#27867;&#37319;&#29992;&#30340;&#20219;&#21153;&#65289;&#65307;&#20197;&#21450;&#20854;&#32858;&#28966;&#20110;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#65292;&#22312;&#36825;&#20123;&#35821;&#35328;&#20013;&#65292;&#31232;&#32570;&#25968;&#25454;&#30340;&#24773;&#20917;&#24448;&#24448;&#26368;&#20026;&#29616;&#23454;&#12290;XTREME-UP &#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312; 88 &#31181;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#19978;&#65292;&#36328;&#36234;&#20102; 9 &#39033;&#20027;&#35201;&#30340;&#29992;&#25143;&#20013;&#24515;&#25216;&#26415;&#65292;&#21253;&#25324; ASR&#12289;OCR&#12289;MT &#21644;&#20449;&#24687;&#35775;&#38382;&#20219;&#21153;&#12290;&#25105;&#20204;&#20026; OCR&#12289;&#33258;&#21160;&#23436;&#25104;&#12289;&#35821;&#20041;&#20998;&#26512;&#21644;&#38899;&#35793;&#21019;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#26500;&#24314;&#24182;&#23436;&#21892;&#20102;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#12290;XTREME-UP &#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272; NLP &#31995;&#32479;&#22312;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#19978;&#31232;&#32570;&#25968;&#25454;&#26041;&#26696;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#23427;&#19987;&#27880;&#20110;&#23454;&#38469;&#20215;&#20540;&#23545;&#20110;&#39640;&#36164;&#28304;&#35821;&#35328;&#20351;&#29992;&#32773;&#30340;&#29992;&#25143;&#20013;&#24515;&#20219;&#21153;&#65292;&#24182;&#28085;&#30422;&#20102; 88 &#31181;&#35821;&#35328;&#12290;&#20854;&#30446;&#26631;&#26159;&#24110;&#21161;&#25512;&#36827;&#22810;&#35821;&#35328; NLP &#31995;&#32479;&#23545;&#20110;&#31232;&#32570;&#25968;&#25454;&#36164;&#28304;&#30340;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data scarcity is a crucial issue for the development of highly multilingual NLP systems. Yet for many under-represented languages (ULs) -- languages for which NLP re-search is particularly far behind in meeting user needs -- it is feasible to annotate small amounts of data. Motivated by this, we propose XTREME-UP, a benchmark defined by: its focus on the scarce-data scenario rather than zero-shot; its focus on user-centric tasks -- tasks with broad adoption by speakers of high-resource languages; and its focus on under-represented languages where this scarce-data scenario tends to be most realistic. XTREME-UP evaluates the capabilities of language models across 88 under-represented languages over 9 key user-centric technologies including ASR, OCR, MT, and information access tasks that are of general utility. We create new datasets for OCR, autocomplete, semantic parsing, and transliteration, and build on and refine existing datasets for other tasks. XTREME-UP provides methodology for e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.11490</link><description>&lt;p&gt;
LLM&#33258;&#36523;&#21487;&#35835;&#21462;&#21644;&#29983;&#25104;CXR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11490
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#20110;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#65292;&#20154;&#20204;&#27491;&#31215;&#26497;&#23581;&#35797;&#23558;LLMs&#30340;&#23454;&#29992;&#24615;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#24050;&#32463;&#26377;&#20154;&#23581;&#35797;&#36830;&#25509;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#19988;&#20063;&#22312;&#19981;&#26029;&#23581;&#35797;&#20026;LLMs&#28155;&#21152;&#35270;&#35273;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#21482;&#20351;&#29992;LLMs&#20316;&#20026;&#22270;&#20687;&#35299;&#30721;&#22120;&#65292;&#27809;&#26377;&#23581;&#35797;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#36890;&#36807;&#37319;&#29992;VQ-GAN&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#30340;&#28508;&#22312;&#34920;&#31034;&#35270;&#20026;&#19968;&#31181;&#25991;&#26412;&#26631;&#35760;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#65292;&#20197;&#20687;&#25991;&#26412;&#19968;&#26679;&#35835;&#21462;&#21644;&#29983;&#25104;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25110;&#35757;&#32451;&#19987;&#38376;&#30340;&#32593;&#32476;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#20449;&#24687;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#33258;&#21160;&#21270;&#21019;&#36896;&#30340;&#20250;&#35805;&#25509;&#21475;&#26469;&#26041;&#20415;&#22823;&#20247;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11326</link><description>&lt;p&gt;
&#33258;&#21160;&#29983;&#25104;&#20250;&#35805;&#25509;&#21475;&#20197;&#20415;&#20110;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data. (arXiv:2305.11326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#33258;&#21160;&#21270;&#21019;&#36896;&#30340;&#20250;&#35805;&#25509;&#21475;&#26469;&#26041;&#20415;&#22823;&#20247;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#26159;&#22312;&#32447;&#21457;&#24067;&#21644;&#20132;&#25442;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#26368;&#24120;&#35265;&#26684;&#24335;&#12290;&#19968;&#20010;&#26126;&#30830;&#30340;&#20363;&#23376;&#26159;&#21508;&#31181;&#31867;&#22411;&#30340;&#20844;&#20849;&#34892;&#25919;&#26426;&#26500;&#21457;&#24067;&#30340;&#24320;&#25918;&#25968;&#25454;&#38376;&#25143;&#25968;&#37327;&#30340;&#22686;&#38271;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#25968;&#25454;&#28304;&#30340;&#21033;&#29992;&#30446;&#21069;&#20165;&#38480;&#20110;&#33021;&#22815;&#20197;&#31243;&#24207;&#26041;&#24335;&#22788;&#29702;&#21644;&#28040;&#21270;&#27492;&#31867;&#25968;&#25454;&#30340;&#25216;&#26415;&#20154;&#21592;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20250;&#35805;&#25509;&#21475;&#65292;&#20197;&#20415;&#20110;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;&#28304;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20219;&#20309;&#26222;&#36890;&#20844;&#27665;&#37117;&#21487;&#20197;&#20174;&#20013;&#21463;&#30410;&#24182;&#21033;&#29992;&#23427;&#20204;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#19981;&#26159;&#25163;&#21160;&#21019;&#24314;&#30340;&#65306;&#30456;&#21453;&#65292;&#23427;&#20204;&#26159;&#36890;&#36807;&#23454;&#20363;&#21270;&#21487;&#37197;&#32622;&#30340;&#23545;&#35805;&#27169;&#24335;&#38598;&#20174;&#25968;&#25454;&#28304;&#26412;&#36523;&#33258;&#21160;&#29983;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data is the most common format to publish and exchange structured data online. A clear example is the growing number of open data portals published by all types of public administrations. However, exploitation of these data sources is currently limited to technical people able to programmatically manipulate and digest such data. As an alternative, we propose the use of chatbots to offer a conversational interface to facilitate the exploration of tabular data sources. With our approach, any regular citizen can benefit and leverage them. Moreover, our chatbots are not manually created: instead, they are automatically generated from the data source itself thanks to the instantiation of a configurable collection of conversation patterns.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#36890;&#36807;&#22312;&#31243;&#24207;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;&#20351;&#27809;&#26377;&#38024;&#23545;&#23398;&#20064;&#35821;&#35328;&#35821;&#20041;&#25552;&#20379;&#24402;&#32435;&#20559;&#24046;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#21547;&#20041;&#12290;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20174;&#27169;&#22411;&#29366;&#24577;&#20013;&#25552;&#21462;&#31243;&#24207;&#29366;&#24577;&#30340;&#25277;&#35937;&#65292;&#20934;&#30830;&#24615;&#19982;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#31243;&#24207;&#30340;&#33021;&#21147;&#26174;&#33879;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.11169</link><description>&lt;p&gt;
&#22312;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#35821;&#20041;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Evidence of Meaning in Language Models Trained on Programs. (arXiv:2305.11169v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11169
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#36890;&#36807;&#22312;&#31243;&#24207;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;&#20351;&#27809;&#26377;&#38024;&#23545;&#23398;&#20064;&#35821;&#35328;&#35821;&#20041;&#25552;&#20379;&#24402;&#32435;&#20559;&#24046;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#21547;&#20041;&#12290;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20174;&#27169;&#22411;&#29366;&#24577;&#20013;&#25552;&#21462;&#31243;&#24207;&#29366;&#24577;&#30340;&#25277;&#35937;&#65292;&#20934;&#30830;&#24615;&#19982;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#31243;&#24207;&#30340;&#33021;&#21147;&#26174;&#33879;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#23613;&#31649;&#34987;&#35757;&#32451;&#21482;&#26159;&#25191;&#34892;&#25991;&#26412;&#19978;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#29305;&#21035;&#26159;&#19968;&#20010;&#31243;&#24207;&#35821;&#26009;&#24211;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#21547;&#20041;&#12290;&#27599;&#20010;&#31243;&#24207;&#37117;&#20197;&#65288;&#25991;&#26412;&#65289;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#30340;&#24418;&#24335;&#20316;&#20026;&#35268;&#33539;&#12290;&#19982;&#31243;&#24207;&#19968;&#36215;&#24037;&#20316;&#20351;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#23450;&#20041;&#19982;&#35821;&#35328;&#20013;&#26377;&#20851;&#21547;&#20041;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#27491;&#30830;&#24615;&#21644;&#35821;&#20041;&#65289;&#65292;&#20351;&#24471;&#31243;&#24207;&#32508;&#21512;&#25104;&#20026;&#19968;&#20010;&#20013;&#38388;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#34920;&#24449;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#21547;&#20041;&#30340;&#23384;&#22312;&#65288;&#25110;&#19981;&#23384;&#22312;&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#31243;&#24207;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;Transformer&#27169;&#22411;&#65292;&#28982;&#21518;&#25506;&#26597;&#20102;&#24050;&#32463;&#23436;&#25104;&#35268;&#33539;&#30340;&#31243;&#24207;&#26102;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#23613;&#31649;&#27809;&#26377;&#38024;&#23545;&#23398;&#20064;&#35821;&#35328;&#35821;&#20041;&#25552;&#20379;&#24402;&#32435;&#20559;&#24046;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#65292;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20174;&#27169;&#22411;&#29366;&#24577;&#20013;&#25552;&#21462;&#24403;&#21069;&#21644;&#26410;&#26469;&#31243;&#24207;&#29366;&#24577;&#30340;&#25277;&#35937;&#12290;&#27492;&#22806;&#65292;&#32447;&#24615;&#25506;&#27979;&#22120;&#30340;&#20934;&#30830;&#24615;&#19982;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#31243;&#24207;&#30340;&#33021;&#21147;&#24378;&#26377;&#21147;&#12289;&#32479;&#35745;&#23398;&#26174;&#33879;&#22320;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present evidence that language models can learn meaning despite being trained only to perform next token prediction on text, specifically a corpus of programs. Each program is preceded by a specification in the form of (textual) input-output examples. Working with programs enables us to precisely define concepts relevant to meaning in language (e.g., correctness and semantics), making program synthesis well-suited as an intermediate testbed for characterizing the presence (or absence) of meaning in language models.  We first train a Transformer model on the corpus of programs, then probe the trained model's hidden states as it completes a program given a specification. Despite providing no inductive bias toward learning the semantics of the language, we find that a linear probe is able to extract abstractions of both current and future program states from the model states. Moreover, there is a strong, statistically significant correlation between the accuracy of the probe and the mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#33258;&#36866;&#24212;&#25628;&#32034;&#24341;&#25806;&#36741;&#21161;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#19982;LLMs&#34701;&#21512;&#65292;&#20174;&#32780;&#36991;&#20813;&#26080;&#29992;&#25110;&#22024;&#26434;&#30340;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2305.10998</link><description>&lt;p&gt;
&#32593;&#32476;&#21487;&#20197;&#20026;&#25913;&#36827;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26356;&#22810;&#30340;&#21487;&#36873;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
The Web Can Be Your Oyster for Improving Large Language Models. (arXiv:2305.10998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#33258;&#36866;&#24212;&#25628;&#32034;&#24341;&#25806;&#36741;&#21161;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#19982;LLMs&#34701;&#21512;&#65292;&#20174;&#32780;&#36991;&#20813;&#26080;&#29992;&#25110;&#22024;&#26434;&#30340;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#32534;&#30721;&#22823;&#37327;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#30693;&#35782;&#22312;&#27169;&#22411;&#35757;&#32451;&#26102;&#24050;&#32463;&#22266;&#21270;&#65292;&#36825;&#20123;&#27169;&#22411;&#21464;&#24471;&#38745;&#24577;&#65292;&#24182;&#19988;&#21463;&#21040;&#20102;&#24403;&#26102;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;LLMs&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#23558;LLMs&#19982;&#28023;&#37327;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#22686;&#24378;&#12290;&#19982;&#20197;&#24448;&#30340;&#22686;&#24378;&#26469;&#28304;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#36716;&#20648;&#65289;&#19981;&#21516;&#65292;&#32593;&#32476;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#12289;&#26356;&#20840;&#38754;&#19988;&#19981;&#26029;&#26356;&#26032;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UNIWEB&#30340;&#32593;&#32476;&#22686;&#24378;LLM&#65292;&#23427;&#20197;&#32479;&#19968;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#26684;&#24335;&#22312;16&#20010;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24182;&#19981;&#26159;&#31616;&#21333;&#22320;&#20351;&#29992;&#20174;&#32593;&#32476;&#26816;&#32034;&#21040;&#30340;&#20869;&#23481;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#25628;&#32034;&#24341;&#25806;&#36741;&#21161;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#25105;&#35780;&#20272;LLM&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#65292;&#24182;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#20309;&#26102;&#21442;&#32771;&#32593;&#32476;&#33719;&#21462;&#26356;&#22810;&#25968;&#25454;&#65292;&#20174;&#32780;&#36991;&#20813;&#26080;&#29992;&#25110;&#22024;&#26434;&#30340;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) encode a large amount of world knowledge. However, as such knowledge is frozen at the time of model training, the models become static and limited by the training data at that time. In order to further improve the capacity of LLMs for knowledge-intensive tasks, we consider augmenting LLMs with the large-scale web using search engine. Unlike previous augmentation sources (e.g., Wikipedia data dump), the web provides broader, more comprehensive and constantly updated information. In this paper, we present a web-augmented LLM UNIWEB, which is trained over 16 knowledge-intensive tasks in a unified text-to-text format. Instead of simply using the retrieved contents from web, our approach has made two major improvements. Firstly, we propose an adaptive search engine assisted learning method that can self-evaluate the confidence level of LLM's predictions, and adaptively determine when to refer to the web for more data, which can avoid useless or noisy augmentatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#20016;&#23500;&#30340;&#23454;&#20307;&#32463;&#39564;&#36827;&#34892;&#24494;&#35843;, &#20197;&#25552;&#39640;&#20854;&#22312;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#24182;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#20445;&#25345;&#25110;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10626</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36935;&#35265;&#19990;&#30028;&#27169;&#22411;&#65306;&#23454;&#20307;&#32463;&#39564;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#20016;&#23500;&#30340;&#23454;&#20307;&#32463;&#39564;&#36827;&#34892;&#24494;&#35843;, &#20197;&#25552;&#39640;&#20854;&#22312;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#24182;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#20445;&#25345;&#25110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LMs) &#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#22312;&#22788;&#29702;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#31616;&#21333;&#25512;&#29702;&#21644;&#35268;&#21010;&#38382;&#39064;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#20363;&#22914;&#29702;&#35299;&#29289;&#20307;&#27704;&#24658;&#25110;&#35268;&#21010;&#23478;&#24237;&#27963;&#21160;&#12290;&#36825;&#31181;&#38480;&#21046;&#28304;&#20110; LM &#20165;&#21463;&#20070;&#38754;&#35821;&#35328;&#35757;&#32451;&#65292;&#32570;&#23569;&#24517;&#35201;&#30340;&#23454;&#20307;&#30693;&#35782;&#21644;&#25216;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378; LM &#30340;&#26041;&#27861;&#65292;&#21363;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#23454;&#20307;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#19968;&#33324;&#35821;&#35328;&#33021;&#21147;&#12290;&#26412;&#26041;&#27861;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#37096;&#32626;&#19968;&#20010;&#34701;&#20837;&#23454;&#20307;&#32463;&#39564;&#30340;&#20195;&#29702;&#65292;&#29305;&#21035;&#26159;&#19968;&#20010;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#30340;&#20223;&#30495;&#22120;(VirtualHome)&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#23454;&#20307;&#32463;&#39564;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#20123;&#32463;&#39564;&#24494;&#35843; LM &#65292;&#20197;&#25945;&#25480;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#21508;&#31181;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#20363;&#22914;&#35268;&#21010;&#21644;&#23436;&#25104;&#30446;&#26631;&#12289;&#29289;&#20307;&#27704;&#24658;&#21644;&#36319;&#36394;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#20197;&#21033;&#29992;&#20854;&#20182;&#27169;&#25311;&#22120;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#30528;&#25552;&#39640;&#20102; LM &#22312;&#19968;&#31995;&#21015;&#29289;&#29702;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#30041;&#24182;&#32463;&#24120;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is
&lt;/p&gt;</description></item><item><title>DoReMi&#26041;&#27861;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#35757;&#32451;&#23567;&#22411;&#20195;&#29702;&#27169;&#22411;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65292;&#20877;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#30456;&#27604;&#20351;&#29992;&#40664;&#35748;&#26435;&#37325;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;The Pile&#21644;GLaM&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;6.5%&#21644;4.7%&#30340;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#65292;&#20998;&#21035;&#20351;&#29992;2.6&#20493;&#21644;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.10429</link><description>&lt;p&gt;
DoReMi: &#20248;&#21270;&#25968;&#25454;&#28151;&#21512;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. (arXiv:2305.10429v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10429
&lt;/p&gt;
&lt;p&gt;
DoReMi&#26041;&#27861;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#35757;&#32451;&#23567;&#22411;&#20195;&#29702;&#27169;&#22411;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65292;&#20877;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#30456;&#27604;&#20351;&#29992;&#40664;&#35748;&#26435;&#37325;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;The Pile&#21644;GLaM&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;6.5%&#21644;4.7%&#30340;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#65292;&#20998;&#21035;&#20351;&#29992;2.6&#20493;&#21644;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#22495;&#30340;&#28151;&#21512;&#27604;&#20363;&#65288;&#20363;&#22914;&#65292;&#32500;&#22522;&#30334;&#31185;&#12289;&#22270;&#20070;&#12289;&#32593;&#39029;&#25991;&#26412;&#65289;&#26497;&#22823;&#22320;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DoReMi&#30340;Domain Reweighting with Minimax Optimization&#26041;&#27861;&#65292;&#23427;&#39318;&#20808;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#65288;Group DRO&#65289;&#35757;&#32451;&#19968;&#20010;&#23567;&#20195;&#29702;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65288;&#28151;&#21512;&#27604;&#20363;&#65289;&#65292;&#32780;&#19981;&#38656;&#35201;&#30693;&#36947;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#22495;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#26356;&#22823;&#30340;&#65292;&#20840;&#23610;&#23544;&#30340;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;DoReMi&#22312;&#19968;&#20010;280M&#21442;&#25968;&#30340;&#20195;&#29702;&#27169;&#22411;&#19978;&#65292;&#26356;&#26377;&#25928;&#22320;&#25214;&#21040;&#35757;&#32451;&#19968;&#20010;8B&#21442;&#25968;&#27169;&#22411;&#65288;30&#20493;&#22823;&#65289;&#30340;&#22495;&#26435;&#37325;&#12290;&#22312;The Pile&#19978;&#65292;&#21363;&#20351;&#22312;&#20943;&#23567;&#19968;&#20123;&#22495;&#30340;&#27604;&#37325;&#26102;&#65292;DoReMi&#20063;&#33021;&#25552;&#39640;&#25152;&#26377;&#22495;&#30340;perplexity&#12290;&#30456;&#27604;&#20351;&#29992;The Pile&#30340;&#40664;&#35748;&#22495;&#26435;&#37325;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;DoReMi&#23558;&#24179;&#22343;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;6.5%&#65292;&#24182;&#20351;&#29992;2.6&#20493;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;&#22312;GLaM&#25968;&#25454;&#38598;&#19978;&#65292;DoReMi&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;4.7%&#65288;&#27425;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65289;&#30340;few-shot&#20934;&#30830;&#24230;&#65292;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#19979;&#25552;&#39640;&#20102;9.0%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no know
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#24212;&#29992;&#20110;&#35821;&#38899;&#21512;&#25104;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#36798;&#24615;&#24378;&#12289;&#22810;&#35821;&#38899;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;TorToise&#12290;</title><link>http://arxiv.org/abs/2305.07243</link><description>&lt;p&gt;
&#36890;&#36807;&#32553;&#25918;&#23454;&#29616;&#26356;&#22909;&#30340;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Better speech synthesis through scaling. (arXiv:2305.07243v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07243
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#24212;&#29992;&#20110;&#35821;&#38899;&#21512;&#25104;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#36798;&#24615;&#24378;&#12289;&#22810;&#35821;&#38899;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;TorToise&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#22312;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#21644;DDPM&#30340;&#24212;&#29992;&#19979;&#24471;&#21040;&#20102;&#38761;&#21629;&#24615;&#31361;&#30772;&#12290;&#36825;&#20123;&#26041;&#27861;&#23558;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#24314;&#27169;&#20026;&#36880;&#27493;&#27010;&#29575;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#26469;&#23398;&#20064;&#22270;&#20687;&#20998;&#24067;&#12290;&#26412;&#25991;&#23558;&#36825;&#31181;&#24615;&#33021;&#25552;&#21319;&#26041;&#27861;&#36816;&#29992;&#21040;&#35821;&#38899;&#21512;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TorToise&#30340;&#34920;&#36798;&#24615;&#12289;&#22810;&#35821;&#38899;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;&#12290;&#25152;&#26377;&#27169;&#22411;&#20195;&#30721;&#21644;&#35757;&#32451;&#26435;&#37325;&#22343;&#24050;&#24320;&#28304;&#65292;&#23384;&#25918;&#20110;https://github.com/neonbjb/tortoise-tts&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the field of image generation has been revolutionized by the application of autoregressive transformers and DDPMs. These approaches model the process of image generation as a step-wise probabilistic processes and leverage large amounts of compute and data to learn the image distribution. This methodology of improving performance need not be confined to images. This paper describes a way to apply advances in the image generative domain to speech synthesis. The result is TorToise -- an expressive, multi-voice text-to-speech system.  All model code and trained weights have been open-sourced at https://github.com/neonbjb/tortoise-tts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#21333;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30446;&#26631;&#32676;&#20307;&#30340;&#39044;&#27979;&#65292;&#27169;&#25311;&#20102;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#22312;&#32447;&#24847;&#35265;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06626</link><description>&lt;p&gt;
&#24403;&#22810;&#25968;&#20154;&#26159;&#38169;&#35823;&#30340;&#65306;&#21033;&#29992;&#26631;&#27880;&#32773;&#19981;&#19968;&#33268;&#24615;&#36827;&#34892;&#20027;&#35266;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#21333;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30446;&#26631;&#32676;&#20307;&#30340;&#39044;&#27979;&#65292;&#27169;&#25311;&#20102;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#22312;&#32447;&#24847;&#35265;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#34429;&#28982;&#36890;&#24120;&#20351;&#29992;&#26631;&#27880;&#32773;&#30340;&#22810;&#25968;&#25237;&#31080;&#26469;&#30830;&#23450;&#26631;&#31614;&#65292;&#20294;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#65292;&#26631;&#27880;&#32773;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#21487;&#33021;&#21453;&#26144;&#20986;&#32676;&#20307;&#35266;&#28857;&#30340;&#24046;&#24322;&#65292;&#32780;&#19981;&#26159;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#19968;&#20010;&#35821;&#21477;&#26159;&#21542;&#20882;&#29359;&#20102;&#23427;&#25152;&#38024;&#23545;&#30340;&#20154;&#32676;&#65292;&#32780;&#36825;&#21487;&#33021;&#21482;&#21344;&#26631;&#27880;&#32773;&#27744;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#39044;&#27979;&#21487;&#33021;&#20855;&#26377;&#20882;&#29359;&#24615;&#25991;&#26412;&#19978;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30340;&#39044;&#27979;&#30446;&#26631;&#32676;&#20307;&#26469;&#27169;&#25311;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#25552;&#39640;&#20102;22&#65285;&#22312;&#39044;&#27979;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#19978;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;33&#65285;&#22312;&#39044;&#27979;&#26631;&#27880;&#32773;&#20043;&#38388;&#26041;&#24046;&#19978;&#30340;&#24615;&#33021;&#65292;&#36825;&#25552;&#20379;&#20102;&#19979;&#28216;&#29992;&#26469;&#34913;&#37327;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21487;&#20197;&#20351;&#29992;&#26631;&#27880;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#21644;&#20854;&#22312;&#32447;&#24847;&#35265;&#26469;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though majority vote among annotators is typically used for ground truth labels in natural language processing, annotator disagreement in tasks such as hate speech detection may reflect differences among group opinions, not noise. Thus, a crucial problem in hate speech detection is whether a statement is offensive to the demographic group that it targets, which may constitute a small fraction of the annotator pool. We construct a model that predicts individual annotator ratings on potentially offensive text and combines this information with the predicted target group of the text to model the opinions of target group members. We show gains across a range of metrics, including raising performance over the baseline by 22% at predicting individual annotators' ratings and 33% at predicting variance among annotators, which provides a method of measuring model uncertainty downstream. We find that annotators' ratings can be predicted using their demographic information and opinions on online 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#23383;&#20856;&#38142;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#33021;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.06575</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23383;&#20856;&#38142;&#25552;&#31034;&#22312;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Dictionary Prompting Elicits Translation in Large Language Models. (arXiv:2305.06575v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06575
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#23383;&#20856;&#38142;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#33021;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(MNMT)&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#27809;&#26377;&#24179;&#34892;&#25968;&#25454;&#20063;&#33021;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35757;&#32451;&#25968;&#25454;&#37327;&#24040;&#22823;&#65292;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#32763;&#35793;&#31232;&#26377;&#35789;&#27719;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#65292;&#24456;&#38590;&#26816;&#32034;&#21040;&#30456;&#20851;&#31034;&#33539;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36825;&#38480;&#21046;&#20102;LLMs&#22312;&#32763;&#35793;&#26041;&#38754;&#30340;&#23454;&#38469;&#24212;&#29992;&#8212;&#8212;&#25105;&#20204;&#35813;&#22914;&#20309;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;CoD&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;&#23383;&#20856;&#38142;&#20026;&#19968;&#37096;&#20998;&#36755;&#20837;&#21333;&#35789;&#22686;&#21152;LLMs&#30340;&#20808;&#21069;&#30693;&#35782;&#65292;&#20174;&#32780;&#20419;&#36827;LLMs&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;FLORES-200&#20840;&#24320;&#21457;&#27979;&#35797;&#38598;&#19978;&#65292;&#36890;&#36807;&#23558;CoD&#21644;ChatGPT&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#39640;&#36798;13&#20493;&#30340;MNMT ChrF++&#20998;&#25968;&#30340;&#25910;&#30410;&#65288;&#33521;&#35821;&#21040;&#22622;&#23572;&#32500;&#20122;&#35821;&#65292;&#35199;&#37324;&#23572;&#23383;&#27597;&#20070;&#20889;&#65292;ChrF ++&#20998;&#25968;&#20174;3.08&#22686;&#21152;&#21040;42.63&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20854;&#20182;&#25968;&#25454;&#38598;&#19978;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown surprisingly good performance in multilingual neural machine translation (MNMT) even when trained without parallel data. Yet, despite the fact that the amount of training data is gigantic, they still struggle with translating rare words, particularly for low-resource languages. Even worse, it is usually unrealistic to retrieve relevant demonstrations for in-context learning with low-resource languages on LLMs, which restricts the practical use of LLMs for translation -- how should we mitigate this problem? To this end, we present a novel method, CoD, which augments LLMs with prior knowledge with the chains of multilingual dictionaries for a subset of input words to elicit translation abilities for LLMs. Extensive experiments indicate that augmenting ChatGPT with CoD elicits large gains by up to 13x ChrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in Cyrillic script) on FLORES-200 full devtest set. We further demonstrate the im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;STS&#27169;&#22411;AlignSTS&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#33410;&#22863;&#36866;&#37197;&#22120;&#26469;&#39044;&#27979;&#30446;&#26631;&#33410;&#22863;&#34920;&#31034;&#20197;&#24357;&#21512;&#20869;&#23481;&#21644;&#38899;&#39640;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#37325;&#26032;&#23545;&#40784;&#20869;&#23481;&#36827;&#34892;&#36328;&#27169;&#24577;&#34701;&#21512;&#37325;&#26032;&#21512;&#25104;&#12290;&#35813;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.04476</link><description>&lt;p&gt;
AlignSTS&#65306;&#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#40784;&#23454;&#29616;&#35821;&#38899;&#36716;&#21809;
&lt;/p&gt;
&lt;p&gt;
AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment. (arXiv:2305.04476v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;STS&#27169;&#22411;AlignSTS&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#33410;&#22863;&#36866;&#37197;&#22120;&#26469;&#39044;&#27979;&#30446;&#26631;&#33410;&#22863;&#34920;&#31034;&#20197;&#24357;&#21512;&#20869;&#23481;&#21644;&#38899;&#39640;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#37325;&#26032;&#23545;&#40784;&#20869;&#23481;&#36827;&#34892;&#36328;&#27169;&#24577;&#34701;&#21512;&#37325;&#26032;&#21512;&#25104;&#12290;&#35813;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#36716;&#21809; (STS) &#20219;&#21153;&#26088;&#22312;&#22312;&#38754;&#23545;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26102;&#65292;&#29983;&#25104;&#19982;&#35821;&#38899;&#24405;&#38899;&#30456;&#23545;&#24212;&#30340;&#21809;&#27468;&#26679;&#26412;&#65306;&#22312;&#27809;&#26377;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#65288;&#21809;&#27468;&#65289;&#38899;&#39640;&#36718;&#24275;&#21644;&#28304;&#65288;&#35821;&#38899;&#65289;&#20869;&#23481;&#20043;&#38388;&#30340;&#23545;&#40784;&#38590;&#20197;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26174;&#24335;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;STS&#27169;&#22411;AlignSTS&#65292;&#23558;&#35821;&#38899;&#21464;&#21270;&#65288;&#22914;&#38899;&#39640;&#21644;&#20869;&#23481;&#65289;&#35270;&#20026;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#21463;&#20154;&#31867;&#22914;&#20309;&#21809;&#20986;&#26059;&#24459;&#30340;&#27468;&#35789;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;AlignSTS: 1&#65289;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#33410;&#22863;&#36866;&#37197;&#22120;&#26469;&#39044;&#27979;&#30446;&#26631;&#33410;&#22863;&#34920;&#31034;&#65292;&#20197;&#24357;&#21512;&#20869;&#23481;&#21644;&#38899;&#39640;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#65292;&#20854;&#20013;&#33410;&#22863;&#34920;&#31034;&#20197;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#24335;&#35745;&#31639;&#65292;&#24182;&#37327;&#21270;&#20026;&#31163;&#25955;&#31354;&#38388;&#65307;2&#65289;&#20351;&#29992;&#39044;&#27979;&#30340;&#33410;&#22863;&#34920;&#31034;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#37325;&#26032;&#23545;&#40784;&#20869;&#23481;&#65292;&#24182;&#36827;&#34892;&#36328;&#27169;&#24577;&#34701;&#21512;&#37325;&#26032;&#21512;&#25104;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;AlignSTS&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The speech-to-singing (STS) voice conversion task aims to generate singing samples corresponding to speech recordings while facing a major challenge: the alignment between the target (singing) pitch contour and the source (speech) content is difficult to learn in a text-free situation. This paper proposes AlignSTS, an STS model based on explicit cross-modal alignment, which views speech variance such as pitch and content as different modalities. Inspired by the mechanism of how humans will sing the lyrics to the melody, AlignSTS: 1) adopts a novel rhythm adaptor to predict the target rhythm representation to bridge the modality gap between content and pitch, where the rhythm representation is computed in a simple yet effective way and is quantized into a discrete space; and 2) uses the predicted rhythm representation to re-align the content based on cross-attention and conducts a cross-modal fusion for re-synthesize. Extensive experiments show that AlignSTS achieves superior performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;LaMini-LM&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#25351;&#20196;&#30340;&#22810;&#26679;&#24615;&#21387;&#32553;&#27169;&#22411;&#32676;&#38598;&#65292;&#20174;&#25351;&#20196;&#24494;&#35843;&#36807;&#30340;LLMs&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#26356;&#23567;&#30340;&#27169;&#22411;&#20013;&#65292;&#20854;&#22312;15&#20010;&#19981;&#21516;&#30340;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#30340;&#34920;&#29616;&#30456;&#24403;&#65292;&#20294;&#20307;&#31215;&#32422;&#23567;&#20102;10&#20493;&#12290;</title><link>http://arxiv.org/abs/2304.14402</link><description>&lt;p&gt;
LaMini-LM: &#22522;&#20110;&#22823;&#35268;&#27169;&#25351;&#20196;&#30340;&#22810;&#26679;&#24615;&#21387;&#32553;&#27169;&#22411;&#32676;&#38598;
&lt;/p&gt;
&lt;p&gt;
LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions. (arXiv:2304.14402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;LaMini-LM&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#25351;&#20196;&#30340;&#22810;&#26679;&#24615;&#21387;&#32553;&#27169;&#22411;&#32676;&#38598;&#65292;&#20174;&#25351;&#20196;&#24494;&#35843;&#36807;&#30340;LLMs&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#26356;&#23567;&#30340;&#27169;&#22411;&#20013;&#65292;&#20854;&#22312;15&#20010;&#19981;&#21516;&#30340;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#30340;&#34920;&#29616;&#30456;&#24403;&#65292;&#20294;&#20307;&#31215;&#32422;&#23567;&#20102;10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#26159;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#36164;&#28304;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20174;&#24494;&#35843;&#36807;&#30340;LLMs&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#26356;&#23567;&#30340;&#27169;&#22411;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20180;&#32454;&#24320;&#21457;&#20102;&#19968;&#32452;258&#19975;&#20221;&#22522;&#20110;&#29616;&#26377;&#21644;&#26032;&#29983;&#25104;&#30340;&#25351;&#20196;&#12290;&#38500;&#20102;&#35268;&#27169;&#22823;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#24191;&#27867;&#30340;&#35805;&#39064;&#65292;&#20197;&#30830;&#20445;&#25351;&#20196;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#20351;&#29992;gpt-3.5-turbo&#20026;&#36825;&#20123;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25351;&#20196;&#26469;&#24494;&#35843;&#22810;&#20010;&#27169;&#22411;&#65292;&#21363;LaMini-LM&#65292;&#21253;&#25324;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#31995;&#21015;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#65288;&#22312;15&#20010;&#19981;&#21516;&#30340;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#65289;&#21644;&#25163;&#21160;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;LaMini-LM&#19982;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#30340;&#34920;&#29616;&#30456;&#24403;&#65292;&#32780;&#19988;&#20307;&#31215;&#32422;&#23567;&#20102;10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with instruction finetuning demonstrate superior generative capabilities. However, these models are resource intensive. To alleviate this issue, we explore distilling knowledge from instruction-tuned LLMs to much smaller ones. To this end, we carefully develop a large set of 2.58M instructions based on both existing and newly-generated instructions. In addition to being sizeable, we design our instructions to cover a broad set of topics to ensure. A thorough investigation of our instruction data demonstrate their diversity, and we generate responses for these instructions using gpt-3.5-turbo. We then exploit the instructions to tune a host of models, dubbed LaMini-LM, of varying sizes, both from the encoder-decoder as well as the decoder-only families. We evaluate our models both automatically (on 15 different NLP benchmarks) and manually. Results show that our proposed LaMini-LM are on par with competitive baselines while being nearly 10 times smaller in s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#22833;&#35823;&#65292;&#24402;&#32435;&#24182;&#30830;&#23450;&#20854;&#22833;&#36133;&#30340;&#21407;&#22240;&#31867;&#22411;&#21644;&#20851;&#38190;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#28508;&#22312;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10513</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;ChatGPT&#26080;&#27861;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Does ChatGPT Fall Short in Answering Questions Faithfully?. (arXiv:2304.10513v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10513
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#22833;&#35823;&#65292;&#24402;&#32435;&#24182;&#30830;&#23450;&#20854;&#22833;&#36133;&#30340;&#21407;&#22240;&#31867;&#22411;&#21644;&#20851;&#38190;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#28508;&#22312;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;ChatGPT&#65292;&#23637;&#31034;&#20986;&#23545;&#20154;&#31867;&#29983;&#27963;&#21508;&#26041;&#38754;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;ChatGPT&#22312;&#35802;&#23454;&#24615;&#31561;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#20197;&#38382;&#31572;&#31995;&#32479;&#20026;&#20195;&#34920;&#24212;&#29992;&#31243;&#24207;&#65292;&#25105;&#20204;&#35797;&#22270;&#20102;&#35299;&#20026;&#20160;&#20040;ChatGPT&#22312;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#26377;&#25152;&#19981;&#36275;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35797;&#22270;&#20998;&#26512;ChatGPT&#22312;&#22797;&#26434;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#22833;&#36133;&#30340;&#21407;&#22240;&#65292;&#24182;&#30830;&#23450;&#19982;&#36825;&#20123;&#22833;&#36133;&#26377;&#20851;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;ChatGPT&#30340;&#22833;&#36133;&#24402;&#20026;&#22235;&#31181;&#31867;&#22411;&#65306;&#29702;&#35299;&#12289;&#20107;&#23454;&#24615;&#12289;&#20855;&#20307;&#24615;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#19982;QA&#22833;&#36133;&#26377;&#20851;&#30340;&#19977;&#20010;&#20851;&#38190;&#33021;&#21147;&#65306;&#30693;&#35782;&#35760;&#24518;&#12289;&#30693;&#35782;&#20851;&#32852;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22260;&#32469;&#36825;&#20123;&#33021;&#21147;&#30340;&#23454;&#39564;&#65292;&#24182;&#25552;&#20986;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21521;&#27169;&#22411;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#22806;&#37096;&#30693;&#35782;&#12289;&#32473;&#20104;&#25552;&#31034;&#26469;&#24110;&#21161;&#23427;&#32858;&#28966;&#24182;&#21152;&#24378;&#20851;&#38190;&#33021;&#21147;&#65292;&#36825;&#37117;&#26377;&#21161;&#20110;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models, such as ChatGPT, have demonstrated significant potential to impact various aspects of human life. However, ChatGPT still faces challenges in aspects like faithfulness. Taking question answering as a representative application, we seek to understand why ChatGPT falls short in answering questions faithfully. To address this question, we attempt to analyze the failures of ChatGPT in complex open-domain question answering and identifies the abilities under the failures. Specifically, we categorize ChatGPT's failures into four types: comprehension, factualness, specificity, and inference. We further pinpoint three critical abilities associated with QA failures: knowledge memorization, knowledge association, and knowledge reasoning. Additionally, we conduct experiments centered on these abilities and propose potential approaches to enhance faithfulness. The results indicate that furnishing the model with fine-grained external knowledge, hints for
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#19982;&#33258;&#36523;&#23545;&#35805;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#36718;&#32842;&#22825;&#35821;&#26009;&#24211;&#65292;&#24182;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26469;&#22686;&#24378;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;Baize&#65292;&#22312;&#26368;&#23567;&#21270;&#28508;&#22312;&#39118;&#38505;&#30340;&#25252;&#26639;&#19979;&#65292;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01196</link><description>&lt;p&gt;
Baize:&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#23545;&#35805;&#25968;&#25454;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#30340;&#24320;&#28304;&#32842;&#22825;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data. (arXiv:2304.01196v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01196
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#19982;&#33258;&#36523;&#23545;&#35805;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#36718;&#32842;&#22825;&#35821;&#26009;&#24211;&#65292;&#24182;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26469;&#22686;&#24378;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;Baize&#65292;&#22312;&#26368;&#23567;&#21270;&#28508;&#22312;&#39118;&#38505;&#30340;&#25252;&#26639;&#19979;&#65292;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#27169;&#22411;&#65292;&#22914;ChatGPT&#65292;&#23637;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#20247;&#22810;&#39046;&#22495;&#24471;&#21040;&#36805;&#36895;&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#21482;&#33021;&#36890;&#36807;&#21463;&#38480;&#21046;&#30340;API&#36827;&#34892;&#35775;&#38382;&#65292;&#20174;&#32780;&#21046;&#36896;&#20102;&#26032;&#30340;&#30740;&#31350;&#21644;&#39046;&#22495;&#36827;&#23637;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#19982;&#33258;&#36523;&#23545;&#35805;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#36718;&#32842;&#22825;&#35821;&#26009;&#24211;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26469;&#22686;&#24378;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#12290;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;Baize&#65292;&#22312;&#26368;&#23567;&#21270;&#28508;&#22312;&#39118;&#38505;&#30340;&#25252;&#26639;&#19979;&#65292;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;Baize&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#20165;&#29992;&#20110;&#30740;&#31350;&#30446;&#30340;&#65292;&#21487;&#22312;https://github.com/project-baize/baize&#36827;&#34892;&#19979;&#36733;&#12290;&#22312;&#32447;&#28436;&#31034;&#20063;&#21487;&#22312;https://huggingface.co/spaces/project-baize/baize-lora-7B&#36827;&#34892;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. The Baize models and data are released for research purposes only at https://github.com/project-baize/baize. An online demo is also available at https://huggingface.co/spaces/project-baize/baize-lora-7B.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GPTEval&#65292;&#19968;&#20010;&#21033;&#29992;&#38142;&#24335;&#24605;&#32771;&#21644;&#24418;&#24335;&#22635;&#20805;&#35780;&#20215;NLG&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#65292;GPTEval&#32467;&#21512;GPT-4&#21462;&#24471;&#20102;0.514&#30340;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#65292;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16634</link><description>&lt;p&gt;
GPTEval&#65306;&#20351;&#29992;GPT-4&#21644;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#26469;&#35780;&#20272;NLG
&lt;/p&gt;
&lt;p&gt;
GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment. (arXiv:2303.16634v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GPTEval&#65292;&#19968;&#20010;&#21033;&#29992;&#38142;&#24335;&#24605;&#32771;&#21644;&#24418;&#24335;&#22635;&#20805;&#35780;&#20215;NLG&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#65292;GPTEval&#32467;&#21512;GPT-4&#21462;&#24471;&#20102;0.514&#30340;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#65292;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#31995;&#32479;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#24456;&#38590;&#36827;&#34892;&#33258;&#21160;&#27979;&#37327;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;BLEU&#21644;ROUGE&#24050;&#34987;&#35777;&#26126;&#22312;&#38656;&#35201;&#21019;&#36896;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#30456;&#23545;&#36739;&#20302;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#26080;&#21442;&#32771;&#30340;NLG&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#36825;&#20123;&#27169;&#22411;&#36866;&#29992;&#20110;&#32570;&#20047;&#20154;&#31867;&#21442;&#32771;&#30340;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#20173;&#28982;&#27604;&#20013;&#31561;&#35268;&#27169;&#30340;&#31070;&#32463;&#35780;&#20272;&#22120;&#30340;&#20154;&#31867;&#23545;&#24212;&#24230;&#20302;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPTEval&#65292;&#19968;&#20010;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#65288;CoT&#65289;&#21644;&#24418;&#24335;&#22635;&#20805;&#33539;&#24335;&#26469;&#35780;&#20272;NLG&#36755;&#20986;&#36136;&#37327;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#38024;&#23545;&#20004;&#20010;&#29983;&#25104;&#20219;&#21153;&#65292;&#25991;&#26412;&#25688;&#35201;&#21644;&#23545;&#35805;&#29983;&#25104;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#65292;GPTEval&#32467;&#21512;GPT-4&#20316;&#20026;&#39592;&#24178;&#27169;&#22411;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;0.514&#30340;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#65292;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present GPTEval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that GPTEval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperform
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#31471;&#21040;&#31471;&#23545;&#35805;&#27169;&#22411;- InterviewBot&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#19982;&#36755;&#20837;&#30340;&#21382;&#21490;&#23545;&#35805;&#21644;&#23450;&#21046;&#20027;&#39064;&#38598;&#25104;&#21040;&#21516;&#19968;&#23884;&#20837;&#31354;&#38388;&#20013;&#26469;&#35780;&#20272;&#22806;&#22269;&#23398;&#29983;&#30003;&#35831;&#32654;&#22269;&#22823;&#23398;&#30340;&#23398;&#26415;&#21644;&#25991;&#21270;&#20934;&#22791;&#24773;&#20917;&#12290;&#21516;&#26102;&#65292;&#20026;&#20811;&#26381;&#22522;&#20110;&#21464;&#24418;&#37329;&#21018;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#36755;&#20837;/&#36755;&#20986;&#22823;&#23567;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20851;&#27880;&#21644;&#20027;&#39064;&#23384;&#20648;&#20004;&#31181;&#26032;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#22312;&#32479;&#35745;&#21644;&#21160;&#24577;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#27969;&#30021;&#24615;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#39640;&#24230;&#28385;&#24847;&#12290;</title><link>http://arxiv.org/abs/2303.15049</link><description>&lt;p&gt;
InterviewBot&#65306;&#38754;&#21521;&#22823;&#23398;&#25307;&#29983;&#32771;&#35797;&#30340;&#23454;&#26102;&#31471;&#21040;&#31471;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
InterviewBot: Real-Time End-to-End Dialogue System to Interview Students for College Admission. (arXiv:2303.15049v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15049
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#31471;&#21040;&#31471;&#23545;&#35805;&#27169;&#22411;- InterviewBot&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#19982;&#36755;&#20837;&#30340;&#21382;&#21490;&#23545;&#35805;&#21644;&#23450;&#21046;&#20027;&#39064;&#38598;&#25104;&#21040;&#21516;&#19968;&#23884;&#20837;&#31354;&#38388;&#20013;&#26469;&#35780;&#20272;&#22806;&#22269;&#23398;&#29983;&#30003;&#35831;&#32654;&#22269;&#22823;&#23398;&#30340;&#23398;&#26415;&#21644;&#25991;&#21270;&#20934;&#22791;&#24773;&#20917;&#12290;&#21516;&#26102;&#65292;&#20026;&#20811;&#26381;&#22522;&#20110;&#21464;&#24418;&#37329;&#21018;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#36755;&#20837;/&#36755;&#20986;&#22823;&#23567;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20851;&#27880;&#21644;&#20027;&#39064;&#23384;&#20648;&#20004;&#31181;&#26032;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#22312;&#32479;&#35745;&#21644;&#21160;&#24577;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#27969;&#30021;&#24615;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#39640;&#24230;&#28385;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;InterviewBot&#65292;&#23427;&#21160;&#24577;&#23558;&#20250;&#35805;&#21382;&#21490;&#21644;&#23450;&#21046;&#20027;&#39064;&#38598;&#25104;&#21040;&#19968;&#33268;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#20197;&#36827;&#34892;10&#20998;&#38047;&#30340;&#28151;&#21512;&#39046;&#22495;&#65288;&#24320;&#25918;&#21644;&#23553;&#38381;&#65289;&#23545;&#35805;&#65292;&#20197;&#35780;&#20272;&#22806;&#22269;&#23398;&#29983;&#30003;&#35831;&#32654;&#22269;&#22823;&#23398;&#30340;&#23398;&#26415;&#21644;&#25991;&#21270;&#20934;&#22791;&#24773;&#20917;&#12290;&#20026;&#26500;&#24314;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#31471;&#21040;&#31471;&#23545;&#35805;&#27169;&#22411;&#65292;&#25105;&#20204;&#33258;&#21160;&#36716;&#24405;&#20102;7,361&#20010;&#20154;&#23545;&#20154;&#30340;&#38754;&#35797;&#38899;&#39057;&#35760;&#24405;&#65292;&#20854;&#20013;440&#20010;&#36827;&#34892;&#20102;&#25163;&#21160;&#32416;&#27491;&#20197;&#36827;&#34892;&#24494;&#35843;&#21644;&#35780;&#20272;&#12290;&#20026;&#20102;&#20811;&#26381;&#22522;&#20110;&#21464;&#24418;&#37329;&#21018;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#36755;&#20837;/&#36755;&#20986;&#22823;&#23567;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;&#19978;&#19979;&#25991;&#20851;&#27880;&#21644;&#20027;&#39064;&#23384;&#20648;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#30456;&#20851;&#21644;&#19968;&#33268;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#27169;&#22411;&#22312;&#32479;&#35745;&#19978;&#34987;&#27979;&#35797;&#65292;&#36890;&#36807;&#23558;&#20854;&#21709;&#24212;&#19982;&#38754;&#35797;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#21160;&#24577;&#22320;&#36992;&#35831;&#19987;&#19994;&#38754;&#35797;&#23448;&#21644;&#21508;&#31181;&#23398;&#29983;&#19982;&#20854;&#23454;&#26102;&#20132;&#20114;&#65292;&#22312;&#27969;&#30021;&#24615;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#26041;&#38754;&#21457;&#29616;&#20854;&#38750;&#24120;&#20196;&#20154;&#28385;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the InterviewBot that dynamically integrates conversation history and customized topics into a coherent embedding space to conduct 10 mins hybrid-domain (open and closed) conversations with foreign students applying to U.S. colleges for assessing their academic and cultural readiness. To build a neural-based end-to-end dialogue model, 7,361 audio recordings of human-to-human interviews are automatically transcribed, where 440 are manually corrected for finetuning and evaluation. To overcome the input/output size limit of a transformer-based encoder-decoder model, two new methods are proposed, context attention and topic storing, allowing the model to make relevant and consistent interactions. Our final model is tested both statistically by comparing its responses to the interview data and dynamically by inviting professional interviewers and various students to interact with it in real-time, finding it highly satisfactory in fluency and context awareness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26799;&#24230;&#33258;&#30001;&#24352;&#37327;&#30340;&#20248;&#21270;&#22120;&#65292;&#21487;&#20197;&#27450;&#39575;&#22312;&#32447;&#32763;&#35793;&#24037;&#20855;&#20135;&#29983;&#38169;&#35823;&#30340;&#32763;&#35793;&#65292;&#36825;&#24433;&#21709;&#20102;&#29992;&#25143;&#23398;&#20064;&#35821;&#35328;&#30340;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2303.10974</link><description>&lt;p&gt;
&#23558;&#24744;&#30340;&#26080;&#24847;&#20041;&#32763;&#35793;&#65306;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Translate your gibberish: black-box adversarial attack on machine translation systems. (arXiv:2303.10974v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26799;&#24230;&#33258;&#30001;&#24352;&#37327;&#30340;&#20248;&#21270;&#22120;&#65292;&#21487;&#20197;&#27450;&#39575;&#22312;&#32447;&#32763;&#35793;&#24037;&#20855;&#20135;&#29983;&#38169;&#35823;&#30340;&#32763;&#35793;&#65292;&#36825;&#24433;&#21709;&#20102;&#29992;&#25143;&#23398;&#20064;&#35821;&#35328;&#30340;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24191;&#27867;&#37096;&#32626;&#20110;&#24037;&#19994;&#23610;&#24230;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#26368;&#24120;&#35265;&#30340;&#29992;&#36884;&#26159;&#33258;&#21160;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#27450;&#39575;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#32763;&#35793;&#24037;&#20855;&#22312;&#20420;&#35821;&#21644;&#33521;&#35821;&#20043;&#38388;&#30340;&#32763;&#35793;&#20219;&#21153;&#20013;&#12290;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26799;&#24230;&#33258;&#30001;&#24352;&#37327;&#30340;&#20248;&#21270;&#22120;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#22312;&#32447;&#32763;&#35793;&#24037;&#20855;&#65292;&#22914; Google&#12289;DeepL &#21644; Yandex&#65292;&#37117;&#21487;&#33021;&#20026;&#26080;&#24847;&#20041;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#26597;&#35810;&#20135;&#29983;&#38169;&#35823;&#25110;&#20882;&#29359;&#24615;&#32763;&#35793;&#65292;&#24182;&#25298;&#32477;&#32763;&#35793;&#30475;&#20284;&#33391;&#24615;&#30340;&#36755;&#20837;&#30701;&#35821;&#12290;&#36825;&#31181;&#28431;&#27934;&#21487;&#33021;&#20250;&#24178;&#25200;&#23545;&#19968;&#31181;&#26032;&#35821;&#35328;&#30340;&#29702;&#35299;&#65292;&#19988;&#22312;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#26102;&#31616;&#21333;&#22320;&#24694;&#21270;&#29992;&#25143;&#30340;&#20307;&#39564;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#36825;&#20123;&#24037;&#20855;&#36827;&#34892;&#39069;&#22806;&#30340;&#25913;&#36827;&#20197;&#24314;&#31435;&#26356;&#22909;&#30340;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are deployed widely in natural language processing tasks on the industrial scale, and perhaps the most often they are used as compounds of automatic machine translation systems. In this work, we present a simple approach to fool state-of-the-art machine translation tools in the task of translation from Russian to English and vice versa. Using a novel black-box gradient-free tensor-based optimizer, we show that many online translation tools, such as Google, DeepL, and Yandex, may both produce wrong or offensive translations for nonsensical adversarial input queries and refuse to translate seemingly benign input phrases. This vulnerability may interfere with understanding a new language and simply worsen the user's experience while using machine translation systems, and, hence, additional improvements of these tools are required to establish better translation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20195;&#25968;&#35821;&#35328;&#29255;&#27573;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#19978;&#32852;&#31995;&#30340;&#26694;&#26550;&#65292;&#24182;&#20174;MATLANG&#23450;&#20041;&#20102;&#19968;&#20010;&#31526;&#21512;3-WL&#27979;&#35797;&#30340;&#35821;&#27861;&#65292;&#36827;&#32780;&#24471;&#20986;&#19968;&#20010;&#31526;&#21512;3-WL GNN&#27169;&#22411;&#30340;G$^2$N$^2$&#12290;&#27492;&#22806;&#65292;&#35821;&#27861;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#35745;&#31639;&#38271;&#24230;&#20026;&#20845;&#21450;&#20197;&#19979;&#30340;&#29615;&#21644;&#24358;&#29615;&#30340;&#20195;&#25968;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.01590</link><description>&lt;p&gt;
&#25216;&#26415;&#25253;&#21578;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#20063;&#21487;&#20197;&#21464;&#24471;&#35821;&#27861;&#21270;
&lt;/p&gt;
&lt;p&gt;
Technical report: Graph Neural Networks go Grammatical. (arXiv:2303.01590v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20195;&#25968;&#35821;&#35328;&#29255;&#27573;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#19978;&#32852;&#31995;&#30340;&#26694;&#26550;&#65292;&#24182;&#20174;MATLANG&#23450;&#20041;&#20102;&#19968;&#20010;&#31526;&#21512;3-WL&#27979;&#35797;&#30340;&#35821;&#27861;&#65292;&#36827;&#32780;&#24471;&#20986;&#19968;&#20010;&#31526;&#21512;3-WL GNN&#27169;&#22411;&#30340;G$^2$N$^2$&#12290;&#27492;&#22806;&#65292;&#35821;&#27861;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#35745;&#31639;&#38271;&#24230;&#20026;&#20845;&#21450;&#20197;&#19979;&#30340;&#29615;&#21644;&#24358;&#29615;&#30340;&#20195;&#25968;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#19968;&#20010;&#20195;&#25968;&#35821;&#35328;&#30340;&#19968;&#20010;&#29255;&#27573;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24418;&#24335;&#19978;&#32852;&#31995;&#36215;&#26469;&#12290;&#23427;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#65288;CFG&#65289;&#65292;&#23558;&#20195;&#25968;&#25805;&#20316;&#32452;&#32455;&#25104;&#21487;&#20197;&#32763;&#35793;&#20026;GNN&#23618;&#27169;&#22411;&#30340;&#29983;&#25104;&#35268;&#21017;&#12290;&#30001;&#20110;&#30452;&#25509;&#20174;&#35821;&#35328;&#27966;&#29983;&#20986;&#30340;CFG&#30340;&#35268;&#21017;&#21644;&#21464;&#37327;&#21253;&#21547;&#20887;&#20313;&#65292;&#22240;&#27492;&#20171;&#32461;&#20102;&#19968;&#31181;&#35821;&#27861;&#31616;&#21270;&#26041;&#26696;&#65292;&#20351;&#24471;&#23558;&#20854;&#32763;&#35793;&#20026;GNN&#23618;&#25104;&#20026;&#21487;&#33021;&#12290;&#24212;&#29992;&#36825;&#31181;&#31574;&#30053;&#65292;&#20174;MATLANG&#23450;&#20041;&#20102;&#19968;&#20010;&#31526;&#21512;&#31532;&#19977;&#38454;Weisfeiler-Lehman&#65288;3-WL&#65289;&#27979;&#35797;&#35201;&#27714;&#30340;&#35821;&#27861;&#12290;&#20174;&#36825;&#20010;3-WL CFG&#20013;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#35777;&#26126;&#31526;&#21512;3-WL GNN&#27169;&#22411;&#30340;G$^2$N$^2$&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#35821;&#27861;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#35745;&#31639;&#38271;&#24230;&#20026;&#20845;&#21450;&#20197;&#19979;&#30340;&#29615;&#21644;&#24358;&#29615;&#30340;&#20195;&#25968;&#20844;&#24335;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;3-WL&#30340;&#35745;&#25968;&#33021;&#21147;&#12290;&#22810;&#20010;&#23454;&#39564;&#35777;&#26126;&#65292;G$^2$N$^2$&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#35201;&#27604;&#20854;&#20182;3-WL GNN&#26356;&#20026;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a framework to formally link a fragment of an algebraic language to a Graph Neural Network (GNN). It relies on Context Free Grammars (CFG) to organise algebraic operations into generative rules that can be translated into a GNN layer model. Since the rules and variables of a CFG directly derived from a language contain redundancies, a grammar reduction scheme is presented making tractable the translation into a GNN layer. Applying this strategy, a grammar compliant with the third-order Weisfeiler-Lehman (3-WL) test is defined from MATLANG. From this 3-WL CFG, we derive a provably 3-WL GNN model called G$^2$N$^2$. Moreover, this grammatical approach allows us to provide algebraic formulas to count the cycles of length up to six and chordal cycles at the edge level, which enlightens the counting power of 3-WL. Several experiments illustrate that G$^2$N$^2$ efficiently outperforms other 3-WL GNNs on many downstream tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#21152;&#26435;&#37319;&#26679;&#31574;&#30053;&#29992;&#20110;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#39640;&#39057;&#29575;&#35789;&#27719;&#23548;&#33268;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.14225</link><description>&lt;p&gt;
&#22522;&#20110;&#21152;&#26435;&#37319;&#26679;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weighted Sampling for Masked Language Modeling. (arXiv:2302.14225v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#21152;&#26435;&#37319;&#26679;&#31574;&#30053;&#29992;&#20110;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#39640;&#39057;&#29575;&#35789;&#27719;&#23548;&#33268;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;MLM&#20013;&#26631;&#20934;&#30340;&#38543;&#26426;&#36974;&#34109;&#31574;&#30053;&#20250;&#23548;&#33268;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20559;&#21521;&#20110;&#39640;&#39057;&#29575;&#30340;&#35789;&#27719;&#12290;&#32597;&#35265;&#35789;&#27719;&#30340;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#36739;&#24046;&#65292;PLMs&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#39057;&#29575;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#35789;&#39057;&#21644;&#35757;&#32451;&#25439;&#22833;&#30340;&#21152;&#26435;&#36974;&#34109;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#20004;&#31181;&#31574;&#30053;&#24212;&#29992;&#20110;BERT&#65292;&#24182;&#33719;&#24471;&#20102;&#21152;&#26435;&#37319;&#26679;BERT&#65288;WSBERT&#65289;&#12290;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#22522;&#20934;&#65288;STS&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;WSBERT&#26174;&#30528;&#25913;&#36827;&#20102;BERT&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;&#23558;WSBERT&#19982;&#26657;&#20934;&#26041;&#27861;&#21644;&#25552;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#21477;&#23376;&#23884;&#20837;&#12290;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#22312;GLUE&#22522;&#20934;&#19978;&#24494;&#35843;WSBERT&#65292;&#24182;&#34920;&#26126;&#21152;&#26435;&#37319;&#26679;&#20063;&#25913;&#21892;&#20102;&#39592;&#24178;PLM&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#24182;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Language Modeling (MLM) is widely used to pretrain language models. The standard random masking strategy in MLM causes the pre-trained language models (PLMs) to be biased toward high-frequency tokens. Representation learning of rare tokens is poor and PLMs have limited performance on downstream tasks. To alleviate this frequency bias issue, we propose two simple and effective Weighted Sampling strategies for masking tokens based on the token frequency and training loss. We apply these two strategies to BERT and obtain Weighted-Sampled BERT (WSBERT). Experiments on the Semantic Textual Similarity benchmark (STS) show that WSBERT significantly improves sentence embeddings over BERT. Combining WSBERT with calibration methods and prompt learning further improves sentence embeddings. We also investigate fine-tuning WSBERT on the GLUE benchmark and show that Weighted Sampling also improves the transfer learning capability of the backbone PLM. We further analyze and provide insights in
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;BabelCode&#26694;&#26550;&#21644;Translating Python Programming Puzzles&#65288;TP3&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#25506;&#35752;&#20102;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;14&#31181;&#32534;&#31243;&#35821;&#35328;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#24179;&#34913;&#20998;&#24067;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2302.01973</link><description>&lt;p&gt;
&#27979;&#37327;&#32534;&#31243;&#35821;&#35328;&#20998;&#24067;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Measuring The Impact Of Programming Language Distribution. (arXiv:2302.01973v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01973
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;BabelCode&#26694;&#26550;&#21644;Translating Python Programming Puzzles&#65288;TP3&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#25506;&#35752;&#20102;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;14&#31181;&#32534;&#31243;&#35821;&#35328;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#24179;&#34913;&#20998;&#24067;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#29992;&#20110;&#35780;&#20272;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#21482;&#38598;&#20013;&#22312;&#24456;&#23569;&#30340;&#19968;&#37096;&#20998;&#32534;&#31243;&#35821;&#35328;&#19978;&#65292;&#19981;&#21253;&#25324;&#35768;&#22810;&#27969;&#34892;&#30340;&#35821;&#35328;&#65292;&#20363;&#22914;Go&#25110;Rust&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BabelCode&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#25191;&#34892;&#30340;&#35780;&#20272;&#20219;&#20309;&#35821;&#35328;&#20013;&#30340;&#20219;&#20309;&#22522;&#20934;&#27979;&#35797;&#12290;BabelCode&#20351;&#24471;&#21487;&#20197;&#23545;&#27169;&#22411;&#30340;&#20869;&#23384;&#12289;&#36816;&#34892;&#26102;&#38388;&#21644;&#21333;&#20010;&#27979;&#35797;&#26696;&#20363;&#32467;&#26524;&#36827;&#34892;&#26032;&#30340;&#23450;&#24615;&#24615;&#33021;&#35843;&#26597;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;Translating Python Programming Puzzles&#65288;TP3&#65289;&#30340;&#26032;&#20195;&#30721;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;Python Programming Puzzles&#65288;Schuster&#31561;&#20154;&#65292;2021&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#28041;&#21450;&#23558;&#19987;&#23478;&#32423;Python&#20989;&#25968;&#32763;&#35793;&#25104;&#20219;&#20309;&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;BabelCode&#21644;TP3&#22522;&#20934;&#27979;&#35797;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#24179;&#34913;14&#31181;&#35821;&#35328;&#30340;&#20998;&#24067;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#24179;&#34913;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#24179;&#22343;&#32780;&#35328;&#65292;&#30456;&#23545;&#20110;&#19981;&#24179;&#34913;&#20998;&#24067;&#30340;&#24773;&#20917;&#65292;&#25152;&#26377;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;$pass@k$&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;12.34%&#12290;
&lt;/p&gt;
&lt;p&gt;
Current benchmarks for evaluating neural code models focus on only a small subset of programming languages, excluding many popular languages such as Go or Rust. To ameliorate this issue, we present the BabelCode framework for execution-based evaluation of any benchmark in any language. BabelCode enables new investigations into the qualitative performance of models' memory, runtime, and individual test case results. Additionally, we present a new code translation dataset called Translating Python Programming Puzzles (TP3) from the Python Programming Puzzles (Schuster et al. 2021) benchmark that involves translating expert-level python functions to any language. With both BabelCode and the TP3 benchmark, we investigate if balancing the distributions of 14 languages in a training dataset improves a large language model's performance on low-resource languages. Training a model on a balanced corpus results in, on average, 12.34% higher $pass@k$ across all tasks and languages compared to the
&lt;/p&gt;</description></item><item><title>REPLUG&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#22312;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#36755;&#20837;&#21069;&#28155;&#21152;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#26469;&#22686;&#24378;&#27169;&#22411;&#65292;&#21516;&#26102;&#21487;&#20197;&#36890;&#36807;LM&#30417;&#30563;&#26816;&#32034;&#27169;&#22411;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12652</link><description>&lt;p&gt;
Replug&#65306;&#26816;&#32034;&#22686;&#24378;&#30340;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Replug: Retrieval-augmented black-box language models. (arXiv:2301.12652v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12652
&lt;/p&gt;
&lt;p&gt;
REPLUG&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#22312;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#36755;&#20837;&#21069;&#28155;&#21152;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#26469;&#22686;&#24378;&#27169;&#22411;&#65292;&#21516;&#26102;&#21487;&#20197;&#36890;&#36807;LM&#30417;&#30563;&#26816;&#32034;&#27169;&#22411;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; REPLUG&#65292;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#35270;&#20026;&#40657;&#30418;&#65292;&#24182;&#29992;&#21487;&#35843;&#25972;&#30340;&#26816;&#32034;&#27169;&#22411;&#22686;&#24378;&#23427;&#12290;&#19982;&#20197;&#21069;&#36890;&#36807;&#29305;&#27530;&#30340;&#20132;&#21449;&#20851;&#27880;&#26426;&#21046;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#23545;&#26816;&#32034;&#25991;&#26412;&#36827;&#34892;&#32534;&#30721;&#30340;&#26816;&#32034;&#22686;&#24378;&#22411;LM&#19981;&#21516;&#65292;REPLUG&#20165;&#23558;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#21069;&#32622;&#21040;&#20923;&#32467;&#30340;&#40657;&#30418;LM&#36755;&#20837;&#20013;&#12290;&#36825;&#31181;&#31616;&#21333;&#30340;&#35774;&#35745;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#26816;&#32034;&#21644;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26174;&#31034;LM&#21487;&#20197;&#29992;&#20110;&#30417;&#30563;&#26816;&#32034;&#27169;&#22411;&#65292;&#35813;&#26816;&#32034;&#27169;&#22411;&#21487;&#20197;&#25214;&#21040;&#24110;&#21161;LM&#36827;&#34892;&#26356;&#22909;&#39044;&#27979;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#35843;&#25972;&#20102;&#26816;&#32034;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;REPLUG&#21487;&#20197;&#20351;GPT-3&#65288;175B&#65289;&#30340;&#35821;&#35328;&#24314;&#27169;&#24615;&#33021;&#25552;&#39640;6.3&#65285;&#65292;&#21516;&#26102;&#20351;Codex&#22312;&#20116;&#27425;&#27979;&#35797;MMLU&#26102;&#24615;&#33021;&#25552;&#39640;5.1&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;-POI&#21305;&#37197;&#26041;&#27861;MGeo&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#22320;&#29702;&#20449;&#24687;&#19978;&#19979;&#25991;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.04283</link><description>&lt;p&gt;
MGeo&#65306;&#22810;&#27169;&#24577;&#22320;&#29702;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MGeo: Multi-Modal Geographic Pre-Training Method. (arXiv:2301.04283v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;-POI&#21305;&#37197;&#26041;&#27861;MGeo&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#22320;&#29702;&#20449;&#24687;&#19978;&#19979;&#25991;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#19982;&#20852;&#36259;&#28857; (POI) &#21305;&#37197;&#26159;&#22522;&#20110;&#20301;&#32622;&#30340;&#26381;&#21153; (LBS) &#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#65288;&#22914;&#23548;&#33322;&#22320;&#22270;&#65289;&#65292;&#23427;&#23558;&#29992;&#25143;&#30340;&#24847;&#22270;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#22320;&#29702;&#20449;&#24687;&#30456;&#36830;&#25509;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411; (PTM) &#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#36890;&#29992;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340; PTM &#24182;&#27809;&#26377;&#36275;&#22815;&#30340;&#22320;&#29702;&#30693;&#35782;&#29992;&#20110;&#26597;&#35810;-POI &#21305;&#37197;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#30456;&#20851;&#25991;&#29486;&#23581;&#35797;&#26681;&#25454;&#22320;&#29702;&#30456;&#20851;&#35821;&#26009;&#24211;&#37319;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#12290;&#20294;&#26159;&#65292;&#26597;&#35810;&#36890;&#24120;&#21253;&#21547;&#22810;&#20010;&#22320;&#29702;&#23545;&#35937;&#30340;&#25552;&#21450;&#65292;&#27604;&#22914;&#38468;&#36817;&#30340;&#36947;&#36335;&#21644;&#21033;&#30410;&#21306;&#22495;&#65288;ROIs&#65289;&#12290;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#22320;&#29702;&#23545;&#35937;&#21450;&#20854;&#20851;&#31995;&#34987;&#31216;&#20026;&#22320;&#29702;&#19978;&#19979;&#25991;&#65288;GC&#65289;,&#23545;&#20110;&#26816;&#32034;&#26368;&#30456;&#20851;&#30340; POI&#65292;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#21333;&#27169;&#24577; PTM &#24456;&#38590;&#21033;&#29992;&#37325;&#35201;&#30340;&#22320;&#29702;&#19978;&#19979;&#25991;&#65292;&#22240;&#27492;&#24615;&#33021;&#21463;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;-POI &#21305;&#37197;&#26041;&#27861;&#8220;&#22810;&#27169;&#24577;&#22320;&#29702;&#35821;&#35328;&#27169;&#22411;&#8221;(MGeo)&#65292;&#23427;&#21253;&#25324;......
&lt;/p&gt;
&lt;p&gt;
As a core task in location-based services (LBS) (e.g., navigation maps), query and point of interest (POI) matching connects users' intent with real-world geographic information. Recently, pre-trained models (PTMs) have made advancements in many natural language processing (NLP) tasks. Generic text-based PTMs do not have enough geographic knowledge for query-POI matching. To overcome this limitation, related literature attempts to employ domain-adaptive pre-training based on geo-related corpus. However, a query generally contains mentions of multiple geographic objects, such as nearby roads and regions of interest (ROIs). The geographic context (GC), i.e., these diverse geographic objects and their relationships, is therefore pivotal to retrieving the most relevant POI. Single-modal PTMs can barely make use of the important GC and therefore have limited performance. In this work, we propose a novel query-POI matching method Multi-modal Geographic language model (MGeo), which comprises 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;"&#24320;&#25918;&#39046;&#22495;"&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#22120;&#22312;&#27492;&#24773;&#22659;&#19979;&#24615;&#33021;&#19979;&#38477;&#20005;&#37325;&#65292;&#20294;&#36827;&#34892;&#39069;&#22806;&#30340;&#24320;&#25918;&#39046;&#22495;&#30340;&#35757;&#32451;&#21487;&#20197;&#38477;&#20302;&#23545;&#19981;&#23436;&#32654;&#26816;&#32034;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.10526</link><description>&lt;p&gt;
&#38754;&#21521;&#24320;&#25918;&#39046;&#22495;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Towards multi-document summarization in the open-domain. (arXiv:2212.10526v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;"&#24320;&#25918;&#39046;&#22495;"&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#22120;&#22312;&#27492;&#24773;&#22659;&#19979;&#24615;&#33021;&#19979;&#38477;&#20005;&#37325;&#65292;&#20294;&#36827;&#34892;&#39069;&#22806;&#30340;&#24320;&#25918;&#39046;&#22495;&#30340;&#35757;&#32451;&#21487;&#20197;&#38477;&#20302;&#23545;&#19981;&#23436;&#32654;&#26816;&#32034;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25991;&#26723;&#25688;&#35201;(MDS)&#36890;&#24120;&#20551;&#35774;&#25552;&#20379;&#19968;&#32452;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#20294;&#26159;&#65292;&#36825;&#20010;&#25991;&#26723;&#38598;&#36890;&#24120;&#26159;&#25968;&#25454;&#38598;&#31574;&#21010;&#36807;&#31243;&#30340;&#20135;&#29289;&#65307;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#19981;&#19968;&#23450;&#21487;&#29992;&#65292;&#38656;&#35201;&#26681;&#25454;&#20449;&#24687;&#38656;&#27714;&#65292;&#21363;&#38382;&#39064;&#25110;&#20027;&#39064;&#38472;&#36848;&#36827;&#34892;&#26816;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#24418;&#24335;&#21270;&#20219;&#21153;&#24182;&#20351;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#26816;&#32034;&#22120;&#21644;&#25688;&#35201;&#22120;&#26469;&#24341;&#23548;&#36825;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#8220;&#24320;&#25918;&#39046;&#22495;&#8221;&#35774;&#32622;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#65306;(1)&#21363;&#20351;&#26816;&#32034;&#24615;&#33021;&#36739;&#39640;&#65292;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#22120;&#22312;&#24212;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#26102;&#20063;&#20250;&#22823;&#24133;&#38477;&#20302;&#24615;&#33021;;(2)&#22312;&#24320;&#25918;&#39046;&#22495;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#21487;&#20197;&#38477;&#20302;&#23545;&#19981;&#23436;&#32654;&#26816;&#32034;&#30340;&#25935;&#24863;&#24615;&#65292;(3)&#25688;&#35201;&#22120;&#23545;&#26816;&#32034;&#37325;&#22797;&#25991;&#26723;&#21644;&#26816;&#32034;&#25991;&#26723;&#30340;&#39034;&#24207;&#19981;&#25935;&#24863;&#65292;&#20294;&#23545;&#20854;&#20182;&#38169;&#35823;&#65292;&#22914;&#26816;&#32034;&#26080;&#20851;&#25991;&#26723;&#30340;&#25935;&#24863;&#24615;&#36739;&#39640;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Multi-document summarization (MDS) traditionally assumes a set of topic-related documents are provided. However, this document set is often an artifact of the dataset curation process; in practice, it is not necessarily available and would need to be retrieved given an information need, i.e. a question or topic statement. We study this more challenging "open-domain" setting by formalizing the task and bootstrapping it using existing datasets, retrievers and summarizers. Via extensive experimentation, we determine that: (1) state-of-the-art summarizers suffer large reductions in performance when applied to the open-domain, even when retrieval performance is high, (2) additional training in the open-domain setting can reduce this sensitivity to imperfect retrieval, and (3) summarizers are insensitive to the retrieval of duplicate documents and the order of retrieved documents, but highly sensitive to other errors, like the retrieval of irrelevant documents. Based on our results, we provi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DePlot&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#27169;&#24577;&#36716;&#25442;&#27169;&#22359;&#65292;&#33021;&#22815;&#23558;&#22270;&#34920;&#30340;&#22270;&#20687;&#36716;&#25442;&#20026;&#32447;&#24615;&#21270;&#30340;&#34920;&#26684;&#12290;&#21033;&#29992;&#36825;&#20010;&#27169;&#22359;&#21644;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#27425;&#24615;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.10505</link><description>&lt;p&gt;
DePlot&#65306;&#21033;&#29992;&#22270;&#34920;&#36716;&#34920;&#26684;&#32763;&#35793;&#30340;&#19968;&#27425;&#24615;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
DePlot: One-shot visual language reasoning by plot-to-table translation. (arXiv:2212.10505v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DePlot&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#27169;&#24577;&#36716;&#25442;&#27169;&#22359;&#65292;&#33021;&#22815;&#23558;&#22270;&#34920;&#30340;&#22270;&#20687;&#36716;&#25442;&#20026;&#32447;&#24615;&#21270;&#30340;&#34920;&#26684;&#12290;&#21033;&#29992;&#36825;&#20010;&#27169;&#22359;&#21644;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#27425;&#24615;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#22914;&#22270;&#34920;&#21644;&#22270;&#24418;&#22312;&#20154;&#31867;&#19990;&#30028;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#29702;&#35299;&#22270;&#34920;&#38656;&#35201;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#27425;&#24615;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;&#65292;&#23558;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#30340;&#25361;&#25112;&#20998;&#35299;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#22270;&#24418;&#21040;&#25991;&#26412;&#30340;&#32763;&#35793;&#65292;&#65288;2&#65289;&#23545;&#32763;&#35793;&#21518;&#30340;&#25991;&#26412;&#36827;&#34892;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#19968;&#20010;&#31216;&#20026;DePlot&#30340;&#27169;&#24577;&#36716;&#25442;&#27169;&#22359;&#65292;&#23427;&#23558;&#22270;&#34920;&#30340;&#22270;&#20687;&#36716;&#25442;&#20026;&#32447;&#24615;&#34920;&#26684;&#12290;DePlot&#30340;&#36755;&#20986;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#21551;&#21160;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#33719;&#24471;DePlot&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#32479;&#19968;&#30340;&#20219;&#21153;&#26684;&#24335;&#21644;&#25351;&#26631;&#65292;&#23545;&#22270;&#24418;&#21040;&#34920;&#26684;&#30340;&#20219;&#21153;&#36827;&#34892;&#26631;&#20934;&#21270;&#65292;&#24182;&#22312;&#27492;&#20219;&#21153;&#19978;&#23545;DePlot&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this tas
&lt;/p&gt;</description></item><item><title>&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#22522;&#20110;&#27133;&#22635;&#20805;&#26694;&#26550;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#24403;&#21069;&#30340; TOD &#22522;&#20934;&#27979;&#35797;&#23616;&#38480;&#20110;&#20195;&#29702;&#30495;&#23454;&#19990;&#30028;&#24773;&#26223;&#12290;WebTOD &#26159;&#24314;&#31435;&#21487;&#20280;&#32553; TOD &#31995;&#32479;&#30340;&#21478;&#19968;&#31181;&#26041;&#21521;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545; Web/&#31227;&#21160;&#25509;&#21475;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2212.10504</link><description>&lt;p&gt;
&#24403;&#21069;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#27169;&#22411;&#33021;&#21542;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22330;&#26223;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Current Task-oriented Dialogue Models Automate Real-world Scenarios in the Wild?. (arXiv:2212.10504v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10504
&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#22522;&#20110;&#27133;&#22635;&#20805;&#26694;&#26550;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#24403;&#21069;&#30340; TOD &#22522;&#20934;&#27979;&#35797;&#23616;&#38480;&#20110;&#20195;&#29702;&#30495;&#23454;&#19990;&#30028;&#24773;&#26223;&#12290;WebTOD &#26159;&#24314;&#31435;&#21487;&#20280;&#32553; TOD &#31995;&#32479;&#30340;&#21478;&#19968;&#31181;&#26041;&#21521;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545; Web/&#31227;&#21160;&#25509;&#21475;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20027;&#35201;&#22522;&#20110;&#22522;&#20110;&#27133;&#22635;&#20805;&#26694;&#26550;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;SF-TOD&#65289;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#19979;&#23545;&#35805;&#34987;&#20998;&#35299;&#20026;&#26356;&#23567;&#30340;&#12289;&#21487;&#25511;&#21046;&#30340;&#21333;&#20803;&#65288;&#21363;&#27133;&#65289;&#65292;&#20197;&#23436;&#25104;&#29305;&#23450;&#30340;&#20219;&#21153;&#12290;&#19968;&#31995;&#21015;&#22522;&#20110;&#35813;&#26694;&#26550;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181; TOD &#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#30340; TOD &#22522;&#20934;&#27979;&#35797;&#23616;&#38480;&#20110;&#20195;&#29702;&#30495;&#23454;&#19990;&#30028;&#24773;&#26223;&#65292;&#24403;&#21069;&#30340; TOD &#27169;&#22411;&#20173;&#26377;&#24456;&#38271;&#30340;&#36335;&#35201;&#36208;&#12290;&#22312;&#36825;&#31687;&#35266;&#28857;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102; SF-TOD &#31995;&#32479;&#30340;&#24403;&#21069;&#29366;&#24577;&#21644;&#23616;&#38480;&#24615;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102; WebTOD &#26694;&#26550;&#65292;&#36825;&#26159;&#24314;&#31435;&#21487;&#20280;&#32553; TOD &#31995;&#32479;&#30340;&#21478;&#19968;&#31181;&#26041;&#21521;&#65292;&#24403; Web/&#31227;&#21160;&#25509;&#21475;&#21487;&#29992;&#26102;&#21487;&#20197;&#23454;&#29616;&#12290;&#22312; WebTOD &#20013;&#65292;&#23545;&#35805;&#31995;&#32479;&#36890;&#36807;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25903;&#25345;&#65292;&#23398;&#20064;&#22914;&#20309;&#29702;&#35299;&#20154;&#31867;&#20195;&#29702;&#19982;&#20043;&#20132;&#20114;&#30340; Web/&#31227;&#21160;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogue (TOD) systems are mainly based on the slot-filling-based TOD (SF-TOD) framework, in which dialogues are broken down into smaller, controllable units (i.e., slots) to fulfill a specific task. A series of approaches based on this framework achieved remarkable success on various TOD benchmarks. However, we argue that the current TOD benchmarks are limited to surrogate real-world scenarios and that the current TOD models are still a long way to cover the scenarios. In this position paper, we first identify current status and limitations of SF-TOD systems. After that, we explore the WebTOD framework, the alternative direction for building a scalable TOD system when a web/mobile interface is available. In WebTOD, the dialogue system learns how to understand the web/mobile interface that the human agent interacts with, powered by a large-scale language model.
&lt;/p&gt;</description></item><item><title>SODA&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#30334;&#19975;&#32423;&#21035;&#39640;&#36136;&#37327;&#31038;&#20132;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#19978;&#19979;&#25991;&#21270;&#31038;&#20132;&#24120;&#35782;&#30693;&#35782;&#36827;&#34892;&#33976;&#39311;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;&#20102;COSMO&#65292;&#20854;&#27604;&#30446;&#21069;&#26368;&#20339;&#34920;&#29616;&#30340;&#23545;&#35805;&#27169;&#22411;&#26356;&#20026;&#33258;&#28982;&#21644;&#19968;&#33268;&#65292;&#36825;&#26377;&#21161;&#20110;&#20102;&#35299;&#30693;&#35782;&#20016;&#23500;&#22411;&#23545;&#35805;&#21644;&#33258;&#28982;&#31038;&#20132;&#38386;&#32842;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2212.10465</link><description>&lt;p&gt;
SODA: &#20855;&#26377;&#31038;&#20132;&#24120;&#35782;&#35821;&#22659;&#21270;&#30340;&#30334;&#19975;&#35268;&#27169;&#23545;&#35805;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization. (arXiv:2212.10465v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10465
&lt;/p&gt;
&lt;p&gt;
SODA&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#30334;&#19975;&#32423;&#21035;&#39640;&#36136;&#37327;&#31038;&#20132;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#19978;&#19979;&#25991;&#21270;&#31038;&#20132;&#24120;&#35782;&#30693;&#35782;&#36827;&#34892;&#33976;&#39311;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;&#20102;COSMO&#65292;&#20854;&#27604;&#30446;&#21069;&#26368;&#20339;&#34920;&#29616;&#30340;&#23545;&#35805;&#27169;&#22411;&#26356;&#20026;&#33258;&#28982;&#21644;&#19968;&#33268;&#65292;&#36825;&#26377;&#21161;&#20110;&#20102;&#35299;&#30693;&#35782;&#20016;&#23500;&#22411;&#23545;&#35805;&#21644;&#33258;&#28982;&#31038;&#20132;&#38386;&#32842;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SODA&#65306;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#30334;&#19975;&#35268;&#27169;&#39640;&#36136;&#37327;&#31038;&#20132;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20247;&#21253;&#23567;&#35268;&#27169;&#23545;&#35805;&#35821;&#26009;&#24211;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#30693;&#35782;&#22270;&#35889;&#65288;Atomic10x; West&#31561;&#20154;&#65292;2022&#65289;&#20013;&#30340;&#31038;&#20132;&#24120;&#35782;&#30693;&#35782;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#65292;&#25552;&#28860;&#20102;150&#19975;&#20010;&#31038;&#20132;&#23545;&#35805;&#12290;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;SODA&#20013;&#30340;&#23545;&#35805;&#27604;&#20197;&#21069;&#30340;&#30001;&#20154;&#31867;&#25776;&#20889;&#30340;&#25968;&#25454;&#38598;&#26356;&#19968;&#33268;&#12289;&#26356;&#20855;&#20307;&#19988;&#65288;&#20196;&#20154;&#24778;&#35766;&#22320;&#65289;&#26356;&#33258;&#28982;&#12290;&#25105;&#20204;&#20351;&#29992;SODA&#35757;&#32451;&#20102;COSMO&#65306;&#19968;&#20010;&#36890;&#29992;&#30340;&#23545;&#35805;&#27169;&#22411;&#65292;&#22312;&#26410;&#30693;&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;&#26368;&#20339;&#34920;&#29616;&#30340;&#23545;&#35805;&#27169;&#22411;&#65288;&#20363;&#22914;GODEL&#12289;BlenderBot-1&#12289;Koala&#12289;Vicuna&#65289;&#26356;&#33258;&#28982;&#21644;&#19968;&#33268;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;COSMO&#26377;&#26102;&#29978;&#33267;&#34987;&#35748;&#20026;&#20248;&#20110;&#21407;&#22987;&#30340;&#20154;&#24037;&#32534;&#20889;&#30340;&#26631;&#20934;&#22238;&#31572;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#30693;&#35782;&#20016;&#23500;&#22411;&#23545;&#35805;&#21644;&#33258;&#28982;&#31038;&#20132;&#38386;&#32842;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SODA: the first publicly available, million-scale high-quality social dialogue dataset. In contrast to most existing crowdsourced, small-scale dialogue corpora, we distill 1.5M socially-grounded dialogues from a large language model (InstructGPT; Ouyang et al., 2022). Dialogues are distilled by contextualizing social commonsense knowledge from a knowledge graph (Atomic10x; West et al., 2022). Human evaluation shows that dialogues in SODA are more consistent, specific, and (surprisingly) natural than those in prior human-authored datasets.  Using SODA, we train COSMO: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is sometimes even preferred to the original human-written gold responses. Additionally, our results shed light on the distinction between knowledge-enriched conversations and natural social chitchats. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21452;&#32534;&#30721;&#22120;&#29992;&#20110;&#31264;&#23494;&#26816;&#32034;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#21521;&#37327;&#34920;&#31034;&#25237;&#24433;&#21040;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#31354;&#38388;&#26469;&#35299;&#37322;&#23427;&#20204;&#65292;&#36827;&#19968;&#27493;&#35299;&#37322;&#20102;&#19968;&#20123;&#22833;&#36133;&#26696;&#20363;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#26102;&#20016;&#23500;&#26597;&#35810;&#21644;&#27573;&#33853;&#34920;&#31034;&#19982;&#35789;&#27719;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10380</link><description>&lt;p&gt;
&#20320;&#25152;&#35859;&#30340;&#20196;&#29260;&#26159;&#20851;&#20110;&#20160;&#20040;&#30340;&#65311;&#31264;&#23494;&#26816;&#32034;&#20316;&#20026;&#35789;&#27719;&#34920;&#19978;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary. (arXiv:2212.10380v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21452;&#32534;&#30721;&#22120;&#29992;&#20110;&#31264;&#23494;&#26816;&#32034;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#21521;&#37327;&#34920;&#31034;&#25237;&#24433;&#21040;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#31354;&#38388;&#26469;&#35299;&#37322;&#23427;&#20204;&#65292;&#36827;&#19968;&#27493;&#35299;&#37322;&#20102;&#19968;&#20123;&#22833;&#36133;&#26696;&#20363;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#26102;&#20016;&#23500;&#26597;&#35810;&#21644;&#27573;&#33853;&#34920;&#31034;&#19982;&#35789;&#27719;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#32534;&#30721;&#22120;&#29616;&#22312;&#26159;&#31264;&#23494;&#26816;&#32034;&#30340;&#20027;&#35201;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#22914;&#20309;&#34920;&#31034;&#25991;&#26412;&#20197;&#21450;&#20026;&#20160;&#20040;&#20250;&#23548;&#33268;&#33391;&#22909;&#24615;&#33021;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35789;&#27719;&#34920;&#19978;&#30340;&#20998;&#24067;&#26469;&#38416;&#26126;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#23558;&#21452;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#21521;&#37327;&#34920;&#31034;&#25237;&#24433;&#21040;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#31354;&#38388;&#20013;&#26469;&#35299;&#37322;&#23427;&#20204;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20135;&#29983;&#30340;&#25237;&#24433;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#31232;&#30095;&#26816;&#32034;&#20043;&#38388;&#36827;&#34892;&#32852;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#35266;&#28857;&#21487;&#20197;&#35299;&#37322;&#31264;&#23494;&#26816;&#32034;&#22120;&#30340;&#19968;&#20123;&#22833;&#36133;&#26696;&#20363;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#23614;&#37096;&#23454;&#20307;&#19982;&#20196;&#29260;&#20998;&#24067;&#20542;&#21521;&#20110;&#24536;&#35760;&#36825;&#20123;&#23454;&#20307;&#30340;&#26576;&#20123;&#20196;&#29260;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#36825;&#19968;&#27934;&#23519;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25512;&#29702;&#26102;&#20016;&#23500;&#26597;&#35810;&#21644;&#27573;&#33853;&#34920;&#31034;&#19982;&#35789;&#27719;&#20449;&#24687;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#30456;&#27604;&#20110;&#24120;&#35268;&#30340;&#21452;&#32534;&#30721;&#22120;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representations produced by dual encoders by projecting them into the model's vocabulary space. We show that the resulting projections contain rich semantic information, and draw connection between them and sparse retrieval. We find that this view can offer an explanation for some of the failure cases of dense retrievers. For example, we observe that the inability of models to handle tail entities is correlated with a tendency of the token distributions to forget some of the tokens of those entities. We leverage this insight and propose a simple way to enrich query and passage representations with lexical information at inference time, and show that this significantly improves performance compared to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24863;&#21463;&#37326;&#20998;&#26512;&#36879;&#35270;&#20102;Transformer&#38271;&#24230;&#22806;&#25512;&#20013;&#30340;&#30456;&#23545;&#20301;&#32622;&#23884;&#20837;&#35774;&#35745;&#65292;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30495;&#27491;&#20351;&#29992;&#27604;&#35757;&#32451;&#24207;&#21015;&#38271;&#30340;&#38271;&#24230;&#20449;&#24687;&#30340;&#26080;&#21442;&#25968;&#30456;&#23545;&#20301;&#32622;&#23884;&#20837;&#35774;&#35745;Sandwich&#12290;</title><link>http://arxiv.org/abs/2212.10356</link><description>&lt;p&gt;
&#36890;&#36807;&#24863;&#21463;&#37326;&#20998;&#26512;&#36879;&#35270;Transformer&#38271;&#24230;&#22806;&#25512;
&lt;/p&gt;
&lt;p&gt;
Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis. (arXiv:2212.10356v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10356
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24863;&#21463;&#37326;&#20998;&#26512;&#36879;&#35270;&#20102;Transformer&#38271;&#24230;&#22806;&#25512;&#20013;&#30340;&#30456;&#23545;&#20301;&#32622;&#23884;&#20837;&#35774;&#35745;&#65292;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30495;&#27491;&#20351;&#29992;&#27604;&#35757;&#32451;&#24207;&#21015;&#38271;&#30340;&#38271;&#24230;&#20449;&#24687;&#30340;&#26080;&#21442;&#25968;&#30456;&#23545;&#20301;&#32622;&#23884;&#20837;&#35774;&#35745;Sandwich&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24230;&#22806;&#25512;&#20801;&#35768;&#22312;&#30701;&#24207;&#21015;&#19978;&#35757;&#32451;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#24403;&#27979;&#35797;&#36739;&#38271;&#24207;&#21015;&#26102;&#20173;&#20445;&#25345;&#22256;&#24785;&#24230;&#12290;&#30456;&#23545;&#20301;&#32622;&#23884;&#20837;&#35774;&#35745;ALiBi&#21040;&#30446;&#21069;&#20026;&#27490;&#24050;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#32047;&#31215;&#26631;&#20934;&#21270;&#26799;&#24230;&#24037;&#20855;&#65292;&#36890;&#36807;&#24863;&#21463;&#37326;&#20998;&#26512;&#35299;&#21078;&#20102;ALiBi&#12290;&#24863;&#21463;&#37326;&#30340;&#27010;&#24565;&#36827;&#19968;&#27493;&#20801;&#35768;&#25105;&#20204;&#20462;&#25913;&#39321;&#33609;&#27491;&#24358;&#20301;&#32622;&#23884;&#20837;&#65292;&#21019;&#24314;&#20102;~\textbf{Sandwich}&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#30495;&#27491;&#20351;&#29992;&#27604;&#35757;&#32451;&#24207;&#21015;&#38271;&#30340;&#38271;&#24230;&#20449;&#24687;&#30340;&#26080;&#21442;&#25968;&#30456;&#23545;&#20301;&#32622;&#23884;&#20837;&#35774;&#35745;&#12290;Sandwich&#19982;KERPLE&#21644;T5&#20849;&#20139;&#20855;&#26377;&#21487;&#23398;&#20064;&#30456;&#23545;&#20301;&#32622;&#23884;&#20837;&#30340;&#23545;&#25968;&#34928;&#20943;&#26102;&#38388;&#20559;&#24046;&#27169;&#24335;&#65307;&#36825;&#20123;&#25581;&#31034;&#20102;&#26410;&#26469;&#21487;&#22806;&#25512;&#30340;&#20301;&#32622;&#23884;&#20837;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length extrapolation permits training a transformer language model on short sequences that preserves perplexities when tested on substantially longer sequences. A relative positional embedding design, ALiBi, has had the widest usage to date. We dissect ALiBi via the lens of receptive field analysis empowered by a novel cumulative normalized gradient tool. The concept of receptive field further allows us to modify the vanilla Sinusoidal positional embedding to create ~\textbf{Sandwich}, the first parameter-free relative positional embedding design that truly length information uses longer than the training sequence. Sandwich shares with KERPLE and T5 the same logarithmic decaying temporal bias pattern with learnable relative positional embeddings; these elucidate future extrapolatable positional embedding design.
&lt;/p&gt;</description></item><item><title>CoCoMIC&#26159;&#19968;&#20010;&#33021;&#32852;&#21512;&#24314;&#27169;&#25991;&#20214;&#20869;&#22806;&#19978;&#19979;&#25991;&#30340;&#20195;&#30721;&#34917;&#20840;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#26597;&#25214;&#24037;&#20855;CCFINDER&#12290;&#20351;&#29992;&#35813;&#26694;&#26550;&#30456;&#23545;&#25552;&#39640;&#20102;33.94&#65285;&#30340;&#31934;&#30830;&#21305;&#37197;&#29575;&#21644;28.6&#65285;&#30340;&#21069;5&#20010;&#20505;&#36873;&#39033;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.10007</link><description>&lt;p&gt;
CoCoMIC: &#22522;&#20110;&#32852;&#21512;&#24314;&#27169;&#25991;&#20214;&#20869;&#22806;&#19978;&#19979;&#25991;&#30340;&#20195;&#30721;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context. (arXiv:2212.10007v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10007
&lt;/p&gt;
&lt;p&gt;
CoCoMIC&#26159;&#19968;&#20010;&#33021;&#32852;&#21512;&#24314;&#27169;&#25991;&#20214;&#20869;&#22806;&#19978;&#19979;&#25991;&#30340;&#20195;&#30721;&#34917;&#20840;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#26597;&#25214;&#24037;&#20855;CCFINDER&#12290;&#20351;&#29992;&#35813;&#26694;&#26550;&#30456;&#23545;&#25552;&#39640;&#20102;33.94&#65285;&#30340;&#31934;&#30830;&#21305;&#37197;&#29575;&#21644;28.6&#65285;&#30340;&#21069;5&#20010;&#20505;&#36873;&#39033;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#34917;&#20840;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#21482;&#26681;&#25454;&#25991;&#20214;&#20869;&#30340;&#20869;&#23481;&#36827;&#34892;&#20195;&#30721;&#34917;&#20840;&#65292;&#21363;&#32771;&#34385;&#25991;&#20214;&#20869;&#19978;&#19979;&#25991;&#65292;&#32780;&#24573;&#30053;&#20102;&#21516;&#19968;&#39033;&#30446;&#20013;&#20854;&#20182;&#25991;&#20214;&#20013;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#21363;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#65292;&#21518;&#32773;&#26159;&#29616;&#20195;&#27169;&#22359;&#21270;&#36719;&#20214;&#24320;&#21457;&#20013;&#29305;&#21035;&#26377;&#29992;&#30340;&#20851;&#38190;&#20449;&#24687;&#26469;&#28304;&#12290;&#36825;&#31181;&#24573;&#35270;&#38480;&#21046;&#20102;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#34917;&#20840;&#20013;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#20986;&#29616;&#24847;&#22806;&#34892;&#20026;&#65292;&#20363;&#22914;&#29983;&#25104;&#24187;&#24819;&#30340;&#31867;&#25104;&#21592;&#20989;&#25968;&#25110;&#24102;&#26377;&#24847;&#22806;&#21442;&#25968;&#30340;&#20989;&#25968;&#35843;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#26597;&#25214;&#24037;&#20855;CCFINDER&#65292;&#35813;&#24037;&#20855;&#33021;&#22815;&#26377;&#25928;&#22320;&#23450;&#20301;&#24182;&#26816;&#32034;&#26368;&#30456;&#20851;&#30340;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CoCoMIC&#26694;&#26550;&#65292;&#23427;&#22312;&#39044;&#20808;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20043;&#19978;&#65292;&#23558;&#25991;&#20214;&#20869;&#21644;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#32852;&#21512;&#36827;&#34892;&#23398;&#20064;&#12290;CoCoMIC&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#29616;&#26377;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#31934;&#30830;&#21305;&#37197;&#29575;&#26041;&#38754;&#30456;&#23545;&#25552;&#39640;&#20102;33.94&#65285;&#65292;&#22312;&#21069;5&#20010;&#20505;&#36873;&#39033;&#20934;&#30830;&#29575;&#26041;&#38754;&#30456;&#23545;&#25552;&#39640;&#20102;28.6&#65285;&#65292;&#36825;&#35777;&#26126;&#20102;&#23545;&#20110;&#20195;&#30721;&#34917;&#20840;&#65292;&#24314;&#27169;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
While pre-trained language models (LM) for code have achieved great success in code completion, they generate code conditioned only on the contents within the file, i.e., in-file context, but ignore the rich semantics in other files within the same project, i.e., cross-file context, a critical source of information that is especially useful in modern modular software development. Such overlooking constrains code language models' capacity in code completion, leading to unexpected behaviors such as generating hallucinated class member functions or function calls with unexpected arguments. In this work, we develop a cross-file context finder tool, CCFINDER, that effectively locates and retrieves the most relevant cross-file context. We propose CoCoMIC, a framework that incorporates cross-file context to learn the in-file and cross-file context jointly on top of pretrained code LMs. CoCoMIC successfully improves the existing code LM with a 33.94% relative increase in exact match and a 28.6
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861; MatCha &#65292;&#25552;&#39640;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#34920;&#21644;&#35821;&#35328;&#25968;&#25454;&#32852;&#21512;&#24314;&#27169;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#36817;20%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2212.09662</link><description>&lt;p&gt;
MatCha&#65306;&#25968;&#23398;&#25512;&#29702;&#21644;&#22270;&#34920;&#35299;&#26512;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering. (arXiv:2212.09662v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861; MatCha &#65292;&#25552;&#39640;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#34920;&#21644;&#35821;&#35328;&#25968;&#25454;&#32852;&#21512;&#24314;&#27169;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#36817;20%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#25968;&#25454;&#22312;&#20154;&#31867;&#19990;&#30028;&#20013;&#38750;&#24120;&#26222;&#36941;&#65292;&#20294;&#26159;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#24182;&#19981;&#29702;&#24819;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986; MatCha&#65288;&#25968;&#23398;&#25512;&#29702;&#21644;&#22270;&#34920;&#35299;&#26512;&#39044;&#35757;&#32451;&#65289;&#26469;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#32852;&#21512;&#24314;&#27169;&#22270;&#34920;/&#32472;&#22270;&#21644;&#35821;&#35328;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#28085;&#30422;&#20102;&#32472;&#22270;&#25286;&#35299;&#21644;&#25968;&#23383;&#25512;&#29702;&#36825;&#20123;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;MatCha &#27169;&#22411;&#22312; PlotQA &#21644; ChartQA &#31561;&#26041;&#38754;&#30340;&#34920;&#29616;&#22343;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36817; 20%&#12290;&#25105;&#20204;&#36824;&#26816;&#26597;&#20102; MatCha &#39044;&#35757;&#32451;&#22312;&#25130;&#22270;&#12289;&#25945;&#31185;&#20070;&#22270;&#31034;&#21644;&#25991;&#26723;&#22270;&#24418;&#31561;&#39046;&#22495;&#30340;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#35266;&#23519;&#21040;&#24635;&#20307;&#25913;&#21892;&#65292;&#36825;&#39564;&#35777;&#20102; MatCha &#39044;&#35757;&#32451;&#22312;&#26356;&#24191;&#27867;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art vision-language models do not perform well on these data. We propose MatCha (Math reasoning and Chart derendering pretraining) to enhance visual language models' capabilities in jointly modeling charts/plots and language data. Specifically, we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling.  We perform the MatCha pretraining starting from Pix2Struct, a recently proposed image-to-text visual language model. On standard benchmarks such as PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as much as nearly 20%. We also examine how well MatCha pretraining transfers to domains such as screenshots, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MatCha pretraining on broader visual language tas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#26469;&#26500;&#24314;&#26032;&#26679;&#26412;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#21407;&#22987;&#26465;&#20214;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.09561</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24102;&#26377;&#33258;&#25105;&#39564;&#35777;&#30340;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are reasoners with Self-Verification. (arXiv:2212.09561v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#26469;&#26500;&#24314;&#26032;&#26679;&#26412;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#21407;&#22987;&#26465;&#20214;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#26102;&#65292;&#23427;&#38750;&#24120;&#25935;&#24863;&#20110;&#20010;&#21035;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24517;&#39035;&#35757;&#32451;&#39564;&#35777;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#33258;&#25105;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#20316;&#20026;&#26465;&#20214;&#26469;&#26500;&#24314;&#19968;&#20010;&#26032;&#26679;&#26412;&#65292;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#34987;&#25513;&#30422;&#30340;&#21407;&#22987;&#26465;&#20214;&#12290;&#25105;&#20204;&#22522;&#20110;&#20934;&#30830;&#24615;&#35745;&#31639;&#21487;&#35299;&#37322;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26102;&#25552;&#39640;&#22810;&#20010;&#31639;&#26415;&#21644;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#24050;&#32463;&#35777;&#26126;LLM&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#22810;&#31181;&#24102;&#26377;&#33258;&#25105;&#39564;&#35777;&#21151;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36991;&#20813;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
When a large language model (LLM) performs complex reasoning by chain of thought (CoT), it can be highly sensitive to individual mistakes. We have had to train verifiers to address this issue. As we all know, after human inferring a conclusion, they often check it by re-verifying it, which can avoid some mistakes. We propose a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked. We calculate an explainable verification score based on the accuracy. This method can improve the accuracy of multiple arithmetics and logical reasoning datasets when using few-shot learning. we have demonstrated that LLMs can conduct explainable self-verification of their own conclusions and achieve competitive reasoning performance. Extensive experimentals have demonstrated that our method can help multiple large language models with self-verification can avoid interference from inco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#30721;&#22120;&#35843;&#25972;&#65288;DecT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#35299;&#30721;&#22120;&#32593;&#32476;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20986;&#31471;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#36755;&#20837;&#31471;&#30340;&#39640;&#35745;&#31639;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.08408</link><description>&lt;p&gt;
&#35299;&#30721;&#22120;&#35843;&#25972;&#65306;&#23558;&#39640;&#25928;&#35821;&#35328;&#29702;&#35299;&#20316;&#20026;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decoder Tuning: Efficient Language Understanding as Decoding. (arXiv:2212.08408v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#30721;&#22120;&#35843;&#25972;&#65288;DecT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#35299;&#30721;&#22120;&#32593;&#32476;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20986;&#31471;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#36755;&#20837;&#31471;&#30340;&#39640;&#35745;&#31639;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#21152;&#65292;&#21482;&#21521;&#29992;&#25143;&#25552;&#20379;&#25512;&#29702;API&#65288;&#21363;&#27169;&#22411;&#20026;&#26381;&#21153;&#65288;MaaS&#65289;&#35774;&#32622;&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#20570;&#27861;&#12290;&#20026;&#20102;&#36866;&#24212;&#21442;&#25968;&#20923;&#32467;&#30340;PTMs&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38598;&#20013;&#22312;&#36755;&#20837;&#31471;&#65292;&#23547;&#25214;&#24378;&#26377;&#21147;&#30340;&#25552;&#31034;&#26469;&#21050;&#28608;&#27169;&#22411;&#20135;&#29983;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#36755;&#20837;&#31471;&#30340;&#36866;&#24212;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#32570;&#23569;&#26799;&#24230;&#20449;&#21495;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#25968;&#21315;&#20010;API&#26597;&#35810;&#65292;&#23548;&#33268;&#39640;&#35745;&#31639;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#30721;&#22120;&#35843;&#25972;&#65288;DecT&#65289;&#65292;&#23427;&#19982;&#24403;&#21069;&#26041;&#27861;&#30456;&#21453;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#35299;&#30721;&#22120;&#32593;&#32476;&#26469;&#36866;&#24212;PTMs&#30340;&#36755;&#20986;&#31471;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DecT&#39318;&#20808;&#25552;&#21462;&#34987;&#25552;&#31034;&#21050;&#28608;&#30340;&#36755;&#20986;&#20998;&#25968;&#20316;&#20026;&#21021;&#22987;&#39044;&#27979;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36824;&#22312;&#36755;&#20986;&#34920;&#31034;&#19978;&#35757;&#32451;&#20102;&#21478;&#19968;&#20010;&#35299;&#30721;&#22120;&#32593;&#32476;&#65292;&#20197;&#32467;&#21512;&#21518;&#39564;&#25968;&#25454;&#30693;&#35782;&#12290;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;DecT&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#35757;&#32451;&#65292;&#24182;&#19988;&#27599;&#20010;&#26679;&#26412;&#21482;&#38656;&#35201;&#19968;&#20010;PTM&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the evergrowing sizes of pre-trained models (PTMs), it has been an emerging practice to only provide the inference APIs for users, namely model-as-a-service (MaaS) setting. To adapt PTMs with model parameters frozen, most current approaches focus on the input side, seeking for powerful prompts to stimulate models for correct answers. However, we argue that input-side adaptation could be arduous due to the lack of gradient signals and they usually require thousands of API queries, resulting in high computation and time costs. In light of this, we present Decoder Tuning (DecT), which in contrast optimizes task-specific decoder networks on the output side. Specifically, DecT first extracts prompt-stimulated output scores for initial predictions. On top of that, we train an additional decoder network on the output representations to incorporate posterior data knowledge. By gradient-based optimization, DecT can be trained within several seconds and requires only one PTM query per sampl
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27010;&#29575;&#23494;&#24230;&#20272;&#35745;&#22312;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#37319;&#29992;&#21487;&#36870;&#21464;&#25442;&#20989;&#25968;&#23558;&#22797;&#26434;&#20998;&#24067;&#26144;&#23556;&#21040;&#20808;&#39564;&#31354;&#38388;&#20013;&#30340;&#31616;&#21333;&#39640;&#26031;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#28789;&#27963;&#12289;&#39640;&#25928;&#12289;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.08307</link><description>&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#23494;&#24230;&#20272;&#35745;&#22312;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controllable Text Generation via Probability Density Estimation in the Latent Space. (arXiv:2212.08307v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08307
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27010;&#29575;&#23494;&#24230;&#20272;&#35745;&#22312;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#37319;&#29992;&#21487;&#36870;&#21464;&#25442;&#20989;&#25968;&#23558;&#22797;&#26434;&#20998;&#24067;&#26144;&#23556;&#21040;&#20808;&#39564;&#31354;&#38388;&#20013;&#30340;&#31616;&#21333;&#39640;&#26031;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#28789;&#27963;&#12289;&#39640;&#25928;&#12289;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#26469;&#33258;&#28508;&#31354;&#38388;&#30340;&#25511;&#21046;&#24605;&#24819;&#65292;&#20363;&#22914;&#36890;&#36807;&#23646;&#24615;&#30456;&#20851;&#20998;&#31867;&#22120;&#20248;&#21270;&#34920;&#31034;&#25110;&#20174;&#30456;&#20851;&#31163;&#25955;&#26679;&#26412;&#20013;&#37319;&#26679;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#24314;&#27169;&#28508;&#31354;&#38388;&#21644;&#25511;&#21046;&#26041;&#38754;&#22343;&#19981;&#22815;&#26377;&#25928;&#65292;&#23548;&#33268;&#21463;&#25511;&#21046;&#30340;&#25991;&#26412;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#36739;&#20302;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25511;&#21046;&#26694;&#26550;&#65292;&#21033;&#29992;&#28508;&#31354;&#38388;&#20013;&#30340;&#27010;&#29575;&#23494;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#21487;&#36870;&#21464;&#25442;&#20989;&#25968;&#65288;&#27491;&#21017;&#27969;&#65289;&#65292;&#23558;&#28508;&#31354;&#38388;&#20013;&#30340;&#22797;&#26434;&#20998;&#24067;&#26144;&#23556;&#21040;&#20808;&#39564;&#31354;&#38388;&#20013;&#31616;&#21333;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#20808;&#39564;&#31354;&#38388;&#20013;&#25191;&#34892;&#22797;&#26434;&#32780;&#28789;&#27963;&#30340;&#25511;&#21046;&#65292;&#24182;&#30001;&#20110;&#21487;&#36870;&#21464;&#25442;&#30340;&#21333;&#19968;&#26144;&#23556;&#23646;&#24615;&#23558;&#25511;&#21046;&#25928;&#26524;&#21453;&#39304;&#21040;&#28508;&#31354;&#38388;&#20013;&#12290;&#21333;&#23646;&#24615;&#25511;&#21046;&#21644;&#22810;&#23646;&#24615;&#25511;&#21046;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20960;&#31181;&#24378;&#22823;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous work on controllable text generation has explored the idea of control from the latent space, such as optimizing a representation with attribute-related classifiers or sampling a representation from relevant discrete samples. However, they are not effective enough in modeling both the latent space and the control, leaving controlled text with low quality and diversity. In this work, we propose a novel control framework using probability density estimation in the latent space. Our method utilizes an invertible transformation function, the Normalizing Flow, that maps the complex distributions in the latent space to simple Gaussian distributions in the prior space. Thus, we can perform sophisticated and flexible control in the prior space and feed the control effects back into the latent space owing to the one-one-mapping property of invertible transformations. Experiments on single-attribute controls and multi-attribute control reveal that our method outperforms several strong ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29616;&#23383;&#38754;&#35299;&#37322;&#26041;&#38754;&#34920;&#29616;&#36739;&#22909;&#65292;&#20294;&#22312;&#20381;&#36182;&#31038;&#20132;&#26041;&#38754;&#30340;&#29616;&#35937;&#26041;&#38754;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2212.06801</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29992;&#35821;&#35328;&#29702;&#35299;&#30340;&#31934;&#32454;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A fine-grained comparison of pragmatic language understanding in humans and language models. (arXiv:2212.06801v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29616;&#23383;&#38754;&#35299;&#37322;&#26041;&#38754;&#34920;&#29616;&#36739;&#22909;&#65292;&#20294;&#22312;&#20381;&#36182;&#31038;&#20132;&#26041;&#38754;&#30340;&#29616;&#35937;&#26041;&#38754;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29992;&#35821;&#35328;&#29702;&#35299;&#26159;&#20154;&#31867;&#20132;&#27969;&#20013;&#33267;&#20851;&#37325;&#35201;&#19988;&#19981;&#26131;&#29702;&#35299;&#30340;&#26041;&#38754;&#65292;&#23545;&#20110;&#20154;&#36896;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#36825;&#19968;&#28857;&#20173;&#28982;&#23384;&#22312;&#24456;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#38646;&#22522;&#30784;&#25552;&#31034;&#19979;&#23545;&#33521;&#35821;&#26448;&#26009;&#36827;&#34892;&#20102;7&#31181;&#23454;&#29992;&#29616;&#35937;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#30340;&#31934;&#32454;&#27604;&#36739;&#65292;&#25506;&#35752;&#27169;&#22411;&#22312;&#35299;&#37322;&#21457;&#35328;&#32773;&#34920;&#36798;&#26102;&#26159;&#21542;&#20855;&#26377;&#35821;&#29992;&#29702;&#35299;&#12289;&#26159;&#21542;&#21644;&#20154;&#31867;&#23384;&#22312;&#30456;&#20284;&#30340;&#38169;&#35823;&#31867;&#22411;&#21644;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#26102;&#20351;&#29992;&#30456;&#20284;&#30340;&#35821;&#35328;&#32447;&#32034;&#12290;&#30740;&#31350;&#21457;&#29616;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#39640;&#31934;&#24230;&#24182;&#19988;&#19982;&#20154;&#31867;&#30340;&#38169;&#35823;&#27169;&#24335;&#30456;&#21305;&#37197;&#65306;&#22312;&#19981;&#27491;&#30830;&#30340;&#31572;&#26696;&#20013;&#65292;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#23383;&#38754;&#19978;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#21021;&#27493;&#35777;&#25454;&#34920;&#26126;&#65292;&#27169;&#22411;&#21644;&#20154;&#31867;&#23545;&#30456;&#20284;&#30340;&#35821;&#35328;&#32447;&#32034;&#20063;&#24456;&#25935;&#24863;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#26126;&#30830;&#26500;&#24314;&#30340;&#24515;&#29702;&#29366;&#24577;&#34920;&#31034;&#65292;&#27169;&#22411;&#20063;&#21487;&#20197;&#34920;&#29616;&#20986;&#23454;&#29992;&#34892;&#20026;&#12290;&#20294;&#26159;&#65292;&#27169;&#22411;&#22312;&#20381;&#36182;&#31038;&#20132;&#26041;&#38754;&#30340;&#29616;&#35937;&#26041;&#38754;&#20173;&#23384;&#22312;&#19968;&#23450;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pragmatics and non-literal language understanding are essential to human communication, and present a long-standing challenge for artificial language models. We perform a fine-grained comparison of language models and humans on seven pragmatic phenomena, using zero-shot prompting on an expert-curated set of English materials. We ask whether models (1) select pragmatic interpretations of speaker utterances, (2) make similar error patterns as humans, and (3) use similar linguistic cues as humans to solve the tasks. We find that the largest models achieve high accuracy and match human error patterns: within incorrect responses, models favor literal interpretations over heuristic-based distractors. We also find preliminary evidence that models and humans are sensitive to similar linguistic cues. Our results suggest that pragmatic behaviors can emerge in models without explicitly constructed representations of mental states. However, models tend to struggle with phenomena relying on social 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#36866;&#29992;&#20110;&#36229;&#36807;&#21313;&#20159;&#21360;&#24230;&#35821;&#35328;&#20351;&#29992;&#32773;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#19977;&#20010;&#37325;&#35201;&#36129;&#29486;&#65292;&#21253;&#25324;&#26368;&#22823;&#30340;&#21333;&#35821;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;20.9B&#30340;&#35760;&#21495;&#65292;&#28085;&#30422;24&#31181;&#35821;&#35328;&#65292;&#20197;&#21450;&#20154;&#24037;&#30417;&#30563;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;IndicXTREME&#65292;&#21253;&#25324;9&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;NLU&#20219;&#21153;&#65292;&#36328;&#36234;20&#31181;&#35821;&#35328;&#12290;&#36825;&#26159;&#39318;&#20010;&#26088;&#22312;&#27979;&#35797;&#21360;&#24230;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;&#29702;&#35299;&#26631;&#20934;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2212.05409</link><description>&lt;p&gt;
&#20026;&#30830;&#20445;&#21360;&#24230;&#35821;&#19981;&#34987;&#33853;&#19979;&#65306;&#24314;&#31435;&#21333;&#35821;&#35821;&#26009;&#24211;&#12289;&#22522;&#20934;&#21644;&#21360;&#24230;&#35821;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages. (arXiv:2212.05409v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#36866;&#29992;&#20110;&#36229;&#36807;&#21313;&#20159;&#21360;&#24230;&#35821;&#35328;&#20351;&#29992;&#32773;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#19977;&#20010;&#37325;&#35201;&#36129;&#29486;&#65292;&#21253;&#25324;&#26368;&#22823;&#30340;&#21333;&#35821;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;20.9B&#30340;&#35760;&#21495;&#65292;&#28085;&#30422;24&#31181;&#35821;&#35328;&#65292;&#20197;&#21450;&#20154;&#24037;&#30417;&#30563;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;IndicXTREME&#65292;&#21253;&#25324;9&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;NLU&#20219;&#21153;&#65292;&#36328;&#36234;20&#31181;&#35821;&#35328;&#12290;&#36825;&#26159;&#39318;&#20010;&#26088;&#22312;&#27979;&#35797;&#21360;&#24230;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;&#29702;&#35299;&#26631;&#20934;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#36866;&#29992;&#20110;&#36229;&#36807;&#21313;&#20159;&#21360;&#24230;&#35821;&#35328;&#20351;&#29992;&#32773;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#19977;&#20010;&#37325;&#35201;&#26041;&#38754;&#30340;&#36129;&#29486;&#26469;&#25913;&#21892;&#21360;&#24230;&#35821;&#35328;&#30340;NLU&#33021;&#21147;&#65292;&#21253;&#25324;(i) &#21333;&#35821;&#35821;&#26009;&#24211; (ii) NLU&#27979;&#35797;&#38598; (iii) &#38024;&#23545;&#21360;&#24230;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;LLM&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#26368;&#22823;&#30340;&#21333;&#35821;&#35821;&#26009;&#24211;IndicCorp&#65292;&#22312;4&#20010;&#35821;&#35328;&#23478;&#26063;&#30340;24&#31181;&#35821;&#35328;&#20013;&#28085;&#30422;20.9B&#20010;&#26631;&#35760;&#65292;2.3&#20493;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#22312;&#25903;&#25345;12&#31181;&#20854;&#20182;&#35821;&#35328;&#30340;&#21516;&#26102;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21019;&#24314;&#19968;&#20010;&#20154;&#24037;&#30417;&#30563;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;IndicXTREME&#65292;&#28085;&#30422;20&#31181;&#35821;&#35328;&#30340;&#20061;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;NLU&#20219;&#21153;&#12290;&#36328;&#35821;&#35328;&#21644;&#20219;&#21153;&#65292;IndicXTREME&#20849;&#21253;&#21547;105&#20010;&#35780;&#20272;&#38598;&#65292;&#20854;&#20013;52&#20010;&#26159;&#23545;&#25991;&#29486;&#30340;&#26032;&#36129;&#29486;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#26088;&#22312;&#27979;&#35797;&#21360;&#24230;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;&#29702;&#35299;&#26631;&#20934;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a collective speaker base of more than one billion speakers is absolutely crucial. In this work, we aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work, while supporting 12 additional languages. Next, we create a human-supervised benchmark, IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions to the literature. To the best of our knowledge, this is the first effort towards creating a standard benchmark for Indic languages that aims to test the multilingual z
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#20154;&#24615;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#38750;&#35821;&#35328;&#22270;&#28789;&#27979;&#35797;&#65292;&#21457;&#29616;&#24403;&#21069;AI&#39550;&#39542;&#21592;&#19981;&#33021;&#21019;&#36896;&#31867;&#20284;&#20154;&#31867;&#30340;&#39550;&#20056;&#20307;&#39564;&#65292;&#38656;&#35201;&#22312;&#24773;&#24863;&#36807;&#28193;&#27169;&#22411;&#20013;&#36827;&#34892;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2212.02908</link><description>&lt;p&gt;
&#36808;&#21521;&#20154;&#31867;&#20860;&#23481;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65306;&#24773;&#24863;&#36807;&#28193;&#27169;&#22411;&#20013;&#30340;&#38750;&#35821;&#35328;&#22270;&#28789;&#27979;&#35797;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards human-compatible autonomous car: A study of non-verbal Turing test in automated driving with affective transition modelling. (arXiv:2212.02908v5 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#20154;&#24615;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#38750;&#35821;&#35328;&#22270;&#28789;&#27979;&#35797;&#65292;&#21457;&#29616;&#24403;&#21069;AI&#39550;&#39542;&#21592;&#19981;&#33021;&#21019;&#36896;&#31867;&#20284;&#20154;&#31867;&#30340;&#39550;&#20056;&#20307;&#39564;&#65292;&#38656;&#35201;&#22312;&#24773;&#24863;&#36807;&#28193;&#27169;&#22411;&#20013;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#31867;&#36208;&#21521;&#26080;&#38656;&#21452;&#25163;&#30340;&#29983;&#27963;&#26041;&#24335;&#26102;&#65292;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#29616;&#26377;&#25991;&#29486;&#24378;&#35843;&#65292;&#22914;&#26524;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#33021;&#22815;&#20197;&#31867;&#20284;&#20154;&#31867;&#30340;&#26041;&#24335;&#39550;&#39542;&#65292;&#20154;&#20204;&#20250;&#26356;&#23481;&#26131;&#25509;&#21463;&#23427;&#12290;&#28982;&#32780;&#65292;&#20165;&#26377;&#23569;&#37327;&#30740;&#31350;&#20174;&#20056;&#23458;&#35282;&#24230;&#30340;&#33258;&#28982;&#20307;&#39564;&#26469;&#26816;&#39564;&#30446;&#21069;&#30340;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#30340;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#39033;&#30495;&#23454;&#36947;&#36335;&#29615;&#22659;&#19979;&#30340;&#35797;&#39564;&#65292;&#27979;&#35797;&#20102;69&#20301;&#21442;&#19982;&#32773;&#30340;&#21453;&#39304;&#65292;&#23581;&#35797;&#20102;&#35299;AI&#39550;&#39542;&#20154;&#21592;&#33021;&#21542;&#20026;&#20056;&#23458;&#21019;&#36896;&#31867;&#20284;&#20154;&#31867;&#30340;&#39550;&#20056;&#20307;&#39564;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#39550;&#20056;&#20307;&#39564;&#30340;&#38750;&#35821;&#35328;&#22270;&#28789;&#27979;&#35797;&#65292;&#35201;&#27714;&#21442;&#19982;&#32773;&#20316;&#20026;&#20056;&#23458;&#20056;&#22352;AI&#39550;&#39542;&#20154;&#21592;&#25110;&#20154;&#31867;&#39550;&#39542;&#20154;&#21592;&#39550;&#39542;&#30340;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65292;&#24182;&#21028;&#26029;&#21496;&#26426;&#26159;&#20154;&#31867;&#36824;&#26159;AI&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20805;&#24403;AI&#39550;&#39542;&#21592;&#30340;&#27773;&#36710;&#26410;&#33021;&#36890;&#36807;&#25105;&#20204;&#30340;&#27979;&#35797;&#65292;&#22240;&#20026;&#20056;&#23458;&#33021;&#22815;&#36229;&#36807;&#38543;&#26426;&#29468;&#27979;&#22320;&#35782;&#21035;&#20986;AI&#39550;&#39542;&#21592;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20154;&#31867;&#39550;&#39542;&#21592;&#39550;&#39542;&#36710;&#36742;&#26102;&#65292;&#20056;&#23458;&#30340;&#21028;&#26029;&#32467;&#26524;&#32422;&#22312;&#38543;&#26426;&#29468;&#27979;&#27700;&#24179;&#38468;&#36817;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#20154;&#31867;&#20056;&#23458;&#22312;&#39550;&#39542;&#36807;&#31243;&#20013;&#32473;&#20104;&#20102;&#21738;&#20123;&#20154;&#24615;&#21270;&#29305;&#24449;&#30340;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous cars are indispensable when humans go further down the hands-free route. Although existing literature highlights that the acceptance of the autonomous car will increase if it drives in a human-like manner, sparse research offers the naturalistic experience from a passenger's seat perspective to examine the human likeness of current autonomous cars. The present study tested whether the AI driver could create a human-like ride experience for passengers based on 69 participants' feedback in a real-road scenario. We designed a ride experience-based version of the non-verbal Turing test for automated driving. Participants rode in autonomous cars (driven by either human or AI drivers) as a passenger and judged whether the driver was human or AI. The AI driver failed to pass our test because passengers detected the AI driver above chance. In contrast, when the human driver drove the car, the passengers' judgement was around chance. We further investigated how human passengers ascri
&lt;/p&gt;</description></item><item><title>SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.13308</link><description>&lt;p&gt;
SciRepEval&#65306;&#19968;&#20010;&#29992;&#20110;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#22810;&#26684;&#24335;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13308
&lt;/p&gt;
&lt;p&gt;
SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#21487;&#20197;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#36755;&#20837;&#29305;&#24449;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#34920;&#31034;&#30340;&#29616;&#26377;&#22522;&#20934;&#26410;&#33021;&#25429;&#25417;&#21040;&#30456;&#20851;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; SciRepEval&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#29616;&#23454;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013; 11 &#20010;&#26159;&#26032;&#20219;&#21153;&#65306;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#25490;&#21517;&#21644;&#25628;&#32034;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#22522;&#20934;&#26469;&#30740;&#31350;&#21644;&#25913;&#36827;&#31185;&#23398;&#25991;&#26723;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#20219;&#21153;&#26684;&#24335;&#26041;&#38754;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#65292;&#31616;&#21333;&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#20063;&#19981;&#33021;&#25913;&#36827;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#27599;&#20010;&#25991;&#26723;&#30340;&#22810;&#20010;&#23884;&#20837;&#65292;&#27599;&#20010;&#23884;&#20837;&#19987;&#38376;&#38024;&#23545;&#19981;&#21516;&#30340;&#26684;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#20219;&#21153;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned representations of scientific documents can serve as valuable input features for downstream tasks, without the need for further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diversity of relevant tasks. In response, we introduce SciRepEval, the first comprehensive benchmark for training and evaluating scientific document representations. It includes 25 challenging and realistic tasks, 11 of which are new, across four formats: classification, regression, ranking and search. We then use the benchmark to study and improve the generalization ability of scientific document representation models. We show how state-of-the-art models struggle to generalize across task formats, and that simple multi-task training fails to improve them. However, a new approach that learns multiple embeddings per document, each tailored to a different format, can improve performance. We experiment with task-format-specific control codes and adapters in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#25991;&#26412;&#20013;&#30340;&#31163;&#32676;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.11300</link><description>&lt;p&gt;
&#25991;&#26412;&#20013;&#30340;&#31163;&#32676;&#26816;&#27979;&#30340;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text. (arXiv:2211.11300v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#25991;&#26412;&#20013;&#30340;&#31163;&#32676;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#35777;&#26126;&#26159;&#21482;&#20351;&#29992;&#20869;&#20998;&#24067;(ID)&#26679;&#20363;&#25991;&#26412;&#36827;&#34892;&#31163;&#32676;&#26816;&#27979;&#30340;&#23453;&#36149;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35201;&#20040;&#36890;&#36807;&#20351;&#29992;ID&#26679;&#20363;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#23558;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#22256;&#24785;&#24230;&#20316;&#20026;&#31163;&#32676;&#24471;&#20998;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#20004;&#31181;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#30340;&#20114;&#34917;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#23427;&#20204;&#20248;&#21183;&#24182;&#20943;&#36731;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#30340;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#20316;&#20026;&#32769;&#24072;&#65292;&#22312;ID&#31034;&#20363;&#19978;&#25945;&#25480;&#19968;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#38500;&#20102;&#39044;&#27979;&#23618;&#33976;&#39311;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20013;&#38388;&#23618;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#20840;&#38754;&#25506;&#32034;&#32769;&#24072;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#23398;&#20064;&#30340;&#23398;&#29983;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;ID&#25968;&#25454;&#27969;&#24418;&#65292;&#21516;&#26102;&#33719;&#24471;&#26356;&#24378;&#30340;&#23558;OoD&#31034;&#20363;&#26144;&#23556;&#21040;&#27969;&#24418;&#20043;&#22806;&#30340;&#33021;&#21147;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#31454;&#20105;&#22522;&#32447;&#30456;&#27604;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised representation learning has proved to be a valuable component for out-of-distribution (OoD) detection with only the texts of in-distribution (ID) examples. These approaches either train a language model from scratch or fine-tune a pre-trained language model using ID examples, and then take the perplexity output by the language model as OoD scores. In this paper, we analyze the complementary characteristics of both OoD detection methods and propose a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations. Specifically, we use a fine-tuned model as the teacher to teach a randomly initialized student model on the ID examples. Besides the prediction layer distillation, we present a similarity-based intermediate layer distillation method to thoroughly explore the representation space of the teacher model. In this way, the learned student can better represent the ID data manifold while gaining a stronger ability to map O
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#21518;&#22788;&#29702;&#21644;&#19968;&#31181;&#26032;&#26550;&#26500;CI-BERT&#23558;&#27010;&#24565;&#22120;&#25237;&#24433;&#32435;&#20837;&#25152;&#26377;&#23618;&#20013;&#12290;&#27010;&#24565;&#22120;&#21518;&#22788;&#29702;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#21435;&#20559;&#35265;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.11087</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#22120;&#36741;&#21161;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21435;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Conceptor-Aided Debiasing of Large Language Models. (arXiv:2211.11087v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#21518;&#22788;&#29702;&#21644;&#19968;&#31181;&#26032;&#26550;&#26500;CI-BERT&#23558;&#27010;&#24565;&#22120;&#25237;&#24433;&#32435;&#20837;&#25152;&#26377;&#23618;&#20013;&#12290;&#27010;&#24565;&#22120;&#21518;&#22788;&#29702;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#21435;&#20559;&#35265;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21453;&#26144;&#20102;&#23427;&#20204;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#22266;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#35768;&#22810;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26410;&#33021;&#21435;&#20559;&#35265;&#25110;&#32773;&#20250;&#29306;&#29298;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#27010;&#24565;&#22120;&#8212;&#8212;&#19968;&#31181;&#36719;&#25237;&#24433;&#26041;&#27861;&#8212;&#8212;&#26469;&#35782;&#21035;&#21644;&#21435;&#38500;&#22914;BERT&#21644;GPT&#31561;LLMs&#20013;&#30340;&#20559;&#35265;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24212;&#29992;&#27010;&#24565;&#22120;&#30340;&#26041;&#27861;&#65306;&#65288;1&#65289;&#36890;&#36807;&#21518;&#22788;&#29702;&#36827;&#34892;&#20559;&#35265;&#23376;&#31354;&#38388;&#25237;&#24433;&#65307;&#65288;2&#65289;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#8212;&#8212;&#27010;&#24565;&#22120;&#20171;&#20837;BERT(CI-BERT)&#65292;&#23427;&#22312;&#35757;&#32451;&#26399;&#38388;&#26126;&#30830;&#22320;&#23558;&#27010;&#24565;&#22120;&#25237;&#24433;&#32435;&#20837;&#25152;&#26377;&#23618;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27010;&#24565;&#22120;&#21518;&#22788;&#29702;&#22312;&#20445;&#25345;&#25110;&#25552;&#39640;LLMs&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21435;&#20559;&#35265;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#37117;&#24456;&#31283;&#20581;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#23545;&#29616;&#26377;&#20559;&#35265;&#23376;&#31354;&#38388;&#30340;&#36923;&#36753;&#25805;&#20316;&#26469;&#26377;&#25928;&#22320;&#20943;&#36731;&#20132;&#38598;&#20559;&#35265;&#12290;&#34429;&#28982;CI-BERT&#30340;&#35757;&#32451;&#32771;&#34385;&#20102;&#25152;&#26377;&#23618;&#30340;&#20559;&#35265;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#23427;&#30340;&#35757;&#32451;&#25104;&#26412;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy. We use conceptors--a soft projection method--to identify and remove the bias subspace in LLMs such as BERT and GPT. We propose two methods of applying conceptors (1) bias subspace projection by post-processing; and (2) a new architecture, conceptor-intervened BERT (CI-BERT), which explicitly incorporates the conceptor projection into all layers during training. We find that conceptor post-processing achieves state-of-the-art (SoTA) debiasing results while maintaining or improving LLMs' performance on the GLUE benchmark. Also, it is robust in various scenarios and can mitigate intersectional bias efficiently by its logical operation on the existing bias subspaces. Although CI-BERT's training takes all layers' bias into account and can beat its post-processing counterpa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35760;&#21495;&#27744;&#21270;&#26426;&#21046;&#65292;&#21487;&#20197;&#22312;&#33258;&#28982;&#24847;&#20041;&#21333;&#20301;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#32553;&#30701;&#20013;&#38388;&#23618;&#30340;&#24207;&#21015;&#38271;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20855;&#26377;&#22266;&#23450;&#27744;&#21270;&#30340;&#26222;&#36890;Transformer&#65292;&#21160;&#24577;&#27744;&#21270;&#26426;&#21046;&#26082;&#26356;&#24555;&#65292;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#20998;&#27573;&#21644;&#35821;&#35328;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2211.09761</link><description>&lt;p&gt;
&#21160;&#24577;&#35760;&#21495;&#27744;&#21270;&#30340;&#39640;&#25928;Transformer
&lt;/p&gt;
&lt;p&gt;
Efficient Transformers with Dynamic Token Pooling. (arXiv:2211.09761v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35760;&#21495;&#27744;&#21270;&#26426;&#21046;&#65292;&#21487;&#20197;&#22312;&#33258;&#28982;&#24847;&#20041;&#21333;&#20301;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#32553;&#30701;&#20013;&#38388;&#23618;&#30340;&#24207;&#21015;&#38271;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20855;&#26377;&#22266;&#23450;&#27744;&#21270;&#30340;&#26222;&#36890;Transformer&#65292;&#21160;&#24577;&#27744;&#21270;&#26426;&#21046;&#26082;&#26356;&#24555;&#65292;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#20998;&#27573;&#21644;&#35821;&#35328;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#21331;&#36234;&#65292;&#20294;&#22312;&#20869;&#23384;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#26041;&#38754;&#25928;&#29575;&#20302;&#19979;&#12290;&#19968;&#31181;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#36890;&#36807;&#22266;&#23450;&#38271;&#24230;&#30340;&#29255;&#27573;&#27744;&#21270;&#35760;&#21495;&#26469;&#32553;&#30701;&#20013;&#38388;&#23618;&#30340;&#24207;&#21015;&#38271;&#24230;&#12290;&#28982;&#32780;&#65292;&#35832;&#22914;&#21333;&#35789;&#25110;&#30701;&#35821;&#20043;&#31867;&#30340;&#33258;&#28982;&#24847;&#20041;&#21333;&#20301;&#20855;&#26377;&#19981;&#21516;&#30340;&#22823;&#23567;&#12290;&#20026;&#35299;&#20915;&#36825;&#31181;&#19981;&#21305;&#37197;&#65292;&#25105;&#20204;&#20026;&#35821;&#35328;&#27169;&#22411;&#37197;&#22791;&#20102;&#19968;&#31181;&#21160;&#24577;&#27744;&#21270;&#26426;&#21046;&#65292;&#21487;&#20197;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#39044;&#27979;&#27573;&#36793;&#30028;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22810;&#31181;&#25512;&#26029;&#36793;&#30028;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#38543;&#26426;&#37325;&#26032;&#21442;&#25968;&#21270;&#36827;&#34892;&#31471;&#23545;&#31471;&#23398;&#20064;&#12289;&#22522;&#20110;&#23376;&#35789;&#20998;&#35789;&#22120;&#25110;&#26465;&#20214;&#29109;&#23792;&#20540;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#20197;&#21450;&#22522;&#20110;&#35821;&#35328;&#23398;&#21407;&#29702;&#30340;&#36793;&#30028;&#12290;&#25105;&#20204;&#23545;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#24418;&#24577;&#19981;&#21516;&#30340;&#35821;&#35328;&#30340;&#25991;&#26412;&#36827;&#34892;&#23383;&#31526;&#32423;&#21035;&#30340;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21160;&#24577;&#27744;&#21270;&#26426;&#21046;&#26082;&#27604;&#20855;&#26377;&#22266;&#23450;&#27744;&#21270;&#30340;&#26222;&#36890;Transformer&#26356;&#24555;&#65292;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#20998;&#27573;&#21644;&#35821;&#35328;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers achieve unrivalled performance in modelling language, but remain inefficient in terms of memory and time complexity. A possible remedy is to reduce the sequence length in the intermediate layers by pooling fixed-length segments of tokens. Nevertheless, natural units of meaning, such as words or phrases, display varying sizes. To address this mismatch, we equip language models with a dynamic-pooling mechanism, which predicts segment boundaries in an autoregressive fashion. We compare several methods to infer boundaries, including end-to-end learning through stochastic re-parameterisation, supervised learning (based on segmentations from subword tokenizers or spikes in conditional entropy), as well as linguistically motivated boundaries. We perform character-level evaluation on texts from multiple datasets and morphologically diverse languages. The results demonstrate that dynamic pooling, which jointly segments and models language, is both faster and more accurate than vani
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#22810;&#35821;&#35328;&#30340;&#24615;&#21035;&#19981;&#26126;&#30830;&#30340;TTS&#22768;&#38899;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#24615;&#21035;&#24863;&#30693;&#26041;&#27861;&#20174;&#28508;&#22312;&#35828;&#35805;&#20154;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#65292;&#25104;&#21151;&#29983;&#25104;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19968;&#33268;&#24615;&#21644;&#24615;&#21035;&#19981;&#26126;&#30830;&#24615;&#26356;&#24378;&#30340;&#22768;&#38899;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#39564;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.00375</link><description>&lt;p&gt;
&#29983;&#25104;&#22810;&#35821;&#35328;&#30340;&#24615;&#21035;&#19981;&#26126;&#30830;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#22768;&#38899;
&lt;/p&gt;
&lt;p&gt;
Generating Multilingual Gender-Ambiguous Text-to-Speech Voices. (arXiv:2211.00375v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00375
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#22810;&#35821;&#35328;&#30340;&#24615;&#21035;&#19981;&#26126;&#30830;&#30340;TTS&#22768;&#38899;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#24615;&#21035;&#24863;&#30693;&#26041;&#27861;&#20174;&#28508;&#22312;&#35828;&#35805;&#20154;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#65292;&#25104;&#21151;&#29983;&#25104;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19968;&#33268;&#24615;&#21644;&#24615;&#21035;&#19981;&#26126;&#30830;&#24615;&#26356;&#24378;&#30340;&#22768;&#38899;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#39564;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#29992;&#25143;&#30028;&#38754;&#30340;&#24615;&#21035;&#26159;&#20854;&#34987;&#24863;&#30693;&#36523;&#20221;&#30340;&#20851;&#38190;&#20803;&#32032;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30028;&#38754;&#24320;&#22987;&#37319;&#29992;&#19981;&#26126;&#30830;&#30340;&#24615;&#21035;&#65292;&#32780;&#19981;&#26159;&#26126;&#30830;&#30028;&#23450;&#20026;&#30007;&#24615;&#25110;&#22899;&#24615;&#12290;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#22312;&#22810;&#35828;&#35805;&#20154;&#65292;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#29983;&#25104;&#26032;&#30340;&#24615;&#21035;&#19981;&#26126;&#30830;&#30340;TTS&#35821;&#38899;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#25552;&#20986;&#30340;&#24615;&#21035;&#24863;&#30693;&#26041;&#27861;&#26377;&#25928;&#22320;&#20174;&#28508;&#22312;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#37319;&#26679;&#26469;&#23454;&#29616;&#30340;&#12290;&#24191;&#27867;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#28165;&#26970;&#22320;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#19968;&#31995;&#21015;&#26032;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#22768;&#38899;&#65292;&#36825;&#20123;&#22768;&#38899;&#22312;&#25152;&#26377;&#32771;&#23519;&#30340;&#35821;&#35328;&#20013;&#37117;&#34987;&#35748;&#20026;&#27604;&#22522;&#32447;&#22768;&#38899;&#26356;&#20855;&#24615;&#21035;&#19981;&#26126;&#30830;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24615;&#21035;&#35748;&#30693;&#34987;&#21457;&#29616;&#22312;&#21548;&#20247;&#30340;&#20004;&#20010;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#65306;&#27597;&#35821;&#21644;&#24615;&#21035;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#21487;&#38752;&#22320;&#29983;&#25104;&#22810;&#31181;&#24615;&#21035;&#19981;&#26126;&#30830;&#22768;&#38899;&#30340;&#31995;&#32479;&#24615;&#21644;&#32463;&#36807;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The gender of any voice user interface is a key element of its perceived identity. Recently, there has been increasing interest in interfaces where the gender is ambiguous rather than clearly identifying as female or male. This work addresses the task of generating novel gender-ambiguous TTS voices in a multi-speaker, multilingual setting. This is accomplished by efficiently sampling from a latent speaker embedding space using a proposed gender-aware method. Extensive objective and subjective evaluations clearly indicate that this method is able to efficiently generate a range of novel, diverse voices that are consistent and perceived as more gender-ambiguous than a baseline voice across all the languages examined. Interestingly, the gender perception is found to be robust across two demographic factors of the listeners: native language and gender. To our knowledge, this is the first systematic and validated approach that can reliably generate a variety of gender-ambiguous voices.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#28085;&#30422;&#24191;&#27867;&#38889;&#35821;&#35821;&#27861;&#38169;&#35823;&#30340;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;KAGAS&#27880;&#37322;&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#26174;&#31034;&#20854;&#22312;&#26356;&#24191;&#27867;&#30340;&#38169;&#35823;&#31867;&#22411;&#19978;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#20351;&#29992;&#30340;&#32479;&#35745;&#38889;&#35821;GEC&#31995;&#32479;(Hanspell)&#12290;</title><link>http://arxiv.org/abs/2210.14389</link><description>&lt;p&gt;
&#36208;&#21521;&#26631;&#20934;&#21270;&#30340;&#38889;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65306;&#25968;&#25454;&#38598;&#19982;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
Towards standardizing Korean Grammatical Error Correction: Datasets and Annotation. (arXiv:2210.14389v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#28085;&#30422;&#24191;&#27867;&#38889;&#35821;&#35821;&#27861;&#38169;&#35823;&#30340;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;KAGAS&#27880;&#37322;&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#26174;&#31034;&#20854;&#22312;&#26356;&#24191;&#27867;&#30340;&#38169;&#35823;&#31867;&#22411;&#19978;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#20351;&#29992;&#30340;&#32479;&#35745;&#38889;&#35821;GEC&#31995;&#32479;(Hanspell)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#27604;&#33521;&#25991;&#31561;&#20854;&#20182;&#20027;&#35201;&#35821;&#35328;&#65292;&#30740;&#31350;&#38889;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;(GEC)&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#38382;&#39064;&#24402;&#22240;&#20110;&#32570;&#20047;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#29992;&#20110;&#38889;&#35821;GEC&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#26412;&#25991;&#20174;&#19981;&#21516;&#26469;&#28304;(Kor-Lang8, Kor-Native, &#21644; Kor-Learner)&#25910;&#38598;&#20102;&#19977;&#20010;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#38889;&#35821;&#35821;&#27861;&#38169;&#35823;&#12290;&#32771;&#34385;&#21040;&#38889;&#35821;&#35821;&#27861;&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#20026;&#38889;&#35821;&#23450;&#20041;&#20102;14&#31181;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;KAGAS(&#38889;&#22269;&#33258;&#21160;&#35821;&#27861;&#38169;&#35823;&#27880;&#37322;&#31995;&#32479;)&#65292;&#21487;&#20197;&#20174;&#24179;&#34892;&#35821;&#26009;&#24211;&#33258;&#21160;&#27880;&#37322;&#38169;&#35823;&#31867;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;KAGAS&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#25552;&#20379;&#20102;&#20174;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#26174;&#31034;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#26356;&#24191;&#27867;&#30340;&#38169;&#35823;&#31867;&#22411;&#19978;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#20351;&#29992;&#30340;&#32479;&#35745;&#38889;&#35821;GEC&#31995;&#32479;(Hanspell)&#65292;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#26377;&#29992;&#24615;&#12290;&#26412;&#25991;&#20013;&#20351;&#29992;&#30340;&#23454;&#29616;&#21644;&#25968;&#25454;&#38598;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on Korean grammatical error correction (GEC) is limited, compared to other major languages such as English. We attribute this problematic circumstance to the lack of a carefully designed evaluation benchmark for Korean GEC. In this work, we collect three datasets from different sources (Kor-Lang8, Kor-Native, and Kor-Learner) that covers a wide range of Korean grammatical errors. Considering the nature of Korean grammar, We then define 14 error types for Korean and provide KAGAS (Korean Automatic Grammatical error Annotation System), which can automatically annotate error types from parallel corpora. We use KAGAS on our datasets to make an evaluation benchmark for Korean, and present baseline models trained from our datasets. We show that the model trained with our datasets significantly outperforms the currently used statistical Korean GEC system (Hanspell) on a wider range of error types, demonstrating the diversity and usefulness of the datasets. The implementations and dat
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HighGEN&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#30701;&#35821;&#23884;&#20837;&#25628;&#32034;&#26041;&#27861;&#29983;&#25104;&#23454;&#20307;&#20016;&#23500;&#30340;&#20266;&#23383;&#20856;&#65292;&#22312;&#20351;&#29992;&#23884;&#20837;&#36317;&#31163;&#39564;&#35777;&#36807;&#31243;&#20943;&#23569;&#35823;&#25253;&#30340;&#22522;&#30784;&#19978;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;NER&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2210.07586</link><description>&lt;p&gt;
&#20351;&#29992;&#30701;&#35821;&#34920;&#31034;&#26597;&#35810;&#33258;&#21160;&#29983;&#25104;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations. (arXiv:2210.07586v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HighGEN&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#30701;&#35821;&#23884;&#20837;&#25628;&#32034;&#26041;&#27861;&#29983;&#25104;&#23454;&#20307;&#20016;&#23500;&#30340;&#20266;&#23383;&#20856;&#65292;&#22312;&#20351;&#29992;&#23884;&#20837;&#36317;&#31163;&#39564;&#35777;&#36807;&#31243;&#20943;&#23569;&#35823;&#25253;&#30340;&#22522;&#30784;&#19978;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;NER&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24369;&#30417;&#30563;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#20381;&#36182;&#20110;&#30001;&#19987;&#23478;&#25552;&#20379;&#30340;&#39046;&#22495;&#29305;&#23450;&#35789;&#20856;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#27809;&#26377;&#23383;&#20856;&#30340;&#39046;&#22495;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#21487;&#34892;&#12290;&#22312;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#30701;&#35821;&#26816;&#32034;&#27169;&#22411;&#33258;&#21160;&#20174;&#32500;&#22522;&#30334;&#31185;&#20013;&#25552;&#21462;&#23454;&#20307;&#26500;&#24314;&#20102;&#20266;&#23383;&#20856;&#65292;&#20294;&#36825;&#20123;&#23383;&#20856;&#30340;&#35206;&#30422;&#38754;&#24448;&#24448;&#26377;&#38480;&#65292;&#22240;&#20026;&#26816;&#32034;&#22120;&#24456;&#21487;&#33021;&#20250;&#26816;&#32034;&#21040;&#27969;&#34892;&#30340;&#23454;&#20307;&#32780;&#19981;&#26159;&#32597;&#35265;&#30340;&#23454;&#20307;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;HighGEN&#65292;&#23427;&#20351;&#29992;&#20855;&#26377;&#39640;&#35206;&#30422;&#29575;&#30340;&#20266;&#23383;&#20856;&#29983;&#25104;NER&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#26041;&#27861;&#8212;&#8212;&#30701;&#35821;&#23884;&#20837;&#25628;&#32034;&#26469;&#21019;&#24314;&#23500;&#23454;&#20307;&#23383;&#20856;&#65292;&#35813;&#26041;&#27861;&#40723;&#21169;&#26816;&#32034;&#22120;&#22312;&#19968;&#20010;&#23494;&#38598;&#30340;&#21508;&#31181;&#23454;&#20307;&#30340;&#31354;&#38388;&#20013;&#25628;&#32034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#23454;&#20307;&#25552;&#21450;&#21644;&#23454;&#20307;&#31867;&#22411;&#20043;&#38388;&#23884;&#20837;&#36317;&#31163;&#30340;&#26032;&#30340;&#39564;&#35777;&#36807;&#31243;&#65292;&#20197;&#20943;&#23569;&#39640;&#35206;&#30422;&#29575;&#20266;&#26631;&#31614;&#20013;&#30340;&#35823;&#25253;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most weakly supervised named entity recognition (NER) models rely on domain-specific dictionaries provided by experts. This approach is infeasible in many domains where dictionaries do not exist. While a phrase retrieval model was used to construct pseudo-dictionaries with entities retrieved from Wikipedia automatically in a recent study, these dictionaries often have limited coverage because the retriever is likely to retrieve popular entities rather than rare ones. In this study, we present a novel framework, HighGEN, that generates NER datasets with high-coverage pseudo-dictionaries. Specifically, we create entity-rich dictionaries with a novel search method, called phrase embedding search, which encourages the retriever to search a space densely populated with various entities. In addition, we use a new verification process based on the embedding distance between candidate entity mentions and entity types to reduce the false-positive noise in weak labels generated by high-coverage 
&lt;/p&gt;</description></item><item><title>GPT-3&#22312;&#26032;&#38395;&#25688;&#35201;&#20013;&#20855;&#26377;&#20986;&#33394;&#34920;&#29616;&#65292;&#20294;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#26080;&#27861;&#21487;&#38752;&#35780;&#20272;&#20854;&#28165;&#26224;&#24230;&#21644;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2209.12356</link><description>&lt;p&gt;
GPT-3&#26102;&#20195;&#30340;&#26032;&#38395;&#25688;&#35201;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
News Summarization and Evaluation in the Era of GPT-3. (arXiv:2209.12356v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12356
&lt;/p&gt;
&lt;p&gt;
GPT-3&#22312;&#26032;&#38395;&#25688;&#35201;&#20013;&#20855;&#26377;&#20986;&#33394;&#34920;&#29616;&#65292;&#20294;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#26080;&#27861;&#21487;&#38752;&#35780;&#20272;&#20854;&#28165;&#26224;&#24230;&#21644;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-3&#22312;NLP&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#26497;&#22823;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#30528;&#30524;&#20110;&#32463;&#20856;&#22522;&#20934;&#39046;&#22495;&#26032;&#38395;&#25688;&#35201;&#65292;&#30740;&#31350;&#20102;&#20854;&#23545;&#25991;&#26412;&#25688;&#35201;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GPT-3&#25688;&#35201;&#19982;&#22522;&#20110;&#22823;&#22411;&#25688;&#35201;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#24494;&#35843;&#27169;&#22411;&#30456;&#27604;&#36739;&#65292;&#19981;&#20165;&#20154;&#31867;&#26356;&#21916;&#27426;&#20351;&#29992;&#20165;&#19968;&#20010;&#20219;&#21153;&#25551;&#36848;&#26469;&#20419;&#21457;&#30340;GPT-3&#25688;&#35201;&#65292;&#32780;&#19988;&#36825;&#20123;&#25688;&#35201;&#20063;&#19981;&#20250;&#20687;&#19968;&#20123;&#24120;&#35268;&#25968;&#25454;&#38598;&#20855;&#26377;&#30340;&#20449;&#24687;&#20107;&#23454;&#19981;&#20934;&#30830;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#36825;&#23545;&#20110;&#35780;&#20272;&#30340;&#24847;&#20041;&#65292;&#23588;&#20854;&#26159;&#37329;&#26631;&#20934;&#27979;&#35797;&#38598;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#22522;&#20110;&#21442;&#32771;&#25991;&#26412;&#36824;&#26159;&#26080;&#21442;&#32771;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#37117;&#19981;&#33021;&#21487;&#38752;&#22320;&#35780;&#20272;GPT-3&#25688;&#35201;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#36890;&#29992;&#25688;&#35201;&#20043;&#22806;&#30340;&#19968;&#31181;&#22330;&#26223;&#65292;&#21363;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#25688;&#35201;&#65292;&#24182;&#23637;&#31034;&#20102;&#24494;&#35843;&#26041;&#27861;&#19982;&#20419;&#21457;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#20026;&#20102;&#25903;&#25345;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of prompting large language models like GPT-3 has led to a paradigm shift in NLP research. In this paper, we study its impact on text summarization, focusing on the classic benchmark domain of news summarization. First, we investigate how GPT-3 compares against fine-tuned models trained on large summarization datasets. We show that not only do humans overwhelmingly prefer GPT-3 summaries, prompted using only a task description, but these also do not suffer from common dataset-specific issues such as poor factuality. Next, we study what this means for evaluation, particularly the role of gold standard test sets. Our experiments show that both reference-based and reference-free automatic metrics cannot reliably evaluate GPT-3 summaries. Finally, we evaluate models on a setting beyond generic summarization, specifically keyword-based summarization, and show how dominant fine-tuning approaches compare to prompting.  To support further research, we release: (a) a corpus o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Machine Personality Inventory (MPI)&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20010;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LLM)&#20855;&#26377;&#20010;&#24615;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;Personality Prompting (P^2)&#26041;&#27861;&#65292;&#29992;&#20110;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;LLM&#20855;&#26377;&#29305;&#23450;&#20010;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.07550</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#35825;&#23548;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Inducing Personality in Pre-trained Language Models. (arXiv:2206.07550v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Machine Personality Inventory (MPI)&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20010;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LLM)&#20855;&#26377;&#20010;&#24615;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;Personality Prompting (P^2)&#26041;&#27861;&#65292;&#29992;&#20110;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;LLM&#20855;&#26377;&#29305;&#23450;&#20010;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#36215;&#28304;&#20110;&#21746;&#23398;&#25506;&#32034;&#65292;&#20851;&#27880;&#20010;&#20307;&#22312;&#24605;&#32771;&#12289;&#24773;&#24863;&#21644;&#34892;&#20026;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#19982;&#20154;&#31867;&#26085;&#24120;&#21512;&#20316;&#30340;&#31038;&#20132;&#26426;&#22120;&#65292;&#25105;&#20204;&#24819;&#30693;&#36947;&#65306;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#25317;&#26377;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20010;&#24615;&#65311;&#22914;&#26524;&#26159;&#65292;&#25105;&#20204;&#22914;&#20309;&#35780;&#20272;&#23427;&#20204;&#65311;&#36827;&#19968;&#27493;&#22320;&#65292;&#22312;&#27492;&#35780;&#20272;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#22914;&#20309;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;&#20855;&#26377;&#29305;&#23450;&#20010;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#65311;&#20026;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#22120;&#20010;&#24615;&#24211;(Machine Personality Inventory, MPI)&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#30340;&#20010;&#24615;&#12290;MPI&#36981;&#24490;&#26631;&#20934;&#21270;&#30340;&#20010;&#24615;&#27979;&#35797;&#65292;&#22522;&#20110;&#20116;&#22240;&#32032;&#20154;&#26684;&#29702;&#35770;&#21644;&#20154;&#26684;&#35780;&#20272;&#24211;&#24314;&#31435;&#12290;&#36890;&#36807;&#29992;MPI&#31995;&#32479;&#22320;&#35780;&#20272;LLM&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;LLM&#30340;&#20010;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#20010;&#24615;&#25552;&#31034;(Personality Prompting, P^2)&#26041;&#27861;&#65292;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;LLMs&#20855;&#26377;&#29305;&#23450;&#30340;&#20010;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Originating as a philosophical quest, the study of personality concerns how individuals differ in thinking, feeling, and behaving. Towards building social machines that work with humans on a daily basis, we are motivated to ask: Do existing Large Language Models (LLMs) possess personalities akin to their human counterparts? If so, how can we evaluate them? Further, given this evaluation framework, how can we induce a particular personality in a controllable fashion? To answer these three questions, we propose the Machine Personality Inventory (MPI) dataset for evaluating the machine personality; MPI follows standardized personality tests, built upon the Big Five Personality Factors (Big Five) theory and personality assessment inventories. By systematically evaluating LLMs with MPI, we provide the first piece of evidence showing the existence of personality in LLMs. We further devise a Personality Prompting (P^2) method to induce LLMs with a specific personality in a controllable manner
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIVERSE&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22810;&#26679;&#30340;&#25552;&#31034;&#12289;&#39564;&#35777;&#22120;&#36807;&#28388;&#19981;&#27491;&#30830;&#30340;&#31572;&#26696;&#24182;&#36880;&#20010;&#39564;&#35777;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.02336</link><description>&lt;p&gt;
&#20351;&#29992;&#27493;&#39588;&#24863;&#30693;&#39564;&#35777;&#22120;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
Making Large Language Models Better Reasoners with Step-Aware Verifier. (arXiv:2206.02336v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIVERSE&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22810;&#26679;&#30340;&#25552;&#31034;&#12289;&#39564;&#35777;&#22120;&#36807;&#28388;&#19981;&#27491;&#30830;&#30340;&#31572;&#26696;&#24182;&#36880;&#20010;&#39564;&#35777;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#23398;&#20064;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#35821;&#35328;&#27169;&#22411;&#20174;&#26377;&#38480;&#30340;&#31034;&#20363;&#20013;&#36827;&#34892;&#27867;&#21270;&#12290;&#20687;GPT-3&#21644;PaLM&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#65292;&#27604;&#22914;GSM8K&#65292;&#36825;&#26159;&#19968;&#20010;&#31639;&#26415;&#38382;&#39064;&#30340;&#22522;&#20934;&#12290;&#20026;&#20102;&#25552;&#39640;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#25552;&#31034;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#22312;&#32473;&#20986;&#26368;&#32456;&#31572;&#26696;&#20043;&#21069;&#36827;&#34892;&#19968;&#31995;&#21015;&#25512;&#29702;&#27493;&#39588;&#65292;&#20174;&#32780;&#22312;GSM8K&#19978;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#25552;&#39640;&#65292;&#20174;17.9%&#25552;&#39640;&#21040;&#20102;58.1%&#30340;&#35299;&#39064;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIVERSE&#65288;&#22810;&#26679;&#30340;&#25512;&#29702;&#27493;&#39588;&#39564;&#35777;&#22120;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;DIVERSE&#26377;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#31532;&#19968;&#65292;&#23427;&#29983;&#25104;&#22810;&#26679;&#30340;&#25552;&#31034;&#65292;&#25506;&#32034;&#30456;&#21516;&#38382;&#39064;&#30340;&#19981;&#21516;&#25512;&#29702;&#36335;&#24452;&#65307;&#31532;&#20108;&#65292;&#23427;&#20351;&#29992;&#39564;&#35777;&#22120;&#26681;&#25454;&#21152;&#26435;&#25237;&#31080;&#26041;&#26696;&#36807;&#28388;&#25481;&#19981;&#27491;&#30830;&#30340;&#31572;&#26696;&#65307;&#31532;&#19977;&#65292;&#23427;&#36880;&#20010;&#39564;&#35777;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DIVERSE (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DIVERSE has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#32780;&#20840;&#30340;EAE&#26412;&#20307;&#35770;&#65292;105&#20010;&#20107;&#20214;&#21644;220&#20010;&#35770;&#20803;&#35282;&#33394;&#30340;&#21253;&#21547;&#22312;&#20869;&#65292;&#21033;&#29992;&#36825;&#20010;&#26412;&#20307;&#35770;&#21019;&#24314;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#30340;&#36890;&#29992;&#24615;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GENEVA&#65292;&#20849;&#21253;&#21547;&#22235;&#20010;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2205.12505</link><description>&lt;p&gt;
GENEVA&#65306;&#8220;&#36890;&#29992;&#24615;&#22522;&#20934;&#27979;&#35797;&#8221;&#20107;&#20214;&#35770;&#20803;&#25552;&#21462;&#65292;&#28085;&#30422;&#25968;&#30334;&#31181;&#20107;&#20214;&#31867;&#22411;&#21644;&#35770;&#20803;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles. (arXiv:2205.12505v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#32780;&#20840;&#30340;EAE&#26412;&#20307;&#35770;&#65292;105&#20010;&#20107;&#20214;&#21644;220&#20010;&#35770;&#20803;&#35282;&#33394;&#30340;&#21253;&#21547;&#22312;&#20869;&#65292;&#21033;&#29992;&#36825;&#20010;&#26412;&#20307;&#35770;&#21019;&#24314;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#30340;&#36890;&#29992;&#24615;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GENEVA&#65292;&#20849;&#21253;&#21547;&#22235;&#20010;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20107;&#20214;&#35770;&#20803;&#25552;&#21462;&#65288;EAE&#65289;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#20197;&#36866;&#24212;&#26032;&#30340;&#20107;&#20214;&#31867;&#22411;&#21644;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#22914;ACE&#21644;ERE&#21482;&#28085;&#30422;&#19981;&#21040;40&#31181;&#20107;&#20214;&#31867;&#22411;&#21644;25&#31181;&#38754;&#21521;&#23454;&#20307;&#30340;&#35770;&#20803;&#35282;&#33394;&#12290;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#22810;&#26679;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#24433;&#21709;&#20102;&#36825;&#20123;&#25968;&#25454;&#38598;&#23545;EAE&#27169;&#22411;&#36890;&#29992;&#24615;&#30340;&#20805;&#20998;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#32780;&#20840;&#30340;EAE&#26412;&#20307;&#35770;&#65292;&#22312;FrameNet&#30340;&#22522;&#30784;&#19978;&#21019;&#24314;&#20102;&#21253;&#21547;115&#20010;&#20107;&#20214;&#21644;220&#20010;&#35770;&#20803;&#35282;&#33394;&#30340;&#26412;&#20307;&#35770;&#65292;&#20854;&#20013;&#35768;&#22810;&#35282;&#33394;&#19981;&#26159;&#23454;&#20307;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26412;&#20307;&#35770;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;GENEVA&#65292;&#19968;&#31181;&#22810;&#26679;&#21270;&#30340;&#36890;&#29992;&#24615;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22235;&#20010;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works in Event Argument Extraction (EAE) have focused on improving model generalizability to cater to new events and domains. However, standard benchmarking datasets like ACE and ERE cover less than 40 event types and 25 entity-centric argument roles. Limited diversity and coverage hinder these datasets from adequately evaluating the generalizability of EAE models. In this paper, we first contribute by creating a large and diverse EAE ontology. This ontology is created by transforming FrameNet, a comprehensive semantic role labeling (SRL) dataset for EAE, by exploiting the similarity between these two tasks. Then, exhaustive human expert annotations are collected to build the ontology, concluding with 115 events and 220 argument roles, with a significant portion of roles not being entities. We utilize this ontology to further introduce GENEVA, a diverse generalizability benchmarking dataset comprising four test suites, aimed at evaluating models' ability to handle limited data a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#37325;&#25552;&#31034;&#22686;&#24378;&#36328;&#35821;&#35328;&#25552;&#31034;&#30340;&#26041;&#27861;DPA&#65292;&#21033;&#29992;&#35821;&#35328;&#26080;&#20851;&#30340;&#36890;&#29992;&#25552;&#31034;&#26041;&#27861;&#65292;&#22823;&#22823;&#32531;&#35299;&#20102;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#22312;XNLI&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;&#26377;16&#20010;&#33521;&#35821;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#24615;&#33021;&#20174;34.99%&#25552;&#39640;&#21040;46.54%&#12290;</title><link>http://arxiv.org/abs/2202.07255</link><description>&lt;p&gt;
&#29992;&#21452;&#37325;&#25552;&#31034;&#22686;&#24378;&#36328;&#35821;&#35328;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing Cross-lingual Prompting with Dual Prompt Augmentation. (arXiv:2202.07255v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07255
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#37325;&#25552;&#31034;&#22686;&#24378;&#36328;&#35821;&#35328;&#25552;&#31034;&#30340;&#26041;&#27861;DPA&#65292;&#21033;&#29992;&#35821;&#35328;&#26080;&#20851;&#30340;&#36890;&#29992;&#25552;&#31034;&#26041;&#27861;&#65292;&#22823;&#22823;&#32531;&#35299;&#20102;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#22312;XNLI&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;&#26377;16&#20010;&#33521;&#35821;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#24615;&#33021;&#20174;34.99%&#25552;&#39640;&#21040;46.54%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20986;&#19968;&#31181;&#26032;&#30340;&#21452;&#37325;&#25552;&#31034;&#26041;&#27861;(DPA)&#65292;&#29992;&#20110;&#22312;&#36328;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#35821;&#35328;&#26080;&#20851;&#30340;&#36890;&#29992;&#25552;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20165;&#26377;&#27599;&#31867;16&#20010;&#33521;&#35821;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#22312;XNLI&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20174;34.99%&#25552;&#39640;&#21040;46.54%&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting shows promising results in few-shot scenarios. However, its strength for multilingual/cross-lingual problems has not been fully exploited. Zhao and Sch\"utze (2021) made initial explorations in this direction by presenting that cross-lingual prompting outperforms cross-lingual finetuning. In this paper, we conduct an empirical exploration on the effect of each component in cross-lingual prompting and derive language-agnostic Universal Prompting, which helps alleviate the discrepancies between source-language training and target-language inference. Based on this, we propose DPA, a dual prompt augmentation framework, aiming at relieving the data scarcity issue in few-shot cross-lingual prompting. Notably, for XNLI, our method achieves 46.54% with only 16 English training examples per class, significantly better than 34.99% of finetuning. Our code is available at https://github.com/DAMO-NLP-SG/DPA.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;Facebook&#21644;&#30701;&#20449;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#23545;&#22522;&#20110;&#35821;&#35328;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#24046;&#24322;&#24182;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#24179;&#21488;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.01802</link><description>&lt;p&gt;
Facebook&#21644;&#30701;&#20449;&#25991;&#26412;&#20043;&#38388;&#30340;&#19981;&#21516;&#26426;&#20250;&#19981;&#20250;&#22952;&#30861;&#22522;&#20110;&#35821;&#35328;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Different Affordances on Facebook and SMS Text Messaging Do Not Impede Generalization of Language-Based Predictive Models. (arXiv:2202.01802v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.01802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;Facebook&#21644;&#30701;&#20449;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#23545;&#22522;&#20110;&#35821;&#35328;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#24046;&#24322;&#24182;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#24179;&#21488;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#30340;&#31227;&#21160;&#35774;&#22791;&#20581;&#24247;&#24178;&#39044;&#36890;&#24120;&#20351;&#29992;&#22312;&#38750;&#31227;&#21160;&#35774;&#22791;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#65292;&#30001;&#20110;&#25910;&#38598;&#22823;&#37327;&#30701;&#20449;&#25968;&#25454;&#30340;&#22256;&#38590;&#21644;&#39640;&#26114;&#30340;&#36153;&#29992;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#36825;&#20123;&#24179;&#21488;&#20043;&#38388;&#30340;&#27169;&#22411;&#24046;&#24322;&#21644;&#27867;&#21270;&#23545;&#20110;&#27491;&#30830;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;120&#20010;&#20849;&#20139;&#36825;&#20004;&#31181;&#24179;&#21488;&#30340;&#29992;&#25143;&#26679;&#26412;&#65292;&#30740;&#31350;&#20102;Facebook&#21644;&#30701;&#20449;&#20043;&#38388;&#30340;&#24515;&#29702;&#35821;&#35328;&#24046;&#24322;&#20197;&#21450;&#20854;&#23545;&#39046;&#22495;&#22806;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#29992;&#25143;&#20351;&#29992;Facebook&#20998;&#20139;&#32463;&#39564;&#65288;&#22914;&#20241;&#38386;&#65289;&#65292;&#32780;&#20351;&#29992;&#30701;&#20449;&#36827;&#34892;&#20219;&#21153;&#23548;&#21521;&#21644;&#20250;&#35805;&#30446;&#30340;&#65288;&#20363;&#22914;&#35745;&#21010;&#30830;&#35748;&#65289;&#65292;&#21453;&#26144;&#20102;&#24046;&#24322;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#26816;&#39564;&#36825;&#20123;&#24046;&#24322;&#30340;&#19979;&#28216;&#25928;&#24212;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22522;&#20110;Facebook&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;Facebook&#21644;&#30701;&#20449;&#19978;&#20272;&#35745;&#24180;&#40836;&#65292;&#24615;&#21035;&#65292;&#25233;&#37057;&#65292;&#29983;&#27963;&#28385;&#24847;&#24230;&#21644;&#21387;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;Facebook&#21644;&#30701;&#20449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20013;&#27809;&#26377;&#26174;&#30528;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive mobile device-based health interventions often use machine learning models trained on non-mobile device data, such as social media text, due to the difficulty and high expense of collecting large text message (SMS) data. Therefore, understanding the differences and generalization of models between these platforms is crucial for proper deployment. We examined the psycho-linguistic differences between Facebook and text messages, and their impact on out-of-domain model performance, using a sample of 120 users who shared both. We found that users use Facebook for sharing experiences (e.g., leisure) and SMS for task-oriented and conversational purposes (e.g., plan confirmations), reflecting the differences in the affordances. To examine the downstream effects of these differences, we used pre-trained Facebook-based language models to estimate age, gender, depression, life satisfaction, and stress on both Facebook and SMS. We found no significant differences in correlations between 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#12289;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#20294;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#36739;&#20302;&#65292;&#38656;&#35201;&#20445;&#35777;&#20854;&#21487;&#25511;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.05337</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models. (arXiv:2201.05337v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05337
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#12289;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#20294;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#36739;&#20302;&#65292;&#38656;&#35201;&#20445;&#35777;&#20854;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#20013;&#26032;&#20852;&#30340;&#26041;&#21521;&#65292;&#34987;&#35748;&#20026;&#23545;&#20110;&#24320;&#21457;&#26356;&#33258;&#28982;&#12289;&#26356;&#31526;&#21512;&#29305;&#23450;&#24212;&#29992;&#22330;&#26223;&#30340;&#20808;&#36827;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#23588;&#20854;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;Transformer&#30340;PLMs&#65292;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26032;&#33539;&#24335;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#12289;&#26356;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#36739;&#20302;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#25511;&#24615;&#38656;&#35201;&#24471;&#21040;&#20445;&#35777;&#12290;&#20026;&#27492;&#65292;&#22522;&#20110;Transformer&#30340;PLMs&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#24050;&#25104;&#20026;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26032;&#30740;&#31350;&#28909;&#28857;&#12290;&#26368;&#36817;3-4&#24180;&#20986;&#29616;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#38024;&#23545;&#21487;&#33021;&#38656;&#35201;&#19981;&#21516;&#31867;&#22411;&#25511;&#21046;&#32422;&#26463;&#30340;&#19981;&#21516;CTG&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24403;&#21069;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controllable Text Generation (CTG) is emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that are more natural and better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the lower level of interpretability of deep neural networks, the controllability of these methods need to be guaranteed. To this end, controllable text generation using transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the recent 3-4 years, targeting different CTG tasks which may require different types of controlled constraints. In this paper, we present a systematic critical review on the com
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSEE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26435;&#37325;&#26356;&#26032;&#21644;&#26368;&#32456;&#27169;&#22411;&#26435;&#37325;&#30340;&#31232;&#30095;&#20808;&#39564;&#65292;&#20197;&#23454;&#29616;&#36164;&#28304;&#21644;&#21442;&#25968;&#25928;&#29575;&#39640;&#30340;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DSEE&#21487;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#24494;&#35843;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2111.00160</link><description>&lt;p&gt;
DSEE&#65306;&#21452;&#31232;&#30095;&#23884;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models. (arXiv:2111.00160v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.00160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSEE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26435;&#37325;&#26356;&#26032;&#21644;&#26368;&#32456;&#27169;&#22411;&#26435;&#37325;&#30340;&#31232;&#30095;&#20808;&#39564;&#65292;&#20197;&#23454;&#29616;&#36164;&#28304;&#21644;&#21442;&#25968;&#25928;&#29575;&#39640;&#30340;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DSEE&#21487;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#24494;&#35843;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24040;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26680;&#24515;&#65292;&#26159;&#24494;&#35843;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#36215;&#28857;&#12290;&#20294;&#35813;&#33539;&#20363;&#20173;&#23384;&#22312;&#20004;&#20010;&#30171;&#28857;&#65306;&#65288;a&#65289;&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#65288;&#20363;&#22914;&#65292;GPT-3&#26377;175B&#20010;&#21442;&#25968;&#65289;&#65292;&#21363;&#20351;&#26159;&#24494;&#35843;&#30340;&#36807;&#31243;&#20063;&#21487;&#33021;&#32791;&#26102;&#21644;&#35745;&#31639;&#36164;&#28304;&#23494;&#38598;&#65307;&#65288;b&#65289;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#19982;&#20854;&#36215;&#28857;&#19968;&#26679;&#22823;&#65292;&#36825;&#26082;&#19981;&#26126;&#26234;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#26356;&#19987;&#19994;&#30340;&#21151;&#33021;&#65292;&#20063;&#19981;&#23454;&#29992;&#65292;&#22240;&#20026;&#35768;&#22810;&#24494;&#35843;&#27169;&#22411;&#23558;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#30171;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#21644;&#21442;&#25968;&#26377;&#25928;&#30340;&#24494;&#35843;&#26694;&#26550;&#65292;&#21033;&#29992;&#26435;&#37325;&#26356;&#26032;&#21644;&#26368;&#32456;&#27169;&#22411;&#26435;&#37325;&#20013;&#30340;&#31232;&#30095;&#20808;&#39564;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21517;&#20026;&#21452;&#31232;&#30095;&#23884;&#20837;&#39640;&#25928;&#35843;&#20248;&#65288;DSEE&#65289;&#65292;&#26088;&#22312;&#23454;&#29616;&#20004;&#20010;&#20851;&#38190;&#30446;&#26631;&#65306;&#65288;i&#65289;&#36890;&#36807;&#24378;&#21046;&#31232;&#30095;&#24863;&#30693;&#20302;&#31209;&#36924;&#36817;&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#21442;&#25968;&#26377;&#25928;&#30340;&#24494;&#35843;&#65292;&#65288;ii&#65289;&#36890;&#36807;&#22312;&#24494;&#35843;&#26399;&#38388;&#21033;&#29992;&#31232;&#30095;&#24863;&#30693;&#26816;&#26597;&#28857;&#21644;&#22312;&#26368;&#32456;&#27169;&#22411;&#26435;&#37325;&#20013;&#20351;&#29992;&#21160;&#24577;&#31232;&#30095;&#26469;&#23454;&#29616;&#36164;&#28304;&#26377;&#25928;&#30340;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DSEE&#21487;&#22312;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#21644;&#25512;&#29702;&#26102;&#38388;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#24494;&#35843;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gigantic pre-trained models have become central to natural language processing (NLP), serving as the starting point for fine-tuning towards a range of downstream tasks. However, two pain points persist for this paradigm: (a) as the pre-trained models grow bigger (e.g., 175B parameters for GPT-3), even the fine-tuning process can be time-consuming and computationally expensive; (b) the fine-tuned model has the same size as its starting point by default, which is neither sensible due to its more specialized functionality, nor practical since many fine-tuned models will be deployed in resource-constrained environments. To address these pain points, we propose a framework for resource- and parameter-efficient fine-tuning by leveraging the sparsity prior in both weight updates and the final model weights. Our proposed framework, dubbed Dually Sparsity-Embedded Efficient Tuning (DSEE), aims to achieve two key objectives: (i) parameter efficient fine-tuning - by enforcing sparsity-aware low-r
&lt;/p&gt;</description></item></channel></rss>