<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#8212;&#8212;VideoTaskformer&#12290;&#36890;&#36807;&#20174;&#25972;&#20307;&#19978;&#23398;&#20064;&#34920;&#31034;&#26469;&#39564;&#35777;&#35270;&#39057;&#30340;&#27491;&#30830;&#25191;&#34892;&#65292;&#24182;&#39044;&#27979;&#19979;&#19968;&#27493;&#39588;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#20063;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#26816;&#27979;&#25945;&#23398;&#35270;&#39057;&#20013;&#38169;&#35823;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2303.13519</link><description>&lt;p&gt;
&#25945;&#23398;&#35270;&#39057;&#20013;&#20219;&#21153;&#32467;&#26500;&#30340;&#23398;&#20064;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Learning and Verification of Task Structure in Instructional Videos. (arXiv:2303.13519v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13519
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#8212;&#8212;VideoTaskformer&#12290;&#36890;&#36807;&#20174;&#25972;&#20307;&#19978;&#23398;&#20064;&#34920;&#31034;&#26469;&#39564;&#35777;&#35270;&#39057;&#30340;&#27491;&#30830;&#25191;&#34892;&#65292;&#24182;&#39044;&#27979;&#19979;&#19968;&#27493;&#39588;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#20063;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#26816;&#27979;&#25945;&#23398;&#35270;&#39057;&#20013;&#38169;&#35823;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22312;&#32447;&#25945;&#23398;&#35270;&#39057;&#25968;&#37327;&#24222;&#22823;&#65292;&#20174;&#35270;&#39057;&#20013;&#23398;&#20064;&#22810;&#27493;&#39588;&#20219;&#21153;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#26159;&#19968;&#20010;&#35825;&#20154;&#30340;&#30446;&#26631;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#8212;&#8212;VideoTaskformer&#65292;&#19987;&#27880;&#20110;&#34920;&#31034;&#25945;&#23398;&#35270;&#39057;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#30446;&#26631;&#26469;&#23545;VideoTaskformer&#36827;&#34892;&#39044;&#35757;&#32451;&#65306;&#20174;&#25945;&#23398;&#35270;&#39057;&#20013;&#38543;&#26426;&#23631;&#34109;&#30340;&#27493;&#39588;&#39044;&#27979;&#24369;&#30417;&#30563;&#30340;&#25991;&#26412;&#26631;&#31614;&#65288;&#36974;&#30422;&#27493;&#39588;&#24314;&#27169;&#65289;&#12290;&#19982;&#20808;&#21069;&#23398;&#20064;&#23616;&#37096;&#27493;&#39588;&#34920;&#31034;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20840;&#23616;&#23398;&#20064;&#65292;&#21033;&#29992;&#25972;&#20010;&#21608;&#22260;&#20219;&#21153;&#30340;&#35270;&#39057;&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;&#20174;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#39564;&#35777;&#19968;&#20010;&#26410;&#35265;&#36807;&#30340;&#35270;&#39057;&#26159;&#21542;&#27491;&#30830;&#25191;&#34892;&#32473;&#23450;&#30340;&#20219;&#21153;&#65292;&#20197;&#21450;&#39044;&#27979;&#22312;&#32473;&#23450;&#27493;&#39588;&#20043;&#21518;&#21487;&#33021;&#37319;&#21462;&#21738;&#20123;&#27493;&#39588;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#26816;&#27979;&#25945;&#23398;&#35270;&#39057;&#20013;&#30340;&#38169;&#35823;&#65292;&#20197;&#39564;&#35777;&#26159;&#21542;&#23384;&#22312;&#24322;&#24120;&#27493;&#39588;&#24182;&#26816;&#26597;&#27493;&#39588;&#26159;&#21542;&#25353;&#27491;&#30830;&#30340;&#39034;&#24207;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the enormous number of instructional videos available online, learning a diverse array of multi-step task models from videos is an appealing goal. We introduce a new pre-trained video model, VideoTaskformer, focused on representing the semantics and structure of instructional videos. We pre-train VideoTaskformer using a simple and effective objective: predicting weakly supervised textual labels for steps that are randomly masked out from an instructional video (masked step modeling). Compared to prior work which learns step representations locally, our approach involves learning them globally, leveraging video of the entire surrounding task as context. From these learned representations, we can verify if an unseen video correctly executes a given task, as well as forecast which steps are likely to be taken after a given step. We introduce two new benchmarks for detecting mistakes in instructional videos, to verify if there is an anomalous step and if steps are executed in the rig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#21462;&#21644;&#25490;&#24207;&#37329;&#34701;&#26415;&#35821;&#19978;&#20041;&#35789;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#19981;&#21516;&#26415;&#35821;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#12290;&#35813;&#31995;&#32479;&#24110;&#21161;&#29992;&#25143;&#26356;&#22909;&#22320;&#20102;&#35299;&#22797;&#26434;&#37329;&#34701;&#26415;&#35821;&#65292;&#24110;&#21161;&#25237;&#36164;&#32773;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.13475</link><description>&lt;p&gt;
&#23398;&#20064;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#26469;&#25490;&#24207;&#37329;&#34701;&#26415;&#35821;&#30340;&#19978;&#20041;&#35789;
&lt;/p&gt;
&lt;p&gt;
Learning Semantic Text Similarity to rank Hypernyms of Financial Terms. (arXiv:2303.13475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#21462;&#21644;&#25490;&#24207;&#37329;&#34701;&#26415;&#35821;&#19978;&#20041;&#35789;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#19981;&#21516;&#26415;&#35821;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#12290;&#35813;&#31995;&#32479;&#24110;&#21161;&#29992;&#25143;&#26356;&#22909;&#22320;&#20102;&#35299;&#22797;&#26434;&#37329;&#34701;&#26415;&#35821;&#65292;&#24110;&#21161;&#25237;&#36164;&#32773;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29992;&#25143;&#20351;&#29992;&#37329;&#34701;&#26381;&#21153;&#30340;&#26041;&#24335;&#21457;&#29983;&#20102;&#21464;&#38761;&#12290;&#38543;&#30528;&#25968;&#23383;&#21270;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#29992;&#25143;&#26356;&#21916;&#27426;&#22312;&#32447;&#26041;&#24335;&#26469;&#25191;&#34892;&#37329;&#34701;&#27963;&#21160;&#12290;&#36825;&#23548;&#33268;&#20102;&#22823;&#37327;&#30340;&#37329;&#34701;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;&#22823;&#22810;&#25968;&#25237;&#36164;&#32773;&#22312;&#20570;&#20986;&#20915;&#31574;&#21069;&#37117;&#20250;&#38405;&#35835;&#36825;&#20123;&#20869;&#23481;&#12290;&#27599;&#20010;&#34892;&#19994;&#37117;&#26377;&#29305;&#23450;&#20110;&#20854;&#36816;&#33829;&#39046;&#22495;&#30340;&#26415;&#35821;&#12290;&#38134;&#34892;&#21644;&#37329;&#34701;&#26381;&#21153;&#20063;&#19981;&#20363;&#22806;&#12290;&#20026;&#20102;&#23436;&#20840;&#29702;&#35299;&#36825;&#20123;&#20869;&#23481;&#65292;&#38656;&#35201;&#23545;&#37329;&#34701;&#26415;&#35821;&#26377;&#28145;&#20837;&#30340;&#20102;&#35299;&#12290;&#24403;&#29992;&#23427;&#25152;&#23646;&#30340;&#24191;&#20041;&#31867;&#21035;&#26469;&#35828;&#26126;&#26102;&#65292;&#19968;&#20010;&#26415;&#35821;&#30340;&#22522;&#26412;&#27010;&#24565;&#21464;&#24471;&#23481;&#26131;&#12290;&#36825;&#20010;&#24191;&#20041;&#31867;&#21035;&#34987;&#31216;&#20026;&#19978;&#20041;&#35789;&#12290;&#20363;&#22914;&#65292;&#8220;&#20538;&#21048;&#8221;&#26159;&#37329;&#34701;&#26415;&#35821;&#8220;&#26367;&#20195;&#20538;&#21048;&#8221;&#30340;&#19978;&#20041;&#35789;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#25552;&#21462;&#21644;&#25490;&#24207;&#32473;&#23450;&#37329;&#34701;&#26415;&#35821;&#30340;&#19978;&#20041;&#35789;&#12290;&#35813;&#31995;&#32479;&#32463;&#36807;&#37329;&#34701;&#25991;&#26412;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#19981;&#21516;&#37329;&#34701;&#26415;&#35821;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#29992;&#20110;&#25552;&#39640;&#23545;&#22797;&#26434;&#37329;&#34701;&#26415;&#35821;&#30340;&#29702;&#35299;&#65292;&#24110;&#21161;&#25237;&#36164;&#32773;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the years, there has been a paradigm shift in how users access financial services. With the advancement of digitalization more users have been preferring the online mode of performing financial activities. This has led to the generation of a huge volume of financial content. Most investors prefer to go through these contents before making decisions. Every industry has terms that are specific to the domain it operates in. Banking and Financial Services are not an exception to this. In order to fully comprehend these contents, one needs to have a thorough understanding of the financial terms. Getting a basic idea about a term becomes easy when it is explained with the help of the broad category to which it belongs. This broad category is referred to as hypernym. For example, "bond" is a hypernym of the financial term "alternative debenture". In this paper, we propose a system capable of extracting and ranking hypernyms for a given financial term. The system has been trained with fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#21330;&#20013;&#24739;&#32773;&#27835;&#30103;&#36807;&#31243;&#30340;&#38203;&#28860;&#20449;&#24687;&#65292;&#24182;&#19982;&#20960;&#20010;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#36275;&#22815;&#30340;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25552;&#21462;&#19968;&#21322;&#30340;&#27010;&#24565;&#26041;&#38754;&#20248;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#19988;&#27599;&#20010;&#27010;&#24565;&#30340;&#20010;&#20307;&#36816;&#21160;&#25551;&#36848;&#21487;&#20197;&#20998;&#37197;&#20108;&#36827;&#21046;&#26631;&#31614;&#65292;&#24182;&#19988;F&#20540;&#19981;&#20302;&#20110;0.75&#12290;&#36825;&#20123;&#31639;&#27861;&#34920;&#29616;&#20986;&#20102;&#20934;&#30830;&#25552;&#21462;&#20020;&#24202;&#31508;&#35760;&#20013;&#24247;&#22797;&#27835;&#30103;&#38203;&#28860;&#20449;&#24687;&#30340;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.13466</link><description>&lt;p&gt;
&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#24247;&#22797;&#38203;&#28860;&#20449;&#24687;&#65306;&#22522;&#20110;&#35268;&#21017;&#21644;&#26426;&#22120;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Extracting Physical Rehabilitation Exercise Information from Clinical Notes: a Comparison of Rule-Based and Machine Learning Natural Language Processing Techniques. (arXiv:2303.13466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#21330;&#20013;&#24739;&#32773;&#27835;&#30103;&#36807;&#31243;&#30340;&#38203;&#28860;&#20449;&#24687;&#65292;&#24182;&#19982;&#20960;&#20010;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#36275;&#22815;&#30340;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25552;&#21462;&#19968;&#21322;&#30340;&#27010;&#24565;&#26041;&#38754;&#20248;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#19988;&#27599;&#20010;&#27010;&#24565;&#30340;&#20010;&#20307;&#36816;&#21160;&#25551;&#36848;&#21487;&#20197;&#20998;&#37197;&#20108;&#36827;&#21046;&#26631;&#31614;&#65292;&#24182;&#19988;F&#20540;&#19981;&#20302;&#20110;0.75&#12290;&#36825;&#20123;&#31639;&#27861;&#34920;&#29616;&#20986;&#20102;&#20934;&#30830;&#25552;&#21462;&#20020;&#24202;&#31508;&#35760;&#20013;&#24247;&#22797;&#27835;&#30103;&#38203;&#28860;&#20449;&#24687;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24247;&#22797;&#38203;&#28860;&#22312;&#21330;&#20013;&#21518;&#24739;&#32773;&#30340;&#24247;&#22797;&#36807;&#31243;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#36890;&#36807;&#20010;&#24615;&#21270;&#27835;&#30103;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21487;&#20197;&#20351;&#24247;&#22797;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#12290;&#22312;&#39044;&#27979;&#24314;&#27169;&#20026;&#24739;&#32773;&#20998;&#37197;&#27835;&#30103;&#35745;&#21010;&#20043;&#21069;&#65292;&#33258;&#21160;&#21270;&#26041;&#27861;&#26159;&#20174;&#38750;&#32467;&#26500;&#21270;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#24247;&#22797;&#38203;&#28860;&#20449;&#24687;&#25152;&#24517;&#38656;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#26469;&#27880;&#37322;&#21330;&#20013;&#24739;&#32773;&#30340;&#27835;&#30103;&#36807;&#31243;&#65292;&#24182;&#23558;&#20854;&#19982;&#20960;&#20010;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36275;&#22815;&#30340;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25552;&#21462;&#19968;&#21322;&#30340;&#27010;&#24565;&#26041;&#38754;&#20248;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#19988;&#27599;&#20010;&#27010;&#24565;&#30340;&#20010;&#20307;&#36816;&#21160;&#25551;&#36848;&#21487;&#20197;&#20998;&#37197;&#20108;&#36827;&#21046;&#26631;&#31614;&#65292;&#24182;&#19988;F&#20540;&#19981;&#20302;&#20110;0.75&#12290;&#22312;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#37096;&#32626;&#21040;&#26080;&#26631;&#31614;&#25991;&#26723;&#20043;&#21069;&#65292;&#38656;&#35201;&#36827;&#34892;&#26356;&#22810;&#30340;&#30740;&#31350;&#65292;&#20294;&#23450;&#21046;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#34920;&#29616;&#20986;&#20102;&#20934;&#30830;&#25552;&#21462;&#20020;&#24202;&#31508;&#35760;&#20013;&#24247;&#22797;&#27835;&#30103;&#38203;&#28860;&#20449;&#24687;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physical rehabilitation plays a crucial role in the recovery process of post-stroke patients. By personalizing therapies for patients leveraging predictive modeling and electronic health records (EHRs), healthcare providers can make the rehabilitation process more efficient. Before predictive modeling can provide decision support for the assignment of treatment plans, automated methods are necessary to extract physical rehabilitation exercise information from unstructured EHRs. We introduce a rule-based natural language processing algorithm to annotate therapeutic procedures for stroke patients and compare it to several small machine learning models. We find that our algorithm outperforms these models in extracting half of the concepts where sufficient data is available, and individual exercise descriptions can be assigned binary labels with an f-score of no less than 0.75 per concept. More research needs to be done before these algorithms can be deployed on unlabeled documents, but cu
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#34892;&#20026;&#25506;&#32034;&#65292;&#20174;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#20013;&#36827;&#34892;&#31163;&#32447;&#23398;&#20064;&#65292;&#24182;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#34892;&#20026;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2303.13465</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#23618;&#34892;&#20026;&#25506;&#32034;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep RL with Hierarchical Action Exploration for Dialogue Generation. (arXiv:2303.13465v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#34892;&#20026;&#25506;&#32034;&#65292;&#20174;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#20013;&#36827;&#34892;&#31163;&#32447;&#23398;&#20064;&#65292;&#24182;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#34892;&#20026;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#30340;&#34892;&#20026;&#31354;&#38388;&#26497;&#20854;&#24222;&#22823;&#65292;&#22240;&#27492;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#65292;&#36817;&#20284;&#21160;&#24577;&#35268;&#21010;&#24517;&#39035;&#20351;&#29992;&#31574;&#30053;&#25913;&#36827;&#21644;&#34892;&#20026;&#37319;&#26679;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#26377;&#20215;&#20540;&#30340;&#22238;&#24212;&#38750;&#24120;&#31232;&#30095;&#65292;&#22240;&#27492;&#20351;&#29992;&#38543;&#26426;&#37319;&#26679;&#30340;&#36138;&#24515;&#31574;&#30053;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21452;&#31890;&#24230;&#30340; Q-function &#24182;&#36890;&#36807;&#25506;&#32034;&#26368;&#26377;&#21069;&#36884;&#30340;&#22238;&#24212;&#31867;&#21035;&#26469;&#32531;&#35299;&#36825;&#20010;&#23616;&#38480;&#24615;&#12290;&#35813;&#31639;&#27861;&#20174;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#32454;&#33410;&#30340;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#20013;&#36827;&#34892;&#31163;&#32447;&#23398;&#20064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventionally, since the natural language action space is astronomical, approximate dynamic programming applied to dialogue generation involves policy improvement with action sampling. However, such a practice is inefficient for reinforcement learning (RL) because the eligible (high action value) responses are very sparse, and the greedy policy sustained by the random sampling is flabby. This paper shows that the performance of dialogue policy positively correlated with sampling size by theoretical and experimental. We introduce a novel dual-granularity Q-function to alleviate this limitation by exploring the most promising response category to intervene in the sampling. It extracts the actions following the grained hierarchy, which can achieve the optimum with fewer policy iterations. Our approach learns in the way of offline RL from multiple reward functions designed to recognize human emotional details. Empirical studies demonstrate that our algorithm outperforms the baseline metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;W2KPE&#31639;&#27861;&#29992;&#20110;&#20851;&#38190;&#35789;&#25277;&#21462;&#12290;&#35813;&#31639;&#27861;&#22312;&#21333;&#19968;&#31867;&#21035;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#24471;&#20998;&#20026;45.04&#20998;&#65292;&#24182;&#20351;&#29992;&#20102;&#35789;-&#35789;&#20851;&#31995;&#12289;&#22810;&#31867;&#32858;&#28966;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#20851;&#38190;&#35789;&#35780;&#20998;&#31561;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2303.13463</link><description>&lt;p&gt;
W2KPE&#65306;&#22522;&#20110;&#35789;-&#35789;&#20851;&#31995;&#30340;&#20851;&#38190;&#35789;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
W2KPE: Keyphrase Extraction with Word-Word Relation. (arXiv:2303.13463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;W2KPE&#31639;&#27861;&#29992;&#20110;&#20851;&#38190;&#35789;&#25277;&#21462;&#12290;&#35813;&#31639;&#27861;&#22312;&#21333;&#19968;&#31867;&#21035;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#24471;&#20998;&#20026;45.04&#20998;&#65292;&#24182;&#20351;&#29992;&#20102;&#35789;-&#35789;&#20851;&#31995;&#12289;&#22810;&#31867;&#32858;&#28966;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#20851;&#38190;&#35789;&#35780;&#20998;&#31561;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;ICASSP 2023 MUG Challenge Track 4&#8212;&#8212;&#20851;&#38190;&#35789;&#25552;&#21462;&#26041;&#38754;&#30340;&#25552;&#20132;&#65292;&#26088;&#22312;&#20174;&#20250;&#35758;&#36164;&#26009;&#20013;&#25552;&#21462;&#19982;&#20250;&#35758;&#20027;&#39064;&#26368;&#30456;&#20851;&#30340;&#20851;&#38190;&#35789;&#12290;&#25105;&#20204;&#23558;&#27492;&#25361;&#25112;&#24314;&#27169;&#20026;&#21333;&#19968;&#31867;&#21035;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#24320;&#21457;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#25216;&#26415;&#65306;&#23545;&#20110;&#25968;&#25454;&#39044;&#22788;&#29702;&#65292;&#25105;&#20204;&#22312;&#21333;&#35789;&#20998;&#21106;&#21518;&#23545;&#25286;&#20998;&#30340;&#20851;&#38190;&#35789;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22810;&#20010;&#39044;&#22788;&#29702;&#21477;&#23376;&#34701;&#21512;&#20026;&#19968;&#20010;&#29255;&#27573;&#26469;&#22686;&#21152;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#25509;&#21463;&#30340;&#36755;&#20837;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#29992;&#22810;&#31867;&#32858;&#28966;&#25439;&#22833;&#20989;&#25968;&#26367;&#25442;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#20851;&#38190;&#35789;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#20986;&#29616;&#30340;&#20851;&#38190;&#35789;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#28155;&#21152;&#20102;&#39069;&#22806;&#30340;&#36755;&#20986;&#23618;&#20197;&#36866;&#24212;&#24471;&#20998;&#24182;&#25490;&#21015;&#20851;&#38190;&#35789;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#20197;&#25214;&#21040;&#21333;&#35789;&#20998;&#21106;&#24037;&#20855;&#12289;&#39044;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#21644;&#30456;&#24212;&#36229;&#21442;&#25968;&#30340;&#26368;&#20339;&#32452;&#21512;&#12290;&#36890;&#36807;&#36825;&#20123;&#24314;&#35758;&#65292;&#25105;&#20204;&#22312;&#25361;&#25112;&#20013;&#24471;&#20998;&#20026;45.04&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission to ICASSP 2023 MUG Challenge Track 4, Keyphrase Extraction, which aims to extract keyphrases most relevant to the conference theme from conference materials. We model the challenge as a single-class Named Entity Recognition task and developed techniques for better performance on the challenge: For the data preprocessing, we encode the split keyphrases after word segmentation. In addition, we increase the amount of input information that the model can accept at one time by fusing multiple preprocessed sentences into one segment. We replace the loss function with the multi-class focal loss to address the sparseness of keyphrases. Besides, we score each appearance of keyphrases and add an extra output layer to fit the score to rank keyphrases. Exhaustive evaluations are performed to find the best combination of the word segmentation tool, the pre-trained embedding model, and the corresponding hyperparameters. With these proposals, we scored 45.04 on the
&lt;/p&gt;</description></item><item><title>CoBIT&#26159;&#19968;&#31181;&#23545;&#27604;&#24335;&#21452;&#21521;&#22270;&#25991;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#32479;&#19968;&#23545;&#27604;&#12289;&#22270;&#20687;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;unicoder-decoder&#32467;&#26500;&#65292;&#28789;&#27963;&#24615;&#39640;&#65292;&#20849;&#20139;&#30693;&#35782;&#65292;&#23545;&#22270;&#20687;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#37117;&#26377;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2303.13455</link><description>&lt;p&gt;
CoBIT: &#19968;&#31181;&#23545;&#27604;&#24335;&#21452;&#21521;&#22270;&#25991;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CoBIT: A Contrastive Bi-directional Image-Text Generation Model. (arXiv:2303.13455v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13455
&lt;/p&gt;
&lt;p&gt;
CoBIT&#26159;&#19968;&#31181;&#23545;&#27604;&#24335;&#21452;&#21521;&#22270;&#25991;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#32479;&#19968;&#23545;&#27604;&#12289;&#22270;&#20687;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;unicoder-decoder&#32467;&#26500;&#65292;&#28789;&#27963;&#24615;&#39640;&#65292;&#20849;&#20139;&#30693;&#35782;&#65292;&#23545;&#22270;&#20687;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#37117;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#20013;&#65292;&#24050;&#32463;&#20986;&#29616;&#20102;&#35768;&#22810;&#19982;&#23545;&#27604;&#24615;&#30446;&#26631;&#65288;&#22914;CLIP&#65289;&#12289;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;&#30446;&#26631;&#65288;&#22914;PaLI&#65289;&#25110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30446;&#26631;&#65288;&#22914;Parti&#65289;&#30456;&#23545;&#24212;&#30340;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#19977;&#20010;&#30446;&#26631;&#21487;&#20197;&#22312;&#30456;&#21516;&#30340;&#25968;&#25454;&#8212;&#8212;&#22270;&#20687;&#25991;&#26412;&#23545;&#19978;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#30456;&#20114;&#34917;&#20805;&#65292;&#22240;&#20026;&#23545;&#27604;&#25552;&#20379;&#20102;&#20840;&#23616;&#23545;&#40784;&#33021;&#21147;&#65292;&#29983;&#25104;&#20063;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#24335;&#21452;&#21521;&#22270;&#25991;&#29983;&#25104;&#27169;&#22411;&#65288;CoBIT&#65289;&#65292;&#35797;&#22270;&#23558;&#19977;&#20010;&#39044;&#35757;&#32451;&#30446;&#26631;&#32479;&#19968;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#12290;&#20855;&#20307;&#22320;&#65292;CoBIT&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;unicoder-decoder&#32467;&#26500;&#65292;&#21253;&#25324;&#22270;&#20687;unicoder&#12289;&#25991;&#26412;unicoder&#21644;&#36328;&#27169;&#24577;decoder&#12290;&#22270;&#20687;/&#25991;&#26412;unicoders&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#22312;&#32534;&#30721;&#21644;&#35299;&#30721;&#20043;&#38388;&#36827;&#34892;&#20999;&#25442;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#28789;&#27963;&#24615;&#21644;&#20849;&#20139;&#30693;&#35782;&#65292;&#23545;&#22270;&#20687;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#37117;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of vision and language has witnessed a proliferation of pre-trained foundation models. Most existing methods are independently pre-trained with contrastive objective like CLIP, image-to-text generative objective like PaLI, or text-to-image generative objective like Parti. However, the three objectives can be pre-trained on the same data, image-text pairs, and intuitively they complement each other as contrasting provides global alignment capacity and generation grants fine-grained understanding. In this work, we present a Contrastive Bi-directional Image-Text generation model (CoBIT), which attempts to unify the three pre-training objectives in one framework. Specifically, CoBIT employs a novel unicoder-decoder structure, consisting of an image unicoder, a text unicoder and a cross-modal decoder. The image/text unicoders can switch between encoding and decoding in different tasks, enabling flexibility and shared knowledge that benefits both image-to-text and text-to-image gen
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#35299;&#20915;&#20020;&#24202;&#25253;&#21578;&#30340;&#21435;&#35782;&#21035;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31995;&#32479;&#65292;&#33021;&#22815;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#25163;&#21160;&#35268;&#21017;&#30340;&#32467;&#26524;&#26377;&#25928;&#22320;&#21512;&#24182;&#65292;&#23454;&#29616;&#20102;&#23545;&#20020;&#24202;&#25991;&#26723;&#30340;&#39640;&#25928;&#20266;&#21517;&#21270;&#65292;&#21487;&#20197;&#20026;&#30740;&#31350;&#30446;&#30340;&#25552;&#20379;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25252;&#24739;&#32773;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2303.13451</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20020;&#24202;&#25968;&#25454;&#20179;&#24211;&#20013;&#25991;&#26723;&#20266;&#21517;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#30340;&#24320;&#21457;&#19982;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Development and validation of a natural language processing algorithm to pseudonymize documents in the context of a clinical data warehouse. (arXiv:2303.13451v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13451
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#35299;&#20915;&#20020;&#24202;&#25253;&#21578;&#30340;&#21435;&#35782;&#21035;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31995;&#32479;&#65292;&#33021;&#22815;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#25163;&#21160;&#35268;&#21017;&#30340;&#32467;&#26524;&#26377;&#25928;&#22320;&#21512;&#24182;&#65292;&#23454;&#29616;&#20102;&#23545;&#20020;&#24202;&#25991;&#26723;&#30340;&#39640;&#25928;&#20266;&#21517;&#21270;&#65292;&#21487;&#20197;&#20026;&#30740;&#31350;&#30446;&#30340;&#25552;&#20379;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25252;&#24739;&#32773;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#35299;&#20915;&#20020;&#24202;&#25253;&#21578;&#30340;&#21435;&#35782;&#21035;&#21270;&#38382;&#39064;&#65292;&#20197;&#20415;&#20026;&#30740;&#31350;&#30446;&#30340;&#35775;&#38382;&#25968;&#25454;&#65292;&#21516;&#26102;&#30830;&#20445;&#24739;&#32773;&#38544;&#31169;&#12290;&#30740;&#31350;&#31361;&#20986;&#20102;&#22312;&#27492;&#39046;&#22495;&#20849;&#20139;&#24037;&#20855;&#21644;&#36164;&#28304;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#65292;&#24182;&#20171;&#32461;&#20102;&#22823;&#24052;&#40654;&#22823;&#23398;&#21307;&#38498;(AP-HP)&#22312;&#20854;&#20020;&#24202;&#25968;&#25454;&#20179;&#24211;&#20013;&#23454;&#29616;&#25991;&#26412;&#25991;&#26723;&#31995;&#32479;&#20266;&#21517;&#21270;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#26681;&#25454;12&#31181;&#35782;&#21035;&#23454;&#20307;&#27880;&#37322;&#20102;&#19968;&#32452;&#20020;&#24202;&#25991;&#26723;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#28151;&#21512;&#31995;&#32479;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#25163;&#21160;&#35268;&#21017;&#30340;&#32467;&#26524;&#21512;&#24182;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;0.99&#30340;F1&#20998;&#25968;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23454;&#26045;&#36873;&#25321;&#65292;&#24182;&#23637;&#31034;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#27492;&#20219;&#21153;&#25152;&#28041;&#21450;&#30340;&#21162;&#21147;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#25991;&#26723;&#31867;&#22411;&#12289;&#35821;&#35328;&#27169;&#22411;&#25110;&#35268;&#21017;&#28155;&#21152;&#12290;&#25105;&#20204;&#22312;3&#26465;&#27454;BSD&#35768;&#21487;&#35777;&#19979;&#20849;&#20139;&#25351;&#21335;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of this study is to address the critical issue of de-identification of clinical reports in order to allow access to data for research purposes, while ensuring patient privacy. The study highlights the difficulties faced in sharing tools and resources in this domain and presents the experience of the Greater Paris University Hospitals (AP-HP) in implementing a systematic pseudonymization of text documents from its Clinical Data Warehouse. We annotated a corpus of clinical documents according to 12 types of identifying entities, and built a hybrid system, merging the results of a deep learning model as well as manual rules. Our results show an overall performance of 0.99 of F1-score. We discuss implementation choices and present experiments to better understand the effort involved in such a task, including dataset size, document types, language models, or rule addition. We share guidelines and code under a 3-Clause BSD license.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#35821;&#20041;&#36716;&#25442;&#23545;&#20110;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#35757;&#32451;&#30340;&#35821;&#20041;&#36716;&#25442;&#29983;&#25104;&#27169;&#22411;&#25104;&#21151;&#28151;&#28102;&#20102;&#22810;&#20010;&#26816;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2303.13408</link><description>&lt;p&gt;
&#35821;&#20041;&#36716;&#25442;&#28151;&#28102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#65292;&#32780;&#26816;&#32034;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. (arXiv:2303.13408v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#35821;&#20041;&#36716;&#25442;&#23545;&#20110;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#35757;&#32451;&#30340;&#35821;&#20041;&#36716;&#25442;&#29983;&#25104;&#27169;&#22411;&#25104;&#21151;&#28151;&#28102;&#20102;&#22810;&#20010;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#26377;&#22810;&#31181;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#29992;&#20110;&#35782;&#21035;&#24694;&#24847;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (&#20363;&#22914;&#34394;&#20551;&#20869;&#23481;&#21019;&#24314;&#25110;&#23398;&#26415;&#25220;&#34989;)&#20013;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#65292;&#21253;&#25324;&#36890;&#36807;&#27700;&#21360;&#25110;&#32479;&#35745;&#24322;&#24120;&#28857;&#12290;&#26412;&#25991;&#25506;&#31350;&#36825;&#20123;&#25991;&#26412;&#26816;&#27979;&#31639;&#27861;&#23545;&#20110;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#21547;&#20041;&#36716;&#25442;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#20102;&#19968;&#20010;11B&#21442;&#25968;&#30340;&#35821;&#20041;&#36716;&#25442;&#29983;&#25104;&#27169;&#22411;(DIPPER)&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23558;&#27573;&#33853;&#36827;&#34892;&#35821;&#20041;&#36716;&#25442;&#65292;&#21487;&#36873;&#25321;&#21033;&#29992;&#21608;&#22260;&#25991;&#26412;(&#20363;&#22914;&#29992;&#25143;&#20889;&#30340;&#25552;&#31034;)&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;DIPPER&#36824;&#20351;&#29992;&#26631;&#37327;&#26059;&#38062;&#26469;&#25511;&#21046;&#35821;&#20041;&#36716;&#25442;&#20013;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#37325;&#26032;&#25490;&#21015;&#30340;&#31243;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;DIPPER&#26469;&#36827;&#34892;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#35821;&#20041;&#36716;&#25442;&#65292;&#25104;&#21151;&#22320;&#28151;&#28102;&#20102;&#22810;&#20010;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#27700;&#21360;&#26816;&#27979;&#12289;GPTZero&#12289;DetectGPT&#21644;OpenAI&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#12290;&#20363;&#22914;&#65292;DIPPER&#23558;DetectGPT&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#20174;70.3%&#38477;&#33267;4.6%&#65288;&#22312;&#24658;&#23450;&#30340;1%&#35823;&#25253;&#29575;&#19979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
To detect the deployment of large language models for malicious use cases (e.g., fake content creation or academic plagiarism), several approaches have recently been proposed for identifying AI-generated text via watermarks or statistical irregularities. How robust are these detection algorithms to paraphrases of AI-generated text? To stress test these detectors, we first train an 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, optionally leveraging surrounding text (e.g., user-written prompts) as context. DIPPER also uses scalar knobs to control the amount of lexical diversity and reordering in the paraphrases. Paraphrasing text generated by three large language models (including GPT3.5-davinci-003) with DIPPER successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops the detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), withou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#38646;&#26679;&#26412;&#39046;&#22495;&#36716;&#31227;&#65292;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#33258;&#30001;&#25991;&#26412;&#36827;&#34892;&#39046;&#22495;&#21644;&#20219;&#21153;&#30693;&#35782;&#30340;&#20849;&#21516;&#23398;&#20064;&#65292;&#36890;&#36807; NLGU &#31574;&#30053;&#23454;&#29616;&#39046;&#22495;&#25968;&#25454;&#22686;&#24378;&#21644;&#26631;&#31614;&#39044;&#27979;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21644;&#25918;&#23556;&#23398;&#23376;&#39046;&#22495;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#32988;&#36807;&#20102;&#29616;&#26377;&#30340; SOTA&#12290;</title><link>http://arxiv.org/abs/2303.13386</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#27169;&#22411;&#30340;&#32452;&#21512;&#38646;&#26679;&#26412;&#39046;&#22495;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Compositional Zero-Shot Domain Transfer with Text-to-Text Models. (arXiv:2303.13386v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13386
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#38646;&#26679;&#26412;&#39046;&#22495;&#36716;&#31227;&#65292;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#33258;&#30001;&#25991;&#26412;&#36827;&#34892;&#39046;&#22495;&#21644;&#20219;&#21153;&#30693;&#35782;&#30340;&#20849;&#21516;&#23398;&#20064;&#65292;&#36890;&#36807; NLGU &#31574;&#30053;&#23454;&#29616;&#39046;&#22495;&#25968;&#25454;&#22686;&#24378;&#21644;&#26631;&#31614;&#39044;&#27979;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21644;&#25918;&#23556;&#23398;&#23376;&#39046;&#22495;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#32988;&#36807;&#20102;&#29616;&#26377;&#30340; SOTA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#65292;&#26631;&#31614;&#31232;&#32570;&#26159;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#29942;&#39048;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65288;DoT5 &#39046;&#22495;&#32452;&#21512;&#38646;&#26679;&#26412; T5&#65289;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#39046;&#22495;&#36716;&#31227;&#12290;&#22312;&#27809;&#26377;&#35775;&#38382;&#39046;&#22495;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;DoT5&#20197;&#22810;&#20219;&#21153;&#30340;&#26041;&#24335;&#20849;&#21516;&#23398;&#20064;&#39046;&#22495;&#30693;&#35782;&#65288;&#20174;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#33258;&#30001;&#25991;&#26412;&#30340; MLM &#20013;&#23398;&#20064;&#65289;&#21644;&#20219;&#21153;&#30693;&#35782;&#65288;&#20174;&#26356;&#23481;&#26131;&#33719;&#21462;&#30340;&#36890;&#29992;&#39046;&#22495;&#25968;&#25454;&#30340;&#20219;&#21153;&#35757;&#32451;&#20013;&#23398;&#20064;&#65289;&#12290;&#20026;&#20102;&#25552;&#39640;&#20219;&#21153;&#35757;&#32451;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026; NLGU &#30340;&#31574;&#30053;&#65306;&#25105;&#20204;&#21516;&#26102;&#20026;&#39046;&#22495;&#26631;&#31614;&#21040;&#25968;&#25454;&#29983;&#25104;&#35757;&#32451; NLG&#65292;&#20174;&#32780;&#23454;&#29616;&#29992;&#20110;&#33258;&#25105;&#24494;&#35843;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#29992;&#20110;&#26631;&#31614;&#39044;&#27979;&#30340; NLU &#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21644;&#25918;&#23556;&#23398;&#30340;&#36164;&#28304;&#36139;&#20047;&#23376;&#39046;&#22495;&#19978;&#35780;&#20272;&#20102; DoT5&#65292;&#37325;&#28857;&#20851;&#27880; NLI&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#23884;&#20837;&#23398;&#20064;&#12290;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;DoT5&#35777;&#26126;&#20102;&#32452;&#21512;&#36716;&#31227;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#38646;&#26679;&#26412;&#36716;&#31227;&#26041;&#38754;&#32988;&#36807;&#20102;&#29616;&#26377;&#30340; SOTA&#12290;
&lt;/p&gt;
&lt;p&gt;
Label scarcity is a bottleneck for improving task performance in specialised domains. We propose a novel compositional transfer learning framework (DoT5 domain compositional zero-shot T5) for zero-shot domain transfer. Without access to in-domain labels, DoT5 jointly learns domain knowledge (from MLM of unlabelled in-domain free text) and task knowledge (from task training on more readily available general-domain data) in a multi-task manner. To improve the transferability of task training, we design a strategy named NLGU: we simultaneously train NLG for in-domain label-to-data generation which enables data augmentation for self-finetuning and NLU for label prediction. We evaluate DoT5 on the biomedical domain and the resource-lean subdomain of radiology, focusing on NLI, text summarisation and embedding learning. DoT5 demonstrates the effectiveness of compositional transfer learning through multi-task learning. In particular, DoT5 outperforms the current SOTA in zero-shot transfer b
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#25945;&#32946;&#20013;&#26377;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#25991;&#26412;&#20869;&#23481;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21019;&#26032;&#30340;&#23454;&#38469;&#24615;&#21644;&#20262;&#29702;&#24615;&#23384;&#22312;&#25285;&#24551;&#65292;&#38656;&#35201;&#32771;&#34385;&#25216;&#26415;&#21487;&#34892;&#24615;&#12289;&#38544;&#31169;&#12289;&#24179;&#31561;&#21644;&#21892;&#24847;&#31561;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2303.13379</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#20013;&#30340;&#23454;&#38469;&#21644;&#20262;&#29702;&#25361;&#25112;&#65306;&#19968;&#39033;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Practical and Ethical Challenges of Large Language Models in Education: A Systematic Literature Review. (arXiv:2303.13379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13379
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#25945;&#32946;&#20013;&#26377;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#25991;&#26412;&#20869;&#23481;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21019;&#26032;&#30340;&#23454;&#38469;&#24615;&#21644;&#20262;&#29702;&#24615;&#23384;&#22312;&#25285;&#24551;&#65292;&#38656;&#35201;&#32771;&#34385;&#25216;&#26415;&#21487;&#34892;&#24615;&#12289;&#38544;&#31169;&#12289;&#24179;&#31561;&#21644;&#21892;&#24847;&#31561;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21457;&#30340;&#25945;&#32946;&#25216;&#26415;&#21019;&#26032;&#26174;&#31034;&#20986;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#25991;&#26412;&#20869;&#23481;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#21019;&#26032;&#26469;&#33258;&#21160;&#21270;&#21508;&#31181;&#25945;&#32946;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#29983;&#25104;&#38382;&#39064;&#12289;&#25552;&#20379;&#21453;&#39304;&#21644;&#35780;&#20998;&#65289;&#65292;&#20294;&#23545;&#36825;&#20123;&#21019;&#26032;&#30340;&#23454;&#38469;&#24615;&#21644;&#20262;&#29702;&#24615;&#23384;&#22312;&#25285;&#24551;&#12290;&#36825;&#20123;&#25285;&#24551;&#21487;&#33021;&#20250;&#38459;&#30861;&#26410;&#26469;&#30740;&#31350;&#21644;&#22312;&#30495;&#23454;&#25945;&#32946;&#29615;&#22659;&#20013;&#37319;&#29992;&#22522;&#20110;LLMs&#30340;&#21019;&#26032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;118&#31687;&#33258;2017&#24180;&#20197;&#26469;&#21457;&#34920;&#30340;&#21516;&#34892;&#35780;&#35758;&#35770;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#20197;&#30830;&#23450;&#20351;&#29992;LLMs&#33258;&#21160;&#21270;&#21644;&#25903;&#25345;&#25945;&#32946;&#20219;&#21153;&#30340;&#24403;&#21069;&#30740;&#31350;&#29366;&#24577;&#12290;&#36890;&#36807;&#35780;&#20272;&#20854;&#25216;&#26415;&#21487;&#34892;&#24615;&#12289;&#27169;&#22411;&#24615;&#33021;&#12289;&#21487;&#22797;&#21046;&#24615;&#12289;&#31995;&#32479;&#36879;&#26126;&#24230;&#12289;&#38544;&#31169;&#12289;&#24179;&#31561;&#21644;&#21892;&#24847;&#65292;&#36824;&#30830;&#23450;&#20102;LLMs&#21019;&#26032;&#30340;&#23454;&#38469;&#21644;&#20262;&#29702;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Educational technology innovations that have been developed based on large language models (LLMs) have shown the potential to automate the laborious process of generating and analysing textual content. While various innovations have been developed to automate a range of educational tasks (e.g., question generation, feedback provision, and essay grading), there are concerns regarding the practicality and ethicality of these innovations. Such concerns may hinder future research and the adoption of LLMs-based innovations in authentic educational contexts. To address this, we conducted a systematic literature review of 118 peer-reviewed papers published since 2017 to pinpoint the current state of research on using LLMs to automate and support educational tasks. The practical and ethical challenges of LLMs-based innovations were also identified by assessing their technological readiness, model performance, replicability, system transparency, privacy, equality, and beneficence. The findings 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#8212;&#8212;GPT-4&#22312;&#21307;&#23398;&#33021;&#21147;&#32771;&#35797;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#21161;&#20110;&#21307;&#23398;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.13375</link><description>&lt;p&gt;
GPT-4&#22312;&#21307;&#23398;&#25361;&#25112;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Capabilities of GPT-4 on Medical Challenge Problems. (arXiv:2303.13375v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#8212;&#8212;GPT-4&#22312;&#21307;&#23398;&#33021;&#21147;&#32771;&#35797;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#21161;&#20110;&#21307;&#23398;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#21253;&#25324;&#21307;&#23398;&#12290;&#25105;&#20204;&#23545;&#19968;&#39033;&#26368;&#20808;&#36827;&#30340;LLM&#8212;&#8212;GPT-4&#22312;&#21307;&#23398;&#33021;&#21147;&#32771;&#35797;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;GPT-4&#26159;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#27809;&#26377;&#32463;&#36807;&#38024;&#23545;&#21307;&#23398;&#38382;&#39064;&#30340;&#35757;&#32451;&#25110;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#32654;&#22269;&#20020;&#24202;&#33021;&#21147;&#35780;&#20272;&#21644;&#25480;&#26435;&#32771;&#26680;&#35745;&#21010;&#65288;USMLE&#65289;&#30340;&#20004;&#32452;&#23448;&#26041;&#32451;&#20064;&#26448;&#26009;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22312;MultiMedQA&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#38500;&#20102;&#27979;&#37327;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36824;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#27979;&#35797;&#38382;&#39064;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25506;&#32034;&#35757;&#32451;&#26399;&#38388;&#20869;&#23481;&#35760;&#24518;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#30740;&#31350;&#27010;&#29575;&#26657;&#20934;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#26368;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#22522;&#20110;ClimaText&#25968;&#25454;&#38598;&#65292;&#24494;&#35843;ClimateBert transformer&#21644;BERT&#27169;&#22411;&#65292;&#25104;&#21151;&#26816;&#27979;&#27668;&#20505;&#21464;&#21270;&#30456;&#20851;&#21477;&#23376;&#65292;&#20026;&#37329;&#34701;&#30417;&#31649;&#26426;&#26500;&#21644;&#25237;&#36164;&#32773;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#27668;&#20505;&#30456;&#20851;&#37329;&#34701;&#39118;&#38505;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#26395;&#25903;&#25345;&#26356;&#21487;&#25345;&#32493;&#30340;&#37329;&#34701;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.13373</link><description>&lt;p&gt;
&#29992;ClimaText&#24494;&#35843;ClimateBERT transformer&#35299;&#26512;&#27668;&#20505;&#30456;&#20851;&#37329;&#34701;&#39118;&#38505;&#25259;&#38706;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning ClimateBert transformer with ClimaText for the disclosure analysis of climate-related financial risks. (arXiv:2303.13373v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26368;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#22522;&#20110;ClimaText&#25968;&#25454;&#38598;&#65292;&#24494;&#35843;ClimateBert transformer&#21644;BERT&#27169;&#22411;&#65292;&#25104;&#21151;&#26816;&#27979;&#27668;&#20505;&#21464;&#21270;&#30456;&#20851;&#21477;&#23376;&#65292;&#20026;&#37329;&#34701;&#30417;&#31649;&#26426;&#26500;&#21644;&#25237;&#36164;&#32773;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#27668;&#20505;&#30456;&#20851;&#37329;&#34701;&#39118;&#38505;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#26395;&#25903;&#25345;&#26356;&#21487;&#25345;&#32493;&#30340;&#37329;&#34701;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#37329;&#34701;&#26426;&#26500;&#65292;&#29305;&#21035;&#26159;&#20010;&#20154;&#21644;&#26426;&#26500;&#25237;&#36164;&#32773;&#65292;&#23545;&#20844;&#21496;&#25253;&#21578;&#27668;&#20505;&#30456;&#20851;&#37329;&#34701;&#39118;&#38505;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#12290;&#20026;&#20102;&#35782;&#21035;&#36825;&#20123;&#31867;&#22411;&#30340;&#39118;&#38505;&#65292;&#20844;&#21496;&#21487;&#20197;&#22312;&#36130;&#21153;&#21644;&#38750;&#36130;&#21153;&#25253;&#21578;&#20013;&#30701;&#26399;&#20869;&#25259;&#38706;&#22823;&#37327;&#25991;&#26412;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#20026;&#20102;&#21709;&#24212;&#19981;&#26029;&#36890;&#36807;&#30340;&#35268;&#23450;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#23454;&#29616;&#27668;&#20505;&#21464;&#21270;&#22312;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#24494;&#35843;&#20004;&#20010;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;BERT&#21644;ClimateBert&#65292;&#36825;&#26159;&#19968;&#20010;&#26368;&#36817;&#21457;&#24067;&#30340;&#22522;&#20110;DistillRoBERTa&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#19987;&#38376;&#20026;&#27668;&#20505;&#25991;&#26412;&#20998;&#31867;&#37327;&#36523;&#23450;&#21046;&#12290;&#36825;&#20004;&#20010;&#31639;&#27861;&#22522;&#20110;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#33021;&#22815;&#23398;&#20064;&#25991;&#26412;&#20013;&#21333;&#35789;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#26032;&#30340;ClimaText&#25968;&#25454;&#38598;&#19978;&#23545;&#20004;&#20010;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#36229;&#36807;2000&#23478;&#20844;&#21496;&#25259;&#38706;&#20854;&#27668;&#20505;&#30456;&#20851;&#39118;&#38505;&#30340;&#25253;&#21578;&#32452;&#25104;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24494;&#35843;&#27169;&#22411;&#22312;&#35782;&#21035;ClimaText&#20013;&#19982;&#27668;&#20505;&#21464;&#21270;&#30456;&#20851;&#30340;&#21477;&#23376;&#26041;&#38754;&#20248;&#20110;&#21407;&#22987;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#26377;&#28508;&#21147;&#25903;&#25345;&#37329;&#34701;&#30417;&#31649;&#26426;&#26500;&#12289;&#25237;&#36164;&#32773;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#20026;&#20182;&#20204;&#25552;&#20379;&#26356;&#20934;&#30830;&#12289;&#19968;&#33268;&#30340;&#20851;&#20110;&#27668;&#20505;&#30456;&#20851;&#37329;&#34701;&#39118;&#38505;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20026;&#19968;&#20010;&#26356;&#21487;&#25345;&#32493;&#30340;&#37329;&#34701;&#31995;&#32479;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years there has been a growing demand from financial agents, especially from particular and institutional investors, for companies to report on climate-related financial risks. A vast amount of information, in text format, can be expected to be disclosed in the short term by firms in order to identify these types of risks in their financial and non financial reports, particularly in response to the growing regulation that is being passed on the matter. To this end, this paper applies state-of-the-art NLP techniques to achieve the detection of climate change in text corpora. We use transfer learning to fine-tune two transformer models, BERT and ClimateBert -a recently published DistillRoBERTa-based model that has been specifically tailored for climate text classification-. These two algorithms are based on the transformer architecture which enables learning the contextual relationships between words in a text. We carry out the fine-tuning process of both models on the novel Cl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#29983;&#25104;&#25991;&#26412;&#30340;ChatGPT&#27169;&#22411;&#65292;&#35813;&#25216;&#26415;&#34987;&#35748;&#20026;&#21487;&#20197;&#25104;&#20026;&#33258;&#21160;&#20934;&#22791;&#23398;&#26415;&#35770;&#25991;&#21450;&#25163;&#31295;&#30340;&#28508;&#22312;&#27169;&#22411;&#65292;&#28982;&#32780;&#65292;&#20854;&#19982;&#31867;&#20284;&#27169;&#22411;&#28508;&#22312;&#30340;&#20262;&#29702;&#38382;&#39064;&#38656;&#35201;&#32771;&#34385;&#21644;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2303.13367</link><description>&lt;p&gt;
ChatGPT&#21644;&#26032;&#30340;&#23398;&#26415;&#29616;&#23454;&#65306;AI&#25776;&#20889;&#30340;&#30740;&#31350;&#35770;&#25991;&#21450;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#26415;&#20986;&#29256;&#20013;&#30340;&#20262;&#29702;&#36947;&#24503;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and a New Academic Reality: AI-Written Research Papers and the Ethics of the Large Language Models in Scholarly Publishing. (arXiv:2303.13367v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#29983;&#25104;&#25991;&#26412;&#30340;ChatGPT&#27169;&#22411;&#65292;&#35813;&#25216;&#26415;&#34987;&#35748;&#20026;&#21487;&#20197;&#25104;&#20026;&#33258;&#21160;&#20934;&#22791;&#23398;&#26415;&#35770;&#25991;&#21450;&#25163;&#31295;&#30340;&#28508;&#22312;&#27169;&#22411;&#65292;&#28982;&#32780;&#65292;&#20854;&#19982;&#31867;&#20284;&#27169;&#22411;&#28508;&#22312;&#30340;&#20262;&#29702;&#38382;&#39064;&#38656;&#35201;&#32771;&#34385;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;OpenAI&#30340;ChatGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#28385;&#36275;&#22522;&#20110;&#25991;&#26412;&#30340;&#29992;&#25143;&#35831;&#27714;&#65288;&#21363;&#32842;&#22825;&#26426;&#22120;&#20154;&#65289;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#12290;&#35752;&#35770;&#20102;ChatGPT&#21450;&#31867;&#20284;&#27169;&#22411;&#32972;&#21518;&#30340;&#21382;&#21490;&#21644;&#21407;&#21017;&#12290;&#28982;&#21518;&#35752;&#35770;&#20102;&#36825;&#31181;&#25216;&#26415;&#23545;&#23398;&#26415;&#21644;&#23398;&#26415;&#30740;&#31350;&#20986;&#29256;&#29289;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;ChatGPT&#34987;&#35270;&#20026;&#33258;&#21160;&#20934;&#22791;&#35770;&#25991;&#21644;&#20854;&#20182;&#31867;&#22411;&#23398;&#26415;&#25163;&#31295;&#30340;&#28508;&#22312;&#27169;&#22411;&#12290;&#35752;&#35770;&#20102;&#21487;&#33021;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#32972;&#21518;&#30340;&#22522;&#30784;&#25216;&#26415;GPT-3&#65289;&#30340;&#20986;&#29616;&#21644;&#20854;&#34987;&#23398;&#26415;&#30028;&#21644;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#32780;&#20986;&#29616;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#23558;&#20854;&#32622;&#20110;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#30740;&#31350;&#21644;&#23398;&#26415;&#20986;&#29256;&#26041;&#38754;&#30340;&#26356;&#24191;&#27867;&#36827;&#23637;&#30340;&#32972;&#26223;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer, which uses natural language processing to fulfill text-based user requests (i.e., a chatbot). The history and principles behind ChatGPT and similar models are discussed. This technology is then discussed in relation to its potential impact on academia and scholarly research and publishing. ChatGPT is seen as a potential model for the automated preparation of essays and other types of scholarly manuscripts. Potential ethical issues that could arise with the emergence of large language models like GPT-3, the underlying technology behind ChatGPT, and its usage by academics and researchers, are discussed and situated within the context of broader advancements in artificial intelligence, machine learning, and natural language processing for research and scholarly publishing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#38656;&#27714;&#35268;&#33539;&#21270;&#20013;&#30340;&#24212;&#29992;&#29616;&#29366;&#65292;&#21457;&#29616;&#21551;&#21457;&#24335;NLP&#26041;&#27861;&#26159;&#33258;&#21160;RF&#20013;&#26368;&#24120;&#29992;&#30340;NLP&#25216;&#26415;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21017;&#21253;&#25324;&#22522;&#20110;&#20998;&#31867;&#21644;&#32858;&#31867;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;NLP&#21644;ML&#25216;&#26415;&#22312;RF&#39046;&#22495;&#20013;&#30340;&#36739;&#22823;&#36827;&#23637;&#65292;&#24182;&#25351;&#20986;&#20102;&#22312;&#24212;&#29992;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.13365</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#38656;&#27714;&#35268;&#33539;&#21270;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31687;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Requirement Formalisation using Natural Language Processing and Machine Learning: A Systematic Review. (arXiv:2303.13365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#38656;&#27714;&#35268;&#33539;&#21270;&#20013;&#30340;&#24212;&#29992;&#29616;&#29366;&#65292;&#21457;&#29616;&#21551;&#21457;&#24335;NLP&#26041;&#27861;&#26159;&#33258;&#21160;RF&#20013;&#26368;&#24120;&#29992;&#30340;NLP&#25216;&#26415;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21017;&#21253;&#25324;&#22522;&#20110;&#20998;&#31867;&#21644;&#32858;&#31867;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;NLP&#21644;ML&#25216;&#26415;&#22312;RF&#39046;&#22495;&#20013;&#30340;&#36739;&#22823;&#36827;&#23637;&#65292;&#24182;&#25351;&#20986;&#20102;&#22312;&#24212;&#29992;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#26041;&#27861;&#30340;&#25913;&#36827;&#21560;&#24341;&#20102;&#24320;&#21457;&#20154;&#21592;&#22312;&#38656;&#27714;&#24037;&#31243;&#39046;&#22495;&#33258;&#21160;&#21270;&#38656;&#27714;&#35268;&#33539;&#21270;&#65288;RF&#65289;&#20013;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#25253;&#21578;&#20102;&#24212;&#29992;NLP&#21644;ML&#22312;&#20943;&#23569;&#33258;&#28982;&#35821;&#35328;&#32534;&#20889;&#30340;&#38656;&#27714;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#23436;&#25972;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;&#21644;&#20998;&#31867;&#29616;&#26377;&#30340;NLP&#21644;ML&#22312;RF&#19978;&#30340;&#24037;&#20316;&#65292;&#35782;&#21035;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#24182;&#25552;&#20379;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#36873;&#21462;&#20102;&#26469;&#33258;&#24120;&#29992;&#24211;&#30340;257&#31687;&#35770;&#25991;&#12290;&#36890;&#36807;&#23450;&#20041;&#21253;&#21547;&#21644;&#25490;&#38500;&#26631;&#20934;&#26469;&#36807;&#28388;&#25628;&#32034;&#32467;&#26524;&#65292;&#24182;&#36873;&#25321;&#20102;47&#39033;&#30456;&#20851;&#30740;&#31350;&#65292;&#26102;&#38388;&#36328;&#24230;&#22312;2012&#24180;&#33267;2022&#24180;&#20043;&#38388;&#12290;&#25105;&#20204;&#21457;&#29616;&#21551;&#21457;&#24335;NLP&#26041;&#27861;&#26159;&#33258;&#21160;RF&#20013;&#26368;&#24120;&#29992;&#30340;NLP&#25216;&#26415;&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21017;&#21253;&#25324;&#22522;&#20110;&#20998;&#31867;&#21644;&#32858;&#31867;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;NLP&#21644;ML&#25216;&#26415;&#22312;RF&#39046;&#22495;&#20013;&#30340;&#36739;&#22823;&#36827;&#23637;&#65292;&#24182;&#25351;&#20986;&#20102;&#22312;&#24212;&#29992;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improvement of software development methodologies attracts developers to automatic Requirement Formalisation (RF) in the Requirement Engineering (RE) field. The potential advantages by applying Natural Language Processing (NLP) and Machine Learning (ML) in reducing the ambiguity and incompleteness of requirement written in natural languages is reported in different studies. The goal of this paper is to survey and classify existing work on NLP and ML for RF, identifying challenges in this domain and providing promising future research directions. To achieve this, we conducted a systematic literature review to outline the current state-of-the-art of NLP and ML techniques in RF by selecting 257 papers from common used libraries. The search result is filtered by defining inclusion and exclusion criteria and 47 relevant studies between 2012 and 2022 are selected. We found that heuristic NLP approaches are the most common NLP techniques used for automatic RF, primary operating on structured 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#35780;&#20272;&#20102;EmoWOZ&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#21010;&#20998;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#25277;&#26679;&#26041;&#27861;&#29992;&#20110;&#22788;&#29702;&#39640;&#24230;&#19981;&#24179;&#34913;&#21644;&#19981;&#22343;&#21248;&#20998;&#24067;&#30340;&#24773;&#24863;&#26631;&#31614;&#12290;&#20351;&#29992;&#36825;&#20010;&#26041;&#27861;&#24314;&#31435;&#22312;EmoWoz&#19978;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#26410;&#26469;&#30740;&#31350;&#32773;&#24212;&#35813;&#37319;&#21462;&#36825;&#31181;&#21010;&#20998;&#20197;&#30830;&#20445;&#19968;&#33268;&#21644;&#20934;&#30830;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2303.13364</link><description>&lt;p&gt;
&#37325;&#26032;&#35780;&#20272;EmoWOZ&#20013;&#38024;&#23545;&#24773;&#24863;&#26816;&#27979;&#30340;&#25968;&#25454;&#21010;&#20998;
&lt;/p&gt;
&lt;p&gt;
Reevaluating Data Partitioning for Emotion Detection in EmoWOZ. (arXiv:2303.13364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#35780;&#20272;&#20102;EmoWOZ&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#21010;&#20998;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#25277;&#26679;&#26041;&#27861;&#29992;&#20110;&#22788;&#29702;&#39640;&#24230;&#19981;&#24179;&#34913;&#21644;&#19981;&#22343;&#21248;&#20998;&#24067;&#30340;&#24773;&#24863;&#26631;&#31614;&#12290;&#20351;&#29992;&#36825;&#20010;&#26041;&#27861;&#24314;&#31435;&#22312;EmoWoz&#19978;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#26410;&#26469;&#30740;&#31350;&#32773;&#24212;&#35813;&#37319;&#21462;&#36825;&#31181;&#21010;&#20998;&#20197;&#30830;&#20445;&#19968;&#33268;&#21644;&#20934;&#30830;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32858;&#28966;&#20110;EmoWOZ&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;MultiWOZ&#25968;&#25454;&#38598;&#30340;&#25193;&#23637;&#65292;&#25552;&#20379;&#20102;&#23545;&#35805;&#30340;&#24773;&#24863;&#26631;&#31614;&#12290;&#19982;&#21407;&#22987;&#30340;MultiWOZ&#25968;&#25454;&#38598;&#22240;&#20854;&#23427;&#30446;&#30340;&#34987;&#21010;&#20998;&#19981;&#21516;&#65292;EmoWOZ&#20013;&#24773;&#24863;&#26631;&#31614;&#39640;&#24230;&#19981;&#24179;&#34913;&#65292;&#20998;&#24067;&#22312;&#19981;&#21516;&#21010;&#20998;&#20013;&#20063;&#19981;&#22343;&#21248;&#65292;&#23548;&#33268;&#27169;&#22411;&#27604;&#36739;&#25928;&#26524;&#27424;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12289;&#25913;&#21892;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#24182;&#20943;&#23569;&#25968;&#25454;&#38598;&#20998;&#24067;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24773;&#24863;&#26631;&#31614;&#30340;&#20998;&#23618;&#25277;&#26679;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29305;&#27530;&#25216;&#26415;&#26469;&#22788;&#29702;&#26377;&#22810;&#20010;&#24773;&#24863;&#26631;&#31614;&#30340;&#23545;&#35805;&#65288;&#24207;&#21015;&#65289;&#25968;&#25454;&#12290;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#25277;&#26679;&#26041;&#27861;&#65292;&#24314;&#31435;&#22312;EmoWoz&#19978;&#30340;&#27169;&#22411;&#21487;&#20197;&#34920;&#29616;&#26356;&#22909;&#65292;&#20351;&#23427;&#25104;&#20026;&#35757;&#32451;&#20855;&#26377;&#24773;&#24863;&#26234;&#33021;&#30340;&#23545;&#35805;&#20195;&#29702;&#26356;&#20026;&#21487;&#38752;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#25512;&#33616;&#26410;&#26469;&#30340;&#30740;&#31350;&#32773;&#20351;&#29992;&#36825;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21010;&#20998;&#26469;&#30830;&#20445;&#19968;&#33268;&#21644;&#20934;&#30830;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on the EmoWoz dataset, an extension of MultiWOZ that provides emotion labels for the dialogues. MultiWOZ was partitioned initially for another purpose, resulting in a distributional shift when considering the new purpose of emotion recognition. The emotion tags in EmoWoz are highly imbalanced and unevenly distributed across the partitions, which causes sub-optimal performance and poor comparison of models. We propose a stratified sampling scheme based on emotion tags to address this issue, improve the dataset's distribution, and reduce dataset shift. We also introduce a special technique to handle conversation (sequential) data with many emotional tags. Using our proposed sampling method, models built upon EmoWoz can perform better, making it a more reliable resource for training conversational agents with emotional intelligence. We recommend that future researchers use this new partitioning to ensure consistent and accurate performance evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#23545;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#24615;&#35780;&#20272;&#30340;&#21487;&#25193;&#23637;&#24615;&#36827;&#34892;&#30740;&#31350;&#65292;&#36890;&#36807;&#29983;&#25104;&#29305;&#23450;&#21338;&#24328;&#35770;&#32467;&#26500;&#22330;&#26223;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#19981;&#36807;&#30446;&#21069;&#29983;&#25104;&#36136;&#37327;&#36739;&#19968;&#33324;&#12290;</title><link>http://arxiv.org/abs/2303.13360</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#24615;&#35780;&#20272;&#30340;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards the Scalable Evaluation of Cooperativeness in Language Models. (arXiv:2303.13360v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#23545;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#24615;&#35780;&#20272;&#30340;&#21487;&#25193;&#23637;&#24615;&#36827;&#34892;&#30740;&#31350;&#65292;&#36890;&#36807;&#29983;&#25104;&#29305;&#23450;&#21338;&#24328;&#35770;&#32467;&#26500;&#22330;&#26223;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#19981;&#36807;&#30446;&#21069;&#29983;&#25104;&#36136;&#37327;&#36739;&#19968;&#33324;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#39537;&#21160;&#30340;AI&#31995;&#32479;&#21487;&#33021;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#36741;&#21161;&#20154;&#31867;&#36827;&#34892;&#28041;&#21450;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#39640; stakes &#20132;&#20114;&#65292;&#20363;&#22914;&#21327;&#21830;&#25110;&#20914;&#31361;&#35299;&#20915;&#12290;&#31526;&#21512;&#21512;&#20316;&#30340;AI&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#24076;&#26395;&#20197;&#20146;&#31038;&#20250;&#30340;&#26041;&#24335;&#29702;&#35299;&#21644;&#22609;&#36896;PLM&#30340;&#22810;&#20195;&#29702;&#34892;&#20026;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#31532;&#19968;&#27493;&#26159;&#23545;&#27169;&#22411;&#22312;&#21508;&#31181;&#21512;&#20316;&#38382;&#39064;&#19978;&#34892;&#20026;&#30340;&#35780;&#20272;&#12290;&#30001;&#20110;&#20132;&#20114;&#20013;&#26399;&#26395;&#30340;&#34892;&#20026;&#21462;&#20915;&#20110;&#31934;&#30830;&#30340;&#21338;&#24328;&#32467;&#26500;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#20247;&#21253;&#24037;&#20154;&#21644;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#32467;&#26500;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22914;&#19979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29983;&#25104;&#29305;&#23450;&#21338;&#24328;&#35770;&#32467;&#26500;&#22330;&#26223;&#30340;&#20851;&#38190;&#26041;&#27861;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#20247;&#21253;&#24037;&#20154;&#21644;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#36825;&#20123;&#22330;&#26223;&#12290;&#25105;&#20204;&#21457;&#29616;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#29983;&#25104;&#36136;&#37327;&#24448;&#24448;&#26159;&#20013;&#31561;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20197;&#19979;&#32467;&#35770;&#65306;
&lt;/p&gt;
&lt;p&gt;
It is likely that AI systems driven by pre-trained language models (PLMs) will increasingly be used to assist humans in high-stakes interactions with other agents, such as negotiation or conflict resolution. Consistent with the goals of Cooperative AI \citep{dafoe_open_2020}, we wish to understand and shape the multi-agent behaviors of PLMs in a pro-social manner. An important first step is the evaluation of model behaviour across diverse cooperation problems. Since desired behaviour in an interaction depends upon precise game-theoretic structure, we focus on generating scenarios with particular structures with both crowdworkers and a language model. Our work proceeds as follows. First, we discuss key methodological issues in the generation of scenarios corresponding to particular game-theoretic structures. Second, we employ both crowdworkers and a language model to generate such scenarios. We find that the quality of generations tends to be mediocre in both cases. We additionally get 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#36234;&#21335;&#35821;&#35328;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#24182;&#25552;&#20986;&#26032;&#26041;&#21521;&#12290;&#25105;&#20204;&#21516;&#26102;&#36824;&#21457;&#29616;&#29616;&#26377;&#36234;&#21335;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#22522;&#20934;&#23384;&#22312;&#20154;&#24037;&#38382;&#39064;&#65292;&#38656;&#36843;&#20999;&#23547;&#27714;&#26032;&#30340;&#39640;&#36136;&#37327;&#22522;&#20934;&#35780;&#20272;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2303.13355</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#25581;&#31034;&#36234;&#21335;&#35821;&#35328;&#27169;&#22411;&#30340;&#24369;&#28857;
&lt;/p&gt;
&lt;p&gt;
Revealing Weaknesses of Vietnamese Language Models Through Unanswerable Questions in Machine Reading Comprehension. (arXiv:2303.13355v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#36234;&#21335;&#35821;&#35328;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#24182;&#25552;&#20986;&#26032;&#26041;&#21521;&#12290;&#25105;&#20204;&#21516;&#26102;&#36824;&#21457;&#29616;&#29616;&#26377;&#36234;&#21335;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#22522;&#20934;&#23384;&#22312;&#20154;&#24037;&#38382;&#39064;&#65292;&#38656;&#36843;&#20999;&#23547;&#27714;&#26032;&#30340;&#39640;&#36136;&#37327;&#22522;&#20934;&#35780;&#20272;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#21333;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#35821;&#35328;&#33021;&#21147;&#21463;&#21040;&#20102;&#24456;&#22823;&#38480;&#21046;&#65292;&#20294;&#30740;&#31350;&#20154;&#21592;&#20173;&#28982;&#19981;&#24471;&#19981;&#20381;&#38752;&#22810;&#35821;&#35328;&#27169;&#22411;&#26469;&#24320;&#21457;&#36234;&#21335;&#25991;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#12290;&#36825;&#31181;&#30740;&#31350;&#22256;&#38590;&#26159;&#30001;&#20110;&#36234;&#21335;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#30340;&#39640;&#36136;&#37327;&#20316;&#21697;&#25968;&#37327;&#26377;&#38480;&#12290;&#20026;&#20102;&#40723;&#21169;&#22312;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#36827;&#34892;&#26356;&#22810;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#23545;&#24403;&#21069;&#36234;&#21335;&#21333;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#24369;&#28857;&#21644;&#20248;&#21183;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#20174;&#20998;&#26512;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21457;&#23637;&#36234;&#21335;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#21521;&#12290;&#38500;&#20102;&#36825;&#20010;&#20027;&#35201;&#36129;&#29486;&#22806;&#65292;&#25105;&#20204;&#36824;&#25104;&#21151;&#22320;&#25581;&#31034;&#20102;&#36234;&#21335;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#22522;&#20934;&#20013;&#23384;&#22312;&#30340;&#20154;&#24037;&#38382;&#39064;&#65292;&#24182;&#24314;&#35758;&#36843;&#20999;&#38656;&#35201;&#26032;&#30340;&#39640;&#36136;&#37327;&#22522;&#20934;&#26469;&#36319;&#36394;&#36234;&#21335;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#36827;&#23637;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#36234;&#21335;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#22914;&#20309;&#26292;&#38706;&#35821;&#35328;&#24314;&#27169;&#30340;&#24369;&#28857;&#65292;&#24182;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the curse of multilinguality significantly restricts the language abilities of multilingual models in monolingual settings, researchers now still have to rely on multilingual models to develop state-of-the-art systems in Vietnamese Machine Reading Comprehension. This difficulty in researching is because of the limited number of high-quality works in developing Vietnamese language models. In order to encourage more work in this research field, we present a comprehensive analysis of language weaknesses and strengths of current Vietnamese monolingual models using the downstream task of Machine Reading Comprehension. From the analysis results, we suggest new directions for developing Vietnamese language models. Besides this main contribution, we also successfully reveal the existence of artifacts in Vietnamese Machine Reading Comprehension benchmarks and suggest an urgent need for new high-quality benchmarks to track the progress of Vietnamese Machine Reading Comprehension. Moreov
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#22312;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#19978;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;10000&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#26159;&#26368;&#22823;&#30340;&#23398;&#26415;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.13351</link><description>&lt;p&gt;
DBLP-QuAD&#65306;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#19978;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph. (arXiv:2303.13351v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13351
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#22312;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#19978;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;10000&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#26159;&#26368;&#22823;&#30340;&#23398;&#26415;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#19978;&#21019;&#24314;&#20102;&#19968;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;DBLP&#26159;&#19968;&#20010;&#22312;&#32447;&#35745;&#31639;&#26426;&#31185;&#23398;&#20027;&#35201;&#20986;&#29256;&#29289;&#30340;&#21442;&#32771;&#25991;&#29486;&#20449;&#24687;&#32034;&#24341;&#65292;&#32034;&#24341;&#20102;&#36229;&#36807;440&#19975;&#31687;&#35770;&#25991;&#65292;&#30001;220&#19975;&#22810;&#20301;&#20316;&#32773;&#21457;&#34920;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;10000&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#20197;&#21450;&#30456;&#24212;&#30340;SPARQL&#26597;&#35810;&#65292;&#21487;&#20197;&#22312;DBLP KG&#19978;&#25191;&#34892;&#20197;&#33719;&#24471;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;DBLP-QuAD&#26159;&#26368;&#22823;&#30340;&#23398;&#26415;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we create a question answering dataset over the DBLP scholarly knowledge graph (KG). DBLP is an on-line reference for bibliographic information on major computer science publications that indexes over 4.4 million publications published by more than 2.2 million authors. Our dataset consists of 10,000 question answer pairs with the corresponding SPARQL queries which can be executed over the DBLP KG to fetch the correct answer. DBLP-QuAD is the largest scholarly question answering dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;NLP&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#20174;&#20020;&#24202;&#25968;&#25454;&#20013;&#25552;&#21462;&#19982;&#20256;&#26579;&#30149;&#30456;&#20851;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#35813;&#26041;&#27861;&#22312;&#35780;&#20272;&#20013;&#20248;&#20110;&#26631;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13314</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Leveraging Foundation Models for Clinical Text Analysis. (arXiv:2303.13314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;NLP&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#20174;&#20020;&#24202;&#25968;&#25454;&#20013;&#25552;&#21462;&#19982;&#20256;&#26579;&#30149;&#30456;&#20851;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#35813;&#26041;&#27861;&#22312;&#35780;&#20272;&#20013;&#20248;&#20110;&#26631;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#26579;&#30149;&#26159;&#20840;&#29699;&#37325;&#35201;&#30340;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#65292;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#21487;&#20197;&#20419;&#36827;&#26377;&#25928;&#30340;&#39044;&#38450;&#21644;&#27835;&#30103;&#31574;&#30053;&#30340;&#21046;&#23450;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#30340;&#20020;&#24202;&#25968;&#25454;&#21487;&#29992;&#24615;&#20250;&#23545;&#20449;&#24687;&#25277;&#21462;&#26500;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#22312;&#29305;&#23450;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#33258;&#30001;&#25991;&#26412;&#20020;&#24202;&#25968;&#25454;&#20013;&#25552;&#21462;&#19982;&#20256;&#26579;&#30149;&#30456;&#20851;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#32452;&#20214;&#65306;&#25968;&#25454;&#23618;&#29992;&#20110;&#20174;&#20020;&#24202;&#25991;&#26412;&#20934;&#22791;&#25968;&#25454;&#38598;&#65292;&#22522;&#30784;&#27169;&#22411;&#23618;&#29992;&#20110;&#23454;&#20307;&#25552;&#21462;&#65292;&#35780;&#20272;&#23618;&#29992;&#20110;&#24615;&#33021;&#20998;&#26512;&#12290;&#35780;&#20272;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#26631;&#20934;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#20351;&#20854;&#26377;&#21161;&#20110;&#30740;&#31350;&#20854;&#20182;&#20256;&#26579;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infectious diseases are a significant public health concern globally, and extracting relevant information from scientific literature can facilitate the development of effective prevention and treatment strategies. However, the large amount of clinical data available presents a challenge for information extraction. To address this challenge, this study proposes a natural language processing (NLP) framework that uses a pre-trained transformer model fine-tuned on task-specific data to extract key information related to infectious diseases from free-text clinical data. The proposed framework includes three components: a data layer for preparing datasets from clinical texts, a foundation model layer for entity extraction, and an assessment layer for performance analysis. The results of the evaluation indicate that the proposed method outperforms standard methods, and leveraging prior knowledge through the pre-trained transformer model makes it useful for investigating other infectious disea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SwissBERT&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#22788;&#29702;&#29790;&#22763;&#30456;&#20851;&#25991;&#26412;&#32780;&#21019;&#24314;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;SwissBERT&#22312;&#19982;&#29790;&#22763;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.13310</link><description>&lt;p&gt;
SwissBERT&#65306;&#29790;&#22763;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SwissBERT: The Multilingual Language Model for Switzerland. (arXiv:2303.13310v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13310
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SwissBERT&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#22788;&#29702;&#29790;&#22763;&#30456;&#20851;&#25991;&#26412;&#32780;&#21019;&#24314;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;SwissBERT&#22312;&#19982;&#29790;&#22763;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SwissBERT&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#22788;&#29702;&#19982;&#29790;&#22763;&#30456;&#20851;&#30340;&#25991;&#26412;&#32780;&#21019;&#24314;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#12290; SwissBERT&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#35843;&#25972;&#20026;&#33021;&#22815;&#22788;&#29702;&#29790;&#22763;&#22269;&#23478;&#35821;&#35328; -&#24503;&#35821;&#12289;&#27861;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#21644;&#32599;&#26364;&#20160;&#35821;&#30340;&#26032;&#38395;&#25991;&#31456;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;SwissBERT&#22312;&#19982;&#29790;&#22763;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#23427;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#24448;&#24448;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#24403;&#20195;&#26032;&#38395;&#21644;/&#25110;&#32599;&#26364;&#20160;&#35821;&#26684;&#37324;&#26031;&#26118;&#26102;&#12290;&#30001;&#20110;SwissBERT&#20351;&#29992;&#35821;&#35328;&#36866;&#37197;&#22120;&#65292;&#22240;&#27492;&#26410;&#26469;&#30340;&#24037;&#20316;&#21487;&#33021;&#23558;&#20854;&#25193;&#23637;&#21040;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;&#20013;&#12290;&#35813;&#27169;&#22411;&#21644;&#25105;&#20204;&#30340;&#24320;&#28304;&#20195;&#30721;&#20844;&#24320;&#21457;&#24067;&#22312;https://github.com/ZurichNLP/swissbert&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SwissBERT, a masked language model created specifically for processing Switzerland-related text. SwissBERT is a pre-trained model that we adapted to news articles written in the national languages of Switzerland -German, French, Italian, and Romansh. We evaluate SwissBERT on natural language understanding tasks related to Switzerland and find that it tends to outperform previous models on these tasks, especially when processing contemporary news and/or Romansh Grischun. Since SwissBERT uses language adapters, it may be extended to Swiss German dialects in future work. The model and our open-source code are publicly released at https://github.com/ZurichNLP/swissbert.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GETT-QA&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;T5&#23545;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;&#31616;&#21270;&#30340;SPARQL&#26597;&#35810;&#65292;&#24182;&#20351;&#29992;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13284</link><description>&lt;p&gt;
GETT-QA&#65306;&#22522;&#20110;&#22270;&#23884;&#20837;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#30340;T2T Transformer
&lt;/p&gt;
&lt;p&gt;
GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering. (arXiv:2303.13284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GETT-QA&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;T5&#23545;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;&#31616;&#21270;&#30340;SPARQL&#26597;&#35810;&#65292;&#24182;&#20351;&#29992;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GETT-QA&#30340;&#31471;&#21040;&#31471;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#31995;&#32479;&#12290;GETT-QA&#20351;&#29992;&#20102;T5&#65292;&#36825;&#26159;&#19968;&#31181;&#28909;&#38376;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#30340;&#38382;&#39064;&#20316;&#20026;&#36755;&#20837;&#24182;&#29983;&#25104;&#25152;&#38656;SPARQL&#26597;&#35810;&#30340;&#31616;&#21270;&#24418;&#24335;&#12290;&#22312;&#31616;&#21270;&#24418;&#24335;&#20013;&#65292;&#27169;&#22411;&#19981;&#30452;&#25509;&#29983;&#25104;&#23454;&#20307;&#21644;&#20851;&#31995;ID&#65292;&#32780;&#26159;&#20135;&#29983;&#30456;&#24212;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#26631;&#31614;&#12290;&#26631;&#31614;&#22312;&#38543;&#21518;&#30340;&#27493;&#39588;&#20013;&#19982;KG&#23454;&#20307;&#21644;&#20851;&#31995;ID&#32852;&#31995;&#36215;&#26469;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#65292;&#25105;&#20204;&#25351;&#23548;&#27169;&#22411;&#20026;&#27599;&#20010;&#23454;&#20307;&#29983;&#25104;KG&#23884;&#20837;&#30340;&#25130;&#26029;&#29256;&#26412;&#12290;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#20351;&#24471;&#26356;&#31934;&#32454;&#30340;&#25628;&#32034;&#20174;&#32780;&#26356;&#26377;&#25928;&#36827;&#34892;&#28040;&#27495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;T5&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#25439;&#22833;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;KGQA&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;Wikidata&#30340;LC-QuAD 2.0&#21644;SimpleQuestions-Wikidata&#25968;&#25454;&#38598;&#19978;&#25253;&#21578;&#20102;&#31471;&#21040;&#31471;KGQA&#30340;&#24378;&#22823;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present an end-to-end Knowledge Graph Question Answering (KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text pre-trained language model. The model takes a question in natural language as input and produces a simpler form of the intended SPARQL query. In the simpler form, the model does not directly produce entity and relation IDs. Instead, it produces corresponding entity and relation labels. The labels are grounded to KG entity and relation IDs in a subsequent step. To further improve the results, we instruct the model to produce a truncated version of the KG embedding for each entity. The truncated KG embedding enables a finer search for disambiguation purposes. We find that T5 is able to learn the truncated KG embeddings without any change of loss function, improving KGQA performance. As a result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata datasets on end-to-end KGQA over Wikidata.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;&#19978;&#19979;&#25991;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#21487;&#23398;&#20064;&#25552;&#31034;&#19982;&#25163;&#24037;&#25552;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#35299;&#20915;&#20855;&#26377;&#29305;&#23450;&#25991;&#26412;&#30693;&#35782;&#30340;&#27169;&#22411;&#22312;&#26410;&#30693;&#31867;&#21035;&#19978;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13283</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;&#19978;&#19979;&#25991;&#20248;&#21270;&#30340;&#35270;&#35273;&#35821;&#35328;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Visual-Language Prompt Tuning with Knowledge-guided Context Optimization. (arXiv:2303.13283v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13283
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;&#19978;&#19979;&#25991;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#21487;&#23398;&#20064;&#25552;&#31034;&#19982;&#25163;&#24037;&#25552;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#35299;&#20915;&#20855;&#26377;&#29305;&#23450;&#25991;&#26412;&#30693;&#35782;&#30340;&#27169;&#22411;&#22312;&#26410;&#30693;&#31867;&#21035;&#19978;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#26159;&#23558;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLM)&#36866;&#24212;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20351;&#29992;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25991;&#26412;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#23569;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#36890;&#29992;&#25991;&#26412;&#30693;&#35782;&#65292;&#20855;&#26377;&#29305;&#23450;&#25991;&#26412;&#30693;&#35782;&#30340;&#20195;&#34920;&#24615;CoOp&#24037;&#20316;&#22312;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;&#19978;&#19979;&#25991;&#20248;&#21270;(KgCoOp)&#26469;&#22686;&#24378;&#23545;&#26410;&#30693;&#31867;&#21035;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;KgCoOp&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#21487;&#20197;&#36890;&#36807;&#38477;&#20302;&#21487;&#23398;&#20064;&#25552;&#31034;&#19982;&#25163;&#24037;&#25552;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#32531;&#35299;&#36951;&#24536;&#37325;&#35201;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#65292;KgCoOp&#26368;&#23567;&#21270;&#36890;&#36807;&#23398;&#20064;&#25552;&#31034;&#29983;&#25104;&#30340;&#25991;&#26412;&#23884;&#20837;&#21644;&#25163;&#24037;&#21046;&#20316;&#25552;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#26368;&#21518;&#65292;&#22312;&#23545;&#27604;&#25439;&#22833;&#19978;&#28155;&#21152;KgCoOp&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning is an effective way to adapt the pre-trained visual-language model (VLM) to the downstream task using task-related textual tokens. Representative CoOp-based work combines the learnable textual tokens with the class tokens to obtain specific textual knowledge. However, the specific textual knowledge is the worse generalization to the unseen classes because it forgets the essential general textual knowledge having a strong generalization ability. To tackle this issue, we introduce a novel Knowledge-guided Context Optimization (KgCoOp) to enhance the generalization ability of the learnable prompt for unseen classes. The key insight of KgCoOp is that forgetting about essential knowledge can be alleviated by reducing the discrepancy between the learnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the discrepancy between the textual embeddings generated by learned prompts and the hand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#20351;&#29992;&#36866;&#37197;&#22120;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;SPLADE&#36825;&#31181;&#31232;&#30095;&#26816;&#32034;&#22120;&#19978;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#37197;&#22120;-SPLADE&#21482;&#38656;&#20248;&#21270;2%&#30340;&#35757;&#32451;&#21442;&#25968;&#65292;&#20294;&#22312;&#25928;&#29575;&#21644;&#25928;&#26524;&#26041;&#38754;&#22343;&#20248;&#20110;&#23436;&#20840;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13220</link><description>&lt;p&gt;
&#20351;&#29992;&#36866;&#37197;&#22120;&#30340;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#26816;&#32034;&#22120;&#21644;&#37325;&#25490;&#22120;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Sparse Retrievers and Rerankers using Adapters. (arXiv:2303.13220v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#20351;&#29992;&#36866;&#37197;&#22120;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;SPLADE&#36825;&#31181;&#31232;&#30095;&#26816;&#32034;&#22120;&#19978;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#37197;&#22120;-SPLADE&#21482;&#38656;&#20248;&#21270;2%&#30340;&#35757;&#32451;&#21442;&#25968;&#65292;&#20294;&#22312;&#25928;&#29575;&#21644;&#25928;&#26524;&#26041;&#38754;&#22343;&#20248;&#20110;&#23436;&#20840;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36866;&#37197;&#22120;&#30340;&#21442;&#25968;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#24050;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30740;&#31350;&#20316;&#20026;&#23436;&#20840;&#24494;&#35843;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#36866;&#37197;&#22120;&#26159;&#20869;&#23384;&#39640;&#25928;&#30340;&#65292;&#24182;&#36890;&#36807;&#22312;&#21464;&#21387;&#22120;&#23618;&#20043;&#38388;&#28155;&#21152;&#23567;&#29942;&#39048;&#23618;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#25345;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20923;&#32467;&#26469;&#19982;&#19979;&#28216;&#20219;&#21153;&#33391;&#22909;&#22320;&#36827;&#34892;&#32553;&#25918;&#12290;&#23613;&#31649;&#22312;NLP&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#26041;&#38754;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#23436;&#21892;&#36866;&#37197;&#22120;&#22312;IR&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36866;&#37197;&#22120;&#23545;&#20110;SPLADE&#65288;&#19968;&#31181;&#31232;&#30095;&#26816;&#32034;&#22120;&#65289;&#30340;&#24212;&#29992;&#65292;&#36866;&#37197;&#22120;&#19981;&#20165;&#20445;&#30041;&#20102;&#36890;&#36807;&#23436;&#20840;&#24494;&#35843;&#23454;&#29616;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#32780;&#19988;&#20869;&#23384;&#39640;&#25928;&#65292;&#35757;&#32451;&#36731;&#37327;&#32423;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36866;&#37197;&#22120;-SPLADE&#20165;&#20248;&#21270;2&#65285;&#30340;&#35757;&#32451;&#21442;&#25968;&#65292;&#20294;&#32988;&#36807;&#23436;&#20840;&#24494;&#35843;&#30340;&#23545;&#24212;&#29289;&#20197;&#21450;&#24050;&#26377;&#30340;&#26368;&#20339;&#31232;&#30095;&#26816;&#32034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient transfer learning with Adapters have been studied in Natural Language Processing (NLP) as an alternative to full fine-tuning. Adapters are memory-efficient and scale well with downstream tasks by training small bottle-neck layers added between transformer layers while keeping the large pretrained language model (PLMs) frozen. In spite of showing promising results in NLP, these methods are under-explored in Information Retrieval. While previous studies have only experimented with dense retriever or in a cross lingual retrieval scenario, in this paper we aim to complete the picture on the use of adapters in IR. First, we study adapters for SPLADE, a sparse retriever, for which adapters not only retain the efficiency and effectiveness otherwise achieved by finetuning, but are memory-efficient and orders of magnitude lighter to train. We observe that Adapters-SPLADE not only optimizes just 2\% of training parameters, but outperforms fully fine-tuned counterpart and exis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#31574;&#30053;-FairPrompt&#65292;&#22312;&#20445;&#35777;&#20844;&#27491;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#36890;&#36807;&#35780;&#20272;&#25552;&#31034;&#39044;&#27979;&#20559;&#24046;&#65292;&#30830;&#23450;&#36817;&#20284;&#26368;&#20248;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13217</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20844;&#27491;&#24341;&#23548;&#23569;&#26679;&#26412;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Fairness-guided Few-shot Prompting for Large Language Models. (arXiv:2303.13217v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#31574;&#30053;-FairPrompt&#65292;&#22312;&#20445;&#35777;&#20844;&#27491;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#36890;&#36807;&#35780;&#20272;&#25552;&#31034;&#39044;&#27979;&#20559;&#24046;&#65292;&#30830;&#23450;&#36817;&#20284;&#26368;&#20248;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#36890;&#36807;&#20960;&#20010;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#26500;&#24314;&#30340;&#25552;&#31034;&#36827;&#34892;&#30452;&#25509;&#24212;&#29992;&#26469;&#35299;&#20915;&#20247;&#22810;&#19979;&#28216;&#20219;&#21153;&#12290;&#20294;&#26159;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#35757;&#32451;&#31034;&#20363;&#65292;&#31034;&#20363;&#39034;&#24207;&#21644;&#25552;&#31034;&#26684;&#24335;&#30340;&#21464;&#21270;&#23548;&#33268;&#19978;&#19979;&#25991;&#23398;&#20064;&#23481;&#26131;&#20986;&#29616;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#26500;&#24314;&#36866;&#24403;&#30340;&#25552;&#31034;&#23545;&#20110;&#25913;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20174;&#39044;&#27979;&#20559;&#24046;&#30340;&#35282;&#24230;&#37325;&#26032;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25351;&#26631;&#26469;&#35780;&#20272;&#22266;&#23450;&#25552;&#31034;&#30456;&#23545;&#20110;&#26631;&#31614;&#25110;&#32473;&#23450;&#23646;&#24615;&#30340;&#39044;&#27979;&#20559;&#24046;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#39044;&#27979;&#20559;&#24046;&#36739;&#22823;&#30340;&#25552;&#31034;&#24635;&#26159;&#23548;&#33268;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#39044;&#27979;&#36136;&#37327;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#22522;&#20110;&#36138;&#23146;&#25628;&#32034;&#26469;&#30830;&#23450;&#36817;&#20284;&#26368;&#20248;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21483;&#20570;"&#20844;&#27491;&#25552;&#31034;"&#65292;&#20854;&#20013;&#34701;&#20837;&#20102;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#20197;&#25351;&#23548;&#25628;&#32034;&#19981;&#23637;&#29616;&#20986;&#23545;&#26576;&#20123;&#20154;&#32676;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;FairPrompt&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated surprising ability to perform in-context learning, i.e., these models can be directly applied to solve numerous downstream tasks by conditioning on a prompt constructed by a few input-output examples. However, prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats. Therefore, the construction of an appropriate prompt is essential for improving the performance of in-context learning. In this paper, we revisit this problem from the view of predictive bias. Specifically, we introduce a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes. Then we empirically show that prompts with higher bias always lead to unsatisfactory predictive quality. Based on this observation, we propose a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30456;&#21464;&#29616;&#35937;&#30340;&#31616;&#21333;&#35299;&#37322;&#65292;&#21033;&#29992;&#21015;&#34920;&#35793;&#30721;&#22120;&#24314;&#27169;&#65292;&#23427;&#33021;&#22815;&#20445;&#35777;&#22312;LLM&#20302;&#20110;&#20020;&#30028;&#38408;&#20540;&#26102;&#38169;&#35823;&#20505;&#36873;&#24207;&#21015;&#25968;&#30340;&#26399;&#26395;&#20445;&#25345;&#26377;&#30028;&#65292;&#32780;&#22312;&#39640;&#20110;&#35813;&#38408;&#20540;&#26102;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2303.13112</link><description>&lt;p&gt;
&#21033;&#29992;&#21015;&#34920;&#35793;&#30721;&#35299;&#37322;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30456;&#21464;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
A Simple Explanation for the Phase Transition in Large Language Models with List Decoding. (arXiv:2303.13112v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30456;&#21464;&#29616;&#35937;&#30340;&#31616;&#21333;&#35299;&#37322;&#65292;&#21033;&#29992;&#21015;&#34920;&#35793;&#30721;&#22120;&#24314;&#27169;&#65292;&#23427;&#33021;&#22815;&#20445;&#35777;&#22312;LLM&#20302;&#20110;&#20020;&#30028;&#38408;&#20540;&#26102;&#38169;&#35823;&#20505;&#36873;&#24207;&#21015;&#25968;&#30340;&#26399;&#26395;&#20445;&#25345;&#26377;&#30028;&#65292;&#32780;&#22312;&#39640;&#20110;&#35813;&#38408;&#20540;&#26102;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21576;&#29616;&#20986;&#23567;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#31361;&#20986;&#33021;&#21147;&#12290;&#24403;&#27169;&#22411;&#36798;&#21040;&#19968;&#23450;&#30340;&#35268;&#27169;&#20851;&#38190;&#28857;&#26102;&#65292;&#31995;&#32479;&#24615;&#33021;&#24471;&#21040;&#20102;&#26497;&#22823;&#22320;&#25552;&#39640;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#37322;&#65292;&#24182;&#23558;LLM&#24314;&#27169;&#20026;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#38543;&#26426;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#21015;&#34920;&#35793;&#30721;&#22120;&#20195;&#26367;&#27599;&#20010;&#27493;&#39588;&#30340;&#21363;&#26102;&#29983;&#25104;&#65292;&#35813;&#35793;&#30721;&#22120;&#22312;&#27599;&#20010;&#27493;&#39588;&#20445;&#30041;&#19968;&#20010;&#20505;&#36873;&#24207;&#21015;&#21015;&#34920;&#65292;&#24182;&#22312;&#32467;&#26463;&#26102;&#25512;&#36831;&#36755;&#20986;&#24207;&#21015;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23384;&#22312;&#19968;&#20010;&#20020;&#30028;&#38408;&#20540;&#65292;&#24403;LLM&#20302;&#20110;&#27492;&#38408;&#20540;&#26102;&#65292;&#26399;&#26395;&#30340;&#38169;&#35823;&#20505;&#36873;&#24207;&#21015;&#25968;&#20445;&#25345;&#26377;&#30028;&#65292;&#24403;LLM&#39640;&#20110;&#27492;&#38408;&#20540;&#26102;&#65292;&#26399;&#26395;&#38169;&#35823;&#24207;&#21015;&#25968;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#36825;&#26679;&#30340;&#38408;&#20540;&#19982;&#20256;&#26579;&#30149;&#30340;&#22522;&#26412;&#32321;&#27542;&#25968;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various recent experimental results show that large language models (LLM) exhibit emergent abilities that are not present in small models. System performance is greatly improved after passing a certain critical threshold of scale. In this letter, we provide a simple explanation for such a phase transition phenomenon. For this, we model an LLM as a sequence-to-sequence random function. Instead of using instant generation at each step, we use a list decoder that keeps a list of candidate sequences at each step and defers the generation of the output sequence at the end. We show that there is a critical threshold such that the expected number of erroneous candidate sequences remains bounded when an LLM is below the threshold, and it grows exponentially when an LLM is above the threshold. Such a threshold is related to the basic reproduction number in a contagious disease.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#21644;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;&#30340;&#35821;&#20041;&#22810;&#35270;&#35282;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#35825;&#23548;&#26032;&#24847;&#22270;&#30340;&#38382;&#39064;&#65292;&#22312;Open Intent Induction&#20013;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2303.13099</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#30340;&#38646;&#26679;&#26412;&#24320;&#25918;&#24847;&#22270;&#24402;&#32435;&#65306;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#21644;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer. (arXiv:2303.13099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#21644;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;&#30340;&#35821;&#20041;&#22810;&#35270;&#35282;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#35825;&#23548;&#26032;&#24847;&#22270;&#30340;&#38382;&#39064;&#65292;&#22312;Open Intent Induction&#20013;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#26816;&#27979;&#21644;&#35825;&#23548;&#26032;&#30340;&#24847;&#22270;&#26159;&#23558;&#35813;&#31995;&#32479;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35821;&#20041;&#22810;&#35270;&#35282;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38590;&#39064;&#65306;&#65288;1&#65289;&#29992;&#20110;&#19968;&#33324;&#23884;&#20837;&#30340;SBERT&#65288;2&#65289;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#65288;MDB&#65289;&#29992;&#20110;&#23545;&#35805;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#21450;&#65288;3&#65289;&#29992;&#20110;&#38598;&#32676;&#19987;&#19994;&#35821;&#20041;&#30340;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;&#65288;PGT&#65289;&#12290; MDB&#19968;&#27425;&#21521;&#27169;&#22411;&#25552;&#20379;&#22810;&#31181;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#39046;&#22495;&#30693;&#35782;&#26469;&#35299;&#20915;&#22810;&#39046;&#22495;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;PGT&#65292;&#23427;&#37319;&#29992;Siamese&#32593;&#32476;&#30452;&#25509;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22914;&#20309;&#20351;&#29992;PGT&#32858;&#31867;&#23545;&#35805;&#35821;&#21477;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#31995;&#32479;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#22810;&#35270;&#35282;&#27169;&#22411;&#19982;MDB&#21644;PGT&#26174;&#30528;&#25552;&#39640;&#20102;Open Intent Induction&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Task Oriented Dialogue (TOD) system, detecting and inducing new intents are two main challenges to apply the system in the real world. In this paper, we suggest the semantic multi-view model to resolve these two challenges: (1) SBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue domain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized semantic. MDB feeds diverse dialogue datasets to the model at once to tackle the multi-domain problem by learning the multiple domain knowledge. We introduce a novel method PGT, which employs the Siamese network to fine-tune the model with a clustering method directly.Our model can learn how to cluster dialogue utterances by using PGT. Experimental results demonstrate that our multi-view model with MDB and PGT significantly improves the Open Intent Induction performance compared to baseline systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;ASR&#20013;&#30340;Transformer&#27169;&#22411;&#30340;&#22359;&#37325;&#29992;&#31574;&#30053;&#24182;&#37197;&#20197;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#32039;&#20945;&#65292;&#21487;&#36866;&#24212;&#24615;&#26356;&#24378;&#65292;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.13072</link><description>&lt;p&gt;
&#36229;&#36234;&#36890;&#29992;Transformer&#65306;&#33258;&#36866;&#24212;&#27169;&#22359;&#22312;ASR&#20013;&#30340;Transformer&#27169;&#22411;&#20013;&#30340;&#27169;&#22359;&#37325;&#29992;
&lt;/p&gt;
&lt;p&gt;
Beyond Universal Transformer: block reusing with adaptor in Transformer for automatic speech recognit. (arXiv:2303.13072v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;ASR&#20013;&#30340;Transformer&#27169;&#22411;&#30340;&#22359;&#37325;&#29992;&#31574;&#30053;&#24182;&#37197;&#20197;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#32039;&#20945;&#65292;&#21487;&#36866;&#24212;&#24615;&#26356;&#24378;&#65292;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290; Transformer&#27169;&#22411;&#20351;&#24471;&#33021;&#22815;&#22312;&#26234;&#33021;&#35774;&#22791;&#19978;&#37096;&#32626;&#31471;&#21040;&#31471;&#30340;ASR&#31995;&#32479;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#26377;&#19968;&#20010;&#32570;&#28857;&#65292;&#21363;&#38656;&#35201;&#22823;&#37327;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#20026;&#20102;&#20811;&#26381;&#36890;&#29992;Transformer&#27169;&#22411;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#24212;&#29992;ASR&#30340;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#37325;&#29992;Transformer&#27169;&#22411;&#20013;&#30340;&#22359;&#20316;&#20026;&#23567;&#23610;&#23544;ASR&#31995;&#32479;&#20351;&#29992;&#30340;&#35774;&#35745;&#31574;&#30053;&#65292;&#26082;&#28385;&#36275;&#36164;&#28304;&#38480;&#21046;&#30340;&#30446;&#26631;&#65292;&#21448;&#19981;&#20250;&#24433;&#21709;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#35821;&#38899;Transformer&#65288;BRST&#65289;&#30340;&#22359;&#37325;&#29992;&#31574;&#30053;&#26469;&#22686;&#24378;&#21442;&#25968;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#37197;&#22120;&#27169;&#22359;&#65288;ADM&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#19968;&#20010;&#21482;&#38656;&#35201;&#23569;&#37327;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#32039;&#20945;&#21644;&#21487;&#36866;&#24212;&#30340;&#27169;&#22411;&#26469;&#30830;&#20445;&#27599;&#20010;&#37325;&#29992;&#22359;&#30340;&#38506;&#20276;&#12290;&#25105;&#20204;&#22312;Aishell-1&#21644;CommonVoice&#26816;&#39564;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;BRST&#23545;&#20934;&#30830;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#31561;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have recently made significant achievements in the application of end-to-end (E2E) automatic speech recognition (ASR). It is possible to deploy the E2E ASR system on smart devices with the help of Transformer-based models. While these models still have the disadvantage of requiring a large number of model parameters. To overcome the drawback of universal Transformer models for the application of ASR on edge devices, we propose a solution that can reuse the block in Transformer models for the occasion of the small footprint ASR system, which meets the objective of accommodating resource limitations without compromising recognition accuracy. Specifically, we design a novel block-reusing strategy for speech Transformer (BRST) to enhance the effectiveness of parameters and propose an adapter module (ADM) that can produce a compact and adaptable model with only a few additional trainable parameters accompanying each reusing block. We conducted an experiment with the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#31890;&#24230;&#30340;&#20013;&#25991;BERT&#65288;MigBERT&#65289;&#65292;&#21033;&#29992;&#21516;&#26102;&#32771;&#34385;&#23383;&#31526;&#21644;&#35789;&#27719;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#20013;&#25991;PLMs&#30340;&#34920;&#29616;&#65292;&#24182;&#22312;&#21508;&#31181;&#20013;&#25991;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#24615;&#33021;&#12290;&#21333;&#35789;&#27604;&#23383;&#31526;&#35821;&#20041;&#26356;&#20016;&#23500;&#12290;&#25968;&#23383;&#20063;&#26174;&#31034;&#20102;MigBERT&#21487;&#20197;&#22312;&#26085;&#35821;&#20013;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.13065</link><description>&lt;p&gt;
&#21033;&#29992;&#35299;&#32806;&#34920;&#31034;&#30340;&#26816;&#32034;&#22686;&#24378;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Classification with Decoupled Representation. (arXiv:2303.13065v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#31890;&#24230;&#30340;&#20013;&#25991;BERT&#65288;MigBERT&#65289;&#65292;&#21033;&#29992;&#21516;&#26102;&#32771;&#34385;&#23383;&#31526;&#21644;&#35789;&#27719;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#20013;&#25991;PLMs&#30340;&#34920;&#29616;&#65292;&#24182;&#22312;&#21508;&#31181;&#20013;&#25991;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#24615;&#33021;&#12290;&#21333;&#35789;&#27604;&#23383;&#31526;&#35821;&#20041;&#26356;&#20016;&#23500;&#12290;&#25968;&#23383;&#20063;&#26174;&#31034;&#20102;MigBERT&#21487;&#20197;&#22312;&#26085;&#35821;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#22823;&#22810;&#25968;&#20013;&#25991;PLM&#23558;&#36755;&#20837;&#25991;&#26412;&#35270;&#20026;&#23383;&#31526;&#24207;&#21015;&#65292;&#24182;&#23436;&#20840;&#24573;&#30053;&#35789;&#20449;&#24687;&#12290;&#34429;&#28982;&#25972;&#35789;&#23631;&#34109;&#21487;&#20197;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#35789;&#27719;&#20013;&#30340;&#35821;&#20041;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20013;&#25991;PLM&#30340;&#20998;&#35789;&#31890;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#23383;&#31526;&#21644;&#35789;&#27719;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#31890;&#24230;&#30340;&#20013;&#25991;BERT&#65288;MigBERT&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#29992;&#20110;&#23398;&#20064;&#23383;&#31526;&#21644;&#21333;&#35789;&#32423;&#34920;&#31034;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#20013;&#25991;NLP&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#29616;&#26377;PLM&#20197;&#21450;&#25152;&#25552;&#20986;&#30340;MigBERT&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MigBERT&#22312;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#19978;&#22343;&#23454;&#29616;&#20102;&#26032;&#30340;SOTA&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#21333;&#35789;&#27604;&#23383;&#31526;&#35821;&#20041;&#26356;&#20016;&#23500;&#12290;&#26356;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MigBERT&#20063;&#21487;&#20197;&#19982;&#26085;&#35821;&#19968;&#36215;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (PLMs) have shown marvelous improvements across various NLP tasks. Most Chinese PLMs simply treat an input text as a sequence of characters, and completely ignore word information. Although Whole Word Masking can alleviate this, the semantics in words is still not well represented. In this paper, we revisit the segmentation granularity of Chinese PLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both characters and words. To achieve this, we design objective functions for learning both character and word-level representations. We conduct extensive experiments on various Chinese NLP tasks to evaluate existing PLMs as well as the proposed MigBERT. Experimental results show that MigBERT achieves new SOTA performance on all these tasks. Further analysis demonstrates that words are semantically richer than characters. More interestingly, we show that MigBERT also works with Japanese. Our code has been released here~\footnote{\url{https://g
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#25552;&#20986;Soft Prompt-Based Calibration (SPeC)&#31649;&#36947;&#65292;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;. &#27492;&#26041;&#27861;&#19981;&#20165;&#27604;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24615;&#33021;&#31283;&#23450;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;.</title><link>http://arxiv.org/abs/2303.13035</link><description>&lt;p&gt;
SPeC&#65306;&#36719;&#25552;&#31034;&#26657;&#20934;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization. (arXiv:2303.13035v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13035
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#25552;&#20986;Soft Prompt-Based Calibration (SPeC)&#31649;&#36947;&#65292;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;. &#27492;&#26041;&#27861;&#19981;&#20165;&#27604;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24615;&#33021;&#31283;&#23450;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#23384;&#20648;&#30528;&#21253;&#25324;&#30149;&#21382;&#12289;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#26816;&#27979;&#32467;&#26524;&#22312;&#20869;&#30340;&#22823;&#37327;&#24739;&#32773;&#20449;&#24687;&#12290;&#36825;&#20123;&#35760;&#24405;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#20570;&#20986;&#26126;&#26234;&#30340;&#24739;&#32773;&#25252;&#29702;&#20915;&#31574;&#38750;&#24120;&#20851;&#38190;&#12290;&#25688;&#35201;&#20020;&#24202;&#31508;&#35760;&#21487;&#20197;&#24110;&#21161;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#26356;&#22909;&#22320;&#21457;&#29616;&#28508;&#22312;&#20581;&#24247;&#39118;&#38505;&#65292;&#20197;&#21450;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#36825;&#19968;&#36807;&#31243;&#36890;&#36807;&#30830;&#20445;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#21487;&#20197;&#35775;&#38382;&#26368;&#30456;&#20851;&#21644;&#26368;&#26032;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#38169;&#35823;&#24182;&#25552;&#39640;&#24739;&#32773;&#30340;&#25252;&#29702;&#25928;&#26524;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#25552;&#31034;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#20063;&#20250;&#23548;&#33268;&#36755;&#20986;&#26041;&#24046;&#22686;&#21152;&#65292;&#21363;&#20351;&#25552;&#31034;&#24847;&#20041;&#30456;&#20284;&#65292;&#36755;&#20986;&#20063;&#20250;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#36719;&#25552;&#31034;&#26657;&#20934;&#65288;SPeC&#65289;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#37319;&#29992;&#36719;&#25552;&#31034;&#23884;&#20837;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SPeC&#19981;&#20165;&#21487;&#20197;&#38477;&#20302;LLM&#30340;&#24615;&#33021;&#21464;&#24322;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHRs) store an extensive array of patient information, encompassing medical histories, diagnoses, treatments, and test outcomes. These records are crucial for enabling healthcare providers to make well-informed decisions regarding patient care. Summarizing clinical notes further assists healthcare professionals in pinpointing potential health risks and making better-informed decisions. This process contributes to reducing errors and enhancing patient outcomes by ensuring providers have access to the most pertinent and current patient data. Recent research has shown that incorporating prompts with large language models (LLMs) substantially boosts the efficacy of summarization tasks. However, we show that this approach also leads to increased output variance, resulting in notably divergent outputs even when prompts share similar meanings. To tackle this challenge, we introduce a model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft prom
&lt;/p&gt;</description></item><item><title>GesGPT &#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#38899;&#25163;&#21183;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; GPT &#30340;&#35821;&#20041;&#20998;&#26512;&#33021;&#21147;&#65292;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#19982;&#25163;&#21183;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#25163;&#21183;&#24211;&#21644;&#38598;&#25104;&#27169;&#22359;&#29983;&#25104;&#24847;&#20041;&#20016;&#23500;&#30340;&#35821;&#38899;&#25163;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.13013</link><description>&lt;p&gt;
GesGPT: &#22522;&#20110; GPT &#25991;&#26412;&#35299;&#26512;&#30340;&#35821;&#38899;&#25163;&#21183;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
GesGPT: Speech Gesture Synthesis With Text Parsing from GPT. (arXiv:2303.13013v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13013
&lt;/p&gt;
&lt;p&gt;
GesGPT &#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#38899;&#25163;&#21183;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; GPT &#30340;&#35821;&#20041;&#20998;&#26512;&#33021;&#21147;&#65292;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#19982;&#25163;&#21183;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#25163;&#21183;&#24211;&#21644;&#38598;&#25104;&#27169;&#22359;&#29983;&#25104;&#24847;&#20041;&#20016;&#23500;&#30340;&#35821;&#38899;&#25163;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21183;&#21512;&#25104;&#20316;&#20026;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#21644;&#33258;&#28982;&#30340;&#25163;&#21183;&#26469;&#23545;&#24212;&#35821;&#38899;&#25110;&#25991;&#26412;&#36755;&#20837;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#24573;&#30053;&#20102;&#25991;&#26412;&#20013;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#23548;&#33268;&#25163;&#21183;&#19981;&#22815;&#34920;&#36798;&#21644;&#24847;&#20041;&#19981;&#22815;&#20016;&#23500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; GesGPT&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914; GPT &#30340;&#35821;&#20041;&#20998;&#26512;&#33021;&#21147;&#36827;&#34892;&#25163;&#21183;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992; LLM &#22312;&#25991;&#26412;&#20998;&#26512;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#25552;&#31034;&#26469;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#19982;&#25163;&#21183;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#24320;&#21457;&#25552;&#31034;&#21407;&#21017;&#65292;&#23558;&#25163;&#21183;&#29983;&#25104;&#36716;&#21270;&#20026;&#22522;&#20110; GPT &#30340;&#24847;&#22270;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#31934;&#24515;&#31574;&#21010;&#30340;&#25163;&#21183;&#24211;&#21644;&#38598;&#25104;&#27169;&#22359;&#29983;&#25104;&#35821;&#20041;&#20016;&#23500;&#30340;&#35821;&#38899;&#25163;&#21183;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GesGPT &#21487;&#20197;&#26377;&#25928;&#22320;&#38024;&#23545;&#35821;&#38899;&#25110;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#12289;&#26377;&#24847;&#20041;&#30340;&#25163;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gesture synthesis has gained significant attention as a critical research area, focusing on producing contextually appropriate and natural gestures corresponding to speech or textual input. Although deep learning-based approaches have achieved remarkable progress, they often overlook the rich semantic information present in the text, leading to less expressive and meaningful gestures. We propose GesGPT, a novel approach to gesture generation that leverages the semantic analysis capabilities of Large Language Models (LLMs), such as GPT. By capitalizing on the strengths of LLMs for text analysis, we design prompts to extract gesture-related information from textual input. Our method entails developing prompt principles that transform gesture generation into an intention classification problem based on GPT, and utilizing a curated gesture library and integration module to produce semantically rich co-speech gestures. Experimental results demonstrate that GesGPT effectively generates conte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;ChatGPT&#20316;&#20026;&#20851;&#38190;&#35789;&#29983;&#25104;&#22120;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#65292;&#21457;&#29616;&#20854;&#22312;&#21508;&#20010;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#39046;&#22495;&#20851;&#38190;&#35789;&#29983;&#25104;&#26041;&#38754;&#12290;ChatGPT&#20173;&#38754;&#20020;&#29983;&#25104;&#32570;&#22833;&#20851;&#38190;&#35789;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.13001</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#27454;&#22909;&#30340;&#20851;&#38190;&#35789;&#29983;&#25104;&#22120;&#21527;&#65311;&#21021;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT A Good Keyphrase Generator? A Preliminary Study. (arXiv:2303.13001v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ChatGPT&#20316;&#20026;&#20851;&#38190;&#35789;&#29983;&#25104;&#22120;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#65292;&#21457;&#29616;&#20854;&#22312;&#21508;&#20010;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#39046;&#22495;&#20851;&#38190;&#35789;&#29983;&#25104;&#26041;&#38754;&#12290;ChatGPT&#20173;&#38754;&#20020;&#29983;&#25104;&#32570;&#22833;&#20851;&#38190;&#35789;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#35745;&#31639;&#35821;&#35328;&#23398;&#30028;&#30340;&#37325;&#35270;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#20316;&#20026;&#20851;&#38190;&#35789;&#29983;&#25104;&#22120;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23545;ChatGPT&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#20197;&#29992;&#20110;&#20851;&#38190;&#35789;&#29983;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20854;&#22312;&#21508;&#20010;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20851;&#38190;&#35789;&#29983;&#25104;&#25552;&#31034;&#65292;&#20851;&#38190;&#35789;&#29983;&#25104;&#22810;&#26679;&#24615;&#65292;&#22810;&#39046;&#22495;&#20851;&#38190;&#35789;&#29983;&#25104;&#21644;&#38271;&#25991;&#26412;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#22522;&#20110;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;OpenAI&#24314;&#35758;&#30340;&#25552;&#31034;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#20026;&#20845;&#20010;&#20505;&#36873;&#25552;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;ChatGPT&#22312;&#25152;&#26377;&#20845;&#20010;&#20505;&#36873;&#25552;&#31034;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#35266;&#23519;&#21040;&#20102;&#36731;&#24494;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;ChatGPT&#26377;&#24456;&#22823;&#30340;&#20851;&#38190;&#35789;&#29983;&#25104;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;ChatGPT&#22312;&#29983;&#25104;&#32570;&#22833;&#20851;&#38190;&#35789;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#22312;&#26368;&#21518;&#19968;&#33410;&#20013;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20123;&#38480;&#21046;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of ChatGPT has recently garnered significant attention from the computational linguistics community. To demonstrate its capabilities as a keyphrase generator, we conduct a preliminary evaluation of ChatGPT for the keyphrase generation task. We evaluate its performance in various aspects, including keyphrase generation prompts, keyphrase generation diversity, multi-domain keyphrase generation, and long document understanding. Our evaluation is based on six benchmark datasets, and we adopt the prompt suggested by OpenAI while extending it to six candidate prompts. We find that ChatGPT performs exceptionally well on all six candidate prompts, with minor performance differences observed across the datasets. Based on our findings, we conclude that ChatGPT has great potential for keyphrase generation. Moreover, we discover that ChatGPT still faces challenges when it comes to generating absent keyphrases. Meanwhile, in the final section, we also present some limitations and futu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#8220;&#20132;&#21449;&#29615;&#22659;&#8221;&#35774;&#32622;&#65292;&#35780;&#20272;&#20102;ELMo&#21644;DistilBERT&#22312;&#26032;&#38395;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#19978;&#30340;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#34920;&#29616;&#12290;&#34429;&#28982;&#20004;&#31181;&#27169;&#22411;&#36229;&#36807;&#20256;&#32479;&#22522;&#32447;&#65292;&#20294;&#22312;&#36328;&#29615;&#22659;&#27979;&#35797;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#26292;&#38706;&#20102;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#20195;&#34920;&#33021;&#21147;&#26497;&#38480;&#12290;</title><link>http://arxiv.org/abs/2303.12936</link><description>&lt;p&gt;
&#28145;&#24230;&#19978;&#19979;&#25991;&#21270;&#35821;&#35328;&#34920;&#31034;&#23545;&#25991;&#26412;&#20998;&#31867;&#21487;&#27867;&#21270;&#24615;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Generalizability of Deep Contextualized Language Representations For Text Classification. (arXiv:2303.12936v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#8220;&#20132;&#21449;&#29615;&#22659;&#8221;&#35774;&#32622;&#65292;&#35780;&#20272;&#20102;ELMo&#21644;DistilBERT&#22312;&#26032;&#38395;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#19978;&#30340;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#34920;&#29616;&#12290;&#34429;&#28982;&#20004;&#31181;&#27169;&#22411;&#36229;&#36807;&#20256;&#32479;&#22522;&#32447;&#65292;&#20294;&#22312;&#36328;&#29615;&#22659;&#27979;&#35797;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#26292;&#38706;&#20102;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#20195;&#34920;&#33021;&#21147;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#19978;&#19979;&#25991;&#35821;&#35328;&#34920;&#31034;&#8212;&#8212;ELMo&#21644;DistilBERT&#22312;&#20108;&#20803;&#25239;&#35758;&#26032;&#38395;&#20998;&#31867;&#21644;&#20135;&#21697;&#35780;&#35770;&#24773;&#24863;&#20998;&#26512;&#30340;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#31283;&#20581;&#24615;&#65292;&#37319;&#29992;&#20102;&#8220;&#20132;&#21449;&#29615;&#22659;&#8221;&#35774;&#32622;&#65292;&#20351;&#29992;&#27979;&#35797;&#38598;&#19981;&#21516;&#20110;&#35757;&#32451;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#26032;&#38395;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#21360;&#24230;&#26412;&#22320;&#26032;&#38395;&#19978;&#24320;&#21457;&#30340;&#65292;&#24182;&#22312;&#20013;&#22269;&#26412;&#22320;&#26032;&#38395;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#30005;&#24433;&#35780;&#35770;&#26041;&#38754;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#32780;&#22312;&#23458;&#25143;&#35780;&#35770;&#26041;&#38754;&#36827;&#34892;&#27979;&#35797;&#30340;&#12290;&#26412;&#27425;&#27604;&#36739;&#26088;&#22312;&#25506;&#32034;&#24403;&#20170;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#20195;&#34920;&#33021;&#21147;&#26497;&#38480;&#65292;&#20197;&#36798;&#21040;&#36890;&#29992;&#20110;&#30495;&#23454;&#22330;&#26223;&#30340;&#31995;&#32479;&#12290;&#27169;&#22411;&#32463;&#36807;&#24494;&#35843;&#65292;&#20351;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#12290;&#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;&#32447;&#24615;&#25903;&#25345;&#21521;&#37327;&#26426;&#29992;&#20316;&#20256;&#32479;&#22522;&#32447;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ELMo&#21644;DistilBERT&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#22522;&#32447;&#65292;&#20294;&#22312;&#36328;&#29615;&#22659;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study evaluates the robustness of two state-of-the-art deep contextual language representations, ELMo and DistilBERT, on supervised learning of binary protest news classification and sentiment analysis of product reviews. A "cross-context" setting is enabled using test sets that are distinct from the training data. Specifically, in the news classification task, the models are developed on local news from India and tested on the local news from China. In the sentiment analysis task, the models are trained on movie reviews and tested on customer reviews. This comparison is aimed at exploring the limits of the representative power of today's Natural Language Processing systems on the path to the systems that are generalizable to real-life scenarios. The models are fine-tuned and fed into a Feed-Forward Neural Network and a Bidirectional Long Short Term Memory network. Multinomial Naive Bayes and Linear Support Vector Machine are used as traditional baselines. The results show that, i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#21307;&#23398;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#24403;&#21069;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20934;&#30830;&#29575;&#19981;&#36275;&#20197;&#28385;&#36275;&#20020;&#24202;&#38656;&#27714;&#65292;&#38656;&#35201;&#26356;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.12898</link><description>&lt;p&gt;
&#25506;&#32034;&#21307;&#23398;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#27867;&#21270;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Generalization of Medical Text-to-SQL Models and Datasets. (arXiv:2303.12898v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#21307;&#23398;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#24403;&#21069;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20934;&#30830;&#29575;&#19981;&#36275;&#20197;&#28385;&#36275;&#20020;&#24202;&#38656;&#27714;&#65292;&#38656;&#35201;&#26356;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30149;&#21382;&#65288;EMR&#65289;&#23384;&#20648;&#22312;&#20851;&#31995;&#22411;&#25968;&#25454;&#24211;&#20013;&#12290;&#22914;&#26524;&#29992;&#25143;&#19981;&#29087;&#24713;&#25968;&#25454;&#24211;&#26550;&#26500;&#25110;&#25968;&#25454;&#24211;&#22522;&#30784;&#30693;&#35782;&#65292;&#21017;&#35775;&#38382;&#25152;&#38656;&#20449;&#24687;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#26041;&#27861;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#25552;&#20379;&#30452;&#25509;&#35775;&#38382;EMR&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#25968;&#25454;&#24211;&#19987;&#23478;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#24050;&#34987;&#8220;&#35299;&#20915;&#8221;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#24050;&#33719;&#24471;&#20102;&#22823;&#20110;&#25110;&#25509;&#36817;90%&#30340;&#20934;&#30830;&#24230;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#35299;&#20915;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#20173;&#26377;&#24456;&#38271;&#30340;&#36335;&#35201;&#36208;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#29616;&#26377;&#21307;&#23398;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;MIMICSQL&#30340;&#26032;&#25968;&#25454;&#20998;&#21106;&#65292;&#26356;&#22909;&#22320;&#34913;&#37327;&#20102;&#32467;&#26524;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#26032;&#25968;&#25454;&#20998;&#21106;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#20934;&#30830;&#24230;&#20174;&#26368;&#39640;&#36798;92%&#38477;&#33267;28%&#65292;&#22240;&#27492;&#34920;&#26126;&#26377;&#22823;&#37327;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic medical records (EMRs) are stored in relational databases. It can be challenging to access the required information if the user is unfamiliar with the database schema or general database fundamentals. Hence, researchers have explored text-to-SQL generation methods that provide healthcare professionals direct access to EMR data without needing a database expert. However, currently available datasets have been essentially "solved" with state-of-the-art models achieving accuracy greater than or near 90%. In this paper, we show that there is still a long way to go before solving text-to-SQL generation in the medical domain. To show this, we create new splits of the existing medical text-to-SQL dataset MIMICSQL that better measure the generalizability of the resulting models. We evaluate state-of-the-art language models on our new split showing substantial drops in performance with accuracy dropping from up to 92% to 28%, thus showing substantial room for improvement. Moreover, w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;Switch Transformer&#26694;&#26550;&#65292;&#24182;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#22312;&#23567;&#22411;&#27861;&#35821;&#20020;&#24202;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#27604;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#37319;&#29992;Switch Transformer&#30340;&#19987;&#23478;&#28151;&#21512;&#26426;&#21046;&#26377;&#21161;&#20110;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24230;&#65292;&#26368;&#32456;&#22312;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;87&#65285;&#30340;&#20934;&#30830;&#29575;&#12289;87&#65285;&#30340;&#31934;&#24230;&#21644;86&#65285;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.12892</link><description>&lt;p&gt;
&#29992;&#20110;&#20020;&#24202;&#21465;&#36848;&#20998;&#31867;&#30340;&#23567;&#35268;&#27169;&#20132;&#25442;&#21464;&#21387;&#22120;&#21644;&#22522;&#20110;NLP&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Small-Scale Switch Transformer and NLP-based Model for Clinical Narratives Classification. (arXiv:2303.12892v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;Switch Transformer&#26694;&#26550;&#65292;&#24182;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#22312;&#23567;&#22411;&#27861;&#35821;&#20020;&#24202;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#27604;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#37319;&#29992;Switch Transformer&#30340;&#19987;&#23478;&#28151;&#21512;&#26426;&#21046;&#26377;&#21161;&#20110;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24230;&#65292;&#26368;&#32456;&#22312;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;87&#65285;&#30340;&#20934;&#30830;&#29575;&#12289;87&#65285;&#30340;&#31934;&#24230;&#21644;86&#65285;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#65288;&#22914;&#20132;&#25442;&#21464;&#21387;&#22120;&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#36807;&#20110;&#22797;&#26434;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26377;&#38480;&#25968;&#25454;&#30340;&#23567;&#22411;&#20020;&#24202;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;Switch Transformer&#26694;&#26550;&#65292;&#24182;&#20174;&#22836;&#24320;&#22987;&#22312;CHU Sainte-Justine&#21307;&#38498;&#30340;&#23567;&#22411;&#27861;&#35821;&#20020;&#24202;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31616;&#21270;&#30340;&#23567;&#35268;&#27169;&#21464;&#21387;&#22120;&#27169;&#22411;&#20248;&#20110;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#21253;&#25324;DistillBERT&#12289;CamemBERT&#12289;FlauBERT&#21644;FrALBERT&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;Switch Transformer&#30340;&#19987;&#23478;&#28151;&#21512;&#26426;&#21046;&#26377;&#21161;&#20110;&#25429;&#33719;&#22810;&#26679;&#30340;&#27169;&#24335;&#65307;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#20855;&#26377;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#30340;&#20256;&#32479;&#21464;&#21387;&#22120;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;87&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;87&#65285;&#30340;&#31934;&#24230;&#21644;86&#65285;&#30340;&#21484;&#22238;&#29575;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#23567;&#22411;&#20020;&#24202;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Transformer-based models such as the Switch Transformer have achieved remarkable results in natural language processing tasks. However, these models are often too complex and require extensive pre-training, which limits their effectiveness for small clinical text classification tasks with limited data. In this study, we propose a simplified Switch Transformer framework and train it from scratch on a small French clinical text classification dataset at CHU Sainte-Justine hospital. Our results demonstrate that the simplified small-scale Transformer models outperform pre-trained BERT-based models, including DistillBERT, CamemBERT, FlauBERT, and FrALBERT. Additionally, using a mixture of expert mechanisms from the Switch Transformer helps capture diverse patterns; hence, the proposed approach achieves better results than a conventional Transformer with the self-attention mechanism. Finally, our proposed framework achieves an accuracy of 87\%, precision at 87\%, and recall 
&lt;/p&gt;</description></item><item><title>JaCoText&#26159;&#19968;&#31181;&#20351;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#29983;&#25104;Java&#28304;&#20195;&#30721;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.12869</link><description>&lt;p&gt;
JaCoText: &#19968;&#31181;&#29992;&#20110;Java&#20195;&#30721;&#25991;&#26412;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
JaCoText: A Pretrained Model for Java Code-Text Generation. (arXiv:2303.12869v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12869
&lt;/p&gt;
&lt;p&gt;
JaCoText&#26159;&#19968;&#31181;&#20351;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#29983;&#25104;Java&#28304;&#20195;&#30721;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;transformer&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19968;&#32929;&#26032;&#30340;&#20852;&#36259;&#28010;&#28526;&#28044;&#36215;&#65306;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#35821;&#35328;&#12290;&#36825;&#39033;&#20219;&#21153;&#21253;&#25324;&#23558;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36716;&#25442;&#20026;&#32534;&#31243;&#20195;&#30721;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;JaCoText&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformers&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#29983;&#25104;Java&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained transformer-based models have shown high performance in natural language generation task. However, a new wave of interest has surged: automatic programming language generation. This task consists of translating natural language instructions to a programming code. Despite the fact that well-known pretrained models on language generation have achieved good performance in learning programming languages, effort is still needed in automatic code generation. In this paper, we introduce JaCoText, a model based on Transformers neural network. It aims to generate java source code from natural language text. JaCoText leverages advantages of both natural language and code generation models. More specifically, we study some findings from the state of the art and use them to (1) initialize our model from powerful pretrained models, (2) explore additional pretraining on our java dataset, (3) carry out experiments combining the unimodal and bimodal data in the training, and (4) scale the i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#29702;&#35299;&#30340;&#26174;&#33879;&#24615;&#36328;&#24230;&#25513;&#34109;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;Temporal Span Masking&#20013;&#38388;&#35757;&#32451;&#24182;&#19982;Salient Span Masking&#32467;&#21512;&#20351;&#29992;&#65292;&#26377;&#25928;&#25552;&#39640;&#22810;&#20010;&#26102;&#38388;&#20219;&#21153;&#30340;&#24615;&#33021;&#21450;&#34920;&#31034;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.12860</link><description>&lt;p&gt;
&#29992;&#20110;&#26102;&#38388;&#29702;&#35299;&#30340;&#26174;&#33879;&#24615;&#36328;&#24230;&#25513;&#34109;
&lt;/p&gt;
&lt;p&gt;
Salient Span Masking for Temporal Understanding. (arXiv:2303.12860v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#29702;&#35299;&#30340;&#26174;&#33879;&#24615;&#36328;&#24230;&#25513;&#34109;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;Temporal Span Masking&#20013;&#38388;&#35757;&#32451;&#24182;&#19982;Salient Span Masking&#32467;&#21512;&#20351;&#29992;&#65292;&#26377;&#25928;&#25552;&#39640;&#22810;&#20010;&#26102;&#38388;&#20219;&#21153;&#30340;&#24615;&#33021;&#21450;&#34920;&#31034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#33879;&#24615;&#36328;&#24230;&#25513;&#34109; (SSM) &#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#23553;&#38381;&#24335;&#38382;&#31572;&#24615;&#33021;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290; SSM&#36890;&#36807;&#21019;&#24314;&#39069;&#22806;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#21477;&#23376;&#23545;&#26222;&#36890;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25193;&#23637;&#65292;&#36825;&#20123;&#21477;&#23376;&#23631;&#34109;&#20102;&#19968;&#20010;&#23454;&#20307;&#25110;&#26085;&#26399;&#36328;&#24230;&#65292;&#20174;&#32780;&#36807;&#24230;&#21462;&#26679;&#20102;&#20107;&#23454;&#20449;&#24687;&#12290; &#23613;&#31649;&#36825;&#31181;&#33539;&#24335;&#24456;&#25104;&#21151;&#65292;&#20294;&#36328;&#24230;&#31867;&#22411;&#21644;&#37319;&#26679;&#31574;&#30053;&#30456;&#23545;&#20219;&#24847;&#65292;&#24182;&#19988;&#19981;&#34987;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#12290; &#22240;&#27492;&#65292;&#25105;&#20204;&#20174;&#26102;&#38388;&#20219;&#21153;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;SSM&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#23398;&#20064;&#21508;&#31181;&#26102;&#38388;&#34920;&#36798;&#30340;&#33391;&#22909;&#34920;&#31034;&#38750;&#24120;&#37325;&#35201;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20013;&#38388;&#22521;&#35757;Temporal Span Masking (TSM)&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#20351;&#29992;SSM&#23601;&#21487;&#20197;&#24179;&#22343;&#25913;&#21892;&#19977;&#20010;&#26102;&#38388;&#20219;&#21153;&#30340;&#19979;&#28216;&#24615;&#33021;5.8&#20010;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;TSM&#20219;&#21153;&#33021;&#22815;&#23454;&#29616;&#39069;&#22806;&#30340;&#25913;&#36827;&#65288;&#24179;&#22343;+0.29&#20010;&#28857;&#65289;&#12290;&#36825;&#20123;&#26159;&#30446;&#26631;&#20219;&#21153;&#25253;&#21578;&#30340;&#26032;&#26368;&#20339;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;SSM&#21644;TSM&#31574;&#30053;&#30340;&#25928;&#26524;&#23545;&#20110;&#22810;&#20010;&#26102;&#38388;&#20219;&#21153;&#26159;&#36890;&#29992;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#30456;&#20114;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Salient Span Masking (SSM) has shown itself to be an effective strategy to improve closed-book question answering performance. SSM extends general masked language model pretraining by creating additional unsupervised training sentences that mask a single entity or date span, thus oversampling factual information. Despite the success of this paradigm, the span types and sampling strategies are relatively arbitrary and not widely studied for other tasks. Thus, we investigate SSM from the perspective of temporal tasks, where learning a good representation of various temporal expressions is important. To that end, we introduce Temporal Span Masking (TSM) intermediate training. First, we find that SSM alone improves the downstream performance on three temporal tasks by an avg. +5.8 points. Further, we are able to achieve additional improvements (avg. +0.29 points) by adding the TSM task. These comprise the new best reported results on the targeted tasks. Our analysis suggests that the effec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12816</link><description>&lt;p&gt;
&#20174;&#23485;&#21040;&#28145;&#65306;&#32500;&#24230;&#25552;&#21319;&#32593;&#32476;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#26144;&#23556;&#21040;&#21521;&#37327;&#34920;&#31034;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;KGE&#26041;&#27861;&#38656;&#35201;&#30456;&#23545;&#39640;&#32500;&#30340;&#23454;&#20307;&#34920;&#31034;&#26469;&#20445;&#30041;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20294;&#20250;&#23548;&#33268;&#24222;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#26469;&#38477;&#20302;&#27169;&#22411;&#21442;&#25968;&#65292;&#21516;&#26102;&#24320;&#21457;&#25216;&#26415;&#65288;&#20363;&#22914;&#30693;&#35782;&#33976;&#39311;&#65289;&#26469;&#34917;&#20607;&#38477;&#32500;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25805;&#20316;&#20250;&#23548;&#33268;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;&#23454;&#20307;&#34920;&#31034;&#30340;&#32423;&#32852;&#35270;&#20026;&#23884;&#20837;&#23618;&#65292;&#37027;&#20040;&#37319;&#29992;&#39640;&#32500;&#23454;&#20307;&#34920;&#31034;&#30340;&#20256;&#32479;KGE&#26041;&#27861;&#31561;&#21516;&#20110;&#25193;&#23637;&#23884;&#20837;&#23618;&#30340;&#23485;&#24230;&#20197;&#33719;&#24471;&#34920;&#29616;&#21147;&#12290;&#20026;&#20102;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21442;&#25968;&#25928;&#29575;&#65292;&#25105;&#20204;&#30456;&#21453;&#22320;&#22686;&#21152;&#28145;&#24230;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26356;&#28145;&#30340;&#23454;&#20307;&#23884;&#20837;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#19981;&#21516;&#39046;&#22495;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;LLM&#22312;&#31867;&#27604;&#21644;&#36947;&#24503;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#31354;&#38388;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;&#36825;&#23545;&#20110;LLM&#26410;&#26469;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.12810</link><description>&lt;p&gt;
LLM&#26159;&#19975;&#33021;&#30340;&#22823;&#24072;&#21527;&#65311;&#25506;&#32034;LLM&#30340;&#39046;&#22495;&#19981;&#21487;&#30693;&#25512;&#29702;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning Skills of LLMs. (arXiv:2303.12810v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#19981;&#21516;&#39046;&#22495;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;LLM&#22312;&#31867;&#27604;&#21644;&#36947;&#24503;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#31354;&#38388;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;&#36825;&#23545;&#20110;LLM&#26410;&#26469;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#31867;&#20284;&#20110;&#20154;&#31867;&#25512;&#29702;&#30340;&#28508;&#21147;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#30028;&#20105;&#35758;&#26368;&#28608;&#28872;&#30340;&#35805;&#39064;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#22810;&#26041;&#38754;&#30340;&#65292;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#24418;&#24335;&#36827;&#34892;&#20307;&#29616;&#65292;&#21253;&#25324;&#31867;&#27604;&#12289;&#31354;&#38388;&#21644;&#36947;&#24503;&#25512;&#29702;&#31561;&#12290;&#36825;&#19968;&#20107;&#23454;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;LLM&#33021;&#21542;&#22312;&#25152;&#26377;&#36825;&#20123;&#19981;&#21516;&#39046;&#22495;&#20013;&#21516;&#26679;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#20351;&#29992;&#25110;&#20174;&#29616;&#26377;&#31867;&#27604;&#21644;&#31354;&#38388;&#25512;&#29702;&#25968;&#25454;&#38598;&#20013;&#27762;&#21462;&#21551;&#31034;&#65292;&#23545;LLM&#22312;&#19981;&#21516;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35780;&#20272;LLM&#20687;&#20154;&#31867;&#19968;&#26679;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#36824;&#23545;&#26356;&#24320;&#25918;&#12289;&#33258;&#28982;&#30340;&#35821;&#35328;&#38382;&#39064;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#22312;&#31867;&#27604;&#21644;&#36947;&#24503;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31354;&#38388;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#24471;&#19981;&#22815;&#29087;&#32451;&#12290;&#25105;&#35748;&#20026;&#36825;&#20123;&#23454;&#39564;&#23545;&#20110;&#25512;&#21160;LLM&#26410;&#26469;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#25913;&#36827;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential of large language models (LLMs) to reason like humans has been a highly contested topic in Machine Learning communities. However, the reasoning abilities of humans are multifaceted and can be seen in various forms, including analogical, spatial and moral reasoning, among others. This fact raises the question whether LLMs can perform equally well across all these different domains. This research work aims to investigate the performance of LLMs on different reasoning tasks by conducting experiments that directly use or draw inspirations from existing datasets on analogical and spatial reasoning. Additionally, to evaluate the ability of LLMs to reason like human, their performance is evaluted on more open-ended, natural language questions. My findings indicate that LLMs excel at analogical and moral reasoning, yet struggle to perform as proficiently on spatial reasoning tasks. I believe these experiments are crucial for informing the future development of LLMs, particularly 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#29616;&#26377;&#30340;&#21360;&#24230;WhatsApp&#24086;&#23376;&#25968;&#25454;&#38598;&#65292;&#21019;&#36896;&#20102;&#19968;&#20010;&#21487;&#20197;&#20174;WhatsApp&#24086;&#23376;&#20013;&#35782;&#21035;&#25361;&#34885;&#21477;&#23376;&#30340;&#27169;&#22411;PACO&#65292;&#24182;&#21033;&#29992;&#35813;&#27169;&#22411;&#21487;&#20197;&#38450;&#27490;&#21487;&#33021;&#30340;&#27495;&#35270;&#25110;&#26292;&#21147;&#20107;&#20214;&#12290;</title><link>http://arxiv.org/abs/2303.12808</link><description>&lt;p&gt;
PACO: &#21253;&#21547;&#34892;&#21160;&#12289;&#25991;&#21270;&#21644;&#21387;&#36843;&#30340;&#25361;&#34885;
&lt;/p&gt;
&lt;p&gt;
PACO: Provocation Involving Action, Culture, and Oppression. (arXiv:2303.12808v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12808
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#29616;&#26377;&#30340;&#21360;&#24230;WhatsApp&#24086;&#23376;&#25968;&#25454;&#38598;&#65292;&#21019;&#36896;&#20102;&#19968;&#20010;&#21487;&#20197;&#20174;WhatsApp&#24086;&#23376;&#20013;&#35782;&#21035;&#25361;&#34885;&#21477;&#23376;&#30340;&#27169;&#22411;PACO&#65292;&#24182;&#21033;&#29992;&#35813;&#27169;&#22411;&#21487;&#20197;&#38450;&#27490;&#21487;&#33021;&#30340;&#27495;&#35270;&#25110;&#26292;&#21147;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21360;&#24230;&#65292;&#20154;&#20204;&#26681;&#25454;&#26576;&#20123;&#23646;&#24615;&#65288;&#22914;&#23447;&#25945;&#65289;&#35748;&#21516;&#20110;&#29305;&#23450;&#32676;&#20307;&#65292;&#21516;&#19968;&#23447;&#25945;&#32676;&#20307;&#32463;&#24120;&#30456;&#20114;&#25361;&#34885;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25361;&#34885;&#22312;&#22686;&#21152;&#21360;&#24230;&#20004;&#20010;&#20027;&#35201;&#23447;&#25945;&#32676;&#20307;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#26041;&#38754;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#20986;&#29616;&#65292;&#36825;&#31181;&#25361;&#34885;&#20063;&#20986;&#29616;&#22312;WhatsApp&#31561;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#30340;&#21360;&#24230;WhatsApp&#24086;&#23376;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#20102;&#19977;&#31181;&#38024;&#23545;&#21360;&#24230;&#31302;&#26031;&#26519;&#30340;&#25361;&#34885;&#21477;&#23376;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#19977;&#31181;&#25361;&#34885;&#31867;&#21035;&#26631;&#35760;&#20102;7000&#20010;&#21477;&#23376;&#65292;&#24182;&#23558;&#20854;&#31216;&#20026;PACO&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;PACO&#26469;&#35757;&#32451;&#19968;&#20010;&#21487;&#20197;&#20174;WhatsApp&#24086;&#23376;&#20013;&#35782;&#21035;&#25361;&#34885;&#21477;&#23376;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#26159;&#31934;&#35843;RoBERTa&#65292;&#24182;&#22312;&#20116;&#20493;&#20132;&#21449;&#39564;&#35777;&#20013;&#23454;&#29616;&#20102;0.851&#30340;&#24179;&#22343;AUC&#20998;&#25968;&#12290;&#33258;&#21160;&#35782;&#21035;&#25361;&#34885;&#21477;&#23376;&#21487;&#20197;&#38459;&#27490;&#25361;&#34885;&#25991;&#26412;&#25193;&#25955;&#21040;&#32676;&#20247;&#20043;&#38388;&#65292;&#21487;&#20197;&#38450;&#27490;&#21487;&#33021;&#30340;&#27495;&#35270;&#25110;&#26292;&#21147;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In India, people identify with a particular group based on certain attributes such as religion. The same religious groups are often provoked against each other. Previous studies show the role of provocation in increasing tensions between India's two prominent religious groups: Hindus and Muslims. With the advent of the Internet, such provocation also surfaced on social media platforms such as WhatsApp.  By leveraging an existing dataset of Indian WhatsApp posts, we identified three categories of provoking sentences against Indian Muslims. Further, we labeled 7,000 sentences for three provocation categories and called this dataset PACO. We leveraged PACO to train a model that can identify provoking sentences from a WhatsApp post. Our best model is fine-tuned RoBERTa and achieved a 0.851 average AUC score over five-fold cross-validation. Automatically identifying provoking sentences could stop provoking text from reaching out to the masses, and can prevent possible discrimination or viol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#34892;&#29305;&#24449;&#21305;&#37197;&#30340;&#26032;&#28151;&#21512;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20943;&#23569;&#21305;&#37197;&#19981;&#21516;&#25968;&#25454;&#38598;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.12804</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#34892;&#29305;&#24449;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Features matching using natural language processing. (arXiv:2303.12804v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#34892;&#29305;&#24449;&#21305;&#37197;&#30340;&#26032;&#28151;&#21512;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20943;&#23569;&#21305;&#37197;&#19981;&#21516;&#25968;&#25454;&#38598;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#21305;&#37197;&#26159;&#21305;&#37197;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#22522;&#26412;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30001;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;BERT&#27169;&#22411;&#19982;&#22522;&#20110;Jaccard&#30456;&#20284;&#24230;&#30340;&#32479;&#35745;&#27169;&#22411;&#24182;&#34892;&#20351;&#29992;&#65292;&#20197;&#27979;&#37327;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#29305;&#24449;&#21015;&#34920;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#36825;&#20943;&#23569;&#20102;&#25628;&#32034;&#30456;&#20851;&#24615;&#25110;&#25163;&#21160;&#21305;&#37197;&#27599;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#29305;&#24449;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The feature matching is a basic step in matching different datasets. This article proposes shows a new hybrid model of a pretrained Natural Language Processing (NLP) based model called BERT used in parallel with a statistical model based on Jaccard similarity to measure the similarity between list of features from two different datasets. This reduces the time required to search for correlations or manually match each feature from one dataset to another.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#25688;&#35201;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;......</title><link>http://arxiv.org/abs/2303.12796</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Abstractive Text Summarization Using Pre-trained Models. (arXiv:2303.12796v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#25688;&#35201;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;......
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#29616;&#22312;&#20351;&#29992;&#20687;&#35895;&#27468;&#12289;&#38597;&#34382;&#21644;&#24517;&#24212;&#36825;&#26679;&#30340;&#25628;&#32034;&#24341;&#25806;&#26469;&#26597;&#25214;&#20114;&#32852;&#32593;&#19978;&#30340;&#20449;&#24687;&#12290;&#30001;&#20110;&#25968;&#25454;&#29190;&#28856;&#65292;&#22914;&#26524;&#20026;&#29992;&#25143;&#25552;&#20379;&#30456;&#20851;&#30340;&#25628;&#32034;&#32467;&#26524;&#25688;&#35201;&#32780;&#19981;&#20165;&#20165;&#26159;&#32593;&#39029;&#38142;&#25509;&#23558;&#20250;&#24456;&#26377;&#24110;&#21161;&#12290;&#25991;&#26412;&#25688;&#35201;&#24050;&#25104;&#20026;&#24110;&#21161;&#29992;&#25143;&#36805;&#36895;&#25484;&#25569;&#22823;&#37327;&#20449;&#24687;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#23545;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20998;&#21035;&#26159;google/pegasus-cnn-dailymail&#12289;T5-base&#12289;facebook/bart-large-cnn&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#26159;CNN-dailymail&#12289;SAMSum&#21644;BillSum&#65292;&#20197;&#20174;&#19978;&#36848;&#19977;&#20010;&#27169;&#22411;&#20013;&#33719;&#21462;&#36755;&#20986;&#12290;&#36890;&#36807;ROUGH&#21644;BLEU&#25351;&#26631;&#65292;&#22312;&#36825;&#20123;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#20102;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#26377;2000&#20010;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
People nowadays use search engines like Google, Yahoo, and Bing to find information on the Internet. Due to explosion in data, it is helpful for users if they are provided relevant summaries of the search results rather than just links to webpages. Text summarization has become a vital approach to help consumers swiftly grasp vast amounts of information.In this paper, different pre-trained models for text summarization are evaluated on different datasets. Specifically, we have used three different pre-trained models, namely, google/pegasus-cnn-dailymail, T5-base, facebook/bart-large-cnn. We have considered three different datasets, namely, CNN-dailymail, SAMSum and BillSum to get the output from the above three models. The pre-trained models are compared over these different datasets, each of 2000 examples, through ROUGH and BLEU metrics.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#20142;&#28857;&#65292;&#25506;&#31350;&#20854;&#26159;&#21542;&#33021;&#25552;&#39640;&#29983;&#25104;&#20142;&#28857;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#21629;&#21517;&#23454;&#20307;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#20142;&#28857;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12795</link><description>&lt;p&gt;
&#22522;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#30740;&#31350;&#20142;&#28857;&#33258;&#21160;&#29983;&#25104;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition Based Automatic Generation of Research Highlights. (arXiv:2303.12795v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12795
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#20142;&#28857;&#65292;&#25506;&#31350;&#20854;&#26159;&#21542;&#33021;&#25552;&#39640;&#29983;&#25104;&#20142;&#28857;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#21629;&#21517;&#23454;&#20307;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#20142;&#28857;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#31185;&#23398;&#35770;&#25991;&#25688;&#35201;&#29992;&#20110;&#24635;&#32467;&#35770;&#25991;&#20869;&#23481;&#12290;&#36817;&#26399;&#65292;&#30740;&#31350;&#20142;&#28857;&#20316;&#20026;&#25688;&#35201;&#30340;&#34917;&#20805;&#65292;&#32858;&#28966;&#20110;&#35770;&#25991;&#30340;&#20027;&#35201;&#21457;&#29616;&#65292;&#20294;&#20351;&#29992;&#39057;&#29575;&#36824;&#19981;&#22914;&#25688;&#35201;&#26222;&#36941;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#35770;&#25991;&#19981;&#21516;&#37096;&#20998;&#30340;&#36755;&#20837;&#65292;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#20142;&#28857;&#12290;&#30740;&#31350;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#65292;&#25506;&#31350;&#23427;&#33021;&#21542;&#25913;&#36827;&#29983;&#25104;&#30740;&#31350;&#20142;&#28857;&#30340;&#36136;&#37327;&#12290;&#30740;&#31350;&#20351;&#29992;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65306;&#31532;&#19968;&#20010;&#26159;&#25351;&#38024;-&#29983;&#25104;&#22120;&#32593;&#32476;&#65292;&#31532;&#20108;&#20010;&#22312;&#31532;&#19968;&#20010;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#22686;&#21152;&#20102;&#35206;&#30422;&#26426;&#21046;&#12290; &#28982;&#21518;&#23558;&#19978;&#36848;&#27599;&#20010;&#27169;&#22411;&#19982;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#29305;&#24449;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20026;&#32570;&#23569;&#20142;&#28857;&#30340;&#35770;&#25991;&#29983;&#25104;&#20142;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22686;&#21152;&#21629;&#21517;&#23454;&#20307;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30740;&#31350;&#20142;&#28857;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A scientific paper is traditionally prefaced by an abstract that summarizes the paper. Recently, research highlights that focus on the main findings of the paper have emerged as a complementary summary in addition to an abstract. However, highlights are not yet as common as abstracts, and are absent in many papers. In this paper, we aim to automatically generate research highlights using different sections of a research paper as input. We investigate whether the use of named entity recognition on the input improves the quality of the generated highlights. In particular, we have used two deep learning-based models: the first is a pointer-generator network, and the second augments the first model with coverage mechanism. We then augment each of the above models with named entity recognition features. The proposed method can be used to produce highlights for papers with missing highlights. Our experiments show that adding named entity information improves the performance of the deep learn
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;cTBL&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#34920;&#26684;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#26816;&#32034;&#20449;&#24687;&#25903;&#25745;&#30340;&#23545;&#35805;&#21709;&#24212;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#23884;&#20837;&#36827;&#34892;&#27987;&#23494;&#34920;&#26816;&#32034;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12024</link><description>&lt;p&gt;
cTBL&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23545;&#35805;&#34920;&#26684;
&lt;/p&gt;
&lt;p&gt;
cTBL: Augmenting Large Language Models for Conversational Tables. (arXiv:2303.12024v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;cTBL&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#34920;&#26684;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#26816;&#32034;&#20449;&#24687;&#25903;&#25745;&#30340;&#23545;&#35805;&#21709;&#24212;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#23884;&#20837;&#36827;&#34892;&#27987;&#23494;&#34920;&#26816;&#32034;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#20013;&#19968;&#20010;&#24320;&#25918;&#30340;&#25361;&#25112;&#26159;&#22914;&#20309;&#20174;&#25991;&#26412;&#21644;&#38750;&#25991;&#26412;&#26469;&#28304;&#20013;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;Conversation Table (cTBL)&#65292;&#36825;&#26159;&#19968;&#31181;&#19977;&#27493;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#32034;&#34920;&#26684;&#20449;&#24687;&#24182;&#29983;&#25104;&#22522;&#20110;&#26816;&#32034;&#20449;&#24687;&#30340;&#23545;&#35805;&#21709;&#24212;&#12290;cTBL&#20351;&#29992;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#23884;&#20837;&#36827;&#34892;&#27987;&#23494;&#34920;&#26816;&#32034;&#65292;&#24182;&#22312;HyrbiDialogue&#25968;&#25454;&#38598;Top-1&#21644;Top-3&#20934;&#30830;&#24615;&#19978;&#30456;&#23545;&#20110;&#31232;&#30095;&#26816;&#32034;&#25552;&#39640;&#20102;&#26368;&#22810;5%&#12290;&#27492;&#22806;&#65292;cTBL&#20351;&#29992;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#30693;&#35782;&#26816;&#32034;&#65292;&#22312;HyrbiDialogue&#19978;&#20135;&#29983;&#20102;&#26368;&#39640;46%&#30340;ROUGE&#20998;&#25968;&#30456;&#23545;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20154;&#24037;&#35780;&#20272;&#21709;&#24212;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
An open challenge in multimodal conversational AI requires augmenting large language models with information from textual and non-textual sources for multi-turn dialogue. To address this problem, this paper introduces Conversational Tables (cTBL), a three-step encoder-decoder approach to retrieve tabular information and generate dialogue responses grounded on the retrieved information. cTBL uses Transformer encoder embeddings for Dense Table Retrieval and obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over sparse retrieval on the HyrbiDialogue dataset. Additionally, cTBL performs tabular knowledge retrieval using both encoder and decoder models, resulting in up to 46% relative improvement in ROUGE scores and better human evaluation for response generation on HyrbiDialogue.
&lt;/p&gt;</description></item><item><title>Memotion 3&#26159;&#19968;&#20010;&#21253;&#21547;10,000&#20010;&#24050;&#27880;&#37322;&#27169;&#22240;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#20102;&#21360;&#24230;-&#33521;&#35821;&#28151;&#21512;&#27169;&#22240;&#65292;&#20351;&#20854;&#25104;&#20026;&#35813;&#39046;&#22495;&#20869;&#39318;&#20010;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#65292;&#24182;&#21487;&#29992;&#20110;&#23545;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#34394;&#20551;&#20449;&#24687;&#25110;&#20167;&#24680;&#20869;&#23481;&#36827;&#34892;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.09892</link><description>&lt;p&gt;
Memotion 3: &#20195;&#34920;&#21360;&#24230;-&#33521;&#35821;&#28151;&#21512;&#30721;&#30340;&#24773;&#24863;&#19982;&#24773;&#32490;&#20998;&#26512;&#30340;&#20114;&#32852;&#32593;&#27169;&#22240;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Memotion 3: Dataset on sentiment and emotion analysis of codemixed Hindi-English Memes. (arXiv:2303.09892v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09892
&lt;/p&gt;
&lt;p&gt;
Memotion 3&#26159;&#19968;&#20010;&#21253;&#21547;10,000&#20010;&#24050;&#27880;&#37322;&#27169;&#22240;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#20102;&#21360;&#24230;-&#33521;&#35821;&#28151;&#21512;&#27169;&#22240;&#65292;&#20351;&#20854;&#25104;&#20026;&#35813;&#39046;&#22495;&#20869;&#39318;&#20010;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#65292;&#24182;&#21487;&#29992;&#20110;&#23545;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#34394;&#20551;&#20449;&#24687;&#25110;&#20167;&#24680;&#20869;&#23481;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22240;&#26159;&#29616;&#20170;&#31038;&#20132;&#23186;&#20307;&#19978;&#20256;&#36798;&#24189;&#40664;&#30340;&#26032;&#22411;&#26426;&#21046;&#12290;&#27169;&#22240;&#36890;&#24120;&#21253;&#21547;&#22270;&#29255;&#21644;&#19968;&#20123;&#25991;&#26412;&#12290;&#27169;&#22240;&#21487;&#34987;&#29992;&#20110;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#25110;&#20167;&#24680;&#65292;&#22240;&#27492;&#23545;&#20854;&#36827;&#34892;&#35814;&#32454;&#30340;&#30740;&#31350;&#38750;&#24120;&#20851;&#38190;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Memotion 3&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;10,000&#20010;&#24050;&#27880;&#37322;&#27169;&#22240;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#19982;&#39046;&#22495;&#20869;&#20854;&#20182;&#26222;&#36941;&#30340;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#21253;&#25324;&#20043;&#21069;&#30340;Memotion&#65292;Memotion 3&#24341;&#20837;&#20102;&#21360;&#24230;-&#33521;&#35821;&#28151;&#21512;&#27169;&#22240;&#65292;&#32780;&#20043;&#21069;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#33521;&#35821;&#27169;&#22240;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;Memotion&#20219;&#21153;&#12289;&#25968;&#25454;&#25910;&#38598;&#21644;&#25968;&#25454;&#38598;&#21019;&#24314;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#20026;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#12290;&#22522;&#20934;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#23558;&#22312; https://github.com/Shreyashm16/Memotion-3.0 &#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memes are the new-age conveyance mechanism for humor on social media sites. Memes often include an image and some text. Memes can be used to promote disinformation or hatred, thus it is crucial to investigate in details. We introduce Memotion 3, a new dataset with 10,000 annotated memes. Unlike other prevalent datasets in the domain, including prior iterations of Memotion, Memotion 3 introduces Hindi-English Codemixed memes while prior works in the area were limited to only the English memes. We describe the Memotion task, the data collection and the dataset creation methodologies. We also provide a baseline for the task. The baseline code and dataset will be made available at https://github.com/Shreyashm16/Memotion-3.0
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;TransFusion&#26550;&#26500;&#65292;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24635;&#32467;&#21160;&#20316;&#19978;&#19979;&#25991;&#65292;&#23454;&#29616;&#23545;&#22810;&#27169;&#24577;&#23545;&#35937;&#20132;&#20114;&#30340;&#39044;&#27979;&#65292;&#26377;&#25928;&#24615;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.09209</link><description>&lt;p&gt;
&#24635;&#32467;&#36807;&#21435;&#20197;&#39044;&#27979;&#26410;&#26469;&#65306;&#33258;&#28982;&#35821;&#35328;&#23545;&#22330;&#26223;&#30340;&#25551;&#36848;&#20419;&#36827;&#22810;&#27169;&#24577;&#23545;&#35937;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction. (arXiv:2301.09209v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;TransFusion&#26550;&#26500;&#65292;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24635;&#32467;&#21160;&#20316;&#19978;&#19979;&#25991;&#65292;&#23454;&#29616;&#23545;&#22810;&#27169;&#24577;&#23545;&#35937;&#20132;&#20114;&#30340;&#39044;&#27979;&#65292;&#26377;&#25928;&#24615;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#20013;&#30340;&#23545;&#35937;&#20132;&#20114;&#39044;&#27979;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#35813;&#20219;&#21153;&#38656;&#35201;&#29702;&#35299;&#20808;&#21069;&#23545;&#23545;&#35937;&#25191;&#34892;&#30340;&#21160;&#20316;&#25152;&#24418;&#25104;&#30340;&#26102;&#31354;&#19978;&#19979;&#25991;&#65292;&#31216;&#20026;&#21160;&#20316;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;transformer&#30340;&#26550;&#26500;TransFusion&#12290;&#23427;&#21033;&#29992;&#35821;&#35328;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23545;&#21160;&#20316;&#19978;&#19979;&#25991;&#36827;&#34892;&#24635;&#32467;&#12290;TransFusion&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20174;&#36807;&#21435;&#30340;&#35270;&#39057;&#24103;&#20013;&#25552;&#21462;&#21160;&#20316;&#19978;&#19979;&#25991;&#12290;&#23558;&#36825;&#20010;&#21160;&#20316;&#19978;&#19979;&#25991;&#19982;&#19979;&#19968;&#20010;&#35270;&#39057;&#24103;&#19968;&#36215;&#32463;&#36807;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#36827;&#34892;&#22788;&#29702;&#65292;&#20174;&#32780;&#39044;&#27979;&#19979;&#19968;&#20010;&#23545;&#35937;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21017;&#22686;&#21152;&#20102;&#36890;&#29992;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;Ego4D&#21644;EPIC-KITCHENS-100&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20063;&#20984;&#26174;&#20102;&#22312;&#19968;&#20010;&#35270;&#35273;&#20284;&#20046;&#36275;&#22815;&#30340;&#20219;&#21153;&#20013;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#25688;&#35201;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study object interaction anticipation in egocentric videos. This task requires an understanding of the spatiotemporal context formed by past actions on objects, coined action context. We propose TransFusion, a multimodal transformer-based architecture. It exploits the representational power of language by summarising the action context. TransFusion leverages pre-trained image captioning and vision-language models to extract the action context from past video frames. This action context together with the next video frame is processed by the multimodal fusion module to forecast the next object interaction. Our model enables more efficient end-to-end learning. The large pre-trained language models add common sense and a generalisation capability. Experiments on Ego4D and EPIC-KITCHENS-100 show the effectiveness of our multimodal fusion model. They also highlight the benefits of using language-based context summaries in a task where vision seems to suffice. Our method outperforms state-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#20026;&#23558;&#26356;&#22810;&#19978;&#19979;&#25991;&#20449;&#24687;&#32435;&#20837;Skip-gram&#26694;&#26550;&#21644;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#20808;&#39564;&#21516;&#20041;&#35789;&#30693;&#35782;&#21644;&#21152;&#26435;&#21521;&#37327;&#20998;&#24067;&#30340;&#38745;&#24577;&#23884;&#20837;&#21518;&#22788;&#29702;&#35013;&#37197;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#32463;&#30001;&#22806;&#37096;&#21644;&#20869;&#37096;&#20219;&#21153;&#30340;&#26816;&#39564;&#65292;&#33021;&#22815;&#22823;&#24133;&#24230;&#36229;&#36234;&#22522;&#20934;&#32447;&#12290;</title><link>http://arxiv.org/abs/2210.16848</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#21521;&#37327;&#21644;&#22270;&#24418;&#35013;&#37197;&#25913;&#36827;&#35789;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings. (arXiv:2210.16848v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#20026;&#23558;&#26356;&#22810;&#19978;&#19979;&#25991;&#20449;&#24687;&#32435;&#20837;Skip-gram&#26694;&#26550;&#21644;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#20808;&#39564;&#21516;&#20041;&#35789;&#30693;&#35782;&#21644;&#21152;&#26435;&#21521;&#37327;&#20998;&#24067;&#30340;&#38745;&#24577;&#23884;&#20837;&#21518;&#22788;&#29702;&#35013;&#37197;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#32463;&#30001;&#22806;&#37096;&#21644;&#20869;&#37096;&#20219;&#21153;&#30340;&#26816;&#39564;&#65292;&#33021;&#22815;&#22823;&#24133;&#24230;&#36229;&#36234;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20174;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#30001;&#20110;&#35745;&#31639;&#25104;&#26412;&#20302;&#12289;&#37096;&#32626;&#20415;&#25463;&#12289;&#31283;&#23450;&#24615;&#39640;&#65292;&#20256;&#32479;&#30340;&#38745;&#24577;&#23884;&#20837;&#65288;&#20363;&#22914;Skip-gram&#12289;Word2Vec&#65289;&#20173;&#22312;&#20302;&#36164;&#28304;&#21644;&#36731;&#37327;&#32423;&#29615;&#22659;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20197;&#19979;&#26041;&#27861;&#25913;&#36827;&#35789;&#23884;&#20837;&#65306;1&#65289;&#23558;&#26356;&#22810;&#20174;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#32435;&#20837;Skip-gram&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;Context-to-Vec&#65307;2&#65289;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#20808;&#39564;&#21516;&#20041;&#35789;&#30693;&#35782;&#21644;&#21152;&#26435;&#21521;&#37327;&#20998;&#24067;&#30340;&#38745;&#24577;&#23884;&#20837;&#21518;&#22788;&#29702;&#35013;&#37197;&#26041;&#27861;&#65292;&#29420;&#31435;&#20110;&#35757;&#32451;&#12290;&#36890;&#36807;&#22806;&#37096;&#21644;&#20869;&#37096;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#35777;&#26126;&#33021;&#22815;&#20197;&#22823;&#24133;&#36229;&#36234;&#22522;&#20934;&#32447;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although contextualized embeddings generated from large-scale pre-trained models perform well in many tasks, traditional static embeddings (e.g., Skip-gram, Word2Vec) still play an important role in low-resource and lightweight settings due to their low computational cost, ease of deployment, and stability. In this paper, we aim to improve word embeddings by 1) incorporating more contextual information from existing pre-trained models into the Skip-gram framework, which we call Context-to-Vec; 2) proposing a post-processing retrofitting method for static embeddings independent of training by employing priori synonym knowledge and weighted vector distribution. Through extrinsic and intrinsic tasks, our methods are well proven to outperform the baselines by a large margin.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#33258;&#21160;&#35780;&#20272;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;&#36739;&#23569;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#21521;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21270;&#65292;&#24182;&#19988;&#30456;&#23545;&#25552;&#39640;&#20102;1.25%&#30340;F1-score&#12290;</title><link>http://arxiv.org/abs/2210.15387</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#27169;&#22411;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#21457;&#38899;&#38556;&#30861;&#33258;&#21160;&#20005;&#37325;&#31243;&#24230;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Automatic Severity Assessment of Dysarthric speech by using Self-supervised Model with Multi-task Learning. (arXiv:2210.15387v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#33258;&#21160;&#35780;&#20272;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;&#36739;&#23569;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#21521;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21270;&#65292;&#24182;&#19988;&#30456;&#23545;&#25552;&#39640;&#20102;1.25%&#30340;F1-score&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#21457;&#38899;&#38556;&#30861;&#30340;&#20005;&#37325;&#31243;&#24230;&#23545;&#20110;&#25345;&#32493;&#27835;&#30103;&#21644;&#24247;&#22797;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#38750;&#20856;&#22411;&#21457;&#38899;&#30340;&#38590;&#24230;&#36739;&#22823;&#65292;&#24448;&#24448;&#20250;&#23548;&#33268;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#35780;&#20272;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#32852;&#21512;&#35757;&#32451;Wav2vec 2.0 XLS-R&#36827;&#34892;&#20004;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#21644;&#36741;&#21161;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#23545;&#20110;&#22522;&#20934;&#23454;&#39564;&#65292;&#25105;&#20204;&#37319;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#22768;&#23398;&#29305;&#24449;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#22914;SVM&#12289;MLP&#21644;XGBoost&#12290;&#22312;&#38889;&#22269;&#21457;&#38899;&#38556;&#30861;QoLT&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#25506;&#31350;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;F1-score&#30456;&#23545;&#25552;&#39640;1.25%&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36229;&#36807;&#20102;&#27809;&#26377;ASR&#22836;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;10.61%&#30340;&#30456;&#23545;&#30334;&#20998;&#27604;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22914;&#20309;&#24433;&#21709;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic assessment of dysarthric speech is essential for sustained treatments and rehabilitation. However, obtaining atypical speech is challenging, often leading to data scarcity issues. To tackle the problem, we propose a novel automatic severity assessment method for dysarthric speech, using the self-supervised model in conjunction with multi-task learning. Wav2vec 2.0 XLS-R is jointly trained for two different tasks: severity classification and auxiliary automatic speech recognition (ASR). For the baseline experiments, we employ hand-crafted acoustic features and machine learning classifiers such as SVM, MLP, and XGBoost. Explored on the Korean dysarthric speech QoLT database, our model outperforms the traditional baseline methods, with a relative percentage increase of 1.25% for F1-score. In addition, the proposed model surpasses the model trained without ASR head, achieving 10.61% relative percentage improvements. Furthermore, we present how multi-task learning affects the seve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#31181;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20197;&#22810;&#35821;&#35328;&#26041;&#24335;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#21333;&#35821;&#35328;&#27169;&#22411;&#12289;&#23569;&#37327;&#25552;&#31034;&#33021;&#25945;&#25480;&#27169;&#22411;&#26032;&#35821;&#35328;&#20197;&#21450;&#22312;&#21333;&#35821;&#35328;&#35774;&#32622;&#19979;&#25317;&#26377;&#38646;-shot&#32763;&#35793;&#33021;&#21147;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.14868</link><description>&lt;p&gt;
&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multi-lingual Evaluation of Code Generation Models. (arXiv:2210.14868v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#31181;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20197;&#22810;&#35821;&#35328;&#26041;&#24335;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#21333;&#35821;&#35328;&#27169;&#22411;&#12289;&#23569;&#37327;&#25552;&#31034;&#33021;&#25945;&#25480;&#27169;&#22411;&#26032;&#35821;&#35328;&#20197;&#21450;&#22312;&#21333;&#35821;&#35328;&#35774;&#32622;&#19979;&#25317;&#26377;&#38646;-shot&#32763;&#35793;&#33021;&#21147;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65306;MBXP&#21644;Multilingual HumanEval&#65292;&#20197;&#21450;MathQA-X&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;10&#31181;&#20197;&#19978;&#30340;&#32534;&#31243;&#35821;&#35328;&#65292;&#24182;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#36716;&#25442;&#26694;&#26550;&#23558;&#21407;&#22987;Python&#25968;&#25454;&#38598;&#20013;&#30340;&#25552;&#31034;&#21644;&#27979;&#35797;&#29992;&#20363;&#36716;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#20013;&#30340;&#30456;&#24212;&#25968;&#25454;&#12290;&#21033;&#29992;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#33021;&#22815;&#20197;&#22810;&#35821;&#35328;&#26041;&#24335;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#39046;&#22495;&#35821;&#35328;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#21333;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#20248;&#21183;&#12289;&#23569;&#37327;&#25552;&#31034;&#25945;&#25480;&#27169;&#22411;&#26032;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#22312;&#21333;&#35821;&#35328;&#35774;&#32622;&#19979;&#30340;&#38646;-shot&#32763;&#35793;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#24341;&#23548;&#65292;&#20197;&#33719;&#21462;&#22810;&#31181;&#35821;&#35328;&#30340;&#21512;&#25104;&#35268;&#33539;&#35299;&#65292;&#36825;&#20123;&#35299;&#21487;&#29992;&#20110;&#20854;&#20182;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#35780;&#20272;&#65292;&#22914;&#20195;&#30721;&#25554;&#20837;&#12289;&#40065;&#26834;&#24615;&#25110;&#25688;&#35201;&#20219;&#21153;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;
&lt;/p&gt;
&lt;p&gt;
We present new benchmarks on evaluation code generation models: MBXP and Multilingual HumanEval, and MathQA-X. These datasets cover over 10 programming languages and are generated using a scalable conversion framework that transpiles prompts and test cases from the original Python datasets into the corresponding data in the target language. Using these benchmarks, we are able to assess the performance of code generation models in a multi-lingual fashion, and discovered generalization ability of language models on out-of-domain languages, advantages of multi-lingual models over mono-lingual, the ability of few-shot prompting to teach the model new languages, and zero-shot translation abilities even on mono-lingual settings. Furthermore, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages, which can be used for other code-related evaluations such as code insertion, robustness, or summarization tasks. Overall, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20010;AL&#21477;&#23376;&#26597;&#35810;&#35780;&#20272;&#20989;&#25968;&#65292;&#20851;&#27880;&#28508;&#22312;&#27491;&#38754;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#26356;&#22909;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24120;&#21270;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;NER&#27880;&#37322;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2111.03837</link><description>&lt;p&gt;
&#38598;&#20013;&#20851;&#27880;&#28508;&#22312;&#21629;&#21517;&#23454;&#20307;&#30340;&#20027;&#21160;&#26631;&#27880;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Focusing on Potential Named Entities During Active Label Acquisition. (arXiv:2111.03837v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.03837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20010;AL&#21477;&#23376;&#26597;&#35810;&#35780;&#20272;&#20989;&#25968;&#65292;&#20851;&#27880;&#28508;&#22312;&#27491;&#38754;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#26356;&#22909;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24120;&#21270;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;NER&#27880;&#37322;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#26088;&#22312;&#35782;&#21035;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#21629;&#21517;&#23454;&#20307;&#30340;&#25552;&#21450;&#24182;&#23558;&#20854;&#20998;&#31867;&#21040;&#39044;&#23450;&#20041;&#30340;&#21629;&#21517;&#23454;&#20307;&#31867;&#21035;&#20013;&#12290;&#34429;&#28982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#22312;NER&#20013;&#23454;&#29616;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#35768;&#22810;&#29305;&#23450;&#39046;&#22495;&#30340;NER&#24212;&#29992;&#20173;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;&#20027;&#21160;&#23398;&#20064;(AL)&#26159;&#35299;&#20915;&#26631;&#31614;&#33719;&#21462;&#38382;&#39064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24050;&#29992;&#20110;NER&#20219;&#21153;&#65292;&#20197;&#26368;&#23567;&#21270;&#27880;&#37322;&#25104;&#26412;&#32780;&#19981;&#29306;&#29298;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26631;&#35760;&#30340;&#20005;&#37325;&#19981;&#22343;&#21248;&#31867;&#20998;&#24067;&#24341;&#20837;&#20102;&#35774;&#35745;&#26377;&#25928;&#30340;NER&#20027;&#21160;&#23398;&#20064;&#26597;&#35810;&#26041;&#27861;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;AL&#21477;&#23376;&#26597;&#35810;&#35780;&#20272;&#20989;&#25968;&#65292;&#26356;&#22810;&#20851;&#27880;&#28508;&#22312;&#30340;&#27491;&#38754;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#21477;&#23376;&#21644;&#26631;&#35760;&#25104;&#26412;&#35780;&#20272;&#31574;&#30053;&#26469;&#35780;&#20272;&#36825;&#20123;&#25552;&#35758;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26356;&#22909;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24120;&#21270;&#26041;&#27861;&#65292;&#20197;&#24809;&#32602;&#36807;&#38271;&#25110;&#36807;&#30701;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named entity recognition (NER) aims to identify mentions of named entities in an unstructured text and classify them into predefined named entity classes. While deep learning-based pre-trained language models help to achieve good predictive performances in NER, many domain-specific NER applications still call for a substantial amount of labeled data. Active learning (AL), a general framework for the label acquisition problem, has been used for NER tasks to minimize the annotation cost without sacrificing model performance. However, the heavily imbalanced class distribution of tokens introduces challenges in designing effective AL querying methods for NER. We propose several AL sentence query evaluation functions that pay more attention to potential positive tokens, and evaluate these proposed functions with both sentence-based and token-based cost evaluation strategies. We also propose a better data-driven normalization approach to penalize sentences that are too long or too short. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#26367;&#20195;&#35821;&#20041;&#35299;&#26512;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#20381;&#36182;&#35299;&#26512;&#20219;&#21153;&#36827;&#34892;&#34920;&#36848;&#24182;&#24212;&#29992;&#20102;&#22522;&#20110;&#22270;&#30340;&#35299;&#30721;&#25216;&#26415;&#65292;&#26377;&#26395;&#25552;&#39640;&#37096;&#20998;&#27880;&#37322;&#25968;&#25454;&#30340;&#25928;&#29575;&#21644;&#25968;&#25454;&#20351;&#29992;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2109.04587</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#35299;&#30721;&#25216;&#26415;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#35821;&#20041;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Graph-Based Decoding for Task Oriented Semantic Parsing. (arXiv:2109.04587v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.04587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#26367;&#20195;&#35821;&#20041;&#35299;&#26512;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#20381;&#36182;&#35299;&#26512;&#20219;&#21153;&#36827;&#34892;&#34920;&#36848;&#24182;&#24212;&#29992;&#20102;&#22522;&#20110;&#22270;&#30340;&#35299;&#30721;&#25216;&#26415;&#65292;&#26377;&#26395;&#25552;&#39640;&#37096;&#20998;&#27880;&#37322;&#25968;&#25454;&#30340;&#25928;&#29575;&#21644;&#25968;&#25454;&#20351;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35821;&#20041;&#35299;&#26512;&#30340;&#20027;&#27969;&#33539;&#24335;&#26159;&#23558;&#35299;&#26512;&#20316;&#20026;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#65292;&#20351;&#29992;&#33258;&#22238;&#24402;&#24207;&#21015;&#35299;&#30721;&#22120;&#29983;&#25104;&#39044;&#27979;&#20540;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26367;&#20195;&#33539;&#24335;&#12290;&#25105;&#20204;&#23558;&#35821;&#20041;&#35299;&#26512;&#20316;&#20026;&#20381;&#36182;&#35299;&#26512;&#20219;&#21153;&#36827;&#34892;&#20102;&#34920;&#36848;&#65292;&#24212;&#29992;&#20102;&#38024;&#23545;&#21477;&#27861;&#35299;&#26512;&#24320;&#21457;&#30340;&#22522;&#20110;&#22270;&#30340;&#35299;&#30721;&#25216;&#26415;&#12290;&#25105;&#20204;&#22312; TOP &#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#20102;&#32473;&#23450;&#30456;&#21516;&#39044;&#20808;&#35757;&#32451;&#30340; Transformer &#32534;&#30721;&#22120;&#30340;&#21508;&#31181;&#35299;&#30721;&#25216;&#26415;&#65292;&#21253;&#25324;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#25110;&#20165;&#21253;&#21547;&#37096;&#20998;&#27880;&#37322;&#31034;&#20363;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#35774;&#32622;&#19978;&#19982;&#24207;&#21015;&#35299;&#30721;&#22120;&#30456;&#24403;&#31454;&#20105;&#65292;&#24182;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#37096;&#20998;&#27880;&#37322;&#25968;&#25454;&#21487;&#29992;&#30340;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm for semantic parsing in recent years is to formulate parsing as a sequence-to-sequence task, generating predictions with auto-regressive sequence decoders. In this work, we explore an alternative paradigm. We formulate semantic parsing as a dependency parsing task, applying graph-based decoding techniques developed for syntactic parsing. We compare various decoding techniques given the same pre-trained Transformer encoder on the TOP dataset, including settings where training data is limited or contains only partially-annotated examples. We find that our graph-based approach is competitive with sequence decoders on the standard setting, and offers significant improvements in data efficiency and settings where partially-annotated data is available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21306;&#20998;&#20845;&#31181;&#30456;&#20284;&#30340;&#21271;&#27431;&#35821;&#35328;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#24037;&#20855;&#30340;&#35823;&#20998;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2012.06431</link><description>&lt;p&gt;
&#21306;&#20998;&#30456;&#20284;&#30340;&#21271;&#27431;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Discriminating Between Similar Nordic Languages. (arXiv:2012.06431v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.06431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21306;&#20998;&#20845;&#31181;&#30456;&#20284;&#30340;&#21271;&#27431;&#35821;&#35328;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#24037;&#20855;&#30340;&#35823;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#35328;&#35782;&#21035;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#21306;&#20998;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#26041;&#38754;&#26356;&#21152;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21271;&#27431;&#35821;&#35328;&#33258;&#21160;&#35821;&#35328;&#35782;&#21035;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#20123;&#35821;&#35328;&#24448;&#24448;&#34987;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#24037;&#20855;&#35823;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#20851;&#27880;&#20845;&#31181;&#21271;&#27431;&#35821;&#35328;&#20043;&#38388;&#30340;&#21306;&#20998;&#65306;&#20025;&#40614;&#35821;&#12289;&#29790;&#20856;&#35821;&#12289;&#25386;&#23041;&#35821;&#65288;&#23612;&#35834;&#26031;&#20811;&#65289;&#12289;&#25386;&#23041;&#35821;&#65288;&#21338;&#20811;&#39532;&#23572;&#65289;&#12289;&#27861;&#32599;&#35821;&#21644;&#20912;&#23707;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic language identification is a challenging problem. Discriminating between closely related languages is especially difficult. This paper presents a machine learning approach for automatic language identification for the Nordic languages, which often suffer miscategorisation by existing state-of-the-art tools. Concretely we will focus on discrimination between six Nordic languages: Danish, Swedish, Norwegian (Nynorsk), Norwegian (Bokm{\aa}l), Faroese and Icelandic.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#26816;&#27979;&#25915;&#20987;&#24615;&#35821;&#35328;&#30340;&#20025;&#40614;&#25968;&#25454;&#38598;&#65292;&#24182;&#20026;&#33521;&#35821;&#21644;&#20025;&#40614;&#35821;&#30340;&#24694;&#24847;&#35821;&#35328;&#26816;&#27979;&#24314;&#31435;&#20102;&#26368;&#26032;&#25216;&#26415;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/1908.04531</link><description>&lt;p&gt;
Offensive Language and Hate Speech Detection for Danish&#65288;&#20025;&#40614;&#35821;&#30340;&#24694;&#24847;&#35821;&#35328;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65289;
&lt;/p&gt;
&lt;p&gt;
Offensive Language and Hate Speech Detection for Danish. (arXiv:1908.04531v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1908.04531
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#26816;&#27979;&#25915;&#20987;&#24615;&#35821;&#35328;&#30340;&#20025;&#40614;&#25968;&#25454;&#38598;&#65292;&#24182;&#20026;&#33521;&#35821;&#21644;&#20025;&#40614;&#35821;&#30340;&#24694;&#24847;&#35821;&#35328;&#26816;&#27979;&#24314;&#31435;&#20102;&#26368;&#26032;&#25216;&#26415;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#23384;&#22312;&#25915;&#20987;&#24615;&#35821;&#35328;&#21450;&#20854;&#21487;&#33021;&#24102;&#26469;&#30340;&#24433;&#21709;&#27491;&#25104;&#20026;&#29616;&#20195;&#31038;&#20250;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#30001;&#20110;&#27599;&#22825;&#21019;&#36896;&#30340;&#20869;&#23481;&#37327;&#24040;&#22823;&#65292;&#22240;&#27492;&#38656;&#35201;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#22788;&#29702;&#27492;&#31867;&#20869;&#23481;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#35299;&#20915;&#33521;&#35821;&#35821;&#35328;&#38382;&#39064;&#19978;&#65292;&#32780;&#36825;&#20010;&#38382;&#39064;&#26159;&#22810;&#35821;&#35328;&#30340;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;Reddit&#21644;Facebook&#20013;&#29992;&#25143;&#29983;&#25104;&#30340;&#35780;&#35770;&#30340;&#20025;&#40614;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#29992;&#25143;&#29983;&#25104;&#30340;&#35780;&#35770;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290; &#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#20197;&#25429;&#25417;&#21508;&#31181;&#25915;&#20987;&#24615;&#35821;&#35328;&#30340;&#31867;&#22411;&#21644;&#30446;&#26631;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22235;&#20010;&#33258;&#21160;&#20998;&#31867;&#31995;&#32479;&#65292;&#27599;&#20010;&#31995;&#32479;&#37117;&#35774;&#35745;&#29992;&#20110;&#33521;&#35821;&#21644;&#20025;&#40614;&#35821;&#12290;&#22312;&#33521;&#35821;&#25915;&#20987;&#24615;&#35821;&#35328;&#30340;&#26816;&#27979;&#20013;&#65292;&#26368;&#20339;&#24615;&#33021;&#31995;&#32479;&#30340;&#24179;&#22343;&#23439;F1&#24471;&#20998;&#20026;0.74&#65292;&#32780;&#23545;&#20110;&#20025;&#40614;&#35821;&#30340;&#26368;&#20339;&#31995;&#32479;&#65292;&#20854;&#24179;&#22343;&#23439;F1&#24471;&#20998;&#20026;0.67&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#26816;&#27979;&#25915;&#20987;&#24615;&#35821;&#35328;&#30340;&#20025;&#40614;&#25968;&#25454;&#38598;&#65307;&#20854;&#27425;&#65292;&#25105;&#20204;&#20026;&#33521;&#35821;&#21644;&#20025;&#40614;&#35821;&#30340;&#25915;&#20987;&#24615;&#35821;&#35328;&#26816;&#27979;&#24314;&#31435;&#20102;&#26368;&#26032;&#25216;&#26415;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The presence of offensive language on social media platforms and the implications this poses is becoming a major concern in modern society. Given the enormous amount of content created every day, automatic methods are required to detect and deal with this type of content. Until now, most of the research has focused on solving the problem for the English language, while the problem is multilingual.  We construct a Danish dataset containing user-generated comments from \textit{Reddit} and \textit{Facebook}. It contains user generated comments from various social media platforms, and to our knowledge, it is the first of its kind. Our dataset is annotated to capture various types and target of offensive language. We develop four automatic classification systems, each designed to work for both the English and the Danish language. In the detection of offensive language in English, the best performing system achieves a macro averaged F1-score of $0.74$, and the best performing system for Dani
&lt;/p&gt;</description></item></channel></rss>